{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Improving English to Sinhala Neural Machine Translation using Part-of-Speech Tag. (arXiv:2202.08882v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08882","description":"<p>The performance of Neural Machine Translation (NMT) depends significantly on\nthe size of the available parallel corpus. Due to this fact, low resource\nlanguage pairs demonstrate low translation performance compared to high\nresource language pairs. The translation quality further degrades when NMT is\nperformed for morphologically rich languages. Even though the web contains a\nlarge amount of information, most people in Sri Lanka are unable to read and\nunderstand English properly. Therefore, there is a huge requirement of\ntranslating English content to local languages to share information among\nlocals. Sinhala language is the primary language in Sri Lanka and building an\nNMT system that can produce quality English to Sinhala translations is\ndifficult due to the syntactic divergence between these two languages under low\nresource constraints. Thus, in this research, we explore effective methods of\nincorporating Part of Speech (POS) tags to the Transformer input embedding and\npositional encoding to further enhance the performance of the baseline English\nto Sinhala neural machine translation model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perera_R/0/1/0/all/0/1\">Ravinga Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseka_T/0/1/0/all/0/1\">Thilakshi Fonseka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naranpanawa_R/0/1/0/all/0/1\">Rashmini Naranpanawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thayasivam_U/0/1/0/all/0/1\">Uthayasanker Thayasivam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effects of Interactive AI Design on User Behavior: An Eye-tracking Study of Fact-checking COVID-19 Claims. (arXiv:2202.08901v1 [cs.HC])","link":"http://arxiv.org/abs/2202.08901","description":"<p>We conducted a lab-based eye-tracking study to investigate how the\ninteractivity of an AI-powered fact-checking system affects user interactions,\nsuch as dwell time, attention, and mental resources involved in using the\nsystem. A within-subject experiment was conducted, where participants used an\ninteractive and a non-interactive version of a mock AI fact-checking system and\nrated their perceived correctness of COVID-19 related claims. We collected\nweb-page interactions, eye-tracking data, and mental workload using NASA-TLX.\nWe found that the presence of the affordance of interactively manipulating the\nAI system's prediction parameters affected users' dwell times, and\neye-fixations on AOIs, but not mental workload. In the interactive system,\nparticipants spent the most time evaluating claims' correctness, followed by\nreading news. This promising result shows a positive role of interactivity in a\nmixed-initiative AI-powered system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Li Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_N/0/1/0/all/0/1\">Nilavra Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anubrata Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1\">Matthew Lease</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwidzka_J/0/1/0/all/0/1\">Jacek Gwidzka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGPT: GPT Sentence Embeddings for Semantic Search. (arXiv:2202.08904v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08904","description":"<p>GPT transformers are the largest language models available, yet semantic\nsearch is dominated by BERT transformers. We present SGPT-BE and SGPT-CE for\napplying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric\nsearch.\n</p>\n<p>SGPT-BE produces semantically meaningful sentence embeddings by contrastive\nfine-tuning of only bias tensors and a novel pooling method. A 5.8 billion\nparameter SGPT-BE outperforms the best available sentence embeddings by 6%\nsetting a new state-of-the-art on BEIR. It outperforms the concurrently\nproposed OpenAI Embeddings of the 175B Davinci endpoint, which fine-tunes\n250,000 times more parameters.\n</p>\n<p>SGPT-CE uses log probabilities from GPT models without any fine-tuning. A 6.1\nbillion parameter SGPT-CE sets an unsupervised state-of-the-art on BEIR. It\nbeats the supervised state-of-the-art on 7 datasets, but significantly loses on\nother datasets. We show how this can be alleviated by adapting the prompt.\n</p>\n<p>SGPT-BE and SGPT-CE performance scales with model size. Yet, increased\nlatency, storage and compute costs should be considered. Code, models and\nresult files are freely available at https://github.com/Muennighoff/sgpt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing Effective Sparse Expert Models. (arXiv:2202.08906v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08906","description":"<p>Scale has opened new frontiers in natural language processing -- but at a\nhigh cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have\nbeen proposed as an energy efficient path to even larger and more capable\nlanguage models. But advancing the state-of-the-art across a broad set of\nnatural language tasks has been hindered by training instabilities and\nuncertain quality during fine-tuning. Our work focuses on these issues and acts\nas a design guide. We conclude by scaling a sparse model to 269B parameters,\nwith a computational cost comparable to a 32B dense encoder-decoder Transformer\n(Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time,\na sparse model achieves state-of-the-art performance in transfer learning,\nacross a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC\nChallenge), summarization (XSum, CNN-DM), closed book question answering\n(WebQA, Natural Questions), and adversarially constructed tasks (Winogrande,\nANLI R3).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_I/0/1/0/all/0/1\">Irwan Bello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sameer Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Fine-Grained Semantics in Knowledge Graph Relations. (arXiv:2202.08917v1 [cs.AI])","link":"http://arxiv.org/abs/2202.08917","description":"<p>When it comes to comprehending and analyzing multi-relational data, the\nsemantics of relations are crucial. Polysemous relations between different\ntypes of entities, that represent multiple semantics, are common in real-world\nrelational datasets represented by knowledge graphs. For numerous use cases,\nsuch as entity type classification, question answering and knowledge graph\ncompletion, the correct semantic interpretation of these relations is\nnecessary. In this work, we provide a strategy for discovering the different\nsemantics associated with abstract relations and deriving many sub-relations\nwith fine-grained meaning. To do this, we leverage the types of the entities\nassociated with the relations and cluster the vector representations of\nentities and relations. The suggested method is able to automatically discover\nthe best number of sub-relations for a polysemous relation and determine their\nsemantic interpretation, according to our empirical evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nitisha Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krestel_R/0/1/0/all/0/1\">Ralf Krestel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Intrinsic Exploration with Language Abstractions. (arXiv:2202.08938v1 [cs.LG])","link":"http://arxiv.org/abs/2202.08938","description":"<p>Reinforcement learning (RL) agents are particularly hard to train when\nrewards are sparse. One common solution is to use intrinsic rewards to\nencourage agents to explore their environment. However, recent intrinsic\nexploration methods often use state-based novelty measures which reward\nlow-level exploration and may not scale to domains requiring more abstract\nskills. Instead, we explore natural language as a general medium for\nhighlighting relevant abstractions in an environment. Unlike previous work, we\nevaluate whether language can improve over existing exploration methods by\ndirectly extending (and comparing to) competitive intrinsic exploration\nbaselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These\nlanguage-based variants outperform their non-linguistic forms by 45-85% across\n13 challenging tasks from the MiniGrid and MiniHack environment suites.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jesse Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1\">Victor Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1\">Roberta Raileanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Minqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1\">Edward Grefenstette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Pretrained Models of Source Code. (arXiv:2202.08975v1 [cs.SE])","link":"http://arxiv.org/abs/2202.08975","description":"<p>Deep learning models are widely used for solving challenging code processing\ntasks, such as code generation or code summarization. Traditionally, a specific\nmodel architecture was carefully built to solve a particular code processing\ntask. However, recently general pretrained models such as CodeBERT or CodeT5\nhave been shown to outperform task-specific models in many applications. While\npretrained models are known to learn complex patterns from data, they may fail\nto understand some properties of source code. To test diverse aspects of code\nunderstanding, we introduce a set of diagnosting probing tasks. We show that\npretrained models of code indeed contain information about code syntactic\nstructure and correctness, the notions of identifiers, data flow and\nnamespaces, and natural language naming. We also investigate how probing\nresults are affected by using code-specific pretraining objectives, varying the\nmodel size, or finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Troshin_S/0/1/0/all/0/1\">Sergey Troshin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end contextual asr based on posterior distribution adaptation for hybrid ctc/attention system. (arXiv:2202.09003v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09003","description":"<p>End-to-end (E2E) speech recognition architectures assemble all components of\ntraditional speech recognition system into a single model. Although it\nsimplifies ASR system, it introduces contextual ASR drawback: the E2E model has\nworse performance on utterances containing infrequent proper nouns. In this\nwork, we propose to add a contextual bias attention (CBA) module to attention\nbased encoder decoder (AED) model to improve its ability of recognizing the\ncontextual phrases. Specifically, CBA utilizes the context vector of source\nattention in decoder to attend to a specific bias embedding. Jointly learned\nwith the basic AED parameters, CBA can tell the model when and where to bias\nits output probability distribution. At inference stage, a list of bias phrases\nis preloaded and we adapt the posterior distributions of both CTC and attention\ndecoder according to the attended bias phrase of CBA. We evaluate the proposed\nmethod on GigaSpeech and achieve a consistent relative improvement on recall\nrate of bias phrases ranging from 15% to 28% compared to the baseline model.\nMeanwhile, our method shows a strong anti-bias ability as the performance on\ngeneral tests only degrades 1.7% even 2,000 bias phrases are present.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TURNER: The Uncertainty-based Retrieval Framework for Chinese NER. (arXiv:2202.09022v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09022","description":"<p>Chinese NER is a difficult undertaking due to the ambiguity of Chinese\ncharacters and the absence of word boundaries. Previous work on Chinese NER\nfocus on lexicon-based methods to introduce boundary information and reduce\nout-of-vocabulary (OOV) cases during prediction. However, it is expensive to\nobtain and dynamically maintain high-quality lexicons in specific domains,\nwhich motivates us to utilize more general knowledge resources, e.g., search\nengines. In this paper, we propose TURNER: The Uncertainty-based Retrieval\nframework for Chinese NER. The idea behind TURNER is to imitate human behavior:\nwe frequently retrieve auxiliary knowledge as assistance when encountering an\nunknown or uncertain entity. To improve the efficiency and effectiveness of\nretrieval, we first propose two types of uncertainty sampling methods for\nselecting the most ambiguous entity-level uncertain components of the input\ntext. Then, the Knowledge Fusion Model re-predict the uncertain samples by\ncombining retrieved knowledge. Experiments on four benchmark datasets\ndemonstrate TURNER's effectiveness. TURNER outperforms existing lexicon-based\napproaches and achieves the new SOTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhichao Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_C/0/1/0/all/0/1\">Chenxin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLSEG: Contrastive Learning of Story Ending Generation. (arXiv:2202.09049v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09049","description":"<p>Story Ending Generation (SEG) is a challenging task in natural language\ngeneration. Recently, methods based on Pre-trained Language Models (PLM) have\nachieved great prosperity, which can produce fluent and coherent story endings.\nHowever, the pre-training objective of PLM-based methods is unable to model the\nconsistency between story context and ending. The goal of this paper is to\nadopt contrastive learning to generate endings more consistent with story\ncontext, while there are two main challenges in contrastive learning of SEG.\nFirst is the negative sampling of wrong endings inconsistent with story\ncontexts. The second challenge is the adaptation of contrastive learning for\nSEG. To address these two issues, we propose a novel Contrastive Learning\nframework for Story Ending Generation (CLSEG), which has two steps:\nmulti-aspect sampling and story-specific contrastive learning. Particularly,\nfor the first issue, we utilize novel multi-aspect sampling mechanisms to\nobtain wrong endings considering the consistency of order, causality, and\nsentiment. To solve the second issue, we well-design a story-specific\ncontrastive training strategy that is adapted for SEG. Experiments show that\nCLSEG outperforms baselines and can produce story endings with stronger\nconsistency and rationality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Ping Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLP: A Survey on Vision-Language Pre-training. (arXiv:2202.09061v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09061","description":"<p>In the past few years, the emergence of pre-training models has brought\nuni-modal fields such as computer vision (CV) and natural language processing\n(NLP) to a new era. Substantial works have shown they are beneficial for\ndownstream uni-modal tasks and avoid training a new model from scratch. So can\nsuch pre-trained models be applied to multi-modal tasks? Researchers have\nexplored this problem and made significant progress. This paper surveys recent\nadvances and new frontiers in vision-language pre-training (VLP), including\nimage-text and video-text pre-training. To give readers a better overall grasp\nof VLP, we first review its recent advances from five aspects: feature\nextraction, model architecture, pre-training objectives, pre-training datasets,\nand downstream tasks. Then, we summarize the specific VLP models in detail.\nFinally, we discuss the new frontiers in VLP. To the best of our knowledge,\nthis is the first survey on VLP. We hope that this survey can shed light on\nfuture research in the VLP field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Duzhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Minglun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker Identity Preservation in Dysarthric Speech Reconstruction by Adversarial Speaker Adaptation. (arXiv:2202.09082v1 [eess.AS])","link":"http://arxiv.org/abs/2202.09082","description":"<p>Dysarthric speech reconstruction (DSR), which aims to improve the quality of\ndysarthric speech, remains a challenge, not only because we need to restore the\nspeech to be normal, but also must preserve the speaker's identity. The speaker\nrepresentation extracted by the speaker encoder (SE) optimized for speaker\nverification has been explored to control the speaker identity. However, the SE\nmay not be able to fully capture the characteristics of dysarthric speakers\nthat are previously unseen. To address this research problem, we propose a\nnovel multi-task learning strategy, i.e., adversarial speaker adaptation (ASA).\nThe primary task of ASA fine-tunes the SE with the speech of the target\ndysarthric speaker to effectively capture identity-related information, and the\nsecondary task applies adversarial training to avoid the incorporation of\nabnormal speaking patterns into the reconstructed speech, by regularizing the\ndistribution of reconstructed speech to be close to that of reference speech\nwith high quality. Experiments show that the proposed approach can achieve\nenhanced speaker similarity and comparable speech naturalness with a strong\nbaseline approach. Compared with dysarthric speech, the reconstructed speech\nachieves 22.3% and 31.5% absolute word error rate reduction for speakers with\nmoderate and moderate-severe dysarthria respectively. Our demo page is released\nhere: https://wendison.github.io/ASA-DSR-demo/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Disong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Songxiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xixin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_H/0/1/0/all/0/1\">Hui Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1\">Lifa Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMS_ADRN at SemEval-2022 Task 5: A Suitable Image-text Multimodal Joint Modeling Method for Multi-task Misogyny Identification. (arXiv:2202.09099v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09099","description":"<p>Women are influential online, especially in image-based social media such as\nTwitter and Instagram. However, many in the network environment contain gender\ndiscrimination and aggressive information, which magnify gender stereotypes and\ngender inequality. Therefore, the filtering of illegal content such as gender\ndiscrimination is essential to maintain a healthy social network environment.\nIn this paper, we describe the system developed by our team for SemEval-2022\nTask 5: Multimedia Automatic Misogyny Identification. More specifically, we\nintroduce two novel system to analyze these posts: a multimodal multi-task\nlearning architecture that combines Bertweet for text encoding with ResNet-18\nfor image representation, and a single-flow transformer structure which\ncombines text embeddings from BERT-Embeddings and image embeddings from several\ndifferent modules such as EfficientNet and ResNet. In this manner, we show that\nthe information behind them can be properly revealed. Our approach achieves\ngood performance on each of the two subtasks of the current competition,\nranking 15th for Subtask A (0.746 macro F1-score), 11th for Subtask B (0.706\nmacro F1-score) while exceeding the official baseline results by high margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_M/0/1/0/all/0/1\">Ming Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yukai He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Acoustic Characterization of Singaporean Children's English Pronunciation. (arXiv:2202.09108v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09108","description":"<p>In this work, we investigate pronunciation differences in English spoken by\nSingaporean children in relation to their American and British counterparts by\nconducting Kmeans clustering and Archetypal analysis on selected vowel pairs\nand approximants. Given that Singapore adopts British English as the\ninstitutional standard due to historical reasons, one might expect Singaporean\nchildren to follow British pronunciation patterns. Indeed, Singaporean and\nBritish children are more similar in their production of syllable-final /r/ --\nthey do not lower their third formant nearly as much as American children do,\nsuggesting a lack of rhoticity. Interestingly, Singaporean children also\npresent similar patterns to American children when it comes to their fronting\nof vowels as demonstrated across various vowels including TRAP-BATH split\nvowels. Singaporean children's English also demonstrated characteristics that\ndo not resemble any of the other two populations. We observe that Singaporean\nchildren's vowel height characteristics are distinct from both that of American\nand British children. In tense and lax vowel pairs, we also consistently\nobserve that the distinction is less conspicuous for Singaporean children\ncompared to the other speaker groups. Further, while American and British\nchildren demonstrate lowering of F1 and F2 formants in transitions into\nsyllable-final /l/s, a wide gap between F2 and F3 formants, and small\ndifference between F1 and F2 formants, all of these are not exhibited in\nSingaporean children's pronunciation. These findings point towards potential\nsociolinguistic implications of how Singapore English might be evolving to\nembody more than British pronunciation characteristics. Furthermore, these\nfindings also suggest that Singapore English could be have been influenced by\nlanguages beyond American and British English, potentially due to Singapore's\nmultilingual environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuling Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modelling the semantics of text in complex document layouts using graph transformer networks. (arXiv:2202.09144v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09144","description":"<p>Representing structured text from complex documents typically calls for\ndifferent machine learning techniques, such as language models for paragraphs\nand convolutional neural networks (CNNs) for table extraction, which prohibits\ndrawing links between text spans from different content types. In this article\nwe propose a model that approximates the human reading pattern of a document\nand outputs a unique semantic representation for every text span irrespective\nof the content type they are found in. We base our architecture on a graph\nrepresentation of the structured text, and we demonstrate that not only can we\nretrieve semantically similar information across documents but also that the\nembedding space we generate captures useful semantic information, similar to\nlanguage models that work only on text sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barillot_T/0/1/0/all/0/1\">Thomas Roland Barillot</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Saks_J/0/1/0/all/0/1\">Jacob Saks</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lilyanova_P/0/1/0/all/0/1\">Polena Lilyanova</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Torgas_E/0/1/0/all/0/1\">Edward Torgas</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yachen Hu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanqing Liu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Balupuri_V/0/1/0/all/0/1\">Varun Balupuri</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gaskell_P/0/1/0/all/0/1\">Paul Gaskell</a> (1) ((1) BlackRock Inc.)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Construct Validity of Text Embeddings with Application to Survey Questions. (arXiv:2202.09166v1 [cs.CY])","link":"http://arxiv.org/abs/2202.09166","description":"<p>Text embedding models from Natural Language Processing can map text data\n(e.g. words, sentences, documents) to supposedly meaningful numerical\nrepresentations (a.k.a. text embeddings). While such models are increasingly\napplied in social science research, one important issue is often not addressed:\nthe extent to which these embeddings are valid representations of constructs\nrelevant for social science research. We therefore propose the use of the\nclassic construct validity framework to evaluate the validity of text\nembeddings. We show how this framework can be adapted to the opaque and\nhigh-dimensional nature of text embeddings, with application to survey\nquestions. We include several popular text embedding methods (e.g. fastText,\nGloVe, BERT, Sentence-BERT, Universal Sentence Encoder) in our construct\nvalidity analyses. We find evidence of convergent and discriminant validity in\nsome cases. We also show that embeddings can be used to predict respondent's\nanswers to completely new survey questions. Furthermore, BERT-based embedding\ntechniques and the Universal Sentence Encoder provide more valid\nrepresentations of survey questions than do others. Our results thus highlight\nthe necessity to examine the construct validity of text embeddings before\ndeploying them in social science research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qixiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberski_D/0/1/0/all/0/1\">Daniel L Oberski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process Approach to Linguistic Relationships. (arXiv:2001.05297v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2001.05297","description":"<p>This paper addresses a series of complex and unresolved issues in the\nhistorical phonology of West Iranian languages. The West Iranian languages\n(Persian, Kurdish, Balochi, and other languages) display a high degree of\nnon-Lautgesetzlich behavior. Most of this irregularity is undoubtedly due to\nlanguage contact; we argue, however, that an oversimplified view of the\nprocesses at work has prevailed in the literature on West Iranian dialectology,\nwith specialists assuming that deviations from an expected outcome in a given\nnon-Persian language are due to lexical borrowing from some chronological stage\nof Persian. It is demonstrated that this qualitative approach yields at times\nproblematic conclusions stemming from the lack of explicit probabilistic\ninferences regarding the distribution of the data: Persian may not be the sole\ndonor language; additionally, borrowing at the lexical level is not always the\nmechanism that introduces irregularity. In many cases, the possibility that\nWest Iranian languages show different reflexes in different conditioning\nenvironments remains under-explored. We employ a novel Bayesian approach\ndesigned to overcome these problems and tease apart the different determinants\nof irregularity in patterns of West Iranian sound change. Our methodology\nallows us to provisionally resolve a number of outstanding questions in the\nliterature on West Iranian dialectology concerning the dialectal affiliation of\ncertain sound changes. We outline future directions for work of this sort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cathcart_C/0/1/0/all/0/1\">Chundra Aroor Cathcart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concadia: Tackling Image Accessibility with Descriptive Texts and Context. (arXiv:2104.08376v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08376","description":"<p>Images have become an integral part of online media. This has enhanced the\ndissemination of knowledge, but it poses serious accessibility challenges. The\nHTML \"alt\" field is hidden by default and designated for supplying a\ndescription that could replace the image, but it is rarely used. By contrast,\nimage captions appear alongside the image and are more abundant, but they are\nwritten to supply additional information and generally lack the details\nrequired for accessibility. These terms are often treated as synonyms, but we\nargue that a distinction is essential. To address this, we introduce the\npublicly available Wikipedia-based corpus Concadia, which consists of 96,918\nimages with corresponding English-language descriptions, captions, and\nsurrounding context. We use Concadia to characterize the commonalities and\ndifferences between descriptions and captions. This leads us to the hypothesis\nthat captions, while not substitutes for descriptions, can provide a useful\nsignal for creating effective descriptions. We substantiate this hypothesis by\nshowing that image description systems trained on Concadia benefit from having\ncaption embeddings as part of their inputs. Finally, we provide evidence from a\nhuman-subjects experiment that human-created captions and descriptions have\ndistinct communicative purposes, and that our generated texts follow this same\npattern. These experiments begin to show how Concadia can be a powerful tool in\naddressing the underlying accessibility issues posed by image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreiss_E/0/1/0/all/0/1\">Elisa Kreiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Text Classification via Contrastive Adversarial Training. (arXiv:2107.10137v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.10137","description":"<p>We propose a simple and general method to regularize the fine-tuning of\nTransformer-based encoders for text classification tasks. Specifically, during\nfine-tuning we generate adversarial examples by perturbing the word embeddings\nof the model and perform contrastive learning on clean and adversarial examples\nin order to teach the model to learn noise-invariant representations. By\ntraining on both clean and adversarial examples along with the additional\ncontrastive objective, we observe consistent improvement over standard\nfine-tuning on clean examples. On several GLUE benchmark tasks, our fine-tuned\nBERT Large model outperforms BERT Large baseline by 1.7% on average, and our\nfine-tuned RoBERTa Large improves over RoBERTa Large baseline by 1.3%. We\nadditionally validate our method in different domains using three intent\nclassification datasets, where our fine-tuned RoBERTa Large outperforms RoBERTa\nLarge baseline by 1-2% on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_C/0/1/0/all/0/1\">Chung-Wei Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potdar_S/0/1/0/all/0/1\">Saloni Potdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets. (arXiv:2109.03537v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03537","description":"<p>Pre-training language models (LMs) on large-scale unlabeled text data makes\nthe model much easier to achieve exceptional downstream performance than their\ncounterparts directly trained on the downstream tasks. In this work, we study\nwhat specific traits in the pre-training data, other than the semantics, make a\npre-trained LM superior to their counterparts trained from scratch on\ndownstream tasks. We propose to use artificially constructed datasets as the\npre-training data to exclude the effect of semantics, and further control what\ncharacteristics the pre-training corpora have. By fine-tuning the pre-trained\nmodels on GLUE benchmark, we can learn how beneficial it is to transfer the\nknowledge from the model trained on the dataset possessing that specific trait.\nWe define and discuss three different characteristics in the artificial\ndataset: 1) matching the token's uni-gram or bi-gram distribution between\npre-training and downstream fine-tuning, 2) the presence of the explicit\ndependencies among the tokens in a sequence, 3) the length of the implicit\ndependencies among the tokens in a sequence. Our experiments show that the\nexplicit dependencies in the sequences of the pre-training data are critical to\nthe downstream performance. Our results also reveal that models achieve better\ndownstream performance when pre-trained on a dataset with a longer range of\nimplicit dependencies. Based on our analysis, we find that models pre-trained\nwith artificial datasets are prone to learn spurious correlation in downstream\ntasks. Our work reveals that even if the LMs are not pre-trained on natural\nlanguage, they still gain transferability on certain human language downstream\ntasks once the LMs learn to model the token dependencies in the sequences. This\nresult helps us understand the exceptional transferability of pre-trained LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1\">Cheng-Han Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Case-based Reasoning for Better Generalization in Text-Adventure Games. (arXiv:2110.08470v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08470","description":"<p>Text-based games (TBG) have emerged as promising environments for driving\nresearch in grounded language understanding and studying problems like\ngeneralization and sample efficiency. Several deep reinforcement learning (RL)\nmethods with varying architectures and learning schemes have been proposed for\nTBGs. However, these methods fail to generalize efficiently, especially under\ndistributional shifts. In a departure from deep RL approaches, in this paper,\nwe propose a general method inspired by case-based reasoning to train agents\nand generalize out of the training distribution. The case-based reasoner\ncollects instances of positive experiences from the agent's interaction with\nthe world in the past and later reuses the collected experiences to act\nefficiently. The method can be applied in conjunction with any existing\non-policy neural agent in the literature for TBGs. Our experiments show that\nthe proposed approach consistently improves existing methods, obtains good\nout-of-distribution generalization, and achieves new state-of-the-art results\non widely used environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atzeni_M/0/1/0/all/0/1\">Mattia Atzeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murugesan_K/0/1/0/all/0/1\">Keerthiram Murugesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two heads are better than one: Enhancing medical representations by pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.10113","description":"<p>The massive amount of electronic health records (EHRs) has created enormous\npotentials for improving healthcare, among which structured (coded) data and\nunstructured (clinical narratives) data are two important textual modalities.\nThey do not exist in isolation and can complement each other in many real-life\nclinical scenarios. Most existing studies in medical informatics, however,\neither only focus on a particular modality or apply simple and na\\\"ive ways to\nconcatenate data from different modalities, which ignores the interactions\nbetween them. To address these issues, we proposed a Unified Medical Multimodal\nPre-trained Language Model, named UMM-PLM, to jointly learn enhanced\nrepresentations from both structured and unstructured EHRs. In UMM-PLM, an\nunimodal information extraction module is used to learn representative\ncharacteristics from each data modality respectively, where two\nTransformer-based components are adopted. A cross-modal module is then\nintroduced to model the interactions between the two modalities. We pre-trained\nthe model on a large EHR dataset containing both structured data and\nunstructured data, and verified the effectiveness of the model on three\ndownstream clinical tasks, i.e., medication recommendation, 30-day readmission,\nand ICD coding, through extensive experiments. The results demonstrate the\npower of UMM-PLM compared with benchmark methods and state-of-the-art\nbaselines. Further analyses show that UMM-PLM can effectively integrate\nmultimodal textual information and potentially provide more comprehensive\ninterpretations for clinical decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yongshuai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding. (arXiv:2201.11766v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11766","description":"<p>Compositional generalization is a troubling blind spot for neural language\nmodels. Recent efforts have presented techniques for improving a model's\nability to encode novel combinations of known inputs, but less work has focused\non generating novel combinations of known outputs. Here we focus on this latter\n\"decode-side\" form of generalization in the context of gSCAN, a synthetic\nbenchmark for compositional generalization in grounded language understanding.\nWe present Recursive Decoding (RD), a novel procedure for training and using\nseq2seq models, targeted towards decode-side generalization. Rather than\ngenerating an entire output sequence in one pass, models are trained to predict\none token at a time. Inputs (i.e., the external gSCAN environment) are then\nincrementally updated based on predicted tokens, and re-encoded for the next\ndecoder time step. RD thus decomposes a complex, out-of-distribution sequence\ngeneration task into a series of incremental predictions that each resemble\nwhat the model has already seen during training. RD yields dramatic improvement\non two previously neglected generalization tasks in gSCAN. We provide analyses\nto elucidate these gains over failure of a baseline, and then discuss\nimplications for generalization in naturalistic grounded language\nunderstanding, and seq2seq more generally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Setzler_M/0/1/0/all/0/1\">Matthew Setzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1\">Scott Howland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_L/0/1/0/all/0/1\">Lauren Phillips</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RescoreBERT: Discriminative Speech Recognition Rescoring with BERT. (arXiv:2202.01094v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2202.01094","description":"<p>Second-pass rescoring is an important component in automatic speech\nrecognition (ASR) systems that is used to improve the outputs from a first-pass\ndecoder by implementing a lattice rescoring or $n$-best re-ranking. While\npretraining with a masked language model (MLM) objective has received great\nsuccess in various natural language understanding (NLU) tasks, it has not\ngained traction as a rescoring model for ASR. Specifically, training a\nbidirectional model like BERT on a discriminative objective such as minimum WER\n(MWER) has not been explored. Here we show how to train a BERT-based rescoring\nmodel with MWER loss, to incorporate the improvements of a discriminative loss\ninto fine-tuning of deep bidirectional pretrained models for ASR. Specifically,\nwe propose a fusion strategy that incorporates the MLM into the discriminative\ntraining process to effectively distill knowledge from a pretrained model. We\nfurther propose an alternative discriminative loss. This approach, which we\ncall RescoreBERT, reduces WER by 6.6%/3.4% relative on the LibriSpeech\nclean/other test sets over a BERT baseline without discriminative objective. We\nalso evaluate our method on an internal dataset from a conversational agent and\nfind that it reduces both latency and WER (by 3 to 8% relative) over an LSTM\nrescoring model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1\">Yile Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kolehmainen_J/0/1/0/all/0/1\">Jari Kolehmainen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_H/0/1/0/all/0/1\">Haidar Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers. (arXiv:2202.06690v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06690","description":"<p>The applications of conversational agents for scientific disciplines (as\nexpert domains) are understudied due to the lack of dialogue data to train such\nagents. While most data collection frameworks, such as Amazon Mechanical Turk,\nfoster data collection for generic domains by connecting crowd workers and task\ndesigners, these frameworks are not much optimized for data collection in\nexpert domains. Scientists are rarely present in these frameworks due to their\nlimited time budget. Therefore, we introduce a novel framework to collect\ndialogues between scientists as domain experts on scientific papers. Our\nframework lets scientists present their scientific papers as groundings for\ndialogues and participate in dialogue they like its paper title. We use our\nframework to collect a novel argumentative dialogue dataset, ArgSciChat. It\nconsists of 498 messages collected from 41 dialogues on 20 scientific papers.\nAlongside extensive analysis on ArgSciChat, we evaluate a recent conversational\nagent on our dataset. Experimental results show that this agent poorly performs\non ArgSciChat, motivating further research on argumentative scientific agents.\nWe release our framework and the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruggeri_F/0/1/0/all/0/1\">Federico Ruggeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesgar_M/0/1/0/all/0/1\">Mohsen Mesgar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-Based Action-Model Acquisition for Planning. (arXiv:2202.08373v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.08373","description":"<p>Although there have been approaches that are capable of learning action\nmodels from plan traces, there is no work on learning action models from\ntextual observations, which is pervasive and much easier to collect from\nreal-world applications compared to plan traces. In this paper we propose a\nnovel approach to learning action models from natural language texts by\nintegrating Constraint Satisfaction and Natural Language Processing techniques.\nSpecifically, we first build a novel language model to extract plan traces from\ntexts, and then build a set of constraints to generate action models based on\nthe extracted plan traces. After that, we iteratively improve the language\nmodel and constraints until we achieve the convergent language model and action\nmodels. We empirically exhibit that our approach is both effective and\nefficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1\">Kebing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huaixun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_H/0/1/0/all/0/1\">Hankz Hankui Zhuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Deep Transfer Learning on Satellite Imagery Improves Air Quality Estimates in Developing Nations. (arXiv:2202.08890v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08890","description":"<p>Urban air pollution is a public health challenge in low- and middle-income\ncountries (LMICs). However, LMICs lack adequate air quality (AQ) monitoring\ninfrastructure. A persistent challenge has been our inability to estimate AQ\naccurately in LMIC cities, which hinders emergency preparedness and risk\nmitigation. Deep learning-based models that map satellite imagery to AQ can be\nbuilt for high-income countries (HICs) with adequate ground data. Here we\ndemonstrate that a scalable approach that adapts deep transfer learning on\nsatellite imagery for AQ can extract meaningful estimates and insights in LMIC\ncities based on spatiotemporal patterns learned in HIC cities. The approach is\ndemonstrated for Accra in Ghana, Africa, with AQ patterns learned from two US\ncities, specifically Los Angeles and New York.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1\">Nishant Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorek_Hamer_M/0/1/0/all/0/1\">Meytar Sorek-Hamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohle_M/0/1/0/all/0/1\">Michael Von Pohle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asanjan_A/0/1/0/all/0/1\">Ata Akbari Asanjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahasrabhojanee_A/0/1/0/all/0/1\">Adwait Sahasrabhojanee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suel_E/0/1/0/all/0/1\">Esra Suel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arku_R/0/1/0/all/0/1\">Raphael Arku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lingenfelter_V/0/1/0/all/0/1\">Violet Lingenfelter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauer_M/0/1/0/all/0/1\">Michael Brauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezzati_M/0/1/0/all/0/1\">Majid Ezzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oza_N/0/1/0/all/0/1\">Nikunj Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Auroop R. Ganguly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing Imperceptible Adversarial Patches to Camouflage Military Assets From Computer Vision Enabled Technologies. (arXiv:2202.08892v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08892","description":"<p>Convolutional neural networks (CNNs) have demonstrated rapid progress and a\nhigh level of success in object detection. However, recent evidence has\nhighlighted their vulnerability to adversarial attacks. These attacks are\ncalculated image perturbations or adversarial patches that result in object\nmisclassification or detection suppression. Traditional camouflage methods are\nimpractical when applied to disguise aircraft and other large mobile assets\nfrom autonomous detection in intelligence, surveillance and reconnaissance\ntechnologies and fifth generation missiles. In this paper we present a unique\nmethod that produces imperceptible patches capable of camouflaging large\nmilitary assets from computer vision-enabled technologies. We developed these\npatches by maximising object detection loss whilst limiting the patch's colour\nperceptibility. This work also aims to further the understanding of adversarial\nexamples and their effects on object detection algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wise_C/0/1/0/all/0/1\">Christopher Wise</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plested_J/0/1/0/all/0/1\">Jo Plested</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous-Time vs. Discrete-Time Vision-based SLAM: A Comparative Study. (arXiv:2202.08894v1 [cs.RO])","link":"http://arxiv.org/abs/2202.08894","description":"<p>Robotic practitioners generally approach the vision-based SLAM problem\nthrough discrete-time formulations. This has the advantage of a consolidated\ntheory and very good understanding of success and failure cases. However,\ndiscrete-time SLAM needs tailored algorithms and simplifying assumptions when\nhigh-rate and/or asynchronous measurements, coming from different sensors, are\npresent in the estimation process. Conversely, continuous-time SLAM, often\noverlooked by practitioners, does not suffer from these limitations. Indeed, it\nallows integrating new sensor data asynchronously without adding a new\noptimization variable for each new measurement. In this way, the integration of\nasynchronous or continuous high-rate streams of sensor data does not require\ntailored and highly-engineered algorithms, enabling the fusion of multiple\nsensor modalities in an intuitive fashion. On the down side, continuous time\nintroduces a prior that could worsen the trajectory estimates in some\nunfavorable situations. In this work, we aim at systematically comparing the\nadvantages and limitations of the two formulations in vision-based SLAM. To do\nso, we perform an extensive experimental analysis, varying robot type, speed of\nmotion, and sensor modalities. Our experimental analysis suggests that,\nindependently of the trajectory type, continuous-time SLAM is superior to its\ndiscrete counterpart whenever the sensors are not time-synchronized. In the\ncontext of this work, we developed, and open source, a modular and efficient\nsoftware architecture containing state-of-the-art algorithms to solve the SLAM\nproblem in discrete and continuous time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cioffi_G/0/1/0/all/0/1\">Giovanni Cioffi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cieslewski_T/0/1/0/all/0/1\">Titus Cieslewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine learning models and facial regions videos for estimating heart rate: a review on Patents, Datasets and Literature. (arXiv:2202.08913v1 [cs.LG])","link":"http://arxiv.org/abs/2202.08913","description":"<p>Estimating heart rate is important for monitoring users in various\nsituations. Estimates based on facial videos are increasingly being researched\nbecause it makes it possible to monitor cardiac information in a non-invasive\nway and because the devices are simpler, requiring only cameras that capture\nthe user's face. From these videos of the user's face, machine learning is able\nto estimate heart rate. This study investigates the benefits and challenges of\nusing machine learning models to estimate heart rate from facial videos,\nthrough patents, datasets, and articles review. We searched Derwent Innovation,\nIEEE Xplore, Scopus, and Web of Science knowledge bases and identified 7 patent\nfilings, 11 datasets, and 20 articles on heart rate, photoplethysmography, or\nelectrocardiogram data. In terms of patents, we note the advantages of\ninventions related to heart rate estimation, as described by the authors. In\nterms of datasets, we discovered that most of them are for academic purposes\nand with different signs and annotations that allow coverage for subjects other\nthan heartbeat estimation. In terms of articles, we discovered techniques, such\nas extracting regions of interest for heart rate reading and using Video\nMagnification for small motion extraction, and models such as EVM-CNN and\nVGG-16, that extract the observed individual's heart rate, the best regions of\ninterest for signal extraction and ways to process them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pagano_T/0/1/0/all/0/1\">Tiago Palma Pagano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_L/0/1/0/all/0/1\">Lucas Lemos Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_V/0/1/0/all/0/1\">Victor Rocha Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonfim_Y/0/1/0/all/0/1\">Yasmin da Silva Bonfim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paranhos_J/0/1/0/all/0/1\">Jos&#xe9; Vin&#xed;cius Dantas Paranhos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_P/0/1/0/all/0/1\">Paulo Henrique Miranda S&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_L/0/1/0/all/0/1\">Lian Filipe Santana Nascimento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkler_I/0/1/0/all/0/1\">Ingrid Winkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_E/0/1/0/all/0/1\">Erick Giovani Sperandio Nascimento</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Convolutional Networks for Multi-modality Medical Imaging: Methods, Architectures, and Clinical Applications. (arXiv:2202.08916v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08916","description":"<p>Image-based characterization and disease understanding involve integrative\nanalysis of morphological, spatial, and topological information across\nbiological scales. The development of graph convolutional networks (GCNs) has\ncreated the opportunity to address this information complexity via graph-driven\narchitectures, since GCNs can perform feature aggregation, interaction, and\nreasoning with remarkable flexibility and efficiency. These GCNs capabilities\nhave spawned a new wave of research in medical imaging analysis with the\noverarching goal of improving quantitative disease understanding, monitoring,\nand diagnosis. Yet daunting challenges remain for designing the important\nimage-to-graph transformation for multi-modality medical imaging and gaining\ninsights into model interpretation and enhanced clinical decision support. In\nthis review, we present recent GCNs developments in the context of medical\nimage analysis including imaging data from radiology and histopathology. We\ndiscuss the fast-growing use of graph network architectures in medical image\nanalysis to improve disease diagnosis and patient outcomes in clinical\npractice. To foster cross-disciplinary research, we present GCNs technical\nadvancements, emerging medical applications, identify common challenges in the\nuse of image-based GCNs and their extensions in model interpretation,\nlarge-scale benchmarks that promise to transform the scope of medical image\nstudies and related graph-driven medical research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ding_K/0/1/0/all/0/1\">Kexin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zichen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_C/0/1/0/all/0/1\">Corey W. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitri N. Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Guiding Visual Attention with Language Specification. (arXiv:2202.08926v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08926","description":"<p>While real world challenges typically define visual categories with language\nwords or phrases, most visual classification methods define categories with\nnumerical indices. However, the language specification of the classes provides\nan especially useful prior for biased and noisy datasets, where it can help\ndisambiguate what features are task-relevant. Recently, large-scale multimodal\nmodels have been shown to recognize a wide variety of high-level concepts from\na language specification even without additional image training data, but they\nare often unable to distinguish classes for more fine-grained tasks. CNNs, in\ncontrast, can extract subtle image features that are required for fine-grained\ndiscrimination, but will overfit to any bias or noise in datasets. Our insight\nis to use high-level language specification as advice for constraining the\nclassification evidence to task-relevant features, instead of distractors. To\ndo this, we ground task-relevant words or phrases with attention maps from a\npretrained large-scale model. We then use this grounding to supervise a\nclassifier's spatial attention away from distracting context. We show that\nsupervising spatial attention in this way improves performance on\nclassification tasks with biased and noisy data, including about 3-15%\nworst-group accuracy improvements and 41-45% relative improvements on fairness\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petryk_S/0/1/0/all/0/1\">Suzanne Petryk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunlap_L/0/1/0/all/0/1\">Lisa Dunlap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasseri_K/0/1/0/all/0/1\">Keyan Nasseri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prior image-based medical image reconstruction using a style-based generative adversarial network. (arXiv:2202.08936v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08936","description":"<p>Computed medical imaging systems require a computational reconstruction\nprocedure for image formation. In order to recover a useful estimate of the\nobject to-be-imaged when the recorded measurements are incomplete, prior\nknowledge about the nature of object must be utilized. In order to improve the\nconditioning of an ill-posed imaging inverse problem, deep learning approaches\nare being actively investigated for better representing object priors and\nconstraints. This work proposes to use a style-based generative adversarial\nnetwork (StyleGAN) to constrain an image reconstruction problem in the case\nwhere additional information in the form of a prior image of the sought-after\nobject is available. An optimization problem is formulated in the intermediate\nlatent-space of a StyleGAN, that is disentangled with respect to meaningful\nimage attributes or \"styles\", such as the contrast used in magnetic resonance\nimaging (MRI). Discrepancy between the sought-after and prior images is\nmeasured in the disentangled latent-space, and is used to regularize the\ninverse problem in the form of constraints on specific styles of the\ndisentangled latent-space. A stylized numerical study inspired by MR imaging is\ndesigned, where the sought-after and the prior image are structurally similar,\nbut belong to different contrast mechanisms. The presented numerical studies\ndemonstrate the superiority of the proposed approach as compared to classical\napproaches in the form of traditional metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kelkar_V/0/1/0/all/0/1\">Varun A. Kelkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When, Why, and Which Pretrained GANs Are Useful?. (arXiv:2202.08937v1 [cs.LG])","link":"http://arxiv.org/abs/2202.08937","description":"<p>The literature has proposed several methods to finetune pretrained GANs on\nnew datasets, which typically results in higher performance compared to\ntraining from scratch, especially in the limited-data regime. However, despite\nthe apparent empirical benefits of GAN pretraining, its inner mechanisms were\nnot analyzed in-depth, and understanding of its role is not entirely clear.\nMoreover, the essential practical details, e.g., selecting a proper pretrained\nGAN checkpoint, currently do not have rigorous grounding and are typically\ndetermined by trial and error.\n</p>\n<p>This work aims to dissect the process of GAN finetuning. First, we show that\ninitializing the GAN training process by a pretrained checkpoint primarily\naffects the model's coverage rather than the fidelity of individual samples.\nSecond, we explicitly describe how pretrained generators and discriminators\ncontribute to the finetuning process and explain the previous evidence on the\nimportance of pretraining both of them. Finally, as an immediate practical\nbenefit of our analysis, we describe a simple recipe to choose an appropriate\nGAN checkpoint that is the most suitable for finetuning to a particular target\ntask. Importantly, for most of the target tasks, Imagenet-pretrained GAN,\ndespite having poor visual quality, appears to be an excellent starting point\nfor finetuning, resembling the typical pretraining scenario of discriminative\ncomputer vision models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grigoryev_T/0/1/0/all/0/1\">Timofey Grigoryev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1\">Andrey Voynov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1\">Artem Babenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of ADHD Patients by Kernel Hierarchical Extreme Learning Machine. (arXiv:2202.08953v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08953","description":"<p>These days, the diagnosis of neuropsychiatric diseases through brain imaging\ntechnology has received more and more attention. The exploration of\ninteractions in brain functional connectivity based on functional magnetic\nresonance imaging (fMRI) data is critical for the study of mental illness.\nBecause attention-deficit/hyperactivity disorder (ADHD) is a chronic disease\nthat affects millions of children, it is difficult to diagnose, so there is\nstill much space for improvement in the accuracy of the diagnosis of the\ndisease. In this paper, we consider the dynamics of brain functional\nconnectivity, modeling a functional brain dynamics model from medical imaging,\nwhich helps to find differences in brain function interactions between normal\ncontrol (NC) children and ADHD children. In more detail, our method is used by\nBayesian Connectivity Change Point Model for dynamic detection, Local Binary\nEncoding Method for local feature extraction, and Kernel Hierarchical Extreme\nLearning Machine implementation classification. To validate our approach,\nexperimental comparisons of fMRI imaging data on 23 ADHD and 45 NC children\nwere performed, and our experimental methods achieved better classification\nresults than existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salman_S/0/1/0/all/0/1\">Sartaj Ahmed Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhichao Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuduo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R2-D2: Repetitive Reprediction Deep Decipher for Semi-Supervised Deep Learning. (arXiv:2202.08955v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08955","description":"<p>Most recent semi-supervised deep learning (deep SSL) methods used a similar\nparadigm: use network predictions to update pseudo-labels and use pseudo-labels\nto update network parameters iteratively. However, they lack theoretical\nsupport and cannot explain why predictions are good candidates for\npseudo-labels in the deep learning paradigm. In this paper, we propose a\nprincipled end-to-end framework named deep decipher (D2) for SSL. Within the D2\nframework, we prove that pseudo-labels are related to network predictions by an\nexponential link function, which gives a theoretical support for using\npredictions as pseudo-labels. Furthermore, we demonstrate that updating\npseudo-labels by network predictions will make them uncertain. To mitigate this\nproblem, we propose a training strategy called repetitive reprediction (R2).\nFinally, the proposed R2-D2 method is tested on the large-scale ImageNet\ndataset and outperforms state-of-the-art methods by 5 percentage points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guo-Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy-Efficient Parking Analytics System using Deep Reinforcement Learning. (arXiv:2202.08973v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08973","description":"<p>Advances in deep vision techniques and ubiquity of smart cameras will drive\nthe next generation of video analytics. However, video analytics applications\nconsume vast amounts of energy as both deep learning techniques and cameras are\npower-hungry. In this paper, we focus on a parking video analytics platform and\npropose RL-CamSleep, a deep reinforcement learning-based technique, to actuate\nthe cameras to reduce the energy footprint while retaining the system's\nutility. Our key insight is that many video-analytics applications do not\nalways need to be operational, and we can design policies to activate video\nanalytics only when necessary. Moreover, our work is complementary to existing\nwork that focuses on improving hardware and software efficiency. We evaluate\nour approach on a city-scale parking dataset having 76 streets spread across\nthe city. Our analysis demonstrates how streets have various parking patterns,\nhighlighting the importance of an adaptive policy. Our approach can learn such\nan adaptive policy that can reduce the average energy consumption by 76.38% and\nachieve an average accuracy of more than 98% in performing video analytics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_Y/0/1/0/all/0/1\">Yoones Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stephen Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosse_D/0/1/0/all/0/1\">Daniel Mosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cyclical Focal Loss. (arXiv:2202.08978v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08978","description":"<p>The cross-entropy softmax loss is the primary loss function used to train\ndeep neural networks. On the other hand, the focal loss function has been\ndemonstrated to provide improved performance when there is an imbalance in the\nnumber of training samples in each class, such as in long-tailed datasets. In\nthis paper, we introduce a novel cyclical focal loss and demonstrate that it is\na more universal loss function than cross-entropy softmax loss or focal loss.\nWe describe the intuition behind the cyclical focal loss and our experiments\nprovide evidence that cyclical focal loss provides superior performance for\nbalanced, imbalanced, or long-tailed datasets. We provide numerous experimental\nresults for CIFAR-10/CIFAR-100, ImageNet, balanced and imbalanced 4,000\ntraining sample versions of CIFAR-10/CIFAR-100, and ImageNet-LT and Places-LT\nfrom the Open Long-Tailed Recognition (OLTR) challenge. Implementing the\ncyclical focal loss function requires only a few lines of code and does not\nincrease training time. In the spirit of reproducibility, our code is available\nat \\url{https://github.com/lnsmith54/CFL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1\">Leslie N. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Learning of Frequency and Spatial Domains for Dense Predictions. (arXiv:2202.08991v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08991","description":"<p>Current artificial neural networks mainly conduct the learning process in the\nspatial domain but neglect the frequency domain learning. However, the learning\ncourse performed in the frequency domain can be more efficient than that in the\nspatial domain. In this paper, we fully explore frequency domain learning and\npropose a joint learning paradigm of frequency and spatial domains. This\nparadigm can take full advantage of the preponderances of frequency learning\nand spatial learning; specifically, frequency and spatial domain learning can\neffectively capture global and local information, respectively. Exhaustive\nexperiments on two dense prediction tasks, i.e., self-supervised depth\nestimation and semantic segmentation, demonstrate that the proposed joint\nlearning paradigm can 1) achieve performance competitive to those of\nstate-of-the-art methods in both depth estimation and semantic segmentation\ntasks, even without pretraining; and 2) significantly reduce the number of\nparameters compared to other state-of-the-art methods, which provides more\nchance to develop real-world applications. We hope that the proposed method can\nencourage more research in cross-domain learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Shaocheng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wei Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REFUGE2 Challenge: Treasure for Multi-Domain Learning in Glaucoma Assessment. (arXiv:2202.08994v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08994","description":"<p>Glaucoma is the second leading cause of blindness and is the leading cause of\nirreversible blindness disease in the world. Early screening for glaucoma in\nthe population is significant. Color fundus photography is the most cost\neffective imaging modality to screen for ocular diseases. Deep learning network\nis often used in color fundus image analysis due to its powful feature\nextraction capability. However, the model training of deep learning method\nneeds a large amount of data, and the distribution of data should be abundant\nfor the robustness of model performance. To promote the research of deep\nlearning in color fundus photography and help researchers further explore the\nclinical application signification of AI technology, we held a REFUGE2\nchallenge. This challenge released 2,000 color fundus images of four models,\nincluding Zeiss, Canon, Kowa and Topcon, which can validate the stabilization\nand generalization of algorithms on multi-domain. Moreover, three sub-tasks\nwere designed in the challenge, including glaucoma classification, cup/optic\ndisc segmentation, and macular fovea localization. These sub-tasks technically\ncover the three main problems of computer vision and clinicly cover the main\nresearchs of glaucoma diagnosis. Over 1,300 international competitors joined\nthe REFUGE2 challenge, 134 teams submitted more than 3,000 valid preliminary\nresults, and 22 teams reached the final. This article summarizes the methods of\nsome of the finalists and analyzes their results. In particular, we observed\nthat the teams using domain adaptation strategies had high and robust\nperformance on the dataset with multi-domain. This indicates that UDA and other\nmulti-domain related researches will be the trend of deep learning field in the\nfuture, and our REFUGE2 datasets will play an important role in these\nresearches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1\">Xingxing Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Son_J/0/1/0/all/0/1\">Jaemin Son</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_S/0/1/0/all/0/1\">Shuang Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Menglu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_C/0/1/0/all/0/1\">Chenglang Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bian_C/0/1/0/all/0/1\">Cheng Bian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_B/0/1/0/all/0/1\">Benjian Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xinxing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shaohua Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fumero_F/0/1/0/all/0/1\">Francisco Fumero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sigut_J/0/1/0/all/0/1\">Jose Sigut</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Almubarak_H/0/1/0/all/0/1\">Haidar Almubarak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bazi_Y/0/1/0/all/0/1\">Yakoub Bazi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanhao Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yating Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1\">Ujjwal Baid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Innani_S/0/1/0/all/0/1\">Shubham Innani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_T/0/1/0/all/0/1\">Tianjiao Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orlando_J/0/1/0/all/0/1\">Jos&#xe9; Ignacio Orlando</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bogunovic_H/0/1/0/all/0/1\">Hrvoje Bogunovi&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiulan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Active and Contrastive Learning Framework for Fine-Grained Off-Road Semantic Segmentation. (arXiv:2202.09002v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09002","description":"<p>Off-road semantic segmentation with fine-grained labels is necessary for\nautonomous vehicles to understand driving scenes, as the coarse-grained road\ndetection can not satisfy off-road vehicles with various mechanical properties.\nFine-grained semantic segmentation in off-road scenes usually has no unified\ncategory definition due to ambiguous nature environments, and the cost of\npixel-wise labeling is extremely high. Furthermore, semantic properties of\noff-road scenes can be very changeable due to various precipitations,\ntemperature, defoliation, etc. To address these challenges, this research\nproposes an active and contrastive learning-based method that does not rely on\npixel-wise labels, but only on patch-based weak annotations for model learning.\nThere is no need for predefined semantic categories, the contrastive\nlearning-based feature representation and adaptive clustering will discover the\ncategory model from scene data. In order to actively adapt to new scenes, a\nrisk evaluation method is proposed to discover and select hard frames with\nhigh-risk predictions for supplemental labeling, so as to update the model\nefficiently. Experiments conducted on our self-developed off-road dataset and\nDeepScene dataset demonstrate that fine-grained semantic segmentation can be\nlearned with only dozens of weakly labeled frames, and the model can\nefficiently adapt across scenes by weak supervision, while achieving almost the\nsame level of performance as typical fully supervised baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1\">Biao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xijun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huijing Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KINet: Keypoint Interaction Networks for Unsupervised Forward Modeling. (arXiv:2202.09006v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09006","description":"<p>Object-centric representation is an essential abstraction for physical\nreasoning and forward prediction. Most existing approaches learn this\nrepresentation through extensive supervision (e.g., object class and bounding\nbox) although such ground-truth information is not readily accessible in\nreality. To address this, we introduce KINet (Keypoint Interaction Network) --\nan end-to-end unsupervised framework to reason about object interactions in\ncomplex systems based on a keypoint representation. Using visual observations,\nour model learns to associate objects with keypoint coordinates and discovers a\ngraph representation of the system as a set of keypoint embeddings and their\nrelations. It then learns an action-conditioned forward model using contrastive\nestimation to predict future keypoint states. By learning to perform physical\nreasoning in the keypoint space, our model automatically generalizes to\nscenarios with a different number of objects, and novel object geometries.\nExperiments demonstrate the effectiveness of our model to accurately perform\nforward prediction and learn plannable object-centric representations which can\nalso be used in downstream model-based control tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezazadeh_A/0/1/0/all/0/1\">Alireza Rezazadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Changhyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LG-LSQ: Learned Gradient Linear Symmetric Quantization. (arXiv:2202.09009v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09009","description":"<p>Deep neural networks with lower precision weights and operations at inference\ntime have advantages in terms of the cost of memory space and accelerator\npower. The main challenge associated with the quantization algorithm is\nmaintaining accuracy at low bit-widths. We propose learned gradient linear\nsymmetric quantization (LG-LSQ) as a method for quantizing weights and\nactivation functions to low bit-widths with high accuracy in integer neural\nnetwork processors. First, we introduce the scaling simulated gradient (SSG)\nmethod for determining the appropriate gradient for the scaling factor of the\nlinear quantizer during the training process. Second, we introduce the\narctangent soft round (ASR) method, which differs from the straight-through\nestimator (STE) method in its ability to prevent the gradient from becoming\nzero, thereby solving the discrete problem caused by the rounding process.\nFinally, to bridge the gap between full-precision and low-bit quantization\nnetworks, we propose the minimize discretization error (MDE) method to\ndetermine an accurate gradient in backpropagation. The ASR+MDE method is a\nsimple alternative to the STE method and is practical for use in different\nuniform quantization methods. In our evaluation, the proposed quantizer\nachieved full-precision baseline accuracy in various 3-bit networks, including\nResNet18, ResNet34, and ResNet50, and an accuracy drop of less than 1% in the\nquantization of 4-bit weights and 4-bit activations in lightweight models such\nas MobileNetV2 and ShuffleNetV2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shih-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaofang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu-Hsiang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1\">Hao-Wen Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chih-Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kea-Tiong Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Well Do Self-Supervised Methods Perform in Cross-Domain Few-Shot Learning?. (arXiv:2202.09014v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09014","description":"<p>Cross-domain few-shot learning (CDFSL) remains a largely unsolved problem in\nthe area of computer vision, while self-supervised learning presents a\npromising solution. Both learning methods attempt to alleviate the dependency\nof deep networks on the requirement of large-scale labeled data. Although\nself-supervised methods have recently advanced dramatically, their utility on\nCDFSL is relatively unexplored. In this paper, we investigate the role of\nself-supervised representation learning in the context of CDFSL via a thorough\nevaluation of existing methods. It comes as a surprise that even with shallow\narchitectures or small training datasets, self-supervised methods can perform\nfavorably compared to the existing SOTA methods. Nevertheless, no single\nself-supervised approach dominates all datasets indicating that existing\nself-supervised methods are not universally applicable. In addition, we find\nthat representations extracted from self-supervised methods exhibit stronger\nrobustness than the supervised method. Intriguingly, whether self-supervised\nrepresentations perform well on the source domain has little correlation with\ntheir applicability on the target domain. As part of our study, we conduct an\nobjective measurement of the performance for six kinds of representative\nclassifiers. The results suggest Prototypical Classifier as the standard\nevaluation recipe for CDFSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Ying Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaogang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey with Quantitative Comparison of Image Analysis Methods for Microorganism Biovolume Measurements. (arXiv:2202.09020v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09020","description":"<p>With the acceleration of urbanization and living standards, microorganisms\nplay increasingly important roles in industrial production, bio-technique, and\nfood safety testing. Microorganism biovolume measurements are one of the\nessential parts of microbial analysis. However, traditional manual measurement\nmethods are time-consuming and challenging to measure the characteristics\nprecisely. With the development of digital image processing techniques, the\ncharacteristics of the microbial population can be detected and quantified. The\nchanging trend can be adjusted in time and provided a basis for the\nimprovement. The applications of the microorganism biovolume measurement method\nhave developed since the 1980s. More than 60 articles are reviewed in this\nstudy, and the articles are grouped by digital image segmentation methods with\nperiods. This study has high research significance and application value, which\ncan be referred to microbial researchers to have a comprehensive understanding\nof microorganism biovolume measurements using digital image analysis methods\nand potential applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Critical Checkpoints for Evaluating Defence Models Against Adversarial Attack and Robustness. (arXiv:2202.09039v1 [cs.CR])","link":"http://arxiv.org/abs/2202.09039","description":"<p>From past couple of years there is a cycle of researchers proposing a defence\nmodel for adversaries in machine learning which is arguably defensible to most\nof the existing attacks in restricted condition (they evaluate on some bounded\ninputs or datasets). And then shortly another set of researcher finding the\nvulnerabilities in that defence model and breaking it by proposing a stronger\nattack model. Some common flaws are been noticed in the past defence models\nthat were broken in very short time. Defence models being broken so easily is a\npoint of concern as decision of many crucial activities are taken with the help\nof machine learning models. So there is an utter need of some defence\ncheckpoints that any researcher should keep in mind while evaluating the\nsoundness of technique and declaring it to be decent defence technique. In this\npaper, we have suggested few checkpoints that should be taken into\nconsideration while building and evaluating the soundness of defence models.\nAll these points are recommended after observing why some past defence models\nfailed and how some model remained adamant and proved their soundness against\nsome of the very strong attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tekwani_K/0/1/0/all/0/1\">Kanak Tekwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Manojkumar Parmar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Specific Attention is one more thing you need for object detection. (arXiv:2202.09048v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09048","description":"<p>Various models have been proposed to solve the object detection problem.\nHowever, most of them require many hand-designed components to demonstrate good\nperformance. To mitigate these issues, Transformer based DETR and its variant\nDeformable DETR were suggested. They solved much of the complex issue of\ndesigning a head of object detection model but it has not been generally clear\nthat the Transformer-based models could be considered as the state-of-the-art\nmethod in object detection without doubt. Furthermore, as DETR adapted\nTransformer method only for the detection head, but still with including CNN\nfor the backbone body, it has not been certain that it would be possible to\nbuild the competent end-to-end pipeline with the combination of attention\nmodules. In this paper, we propose that combining several attention modules\nwith our new Task Specific Split Transformer(TSST) is a fairly good enough\nmethod to produce the best COCO results without traditionally hand-designed\ncomponents. By splitting generally purposed attention module into two separated\nmission specific attention module, the proposed method addresses the way to\ndesign simpler object detection models than before. Extensive experiments on\nthe COCO benchmark demonstrate the effectiveness of our approach. Code is\nreleased at https://github.com/navervision/tsst\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang Yon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guide Local Feature Matching by Overlap Estimation. (arXiv:2202.09050v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09050","description":"<p>Local image feature matching under large appearance, viewpoint, and distance\nchanges is challenging yet important. Conventional methods detect and match\ntentative local features across the whole images, with heuristic consistency\nchecks to guarantee reliable matches. In this paper, we introduce a novel\nOverlap Estimation method conditioned on image pairs with TRansformer, named\nOETR, to constrain local feature matching in the commonly visible region. OETR\nperforms overlap estimation in a two-step process of feature correlation and\nthen overlap regression. As a preprocessing module, OETR can be plugged into\nany existing local feature detection and matching pipeline, to mitigate\npotential view angle or scale variance. Intensive experiments show that OETR\ncan boost state-of-the-art local feature matching performance substantially,\nespecially for image pairs with small shared regions. The code will be publicly\navailable at https://github.com/AbyssGaze/OETR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dihe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards better understanding and better generalization of few-shot classification in histology images with contrastive learning. (arXiv:2202.09059v1 [eess.IV])","link":"http://arxiv.org/abs/2202.09059","description":"<p>Few-shot learning is an established topic in natural images for years, but\nfew work is attended to histology images, which is of high clinical value since\nwell-labeled datasets and rare abnormal samples are expensive to collect. Here,\nwe facilitate the study of few-shot learning in histology images by setting up\nthree cross-domain tasks that simulate real clinics problems. To enable\nlabel-efficient learning and better generalizability, we propose to incorporate\ncontrastive learning (CL) with latent augmentation (LA) to build a few-shot\nsystem. CL learns useful representations without manual labels, while LA\ntransfers semantic variations of the base dataset in an unsupervised way. These\ntwo components fully exploit unlabeled training data and can scale gracefully\nto other label-hungry problems. In experiments, we find i) models learned by CL\ngeneralize better than supervised learning for histology images in unseen\nclasses, and ii) LA brings consistent gains over baselines. Prior studies of\nself-supervised learning mainly focus on ImageNet-like images, which only\npresent a dominant object in their centers. Recent attention has been paid to\nimages with multi-objects and multi-textures. Histology images are a natural\nchoice for such a study. We show the superiority of CL over supervised learning\nin terms of generalization for such data and provide our empirical\nunderstanding for this observation. The findings in this work could contribute\nto understanding how the model generalizes in the context of both\nrepresentation learning and histological image analysis. Code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jiawei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hanbo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_J/0/1/0/all/0/1\">Jiangpeng Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1\">Jianhua Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLP: A Survey on Vision-Language Pre-training. (arXiv:2202.09061v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09061","description":"<p>In the past few years, the emergence of pre-training models has brought\nuni-modal fields such as computer vision (CV) and natural language processing\n(NLP) to a new era. Substantial works have shown they are beneficial for\ndownstream uni-modal tasks and avoid training a new model from scratch. So can\nsuch pre-trained models be applied to multi-modal tasks? Researchers have\nexplored this problem and made significant progress. This paper surveys recent\nadvances and new frontiers in vision-language pre-training (VLP), including\nimage-text and video-text pre-training. To give readers a better overall grasp\nof VLP, we first review its recent advances from five aspects: feature\nextraction, model architecture, pre-training objectives, pre-training datasets,\nand downstream tasks. Then, we summarize the specific VLP models in detail.\nFinally, we discuss the new frontiers in VLP. To the best of our knowledge,\nthis is the first survey on VLP. We hope that this survey can shed light on\nfuture research in the VLP field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Duzhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Minglun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VCVTS: Multi-speaker Video-to-Speech synthesis via cross-modal knowledge transfer from voice conversion. (arXiv:2202.09081v1 [eess.AS])","link":"http://arxiv.org/abs/2202.09081","description":"<p>Though significant progress has been made for speaker-dependent\nVideo-to-Speech (VTS) synthesis, little attention is devoted to multi-speaker\nVTS that can map silent video to speech, while allowing flexible control of\nspeaker identity, all in a single system. This paper proposes a novel\nmulti-speaker VTS system based on cross-modal knowledge transfer from voice\nconversion (VC), where vector quantization with contrastive predictive coding\n(VQCPC) is used for the content encoder of VC to derive discrete phoneme-like\nacoustic units, which are transferred to a Lip-to-Index (Lip2Ind) network to\ninfer the index sequence of acoustic units. The Lip2Ind network can then\nsubstitute the content encoder of VC to form a multi-speaker VTS system to\nconvert silent video to acoustic units for reconstructing accurate spoken\ncontent. The VTS system also inherits the advantages of VC by using a speaker\nencoder to produce speaker representations to effectively control the speaker\nidentity of generated speech. Extensive evaluations verify the effectiveness of\nproposed approach, which can be applied in both constrained vocabulary and open\nvocabulary conditions, achieving state-of-the-art performance in generating\nhigh-quality speech with high naturalness, intelligibility and speaker\nsimilarity. Our demo page is released here:\nhttps://wendison.github.io/VCVTS-demo/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Disong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Multi-Drone Detection and 3D-Localization via YOLO. (arXiv:2202.09097v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09097","description":"<p>In this work, we present and evaluate a method to perform real-time multiple\ndrone detection and three-dimensional localization using state-of-the-art\ntiny-YOLOv4 object detection algorithm and stereo triangulation. Our computer\nvision approach eliminates the need for computationally expensive stereo\nmatching algorithms, thereby significantly reducing the memory footprint and\nmaking it deployable on embedded systems. Our drone detection system is highly\nmodular (with support for various detection algorithms) and capable of\nidentifying multiple drones in a system, with real-time detection accuracy of\nup to 77\\% with an average FPS of 332 (on Nvidia Titan Xp). We also test the\ncomplete pipeline in AirSim environment, detecting drones at a maximum distance\nof 8 meters, with a mean error of $23\\%$ of the distance. We also release the\nsource code for the project, with pre-trained models and the curated synthetic\nstereo dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Aryan Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nitik Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_M/0/1/0/all/0/1\">Mangal Kothari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Learning for Instance Segmentation. (arXiv:2202.09110v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09110","description":"<p>Instance segmentation is a computer vision task where separate objects in an\nimage are detected and segmented. State-of-the-art deep neural network models\nrequire large amounts of labeled data in order to perform well in this task.\nMaking these annotations is time-consuming. We propose for the first time, an\niterative learning and annotation method that is able to detect, segment and\nannotate instances in datasets composed of multiple similar objects. The\napproach requires minimal human intervention and needs only a bootstrapping set\ncontaining very few annotations. Experiments on two different datasets show the\nvalidity of the approach in different applications related to visual\ninspection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sormunen_T/0/1/0/all/0/1\">Tuomas Sormunen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamsa_A/0/1/0/all/0/1\">Arttu L&#xe4;ms&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_M/0/1/0/all/0/1\">Miguel Bordallo Lopez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Simple and Accurate Human Pose Estimation with Stair Network. (arXiv:2202.09115v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09115","description":"<p>In this paper, we focus on tackling the precise keypoint coordinates\nregression task. Most existing approaches adopt complicated networks with a\nlarge number of parameters, leading to a heavy model with poor\ncost-effectiveness in practice. To overcome this limitation, we develop a small\nyet discrimicative model called STair Network, which can be simply stacked\ntowards an accurate multi-stage pose estimation system. Specifically, to reduce\ncomputational cost, STair Network is composed of novel basic feature extraction\nblocks which focus on promoting feature diversity and obtaining rich local\nrepresentations with fewer parameters, enabling a satisfactory balance on\nefficiency and performance. To further improve the performance, we introduce\ntwo mechanisms with negligible computational cost, focusing on feature fusion\nand replenish. We demonstrate the effectiveness of the STair Network on two\nstandard datasets, e.g., 1-stage STair Network achieves a higher accuracy than\nHRNet by 5.5% on COCO test dataset with 80\\% fewer parameters and 68% fewer\nGFLOPs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chenru Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shufei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shufei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jimin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1\">Amir Hussain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing Aggregation Functions in GNNs:High-Capacity GNNs via Nonlinear Neighborhood Aggregators. (arXiv:2202.09145v1 [cs.LG])","link":"http://arxiv.org/abs/2202.09145","description":"<p>Graph neural networks (GNNs) have achieved great success in many graph\nlearning tasks. The main aspect powering existing GNNs is the multi-layer\nnetwork architecture to learn the nonlinear graph representations for the\nspecific learning tasks. The core operation in GNNs is message propagation in\nwhich each node updates its representation by aggregating its neighbors'\nrepresentations. Existing GNNs mainly adopt either linear neighborhood\naggregation (mean,sum) or max aggregator in their message propagation. (1) For\nlinear aggregators, the whole nonlinearity and network's capacity of GNNs are\ngenerally limited due to deeper GNNs usually suffer from over-smoothing issue.\n(2) For max aggregator, it usually fails to be aware of the detailed\ninformation of node representations within neighborhood. To overcome these\nissues, we re-think the message propagation mechanism in GNNs and aim to\ndevelop the general nonlinear aggregators for neighborhood information\naggregation in GNNs. One main aspect of our proposed nonlinear aggregators is\nthat they provide the optimally balanced aggregators between max and mean/sum\naggregations. Thus, our aggregators can inherit both (i) high nonlinearity that\nincreases network's capacity and (ii) detail-sensitivity that preserves the\ndetailed information of representations together in GNNs' message propagation.\nPromising experiments on several datasets show the effectiveness of the\nproposed nonlinear aggregators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Beibei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiRes-NetVLAD: Augmenting Place Recognition Training with Low-Resolution Imagery. (arXiv:2202.09146v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09146","description":"<p>Visual Place Recognition (VPR) is a crucial component of 6-DoF localization,\nvisual SLAM and structure-from-motion pipelines, tasked to generate an initial\nlist of place match hypotheses by matching global place descriptors. However,\ncommonly-used CNN-based methods either process multiple image resolutions after\ntraining or use a single resolution and limit multi-scale feature extraction to\nthe last convolutional layer during training. In this paper, we augment NetVLAD\nrepresentation learning with low-resolution image pyramid encoding which leads\nto richer place representations. The resultant multi-resolution feature pyramid\ncan be conveniently aggregated through VLAD into a single compact\nrepresentation, avoiding the need for concatenation or summation of multiple\npatches in recent multi-scale approaches. Furthermore, we show that the\nunderlying learnt feature tensor can be combined with existing multi-scale\napproaches to improve their baseline performance. Evaluation on 15\nviewpoint-varying and viewpoint-consistent benchmarking datasets confirm that\nthe proposed MultiRes-NetVLAD leads to state-of-the-art Recall@N performance\nfor global descriptor based retrieval, compared against 11 existing techniques.\nSource code is publicly available at\nhttps://github.com/Ahmedest61/MultiRes-NetVLAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khaliq_A/0/1/0/all/0/1\">Ahmad Khaliq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Texture Information into Dimensionality Reduction for High-Dimensional Images. (arXiv:2202.09179v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09179","description":"<p>High-dimensional imaging is becoming increasingly relevant in many fields\nfrom astronomy and cultural heritage to systems biology. Visual exploration of\nsuch high-dimensional data is commonly facilitated by dimensionality reduction.\nHowever, common dimensionality reduction methods do not include spatial\ninformation present in images, such as local texture features, into the\nconstruction of low-dimensional embeddings. Consequently, exploration of such\ndata is typically split into a step focusing on the attribute space followed by\na step focusing on spatial information, or vice versa. In this paper, we\npresent a method for incorporating spatial neighborhood information into\ndistance-based dimensionality reduction methods, such as t-Distributed\nStochastic Neighbor Embedding (t-SNE). We achieve this by modifying the\ndistance measure between high-dimensional attribute vectors associated with\neach pixel such that it takes the pixel's spatial neighborhood into account.\nBased on a classification of different methods for comparing image patches, we\nexplore a number of different approaches. We compare these approaches from a\ntheoretical and experimental point of view. Finally, we illustrate the value of\nthe proposed methods by qualitative and quantitative evaluation on synthetic\ndata and two real-world use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vieth_A/0/1/0/all/0/1\">Alexander Vieth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilanova_A/0/1/0/all/0/1\">Anna Vilanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lelieveldt_B/0/1/0/all/0/1\">Boudewijn Lelieveldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisemann_E/0/1/0/all/0/1\">Elmar Eisemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollt_T/0/1/0/all/0/1\">Thomas H&#xf6;llt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Outdoor Lighting Aggregation on Image Sequences using Transformer Networks. (arXiv:2202.09206v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09206","description":"<p>In this work, we focus on outdoor lighting estimation by aggregating\nindividual noisy estimates from images, exploiting the rich image information\nfrom wide-angle cameras and/or temporal image sequences. Photographs inherently\nencode information about the scene's lighting in the form of shading and\nshadows. Recovering the lighting is an inverse rendering problem and as that\nill-posed. Recent work based on deep neural networks has shown promising\nresults for single image lighting estimation, but suffers from robustness. We\ntackle this problem by combining lighting estimates from several image views\nsampled in the angular and temporal domain of an image sequence. For this task,\nwe introduce a transformer architecture that is trained in an end-2-end fashion\nwithout any statistical post-processing as required by previous work. Thereby,\nwe propose a positional encoding that takes into account the camera calibration\nand ego-motion estimation to globally register the individual estimates when\ncomputing attention between visual words. We show that our method leads to\nimproved lighting estimation while requiring less hyper-parameters compared to\nthe state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Haebom Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homeyer_C/0/1/0/all/0/1\">Christian Homeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzog_R/0/1/0/all/0/1\">Robert Herzog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rexilius_J/0/1/0/all/0/1\">Jan Rexilius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1\">Carsten Rother</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoencoding Low-Resolution MRI for Semantically Smooth Interpolation of Anisotropic MRI. (arXiv:2202.09258v1 [eess.IV])","link":"http://arxiv.org/abs/2202.09258","description":"<p>High-resolution medical images are beneficial for analysis but their\nacquisition may not always be feasible. Alternatively, high-resolution images\ncan be created from low-resolution acquisitions using conventional upsampling\nmethods, but such methods cannot exploit high-level contextual information\ncontained in the images. Recently, better performing deep-learning based\nsuper-resolution methods have been introduced. However, these methods are\nlimited by their supervised character, i.e. they require high-resolution\nexamples for training. Instead, we propose an unsupervised deep learning\nsemantic interpolation approach that synthesizes new intermediate slices from\nencoded low-resolution examples. To achieve semantically smooth interpolation\nin through-plane direction, the method exploits the latent space generated by\nautoencoders. To generate new intermediate slices, latent space encodings of\ntwo spatially adjacent slices are combined using their convex combination.\nSubsequently, the combined encoding is decoded to an intermediate slice. To\nconstrain the model, a notion of semantic similarity is defined for a given\ndataset. For this, a new loss is introduced that exploits the spatial\nrelationship between slices of the same volume. During training, an existing\nin-between slice is generated using a convex combination of its neighboring\nslice encodings. The method was trained and evaluated using publicly available\ncardiac cine, neonatal brain and adult brain MRI scans. In all evaluations, the\nnew method produces significantly better results in terms of Structural\nSimilarity Index Measure and Peak Signal-to-Noise Ratio (p&lt; 0.001 using\none-sided Wilcoxon signed-rank test) than a cubic B-spline interpolation\napproach. Given the unsupervised nature of the method, high-resolution training\ndata is not required and hence, the method can be readily applied in clinical\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sander_J/0/1/0/all/0/1\">J&#xf6;rg Sander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vos_B/0/1/0/all/0/1\">Bob D. de Vos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isgum_I/0/1/0/all/0/1\">Ivana I&#x161;gum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"(2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering. (arXiv:2202.09277v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09277","description":"<p>Spatio-temporal scene-graph approaches to video-based reasoning tasks such as\nvideo question-answering (QA) typically construct such graphs for every video\nframe. Such approaches often ignore the fact that videos are essentially\nsequences of 2D \"views\" of events happening in a 3D space, and that the\nsemantics of the 3D scene can thus be carried over from frame to frame.\nLeveraging this insight, we propose a (2.5+1)D scene graph representation to\nbetter capture the spatio-temporal information flows inside the videos.\nSpecifically, we first create a 2.5D (pseudo-3D) scene graph by transforming\nevery 2D frame to have an inferred 3D structure using an off-the-shelf 2D-to-3D\ntransformation module, following which we register the video frames into a\nshared (2.5+1)D spatio-temporal space and ground each 2D scene graph within it.\nSuch a (2.5+1)D graph is then segregated into a static sub-graph and a dynamic\nsub-graph, corresponding to whether the objects within them usually move in the\nworld. The nodes in the dynamic graph are enriched with motion features\ncapturing their interactions with other graph nodes. Next, for the video QA\ntask, we present a novel transformer-based reasoning pipeline that embeds the\n(2.5+1)D graph into a spatio-temporal hierarchical latent space, where the\nsub-graphs and their interactions are captured at varied granularity. To\ndemonstrate the effectiveness of our approach, we present experiments on the\nNExT-QA and AVSD-QA datasets. Our results show that our proposed (2.5+1)D\nrepresentation leads to faster training and inference, while our hierarchical\nmodel showcases superior performance on the video QA task versus the state of\nthe art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hori_C/0/1/0/all/0/1\">Chiori Hori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marks_T/0/1/0/all/0/1\">Tim K. Marks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Adversarially Robust Training for Unsupervised Domain Adaptation. (arXiv:2202.09300v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09300","description":"<p>Unsupervised Domain Adaptation (UDA) methods aim to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. UDA has been extensively\nstudied in the computer vision literature. Deep networks have been shown to be\nvulnerable to adversarial attacks. However, very little focus is devoted to\nimproving the adversarial robustness of deep UDA models, causing serious\nconcerns about model reliability. Adversarial Training (AT) has been considered\nto be the most successful adversarial defense approach. Nevertheless,\nconventional AT requires ground-truth labels to generate adversarial examples\nand train models, which limits its effectiveness in the unlabeled target\ndomain. In this paper, we aim to explore AT to robustify UDA models: How to\nenhance the unlabeled data robustness via AT while learning domain-invariant\nfeatures for UDA? To answer this, we provide a systematic study into multiple\nAT variants that potentially apply to UDA. Moreover, we propose a novel\nAdversarially Robust Training method for UDA accordingly, referred to as\nARTUDA. Extensive experiments on multiple attacks and benchmarks show that\nARTUDA consistently improves the adversarial robustness of UDA models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Multiple-Object Tracking with a Dynamical Variational Autoencoder. (arXiv:2202.09315v1 [cs.LG])","link":"http://arxiv.org/abs/2202.09315","description":"<p>In this paper, we present an unsupervised probabilistic model and associated\nestimation algorithm for multi-object tracking (MOT) based on a dynamical\nvariational autoencoder (DVAE), called DVAE-UMOT. The DVAE is a latent-variable\ndeep generative model that can be seen as an extension of the variational\nautoencoder for the modeling of temporal sequences. It is included in DVAE-UMOT\nto model the objects' dynamics, after being pre-trained on an unlabeled\nsynthetic dataset of single-object trajectories. Then the distributions and\nparameters of DVAE-UMOT are estimated on each multi-object sequence to track\nusing the principles of variational inference: Definition of an approximate\nposterior distribution of the latent variables and maximization of the\ncorresponding evidence lower bound of the data likehood function. DVAE-UMOT is\nshown experimentally to compete well with and even surpass the performance of\ntwo state-of-the-art probabilistic MOT models. Code and data are publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaoyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girin_L/0/1/0/all/0/1\">Laurent Girin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Machine Learning Paradigm for Studying Pictorial Realism: Are Constable's Clouds More Real than His Contemporaries?. (arXiv:2202.09348v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09348","description":"<p>European artists have sought to create life-like images since the\nRenaissance. The techniques used by artists to impart realism to their\npaintings often rely on approaches based in mathematics, like linear\nperspective; yet the means used to assess the verisimilitude of realist\npaintings have remained subjective, even intuitive. An exploration of\nalternative and relatively objective methods for evaluating pictorial realism\ncould enhance existing art historical research. We propose a\nmachine-learning-based paradigm for studying pictorial realism in an\nexplainable way. Unlike subjective evaluations made by art historians or\ncomputer-based painting analysis exploiting inexplicable learned features, our\nframework assesses realism by measuring the similarity between clouds painted\nby exceptionally skillful 19th-century landscape painters like John Constable\nand photographs of clouds. The experimental results of cloud classification\nshow that Constable approximates more consistently than his contemporaries the\nformal features of actual clouds in his paintings. Our analyses suggest that\nartists working in the decades leading up to the invention of photography\nworked in a mode that anticipated some of the stylistic features of\nphotography. The study is a springboard for deeper analyses of pictorial\nrealism using computer vision and machine learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuomin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansfield_E/0/1/0/all/0/1\">Elizabeth C. Mansfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_J/0/1/0/all/0/1\">John Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_G/0/1/0/all/0/1\">George S. Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_C/0/1/0/all/0/1\">Catherine Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">James Z. Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No-Reference Light Field Image Quality Assessment Based on Spatial-Angular Measurement. (arXiv:1908.06280v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/1908.06280","description":"<p>Light field image quality assessment (LFI-QA) is a significant and\nchallenging research problem. It helps to better guide light field acquisition,\nprocessing and applications. However, only a few objective models have been\nproposed and none of them completely consider intrinsic factors affecting the\nLFI quality. In this paper, we propose a No-Reference Light Field image Quality\nAssessment (NR-LFQA) scheme, where the main idea is to quantify the LFI quality\ndegradation through evaluating the spatial quality and angular consistency. We\nfirst measure the spatial quality deterioration by capturing the naturalness\ndistribution of the light field cyclopean image array, which is formed when\nhuman observes the LFI. Then, as a transformed representation of LFI, the\nEpipolar Plane Image (EPI) contains the slopes of lines and involves the\nangular information. Therefore, EPI is utilized to extract the global and local\nfeatures from LFI to measure angular consistency degradation. Specifically, the\ndistribution of gradient direction map of EPI is proposed to measure the global\nangular consistency distortion in the LFI. We further propose the weighted\nlocal binary pattern to capture the characteristics of local angular\nconsistency degradation. Extensive experimental results on four publicly\navailable LFI quality datasets demonstrate that the proposed method outperforms\nstate-of-the-art 2D, 3D, multi-view, and LFI quality assessment algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shi_L/0/1/0/all/0/1\">Likun Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jinglin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Oriented No-Reference Light Field Image Quality Assessment. (arXiv:1909.02358v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/1909.02358","description":"<p>Light field image (LFI) quality assessment is becoming more and more\nimportant, which helps to better guide the acquisition, processing and\napplication of immersive media. However, due to the inherent high dimensional\ncharacteristics of LFI, the LFI quality assessment turns into a\nmulti-dimensional problem that requires consideration of the quality\ndegradation in both spatial and angular dimensions. Therefore, we propose a\nnovel Tensor oriented No-reference Light Field image Quality evaluator\n(Tensor-NLFQ) based on tensor theory. Specifically, since the LFI is regarded\nas a low-rank 4D tensor, the principal components of four oriented sub-aperture\nview stacks are obtained via Tucker decomposition. Then, the Principal\nComponent Spatial Characteristic (PCSC) is designed to measure the\nspatial-dimensional quality of LFI considering its global naturalness and local\nfrequency properties. Finally, the Tensor Angular Variation Index (TAVI) is\nproposed to measure angular consistency quality by analyzing the structural\nsimilarity distribution between the first principal component and each view in\nthe view stack. Extensive experimental results on four publicly available LFI\nquality databases demonstrate that the proposed Tensor-NLFQ model outperforms\nstate-of-the-art 2D, 3D, multi-view, and LFI quality assessment algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_L/0/1/0/all/0/1\">Likun Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jinglin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point and Ask: Incorporating Pointing into Visual Question Answering. (arXiv:2011.13681v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13681","description":"<p>Visual Question Answering (VQA) has become one of the key benchmarks of\nvisual recognition progress. Multiple VQA extensions have been explored to\nbetter simulate real-world settings: different question formulations, changing\ntraining and test distributions, conversational consistency in dialogues, and\nexplanation-based answering. In this work, we further expand this space by\nconsidering visual questions that include a spatial point of reference.\nPointing is a nearly universal gesture among humans, and real-world VQA is\nlikely to involve a gesture towards the target region.\n</p>\n<p>Concretely, we (1) introduce and motivate point-input questions as an\nextension of VQA, (2) define three novel classes of questions within this\nspace, and (3) for each class, introduce both a benchmark dataset and a series\nof baseline models to handle its unique challenges. There are two key\ndistinctions from prior work. First, we explicitly design the benchmarks to\nrequire the point input, i.e., we ensure that the visual question cannot be\nanswered accurately without the spatial reference. Second, we explicitly\nexplore the more realistic point spatial input rather than the standard but\nunnatural bounding box input. Through our exploration we uncover and address\nseveral visual recognition challenges, including the ability to infer human\nintent, reason both locally and globally about the image, and effectively\ncombine visual, language and spatial inputs. Code is available at:\nhttps://github.com/princetonvisualai/pointingqa .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mani_A/0/1/0/all/0/1\">Arjun Mani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_N/0/1/0/all/0/1\">Nobline Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinthorn_W/0/1/0/all/0/1\">Will Hinthorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Target Detection and Segmentation in Circular-Scan Synthetic-Aperture-Sonar Images using Semi-Supervised Convolutional Encoder-Decoders. (arXiv:2101.03603v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.03603","description":"<p>We propose a framework for saliency-based, multi-target detection and\nsegmentation of circular-scan, synthetic-aperture-sonar (CSAS) imagery. Our\nframework relies on a multi-branch, convolutional encoder-decoder network\n(MB-CEDN). The encoder portion of the MB-CEDN extracts visual contrast features\nfrom CSAS images. These features are fed into dual decoders that perform\npixel-level segmentation to mask targets. Each decoder provides different\nperspectives as to what constitutes a salient target. These opinions are\naggregated and cascaded into a deep-parsing network to refine the segmentation.\n</p>\n<p>We evaluate our framework using real-world CSAS imagery consisting of five\nbroad target classes. We compare against existing approaches from the\ncomputer-vision literature. We show that our framework outperforms supervised,\ndeep-saliency networks designed for natural imagery. It greatly outperforms\nunsupervised saliency approaches developed for natural imagery. This\nillustrates that natural-image-based models may need to be altered to be\neffective for this imaging-sonar modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1\">Isaac J. Sledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emigh_M/0/1/0/all/0/1\">Matthew S. Emigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_J/0/1/0/all/0/1\">Jonathan L. King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woods_D/0/1/0/all/0/1\">Denton L. Woods</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobb_J/0/1/0/all/0/1\">J. Tory Cobb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1\">Jose C. Principe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Doesn't Kill You Makes You Robust(er): How to Adversarially Train against Data Poisoning. (arXiv:2102.13624v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.13624","description":"<p>Data poisoning is a threat model in which a malicious actor tampers with\ntraining data to manipulate outcomes at inference time. A variety of defenses\nagainst this threat model have been proposed, but each suffers from at least\none of the following flaws: they are easily overcome by adaptive attacks, they\nseverely reduce testing performance, or they cannot generalize to diverse data\npoisoning threat models. Adversarial training, and its variants, are currently\nconsidered the only empirically strong defense against (inference-time)\nadversarial attacks. In this work, we extend the adversarial training framework\nto defend against (training-time) data poisoning, including targeted and\nbackdoor attacks. Our method desensitizes networks to the effects of such\nattacks by creating poisons during training and injecting them into training\nbatches. We show that this defense withstands adaptive attacks, generalizes to\ndiverse threat models, and incurs a better performance trade-off than previous\ndefenses such as DP-SGD or (evasion) adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1\">Liam Fowl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somepalli_G/0/1/0/all/0/1\">Gowthami Somepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Compression Auto-Encoder for Detecting Road Surface Abnormality via Vehicle Driving Noise. (arXiv:2103.12992v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12992","description":"<p>Road accident can be triggered by wet road because it decreases skid\nresistance. To prevent the road accident, detecting road surface abnomality is\nhighly useful. In this paper, we propose the deep learning based cost-effective\nreal-time anomaly detection architecture, naming with non-compression\nauto-encoder (NCAE). The proposed architecture can reflect forward and backward\ncausality of time series information via convolutional operation. Moreover, the\nabove architecture shows higher anomaly detection performance of published\nanomaly detection model via experiments. We conclude that NCAE as a\ncutting-edge model for road surface anomaly detection with 4.20\\% higher AUROC\nand 2.99 times faster decision than before.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">YeongHyeon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">JongHee Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Local Descriptor for Improved Few-Shot Classification. (arXiv:2103.16009v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16009","description":"<p>Few-shot classification studies the problem of quickly adapting a deep\nlearner to understanding novel classes based on few support images. In this\ncontext, recent research efforts have been aimed at designing more and more\ncomplex classifiers that measure similarities between query and support images,\nbut left the importance of feature embeddings seldom explored. We show that the\nreliance on sophisticated classifiers is not necessary, and a simple classifier\napplied directly to improved feature embeddings can instead outperform most of\nthe leading methods in the literature. To this end, we present a new method\nnamed \\textbf{DCAP} for few-shot classification, in which we investigate how\none can improve the quality of embeddings by leveraging \\textbf{D}ense\n\\textbf{C}lassification and \\textbf{A}ttentive \\textbf{P}ooling. Specifically,\nwe propose to train a learner on base classes with abundant samples to solve\ndense classification problem first and then meta-train the learner on a bunch\nof randomly sampled few-shot tasks to adapt it to few-shot scenario or the test\ntime scenario. During meta-training, we suggest to pool feature maps by\napplying attentive pooling instead of the widely used global average pooling\n(GAP) to prepare embeddings for few-shot classification. Attentive pooling\nlearns to reweight local descriptors, explaining what the learner is looking\nfor as evidence for decision making. Experiments on two benchmark datasets show\nthe proposed method to be superior in multiple few-shot settings while being\nsimpler and more explainable. Code is available at:\n\\url{https://github.com/Ukeyboard/dcap/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Richang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xueliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qianru Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09124","description":"<p>While self-supervised representation learning (SSL) has received widespread\nattention from the community, recent research argue that its performance will\nsuffer a cliff fall when the model size decreases. The current method mainly\nrelies on contrastive learning to train the network and in this work, we\npropose a simple yet effective Distilled Contrastive Learning (DisCo) to ease\nthe issue by a large margin. Specifically, we find the final embedding obtained\nby the mainstream SSL methods contains the most fruitful information, and\npropose to distill the final embedding to maximally transmit a teacher's\nknowledge to a lightweight model by constraining the last embedding of the\nstudent to be consistent with that of the teacher. In addition, in the\nexperiment, we find that there exists a phenomenon termed Distilling BottleNeck\nand present to enlarge the embedding dimension to alleviate this problem. Our\nmethod does not introduce any extra parameter to lightweight models during\ndeployment. Experimental results demonstrate that our method achieves the\nstate-of-the-art on all lightweight models. Particularly, when\nResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear\nresult of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,\nbut the number of parameters of EfficientNet-B0 is only 9.4\\%/16.3\\% of\nResNet-101/ResNet-50. Code is available at https://github.\ncom/Yuting-Gao/DisCo-pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jia-Xin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaohui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-Aware Long Short-Term Memory Network for 3D Cephalometric Landmark Detection. (arXiv:2107.09899v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.09899","description":"<p>Detecting 3D landmarks on cone-beam computed tomography (CBCT) is crucial to\nassessing and quantifying the anatomical abnormalities in 3D cephalometric\nanalysis. However, the current methods are time-consuming and suffer from large\nbiases in landmark localization, leading to unreliable diagnosis results. In\nthis work, we propose a novel Structure-Aware Long Short-Term Memory framework\n(SA-LSTM) for efficient and accurate 3D landmark detection. To reduce the\ncomputational burden, SA-LSTM is designed in two stages. It first locates the\ncoarse landmarks via heatmap regression on a down-sampled CBCT volume and then\nprogressively refines landmarks by attentive offset regression using\nmulti-resolution cropped patches. To boost accuracy, SA-LSTM captures\nglobal-local dependence among the cropping patches via self-attention.\nSpecifically, a novel graph attention module implicitly encodes the landmark's\nglobal structure to rationalize the predicted position. Moreover, a novel\nattention-gated module recursively filters irrelevant local features and\nmaintains high-confident local predictions for aggregating the final result.\nExperiments conducted on an in-house dataset and a public dataset show that our\nmethod outperforms state-of-the-art methods, achieving 1.64 mm and 2.37 mm\naverage errors, respectively. Furthermore, our method is very efficient, taking\nonly 0.5 seconds for inferring the whole CBCT volume of resolution\n768$\\times$768$\\times$576.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runnan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nenglun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yanhong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Knowledge Distillation for Efficient Pose Estimation. (arXiv:2108.02092v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02092","description":"<p>Existing state-of-the-art human pose estimation methods require heavy\ncomputational resources for accurate predictions. One promising technique to\nobtain an accurate yet lightweight pose estimator is knowledge distillation,\nwhich distills the pose knowledge from a powerful teacher model to a\nless-parameterized student model. However, existing pose distillation works\nrely on a heavy pre-trained estimator to perform knowledge transfer and require\na complex two-stage learning procedure. In this work, we investigate a novel\nOnline Knowledge Distillation framework by distilling Human Pose structure\nknowledge in a one-stage manner to guarantee the distillation efficiency,\ntermed OKDHP. Specifically, OKDHP trains a single multi-branch network and\nacquires the predicted heatmaps from each, which are then assembled by a\nFeature Aggregation Unit (FAU) as the target heatmaps to teach each branch in\nreverse. Instead of simply averaging the heatmaps, FAU which consists of\nmultiple parallel transformations with different receptive fields, leverages\nthe multi-scale information, thus obtains target heatmaps with higher-quality.\nSpecifically, the pixel-wise Kullback-Leibler (KL) divergence is utilized to\nminimize the discrepancy between the target heatmaps and the predicted ones,\nwhich enables the student network to learn the implicit keypoint relationship.\nBesides, an unbalanced OKDHP scheme is introduced to customize the student\nnetworks with different compression rates. The effectiveness of our approach is\ndemonstrated by extensive experiments on two common benchmark datasets, MPII\nand COCO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jingwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Ying Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhigeng Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupled Adaptation for Cross-Domain Object Detection. (arXiv:2110.02578v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02578","description":"<p>Cross-domain object detection is more challenging than object classification\nsince multiple objects exist in an image and the location of each object is\nunknown in the unlabeled target domain. As a result, when we adapt features of\ndifferent objects to enhance the transferability of the detector, the features\nof the foreground and the background are easy to be confused, which may hurt\nthe discriminability of the detector. Besides, previous methods focused on\ncategory adaptation but ignored another important part for object detection,\ni.e., the adaptation on bounding box regression. To this end, we propose\nD-adapt, namely Decoupled Adaptation, to decouple the adversarial adaptation\nand the training of the detector. Besides, we fill the blank of regression\ndomain adaptation in object detection by introducing a bounding box adaptor.\nExperiments show that D-adapt achieves state-of-the-art results on four\ncross-domain object detection tasks and yields 17% and 21% relative improvement\non benchmark datasets Clipart1k and Comic2k in particular.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junguang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baixu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth360: Self-supervised Learning for Monocular Depth Estimation using Learnable Camera Distortion Model. (arXiv:2110.10415v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.10415","description":"<p>Self-supervised monocular depth estimation has been widely investigated to\nestimate depth images and relative poses from RGB images. This framework is\nattractive for researchers because the depth and pose networks can be trained\nfrom just time sequence images without the need for the ground truth depth and\nposes.\n</p>\n<p>In this work, we estimate the depth around a robot (360 degree view) using\ntime sequence spherical camera images, from a camera whose parameters are\nunknown. We propose a learnable axisymmetric camera model which accepts\ndistorted spherical camera images with two fisheye camera images. In addition,\nwe trained our models with a photo-realistic simulator to generate ground truth\ndepth images to provide supervision. Moreover, we introduced loss functions to\nprovide floor constraints to reduce artifacts that can result from reflective\nfloor surfaces. We demonstrate the efficacy of our method using the spherical\ncamera images from the GO Stanford dataset and pinhole camera images from the\nKITTI dataset to compare our method's performance with that of baseline method\nin learning the camera parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hirose_N/0/1/0/all/0/1\">Noriaki Hirose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahara_K/0/1/0/all/0/1\">Kosuke Tahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vertebrae segmentation, identification and localization using a graph optimization and a synergistic cycle. (arXiv:2110.12177v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.12177","description":"<p>This paper considers the segmentation, identification and localization of\nvertebrae in CT images. Although these three tasks are related, they face\nspecific problems that add up when they are addressed together. For example\nneighboring vertebrae with similar shapes perturb the identification and\nvertebrae with complex or even pathological morphologies impact the\nsegmentation. Consequently, the three tasks tend to be approached\nindependently, e.g. labelling (localization and identification) or segmenting\nonly, or, when treated globally, a sequential strategy is used. Sequential\nmethods however are prone to accumulate errors as they are not able to recover\nfrom mistakes of the previous module. In this work, we propose to combine all\nthree tasks and leverage their interdependence: locations ease the\nsegmentation, the segmentations in turn improve the locations and they all\ncontribute and benefit from the identification task. To this purpose we propose\na virtuous cycle to enforce coherence between the three tasks. Within such a\ncycle, the tasks interoperate and are iterated until a global consistency\ncriterion is satisfied. Our experiments validate this strategy with\nanatomically coherent results that outperform the state of the art on the\nVerSe20 challenge benchmark. Our code and model are openly available for\nresearch purposes at https://gitlab.inria.fr/spine/vertebrae_segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Di Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammed_E/0/1/0/all/0/1\">Eslam Mohammed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boyer_E/0/1/0/all/0/1\">Edmond Boyer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pujades_S/0/1/0/all/0/1\">Sergi Pujades</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Motion History Images with 3D Convolutional Networks in Isolated Sign Language Recognition. (arXiv:2110.12396v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12396","description":"<p>Sign language recognition using computational models is a challenging problem\nthat requires simultaneous spatio-temporal modeling of the multiple sources,\ni.e. faces, hands, body, etc. In this paper, we propose an isolated sign\nlanguage recognition model based on a model trained using Motion History Images\n(MHI) that are generated from RGB video frames. RGB-MHI images represent\nspatio-temporal summary of each sign video effectively in a single RGB image.\nWe propose two different approaches using this RGB-MHI model. In the first\napproach, we use the RGB-MHI model as a motion-based spatial attention module\nintegrated into a 3D-CNN architecture. In the second approach, we use RGB-MHI\nmodel features directly with the features of a 3D-CNN model using a late fusion\ntechnique. We perform extensive experiments on two recently released\nlarge-scale isolated sign language datasets, namely AUTSL and BosphorusSign22k.\nOur experiments show that our models, which use only RGB data, can compete with\nthe state-of-the-art models in the literature that use multi-modal data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sincan_O/0/1/0/all/0/1\">Ozge Mercanoglu Sincan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keles_H/0/1/0/all/0/1\">Hacer Yalim Keles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VAQF: Fully Automatic Software-Hardware Co-Design Framework for Low-Bit Vision Transformer. (arXiv:2201.06618v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.06618","description":"<p>The transformer architectures with attention mechanisms have obtained success\nin Nature Language Processing (NLP), and Vision Transformers (ViTs) have\nrecently extended the application domains to various vision tasks. While\nachieving high performance, ViTs suffer from large model size and high\ncomputation complexity that hinders the deployment of them on edge devices. To\nachieve high throughput on hardware and preserve the model accuracy\nsimultaneously, we propose VAQF, a framework that builds inference accelerators\non FPGA platforms for quantized ViTs with binary weights and low-precision\nactivations. Given the model structure and the desired frame rate, VAQF will\nautomatically output the required quantization precision for activations as\nwell as the optimized parameter settings of the accelerator that fulfill the\nhardware requirements. The implementations are developed with Vivado High-Level\nSynthesis (HLS) on the Xilinx ZCU102 FPGA board, and the evaluation results\nwith the DeiT-base model indicate that a frame rate requirement of 24 frames\nper second (FPS) is satisfied with 8-bit activation quantization, and a target\nof 30 FPS is met with 6-bit activation quantization. To the best of our\nknowledge, this is the first time quantization has been incorporated into ViT\nacceleration on FPGAs with the help of a fully automatic framework to guide the\nquantization strategy on the software side and the accelerator implementations\non the hardware side given the target frame rate. Very small compilation time\ncost is incurred compared with quantization training, and the generated\naccelerators show the capability of achieving real-time execution for\nstate-of-the-art ViT models on FPGAs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mengshu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1\">Guoliang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Balanced Batch Normalization for Exemplar-based Class-Incremental Learning. (arXiv:2201.12559v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12559","description":"<p>Batch Normalization (BN) is an essential layer for training neural network\nmodels in various computer vision tasks. It has been widely used in continual\nlearning scenarios with little discussion, but we find that BN should be\ncarefully applied, particularly for the exemplar memory based class incremental\nlearning (CIL). We first analyze that the empirical mean and variance obtained\nfor normalization in a BN layer become highly biased toward the current task.\nTo tackle its significant problems in training and test phases, we propose\nTask-Balanced Batch Normalization (TBBN). Given each mini-batch imbalanced\nbetween the current and previous tasks, TBBN first reshapes and repeats the\nbatch, calculating near task-balanced mean and variance. Second, we show that\nwhen the affine transformation parameters of BN are learned from a reshaped\nfeature map, they become less-biased toward the current task. Based on our\nextensive CIL experiments with CIFAR-100 and ImageNet-100 datasets, we\ndemonstrate that our TBBN is easily applicable to most of existing\nexemplar-based CIL algorithms, improving their performance by decreasing the\nforgetting on the previous tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Soonwon Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Moontae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Online Meta-Learning Without Task Boundaries. (arXiv:2202.00263v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.00263","description":"<p>While deep networks can learn complex functions such as classifiers,\ndetectors, and trackers, many applications require models that continually\nadapt to changing input distributions, changing tasks, and changing\nenvironmental conditions. Indeed, this ability to continuously accrue knowledge\nand use past experience to learn new tasks quickly in continual settings is one\nof the key properties of an intelligent system. For complex and\nhigh-dimensional problems, simply updating the model continually with standard\nlearning algorithms such as gradient descent may result in slow adaptation.\nMeta-learning can provide a powerful tool to accelerate adaptation yet is\nconventionally studied in batch settings. In this paper, we study how\nmeta-learning can be applied to tackle online problems of this nature,\nsimultaneously adapting to changing tasks and input distributions and\nmeta-training the model in order to adapt more quickly in the future. Extending\nmeta-learning into the online setting presents its own challenges, and although\nseveral prior methods have studied related problems, they generally require a\ndiscrete notion of tasks, with known ground-truth task boundaries. Such methods\ntypically adapt to each task in sequence, resetting the model between tasks,\nrather than adapting continuously across tasks. In many real-world settings,\nsuch discrete boundaries are unavailable, and may not even exist. To address\nthese settings, we propose a Fully Online Meta-Learning (FOML) algorithm, which\ndoes not require any ground truth knowledge about the task boundaries and stays\nfully online without resetting back to pre-trained weights. Our experiments\nshow that FOML was able to learn new tasks faster than the state-of-the-art\nonline learning methods on Rainbow-MNIST, CIFAR100 and CELEBA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajasegaran_J/0/1/0/all/0/1\">Jathushan Rajasegaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate calibration of multi-perspective cameras from a generalization of the hand-eye constraint. (arXiv:2202.00886v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.00886","description":"<p>Multi-perspective cameras are quickly gaining importance in many applications\nsuch as smart vehicles and virtual or augmented reality. However, a large\nsystem size or absence of overlap in neighbouring fields-of-view often\ncomplicate their calibration. We present a novel solution which relies on the\navailability of an external motion capture system. Our core contribution\nconsists of an extension to the hand-eye calibration problem which jointly\nsolves multi-eye-to-base problems in closed form. We furthermore demonstrate\nits equivalence to the multi-eye-in-hand problem. The practical validity of our\napproach is supported by our experiments, indicating that the method is highly\nefficient and accurate, and outperforms existing closed-form alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1\">S&#xf6;ren Schwertfeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSHA: Video Violence Recognition and Localization Using a Semi-Supervised Hard Attention Model. (arXiv:2202.02212v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02212","description":"<p>Empowering automated violence monitoring and surveillance systems amid the\ngrowing social violence and extremist activities worldwide could keep\ncommunities safe and save lives. The questionable reliability of human\nmonitoring personnel and the increasing number of surveillance cameras makes\nautomated artificial intelligence-based solutions compelling. Improving the\ncurrent state-of-the-art deep learning approaches to video violence recognition\nto higher levels of accuracy and performance could enable surveillance systems\nto be more reliable and scalable. The main contribution of the proposed deep\nreinforcement learning method is to achieve state-of-the-art accuracy on RWF,\nHockey, and Movies datasets while removing some of the computationally\nexpensive processes and input features used in the previous solutions. The\nimplementation of hard attention using a semi-supervised learning method made\nthe proposed method capable of rough violence localization and added increased\nagent interpretability to the violence detection system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_H/0/1/0/all/0/1\">Hamid Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazerfard_E/0/1/0/all/0/1\">Ehsan Nazerfard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-driven Neural Physically-based Facial Asset for Production. (arXiv:2202.05592v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05592","description":"<p>Production-level workflows for producing convincing 3D dynamic human faces\nhave long relied on a disarray of labor-intensive tools for geometry and\ntexture generation, motion capture and rigging, and expression synthesis.\nRecent neural approaches automate individual components but the corresponding\nlatent representations cannot provide artists explicit controls as in\nconventional tools. In this paper, we present a new learning-based,\nvideo-driven approach for generating dynamic facial geometries with\nhigh-quality physically-based assets. Two key components are well-structured\nlatent spaces due to dense temporal samplings from videos and explicit facial\nexpression controls to regulate the latent spaces. For data collection, we\nconstruct a hybrid multiview-photometric capture stage, coupling with an\nultra-fast video camera to obtain raw 3D facial assets. We then model the\nfacial expression, geometry and physically-based textures using separate VAEs\nwith a global MLP-based expression mapping across the latent spaces, to\npreserve characteristics across respective attributes while maintaining\nexplicit controls over geometry and texture. We also introduce to model the\ndelta information as wrinkle maps for physically-base textures, achieving\nhigh-quality rendering of dynamic textures. We demonstrate our approach in\nhigh-fidelity performer-specific facial capture and cross-identity facial\nmotion retargeting. In addition, our neural asset along with fast adaptation\nschemes can also be deployed to handle in-the-wild videos. Besides, we motivate\nthe utility of our explicit facial disentangle strategy by providing promising\nphysically-based editing results like geometry and material editing or winkle\ntransfer with high realism. Comprehensive experiments show that our technique\nprovides higher accuracy and visual fidelity than previous video-driven facial\nreconstruction and animation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chuxiao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Ruixiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data Augmentation Method for Fully Automatic Brain Tumor Segmentation. (arXiv:2202.06344v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.06344","description":"<p>Automatic segmentation of glioma and its subregions is of great significance\nfor diagnosis, treatment and monitoring of disease. In this paper, an\naugmentation method, called TensorMixup, was proposed and applied to the three\ndimensional U-Net architecture for brain tumor segmentation. The main ideas\nincluded that first, two image patches with size of 128 in three dimensions\nwere selected according to glioma information of ground truth labels from the\nmagnetic resonance imaging data of any two patients with the same modality.\nNext, a tensor in which all elements were independently sampled from Beta\ndistribution was used to mix the image patches. Then the tensor was mapped to a\nmatrix which was used to mix the one-hot encoded labels of the above image\npatches. Therefore, a new image and its one-hot encoded label were synthesized.\nFinally, the new data was used to train the model which could be used to\nsegment glioma. The experimental results show that the mean accuracy of Dice\nscores are 91.32%, 85.67%, and 82.20% respectively on the whole tumor, tumor\ncore, and enhancing tumor segmentation, which proves that the proposed\nTensorMixup is feasible and effective for brain tumor segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_Y/0/1/0/all/0/1\">Yarong Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_H/0/1/0/all/0/1\">Hongbing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased Pseudo Labeling in Self-Training. (arXiv:2202.07136v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.07136","description":"<p>Deep neural networks achieve remarkable performances on a wide range of tasks\nwith the aid of large-scale labeled datasets. However, large-scale annotations\nare time-consuming and labor-exhaustive to obtain on realistic tasks. To\nmitigate the requirement for labeled data, self-training is widely used in both\nacademia and industry by pseudo labeling on readily-available unlabeled data.\nDespite its popularity, pseudo labeling is well-believed to be unreliable and\noften leads to training instability. Our experimental studies further reveal\nthat the performance of self-training is biased due to data sampling,\npre-trained models, and training strategies, especially the inappropriate\nutilization of pseudo labels. To this end, we propose Debiased, in which the\ngeneration and utilization of pseudo labels are decoupled by two independent\nheads. To further improve the quality of pseudo labels, we introduce a\nworst-case estimation of pseudo labeling and seamlessly optimize the\nrepresentations to avoid the worst-case. Extensive experiments justify that the\nproposed Debiased not only yields an average improvement of $14.4$\\% against\nstate-of-the-art algorithms on $11$ tasks (covering generic object recognition,\nfine-grained object recognition, texture classification, and scene\nclassification) but also helps stabilize training and balance performance\nacross classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baixu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junguang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Ximei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Human Sperm Head Morphology Classification with Unsupervised Anatomical Feature Distillation. (arXiv:2202.07191v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07191","description":"<p>With rising male infertility, sperm head morphology classification becomes\ncritical for accurate and timely clinical diagnosis. Recent deep learning (DL)\nmorphology analysis methods achieve promising benchmark results, but leave\nperformance and robustness on the table by relying on limited and possibly\nnoisy class labels. To address this, we introduce a new DL training framework\nthat leverages anatomical and image priors from human sperm microscopy crops to\nextract useful features without additional labeling cost. Our core idea is to\ndistill sperm head information with reliably-generated pseudo-masks and\nunsupervised spatial prediction tasks. The predicted foreground masks from this\ndistillation step are then leveraged to regularize and reduce image and label\nnoise in the tuning stage. We evaluate our new approach on two public sperm\ndatasets and achieve state-of-the-art performances (e.g. 65.9% SCIAN accuracy\nand 96.5% HuSHeM accuracy).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yejia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingjing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_X/0/1/0/all/0/1\">Xiaomin Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiru Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunxia Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danny Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADAM Challenge: Detecting Age-related Macular Degeneration from Fundus Images. (arXiv:2202.07983v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.07983","description":"<p>Age-related macular degeneration (AMD) is the leading cause of visual\nimpairment among elderly in the world. Early detection of AMD is of great\nimportance as the vision loss caused by AMD is irreversible and permanent.\nColor fundus photography is the most cost-effective imaging modality to screen\nfor retinal disorders. \\textcolor{red}{Recently, some algorithms based on deep\nlearning had been developed for fundus image analysis and automatic AMD\ndetection. However, a comprehensive annotated dataset and a standard evaluation\nbenchmark are still missing.} To deal with this issue, we set up the Automatic\nDetection challenge on Age-related Macular degeneration (ADAM) for the first\ntime, held as a satellite event of the ISBI 2020 conference. The ADAM challenge\nconsisted of four tasks which cover the main topics in detecting AMD from\nfundus images, including classification of AMD, detection and segmentation of\noptic disc, localization of fovea, and detection and segmentation of lesions.\nThe ADAM challenge has released a comprehensive dataset of 1200 fundus images\nwith the category labels of AMD, the pixel-wise segmentation masks of the full\noptic disc and lesions (drusen, exudate, hemorrhage, scar, and other), as well\nas the location coordinates of the macular fovea. A uniform evaluation\nframework has been built to make a fair comparison of different models. During\nthe ADAM challenge, 610 results were submitted for online evaluation, and\nfinally, 11 teams participated in the onsite challenge. This paper introduces\nthe challenge, dataset, and evaluation methods, as well as summarizes the\nmethods and analyzes the results of the participating teams of each task. In\nparticular, we observed that ensembling strategy and clinical prior knowledge\ncan better improve the performances of the deep learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1\">Xingxing Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_F/0/1/0/all/0/1\">Fengbin Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Son_J/0/1/0/all/0/1\">Jaemin Son</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sunho Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quellec_G/0/1/0/all/0/1\">Gwenole Quellec</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matta_S/0/1/0/all/0/1\">Sarah Matta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shankaranarayana_S/0/1/0/all/0/1\">Sharath M Shankaranarayana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ting Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chuen-heng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_N/0/1/0/all/0/1\">Nisarg A. Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Yen Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_C/0/1/0/all/0/1\">Chih-Chung Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1\">Hai Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1\">Ujjwal Baid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Innani_S/0/1/0/all/0/1\">Shubham Innani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dang_K/0/1/0/all/0/1\">Kang Dang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_W/0/1/0/all/0/1\">Wenxiu Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamble_R/0/1/0/all/0/1\">Ravi Kamble</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singhal_N/0/1/0/all/0/1\">Nitin Singhal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orlando_J/0/1/0/all/0/1\">Jos&#xe9; Ignacio Orlando</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bogunovic_H/0/1/0/all/0/1\">Hrvoje Bogunovi&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiulan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-Aware Indoor Scene Synthesis with Depth Priors. (arXiv:2202.08553v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08553","description":"<p>Despite the recent advancement of Generative Adversarial Networks (GANs) in\nlearning 3D-aware image synthesis from 2D data, existing methods fail to model\nindoor scenes due to the large diversity of room layouts and the objects\ninside. We argue that indoor scenes do not have a shared intrinsic structure,\nand hence only using 2D images cannot adequately guide the model with the 3D\ngeometry. In this work, we fill in this gap by introducing depth as a 3D prior.\nCompared with other 3D data formats, depth better fits the convolution-based\ngeneration mechanism and is more easily accessible in practice. Specifically,\nwe propose a dual-path generator, where one path is responsible for depth\ngeneration, whose intermediate features are injected into the other path as the\ncondition for appearance rendering. Such a design eases the 3D-aware synthesis\nwith explicit geometry information. Meanwhile, we introduce a switchable\ndiscriminator both to differentiate real v.s. fake domains and to predict the\ndepth from a given input. In this way, the discriminator can take the spatial\narrangement into account and advise the generator to learn an appropriate depth\ncondition. Extensive experimental results suggest that our approach is capable\nof synthesizing indoor scenes with impressively good quality and 3D\nconsistency, significantly outperforming state-of-the-art alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zifan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiapeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1\">Dit-Yan Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stage Architectural Fine-Tuning with Neural Architecture Search using Early-Stopping in Image Classification. (arXiv:2202.08604v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08604","description":"<p>Deep neural networks (NN) perform well in various tasks (e.g., computer\nvision) because of the convolutional neural networks (CNN). However, the\ndifficulty of gathering quality data in the industry field hinders the\npractical use of NN. To cope with this issue, the concept of transfer learning\n(TL) has emerged, which leverages the fine-tuning of NNs trained on large-scale\ndatasets in data-scarce situations. Therefore, this paper suggests a two-stage\narchitectural fine-tuning method for image classification, inspired by the\nconcept of neural architecture search (NAS). One of the main ideas of our\nproposed method is a mutation with base architectures, which reduces the search\ncost by using given architectural information. Moreover, an early-stopping is\nalso considered which directly reduces NAS costs. Experimental results verify\nthat our proposed method reduces computational and searching costs by up to\n28.2% and 22.3%, compared to existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngkee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_W/0/1/0/all/0/1\">Won Joon Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youn Kyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}