{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Birth of Bias: A case study on the evolution of gender bias in an English language model. (arXiv:2207.10245v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10245","description":"<p>Detecting and mitigating harmful biases in modern language models are widely\nrecognized as crucial, open problems. In this paper, we take a step back and\ninvestigate how language models come to be biased in the first place. We use a\nrelatively small language model, using the LSTM architecture trained on an\nEnglish Wikipedia corpus. With full access to the data and to the model\nparameters as they change during every step while training, we can map in\ndetail how the representation of gender develops, what patterns in the dataset\ndrive this, and how the model's internal state relates to the bias in a\ndownstream task (semantic textual similarity). We find that the representation\nof gender is dynamic and identify different phases during training.\nFurthermore, we show that gender information is represented increasingly\nlocally in the input embeddings of the model and that, as a consequence,\ndebiasing these can be effective in reducing the downstream bias. Monitoring\nthe training dynamics, allows us to detect an asymmetry in how the female and\nmale gender are represented in the input embeddings. This is important, as it\nmay cause naive mitigation strategies to introduce new undesirable biases. We\ndiscuss the relevance of the findings for mitigation strategies more generally\nand the prospects of generalizing our methods to larger language models, the\nTransformer architecture, other languages and other undesirable biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wal_O/0/1/0/all/0/1\">Oskar van der Wal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jumelet_J/0/1/0/all/0/1\">Jaap Jumelet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_K/0/1/0/all/0/1\">Katrin Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuidema_W/0/1/0/all/0/1\">Willem Zuidema</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi Resolution Analysis (MRA) for Approximate Self-Attention. (arXiv:2207.10284v1 [cs.LG])","link":"http://arxiv.org/abs/2207.10284","description":"<p>Transformers have emerged as a preferred model for many tasks in natural\nlangugage processing and vision. Recent efforts on training and deploying\nTransformers more efficiently have identified many strategies to approximate\nthe self-attention matrix, a key module in a Transformer architecture.\nEffective ideas include various prespecified sparsity patterns, low-rank basis\nexpansions and combinations thereof. In this paper, we revisit classical\nMultiresolution Analysis (MRA) concepts such as Wavelets, whose potential value\nin this setting remains underexplored thus far. We show that simple\napproximations based on empirical feedback and design choices informed by\nmodern hardware and implementation challenges, eventually yield a MRA-based\napproach for self-attention with an excellent performance profile across most\ncriteria of interest. We undertake an extensive set of experiments and\ndemonstrate that this multi-resolution scheme outperforms most efficient\nself-attention proposals and is favorable for both short and long sequences.\nCode is available at \\url{https://github.com/mlpen/mra-attention}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhanpeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Sourav Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_J/0/1/0/all/0/1\">Jeffery Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1\">Glenn M Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vikas Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Visual Representations with Texts for Domain Generalization. (arXiv:2207.10285v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10285","description":"<p>Reducing the representational discrepancy between source and target domains\nis a key component to maximize the model generalization. In this work, we\nadvocate for leveraging natural language supervision for the domain\ngeneralization task. We introduce two modules to ground visual representations\nwith texts containing typical reasoning of humans: (1) Visual and Textual Joint\nEmbedder and (2) Textual Explanation Generator. The former learns the\nimage-text joint embedding space where we can ground high-level\nclass-discriminative information into the model. The latter leverages an\nexplainable model and generates explanations justifying the rationale behind\nits decision. To the best of our knowledge, this is the first work to leverage\nthe vision-and-language cross-modality approach for the domain generalization\ntask. Our experiments with a newly created CUB-DG benchmark dataset demonstrate\nthat cross-modality supervision can be successfully used to ground\ndomain-invariant visual representations and improve the model generalization.\nFurthermore, in the large-scale DomainBed benchmark, our proposed method\nachieves state-of-the-art results and ranks 1st in average performance for five\nmulti-domain datasets. The dataset and codes are available at\nhttps://github.com/mswzeus/GVRT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Seonwoo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Nokyung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Siwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinkyu Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Cascades. (arXiv:2207.10342v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10342","description":"<p>Prompted models have demonstrated impressive few-shot learning abilities.\nRepeated interactions at test-time with a single model, or the composition of\nmultiple models together, further expands capabilities. These compositions are\nprobabilistic models, and may be expressed in the language of graphical models\nwith random variables whose values are complex data types such as strings.\nCases with control flow and dynamic structure require techniques from\nprobabilistic programming, which allow implementing disparate model structures\nand inference strategies in a unified language. We formalize several existing\ntechniques from this perspective, including scratchpads / chain of thought,\nverifiers, STaR, selection-inference, and tool use. We refer to the resulting\nprograms as language model cascades.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Winnie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewkowycz_A/0/1/0/all/0/1\">Aitor Lewkowycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieber_D/0/1/0/all/0/1\">David Bieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopes_R/0/1/0/all/0/1\">Raphael Gontijo Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saurous_R/0/1/0/all/0/1\">Rif A. Saurous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohl_dickstein_J/0/1/0/all/0/1\">Jascha Sohl-dickstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1\">Kevin Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeT: Code Generation with Generated Tests. (arXiv:2207.10397v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10397","description":"<p>Given a programming problem, pre-trained language models such as Codex have\ndemonstrated the ability to generate multiple different code solutions via\nsampling. However, selecting a correct or best solution from those samples\nstill remains a challenge. While an easy way to verify the correctness of a\ncode solution is through executing test cases, producing high-quality test\ncases is prohibitively expensive. In this paper, we explore the use of\npre-trained language models to automatically generate test cases, calling our\nmethod CodeT: Code generation with generated Tests. CodeT executes the code\nsolutions using the generated test cases, and then chooses the best solution\nbased on a dual execution agreement with both the generated test cases and\nother generated solutions. We evaluate CodeT on five different pre-trained\nmodels with both HumanEval and MBPP benchmarks. Extensive experimental results\ndemonstrate CodeT can achieve significant, consistent, and surprising\nimprovements over previous methods. For example, CodeT improves the pass@1 on\nHumanEval to 65.8%, an increase of absolute 18.8% on the code-davinci-002\nmodel, and an absolute 20+% improvement over previous state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fengji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1\">Daoguang Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NusaCrowd: A Call for Open and Reproducible NLP Research in Indonesian Languages. (arXiv:2207.10524v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10524","description":"<p>At the center of the underlying issues that halt Indonesian natural language\nprocessing (NLP) research advancement, we find data scarcity. Resources in\nIndonesian languages, especially the local ones, are extremely scarce and\nunderrepresented. Many Indonesian researchers do not publish their dataset.\nFurthermore, the few public datasets that we have are scattered across\ndifferent platforms, thus makes performing reproducible and data-centric\nresearch in Indonesian NLP even more arduous. Rising to this challenge, we\ninitiate the first Indonesian NLP crowdsourcing effort, NusaCrowd. NusaCrowd\nstrives to provide the largest datasheets aggregation with standardized data\nloading for NLP tasks in all Indonesian languages. By enabling open and\ncentralized access to Indonesian NLP resources, we hope NusaCrowd can tackle\nthe data scarcity problem hindering NLP progress in Indonesia and bring NLP\npractitioners to move towards collaboration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendra_R/0/1/0/all/0/1\">Rahmad Mahendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1\">Fajri Koto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeljadi_D/0/1/0/all/0/1\">David Moeljadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincentio_K/0/1/0/all/0/1\">Karissa Vincentio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romadhony_A/0/1/0/all/0/1\">Ade Romadhony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1\">Ayu Purwarianti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?. (arXiv:2207.10551v1 [cs.LG])","link":"http://arxiv.org/abs/2207.10551","description":"<p>There have been a lot of interest in the scaling properties of Transformer\nmodels. However, not much has been done on the front of investigating the\neffect of scaling properties of different inductive biases and model\narchitectures. Do model architectures scale differently? If so, how does\ninductive bias affect scaling behaviour? How does this influence upstream\n(pretraining) and downstream (transfer)? This paper conducts a systematic study\nof scaling behaviour of ten diverse model architectures such as Transformers,\nSwitch Transformers, Universal Transformers, Dynamic convolutions, Performers,\nand recently proposed MLP-Mixers. Via extensive experiments, we show that (1)\narchitecture is an indeed an important consideration when performing scaling\nand (2) the best performing model can fluctuate at different scales. We believe\nthat the findings outlined in this work has significant implications to how\nmodel architectures are currently evaluated in the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abnar_S/0/1/0/all/0/1\">Samira Abnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jinfeng Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Reinforcement Learning-based Offensive semantics Censorship System for Chatbots. (arXiv:2207.10569v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10569","description":"<p>The rapid development of artificial intelligence (AI) technology has enabled\nlarge-scale AI applications to land in the market and practice. However, while\nAI technology has brought many conveniences to people in the productization\nprocess, it has also exposed many security issues. Especially, attacks against\nonline learning vulnerabilities of chatbots occur frequently. Therefore, this\npaper proposes a semantics censorship chatbot system based on reinforcement\nlearning, which is mainly composed of two parts: the Offensive semantics\ncensorship model and the semantics purification model. Offensive semantics\nreview can combine the context of user input sentences to detect the rapid\nevolution of Offensive semantics and respond to Offensive semantics responses.\nThe semantics purification model For the case of chatting robot models, it has\nbeen contaminated by large numbers of offensive semantics, by strengthening the\noffensive reply learned by the learning algorithm, rather than rolling back to\nthe early versions. In addition, by integrating a once-through learning\napproach, the speed of semantics purification is accelerated while reducing the\nimpact on the quality of replies. The experimental results show that our\nproposed approach reduces the probability of the chat model generating\noffensive replies and that the integration of the few-shot learning algorithm\nimproves the training speed rapidly while effectively slowing down the decline\nin BLEU values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaokang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dezhi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zibin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+NoelCrespi/0/1/0/all/0/1\">NoelCrespi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Big Data and Education: using big data analytics in language learning. (arXiv:2207.10572v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10572","description":"<p>Working with big data using data mining tools is rapidly becoming a trend in\neducation industry. The combination of the current capacity to collect, store,\nmanage and process data in a timely manner, and data from online educational\nplatforms represents an unprecedented opportunity for educational institutes,\nlearners, educators, and researchers. In this position paper, we consider some\nbasic concepts as well as most popular tools, methods and techniques regarding\nEducational Data Mining and Learning Analytics, and discuss big data\napplications in language learning, in particular.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashrafimoghari_V/0/1/0/all/0/1\">Vahid Ashrafimoghari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Based Chatbot: An Approach of Utilizing On Customer Service Assistance. (arXiv:2207.10573v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10573","description":"<p>Providing the best customer experience is one of the primary concerns for the\nfirms that are based online. The advancement of machine learning is\nrevolutionising the company's attitude towards the client through improving the\nservice quality by implementing chatbot solutions, which gives the user instant\nand satisfactory answers to their enquiries. The acceptance of this technology\nis increasing with the new improvements and efficiency of the chatbot system.\nThis thesis paper will cover the concept of chatbot system for the company, as\na use case we took AK traders Ltd. It involves the research work on various\nchatbot technologies available and based on research, use them to develop a\nchatbot system for the company. This system will work based on the text as a\nconversational agent that can interact with humans by natural language. The\nmain objective project is to develop the chatbot solution that could comply\nwith complex questions and logical output answers in a well-defined approach.\nThe ultimate goal is to give high-quality results (answers) based on user input\n(question). For the successful implementation of this project, we have\nundertaken an in-depth analysis of the various machine learning techniques\navailable and followed well-structured implementation to figure out the best\nsolution for the company. The primary concern of this project includes natural\nlanguage processing (NLP), machine learning and the vector space model (VSM).\nThe outcome of the project shows the problem-solving technique for the\nimplementation of the chatbot system for the company at a reasonable quality\nlevel\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sulaiman_R/0/1/0/all/0/1\">Rejwan Bin Sulaiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Democratizing Ethical Assessment of Natural Language Generation Models. (arXiv:2207.10576v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10576","description":"<p>Natural language generation models are computer systems that generate\ncoherent language when prompted with a sequence of words as context. Despite\ntheir ubiquity and many beneficial applications, language generation models\nalso have the potential to inflict social harms by generating discriminatory\nlanguage, hateful speech, profane content, and other harmful material. Ethical\nassessment of these models is therefore critical. But it is also a challenging\ntask, requiring an expertise in several specialized domains, such as\ncomputational linguistics and social justice. While significant strides have\nbeen made by the research community in this domain, accessibility of such\nethical assessments to the wider population is limited due to the high entry\nbarriers. This article introduces a new tool to democratize and standardize\nethical assessment of natural language generation models: Tool for Ethical\nAssessment of Language generation models (TEAL), a component of Credo AI Lens,\nan open-source assessment framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rasekh_A/0/1/0/all/0/1\">Amin Rasekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenberg_I/0/1/0/all/0/1\">Ian Eisenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Natural Supervision for Language Representation Learning and Generation. (arXiv:2207.10617v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10617","description":"<p>Recent breakthroughs in Natural Language Processing (NLP) have been driven by\nlanguage models trained on a massive amount of plain text. While powerful,\nderiving supervision from textual resources is still an open question. For\nexample, language model pretraining often neglects the rich, freely-available\nstructures in textual data. In this thesis, we describe three lines of work\nthat seek to improve the training and evaluation of neural models using\nnaturally-occurring supervision.\n</p>\n<p>We first investigate self-supervised training losses to help enhance the\nperformance of pretrained language models for various NLP tasks. Specifically,\nwe alter the sentence prediction loss to make it better suited to other\npretraining losses and more challenging to solve. We design an intermediate\nfinetuning step that uses self-supervised training to promote models' ability\nin cross-task generalization.\n</p>\n<p>Then we describe methods to leverage the structures in Wikipedia and\nparaphrases. In particular, we propose training losses to exploit hyperlinks,\narticle structures, and article category graphs for entity-, discourse-,\nentailment-related knowledge. We propose a framework that uses paraphrase pairs\nto disentangle semantics and syntax in sentence representations. We extend the\nframework for a novel generation task that controls the syntax of output text\nwith a sentential exemplar.\n</p>\n<p>Lastly, we discuss our work on tailoring textual resources for establishing\nchallenging evaluation tasks. We introduce three datasets by defining novel\ntasks using various fan-contributed websites, including a long-form\ndata-to-text generation dataset, a screenplay summarization dataset, and a\nlong-form story generation dataset. These datasets have unique characteristics\noffering challenges to future work in their respective task settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingda Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Session-based Cyberbullying Detection in Social Media: A Survey. (arXiv:2207.10639v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10639","description":"<p>Cyberbullying is a pervasive problem in online social media, where a bully\nabuses a victim through a social media session. By investigating cyberbullying\nperpetrated through social media sessions, recent research has looked into\nmining patterns and features for modeling and understanding the two defining\ncharacteristics of cyberbullying: repetitive behavior and power imbalance. In\nthis survey paper, we define the Session-based Cyberbullying Detection\nframework that encapsulates the different steps and challenges of the problem.\nBased on this framework, we provide a comprehensive overview of session-based\ncyberbullying detection in social media, delving into existing efforts from a\ndata and methodological perspective. Our review leads us to propose\nevidence-based criteria for a set of best practices to create session-based\ncyberbullying datasets. In addition, we perform benchmark experiments comparing\nthe performance of state-of-the-art session-based cyberbullying detection\nmodels as well as large pre-trained language models across two different\ndatasets. Through our review, we also put forth a set of open challenges as\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_P/0/1/0/all/0/1\">Peiling Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Reveals Patterns of Diverse and Changing Sentiments Towards COVID-19 Vaccines Based on 11 Million Tweets. (arXiv:2207.10641v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10641","description":"<p>Over 12 billion doses of COVID-19 vaccines have been administered at the time\nof writing. However, public perceptions of vaccines have been complex. We\nanalyzed COVID-19 vaccine-related tweets to understand the evolving perceptions\nof COVID-19 vaccines. We finetuned a deep learning classifier using a\nstate-of-the-art model, XLNet, to detect each tweet's sentiment automatically.\nWe employed validated methods to extract the users' race or ethnicity, gender,\nage, and geographical locations from user profiles. Incorporating multiple data\nsources, we assessed the sentiment patterns among subpopulations and juxtaposed\nthem against vaccine uptake data to unravel their interactive patterns.\n11,211,672 COVID-19 vaccine-related tweets corresponding to 2,203,681 users\nover two years were analyzed. The finetuned model for sentiment classification\nyielded an accuracy of 0.92 on testing set. Users from various demographic\ngroups demonstrated distinct patterns in sentiments towards COVID-19 vaccines.\nUser sentiments became more positive over time, upon which we observed\nsubsequent upswing in the population-level vaccine uptake. Surrounding dates\nwhere positive sentiments crest, we detected encouraging news or events\nregarding vaccine development and distribution. Positive sentiments in\npregnancy-related tweets demonstrated a delayed pattern compared with trends in\ngeneral population, with postponed vaccine uptake trends. Distinctive patterns\nacross subpopulations suggest the need of tailored strategies. Global news and\nevents profoundly involved in shaping users' thoughts on social media.\nPopulations with additional concerns, such as pregnancy, demonstrated more\nsubstantial hesitancy since lack of timely recommendations. Feature analysis\nrevealed hesitancies of various subpopulations stemmed from clinical trial\nlogics, risks and complications, and urgency of scientific evidence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutch_M/0/1/0/all/0/1\">Meghan R. Hutch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_A/0/1/0/all/0/1\">Adrienne S. Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otero_S/0/1/0/all/0/1\">Sebastian Otero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mithal_L/0/1/0/all/0/1\">Leena B. Mithal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_E/0/1/0/all/0/1\">Emily S. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidech_A/0/1/0/all/0/1\">Andrew Naidech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STOP: A dataset for Spoken Task Oriented Semantic Parsing. (arXiv:2207.10643v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10643","description":"<p>End-to-end spoken language understanding (SLU) predicts intent directly from\naudio using a single model. It promises to improve the performance of assistant\nsystems by leveraging acoustic information lost in the intermediate textual\nrepresentation and preventing cascading errors from Automatic Speech\nRecognition (ASR). Further, having one unified model has efficiency advantages\nwhen deploying assistant systems on-device. However, the limited number of\npublic audio datasets with semantic parse labels hinders the research progress\nin this area. In this paper, we release the Spoken Task-Oriented semantic\nParsing (STOP) dataset, the largest and most complex SLU dataset to be publicly\navailable. Additionally, we define low-resource splits to establish a benchmark\nfor improving SLU when limited labeled data is available. Furthermore, in\naddition to the human-recorded audio, we are releasing a TTS-generated version\nto benchmark the performance for low-resource domain adaptation of end-to-end\nSLU systems. Initial experimentation show end-to-end SLU models performing\nslightly worse than their cascaded counterparts, which we hope encourages\nfuture work in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomasello_P/0/1/0/all/0/1\">Paden Tomasello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_P/0/1/0/all/0/1\">Po-Chun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazar_D/0/1/0/all/0/1\">Daniel Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Adithya Sagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elkahky_A/0/1/0/all/0/1\">Ali Elkahky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordechay_Y/0/1/0/all/0/1\">Yossef Mordechay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Algayres_R/0/1/0/all/0/1\">Robin Algayres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Ahn Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed Task Net for the Single-Corpus and Cross-Corpus Speech Emotion Recognition. (arXiv:2207.10644v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10644","description":"<p>Speech Emotion Recognition (SER) has become a growing focus of research in\nhuman-computer interaction. An essential challenge in SER is to extract common\nattributes from different speakers or languages, especially when a specific\nsource corpus has to be trained to recognize the unknown data coming from\nanother speech corpus. To address this challenge, a Capsule Network (CapsNet)\nand Transfer Learning based Mixed Task Net (CTLMTNet) are proposed to deal with\nboth the singlecorpus and cross-corpus SER tasks simultaneously in this paper.\nFor the single-corpus task, the combination of Convolution-Pooling and\nAttention CapsNet module CPAC) is designed by embedding the self-attention\nmechanism to the CapsNet, guiding the module to focus on the important features\nthat can be fed into different capsules. The extracted high-level features by\nCPAC provide sufficient discriminative ability. Furthermore, to handle the\ncross-corpus task, CTL-MTNet employs a Corpus Adaptation Adversarial Module\n(CAAM) by combining CPAC with Margin Disparity Discrepancy (MDD), which can\nlearn the domain-invariant emotion representations through extracting the\nstrong emotion commonness. Experiments including ablation studies and\nvisualizations on both singleand cross-corpus tasks using four well-known SER\ndatasets in different languages are conducted for performance evaluation and\ncomparison. The results indicate that in both tasks the CTL-MTNet showed better\nperformance in all cases compared to a number of state-of-the-art methods. The\nsource code and the supplementary materials are available at:\nhttps://github.com/MLDMXM2017/CTLMTNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin-Cheng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jia-Xin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan-Ze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chang-Li Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kun-Hong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wide & Deep Learning for Judging Student Performance in Online One-on-one Math Classes. (arXiv:2207.10645v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10645","description":"<p>In this paper, we investigate the opportunities of automating the judgment\nprocess in online one-on-one math classes. We build a Wide &amp; Deep framework to\nlearn fine-grained predictive representations from a limited amount of noisy\nclassroom conversation data that perform better student judgments. We conducted\nexperiments on the task of predicting students' levels of mastery of example\nquestions and the results demonstrate the superiority and availability of our\nmodel in terms of various evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weiqi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A No-Code Low-Code Paradigm for Authoring Business Automations Using Natural Language. (arXiv:2207.10648v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10648","description":"<p>Most business process automation is still developed using traditional\nautomation technologies such as workflow engines. These systems provide domain\nspecific languages that require both business knowledge and programming skills\nto effectively use. As such, business users often lack adequate programming\nskills to fully leverage these code oriented environments. We propose a\nparadigm for the construction of business automations using natural language.\nThe approach applies a large language model to translate business rules and\nautomations described in natural language, into a domain specific language\ninterpretable by a business rule engine. We compare the performance of various\nlanguage model configurations, across various target domains, and explore the\nuse of constrained decoding to ensure syntactically correct generation of\noutput.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Desmond_M/0/1/0/all/0/1\">Michael Desmond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duesterwald_E/0/1/0/all/0/1\">Evelyn Duesterwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isahagian_V/0/1/0/all/0/1\">Vatche Isahagian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthusamy_V/0/1/0/all/0/1\">Vinod Muthusamy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Disinformation Detection for Digital Advertising. (arXiv:2207.10649v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10649","description":"<p>In today's world, the presence of online disinformation and propaganda is\nmore widespread than ever. Independent publishers are funded mostly via digital\nadvertising, which is unfortunately also the case for those publishing\ndisinformation content. The question of how to remove such publishers from\nadvertising inventory has long been ignored, despite the negative impact on the\nopen internet. In this work, we make the first step towards quickly detecting\nand red-flagging websites that potentially manipulate the public with\ndisinformation. We build a machine learning model based on multilingual text\nembeddings that first determines whether the page mentions a topic of interest,\nthen estimates the likelihood of the content being malicious, creating a\nshortlist of publishers that will be reviewed by human experts. Our system\nempowers internal teams to proactively, rather than defensively, blacklist\nunsafe content, thus protecting the reputation of the advertisement provider.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trstanova_Z/0/1/0/all/0/1\">Zofia Trstanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manouzi_N/0/1/0/all/0/1\">Nadir El Manouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maryline Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunha_A/0/1/0/all/0/1\">Andre L. V. da Cunha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_S/0/1/0/all/0/1\">Sergei Ivanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"O-Dang! The Ontology of Dangerous Speech Messages. (arXiv:2207.10652v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10652","description":"<p>Inside the NLP community there is a considerable amount of language resources\ncreated, annotated and released every day with the aim of studying specific\nlinguistic phenomena. Despite a variety of attempts in order to organize such\nresources has been carried on, a lack of systematic methods and of possible\ninteroperability between resources are still present. Furthermore, when storing\nlinguistic information, still nowadays, the most common practice is the concept\nof \"gold standard\", which is in contrast with recent trends in NLP that aim at\nstressing the importance of different subjectivities and points of view when\ntraining machine learning and deep learning methods. In this paper we present\nO-Dang!: The Ontology of Dangerous Speech Messages, a systematic and\ninteroperable Knowledge Graph (KG) for the collection of linguistic annotated\ndata. O-Dang! is designed to gather and organize Italian datasets into a\nstructured KG, according to the principles shared within the Linguistic Linked\nOpen Data community. The ontology has also been designed to account for a\nperspectivist approach, since it provides a model for encoding both gold\nstandard and single-annotator labels in the KG. The paper is structured as\nfollows. In Section 1 the motivations of our work are outlined. Section 2\ndescribes the O-Dang! Ontology, that provides a common semantic model for the\nintegration of datasets in the KG. The Ontology Population stage with\ninformation about corpora, users, and annotations is presented in Section 3.\nFinally, in Section 4 an analysis of offensiveness across corpora is provided\nas a first case study for the resource.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stranisci_M/0/1/0/all/0/1\">Marco A. Stranisci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frenda_S/0/1/0/all/0/1\">Simona Frenda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_M/0/1/0/all/0/1\">Mirko Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araque_O/0/1/0/all/0/1\">Oscar Araque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cignarella_A/0/1/0/all/0/1\">Alessandra T. Cignarella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_V/0/1/0/all/0/1\">Valerio Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patti_V/0/1/0/all/0/1\">Viviana Patti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosco_C/0/1/0/all/0/1\">Cristina Bosco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion detection of social data: APIs comparative study. (arXiv:2207.10654v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10654","description":"<p>The development of emotion detection technology has emerged as a highly\nvaluable possibility in the corporate sector due to the nearly limitless uses\nof this new discipline, particularly with the unceasing propagation of social\ndata. In recent years, the electronic marketplace has witnessed the\nestablishment of a large number of start-up businesses with an almost sole\nfocus on building new commercial and open-source tools and APIs for emotion\ndetection and recognition. Yet, these tools and APIs must be continuously\nreviewed and evaluated, and their performances should be reported and\ndiscussed. There is a lack of research to empirically compare current emotion\ndetection technologies in terms of the results obtained from each model using\nthe same textual dataset. Also, there is a lack of comparative studies that\napply benchmark comparison to social data. This study compares eight\ntechnologies; IBM Watson NLU, ParallelDots, Symanto-Ekman, Crystalfeel, Text to\nEmotion, Senpy, Textprobe, and NLP Cloud. The comparison was undertaken using\ntwo different datasets. The emotions from the chosen datasets were then derived\nusing the incorporated APIs. The performance of these APIs was assessed using\nthe aggregated scores that they delivered as well as the theoretically proven\nevaluation metrics such as the micro-average of accuracy, classification error,\nprecision, recall, and f1-score. Lastly, the assessment of these APIs\nincorporating the evaluation measures is reported and discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abu_Salih_B/0/1/0/all/0/1\">Bilal Abu-Salih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhabashneh_M/0/1/0/all/0/1\">Mohammad Alhabashneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dengya Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awajan_A/0/1/0/all/0/1\">Albara Awajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshamaileh_Y/0/1/0/all/0/1\">Yazan Alshamaileh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Shboul_B/0/1/0/all/0/1\">Bashar Al-Shboul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshraideh_M/0/1/0/all/0/1\">Mohammad Alshraideh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Triple Extraction with Generative Transformer. (arXiv:2009.06207v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.06207","description":"<p>Triple extraction is an essential task in information extraction for natural\nlanguage processing and knowledge graph construction. In this paper, we revisit\nthe end-to-end triple extraction task for sequence generation. Since generative\ntriple extraction may struggle to capture long-term dependencies and generate\nunfaithful triples, we introduce a novel model, contrastive triple extraction\nwith a generative transformer. Specifically, we introduce a single shared\ntransformer module for encoder-decoder-based generation. To generate faithful\nresults, we propose a novel triplet contrastive training object. Moreover, we\nintroduce two mechanisms to further improve model performance (i.e., batch-wise\ndynamic attention-masking and triple-wise calibration). Experimental results on\nthree datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves\nbetter performance than that of baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion analysis and detection during COVID-19. (arXiv:2107.11020v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.11020","description":"<p>Crises such as natural disasters, global pandemics, and social unrest\ncontinuously threaten our world and emotionally affect millions of people\nworldwide in distinct ways. Understanding emotions that people express during\nlarge-scale crises helps inform policy makers and first responders about the\nemotional states of the population as well as provide emotional support to\nthose who need such support. We present CovidEmo, ~3K English tweets labeled\nwith emotions and temporally distributed across 18 months. Our analyses reveal\nthe emotional toll caused by COVID-19, and changes of the social narrative and\nassociated emotions over time. Motivated by the time-sensitive nature of crises\nand the cost of large-scale annotation efforts, we examine how well large\npre-trained language models generalize across domains and timeline in the task\nof perceived emotion prediction in the context of COVID-19. Our analyses\nsuggest that cross-domain information transfers occur, yet there are still\nsignificant gaps. We propose semi-supervised learning as a way to bridge this\ngap, obtaining significantly better performance using unlabeled data from the\ntarget domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sosea_T/0/1/0/all/0/1\">Tiberiu Sosea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1\">Chau Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekle_A/0/1/0/all/0/1\">Alexander Tekle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Learning the Transformer Kernel. (arXiv:2110.08323v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08323","description":"<p>In this work we introduce KERNELIZED TRANSFORMER, a generic, scalable, data\ndriven framework for learning the kernel function in Transformers. Our\nframework approximates the Transformer kernel as a dot product between spectral\nfeature maps and learns the kernel by learning the spectral distribution. This\nnot only helps in learning a generic kernel end-to-end, but also reduces the\ntime and space complexity of Transformers from quadratic to linear. We show\nthat KERNELIZED TRANSFORMERS achieve performance comparable to existing\nefficient Transformer architectures, both in terms of accuracy as well as\ncomputational efficiency. Our study also demonstrates that the choice of the\nkernel has a substantial impact on performance, and kernel learning variants\nare competitive alternatives to fixed kernel Transformers, both in long as well\nas short sequence tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Sankalan Pal Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solomou_A/0/1/0/all/0/1\">Adamos Solomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Avinava Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Explanation of In-context Learning as Implicit Bayesian Inference. (arXiv:2111.02080v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.02080","description":"<p>Large language models (LMs) such as GPT-3 have the surprising ability to do\nin-context learning, where the model learns to do a downstream task simply by\nconditioning on a prompt consisting of input-output examples. The LM learns\nfrom these examples without being explicitly pretrained to learn. Thus, it is\nunclear what enables in-context learning. In this paper, we study how\nin-context learning can emerge when pretraining documents have long-range\ncoherence. Here, the LM must infer a latent document-level concept to generate\ncoherent next tokens during pretraining. At test time, in-context learning\noccurs when the LM also infers a shared latent concept between examples in a\nprompt. We prove when this occurs despite a distribution mismatch between\nprompts and pretraining data in a setting where the pretraining distribution is\na mixture of HMMs. In contrast to messy large-scale datasets used to train LMs\ncapable of in-context learning, we generate a small-scale synthetic dataset\n(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond\nthe theory, experiments on GINC exhibit large-scale real-world phenomena\nincluding improved in-context performance with model scaling (despite the same\npretraining loss), sensitivity to example order, and instances where zero-shot\nis better than few-shot in-context learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Aditi Raghunathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP2TV: Align, Match and Distill for Video-Text Retrieval. (arXiv:2111.05610v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.05610","description":"<p>Modern video-text retrieval frameworks basically consist of three parts:\nvideo encoder, text encoder and the similarity head. With the success on both\nvisual and textual representation learning, transformer based encoders and\nfusion methods have also been adopted in the field of video-text retrieval. In\nthis report, we present CLIP2TV, aiming at exploring where the critical\nelements lie in transformer based methods. To achieve this, We first revisit\nsome recent works on multi-modal learning, then introduce some techniques into\nvideo-text retrieval, finally evaluate them through extensive experiments in\ndifferent configurations. Notably, CLIP2TV achieves 52.9@R1 on MSR-VTT dataset,\noutperforming the previous SOTA result by 4.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zijian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dedan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lili Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds. (arXiv:2112.08879v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08879","description":"<p>Most models tasked to ground referential utterances in 2D and 3D scenes learn\nto select the referred object from a pool of object proposals provided by a\npre-trained detector. This is limiting because an utterance may refer to visual\nentities at various levels of granularity, such as the chair, the leg of the\nchair, or the tip of the front leg of the chair, which may be missed by the\ndetector. We propose a language grounding model that attends on the referential\nutterance and on the object proposal pool computed from a pre-trained detector\nto decode referenced objects with a detection head, without selecting them from\nthe pool. In this way, it is helped by powerful pre-trained object detectors\nwithout being restricted by their misses. We call our model Bottom Up Top Down\nDEtection TRansformers (BUTD-DETR) because it uses both language guidance (top\ndown) and objectness guidance (bottom-up) to ground referential utterances in\nimages and point clouds. Moreover, BUTD-DETR casts object detection as\nreferential grounding and uses object labels as language prompts to be grounded\nin the visual scene, augmenting supervision for the referential grounding task\nin this way. The proposed model sets a new state-of-the-art across popular 3D\nlanguage grounding benchmarks with significant performance gains over previous\n3D approaches (12.6% on SR3D, 11.6% on NR3D and 6.3% on ScanRefer). When\napplied in 2D images, it performs on par with the previous state of the art. We\nablate the design choices of our model and quantify their contribution to\nperformance. Our code and checkpoints can be found at the project website\nhttps://butd-detr.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1\">Nikolaos Gkanatsios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mediratta_I/0/1/0/all/0/1\">Ishita Mediratta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Webly Supervised Concept Expansion for General Purpose Vision Models. (arXiv:2202.02317v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02317","description":"<p>General Purpose Vision (GPV) systems are models that are designed to solve a\nwide array of visual tasks without requiring architectural changes. Today, GPVs\nprimarily learn both skills and concepts from large fully supervised datasets.\nScaling GPVs to tens of thousands of concepts by acquiring data to learn each\nconcept for every skill quickly becomes prohibitive. This work presents an\neffective and inexpensive alternative: learn skills from supervised datasets,\nlearn concepts from web image search, and leverage a key characteristic of\nGPVs: the ability to transfer visual knowledge across skills. We use a dataset\nof 1M+ images spanning 10k+ visual concepts to demonstrate webly-supervised\nconcept expansion for two existing GPVs (GPV-1 and VL-T5) on 3 benchmarks: 5\nCOCO-based datasets (80 primary concepts), a newly curated series of 5 datasets\nbased on the OpenImages and VisualGenome repositories (~500 concepts), and the\nWeb-derived dataset (10k+ concepts). We also propose a new architecture, GPV-2\nthat supports a variety of tasks -- from vision tasks like classification and\nlocalization to vision+language tasks like QA and captioning, to more niche\nones like human-object interaction detection. GPV-2 benefits hugely from web\ndata and outperforms GPV-1 and VL-T5 across these benchmarks. Our data, code,\nand web demo are available at https://prior.allenai.org/projects/gpv2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Amita Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_C/0/1/0/all/0/1\">Christopher Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanmay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolve_E/0/1/0/all/0/1\">Eric Kolve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1\">Derek Hoiem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. (arXiv:2204.02874v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02874","description":"<p>We introduce an audiovisual method for long-range text-to-video retrieval.\nUnlike previous approaches designed for short video retrieval (e.g., 5-15\nseconds in duration), our approach aims to retrieve minute-long videos that\ncapture complex human actions. One challenge of standard video-only approaches\nis the large computational cost associated with processing hundreds of densely\nextracted frames from such long videos. To address this issue, we propose to\nreplace parts of the video with compact audio cues that succinctly summarize\ndynamic audio events and are cheap to process. Our method, named ECLIPSE\n(Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an\naudiovisual video setting, by adding a unified audiovisual transformer block\nthat captures complementary cues from the video and audio streams. In addition\nto being 2.92x faster and 2.34x memory-efficient than long-range video-only\napproaches, our method also achieves better text-to-video retrieval accuracy on\nseveral diverse long-range video datasets such as ActivityNet, QVHighlights,\nYouCook2, DiDeMo and Charades.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yan-Bo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09817","description":"<p>Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision--language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1\">Stephanie Hyland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1\">Maria Wetscherek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Split for Automatic Bias Detection. (arXiv:2204.13749v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.13749","description":"<p>Classifiers are biased when trained on biased datasets. As a remedy, we\npropose Learning to Split (ls), an algorithm for automatic bias detection.\nGiven a dataset with input-label pairs, ls learns to split this dataset so that\npredictors trained on the training split cannot generalize to the testing\nsplit. This performance gap suggests that the testing split is\nunder-represented in the dataset, which is a signal of potential bias.\nIdentifying non-generalizable splits is challenging since we have no\nannotations about the bias. In this work, we show that the prediction\ncorrectness of each example in the testing split can be used as a source of\nweak supervision: generalization performance will drop if we move examples that\nare predicted correctly away from the testing split, leaving only those that\nare mis-predicted. ls is task-agnostic and can be applied to any supervised\nlearning problem, ranging from natural language understanding and image\nclassification to molecular property prediction. Empirical results show that ls\nis able to generate astonishingly challenging splits that correlate with\nhuman-identified biases. Moreover, we demonstrate that combining robust\nlearning algorithms (such as group DRO) with splits identified by ls enables\nautomatic de-biasing. Compared to previous state-of-the-art, we substantially\nimprove the worst-group performance (23.4% on average) when the source of\nbiases is unknown during training and validation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nominal Metaphor Generation with Multitask Learning. (arXiv:2206.05195v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.05195","description":"<p>Metaphor generation is a challenging task which can impact many downstream\ntasks such as improving user satisfaction with dialogue systems and story\ngeneration. This paper tackles the problem of Chinese nominal metaphor\ngeneration by introducing a multitask metaphor generation framework with\nself-training and metaphor identification mechanisms. Self-training addresses\nthe data scarcity issue of metaphor datasets. That is, instead of solely\nrelying on labelled metaphor datasets which are usually small in size,\nself-training helps identify potential metaphors from a large-scale unlabelled\ncorpus for metaphor generation. The metaphor weighting mechanism enables our\nmodel to focus on the metaphor-related parts of the input (e.g., the comparison\nof the metaphor and comparator) during model learning and thus improves the\nmetaphoricity of the generated metaphors. Our model is trained on an annotated\ncorpus consisting of 6.3k sentences that contain diverse metaphorical\nexpressions. Experimental results show that our model is able to generate\nmetaphors with better readability and creativity compared to the baseline\nmodels, even in the situation where training data is insufficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geurin_F/0/1/0/all/0/1\">Frank Geurin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrapping a User-Centered Task-Oriented Dialogue System. (arXiv:2207.05223v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.05223","description":"<p>We present TacoBot, a task-oriented dialogue system built for the inaugural\nAlexa Prize TaskBot Challenge, which assists users in completing multi-step\ncooking and home improvement tasks. TacoBot is designed with a user-centered\nprinciple and aspires to deliver a collaborative and accessible dialogue\nexperience. Towards that end, it is equipped with accurate language\nunderstanding, flexible dialogue management, and engaging response generation.\nFurthermore, TacoBot is backed by a strong search engine and an automated\nend-to-end test suite. In bootstrapping the development of TacoBot, we explore\na series of data augmentation strategies to train advanced neural language\nprocessing models and continuously improve the dialogue experience with\ncollected real conversations. At the end of the semifinals, TacoBot achieved an\naverage rating of 3.55/5.0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_A/0/1/0/all/0/1\">Ashley Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1\">Lingbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1\">Samuel Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReFactorGNNs: Revisiting Factorisation-based Models from a Message-Passing Perspective. (arXiv:2207.09980v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.09980","description":"<p>Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring\nsuccess for Knowledge Graph Completion (KGC) tasks, often outperforming Graph\nNeural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node\nfeatures and to generalise to unseen nodes in inductive settings. Our work\nbridges the gap between FMs and GNNs by proposing ReFactorGNNs. This new\narchitecture draws upon both modelling paradigms, which previously were largely\nthought of as disjoint. Concretely, using a message-passing formalism, we show\nhow FMs can be cast as GNNs by reformulating the gradient descent procedure as\nmessage-passing operations, which forms the basis of our ReFactorGNNs. Across a\nmultitude of well-established KGC benchmarks, our ReFactorGNNs achieve\ncomparable transductive performance to FMs, and state-of-the-art inductive\nperformance while using an order of magnitude fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Pushkar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franceschi_L/0/1/0/all/0/1\">Luca Franceschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Model Compression for Resource-Constrained Mobile Robots. (arXiv:2207.10082v1 [cs.LG])","link":"http://arxiv.org/abs/2207.10082","description":"<p>The number of mobile robots with constrained computing resources that need to\nexecute complex machine learning models has been increasing during the past\ndecade. Commonly, these robots rely on edge infrastructure accessible over\nwireless communication to execute heavy computational complex tasks. However,\nthe edge might become unavailable and, consequently, oblige the execution of\nthe tasks on the robot. This work focuses on making it possible to execute the\ntasks on the robots by reducing the complexity and the total number of\nparameters of pre-trained computer vision models. This is achieved by using\nmodel compression techniques such as Pruning and Knowledge Distillation. These\ncompression techniques have strong theoretical and practical foundations, but\ntheir combined usage has not been widely explored in the literature. Therefore,\nthis work especially focuses on investigating the effects of combining these\ntwo compression techniques. The results of this work reveal that up to 90% of\nthe total number of parameters of a computer vision model can be removed\nwithout any considerable reduction in the model's accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souroulla_T/0/1/0/all/0/1\">Timotheos Souroulla</a> (Ericsson Research AI), <a href=\"http://arxiv.org/find/cs/1/au:+Hata_A/0/1/0/all/0/1\">Alberto Hata</a> (Ericsson Research AI), <a href=\"http://arxiv.org/find/cs/1/au:+Terra_A/0/1/0/all/0/1\">Ahmad Terra</a> (Ericsson Research AI), <a href=\"http://arxiv.org/find/cs/1/au:+Ozkahraman_O/0/1/0/all/0/1\">&#xd6;zer &#xd6;zkahraman</a> (KTH, Royal Institute of Technology), <a href=\"http://arxiv.org/find/cs/1/au:+Inam_R/0/1/0/all/0/1\">Rafia Inam</a> (Ericsson Research AI)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"World Robot Challenge 2020 -- Partner Robot: A Data-Driven Approach for Room Tidying with Mobile Manipulator. (arXiv:2207.10106v1 [cs.RO])","link":"http://arxiv.org/abs/2207.10106","description":"<p>Tidying up a household environment using a mobile manipulator poses various\nchallenges in robotics, such as adaptation to large real-world environmental\nvariations, and safe and robust deployment in the presence of humans.The\nPartner Robot Challenge in World Robot Challenge (WRC) 2020, a global\ncompetition held in September 2021, benchmarked tidying tasks in the real home\nenvironments, and importantly, tested for full system performances.For this\nchallenge, we developed an entire household service robot system, which\nleverages a data-driven approach to adapt to numerous edge cases that occur\nduring the execution, instead of classical manual pre-programmed solutions.In\nthis paper, we describe the core ingredients of the proposed robot system,\nincluding visual recognition, object manipulation, and motion planning. Our\nrobot system won the second prize, verifying the effectiveness and potential of\ndata-driven robot systems for mobile manipulation in home environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsushima_T/0/1/0/all/0/1\">Tatsuya Matsushima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noguchi_Y/0/1/0/all/0/1\">Yuki Noguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arima_J/0/1/0/all/0/1\">Jumpei Arima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoki_T/0/1/0/all/0/1\">Toshiki Aoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okita_Y/0/1/0/all/0/1\">Yuki Okita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikeda_Y/0/1/0/all/0/1\">Yuya Ikeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishimoto_K/0/1/0/all/0/1\">Koki Ishimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_S/0/1/0/all/0/1\">Shohei Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_Y/0/1/0/all/0/1\">Yuki Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seto_S/0/1/0/all/0/1\">Shoichi Seto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwasawa_Y/0/1/0/all/0/1\">Yusuke Iwasawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis. (arXiv:2207.10120v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10120","description":"<p>Generative models for audio-conditioned dance motion synthesis map music\nfeatures to dance movements. Models are trained to associate motion patterns to\naudio patterns, usually without an explicit knowledge of the human body. This\napproach relies on a few assumptions: strong music-dance correlation,\ncontrolled motion data and relatively simple poses and movements. These\ncharacteristics are found in all existing datasets for dance motion synthesis,\nand indeed recent methods can achieve good results.We introduce a new dataset\naiming to challenge these common assumptions, compiling a set of dynamic dance\nsequences displaying complex human poses. We focus on breakdancing which\nfeatures acrobatic moves and tangled postures. We source our data from the Red\nBull BC One competition videos. Estimating human keypoints from these videos is\ndifficult due to the complexity of the dance, as well as the multiple moving\ncameras recording setup. We adopt a hybrid labelling pipeline leveraging deep\nestimation models as well as manual annotations to obtain good quality keypoint\nsequences at a reduced cost. Our efforts produced the BRACE dataset, which\ncontains over 3 hours and 30 minutes of densely annotated poses. We test\nstate-of-the-art methods on BRACE, showing their limitations when evaluated on\ncomplex sequences. Our dataset can readily foster advance in dance motion\nsynthesis. With intricate poses and swift movements, models are forced to go\nbeyond learning a mapping between modalities and reason more effectively about\nbody structure and movements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moltisanti_D/0/1/0/all/0/1\">Davide Moltisanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jinyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance. (arXiv:2207.10123v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10123","description":"<p>We study the challenging problem of recovering detailed motion from a single\nmotion-blurred image. Existing solutions to this problem estimate a single\nimage sequence without considering the motion ambiguity for each region.\nTherefore, the results tend to converge to the mean of the multi-modal\npossibilities. In this paper, we explicitly account for such motion ambiguity,\nallowing us to generate multiple plausible solutions all in sharp detail. The\nkey idea is to introduce a motion guidance representation, which is a compact\nquantization of 2D optical flow with only four discrete motion directions.\nConditioned on the motion guidance, the blur decomposition is led to a\nspecific, unambiguous solution by using a novel two-stage decomposition\nnetwork. We propose a unified framework for blur decomposition, which supports\nvarious interfaces for generating our motion guidance, including human input,\nmotion information from adjacent video frames, and learning from a video\ndataset. Extensive experiments on synthesized datasets and real-world data show\nthat the proposed framework is qualitatively and quantitatively superior to\nprevious methods, and also offers the merit of producing physically plausible\nand diverse solutions. Code is available at\nhttps://github.com/zzh-tech/Animation-from-Blur.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhihang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Imari Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Discriminant deterministic Uncertainty. (arXiv:2207.10130v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10130","description":"<p>Predictive uncertainty estimation is essential for deploying Deep Neural\nNetworks in real-world autonomous systems. However, most successful approaches\nare computationally intensive. In this work, we attempt to address these\nchallenges in the context of autonomous driving perception tasks. Recently\nproposed Deterministic Uncertainty Methods (DUM) can only partially meet such\nrequirements as their scalability to complex computer vision tasks is not\nobvious. In this work we advance a scalable and effective DUM for\nhigh-resolution semantic segmentation, that relaxes the Lipschitz constraint\ntypically hindering practicality of such architectures. We learn a discriminant\nlatent space by leveraging a distinction maximization layer over an\narbitrarily-sized set of trainable prototypes. Our approach achieves\ncompetitive results over Deep Ensembles, the state-of-the-art for uncertainty\nprediction, on image classification, segmentation and monocular depth\nestimation tasks. Our code is available at https://github.com/ENSTA-U2IS/LDU\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuanlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1\">Andrei Bursuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1\">Emanuel Aldea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubuisson_S/0/1/0/all/0/1\">Severine Dubuisson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1\">David Filliat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Variational Autoencoder Learning via Online Cooperative Memorization. (arXiv:2207.10131v1 [cs.LG])","link":"http://arxiv.org/abs/2207.10131","description":"<p>Due to their inference, data representation and reconstruction properties,\nVariational Autoencoders (VAE) have been successfully used in continual\nlearning classification tasks. However, their ability to generate images with\nspecifications corresponding to the classes and databases learned during\nContinual Learning (CL) is not well understood and catastrophic forgetting\nremains a significant challenge. In this paper, we firstly analyze the\nforgetting behaviour of VAEs by developing a new theoretical framework that\nformulates CL as a dynamic optimal transport problem. This framework proves\napproximate bounds to the data likelihood without requiring the task\ninformation and explains how the prior knowledge is lost during the training\nprocess. We then propose a novel memory buffering approach, namely the Online\nCooperative Memorization (OCM) framework, which consists of a Short-Term Memory\n(STM) that continually stores recent samples to provide future information for\nthe model, and a Long-Term Memory (LTM) aiming to preserve a wide diversity of\nsamples. The proposed OCM transfers certain samples from STM to LTM according\nto the information diversity selection criterion without requiring any\nsupervised signals. The OCM framework is then combined with a dynamic VAE\nexpansion mixture network for further enhancing its performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generalized & Robust Framework For Timestamp Supervision in Temporal Action Segmentation. (arXiv:2207.10137v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10137","description":"<p>In temporal action segmentation, Timestamp supervision requires only a\nhandful of labelled frames per video sequence. For unlabelled frames, previous\nworks rely on assigning hard labels, and performance rapidly collapses under\nsubtle violations of the annotation assumptions. We propose a novel\nExpectation-Maximization (EM) based approach that leverages the label\nuncertainty of unlabelled frames and is robust enough to accommodate possible\nannotation errors. With accurate timestamp annotations, our proposed method\nproduces SOTA results and even exceeds the fully-supervised setup in several\nmetrics and datasets. When applied to timestamp annotations with missing action\nsegments, our method presents stable performance. To further test our\nformulation's robustness, we introduce the new challenging annotation setup of\nSkip-tag supervision. This setup relaxes constraints and requires annotations\nof any fixed number of random frames in a video, making it more flexible than\nTimestamp supervision while remaining competitive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_R/0/1/0/all/0/1\">Rahul Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhania_D/0/1/0/all/0/1\">Dipika Singhania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiery_A/0/1/0/all/0/1\">Alexandre Thiery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AudioScopeV2: Audio-Visual Attention Architectures for Calibrated Open-Domain On-Screen Sound Separation. (arXiv:2207.10141v1 [cs.SD])","link":"http://arxiv.org/abs/2207.10141","description":"<p>We introduce AudioScopeV2, a state-of-the-art universal audio-visual\non-screen sound separation system which is capable of learning to separate\nsounds and associate them with on-screen objects by looking at in-the-wild\nvideos. We identify several limitations of previous work on audio-visual\non-screen sound separation, including the coarse resolution of spatio-temporal\nattention, poor convergence of the audio separation model, limited variety in\ntraining and evaluation data, and failure to account for the trade off between\npreservation of on-screen sounds and suppression of off-screen sounds. We\nprovide solutions to all of these issues. Our proposed cross-modal and\nself-attention network architectures capture audio-visual dependencies at a\nfiner resolution over time, and we also propose efficient separable variants\nthat are capable of scaling to longer videos without sacrificing much\nperformance. We also find that pre-training the separation model only on audio\ngreatly improves results. For training and evaluation, we collected new human\nannotations of onscreen sounds from a large database of in-the-wild videos\n(YFCC100M). This new dataset is more diverse and challenging. Finally, we\npropose a calibration procedure that allows exact tuning of on-screen\nreconstruction versus off-screen suppression, which greatly simplifies\ncomparing performance between models with different operating points. Overall,\nour experimental results show marked improvements in on-screen separation\nperformance under much more general conditions than previous methods with\nminimal additional computational complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisdom_S/0/1/0/all/0/1\">Scott Wisdom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershey_J/0/1/0/all/0/1\">John R. Hershey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling Long-Tailed Category Distribution Under Domain Shifts. (arXiv:2207.10150v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10150","description":"<p>Machine learning models fail to perform well on real-world applications when\n1) the category distribution P(Y) of the training dataset suffers from\nlong-tailed distribution and 2) the test data is drawn from different\nconditional distributions P(X|Y). Existing approaches cannot handle the\nscenario where both issues exist, which however is common for real-world\napplications. In this study, we took a step forward and looked into the problem\nof long-tailed classification under domain shifts. We designed three novel core\nfunctional blocks including Distribution Calibrated Classification Loss,\nVisual-Semantic Mapping and Semantic-Similarity Guided Augmentation.\nFurthermore, we adopted a meta-learning framework which integrates these three\nblocks to improve domain generalization on unseen target domains. Two new\ndatasets were proposed for this problem, named AWA2-LTS and ImageNet-LTS. We\nevaluated our method on the two datasets and extensive experimental results\ndemonstrate that our proposed method can achieve superior performance over\nstate-of-the-art long-tailed/domain generalization approaches and the\ncombinations. Source codes and datasets can be found at our project page\nhttps://xiaogu.site/LTDS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeju Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang-Zhong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of the Effect of Low-Overhead Lossy Image Compression on the Performance of Visual Crowd Counting for Smart City Applications. (arXiv:2207.10155v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10155","description":"<p>Images and video frames captured by cameras placed throughout smart cities\nare often transmitted over the network to a server to be processed by deep\nneural networks for various tasks. Transmission of raw images, i.e., without\nany form of compression, requires high bandwidth and can lead to congestion\nissues and delays in transmission. The use of lossy image compression\ntechniques can reduce the quality of the images, leading to accuracy\ndegradation. In this paper, we analyze the effect of applying low-overhead\nlossy image compression methods on the accuracy of visual crowd counting, and\nmeasure the trade-off between bandwidth reduction and the obtained accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakhtiarnia_A/0/1/0/all/0/1\">Arian Bakhtiarnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leporowski_B/0/1/0/all/0/1\">B&#x142;a&#x17c;ej Leporowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esterle_L/0/1/0/all/0/1\">Lukas Esterle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structural Causal 3D Reconstruction. (arXiv:2207.10156v1 [cs.LG])","link":"http://arxiv.org/abs/2207.10156","description":"<p>This paper considers the problem of unsupervised 3D object reconstruction\nfrom in-the-wild single-view images. Due to ambiguity and intrinsic\nill-posedness, this problem is inherently difficult to solve and therefore\nrequires strong regularization to achieve disentanglement of different latent\nfactors. Unlike existing works that introduce explicit regularizations into\nobjective functions, we look into a different space for implicit regularization\n-- the structure of latent space. Specifically, we restrict the structure of\nlatent space to capture a topological causal ordering of latent factors (i.e.,\nrepresenting causal dependency as a directed acyclic graph). We first show that\ndifferent causal orderings matter for 3D reconstruction, and then explore\nseveral approaches to find a task-dependent causal factor ordering. Our\nexperiments demonstrate that the latent space structure indeed serves as an\nimplicit regularization and introduces an inductive bias beneficial for\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1\">Liam Paull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Knowledge Tracing. (arXiv:2207.10157v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10157","description":"<p>Each year, thousands of people learn new visual categorization tasks --\nradiologists learn to recognize tumors, birdwatchers learn to distinguish\nsimilar species, and crowd workers learn how to annotate valuable data for\napplications like autonomous driving. As humans learn, their brain updates the\nvisual features it extracts and attend to, which ultimately informs their final\nclassification decisions. In this work, we propose a novel task of tracing the\nevolving classification behavior of human learners as they engage in\nchallenging visual classification tasks. We propose models that jointly extract\nthe visual features used by learners as well as predicting the classification\nfunctions they utilize. We collect three challenging new datasets from real\nhuman learners in order to evaluate the performance of different visual\nknowledge tracing methods. Our results show that our recurrent models are able\nto predict the classification behavior of human learners on three challenging\nmedical image and species identification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kondapaneni_N/0/1/0/all/0/1\">Neehar Kondapaneni</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a> (2) ((1) Caltech, (2) University of Edinburgh)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GOCA: Guided Online Cluster Assignment for Self-Supervised Video Representation Learning. (arXiv:2207.10158v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10158","description":"<p>Clustering is a ubiquitous tool in unsupervised learning. Most of the\nexisting self-supervised representation learning methods typically cluster\nsamples based on visually dominant features. While this works well for\nimage-based self-supervision, it often fails for videos, which require\nunderstanding motion rather than focusing on background. Using optical flow as\ncomplementary information to RGB can alleviate this problem. However, we\nobserve that a naive combination of the two views does not provide meaningful\ngains. In this paper, we propose a principled way to combine two views.\nSpecifically, we propose a novel clustering strategy where we use the initial\ncluster assignment of each view as prior to guide the final cluster assignment\nof the other view. This idea will enforce similar cluster structures for both\nviews, and the formed clusters will be semantically abstract and robust to\nnoisy inputs coming from each individual view. Additionally, we propose a novel\nregularization strategy to address the feature collapse problem, which is\ncommon in cluster-based self-supervised learning methods. Our extensive\nevaluation shows the effectiveness of our learned representations on downstream\ntasks, e.g., video retrieval and action recognition. Specifically, we\noutperform the state of the art by 7% on UCF and 4% on HMDB for video\nretrieval, and 5% on UCF and 6% on HMDB for video classification\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coskun_H/0/1/0/all/0/1\">Huseyin Coskun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareian_A/0/1/0/all/0/1\">Alireza Zareian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1\">Joshua L. Moore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Liver Segmentation using Turbolift Learning for CT and Cone-beam C-arm Perfusion Imaging. (arXiv:2207.10167v1 [eess.IV])","link":"http://arxiv.org/abs/2207.10167","description":"<p>Model-based reconstruction employing the time separation technique (TST) was\nfound to improve dynamic perfusion imaging of the liver using C-arm cone-beam\ncomputed tomography (CBCT). To apply TST using prior knowledge extracted from\nCT perfusion data, the liver should be accurately segmented from the CT scans.\nReconstructions of primary and model-based CBCT data need to be segmented for\nproper visualisation and interpretation of perfusion maps. This research\nproposes Turbolift learning, which trains a modified version of the multi-scale\nAttention UNet on different liver segmentation tasks serially, following the\norder of the trainings CT, CBCT, CBCT TST - making the previous trainings act\nas pre-training stages for the subsequent ones - addressing the problem of\nlimited number of datasets for training. For the final task of liver\nsegmentation from CBCT TST, the proposed method achieved an overall Dice scores\nof 0.874$\\pm$0.031 and 0.905$\\pm$0.007 in 6-fold and 4-fold cross-validation\nexperiments, respectively - securing statistically significant improvements\nover the model, which was trained only for that task. Experiments revealed that\nTurbolift not only improves the overall performance of the model but also makes\nit robust against artefacts originating from the embolisation materials and\ntruncation artefacts. Additionally, in-depth analyses confirmed the order of\nthe segmentation tasks. This paper shows the potential of segmenting the liver\nfrom CT, CBCT, and CBCT TST, learning from the available limited training data,\nwhich can possibly be used in the future for the visualisation and evaluation\nof the perfusion maps for the treatment evaluation of liver diseases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Haseljic_H/0/1/0/all/0/1\">Hana Haselji&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumick Chatterjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frysch_R/0/1/0/all/0/1\">Robert Frysch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kulvait_V/0/1/0/all/0/1\">Vojt&#x11b;ch Kulvait</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Semshchikov_V/0/1/0/all/0/1\">Vladimir Semshchikov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hensen_B/0/1/0/all/0/1\">Bennet Hensen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wacker_F/0/1/0/all/0/1\">Frank Wacker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brusch_I/0/1/0/all/0/1\">Inga Br&#xfc;sch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Werncke_T/0/1/0/all/0/1\">Thomas Werncke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Speck_O/0/1/0/all/0/1\">Oliver Speck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rose_G/0/1/0/all/0/1\">Georg Rose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pediatric Bone Age Assessment using Deep Learning Models. (arXiv:2207.10169v1 [eess.IV])","link":"http://arxiv.org/abs/2207.10169","description":"<p>Bone age assessment (BAA) is a standard method for determining the age\ndifference between skeletal and chronological age. Manual processes are\ncomplicated and necessitate the expertise of experts. This is where deep\nlearning comes into play. In this study, pre-trained models like VGG-16,\nInceptionV3, XceptionNet, and MobileNet are used to assess the bone age of the\ninput data, and their mean average errors are compared and evaluated to see\nwhich model predicts the best.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Raman_A/0/1/0/all/0/1\">Aravinda Raman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pathan_S/0/1/0/all/0/1\">Sameena Pathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_T/0/1/0/all/0/1\">Tanweer Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles. (arXiv:2207.10172v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10172","description":"<p>Video Anomaly Detection (VAD) is an important topic in computer vision.\nMotivated by the recent advances in self-supervised learning, this paper\naddresses VAD by solving an intuitive yet challenging pretext task, i.e.,\nspatio-temporal jigsaw puzzles, which is cast as a multi-label fine-grained\nclassification problem. Our method exhibits several advantages over existing\nworks: 1) the spatio-temporal jigsaw puzzles are decoupled in terms of spatial\nand temporal dimensions, responsible for capturing highly discriminative\nappearance and motion features, respectively; 2) full permutations are used to\nprovide abundant jigsaw puzzles covering various difficulty levels, allowing\nthe network to distinguish subtle spatio-temporal differences between normal\nand abnormal events; and 3) the pretext task is tackled in an end-to-end manner\nwithout relying on any pre-trained models. Our method outperforms\nstate-of-the-art counterparts on three public benchmarks. Especially on\nShanghaiTech Campus, the result is superior to reconstruction and\nprediction-based methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guodong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1\">Xiuguo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Recognition with Objectness, Attribute and Category Learning. (arXiv:2207.10174v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10174","description":"<p>Scene classification has established itself as a challenging research\nproblem. Compared to images of individual objects, scene images could be much\nmore semantically complex and abstract. Their difference mainly lies in the\nlevel of granularity of recognition. Yet, image recognition serves as a key\npillar for the good performance of scene recognition as the knowledge attained\nfrom object images can be used for accurate recognition of scenes. The existing\nscene recognition methods only take the category label of the scene into\nconsideration. However, we find that the contextual information that contains\ndetailed local descriptions are also beneficial in allowing the scene\nrecognition model to be more discriminative. In this paper, we aim to improve\nscene recognition using attribute and category label information encoded in\nobjects. Based on the complementarity of attribute and category labels, we\npropose a Multi-task Attribute-Scene Recognition (MASR) network which learns a\ncategory embedding and at the same time predicts scene attributes. Attribute\nacquisition and object annotation are tedious and time consuming tasks. We\ntackle the problem by proposing a partially supervised annotation strategy in\nwhich human intervention is significantly reduced. The strategy provides a much\nmore cost-effective solution to real world scenarios, and requires considerably\nless annotation efforts. Moreover, we re-weight the attribute predictions\nconsidering the level of importance indicated by the object detected scores.\nUsing the proposed method, we efficiently annotate attribute labels for four\nlarge-scale datasets, and systematically investigate how scene and attribute\nrecognition benefit from each other. The experimental results demonstrate that\nMASR learns a more discriminative representation and achieves competitive\nrecognition performance compared to the state-of-the-art methods\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainam_J/0/1/0/all/0/1\">Jean-Paul Ainam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Li-hui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenai Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable and Guided Face Synthesis for Unconstrained Face Recognition. (arXiv:2207.10180v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10180","description":"<p>Although significant advances have been made in face recognition (FR), FR in\nunconstrained environments remains challenging due to the domain gap between\nthe semi-constrained training datasets and unconstrained testing scenarios. To\naddress this problem, we propose a controllable face synthesis model (CFSM)\nthat can mimic the distribution of target datasets in a style latent space.\nCFSM learns a linear subspace with orthogonal bases in the style latent space\nwith precise control over the diversity and degree of synthesis. Furthermore,\nthe pre-trained synthesis model can be guided by the FR model, making the\nresulting images more beneficial for FR model training. Besides, target dataset\ndistributions are characterized by the learned orthogonal bases, which can be\nutilized to measure the distributional similarity among face datasets. Our\napproach yields significant performance gains on unconstrained benchmarks, such\nas IJB-B, IJB-C, TinyFace and IJB-S (+5.76% Rank1).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minchul Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flow-based Visual Quality Enhancer for Super-resolution Magnetic Resonance Spectroscopic Imaging. (arXiv:2207.10181v1 [eess.IV])","link":"http://arxiv.org/abs/2207.10181","description":"<p>Magnetic Resonance Spectroscopic Imaging (MRSI) is an essential tool for\nquantifying metabolites in the body, but the low spatial resolution limits its\nclinical applications. Deep learning-based super-resolution methods provided\npromising results for improving the spatial resolution of MRSI, but the\nsuper-resolved images are often blurry compared to the experimentally-acquired\nhigh-resolution images. Attempts have been made with the generative adversarial\nnetworks to improve the image visual quality. In this work, we consider another\ntype of generative model, the flow-based model, of which the training is more\nstable and interpretable compared to the adversarial networks. Specifically, we\npropose a flow-based enhancer network to improve the visual quality of\nsuper-resolution MRSI. Different from previous flow-based models, our enhancer\nnetwork incorporates anatomical information from additional image modalities\n(MRI) and uses a learnable base distribution. In addition, we impose a guide\nloss and a data-consistency loss to encourage the network to generate images\nwith high visual quality while maintaining high fidelity. Experiments on a\n1H-MRSI dataset acquired from 25 high-grade glioma patients indicate that our\nenhancer network outperforms the adversarial networks and the baseline\nflow-based methods. Our method also allows visual quality adjustment and\nuncertainty estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dong_S/0/1/0/all/0/1\">Siyuan Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hangel_G/0/1/0/all/0/1\">Gilbert Hangel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_E/0/1/0/all/0/1\">Eric Z. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_S/0/1/0/all/0/1\">Shanhui Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bogner_W/0/1/0/all/0/1\">Wolfgang Bogner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Widhalm_G/0/1/0/all/0/1\">Georg Widhalm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Onofrey_J/0/1/0/all/0/1\">John A. Onofrey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Graaf_R/0/1/0/all/0/1\">Robin de Graaf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2D GANs Meet Unsupervised Single-view 3D Reconstruction. (arXiv:2207.10183v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10183","description":"<p>Recent research has shown that controllable image generation based on\npre-trained GANs can benefit a wide range of computer vision tasks. However,\nless attention has been devoted to 3D vision tasks. In light of this, we\npropose a novel image-conditioned neural implicit field, which can leverage 2D\nsupervisions from GAN-generated multi-view images and perform the single-view\nreconstruction of generic objects. Firstly, a novel offline StyleGAN-based\ngenerator is presented to generate plausible pseudo images with full control\nover the viewpoint. Then, we propose to utilize a neural implicit function,\nalong with a differentiable renderer to learn 3D geometry from pseudo images\nwith object masks and rough pose initializations. To further detect the\nunreliable supervisions, we introduce a novel uncertainty module to predict\nuncertainty maps, which remedy the negative effect of uncertain regions in\npseudo images, leading to a better reconstruction performance. The\neffectiveness of our approach is demonstrated through superior single-view 3D\nreconstruction results of generic objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bitwidth-Adaptive Quantization-Aware Neural Network Training: A Meta-Learning Approach. (arXiv:2207.10188v1 [cs.LG])","link":"http://arxiv.org/abs/2207.10188","description":"<p>Deep neural network quantization with adaptive bitwidths has gained\nincreasing attention due to the ease of model deployment on various platforms\nwith different resource budgets. In this paper, we propose a meta-learning\napproach to achieve this goal. Specifically, we propose MEBQAT, a simple yet\neffective way of bitwidth-adaptive quantization aware training (QAT) where\nmeta-learning is effectively combined with QAT by redefining meta-learning\ntasks to incorporate bitwidths. After being deployed on a platform, MEBQAT\nallows the (meta-)trained model to be quantized to any candidate bitwidth then\nhelps to conduct inference without much accuracy drop from quantization.\nMoreover, with a few-shot learning scenario, MEBQAT can also adapt a model to\nany bitwidth as well as any unseen target classes by adding conventional\noptimization or metric-based meta-learning. We design variants of MEBQAT to\nsupport both (1) a bitwidth-adaptive quantization scenario and (2) a new\nfew-shot learning scenario where both quantization bitwidths and target classes\nare jointly adapted. We experimentally demonstrate their validity in multiple\nQAT schemes. By comparing their performance to (bitwidth-dedicated) QAT,\nexisting bitwidth adaptive QAT and vanilla meta-learning, we find that merging\nbitwidths into meta-learning tasks achieves a higher level of robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Youn_J/0/1/0/all/0/1\">Jiseok Youn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jaehun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyung-Sin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahk_S/0/1/0/all/0/1\">Saewoong Bahk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Hotels-50K and Hotel-ID. (arXiv:2207.10200v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10200","description":"<p>In this paper, we propose revisited versions for two recent hotel recognition\ndatasets: Hotels50K and Hotel-ID. The revisited versions provide evaluation\nsetups with different levels of difficulty to better align with the intended\nreal-world application, i.e. countering human trafficking. Real-world scenarios\ninvolve hotels and locations that are not captured in the current data sets,\ntherefore it is important to consider evaluation settings where classes are\ntruly unseen. We test this setup using multiple state-of-the-art image\nretrieval models and show that as expected, the models' performances decrease\nas the evaluation gets closer to the real-world unseen settings. The rankings\nof the best performing models also change across the different evaluation\nsettings, which further motivates using the proposed revisited datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feizi_A/0/1/0/all/0/1\">Aarash Feizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casanova_A/0/1/0/all/0/1\">Arantxa Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Soriano_A/0/1/0/all/0/1\">Adriana Romero-Soriano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbany_R/0/1/0/all/0/1\">Reihaneh Rabbany</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid CNN-Transformer Model For Facial Affect Recognition In the ABAW4 Challenge. (arXiv:2207.10201v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10201","description":"<p>This paper describes our submission to the fourth Affective Behavior Analysis\n(ABAW) competition. We proposed a hybrid CNN-Transformer model for the\nMulti-Task-Learning (MTL) and Learning from Synthetic Data (LSD) task.\nExperimental results on validation dataset shows that our method achieves\nbetter performance than baseline model, which verifies that the effectiveness\nof proposed network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haocheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunyin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Robustness of 3D Object Detectors. (arXiv:2207.10205v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10205","description":"<p>In recent years, significant progress has been achieved for 3D object\ndetection on point clouds thanks to the advances in 3D data collection and deep\nlearning techniques. Nevertheless, 3D scenes exhibit a lot of variations and\nare prone to sensor inaccuracies as well as information loss during\npre-processing. Thus, it is crucial to design techniques that are robust\nagainst these variations. This requires a detailed analysis and understanding\nof the effect of such variations. This work aims to analyze and benchmark\npopular point-based 3D object detectors against several data corruptions. To\nthe best of our knowledge, we are the first to investigate the robustness of\npoint-based 3D object detectors. To this end, we design and evaluate\ncorruptions that involve data addition, reduction, and alteration. We further\nstudy the robustness of different modules against local and global variations.\nOur experimental results reveal several intriguing findings. For instance, we\nshow that methods that integrate Transformers at a patch or object level lead\nto increased robustness, compared to using Transformers at the point level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albreiki_F/0/1/0/all/0/1\">Fatima Albreiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abughazal_S/0/1/0/all/0/1\">Sultan Abughazal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahoud_J/0/1/0/all/0/1\">Jean Lahoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1\">Rao Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1\">Hisham Cholakkal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spotting Temporally Precise, Fine-Grained Events in Video. (arXiv:2207.10213v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10213","description":"<p>We introduce the task of spotting temporally precise, fine-grained events in\nvideo (detecting the precise moment in time events occur). Precise spotting\nrequires models to reason globally about the full-time scale of actions and\nlocally to identify subtle frame-to-frame appearance and motion differences\nthat identify events during these actions. Surprisingly, we find that top\nperforming solutions to prior video understanding tasks such as action\ndetection and segmentation do not simultaneously meet both requirements. In\nresponse, we propose E2E-Spot, a compact, end-to-end model that performs well\non the precise spotting task and can be trained quickly on a single GPU. We\ndemonstrate that E2E-Spot significantly outperforms recent baselines adapted\nfrom the video action detection, segmentation, and spotting literature to the\nprecise spotting task. Finally, we contribute new annotations and splits to\nseveral fine-grained sports action datasets to make these datasets suitable for\nfuture work on precise spotting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">James Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharbi_M/0/1/0/all/0/1\">Micha&#xeb;l Gharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatahalian_K/0/1/0/all/0/1\">Kayvon Fatahalian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Label Granularity and Object Localization. (arXiv:2207.10225v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10225","description":"<p>Weakly supervised object localization (WSOL) aims to learn representations\nthat encode object location using only image-level category labels. However,\nmany objects can be labeled at different levels of granularity. Is it an\nanimal, a bird, or a great horned owl? Which image-level labels should we use?\nIn this paper we study the role of label granularity in WSOL. To facilitate\nthis investigation we introduce iNatLoc500, a new large-scale fine-grained\nbenchmark dataset for WSOL. Surprisingly, we find that choosing the right\ntraining label granularity provides a much larger performance boost than\nchoosing the best WSOL algorithm. We also show that changing the label\ngranularity can significantly improve data efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cole_E/0/1/0/all/0/1\">Elijah Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1\">Kimberly Wilber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horn_G/0/1/0/all/0/1\">Grant Van Horn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fornoni_M/0/1/0/all/0/1\">Marco Fornoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1\">Andrew Howard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeshMAE: Masked Autoencoders for 3D Mesh Data Analysis. (arXiv:2207.10228v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10228","description":"<p>Recently, self-supervised pre-training has advanced Vision Transformers on\nvarious tasks w.r.t. different data modalities, e.g., image and 3D point cloud\ndata. In this paper, we explore this learning paradigm for 3D mesh data\nanalysis based on Transformers. Since applying Transformer architectures to new\nmodalities is usually non-trivial, we first adapt Vision Transformer to 3D mesh\ndata processing, i.e., Mesh Transformer. In specific, we divide a mesh into\nseveral non-overlapping local patches with each containing the same number of\nfaces and use the 3D position of each patch's center point to form positional\nembeddings. Inspired by MAE, we explore how pre-training on 3D mesh data with\nthe Transformer-based structure benefits downstream 3D mesh analysis tasks. We\nfirst randomly mask some patches of the mesh and feed the corrupted mesh into\nMesh Transformers. Then, through reconstructing the information of masked\npatches, the network is capable of learning discriminative representations for\nmesh data. Therefore, we name our method MeshMAE, which can yield\nstate-of-the-art or comparable performance on mesh analysis tasks, i.e.,\nclassification and segmentation. In addition, we also conduct comprehensive\nablation studies to show the effectiveness of key designs in our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaqian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shanshan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fazhi He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPIN: An Empirical Evaluation on Sharing Parameters of Isotropic Networks. (arXiv:2207.10237v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10237","description":"<p>Recent isotropic networks, such as ConvMixer and vision transformers, have\nfound significant success across visual recognition tasks, matching or\noutperforming non-isotropic convolutional neural networks (CNNs). Isotropic\narchitectures are particularly well-suited to cross-layer weight sharing, an\neffective neural network compression technique. In this paper, we perform an\nempirical evaluation on methods for sharing parameters in isotropic networks\n(SPIN). We present a framework to formalize major weight sharing design\ndecisions and perform a comprehensive empirical evaluation of this design\nspace. Guided by our experimental results, we propose a weight sharing strategy\nto generate a family of models with better overall efficiency, in terms of\nFLOPs and parameters versus accuracy, compared to traditional scaling methods\nalone, for example compressing ConvMixer by 1.9x while improving accuracy on\nImageNet. Finally, we perform a qualitative study to further understand the\nbehavior of weight sharing in isotropic architectures. The code is available at\nhttps://github.com/apple/ml-spin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chien-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_A/0/1/0/all/0/1\">Anish Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merth_T/0/1/0/all/0/1\">Thomas Merth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sachin Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1\">Anurag Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horton_M/0/1/0/all/0/1\">Maxwell Horton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GBDF: Gender Balanced DeepFake Dataset Towards Fair DeepFake Detection. (arXiv:2207.10246v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10246","description":"<p>Facial forgery by deepfakes has raised severe societal concerns. Several\nsolutions have been proposed by the vision community to effectively combat the\nmisinformation on the internet via automated deepfake detection systems. Recent\nstudies have demonstrated that facial analysis-based deep learning models can\ndiscriminate based on protected attributes. For the commercial adoption and\nmassive roll-out of the deepfake detection technology, it is vital to evaluate\nand understand the fairness (the absence of any prejudice or favoritism) of\ndeepfake detectors across demographic variations such as gender and race. As\nthe performance differential of deepfake detectors between demographic\nsubgroups would impact millions of people of the deprived sub-group. This paper\naims to evaluate the fairness of the deepfake detectors across males and\nfemales. However, existing deepfake datasets are not annotated with demographic\nlabels to facilitate fairness analysis. To this aim, we manually annotated\nexisting popular deepfake datasets with gender labels and evaluated the\nperformance differential of current deepfake detectors across gender. Our\nanalysis on the gender-labeled version of the datasets suggests (a) current\ndeepfake datasets have skewed distribution across gender, and (b) commonly\nadopted deepfake detectors obtain unequal performance across gender with mostly\nmales outperforming females. Finally, we contributed a gender-balanced and\nannotated deepfake dataset, GBDF, to mitigate the performance differential and\nto promote research and development towards fairness-aware deep fake detectors.\nThe GBDF dataset is publicly available at: https://github.com/aakash4305/GBDF\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nadimpalli_A/0/1/0/all/0/1\">Aakash Varma Nadimpalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rattani_A/0/1/0/all/0/1\">Ajita Rattani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SplitMixer: Fat Trimmed From MLP-like Models. (arXiv:2207.10255v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10255","description":"<p>We present SplitMixer, a simple and lightweight isotropic MLP-like\narchitecture, for visual recognition. It contains two types of interleaving\nconvolutional operations to mix information across spatial locations (spatial\nmixing) and channels (channel mixing). The first one includes sequentially\napplying two depthwise 1D kernels, instead of a 2D kernel, to mix spatial\ninformation. The second one is splitting the channels into overlapping or\nnon-overlapping segments, with or without shared parameters, and applying our\nproposed channel mixing approaches or 3D convolution to mix channel\ninformation. Depending on design choices, a number of SplitMixer variants can\nbe constructed to balance accuracy, the number of parameters, and speed. We\nshow, both theoretically and experimentally, that SplitMixer performs on par\nwith the state-of-the-art MLP-like models while having a significantly lower\nnumber of parameters and FLOPS. For example, without strong data augmentation\nand optimization, SplitMixer achieves around 94% accuracy on CIFAR-10 with only\n0.28M parameters, while ConvMixer achieves the same accuracy with about 0.6M\nparameters. The well-known MLP-Mixer achieves 85.45% with 17.1M parameters. On\nCIFAR-100 dataset, SplitMixer achieves around 73% accuracy, on par with\nConvMixer, but with about 52% fewer parameters and FLOPS. We hope that our\nresults spark further research towards finding more efficient vision\narchitectures and facilitate the development of MLP-like models. Code is\navailable at https://github.com/aliborji/splitmixer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sikun Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGBANet: Semantic GAN and Balanced Attention Network for Arbitrarily Oriented Scene Text Recognition. (arXiv:2207.10256v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10256","description":"<p>Scene text recognition is a challenging task due to the complex backgrounds\nand diverse variations of text instances. In this paper, we propose a novel\nSemantic GAN and Balanced Attention Network (SGBANet) to recognize the texts in\nscene images. The proposed method first generates the simple semantic feature\nusing Semantic GAN and then recognizes the scene text with the Balanced\nAttention Module. The Semantic GAN aims to align the semantic feature\ndistribution between the support domain and target domain. Different from the\nconventional image-to-image translation methods that perform at the image\nlevel, the Semantic GAN performs the generation and discrimination on the\nsemantic level with the Semantic Generator Module (SGM) and Semantic\nDiscriminator Module (SDM). For target images (scene text images), the Semantic\nGenerator Module generates simple semantic features that share the same feature\ndistribution with support images (clear text images). The Semantic\nDiscriminator Module is used to distinguish the semantic features between the\nsupport domain and target domain. In addition, a Balanced Attention Module is\ndesigned to alleviate the problem of attention drift. The Balanced Attention\nModule first learns a balancing parameter based on the visual glimpse vector\nand semantic glimpse vector, and then performs the balancing operation for\nobtaining a balanced glimpse vector. Experiments on six benchmarks, including\nregular datasets, i.e., IIIT5K, SVT, ICDAR2013, and irregular datasets, i.e.,\nICDAR2015, SVTP, CUTE80, validate the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_D/0/1/0/all/0/1\">Dajian Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shujing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shivakumara_P/0/1/0/all/0/1\">Palaiahnakote Shivakumara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yue Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis. (arXiv:2207.10257v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10257","description":"<p>Over the years, 2D GANs have achieved great successes in photorealistic\nportrait generation. However, they lack 3D understanding in the generation\nprocess, thus they suffer from multi-view inconsistency problem. To alleviate\nthe issue, many 3D-aware GANs have been proposed and shown notable results, but\n3D GANs struggle with editing semantic attributes. The controllability and\ninterpretability of 3D GANs have not been much explored. In this work, we\npropose two solutions to overcome these weaknesses of 2D GANs and 3D-aware\nGANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of\ndiscovering semantic attributes during training and controlling them in an\nunsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN\nto obtain a high-fidelity 3D-controllable generator. Unlike existing\nlatent-based methods allowing implicit pose control, the proposed\n3D-controllable StyleGAN enables explicit pose control over portrait\ngeneration. This distillation allows direct compatibility between 3D control\nand many StyleGAN-based techniques (e.g., inversion and stylization), and also\nbrings an advantage in terms of computational resources. Our codes are\navailable at https://github.com/jgkwak95/SURF-GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_J/0/1/0/all/0/1\">Jeong-gi Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1\">Dongsik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">David Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hanseok Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region Aware Video Object Segmentation with Deep Motion Modeling. (arXiv:2207.10258v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10258","description":"<p>Current semi-supervised video object segmentation (VOS) methods usually\nleverage the entire features of one frame to predict object masks and update\nmemory. This introduces significant redundant computations. To reduce\nredundancy, we present a Region Aware Video Object Segmentation (RAVOS)\napproach that predicts regions of interest (ROIs) for efficient object\nsegmentation and memory storage. RAVOS includes a fast object motion tracker to\npredict their ROIs in the next frame. For efficient segmentation, object\nfeatures are extracted according to the ROIs, and an object decoder is designed\nfor object-level segmentation. For efficient memory storage, we propose motion\npath memory to filter out redundant context by memorizing the features within\nthe motion path of objects between two frames. Besides RAVOS, we also propose a\nlarge-scale dataset, dubbed OVOS, to benchmark the performance of VOS models\nunder occlusions. Evaluation on DAVIS and YouTube-VOS benchmarks and our new\nOVOS dataset show that our method achieves state-of-the-art performance with\nsignificantly faster inference time, e.g., 86.1 J&amp;F at 42 FPS on DAVIS and 84.4\nJ&amp;F at 23 FPS on YouTube-VOS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_B/0/1/0/all/0/1\">Bo Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-centric Image Cropping with Partition-aware and Content-preserving Features. (arXiv:2207.10269v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10269","description":"<p>Image cropping aims to find visually appealing crops in an image, which is an\nimportant yet challenging task. In this paper, we consider a specific and\npractical application: human-centric image cropping, which focuses on the\ndepiction of a person. To this end, we propose a human-centric image cropping\nmethod with two novel feature designs for the candidate crop: partition-aware\nfeature and content-preserving feature. For partition-aware feature, we divide\nthe whole image into nine partitions based on the human bounding box and treat\ndifferent partitions in a candidate crop differently conditioned on the human\ninformation. For content-preserving feature, we predict a heatmap indicating\nthe important content to be included in a good crop, and extract the geometric\nrelation between the heatmap and a candidate crop. Extensive experiments\ndemonstrate that our method can perform favorably against state-of-the-art\nimage cropping methods on human-centric image cropping task. Code is available\nat https://github.com/bcmi/Human-Centric-Image-Cropping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta. (arXiv:2207.10271v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10271","description":"<p>Learning to generate new images for a novel category based on only a few\nimages, named as few-shot image generation, has attracted increasing research\ninterest. Several state-of-the-art works have yielded impressive results, but\nthe diversity is still limited. In this work, we propose a novel Delta\nGenerative Adversarial Network (DeltaGAN), which consists of a reconstruction\nsubnetwork and a generation subnetwork. The reconstruction subnetwork captures\nintra-category transformation, i.e., delta, between same-category pairs. The\ngeneration subnetwork generates sample-specific delta for an input image, which\nis combined with this input image to generate a new image within the same\ncategory. Besides, an adversarial delta matching loss is designed to link the\nabove two subnetworks together. Extensive experiments on six benchmark datasets\ndemonstrate the effectiveness of our proposed method. Our code is available at\nhttps://github.com/bcmi/DeltaGAN-Few-Shot-Image-Generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianfu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Forget Me: Accurate Background Recovery for Text Removal via Modeling Local-Global Context. (arXiv:2207.10273v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10273","description":"<p>Text removal has attracted increasingly attention due to its various\napplications on privacy protection, document restoration, and text editing. It\nhas shown significant progress with deep neural network. However, most of the\nexisting methods often generate inconsistent results for complex background. To\naddress this issue, we propose a Contextual-guided Text Removal Network, termed\nas CTRNet. CTRNet explores both low-level structure and high-level\ndiscriminative context feature as prior knowledge to guide the process of\nbackground restoration. We further propose a Local-global Content Modeling\n(LGCM) block with CNNs and Transformer-Encoder to capture local features and\nestablish the long-term relationship among pixels globally. Finally, we\nincorporate LGCM with context guidance for feature modeling and decoding.\nExperiments on benchmark datasets, SCUT-EnsText and SCUT-Syn show that CTRNet\nsignificantly outperforms the existing state-of-the-art methods. Furthermore, a\nqualitative experiment on examination papers also demonstrates the\ngeneralization ability of our method. The codes and supplement materials are\navailable at https://github.com/lcy0604/CTRNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chongyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Canjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bangdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fengjun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Kai Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond single receptive field: A receptive field fusion-and-stratification network for airborne laser scanning point cloud classification. (arXiv:2207.10278v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10278","description":"<p>The classification of airborne laser scanning (ALS) point clouds is a\ncritical task of remote sensing and photogrammetry fields. Although recent deep\nlearning-based methods have achieved satisfactory performance, they have\nignored the unicity of the receptive field, which makes the ALS point cloud\nclassification remain challenging for the distinguishment of the areas with\ncomplex structures and extreme scale variations. In this article, for the\nobjective of configuring multi-receptive field features, we propose a novel\nreceptive field fusion-and-stratification network (RFFS-Net). With a novel\ndilated graph convolution (DGConv) and its extension annular dilated\nconvolution (ADConv) as basic building blocks, the receptive field fusion\nprocess is implemented with the dilated and annular graph fusion (DAGFusion)\nmodule, which obtains multi-receptive field feature representation through\ncapturing dilated and annular graphs with various receptive regions. The\nstratification of the receptive fields with point sets of different resolutions\nas the calculation bases is performed with Multi-level Decoders nested in\nRFFS-Net and driven by the multi-level receptive field aggregation loss\n(MRFALoss) to drive the network to learn in the direction of the supervision\nlabels with different resolutions. With receptive field\nfusion-and-stratification, RFFS-Net is more adaptable to the classification of\nregions with complex structures and extreme scale variations in large-scale ALS\npoint clouds. Evaluated on the ISPRS Vaihingen 3D dataset, our RFFS-Net\nsignificantly outperforms the baseline approach by 5.3% on mF1 and 5.4% on\nmIoU, accomplishing an overall accuracy of 82.1%, an mF1 of 71.6%, and an mIoU\nof 58.2%. Furthermore, experiments on the LASDU dataset and the 2019 IEEE-GRSS\nData Fusion Contest dataset show that RFFS-Net achieves a new state-of-the-art\nclassification performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yongqiang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaiqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_W/0/1/0/all/0/1\">Wenhui Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaonan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1\">Martin Weinmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient-based Point Cloud Denoising with Uniformity. (arXiv:2207.10279v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10279","description":"<p>Point clouds captured by depth sensors are often contaminated by noises,\nobstructing further analysis and applications. In this paper, we emphasize the\nimportance of point distribution uniformity to downstream tasks. We demonstrate\nthat point clouds produced by existing gradient-based denoisers lack uniformity\ndespite having achieved promising quantitative results. To this end, we propose\nGPCD++, a gradient-based denoiser with an ultra-lightweight network named\nUniNet to address uniformity. Compared with previous state-of-the-art methods,\nour approach not only generates competitive or even better denoising results,\nbut also significantly improves uniformity which largely benefits applications\nsuch as surface reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tian-Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuan-Chen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yong-Liang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Song-Hai Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Visual Representations with Texts for Domain Generalization. (arXiv:2207.10285v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10285","description":"<p>Reducing the representational discrepancy between source and target domains\nis a key component to maximize the model generalization. In this work, we\nadvocate for leveraging natural language supervision for the domain\ngeneralization task. We introduce two modules to ground visual representations\nwith texts containing typical reasoning of humans: (1) Visual and Textual Joint\nEmbedder and (2) Textual Explanation Generator. The former learns the\nimage-text joint embedding space where we can ground high-level\nclass-discriminative information into the model. The latter leverages an\nexplainable model and generates explanations justifying the rationale behind\nits decision. To the best of our knowledge, this is the first work to leverage\nthe vision-and-language cross-modality approach for the domain generalization\ntask. Our experiments with a newly created CUB-DG benchmark dataset demonstrate\nthat cross-modality supervision can be successfully used to ground\ndomain-invariant visual representations and improve the model generalization.\nFurthermore, in the large-scale DomainBed benchmark, our proposed method\nachieves state-of-the-art results and ranks 1st in average performance for five\nmulti-domain datasets. The dataset and codes are available at\nhttps://github.com/mswzeus/GVRT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Seonwoo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Nokyung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Siwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinkyu Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Accurate Open-Set Recognition via Background-Class Regularization. (arXiv:2207.10287v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10287","description":"<p>In open-set recognition (OSR), classifiers should be able to reject\nunknown-class samples while maintaining high closed-set classification\naccuracy. To effectively solve the OSR problem, previous studies attempted to\nlimit latent feature space and reject data located outside the limited space\nvia offline analyses, e.g., distance-based feature analyses, or complicated\nnetwork architectures. To conduct OSR via a simple inference process (without\noffline analyses) in standard classifier architectures, we use distance-based\nclassifiers instead of conventional Softmax classifiers. Afterwards, we design\na background-class regularization strategy, which uses background-class data as\nsurrogates of unknown-class ones during training phase. Specifically, we\nformulate a novel regularization loss suitable for distance-based classifiers,\nwhich reserves sufficiently large class-wise latent feature spaces for known\nclasses and forces background-class samples to be located far away from the\nlimited spaces. Through our extensive experiments, we show that the proposed\nmethod provides robust OSR results, while maintaining high closed-set\nclassification accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Wonwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AugRmixAT: A Data Processing and Training Method for Improving Multiple Robustness and Generalization Performance. (arXiv:2207.10290v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10290","description":"<p>Deep neural networks are powerful, but they also have shortcomings such as\ntheir sensitivity to adversarial examples, noise, blur, occlusion, etc.\nMoreover, ensuring the reliability and robustness of deep neural network models\nis crucial for their application in safety-critical areas. Much previous work\nhas been proposed to improve specific robustness. However, we find that the\nspecific robustness is often improved at the sacrifice of the additional\nrobustness or generalization ability of the neural network model. In\nparticular, adversarial training methods significantly hurt the generalization\nperformance on unperturbed data when improving adversarial robustness. In this\npaper, we propose a new data processing and training method, called AugRmixAT,\nwhich can simultaneously improve the generalization ability and multiple\nrobustness of neural network models. Finally, we validate the effectiveness of\nAugRmixAT on the CIFAR-10/100 and Tiny-ImageNet datasets. The experiments\ndemonstrate that AugRmixAT can improve the model's generalization performance\nwhile enhancing the white-box robustness, black-box robustness, common\ncorruption robustness, and partial occlusion robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Furao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_C/0/1/0/all/0/1\">Changhai Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Generation Network for Covert Transmission in Online Social Network. (arXiv:2207.10292v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10292","description":"<p>Online social networks have stimulated communications over the Internet more\nthan ever, making it possible for secret message transmission over such noisy\nchannels. In this paper, we propose a Coverless Image Steganography Network,\ncalled CIS-Net, that synthesizes a high-quality image directly conditioned on\nthe secret message to transfer. CIS-Net is composed of four modules, namely,\nthe Generation, Adversarial, Extraction, and Noise Module. The receiver can\nextract the hidden message without any loss even the images have been distorted\nby JPEG compression attacks. To disguise the behaviour of steganography, we\ncollected images in the context of profile photos and stickers and train our\nnetwork accordingly. As such, the generated images are more inclined to escape\nfrom malicious detection and attack. The distinctions from previous image\nsteganography methods are majorly the robustness and losslessness against\ndiverse attacks. Experiments over diverse public datasets have manifested the\nsuperior ability of anti-steganalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhengxin You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1\">Qichao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Cross Attention Network in Facial Behavior Analysis. (arXiv:2207.10293v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10293","description":"<p>Facial behavior analysis is a broad topic with various categories such as\nfacial emotion recognition, age and gender recognition, ... Many studies focus\non individual tasks while the multi-task learning approach is still open and\nrequires more research. In this paper, we present our solution and experiment\nresult for the Multi-Task Learning challenge of the Affective Behavior Analysis\nin-the-wild competition. The challenge is a combination of three tasks: action\nunit detection, facial expression recognition and valance-arousal estimation.\nTo address this challenge, we introduce a cross-attentive module to improve\nmulti-task learning performance. Additionally, a facial graph is applied to\ncapture the association among action units. As a result, we achieve the\nevaluation measure of 1.24 on the validation data provided by the organizers,\nwhich is better than the baseline result of 0.30.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dang-Khanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pant_S/0/1/0/all/0/1\">Sudarshan Pant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Ngoc-Huynh Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Guee-Sang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo-Huyng Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hyung-Jeong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Unsupervised Anomaly Localization in Industrial Images: A Survey. (arXiv:2207.10298v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10298","description":"<p>Currently, deep learning-based visual inspection has been highly successful\nwith the help of supervised learning methods. However, in real industrial\nscenarios, the scarcity of defect samples, the cost of annotation, and the lack\nof a priori knowledge of defects may render supervised-based methods\nineffective. In recent years, unsupervised anomaly localization algorithms have\nbecome more widely used in industrial inspection tasks. This paper aims to help\nresearchers in this field by comprehensively surveying recent achievements in\nunsupervised anomaly localization in industrial images using deep learning. The\nsurvey reviews more than 120 significant publications covering different\naspects of anomaly localization, mainly covering various concepts, challenges,\ntaxonomies, benchmark datasets, and quantitative performance comparisons of the\nmethods reviewed. In reviewing the achievements to date, this paper provides\ndetailed predictions and analysis of several future research directions. This\nreview provides detailed technical information for researchers interested in\nindustrial anomaly localization and who wish to apply it to the localization of\nanomalies in other fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xian Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xinyi Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shaohua Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adak_C/0/1/0/all/0/1\">Chandranath Adak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn From All: Erasing Attention Consistency for Noisy Label Facial Expression Recognition. (arXiv:2207.10299v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10299","description":"<p>Noisy label Facial Expression Recognition (FER) is more challenging than\ntraditional noisy label classification tasks due to the inter-class similarity\nand the annotation ambiguity. Recent works mainly tackle this problem by\nfiltering out large-loss samples. In this paper, we explore dealing with noisy\nlabels from a new feature-learning perspective. We find that FER models\nremember noisy samples by focusing on a part of the features that can be\nconsidered related to the noisy labels instead of learning from the whole\nfeatures that lead to the latent truth. Inspired by that, we propose a novel\nErasing Attention Consistency (EAC) method to suppress the noisy samples during\nthe training process automatically. Specifically, we first utilize the flip\nsemantic consistency of facial images to design an imbalanced framework. We\nthen randomly erase input images and use flip attention consistency to prevent\nthe model from focusing on a part of the features. EAC significantly\noutperforms state-of-the-art noisy label FER methods and generalizes well to\nother tasks with a large number of classes like CIFAR100 and Tiny-ImageNet. The\ncode is available at\nhttps://github.com/zyh-uaiaaaa/Erasing-Attention-Consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xu Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On an Edge-Preserving Variational Model for Optical Flow Estimation. (arXiv:2207.10302v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10302","description":"<p>It is well known that classical formulations resembling the Horn and Schunck\nmodel are still largely competitive due to the modern implementation practices.\nIn most cases, these models outperform many modern flow estimation methods. In\nview of this, we propose an effective implementation design for an\nedge-preserving $L^1$ regularization approach to optical flow. The mathematical\nwell-posedness of our proposed model is studied in the space of functions of\nbounded variations $BV(\\Omega,\\mathbb{R}^2)$. The implementation scheme is\ndesigned in multiple steps. The flow field is computed using the robust\nChambolle-Pock primal-dual algorithm. Motivated by the recent studies of Castro\nand Donoho we extend the heuristic of iterated median filtering to our flow\nestimation. Further, to refine the flow edges we use the weighted median filter\nestablished by Li and Osher as a post-processing step. Our experiments on the\nMiddlebury dataset show that the proposed method achieves the best average\nangular and end-point errors compared to some of the state-of-the-art Horn and\nSchunck based variational methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doshi_H/0/1/0/all/0/1\">Hirak Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiran_N/0/1/0/all/0/1\">N. Uday Kiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Leveraging Pre-trained Generative Adversarial Networks for Image Editing and Restoration. (arXiv:2207.10309v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10309","description":"<p>Generative adversarial networks (GANs) have drawn enormous attention due to\nthe simple yet effective training mechanism and superior image generation\nquality. With the ability to generate photo-realistic high-resolution (e.g.,\n$1024\\times1024$) images, recent GAN models have greatly narrowed the gaps\nbetween the generated images and the real ones. Therefore, many recent works\nshow emerging interest to take advantage of pre-trained GAN models by\nexploiting the well-disentangled latent space and the learned GAN priors. In\nthis paper, we briefly review recent progress on leveraging pre-trained\nlarge-scale GAN models from three aspects, i.e., 1) the training of large-scale\ngenerative adversarial networks, 2) exploring and understanding the pre-trained\nGAN models, and 3) leveraging these models for subsequent tasks like image\nrestoration and editing. More information about relevant methods and\nrepositories can be found at https://github.com/csmliu/pretrained-GANs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yuxiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaohe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance Fields. (arXiv:2207.10312v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10312","description":"<p>Novel view synthesis has recently been revolutionized by learning neural\nradiance fields directly from sparse observations. However, rendering images\nwith this new paradigm is slow due to the fact that an accurate quadrature of\nthe volume rendering equation requires a large number of samples for each ray.\nPrevious work has mainly focused on speeding up the network evaluations that\nare associated with each sample point, e.g., via caching of radiance values\ninto explicit spatial data structures, but this comes at the expense of model\ncompactness. In this paper, we propose a novel dual-network architecture that\ntakes an orthogonal direction by learning how to best reduce the number of\nrequired sample points. To this end, we split our network into a sampling and\nshading network that are jointly trained. Our training scheme employs fixed\nsample positions along each ray, and incrementally introduces sparsity\nthroughout training to achieve high quality even at low sample counts. After\nfine-tuning with the target number of samples, the resulting compact neural\nrepresentation can be rendered in real-time. Our experiments demonstrate that\nour approach outperforms concurrent compact neural representations in terms of\nquality and frame rate and performs on par with highly efficient hybrid\nrepresentations. Code and supplementary material is available at\nhttps://thomasneff.github.io/adanerf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurz_A/0/1/0/all/0/1\">Andreas Kurz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neff_T/0/1/0/all/0/1\">Thomas Neff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhaoyang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1\">Michael Zollh&#xf6;fer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinberger_M/0/1/0/all/0/1\">Markus Steinberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Learning of Optical Flow by Flow Supervisor. (arXiv:2207.10314v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10314","description":"<p>A training pipeline for optical flow CNNs consists of a pretraining stage on\na synthetic dataset followed by a fine tuning stage on a target dataset.\nHowever, obtaining ground truth flows from a target video requires a tremendous\neffort. This paper proposes a practical fine tuning method to adapt a\npretrained model to a target dataset without ground truth flows, which has not\nbeen explored extensively. Specifically, we propose a flow supervisor for\nself-supervision, which consists of parameter separation and a student output\nconnection. This design is aimed at stable convergence and better accuracy over\nconventional self-supervision methods which are unstable on the fine tuning\ntask. Experimental results show the effectiveness of our method compared to\ndifferent self-supervision methods for semi-supervised learning. In addition,\nwe achieve meaningful improvements over state-of-the-art optical flow models on\nSintel and KITTI benchmarks by exploiting additional unlabeled datasets. Code\nis available at https://github.com/iwbn/flow-supervisor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Im_W/0/1/0/all/0/1\">Woobin Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sebin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sung-Eui Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer. (arXiv:2207.10315v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10315","description":"<p>Point cloud completion has become increasingly popular among generation tasks\nof 3D point clouds, as it is a challenging yet indispensable problem to recover\nthe complete shape of a 3D object from its partial observation. In this paper,\nwe propose a novel SeedFormer to improve the ability of detail preservation and\nrecovery in point cloud completion. Unlike previous methods based on a global\nfeature vector, we introduce a new shape representation, namely Patch Seeds,\nwhich not only captures general structures from partial inputs but also\npreserves regional information of local patterns. Then, by integrating seed\nfeatures into the generation process, we can recover faithful details for\ncomplete point clouds in a coarse-to-fine manner. Moreover, we devise an\nUpsample Transformer by extending the transformer structure into basic\noperations of point generators, which effectively incorporates spatial and\nsemantic relationships between neighboring points. Qualitative and quantitative\nevaluations demonstrate that our method outperforms state-of-the-art completion\nnetworks on several benchmark datasets. Our code is available at\nhttps://github.com/hrzhou2/seedformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Haoran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wenqing Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Junwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoAlignV2: Deformable Feature Aggregation for Dynamic Multi-Modal 3D Object Detection. (arXiv:2207.10316v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10316","description":"<p>Point clouds and RGB images are two general perceptional sources in\nautonomous driving. The former can provide accurate localization of objects,\nand the latter is denser and richer in semantic information. Recently,\nAutoAlign presents a learnable paradigm in combining these two modalities for\n3D object detection. However, it suffers from high computational cost\nintroduced by the global-wise attention. To solve the problem, we propose\nCross-Domain DeformCAFA module in this work. It attends to sparse learnable\nsampling points for cross-modal relational modeling, which enhances the\ntolerance to calibration error and greatly speeds up the feature aggregation\nacross different modalities. To overcome the complex GT-AUG under multi-modal\nsettings, we design a simple yet effective cross-modal augmentation strategy on\nconvex combination of image patches given their depth information. Moreover, by\ncarrying out a novel image-level dropout training scheme, our model is able to\ninfer in a dynamic manner. To this end, we propose AutoAlignV2, a faster and\nstronger multi-modal 3D detection framework, built on top of AutoAlign.\nExtensive experiments on nuScenes benchmark demonstrate the effectiveness and\nefficiency of AutoAlignV2. Notably, our best model reaches 72.4 NDS on nuScenes\ntest leaderboard, achieving new state-of-the-art results among all published\nmulti-modal 3D object detectors. Code will be available at\nhttps://github.com/zehuichen123/AutoAlignV2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Liangji Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qinhong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient CNN Architecture Design Guided by Visualization. (arXiv:2207.10318v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10318","description":"<p>Modern efficient Convolutional Neural Networks(CNNs) always use Depthwise\nSeparable Convolutions(DSCs) and Neural Architecture Search(NAS) to reduce the\nnumber of parameters and the computational complexity. But some inherent\ncharacteristics of networks are overlooked. Inspired by visualizing feature\nmaps and N$\\times$N(N$&gt;$1) convolution kernels, several guidelines are\nintroduced in this paper to further improve parameter efficiency and inference\nspeed. Based on these guidelines, our parameter-efficient CNN architecture,\ncalled \\textit{VGNetG}, achieves better accuracy and lower latency than\nprevious networks with about 30%$\\thicksim$50% parameters reduction. Our\nVGNetG-1.0MP achieves 67.7% top-1 accuracy with 0.99M parameters and 69.2%\ntop-1 accuracy with 1.14M parameters on ImageNet classification dataset.\n</p>\n<p>Furthermore, we demonstrate that edge detectors can replace learnable\ndepthwise convolution layers to mix features by replacing the N$\\times$N\nkernels with fixed edge detection kernels. And our VGNetF-1.5MP archives\n64.4%(-3.2%) top-1 accuracy and 66.2%(-1.4%) top-1 accuracy with additional\nGaussian kernels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Haibo Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yihao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Leixilan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OIMNet++: Prototypical Normalization and Localization-aware Learning for Person Search. (arXiv:2207.10320v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10320","description":"<p>We address the task of person search, that is, localizing and re-identifying\nquery persons from a set of raw scene images. Recent approaches are typically\nbuilt upon OIMNet, a pioneer work on person search, that learns joint person\nrepresentations for performing both detection and person re-identification\n(reID) tasks. To obtain the representations, they extract features from\npedestrian proposals, and then project them on a unit hypersphere with L2\nnormalization. These methods also incorporate all positive proposals, that\nsufficiently overlap with the ground truth, equally to learn person\nrepresentations for reID. We have found that 1) the L2 normalization without\nconsidering feature distributions degenerates the discriminative power of\nperson representations, and 2) positive proposals often also depict background\nclutter and person overlaps, which could encode noisy features to person\nrepresentations. In this paper, we introduce OIMNet++ that addresses the\naforementioned limitations. To this end, we introduce a novel normalization\nlayer, dubbed ProtoNorm, that calibrates features from pedestrian proposals,\nwhile considering a long-tail distribution of person IDs, enabling L2\nnormalized person representations to be discriminative. We also propose a\nlocalization-aware feature learning scheme that encourages better-aligned\nproposals to contribute more in learning discriminative representations.\nExperimental results and analysis on standard person search benchmarks\ndemonstrate the effectiveness of OIMNet++.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1\">Youngmin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_D/0/1/0/all/0/1\">Donghyeon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junghyup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_B/0/1/0/all/0/1\">Bumsub Ham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Generative Model for Weakly Supervised Chest Anomaly Localization via Pseudo-paired Registration with Bilaterally Symmetrical Data Augmentation. (arXiv:2207.10324v1 [eess.IV])","link":"http://arxiv.org/abs/2207.10324","description":"<p>Image translation based on a generative adversarial network (GAN-IT) is a\npromising method for precise localization of abnormal regions in chest X-ray\nimages (AL-CXR). However, heterogeneous unpaired datasets undermine existing\nmethods to extract key features and distinguish normal from abnormal cases,\nresulting in inaccurate and unstable AL-CXR. To address this problem, we\npropose an improved two-stage GAN-IT involving registration and data\naugmentation. For the first stage, we introduce an invertible\ndeep-learning-based registration technique that virtually and reasonably\nconverts unpaired data into paired data for learning registration maps. This\nnovel approach achieves high registration performance. For the second stage, we\napply data augmentation to diversify anomaly locations by swapping the left and\nright lung regions on the uniform registered frames, further improving the\nperformance by alleviating imbalance in data distribution showing left and\nright lung lesions. Our method is intended for application to existing GAN-IT\nmodels, allowing existing architecture to benefit from key features for\ntranslation. By showing that the AL-CXR performance is uniformly improved when\napplying the proposed method, we believe that GAN-IT for AL-CXR can be deployed\nin clinical environments, even if learning data are scarce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Su Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oh_S/0/1/0/all/0/1\">Seong Je Oh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_T/0/1/0/all/0/1\">Tae Uk Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_M/0/1/0/all/0/1\">Myung Jin Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UFO: Unified Feature Optimization. (arXiv:2207.10341v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10341","description":"<p>This paper proposes a novel Unified Feature Optimization (UFO) paradigm for\ntraining and deploying deep models under real-world and large-scale scenarios,\nwhich requires a collection of multiple AI functions. UFO aims to benefit each\nsingle task with a large-scale pretraining on all tasks. Compared with the well\nknown foundation model, UFO has two different points of emphasis, i.e.,\nrelatively smaller model size and NO adaptation cost: 1) UFO squeezes a wide\nrange of tasks into a moderate-sized unified model in a multi-task learning\nmanner and further trims the model size when transferred to down-stream tasks.\n2) UFO does not emphasize transfer to novel tasks. Instead, it aims to make the\ntrimmed model dedicated for one or more already-seen task. With these two\ncharacteristics, UFO provides great convenience for flexible deployment, while\nmaintaining the benefits of large-scale pretraining. A key merit of UFO is that\nthe trimming process not only reduces the model size and inference consumption,\nbut also even improves the accuracy on certain tasks. Specifically, UFO\nconsiders the multi-task training and brings two-fold impact on the unified\nmodel: some closely related tasks have mutual benefits, while some tasks have\nconflicts against each other. UFO manages to reduce the conflicts and to\npreserve the mutual benefits through a novel Network Architecture Search (NAS)\nmethod. Experiments on a wide range of deep representation learning tasks\n(i.e., face recognition, person re-identification, vehicle re-identification\nand product retrieval) show that the model trimmed from UFO achieves higher\naccuracy than its single-task-trained counterpart and yet has smaller model\nsize, validating the concept of UFO. Besides, UFO also supported the release of\n17 billion parameters computer vision (CV) foundation model which is the\nlargest CV model in the industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xi_T/0/1/0/all/0/1\">Teng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Deli Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lufei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Haocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution. (arXiv:2207.10345v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10345","description":"<p>Despite breakthrough advances in image super-resolution (SR) with\nconvolutional neural networks (CNNs), SR has yet to enjoy ubiquitous\napplications due to the high computational complexity of SR networks.\nQuantization is one of the promising approaches to solve this problem. However,\nexisting methods fail to quantize SR models with a bit-width lower than 8 bits,\nsuffering from severe accuracy loss due to fixed bit-width quantization applied\neverywhere. In this work, to achieve high average bit-reduction with less\naccuracy loss, we propose a novel Content-Aware Dynamic Quantization (CADyQ)\nmethod for SR networks that allocates optimal bits to local regions and layers\nadaptively based on the local contents of an input image. To this end, a\ntrainable bit selector module is introduced to determine the proper bit-width\nand quantization level for each layer and a given local image patch. This\nmodule is governed by the quantization sensitivity that is estimated by using\nboth the average magnitude of image gradient of the patch and the standard\ndeviation of the input feature of the layer. The proposed quantization pipeline\nhas been tested on various SR networks and evaluated on several standard\nbenchmarks extensively. Significant reduction in computational complexity and\nthe elevated restoration accuracy clearly demonstrate the effectiveness of the\nproposed CADyQ framework for SR. Codes are available at\nhttps://github.com/Cheeun/CADyQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Cheeun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baik_S/0/1/0/all/0/1\">Sungyong Baik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nah_S/0/1/0/all/0/1\">Seungjun Nah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto Machine Learning for Medical Image Analysis by Unifying the Search on Data Augmentation and Neural Architecture. (arXiv:2207.10351v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10351","description":"<p>Automated data augmentation, which aims at engineering augmentation policy\nautomatically, recently draw a growing research interest. Many previous\nauto-augmentation methods utilized a Density Matching strategy by evaluating\npolicies in terms of the test-time augmentation performance. In this paper, we\ntheoretically and empirically demonstrated the inconsistency between the train\nand validation set of small-scale medical image datasets, referred to as\nin-domain sampling bias. Next, we demonstrated that the in-domain sampling bias\nmight cause the inefficiency of Density Matching. To address the problem, an\nimproved augmentation search strategy, named Augmented Density Matching, was\nproposed by randomly sampling policies from a prior distribution for training.\nMoreover, an efficient automatical machine learning(AutoML) algorithm was\nproposed by unifying the search on data augmentation and neural architecture.\nExperimental results indicated that the proposed methods outperformed\nstate-of-the-art approaches on MedMNIST, a pioneering benchmark designed for\nAutoML in medical image analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lituan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Data with Noisy Labels Using Temporal Self-Ensemble. (arXiv:2207.10354v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10354","description":"<p>There are inevitably many mislabeled data in real-world datasets. Because\ndeep neural networks (DNNs) have an enormous capacity to memorize noisy labels,\na robust training scheme is required to prevent labeling errors from degrading\nthe generalization performance of DNNs. Current state-of-the-art methods\npresent a co-training scheme that trains dual networks using samples associated\nwith small losses. In practice, however, training two networks simultaneously\ncan burden computing resources. In this study, we propose a simple yet\neffective robust training scheme that operates by training only a single\nnetwork. During training, the proposed method generates temporal self-ensemble\nby sampling intermediate network parameters from the weight trajectory formed\nby stochastic gradient descent optimization. The loss sum evaluated with these\nself-ensembles is used to identify incorrectly labeled samples. In parallel,\nour method generates multi-view predictions by transforming an input data into\nvarious forms and considers their agreement to identify incorrectly labeled\nsamples. By combining the aforementioned metrics, we present the proposed {\\it\nself-ensemble-based robust training} (SRT) method, which can filter the samples\nwith noisy labels to reduce their influence on training. Experiments on\nwidely-used public datasets demonstrate that the proposed method achieves a\nstate-of-the-art performance in some categories without training the dual\nnetworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jun Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baik_J/0/1/0/all/0/1\">Jae Soon Baik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_T/0/1/0/all/0/1\">Tae Hwan Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jun Won Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LocVTP: Video-Text Pre-training for Temporal Localization. (arXiv:2207.10362v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10362","description":"<p>Video-Text Pre-training (VTP) aims to learn transferable representations for\nvarious downstream tasks from large-scale web videos. To date, almost all\nexisting VTP methods are limited to retrieval-based downstream tasks, e.g.,\nvideo retrieval, whereas their transfer potentials on localization-based tasks,\ne.g., temporal grounding, are under-explored. In this paper, we experimentally\nanalyze and demonstrate the incompatibility of current VTP methods with\nlocalization tasks, and propose a novel Localization-oriented Video-Text\nPre-training framework, dubbed as LocVTP. Specifically, we perform the\nfine-grained contrastive alignment as a complement to the coarse-grained one by\na clip-word correspondence discovery scheme. To further enhance the temporal\nreasoning ability of the learned feature, we propose a context projection head\nand a temporal aware contrastive loss to perceive the contextual relationships.\nExtensive experiments on four downstream tasks across six datasets demonstrate\nthat our LocVTP achieves state-of-the-art performance on both retrieval-based\nand localization-based tasks. Furthermore, we conduct comprehensive ablation\nstudies and thorough analyses to explore the optimum model designs and training\nstrategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Junwu Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Land Classification in Satellite Images by Injecting Traditional Features to CNN Models. (arXiv:2207.10368v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10368","description":"<p>Deep learning methods have been successfully applied to remote sensing\nproblems for several years. Among these methods, CNN based models have high\naccuracy in solving the land classification problem using satellite or aerial\nimages. Although these models have high accuracy, this generally comes with\nlarge memory size requirements. On the other hand, it is desirable to have\nsmall-sized models for applications, such as the ones implemented on unmanned\naerial vehicles, with low memory space. Unfortunately, small-sized CNN models\ndo not provide high accuracy as with their large-sized versions. In this study,\nwe propose a novel method to improve the accuracy of CNN models, especially the\nones with small size, by injecting traditional features to them. To test the\neffectiveness of the proposed method, we applied it to the CNN models\nSqueezeNet, MobileNetV2, ShuffleNetV2, VGG16, and ResNet50V2 having size 0.5 MB\nto 528 MB. We used the sample mean, gray level co-occurrence matrix features,\nHu moments, local binary patterns, histogram of oriented gradients, and color\ninvariants as traditional features for injection. We tested the proposed method\non the EuroSAT dataset to perform land classification. Our experimental results\nshow that the proposed method significantly improves the land classification\naccuracy especially when applied to small-sized CNN models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aksoy_M/0/1/0/all/0/1\">Mehmet Cagri Aksoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirmacek_B/0/1/0/all/0/1\">Beril Sirmacek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unsalan_C/0/1/0/all/0/1\">Cem Unsalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Saliency Query Network for Efficient Video Recognition. (arXiv:2207.10379v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10379","description":"<p>Efficient video recognition is a hot-spot research topic with the explosive\ngrowth of multimedia data on the Internet and mobile devices. Most existing\nmethods select the salient frames without awareness of the class-specific\nsaliency scores, which neglect the implicit association between the saliency of\nframes and its belonging category. To alleviate this issue, we devise a novel\nTemporal Saliency Query (TSQ) mechanism, which introduces class-specific\ninformation to provide fine-grained cues for saliency measurement.\nSpecifically, we model the class-specific saliency measuring process as a\nquery-response task. For each category, the common pattern of it is employed as\na query and the most salient frames are responded to it. Then, the calculated\nsimilarities are adopted as the frame saliency scores. To achieve it, we\npropose a Temporal Saliency Query Network (TSQNet) that includes two\ninstantiations of the TSQ mechanism based on visual appearance similarities and\ntextual event-object relations. Afterward, cross-modality interactions are\nimposed to promote the information exchange between them. Finally, we use the\nclass-specific saliencies of the most confident categories generated by two\nmodalities to perform the selection of salient frames. Extensive experiments\ndemonstrate the effectiveness of our method by achieving state-of-the-art\nresults on ActivityNet, FCVID and Mini-Kinetics datasets. Our project page is\nat https://lawrencexia2008.github.io/projects/tsqnet .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Boyang Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose for Everything: Towards Category-Agnostic Pose Estimation. (arXiv:2207.10387v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10387","description":"<p>Existing works on 2D pose estimation mainly focus on a certain category, e.g.\nhuman, animal, and vehicle. However, there are lots of application scenarios\nthat require detecting the poses/keypoints of the unseen class of objects. In\nthis paper, we introduce the task of Category-Agnostic Pose Estimation (CAPE),\nwhich aims to create a pose estimation model capable of detecting the pose of\nany class of object given only a few samples with keypoint definition. To\nachieve this goal, we formulate the pose estimation problem as a keypoint\nmatching problem and design a novel CAPE framework, termed POse Matching\nNetwork (POMNet). A transformer-based Keypoint Interaction Module (KIM) is\nproposed to capture both the interactions among different keypoints and the\nrelationship between the support and query images. We also introduce\nMulti-category Pose (MP-100) dataset, which is a 2D pose dataset of 100 object\ncategories containing over 20K instances and is well-designed for developing\nCAPE algorithms. Experiments show that our method outperforms other baseline\napproaches by a large margin. Codes and data are available at\nhttps://github.com/luminxu/Pose-for-Everything.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lumin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition. (arXiv:2207.10388v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10388","description":"<p>It is challenging for artificial intelligence systems to achieve accurate\nvideo recognition under the scenario of low computation costs. Adaptive\ninference based efficient video recognition methods typically preview videos\nand focus on salient parts to reduce computation costs. Most existing works\nfocus on complex networks learning with video classification based objectives.\nTaking all frames as positive samples, few of them pay attention to the\ndiscrimination between positive samples (salient frames) and negative samples\n(non-salient frames) in supervisions. To fill this gap, in this paper, we\npropose a novel Non-saliency Suppression Network (NSNet), which effectively\nsuppresses the responses of non-salient frames. Specifically, on the frame\nlevel, effective pseudo labels that can distinguish between salient and\nnon-salient frames are generated to guide the frame saliency learning. On the\nvideo level, a temporal attention module is learned under dual video-level\nsupervisions on both the salient and the non-salient representations. Saliency\nmeasurements from both two levels are combined for exploitation of\nmulti-granularity complementary information. Extensive experiments conducted on\nfour well-known benchmarks verify our NSNet not only achieves the\nstate-of-the-art accuracy-efficiency trade-off but also present a significantly\nfaster (2.4~4.3x) practical inference speed than state-of-the-art methods. Our\nproject page is at https://lawrencexia2008.github.io/projects/nsnet .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Boyang Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1\">Rui Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haosen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaoran Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Error Compensation Framework for Flow-Guided Video Inpainting. (arXiv:2207.10391v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10391","description":"<p>The key to video inpainting is to use correlation information from as many\nreference frames as possible. Existing flow-based propagation methods split the\nvideo synthesis process into multiple steps: flow completion -&gt; pixel\npropagation -&gt; synthesis. However, there is a significant drawback that the\nerrors in each step continue to accumulate and amplify in the next step. To\nthis end, we propose an Error Compensation Framework for Flow-guided Video\nInpainting (ECFVI), which takes advantage of the flow-based method and offsets\nits weaknesses. We address the weakness with the newly designed flow completion\nmodule and the error compensation network that exploits the error guidance map.\nOur approach greatly improves the temporal consistency and the visual quality\nof the completed videos. Experimental results show the superior performance of\nour proposed method with the speed up of x6, compared to the state-of-the-art\nmethods. In addition, we present a new benchmark dataset for evaluation by\nsupplementing the weaknesses of existing test datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaeyeon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seoung Wug Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seon Joo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FADE: Fusing the Assets of Decoder and Encoder for Task-Agnostic Upsampling. (arXiv:2207.10392v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10392","description":"<p>We consider the problem of task-agnostic feature upsampling in dense\nprediction where an upsampling operator is required to facilitate both\nregion-sensitive tasks like semantic segmentation and detail-sensitive tasks\nsuch as image matting. Existing upsampling operators often can work well in\neither type of the tasks, but not both. In this work, we present FADE, a novel,\nplug-and-play, and task-agnostic upsampling operator. FADE benefits from three\ndesign choices: i) considering encoder and decoder features jointly in\nupsampling kernel generation; ii) an efficient semi-shift convolutional\noperator that enables granular control over how each feature point contributes\nto upsampling kernels; iii) a decoder-dependent gating mechanism for enhanced\ndetail delineation. We first study the upsampling properties of FADE on toy\ndata and then evaluate it on large-scale semantic segmentation and image\nmatting. In particular, FADE reveals its effectiveness and task-agnostic\ncharacteristic by consistently outperforming recent dynamic upsampling\noperators in different tasks. It also generalizes well across convolutional and\ntransformer architectures with little computational overhead. Our work\nadditionally provides thoughtful insights on what makes for task-agnostic\nupsampling. Code is available at: <a href=\"http://lnkiy.in/fade_in\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongtao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiguo Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sobolev Training for Implicit Neural Representations with Approximated Image Derivatives. (arXiv:2207.10395v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10395","description":"<p>Recently, Implicit Neural Representations (INRs) parameterized by neural\nnetworks have emerged as a powerful and promising tool to represent different\nkinds of signals due to its continuous, differentiable properties, showing\nsuperiorities to classical discretized representations. However, the training\nof neural networks for INRs only utilizes input-output pairs, and the\nderivatives of the target output with respect to the input, which can be\naccessed in some cases, are usually ignored. In this paper, we propose a\ntraining paradigm for INRs whose target output is image pixels, to encode image\nderivatives in addition to image values in the neural network. Specifically, we\nuse finite differences to approximate image derivatives. We show how the\ntraining paradigm can be leveraged to solve typical INRs problems, i.e., image\nregression and inverse rendering, and demonstrate this training paradigm can\nimprove the data-efficiency and generalization capabilities of INRs. The code\nof our method is available at\n\\url{https://github.com/megvii-research/Sobolev_INRs}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Wentao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingtian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yikang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D2-TPred: Discontinuous Dependency for Trajectory Prediction under Traffic Lights. (arXiv:2207.10398v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10398","description":"<p>A profound understanding of inter-agent relationships and motion behaviors is\nimportant to achieve high-quality planning when navigating in complex\nscenarios, especially at urban traffic intersections. We present a trajectory\nprediction approach with respect to traffic lights, D2-TPred, which uses a\nspatial dynamic interaction graph (SDG) and a behavior dependency graph (BDG)\nto handle the problem of discontinuous dependency in the spatial-temporal\nspace. Specifically, the SDG is used to capture spatial interactions by\nreconstructing sub-graphs for different agents with dynamic and changeable\ncharacteristics during each frame. The BDG is used to infer motion tendency by\nmodeling the implicit dependency of the current state on priors behaviors,\nespecially the discontinuous motions corresponding to acceleration,\ndeceleration, or turning direction. Moreover, we present a new dataset for\nvehicle trajectory prediction under traffic lights called VTP-TL. Our\nexperimental results show that our model achieves more than {20.45% and 20.78%\n}improvement in terms of ADE and FDE, respectively, on VTP-TL as compared to\nother trajectory prediction algorithms. The dataset and code are available at:\nhttps://github.com/VTP-TL/D2-TPred.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wentong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weizhi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_P/0/1/0/all/0/1\">Pei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correspondence Matters for Video Referring Expression Comprehension. (arXiv:2207.10400v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10400","description":"<p>We investigate the problem of video Referring Expression Comprehension (REC),\nwhich aims to localize the referent objects described in the sentence to visual\nregions in the video frames. Despite the recent progress, existing methods\nsuffer from two problems: 1) inconsistent localization results across video\nframes; 2) confusion between the referent and contextual objects. To this end,\nwe propose a novel Dual Correspondence Network (dubbed as DCNet) which\nexplicitly enhances the dense associations in both the inter-frame and\ncross-modal manners. Firstly, we aim to build the inter-frame correlations for\nall existing instances within the frames. Specifically, we compute the\ninter-frame patch-wise cosine similarity to estimate the dense alignment and\nthen perform the inter-frame contrastive learning to map them close in feature\nspace. Secondly, we propose to build the fine-grained patch-word alignment to\nassociate each patch with certain words. Due to the lack of this kind of\ndetailed annotations, we also predict the patch-word correspondence through the\ncosine similarity. Extensive experiments demonstrate that our DCNet achieves\nstate-of-the-art performance on both video and image REC benchmarks.\nFurthermore, we conduct comprehensive ablation studies and thorough analyses to\nexplore the optimal model designs. Notably, our inter-frame and cross-modal\ncontrastive losses are plug-and-play functions and are applicable to any video\nREC architectures. For example, by building on top of Co-grounding, we boost\nthe performance by 1.48% absolute improvement on Accu.@0.5 for VID-Sentence\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Ji Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Deepfake by Creating Spatio-Temporal Regularity Disruption. (arXiv:2207.10402v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10402","description":"<p>Despite encouraging progress in deepfake detection, generalization to unseen\nforgery types remains a significant challenge due to the limited forgery clues\nexplored during training. In contrast, we notice a common phenomenon in\ndeepfake: fake video creation inevitably disrupts the statistical regularity in\noriginal videos. Inspired by this observation, we propose to boost the\ngeneralization of deepfake detection by distinguishing the \"regularity\ndisruption\" that does not appear in real videos. Specifically, by carefully\nexamining the spatial and temporal properties, we propose to disrupt a real\nvideo through a Pseudo-fake Generator and create a wide range of pseudo-fake\nvideos for training. Such practice allows us to achieve deepfake detection\nwithout using fake videos and improves the generalization ability in a simple\nand efficient manner. To jointly capture the spatial and temporal disruptions,\nwe propose a Spatio-Temporal Enhancement block to learn the regularity\ndisruption across space and time on our self-created videos. Through\ncomprehensive experiments, our method exhibits excellent performance on several\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jiazhi Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Youjian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-aware Modular Capsule Routing for Visual Question Answering. (arXiv:2207.10404v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10404","description":"<p>Visual Question Answering (VQA) is fundamentally compositional in nature, and\nmany questions are simply answered by decomposing them into modular\nsub-problems. The recent proposed Neural Module Network (NMN) employ this\nstrategy to question answering, whereas heavily rest with off-the-shelf layout\nparser or additional expert policy regarding the network architecture design\ninstead of learning from the data. These strategies result in the\nunsatisfactory adaptability to the semantically-complicated variance of the\ninputs, thereby hindering the representational capacity and generalizability of\nthe model. To tackle this problem, we propose a Semantic-aware modUlar caPsulE\nRouting framework, termed as SUPER, to better capture the instance-specific\nvision-semantic characteristics and refine the discriminative representations\nfor prediction. Particularly, five powerful specialized modules as well as\ndynamic routers are tailored in each layer of the SUPER network, and the\ncompact routing spaces are constructed such that a variety of customizable\nroutes can be sufficiently exploited and the vision-semantic representations\ncan be explicitly calibrated. We comparatively justify the effectiveness and\ngeneralization ability of our proposed SUPER scheme over five benchmark\ndatasets, as well as the parametric-efficient advantage. It is worth\nemphasizing that this work is not to pursue the state-of-the-art results in\nVQA. Instead, we expect that our model is responsible to provide a novel\nperspective towards architecture learning and representation calibration for\nVQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianhua Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianlong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yinwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Models for Drone vs Bird Classification. (arXiv:2207.10409v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10409","description":"<p>Drone detection has become an essential task in object detection as drone\ncosts have decreased and drone technology has improved. It is, however,\ndifficult to detect distant drones when there is weak contrast, long range, and\nlow visibility. In this work, we propose several sequence classification\narchitectures to reduce the detected false-positive ratio of drone tracks.\nMoreover, we propose a new drone vs. bird sequence classification dataset to\ntrain and evaluate the proposed architectures. 3D CNN, LSTM, and Transformer\nbased sequence classification architectures have been trained on the proposed\ndataset to show the effectiveness of the proposed idea. As experiments show,\nusing sequence information, bird classification and overall F1 scores can be\nincreased by up to 73% and 35%, respectively. Among all sequence classification\nmodels, R(2+1)D-based fully convolutional model yields the best transfer\nlearning and fine-tuning results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyon_F/0/1/0/all/0/1\">Fatih Cagatay Akyon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akagunduz_E/0/1/0/all/0/1\">Erdem Akagunduz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altinuc_S/0/1/0/all/0/1\">Sinan Onur Altinuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1\">Alptekin Temizel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KD-MVS: Knowledge Distillation Based Self-supervised Learning for MVS. (arXiv:2207.10425v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10425","description":"<p>Supervised multi-view stereo (MVS) methods have achieved remarkable progress\nin terms of reconstruction quality, but suffer from the challenge of collecting\nlarge-scale ground-truth depth. In this paper, we propose a novel\nself-supervised training pipeline for MVS based on knowledge distillation,\ntermed \\textit{KD-MVS}, which mainly consists of self-supervised teacher\ntraining and distillation-based student training. Specifically, the teacher\nmodel is trained in a self-supervised fashion using both photometric and\nfeaturemetric consistency. Then we distill the knowledge of the teacher model\nto the student model through probabilistic knowledge transferring. With the\nsupervision of validated knowledge, the student model is able to outperform its\nteacher by a large margin. Extensive experiments performed on multiple datasets\nshow our method can even outperform supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yikang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingtian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Wentao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">CHi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StreamYOLO: Real-time Object Detection for Streaming Perception. (arXiv:2207.10433v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10433","description":"<p>The perceptive models of autonomous driving require fast inference within a\nlow latency for safety. While existing works ignore the inevitable\nenvironmental changes after processing, streaming perception jointly evaluates\nthe latency and accuracy into a single metric for video online perception,\nguiding the previous works to search trade-offs between accuracy and speed. In\nthis paper, we explore the performance of real time models on this metric and\nendow the models with the capacity of predicting the future, significantly\nimproving the results for streaming perception. Specifically, we build a simple\nframework with two effective modules. One is a Dual Flow Perception module\n(DFP). It consists of dynamic flow and static flow in parallel to capture\nmoving tendency and basic detection feature, respectively. Trend Aware Loss\n(TAL) is the other module which adaptively generates loss weight for each\nobject with its moving speed. Realistically, we consider multiple velocities\ndriving scene and further propose Velocity-awared streaming AP (VsAP) to\njointly evaluate the accuracy. In this realistic setting, we design a efficient\nmix-velocity training strategy to guide detector perceive any velocities. Our\nsimple method achieves the state-of-the-art performance on Argoverse-HD dataset\nand improves the sAP and VsAP by 4.7% and 8.2% respectively compared to the\nstrong baseline, validating its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DC-ShadowNet: Single-Image Hard and Soft Shadow Removal Using Unsupervised Domain-Classifier Guided Network. (arXiv:2207.10434v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10434","description":"<p>Shadow removal from a single image is generally still an open problem. Most\nexisting learning-based methods use supervised learning and require a large\nnumber of paired images (shadow and corresponding non-shadow images) for\ntraining. A recent unsupervised method, Mask-ShadowGAN, addresses this\nlimitation. However, it requires a binary mask to represent shadow regions,\nmaking it inapplicable to soft shadows. To address the problem, in this paper,\nwe propose an unsupervised domain-classifier guided shadow removal network,\nDC-ShadowNet. Specifically, we propose to integrate a shadow/shadow-free domain\nclassifier into a generator and its discriminator, enabling them to focus on\nshadow regions. To train our network, we introduce novel losses based on\nphysics-based shadow-free chromaticity, shadow-robust perceptual features, and\nboundary smoothness. Moreover, we show that our unsupervised network can be\nused for test-time training that further improves the results. Our experiments\nshow that all these novel components allow our method to handle soft shadows,\nand also to perform better on hard shadows both quantitatively and\nqualitatively than the existing state-of-the-art shadow removal methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yeying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Aashish Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1\">Robby T. Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Trajectory Prediction via Neural Social Physics. (arXiv:2207.10435v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10435","description":"<p>Trajectory prediction has been widely pursued in many fields, and many\nmodel-based and model-free methods have been explored. The former include\nrule-based, geometric or optimization-based models, and the latter are mainly\ncomprised of deep learning approaches. In this paper, we propose a new method\ncombining both methodologies based on a new Neural Differential Equation model.\nOur new model (Neural Social Physics or NSP) is a deep neural network within\nwhich we use an explicit physics model with learnable parameters. The explicit\nphysics model serves as a strong inductive bias in modeling pedestrian\nbehaviors, while the rest of the network provides a strong data-fitting\ncapability in terms of system parameter estimation and dynamics stochasticity\nmodeling. We compare NSP with 15 recent deep learning methods on 6 datasets and\nimprove the state-of-the-art performance by 5.56%-70%. Besides, we show that\nNSP has better generalizability in predicting plausible trajectories in\ndrastically different scenarios where the density is 2-5 times as high as the\ntesting data. Finally, we show that the physics model in NSP can provide\nplausible explanations for pedestrian behaviors, as opposed to black-box deep\nlearning. Code is available:\nhttps://github.com/realcrane/Human-Trajectory-Prediction-via-Neural-Social-Physics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1\">Jiangbei Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Relations among Cross-Frame Affinities for Video Semantic Segmentation. (arXiv:2207.10436v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10436","description":"<p>The essence of video semantic segmentation (VSS) is how to leverage temporal\ninformation for prediction. Previous efforts are mainly devoted to developing\nnew techniques to calculate the cross-frame affinities such as optical flow and\nattention. Instead, this paper contributes from a different angle by mining\nrelations among cross-frame affinities, upon which better temporal information\naggregation could be achieved. We explore relations among affinities in two\naspects: single-scale intrinsic correlations and multi-scale relations.\nInspired by traditional feature processing, we propose Single-scale Affinity\nRefinement (SAR) and Multi-scale Affinity Aggregation (MAA). To make it\nfeasible to execute MAA, we propose a Selective Token Masking (STM) strategy to\nselect a subset of consistent reference tokens for different scales when\ncalculating affinities, which also improves the efficiency of our method. At\nlast, the cross-frame affinities strengthened by SAR and MAA are adopted for\nadaptively aggregating temporal information. Our experiments demonstrate that\nthe proposed method performs favorably against state-of-the-art VSS methods.\nThe code is publicly available at https://github.com/GuoleiSun/VSS-MRCFA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guolei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1\">Ajad Chhatkuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COBRA: Cpu-Only aBdominal oRgan segmentAtion. (arXiv:2207.10446v1 [eess.IV])","link":"http://arxiv.org/abs/2207.10446","description":"<p>Abdominal organ segmentation is a difficult and time-consuming task. To\nreduce the burden on clinical experts, fully-automated methods are highly\ndesirable. Current approaches are dominated by Convolutional Neural Networks\n(CNNs) however the computational requirements and the need for large data sets\nlimit their application in practice. By implementing a small and efficient\ncustom 3D CNN, compiling the trained model and optimizing the computational\ngraph: our approach produces high accuracy segmentations (Dice Similarity\nCoefficient (%): Liver: 97.3$\\pm$1.3, Kidneys: 94.8$\\pm$3.6, Spleen:\n96.4$\\pm$3.0, Pancreas: 80.9$\\pm$10.1) at a rate of 1.6 seconds per image.\nCrucially, we are able to perform segmentation inference solely on CPU (no GPU\nrequired), thereby facilitating easy and widespread deployment of the model\nwithout specialist hardware.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Henderson_E/0/1/0/all/0/1\">Edward G. A. Henderson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McSweeney_D/0/1/0/all/0/1\">D&#xf3;nal M. McSweeney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Green_A/0/1/0/all/0/1\">Andrew F. Green</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Object Localization via Transformer with Implicit Spatial Calibration. (arXiv:2207.10447v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10447","description":"<p>Weakly Supervised Object Localization (WSOL), which aims to localize objects\nby only using image-level labels, has attracted much attention because of its\nlow annotation cost in real applications. Recent studies leverage the advantage\nof self-attention in visual Transformer for long-range dependency to re-active\nsemantic regions, aiming to avoid partial activation in traditional class\nactivation mapping (CAM). However, the long-range modeling in Transformer\nneglects the inherent spatial coherence of the object, and it usually diffuses\nthe semantic-aware regions far from the object boundary, making localization\nresults significantly larger or far smaller. To address such an issue, we\nintroduce a simple yet effective Spatial Calibration Module (SCM) for accurate\nWSOL, incorporating semantic similarities of patch tokens and their spatial\nrelationships into a unified diffusion model. Specifically, we introduce a\nlearnable parameter to dynamically adjust the semantic correlations and spatial\ncontext intensities for effective information propagation. In practice, SCM is\ndesigned as an external module of Transformer, and can be removed during\ninference to reduce the computation cost. The object-sensitive localization\nability is implicitly embedded into the Transformer encoder through\noptimization in the training phase. It enables the generated attention maps to\ncapture the sharper object boundaries and filter the object-irrelevant\nbackground area. Extensive experimental results demonstrate the effectiveness\nof the proposed method, which significantly outperforms its counterpart TS-CAM\non both CUB-200 and ImageNet-1K benchmarks. The code is available at\nhttps://github.<a href=\"/abs/com/1641407\">com/1641407</a>57/SCM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haotian Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Spatio-Temporal Pyramid Transformer for Action Detection. (arXiv:2207.10448v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10448","description":"<p>The task of action detection aims at deducing both the action category and\nlocalization of the start and end moment for each action instance in a long,\nuntrimmed video. While vision Transformers have driven the recent advances in\nvideo understanding, it is non-trivial to design an efficient architecture for\naction detection due to the prohibitively expensive self-attentions over a long\nsequence of video clips. To this end, we present an efficient hierarchical\nSpatio-Temporal Pyramid Transformer (STPT) for action detection, building upon\nthe fact that the early self-attention layers in Transformers still focus on\nlocal patterns. Specifically, we propose to use local window attention to\nencode rich local spatio-temporal representations in the early stages while\napplying global attention modules to capture long-term space-time dependencies\nin the later stages. In this way, our STPT can encode both locality and\ndependency with largely reduced redundancy, delivering a promising trade-off\nbetween accuracy and efficiency. For example, with only RGB input, the proposed\nSTPT achieves 53.6% mAP on THUMOS14, surpassing I3D+AFSD RGB model by over 10%\nand performing favorably against state-of-the-art AFSD that uses additional\nflow features with 31% fewer GFLOPs, which serves as an effective and efficient\nend-to-end Transformer-based framework for action detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yuetian Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mingfei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Magic ELF: Image Deraining Meets Association Learning and Transformer. (arXiv:2207.10455v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10455","description":"<p>Convolutional neural network (CNN) and Transformer have achieved great\nsuccess in multimedia applications. However, little effort has been made to\neffectively and efficiently harmonize these two architectures to satisfy image\nderaining. This paper aims to unify these two architectures to take advantage\nof their learning merits for image deraining. In particular, the local\nconnectivity and translation equivariance of CNN and the global aggregation\nability of self-attention (SA) in Transformer are fully exploited for specific\nlocal context and global structure representations. Based on the observation\nthat rain distribution reveals the degradation location and degree, we\nintroduce degradation prior to help background recovery and accordingly present\nthe association refinement deraining scheme. A novel multi-input attention\nmodule (MAM) is proposed to associate rain perturbation removal and background\nrecovery. Moreover, we equip our model with effective depth-wise separable\nconvolutions to learn the specific feature representations and trade off\ncomputational complexity. Extensive experiments show that our proposed method\n(dubbed as ELF) outperforms the state-of-the-art approach (MPRNet) by 0.25 dB\non average, but only accounts for 11.7\\% and 42.1\\% of its computational cost\nand parameters. The source code is available at\nhttps://github.com/kuijiang94/Magic-ELF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Laizhong Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Aware Fine-Grained Correspondence. (arXiv:2207.10456v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10456","description":"<p>Establishing visual correspondence across images is a challenging and\nessential task. Recently, an influx of self-supervised methods have been\nproposed to better learn representations for visual correspondence. However, we\nfind that these methods often fail to leverage semantic information and\nover-rely on the matching of low-level features. In contrast, human vision is\ncapable of distinguishing between distinct objects as a pretext to tracking.\nInspired by this paradigm, we propose to learn semantic-aware fine-grained\ncorrespondence. Firstly, we demonstrate that semantic correspondence is\nimplicitly available through a rich set of image-level self-supervised methods.\nWe further design a pixel-level self-supervised learning objective which\nspecifically targets fine-grained correspondence. For downstream tasks, we fuse\nthese two kinds of complementary correspondence representations together,\ndemonstrating that they boost performance synergistically. Our method surpasses\nprevious state-of-the-art self-supervised methods using convolutional networks\non a variety of visual correspondence tasks, including video object\nsegmentation, human pose tracking, and human part tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yingdong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaifeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Data Driven Estimation of Cluster Number in Multiplex Images using Embedded Density Outliers. (arXiv:2207.10469v1 [cs.LG])","link":"http://arxiv.org/abs/2207.10469","description":"<p>The usage of chemical imaging technologies is becoming a routine\naccompaniment to traditional methods in pathology. Significant technological\nadvances have developed these next generation techniques to provide rich,\nspatially resolved, multidimensional chemical images. The rise of digital\npathology has significantly enhanced the synergy of these imaging modalities\nwith optical microscopy and immunohistochemistry, enhancing our understanding\nof the biological mechanisms and progression of diseases. Techniques such as\nimaging mass cytometry provide labelled multidimensional (multiplex) images of\nspecific components used in conjunction with digital pathology techniques.\nThese powerful techniques generate a wealth of high dimensional data that\ncreate significant challenges in data analysis. Unsupervised methods such as\nclustering are an attractive way to analyse these data, however, they require\nthe selection of parameters such as the number of clusters. Here we propose a\nmethodology to estimate the number of clusters in an automatic data-driven\nmanner using a deep sparse autoencoder to embed the data into a lower\ndimensional space. We compute the density of regions in the embedded space, the\nmajority of which are empty, enabling the high density regions to be detected\nas outliers and provide an estimate for the number of clusters. This framework\nprovides a fully unsupervised and data-driven method to analyse\nmultidimensional data. In this work we demonstrate our method using 45\nmultiplex imaging mass cytometry datasets. Moreover, our model is trained using\nonly one of the datasets and the learned embedding is applied to the remaining\n44 images providing an efficient process for data analysis. Finally, we\ndemonstrate the high computational efficiency of our method which is two orders\nof magnitude faster than estimating via computing the sum squared distances as\na function of cluster number.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Spencer A. Thomas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LPYOLO: Low Precision YOLO for Face Detection on FPGA. (arXiv:2207.10482v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10482","description":"<p>In recent years, number of edge computing devices and artificial intelligence\napplications on them have advanced excessively. In edge computing, decision\nmaking processes and computations are moved from servers to edge devices.\nHence, cheap and low power devices are required. FPGAs are very low power,\ninclined to do parallel operations and deeply suitable devices for running\nConvolutional Neural Networks (CNN) which are the fundamental unit of an\nartificial intelligence application. Face detection on surveillance systems is\nthe most expected application on the security market. In this work, TinyYolov3\narchitecture is redesigned and deployed for face detection. It is a CNN based\nobject detection method and developed for embedded systems. PYNQ-Z2 is selected\nas a target board which has low-end Xilinx Zynq 7020 System-on-Chip (SoC) on\nit. Redesigned TinyYolov3 model is defined in numerous bit width precisions\nwith Brevitas library which brings fundamental CNN layers and activations in\ninteger quantized form. Then, the model is trained in a quantized structure\nwith WiderFace dataset. In order to decrease latency and power consumption,\nonchip memory of the FPGA is configured as a storage of whole network\nparameters and the last activation function is modified as rescaled HardTanh\ninstead of Sigmoid. Also, high degree of parallelism is applied to logical\nresources of the FPGA. The model is converted to an HLS based application with\nusing FINN framework and FINN-HLS library which includes the layer definitions\nin C++. Later, the model is synthesized and deployed. CPU of the SoC is\nemployed with multithreading mechanism and responsible for preprocessing,\npostprocessing and TCP/IP streaming operations. Consequently, 2.4 Watt total\nboard power consumption, 18 Frames-Per-Second (FPS) throughput and 0.757 mAP\naccuracy rate on Easy category of the WiderFace are achieved with 4 bits\nprecision model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunay_B/0/1/0/all/0/1\">Bestami G&#xfc;nay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okcu_S/0/1/0/all/0/1\">Sefa Burak Okcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilge_H/0/1/0/all/0/1\">Hasan &#x15e;akir Bilge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Confident Detection of Prostate Cancer using High Resolution Micro-ultrasound. (arXiv:2207.10485v1 [eess.IV])","link":"http://arxiv.org/abs/2207.10485","description":"<p>MOTIVATION: Detection of prostate cancer during transrectal ultrasound-guided\nbiopsy is challenging. The highly heterogeneous appearance of cancer, presence\nof ultrasound artefacts, and noise all contribute to these difficulties. Recent\nadvancements in high-frequency ultrasound imaging - micro-ultrasound - have\ndrastically increased the capability of tissue imaging at high resolution. Our\naim is to investigate the development of a robust deep learning model\nspecifically for micro-ultrasound-guided prostate cancer biopsy. For the model\nto be clinically adopted, a key challenge is to design a solution that can\nconfidently identify the cancer, while learning from coarse histopathology\nmeasurements of biopsy samples that introduce weak labels. METHODS: We use a\ndataset of micro-ultrasound images acquired from 194 patients, who underwent\nprostate biopsy. We train a deep model using a co-teaching paradigm to handle\nnoise in labels, together with an evidential deep learning method for\nuncertainty estimation. We evaluate the performance of our model using the\nclinically relevant metric of accuracy vs. confidence. RESULTS: Our model\nachieves a well-calibrated estimation of predictive uncertainty with area under\nthe curve of 88$\\%$. The use of co-teaching and evidential deep learning in\ncombination yields significantly better uncertainty estimation than either\nalone. We also provide a detailed comparison against state-of-the-art in\nuncertainty estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gilany_M/0/1/0/all/0/1\">Mahdi Gilany</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilson_P/0/1/0/all/0/1\">Paul Wilson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jamzad_A/0/1/0/all/0/1\">Amoon Jamzad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fooladgar_F/0/1/0/all/0/1\">Fahimeh Fooladgar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+To_M/0/1/0/all/0/1\">Minh Nguyen Nhat To</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wodlinger_B/0/1/0/all/0/1\">Brian Wodlinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abolmaesumi_P/0/1/0/all/0/1\">Purang Abolmaesumi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mousavi_P/0/1/0/all/0/1\">Parvin Mousavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Localisation and Colored Mesh Reconstruction Architecture for 3D Visual Feedback in Robotic Exploration Missions. (arXiv:2207.10489v1 [cs.RO])","link":"http://arxiv.org/abs/2207.10489","description":"<p>This paper introduces an Online Localisation and Colored Mesh Reconstruction\n(OLCMR) ROS perception architecture for ground exploration robots aiming to\nperform robust Simultaneous Localisation And Mapping (SLAM) in challenging\nunknown environments and provide an associated colored 3D mesh representation\nin real time. It is intended to be used by a remote human operator to easily\nvisualise the mapped environment during or after the mission or as a\ndevelopment base for further researches in the field of exploration robotics.\nThe architecture is mainly composed of carefully-selected open-source ROS\nimplementations of a LiDAR-based SLAM algorithm alongside a colored surface\nreconstruction procedure using a point cloud and RGB camera images projected\ninto the 3D space. The overall performances are evaluated on the Newer College\nhandheld LiDAR-Vision reference dataset and on two experimental trajectories\ngathered on board of representative wheeled robots in respectively urban and\ncountryside outdoor environments. Index Terms: Field Robots, Mapping, SLAM,\nColored Surface Reconstruction\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serdel_Q/0/1/0/all/0/1\">Quentin Serdel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grand_C/0/1/0/all/0/1\">Christophe Grand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzat_J/0/1/0/all/0/1\">Julien Marzat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moras_J/0/1/0/all/0/1\">Julien Moras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Event-Camera Depth Estimation and Outlier Rejection by Refocused Events Fusion. (arXiv:2207.10494v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10494","description":"<p>Event cameras are bio-inspired sensors that offer advantages over traditional\ncameras. They work asynchronously, sampling the scene with microsecond\nresolution and producing a stream of brightness changes. This unconventional\noutput has sparked novel computer vision methods to unlock the camera's\npotential. We tackle the problem of event-based stereo 3D reconstruction for\nSLAM. Most event-based stereo methods try to exploit the camera's high temporal\nresolution and event simultaneity across cameras to establish matches and\nestimate depth. By contrast, we investigate how to estimate depth without\nexplicit data association by fusing Disparity Space Images (DSIs) originated in\nefficient monocular methods. We develop fusion theory and apply it to design\nmulti-camera 3D reconstruction algorithms that produce state-of-the-art\nresults, as we confirm by comparing against four baseline methods and testing\non a variety of available datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Suman Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Guillermo Gallego</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Efficient Adversarial Training on Vision Transformers. (arXiv:2207.10498v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10498","description":"<p>Vision Transformer (ViT), as a powerful alternative to Convolutional Neural\nNetwork (CNN), has received much attention. Recent work showed that ViTs are\nalso vulnerable to adversarial examples like CNNs. To build robust ViTs, an\nintuitive way is to apply adversarial training since it has been shown as one\nof the most effective ways to accomplish robust CNNs. However, one major\nlimitation of adversarial training is its heavy computational cost. The\nself-attention mechanism adopted by ViTs is a computationally intense operation\nwhose expense increases quadratically with the number of input patches, making\nadversarial training on ViTs even more time-consuming. In this work, we first\ncomprehensively study fast adversarial training on a variety of vision\ntransformers and illustrate the relationship between the efficiency and\nrobustness. Then, to expediate adversarial training on ViTs, we propose an\nefficient Attention Guided Adversarial Training mechanism. Specifically,\nrelying on the specialty of self-attention, we actively remove certain patch\nembeddings of each layer with an attention-guided dropping strategy during\nadversarial training. The slimmed self-attention modules accelerate the\nadversarial training on ViTs significantly. With only 65\\% of the fast\nadversarial training time, we match the state-of-the-art results on the\nchallenging ImageNet benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Boxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jindong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Retinal Image Registration Using a Keypoint-Based Vessel Structure Aligning Network. (arXiv:2207.10506v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10506","description":"<p>In ophthalmological imaging, multiple imaging systems, such as color fundus,\ninfrared, fluorescein angiography, optical coherence tomography (OCT) or OCT\nangiography, are often involved to make a diagnosis of retinal disease.\nMulti-modal retinal registration techniques can assist ophthalmologists by\nproviding a pixel-based comparison of aligned vessel structures in images from\ndifferent modalities or acquisition times. To this end, we propose an\nend-to-end trainable deep learning method for multi-modal retinal image\nregistration. Our method extracts convolutional features from the vessel\nstructure for keypoint detection and description and uses a graph neural\nnetwork for feature matching. The keypoint detection and description network\nand graph neural network are jointly trained in a self-supervised manner using\nsynthetic multi-modal image pairs and are guided by synthetically sampled\nground truth homographies. Our method demonstrates higher registration accuracy\nas competing methods for our synthetic retinal dataset and generalizes well for\nour real macula dataset and a public fundus dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sindel_A/0/1/0/all/0/1\">Aline Sindel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohberger_B/0/1/0/all/0/1\">Bettina Hohberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christlein_V/0/1/0/all/0/1\">Vincent Christlein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Elderly Monitoring for Senior Safety by Lightweight Human Action Recognition. (arXiv:2207.10519v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10519","description":"<p>With an increasing number of elders living alone, care-giving from a distance\nbecomes a compelling need, particularly for safety. Real-time monitoring and\naction recognition are essential to raise an alert timely when abnormal\nbehaviors or unusual activities occur. While wearable sensors are widely\nrecognized as a promising solution, highly depending on user's ability and\nwillingness makes them inefficient. In contrast, video streams collected\nthrough non-contact optical cameras provide richer information and release the\nburden on elders. In this paper, leveraging the Independently-Recurrent neural\nNetwork (IndRNN) we propose a novel Real-time Elderly Monitoring for senior\nSafety (REMS) based on lightweight human action recognition (HAR) technology.\nUsing captured skeleton images, the REMS scheme is able to recognize abnormal\nbehaviors or actions and preserve the user's privacy. To achieve high accuracy,\nthe HAR module is trained and fine-tuned using multiple databases. An extensive\nexperimental study verified that REMS system performs action recognition\naccurately and timely. REMS meets the design goals as a privacy-preserving\nelderly safety monitoring system and possesses the potential to be adopted in\nvarious smart monitoring systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Han Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Network Learning of Chemical Bond Representations in Spectral Indices and Features. (arXiv:2207.10530v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10530","description":"<p>In this paper we investigate neural networks for classification in\nhyperspectral imaging with a focus on connecting the architecture of the\nnetwork with the physics of the sensing and materials present. Spectroscopy is\nthe process of measuring light reflected or emitted by a material as a function\nwavelength. Molecular bonds present in the material have vibrational\nfrequencies which affect the amount of light measured at each wavelength. Thus\nthe measured spectrum contains information about the particular chemical\nconstituents and types of bonds. For example, chlorophyll reflects more light\nin the near-IR rage (800-900nm) than in the red (625-675nm) range, and this\ndifference can be measured using a normalized vegetation difference index\n(NDVI), which is commonly used to detect vegetation presence, health, and type\nin imagery collected at these wavelengths. In this paper we show that the\nweights in a Neural Network trained on different vegetation classes learn to\nmeasure this difference in reflectance. We then show that a Neural Network\ntrained on a more complex set of ten different polymer materials will learn\nspectral 'features' evident in the weights for the network, and these features\ncan be used to reliably distinguish between the different types of polymers.\nExamination of the weights provides a human-interpretable understanding of the\nnetwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basener_B/0/1/0/all/0/1\">Bill Basener</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Primer on Topological Data Analysis to Support Image Analysis Tasks in Environmental Science. (arXiv:2207.10552v1 [cs.LG])","link":"http://arxiv.org/abs/2207.10552","description":"<p>Topological data analysis (TDA) is a tool from data science and mathematics\nthat is beginning to make waves in environmental science. In this work, we seek\nto provide an intuitive and understandable introduction to a tool from TDA that\nis particularly useful for the analysis of imagery, namely persistent homology.\nWe briefly discuss the theoretical background but focus primarily on\nunderstanding the output of this tool and discussing what information it can\nglean. To this end, we frame our discussion around a guiding example of\nclassifying satellite images from the Sugar, Fish, Flower, and Gravel Dataset\nproduced for the study of mesocale organization of clouds by Rasp et. al. in\n2020 (arXiv:1906:01906). We demonstrate how persistent homology and its\nvectorization, persistence landscapes, can be used in a workflow with a simple\nmachine learning algorithm to obtain good results, and explore in detail how we\ncan explain this behavior in terms of image-level features. One of the core\nstrengths of persistent homology is how interpretable it can be, so throughout\nthis paper we discuss not just the patterns we find, but why those results are\nto be expected given what we know about the theory of persistent homology. Our\ngoal is that a reader of this paper will leave with a better understanding of\nTDA and persistent homology, be able to identify problems and datasets of their\nown for which persistent homology could be helpful, and gain an understanding\nof results they obtain from applying the included GitHub example code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoef_L/0/1/0/all/0/1\">Lander Ver Hoef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_H/0/1/0/all/0/1\">Henry Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_E/0/1/0/all/0/1\">Emily J. King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebert_Uphoff_I/0/1/0/all/0/1\">Imme Ebert-Uphoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The MABe22 Benchmarks for Representation Learning of Multi-Agent Behavior. (arXiv:2207.10553v1 [cs.LG])","link":"http://arxiv.org/abs/2207.10553","description":"<p>Real-world behavior is often shaped by complex interactions between multiple\nagents. To scalably study multi-agent behavior, advances in unsupervised and\nself-supervised learning have enabled a variety of different behavioral\nrepresentations to be learned from trajectory data. To date, there does not\nexist a unified set of benchmarks that can enable comparing methods\nquantitatively and systematically across a broad set of behavior analysis\nsettings. We aim to address this by introducing a large-scale, multi-agent\ntrajectory dataset from real-world behavioral neuroscience experiments that\ncovers a range of behavior analysis tasks. Our dataset consists of trajectory\ndata from common model organisms, with 9.6 million frames of mouse data and 4.4\nmillion frames of fly data, in a variety of experimental settings, such as\ndifferent strains, lengths of interaction, and optogenetic stimulation. A\nsubset of the frames also consist of expert-annotated behavior labels.\nImprovements on our dataset corresponds to behavioral representations that work\nacross multiple organisms and is able to capture differences for common\nbehavior analysis tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer J. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulmer_A/0/1/0/all/0/1\">Andrew Ulmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1\">Dipam Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geuther_B/0/1/0/all/0/1\">Brian Geuther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_E/0/1/0/all/0/1\">Edward Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1\">Heng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vivek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Partridge_Z/0/1/0/all/0/1\">Zachary Partridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robie_A/0/1/0/all/0/1\">Alice Robie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schretter_C/0/1/0/all/0/1\">Catherine E. Schretter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheppard_K/0/1/0/all/0/1\">Keith Sheppard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uttarwar_P/0/1/0/all/0/1\">Param Uttarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branson_K/0/1/0/all/0/1\">Kristin Branson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1\">Ann Kennedy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Night Image Enhancement: When Layer Decomposition Meets Light-Effects Suppression. (arXiv:2207.10564v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10564","description":"<p>Night images suffer not only from low light, but also from uneven\ndistributions of light. Most existing night visibility enhancement methods\nfocus mainly on enhancing low-light regions. This inevitably leads to over\nenhancement and saturation in bright regions, such as those regions affected by\nlight effects (glare, floodlight, etc). To address this problem, we need to\nsuppress the light effects in bright regions while, at the same time, boosting\nthe intensity of dark regions. With this idea in mind, we introduce an\nunsupervised method that integrates a layer decomposition network and a\nlight-effects suppression network. Given a single night image as input, our\ndecomposition network learns to decompose shading, reflectance and\nlight-effects layers, guided by unsupervised layer-specific prior losses. Our\nlight-effects suppression network further suppresses the light effects and, at\nthe same time, enhances the illumination in dark regions. This light-effects\nsuppression network exploits the estimated light-effects layer as the guidance\nto focus on the light-effects regions. To recover the background details and\nreduce hallucination/artefacts, we propose structure and high-frequency\nconsistency losses. Our quantitative and qualitative evaluations on real images\nshow that our method outperforms state-of-the-art methods in suppressing night\nlight effects and boosting the intensity of dark regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yeying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1\">Robby T. Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face-to-Face Co-Located Human-Human Social Interaction Analysis using Nonverbal Cues: A Survey. (arXiv:2207.10574v1 [cs.HC])","link":"http://arxiv.org/abs/2207.10574","description":"<p>This work presents a systematic review of recent efforts (since 2010) aimed\nat automatic analysis of nonverbal cues displayed in face-to-face co-located\nhuman-human social interactions. The main reason for focusing on nonverbal cues\nis that these are the physical, machine detectable traces of social and\npsychological phenomena. Therefore, detecting and understanding nonverbal cues\nmeans, at least to a certain extent, to detect and understand social and\npsychological phenomena. The covered topics are categorized into three as: a)\nmodeling social traits, such as leadership, dominance, personality traits, b)\nsocial role recognition and social relations detection and c) interaction\ndynamics analysis in terms of group cohesion, empathy, rapport and so forth. We\ntarget the co-located interactions, in which the interactants are always\nhumans. The survey covers a wide spectrum of settings and scenarios, including\nfree-standing interactions, meetings, indoor and outdoor social exchanges,\ndyadic conversations, and crowd dynamics. For each of them, the survey\nconsiders the three main elements of nonverbal cues analysis, namely data,\nsensing approaches and computational methodologies. The goal is to highlight\nthe main advances of the last decade, to point out existing limitations, and to\noutline future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beyan_C/0/1/0/all/0/1\">Cigdem Beyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinciarelli_A/0/1/0/all/0/1\">Alessandro Vinciarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1\">Alessio Del Bue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing An Illumination-Aware Network for Deep Image Relighting. (arXiv:2207.10582v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10582","description":"<p>Lighting is a determining factor in photography that affects the style,\nexpression of emotion, and even quality of images. Creating or finding\nsatisfying lighting conditions, in reality, is laborious and time-consuming, so\nit is of great value to develop a technology to manipulate illumination in an\nimage as post-processing. Although previous works have explored techniques\nbased on the physical viewpoint for relighting images, extensive supervisions\nand prior knowledge are necessary to generate reasonable images, restricting\nthe generalization ability of these works. In contrast, we take the viewpoint\nof image-to-image translation and implicitly merge ideas of the conventional\nphysical viewpoint. In this paper, we present an Illumination-Aware Network\n(IAN) which follows the guidance from hierarchical sampling to progressively\nrelight a scene from a single image with high efficiency. In addition, an\nIllumination-Aware Residual Block (IARB) is designed to approximate the\nphysical rendering process and to extract precise descriptors of light sources\nfor further manipulations. We also introduce a depth-guided geometry encoder\nfor acquiring valuable geometry- and structure-related representations once the\ndepth information is available. Experimental results show that our proposed\nmethod produces better quantitative and qualitative relighting results than\nprevious state-of-the-art methods. The code and models are publicly available\non https://github.com/NK-CS-ZZL/IAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zuo-Liang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui-Xun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chun-Le Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting 3D Object Detection via Object-Focused Image Fusion. (arXiv:2207.10589v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10589","description":"<p>3D object detection has achieved remarkable progress by taking point clouds\nas the only input. However, point clouds often suffer from incomplete geometric\nstructures and the lack of semantic information, which makes detectors hard to\naccurately classify detected objects. In this work, we focus on how to\neffectively utilize object-level information from images to boost the\nperformance of point-based 3D detector. We present DeMF, a simple yet effective\nmethod to fuse image information into point features. Given a set of point\nfeatures and image feature maps, DeMF adaptively aggregates image features by\ntaking the projected 2D location of the 3D point as reference. We evaluate our\nmethod on the challenging SUN RGB-D dataset, improving state-of-the-art results\nby a large margin (+2.1 mAP@0.25 and +2.3mAP@0.5). Code is available at\nhttps://github.com/haoy945/DeMF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximate Differentiable Rendering with Algebraic Surfaces. (arXiv:2207.10606v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10606","description":"<p>Differentiable renderers provide a direct mathematical link between an\nobject's 3D representation and images of that object. In this work, we develop\nan approximate differentiable renderer for a compact, interpretable\nrepresentation, which we call Fuzzy Metaballs. Our approximate renderer focuses\non rendering shapes via depth maps and silhouettes. It sacrifices fidelity for\nutility, producing fast runtimes and high-quality gradient information that can\nbe used to solve vision tasks. Compared to mesh-based differentiable renderers,\nour method has forward passes that are 5x faster and backwards passes that are\n30x faster. The depth maps and silhouette images generated by our method are\nsmooth and defined everywhere. In our evaluation of differentiable renderers\nfor pose estimation, we show that our method is the only one comparable to\nclassic techniques. In shape from silhouette, our method performs well using\nonly gradient descent and a per-pixel loss, without any surrogate losses or\nregularization. These reconstructions work well even on natural video sequences\nwith segmentation artifacts. Project page:\nhttps://leonidk.github.io/fuzzy-metaballs\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keselman_L/0/1/0/all/0/1\">Leonid Keselman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebert_M/0/1/0/all/0/1\">Martial Hebert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Statistic Shape Model for Myocardium Segmentation. (arXiv:2207.10607v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10607","description":"<p>Accurate segmentation and motion estimation of myocardium have always been\nimportant in clinic field, which essentially contribute to the downstream\ndiagnosis. However, existing methods cannot always guarantee the shape\nintegrity for myocardium segmentation. In addition, motion estimation requires\npoint correspondence on the myocardium region across different frames. In this\npaper, we propose a novel end-to-end deep statistic shape model to focus on\nmyocardium segmentation with both shape integrity and boundary correspondence\npreserving. Specifically, myocardium shapes are represented by a fixed number\nof points, whose variations are extracted by Principal Component Analysis\n(PCA). Deep neural network is used to predict the transformation parameters\n(both affine and deformation), which are then used to warp the mean point cloud\nto the image domain. Furthermore, a differentiable rendering layer is\nintroduced to incorporate mask supervision into the framework to learn more\naccurate point clouds. In this way, the proposed method is able to consistently\nproduce anatomically reasonable segmentation mask without post processing.\nAdditionally, the predicted point cloud guarantees boundary correspondence for\nsequential images, which contributes to the downstream tasks, such as the\nmotion estimation of myocardium. We conduct several experiments to demonstrate\nthe effectiveness of the proposed method on several benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoling Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Eric Z. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shanhui Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dense Material Segmentation Dataset for Indoor and Outdoor Scene Parsing. (arXiv:2207.10614v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10614","description":"<p>A key algorithm for understanding the world is material segmentation, which\nassigns a label (metal, glass, etc.) to each pixel. We find that a model\ntrained on existing data underperforms in some settings and propose to address\nthis with a large-scale dataset of 3.2 million dense segments on 44,560 indoor\nand outdoor images, which is 23x more segments than existing data. Our data\ncovers a more diverse set of scenes, objects, viewpoints and materials, and\ncontains a more fair distribution of skin types. We show that a model trained\non our data outperforms a state-of-the-art model across datasets and\nviewpoints. We propose a large-scale scene parsing benchmark and baseline of\n0.729 per-pixel accuracy, 0.585 mean class accuracy and 0.420 mean IoU across\n46 materials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Upchurch_P/0/1/0/all/0/1\">Paul Upchurch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_R/0/1/0/all/0/1\">Ransen Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaComp: Learning to Adapt for Online Depth Completion. (arXiv:2207.10623v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10623","description":"<p>Relying on deep supervised or self-supervised learning, previous methods for\ndepth completion from paired single image and sparse depth data have achieved\nimpressive performance in recent years. However, facing a new environment where\nthe test data occurs online and differs from the training data in the RGB image\ncontent and depth sparsity, the trained model might suffer severe performance\ndrop. To encourage the trained model to work well in such conditions, we expect\nit to be capable of adapting to the new environment continuously and\neffectively. To achieve this, we propose MetaComp. It utilizes the\nmeta-learning technique to simulate adaptation policies during the training\nphase, and then adapts the model to new environments in a self-supervised\nmanner in testing. Considering that the input is multi-modal data, it would be\nchallenging to adapt a model to variations in two modalities simultaneously,\ndue to significant differences in structure and form of the two modal data.\nTherefore, we further propose to disentangle the adaptation procedure in the\nbasic meta-learning training into two steps, the first one focusing on the\ndepth sparsity while the second attending to the image content. During testing,\nwe take the same strategy to adapt the model online to new multi-modal data.\nExperimental results and comprehensive ablations show that our MetaComp is\ncapable of adapting to the depth completion in a new environment effectively\nand robust to changes in different modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shanshan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Liping Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dynamical Systems Algorithm for Clustering in Hyperspectral Imagery. (arXiv:2207.10625v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10625","description":"<p>In this paper we present a new dynamical systems algorithm for clustering in\nhyperspectral images. The main idea of the algorithm is that data points are\n\\`pushed\\' in the direction of increasing density and groups of pixels that end\nup in the same dense regions belong to the same class. This is essentially a\nnumerical solution of the differential equation defined by the gradient of the\ndensity of data points on the data manifold. The number of classes is automated\nand the resulting clustering can be extremely accurate. In addition to\nproviding a accurate clustering, this algorithm presents a new tool for\nunderstanding hyperspectral data in high dimensions. We evaluate the algorithm\non the Urban (Available at www.tec.ary.mil/Hypercube/) scene comparing\nperformance against the k-means algorithm using pre-identified classes of\nmaterials as ground truth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basener_W/0/1/0/all/0/1\">William F. Basener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castrodad_A/0/1/0/all/0/1\">Alexey Castrodad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messinger_D/0/1/0/all/0/1\">David Messinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahle_J/0/1/0/all/0/1\">Jennifer Mahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prue_P/0/1/0/all/0/1\">Paul Prue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Multiplane Images: Making a 2D GAN 3D-Aware. (arXiv:2207.10642v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10642","description":"<p>What is really needed to make an existing 2D GAN 3D-aware? To answer this\nquestion, we modify a classical GAN, i.e., StyleGANv2, as little as possible.\nWe find that only two modifications are absolutely necessary: 1) a multiplane\nimage style generator branch which produces a set of alpha maps conditioned on\ntheir depth; 2) a pose-conditioned discriminator. We refer to the generated\noutput as a 'generative multiplane image' (GMPI) and emphasize that its\nrenderings are not only high-quality but also guaranteed to be view-consistent,\nwhich makes GMPIs different from many prior works. Importantly, the number of\nalpha maps can be dynamically adjusted and can differ between training and\ninference, alleviating memory concerns and enabling fast training of GMPIs in\nless than half a day at a resolution of $1024^2$. Our findings are consistent\nacross three challenging and common high-resolution datasets, including FFHQ,\nAFHQv2, and MetFaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fangchang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guera_D/0/1/0/all/0/1\">David G&#xfc;era</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhile Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander G. Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colburn_A/0/1/0/all/0/1\">Alex Colburn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novel Class Discovery without Forgetting. (arXiv:2207.10659v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10659","description":"<p>Humans possess an innate ability to identify and differentiate instances that\nthey are not familiar with, by leveraging and adapting the knowledge that they\nhave acquired so far. Importantly, they achieve this without deteriorating the\nperformance on their earlier learning. Inspired by this, we identify and\nformulate a new, pragmatic problem setting of NCDwF: Novel Class Discovery\nwithout Forgetting, which tasks a machine learning model to incrementally\ndiscover novel categories of instances from unlabeled data, while maintaining\nits performance on the previously seen categories. We propose 1) a method to\ngenerate pseudo-latent representations which act as a proxy for (no longer\navailable) labeled data, thereby alleviating forgetting, 2) a\nmutual-information based regularizer which enhances unsupervised discovery of\nnovel classes, and 3) a simple Known Class Identifier which aids generalized\ninference when the testing data contains instances form both seen and unseen\ncategories. We introduce experimental protocols based on CIFAR-10, CIFAR-100\nand ImageNet-1000 to measure the trade-off between knowledge retention and\nnovel class discovery. Our extensive evaluations reveal that existing models\ncatastrophically forget previously seen categories while identifying novel\ncategories, while our method is able to effectively balance between the\ncompeting objectives. We hope our work will attract further research into this\nnewly identified pragmatic problem setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_K/0/1/0/all/0/1\">K J Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sujoy Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1\">Gaurav Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Soma Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1\">Piyush Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild. (arXiv:2207.10660v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10660","description":"<p>Recognizing scenes and objects in 3D from a single image is a longstanding\ngoal of computer vision with applications in robotics and AR/VR. For 2D\nrecognition, large datasets and scalable solutions have led to unprecedented\nadvances. In 3D, existing benchmarks are small in size and approaches\nspecialize in few object categories and specific domains, e.g. urban driving\nscenes. Motivated by the success of 2D recognition, we revisit the task of 3D\nobject detection by introducing a large benchmark, called Omni3D. Omni3D\nre-purposes and combines existing datasets resulting in 234k images annotated\nwith more than 3 million instances and 97 categories.3D detection at such scale\nis challenging due to variations in camera intrinsics and the rich diversity of\nscene and object types. We propose a model, called Cube R-CNN, designed to\ngeneralize across camera and scene types with a unified approach. We show that\nCube R-CNN outperforms prior works on the larger Omni3D and existing\nbenchmarks. Finally, we prove that Omni3D is a powerful dataset for 3D object\nrecognition, show that it improves single-dataset performance and can\naccelerate learning on new smaller datasets via pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brazil_G/0/1/0/all/0/1\">Garrick Brazil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Straub_J/0/1/0/all/0/1\">Julian Straub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_N/0/1/0/all/0/1\">Nikhila Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Justin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkioxari_G/0/1/0/all/0/1\">Georgia Gkioxari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In Defense of Online Models for Video Instance Segmentation. (arXiv:2207.10661v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10661","description":"<p>In recent years, video instance segmentation (VIS) has been largely advanced\nby offline models, while online models gradually attracted less attention\npossibly due to their inferior performance. However, online methods have their\ninherent advantage in handling long video sequences and ongoing videos while\noffline models fail due to the limit of computational resources. Therefore, it\nwould be highly desirable if online models can achieve comparable or even\nbetter performance than offline models. By dissecting current online models and\noffline models, we demonstrate that the main cause of the performance gap is\nthe error-prone association between frames caused by the similar appearance\namong different instances in the feature space. Observing this, we propose an\nonline framework based on contrastive learning that is able to learn more\ndiscriminative instance embeddings for association and fully exploit history\ninformation for stability. Despite its simplicity, our method outperforms all\nonline and offline methods on three benchmarks. Specifically, we achieve 49.5\nAP on YouTube-VIS 2019, a significant improvement of 13.2 AP and 2.1 AP over\nthe prior online and offline art, respectively. Moreover, we achieve 30.2 AP on\nOVIS, a more challenging dataset with significant crowding and occlusions,\nsurpassing the prior art by 14.8 AP. The proposed method won first place in the\nvideo instance segmentation track of the 4th Large-scale Video Object\nSegmentation Challenge (CVPR2022). We hope the simplicity and effectiveness of\nour method, as well as our insight into current methods, could shed light on\nthe exploration of VIS models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junfeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Patch-Based Neural Rendering. (arXiv:2207.10662v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10662","description":"<p>Neural rendering has received tremendous attention since the advent of Neural\nRadiance Fields (NeRF), and has pushed the state-of-the-art on novel-view\nsynthesis considerably. The recent focus has been on models that overfit to a\nsingle scene, and the few attempts to learn models that can synthesize novel\nviews of unseen scenes mostly consist of combining deep convolutional features\nwith a NeRF-like model. We propose a different paradigm, where no deep features\nand no NeRF-like volume rendering are needed. Our method is capable of\npredicting the color of a target ray in a novel scene directly, just from a\ncollection of patches sampled from the scene. We first leverage epipolar\ngeometry to extract patches along the epipolar lines of each reference view.\nEach patch is linearly projected into a 1D feature vector and a sequence of\ntransformers process the collection. For positional encoding, we parameterize\nrays as in a light field representation, with the crucial difference that the\ncoordinates are canonicalized with respect to the target ray, which makes our\nmethod independent of the reference frame and improves generalization. We show\nthat our approach outperforms the state-of-the-art on novel view synthesis of\nunseen scenes even when being trained with considerably less data than prior\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suhail_M/0/1/0/all/0/1\">Mohammed Suhail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteves_C/0/1/0/all/0/1\">Carlos Esteves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1\">Leonid Sigal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1\">Ameesh Makadia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Pixel Composition: 3D-4D View Synthesis from Multi-Views. (arXiv:2207.10663v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10663","description":"<p>We present Neural Pixel Composition (NPC), a novel approach for continuous\n3D-4D view synthesis given only a discrete set of multi-view observations as\ninput. Existing state-of-the-art approaches require dense multi-view\nsupervision and an extensive computational budget. The proposed formulation\nreliably operates on sparse and wide-baseline multi-view imagery and can be\ntrained efficiently within a few seconds to 10 minutes for hi-res (12MP)\ncontent, i.e., 200-400X faster convergence than existing methods. Crucial to\nour approach are two core novelties: 1) a representation of a pixel that\ncontains color and depth information accumulated from multi-views for a\nparticular location and time along a line of sight, and 2) a multi-layer\nperceptron (MLP) that enables the composition of this rich information provided\nfor a pixel location to obtain the final color output. We experiment with a\nlarge variety of multi-view sequences, compare to existing approaches, and\nachieve better results in diverse and challenging settings. Finally, our\napproach enables dense 3D reconstruction from sparse multi-views, where COLMAP,\na state-of-the-art 3D reconstruction approach, struggles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Aayush Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Fine-Grained Audiovisual Categorization with the SSW60 Dataset. (arXiv:2207.10664v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10664","description":"<p>We present a new benchmark dataset, Sapsucker Woods 60 (SSW60), for advancing\nresearch on audiovisual fine-grained categorization. While our community has\nmade great strides in fine-grained visual categorization on images, the\ncounterparts in audio and video fine-grained categorization are relatively\nunexplored. To encourage advancements in this space, we have carefully\nconstructed the SSW60 dataset to enable researchers to experiment with\nclassifying the same set of categories in three different modalities: images,\naudio, and video. The dataset covers 60 species of birds and is comprised of\nimages from existing datasets, and brand new, expert-curated audio and video\ndatasets. We thoroughly benchmark audiovisual classification performance and\nmodality fusion experiments through the use of state-of-the-art transformer\nmethods. Our findings show that performance of audiovisual fusion methods is\nbetter than using exclusively image or audio based methods for the task of\nvideo classification. We also present interesting modality transfer\nexperiments, enabled by the unique construction of SSW60 to encompass three\ndifferent modalities. We hope the SSW60 dataset and accompanying baselines spur\nresearch in this fascinating area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horn_G/0/1/0/all/0/1\">Grant Van Horn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1\">Kimberly Wilber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1\">Hartwig Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TinyViT: Fast Pretraining Distillation for Small Vision Transformers. (arXiv:2207.10666v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10666","description":"<p>Vision transformer (ViT) recently has drawn great attention in computer\nvision due to its remarkable model capability. However, most prevailing ViT\nmodels suffer from huge number of parameters, restricting their applicability\non devices with limited resources. To alleviate this issue, we propose TinyViT,\na new family of tiny and efficient small vision transformers pretrained on\nlarge-scale datasets with our proposed fast distillation framework. The central\nidea is to transfer knowledge from large pretrained models to small ones, while\nenabling small models to get the dividends of massive pretraining data. More\nspecifically, we apply distillation during pretraining for knowledge transfer.\nThe logits of large teacher models are sparsified and stored in disk in advance\nto save the memory cost and computation overheads. The tiny student\ntransformers are automatically scaled down from a large pretrained model with\ncomputation and parameter constraints. Comprehensive experiments demonstrate\nthe efficacy of TinyViT. It achieves a top-1 accuracy of 84.8% on ImageNet-1k\nwith only 21M parameters, being comparable to Swin-B pretrained on ImageNet-21k\nwhile using 4.2 times fewer parameters. Moreover, increasing image resolutions,\nTinyViT can reach 86.5% accuracy, being slightly better than Swin-L while using\nonly 11% parameters. Last but not the least, we demonstrate a good transfer\nability of TinyViT on various downstream tasks. Code and models are available\nat https://github.com/microsoft/Cream/tree/main/TinyViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinnian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Domain Adaptation for Semantic Segmentation in Ever-Changing Conditions. (arXiv:2207.10667v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10667","description":"<p>Unsupervised Domain Adaptation (UDA) aims at reducing the domain gap between\ntraining and testing data and is, in most cases, carried out in offline manner.\nHowever, domain changes may occur continuously and unpredictably during\ndeployment (e.g. sudden weather changes). In such conditions, deep neural\nnetworks witness dramatic drops in accuracy and offline adaptation may not be\nenough to contrast it. In this paper, we tackle Online Domain Adaptation (OnDA)\nfor semantic segmentation. We design a pipeline that is robust to continuous\ndomain shifts, either gradual or sudden, and we evaluate it in the case of\nrainy and foggy scenarios. Our experiments show that our framework can\neffectively adapt to new domains during deployment, while not being affected by\ncatastrophic forgetting of the previous domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panagiotakopoulos_T/0/1/0/all/0/1\">Theodoros Panagiotakopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dovesi_P/0/1/0/all/0/1\">Pier Luigi Dovesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harenstam_Nielsen_L/0/1/0/all/0/1\">Linus H&#xe4;renstam-Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poggi_M/0/1/0/all/0/1\">Matteo Poggi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Imitating Collaborative Manipulation Plans from YouTube Cooking Videos. (arXiv:1911.10686v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/1911.10686","description":"<p>People often watch videos on the web to learn how to cook new recipes,\nassemble furniture or repair a computer. We wish to enable robots with the very\nsame capability. This is challenging; there is a large variation in\nmanipulation actions and some videos even involve multiple persons, who\ncollaborate by sharing and exchanging objects and tools. Furthermore, the\nlearned representations need to be general enough to be transferable to robotic\nsystems. On the other hand, previous work has shown that the space of human\nmanipulation actions has a linguistic, hierarchical structure that relates\nactions to manipulated objects and tools. Building upon this theory of language\nfor action, we propose a system for understanding and executing demonstrated\naction sequences from full-length, real-world cooking videos on the web. The\nsystem takes as input a new, previously unseen cooking video annotated with\nobject labels and bounding boxes, and outputs a collaborative manipulation\naction plan for one or more robotic arms. We demonstrate performance of the\nsystem in a standardized dataset of 100 YouTube cooking videos, as well as in\nsix full-length Youtube videos that include collaborative actions between two\nparticipants. We compare our system with a baseline system that consists of a\nstate-of-the-art action detection baseline and show our system achieves higher\naction detection accuracy. We additionally propose an open-source platform for\nexecuting the learned plans in a simulation environment as well as with an\nactual robotic arm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hejia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaidis_S/0/1/0/all/0/1\">Stefanos Nikolaidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Photorealism in Driving Simulations: Blending Generative Adversarial Image Synthesis with Rendering. (arXiv:2007.15820v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.15820","description":"<p>Driving simulators play a large role in developing and testing new\nintelligent vehicle systems. The visual fidelity of the simulation is critical\nfor building vision-based algorithms and conducting human driver experiments.\nLow visual fidelity breaks immersion for human-in-the-loop driving experiments.\nConventional computer graphics pipelines use detailed 3D models, meshes,\ntextures, and rendering engines to generate 2D images from 3D scenes. These\nprocesses are labor-intensive, and they do not generate photorealistic imagery.\nHere we introduce a hybrid generative neural graphics pipeline for improving\nthe visual fidelity of driving simulations. Given a 3D scene, we partially\nrender only important objects of interest, such as vehicles, and use generative\nadversarial processes to synthesize the background and the rest of the image.\nTo this end, we propose a novel image formation strategy to form 2D semantic\nimages from 3D scenery consisting of simple object models without textures.\nThese semantic images are then converted into photorealistic RGB images with a\nstate-of-the-art Generative Adversarial Network (GAN) trained on real-world\ndriving scenes. This replaces repetitiveness with randomly generated but\nphotorealistic surfaces. Finally, the partially-rendered and GAN synthesized\nimages are blended with a blending GAN. We show that the photorealism of images\ngenerated with the proposed method is more similar to real-world driving\ndatasets such as Cityscapes and KITTI than conventional approaches. This\ncomparison is made using semantic retention analysis and Frechet Inception\nDistance (FID) measurements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yurtsever_E/0/1/0/all/0/1\">Ekim Yurtsever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongfang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koc_I/0/1/0/all/0/1\">Ibrahim Mert Koc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redmill_K/0/1/0/all/0/1\">Keith A. Redmill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Registration for Unsupervised Medical Image Segmentation. (arXiv:2011.08894v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.08894","description":"<p>Medical image segmentation is a relevant task as it serves as the first step\nfor several diagnosis processes, thus it is indispensable in clinical usage.\nWhilst major success has been reported using supervised techniques, they assume\na large and well-representative labelled set. This is a strong assumption in\nthe medical domain where annotations are expensive, time-consuming, and\ninherent to human bias. To address this problem, unsupervised techniques have\nbeen proposed in the literature yet it is still an open problem due to the\ndifficulty of learning any transformation pattern. In this work, we present a\nnovel optimisation model framed into a new CNN-based contrastive registration\narchitecture for unsupervised medical image segmentation. The core of our\napproach is to exploit image-level registration and feature-level from a\ncontrastive learning mechanism, to perform registration-based segmentation.\nFirstly, we propose an architecture to capture the image-to-image\ntransformation pattern via registration for unsupervised medical image\nsegmentation. Secondly, we embed a contrastive learning mechanism into the\nregistration architecture to enhance the discriminating capacity of the network\nin the feature-level. We show that our proposed technique mitigates the major\ndrawbacks of existing unsupervised techniques. We demonstrate, through\nnumerical and visual experiments, that our technique substantially outperforms\nthe current state-of-the-art unsupervised segmentation methods on two major\nmedical image datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica I Aviles-Rivero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AXM-Net: Implicit Cross-Modal Feature Alignment for Person Re-identification. (arXiv:2101.08238v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.08238","description":"<p>Cross-modal person re-identification (Re-ID) is critical for modern video\nsurveillance systems. The key challenge is to align cross-modality\nrepresentations induced by the semantic information present for a person and\nignore background information. This work presents a novel convolutional neural\nnetwork (CNN) based architecture designed to learn semantically aligned\ncross-modal visual and textual representations. The underlying building block,\nnamed AXM-Block, is a unified multi-layer network that dynamically exploits the\nmulti-scale knowledge from both modalities and re-calibrates each modality\naccording to shared semantics. To complement the convolutional design,\ncontextual attention is applied in the text branch to manipulate long-term\ndependencies. Moreover, we propose a unique design to enhance visual part-based\nfeature coherence and locality information. Our framework is novel in its\nability to implicitly learn aligned semantics between modalities during the\nfeature learning stage. The unified feature learning effectively utilizes\ntextual data as a super-annotation signal for visual representation learning\nand automatically rejects irrelevant information. The entire AXM-Net is trained\nend-to-end on CUHK-PEDES data. We report results on two tasks, person search\nand cross-modal Re-ID. The AXM-Net outperforms the current state-of-the-art\n(SOTA) methods and achieves 64.44\\% Rank@1 on the CUHK-PEDES test set. It also\noutperforms its competitors by $&gt;$10\\% in cross-viewpoint text-to-image Re-ID\nscenarios on CrossRe-ID and CUHK-SYSU datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farooq_A/0/1/0/all/0/1\">Ammarah Farooq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awais_M/0/1/0/all/0/1\">Muhammad Awais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_S/0/1/0/all/0/1\">Syed Safwan Khalid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comprehensive Multi-Modal Interactions for Referring Image Segmentation. (arXiv:2104.10412v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10412","description":"<p>We investigate Referring Image Segmentation (RIS), which outputs a\nsegmentation map corresponding to the natural language description. Addressing\nRIS efficiently requires considering the interactions happening \\emph{across}\nvisual and linguistic modalities and the interactions \\emph{within} each\nmodality. Existing methods are limited because they either compute different\nforms of interactions \\emph{sequentially} (leading to error propagation) or\n\\emph{ignore} intramodal interactions. We address this limitation by performing\nall three interactions \\emph{simultaneously} through a Synchronous Multi-Modal\nFusion Module (SFM). Moreover, to produce refined segmentation masks, we\npropose a novel Hierarchical Cross-Modal Aggregation Module (HCAM), where\nlinguistic features facilitate the exchange of contextual information across\nthe visual hierarchy. We present thorough ablation studies and validate our\napproach's performance on four benchmark datasets, showing considerable\nperformance gains over the existing state-of-the-art (SOTA) methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_K/0/1/0/all/0/1\">Kanishk Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1\">Vineet Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Well Does Self-Supervised Pre-Training Perform with Streaming Data?. (arXiv:2104.12081v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.12081","description":"<p>Prior works on self-supervised pre-training focus on the joint training\nscenario, where massive unlabeled data are assumed to be given as input all at\nonce, and only then is a learner trained. Unfortunately, such a problem setting\nis often impractical if not infeasible since many real-world tasks rely on\nsequential learning, e.g., data are decentralized or collected in a streaming\nfashion. In this paper, we conduct the first thorough and dedicated\ninvestigation on self-supervised pre-training with streaming data, aiming to\nshed light on the model behavior under this overlooked setup. Specifically, we\npre-train over 500 models on four categories of pre-training streaming data\nfrom ImageNet and DomainNet and evaluate them on three types of downstream\ntasks and 12 different downstream datasets. Our studies show that, somehow\nbeyond our expectation, with simple data replay or parameter regularization,\nsequential self-supervised pre-training turns out to be an efficient\nalternative for joint pre-training, as the performances of the former are\nmostly on par with those of the latter. Moreover, catastrophic forgetting, a\ncommon issue in sequential supervised learning, is much alleviated in\nsequential self-supervised learning (SSL), which is well justified through our\ncomprehensive empirical analysis on representations and the sharpness of minima\nin the loss landscape. Our findings, therefore, suggest that, in practice, for\nSSL, the cumbersome joint training can be replaced mainly by sequential\nlearning, which in turn enables a much broader spectrum of potential\napplication scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dapeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shipeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qizhengqiu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hailin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Unsupervised Sketch-based Image Retrieval. (arXiv:2105.08237v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.08237","description":"<p>In this paper, we present the first attempt at unsupervised SBIR to remove\nthe labeling cost (category annotations and sketch-photo pairings) that is\nconventionally needed for training. Existing single-domain unsupervised\nrepresentation learning methods perform poorly in this application, due to the\nunique cross-domain (sketch and photo) nature of the problem. We therefore\nintroduce a novel framework that simultaneously performs unsupervised\nrepresentation learning and sketch-photo domain alignment. Technically this is\nunderpinned by exploiting joint distribution optimal transport (JDOT) to align\ndata from different domains during representation learning, which we extend\nwith trainable cluster prototypes and feature memory banks to further improve\nscalability and efficacy. Extensive experiments show that our framework\nachieves excellent performance in the new unsupervised setting, and performs\ncomparably or better than state-of-the-art in the zero-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Conghui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy M. Hospedales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning with Complex Heterogeneity. (arXiv:2105.09401v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.09401","description":"<p>With the advent of big data across multiple high-impact applications, we are\noften facing the challenge of complex heterogeneity. The newly collected data\nusually consist of multiple modalities and are characterized with multiple\nlabels, thus exhibiting the co-existence of multiple types of heterogeneity.\nAlthough state-of-the-art techniques are good at modeling complex heterogeneity\nwith sufficient label information, such label information can be quite\nexpensive to obtain in real applications. Recently, researchers pay great\nattention to contrastive learning due to its prominent performance by utilizing\nrich unlabeled data. However, existing work on contrastive learning is not able\nto address the problem of false negative pairs, i.e., some `negative' pairs may\nhave similar representations if they have the same label. To overcome the\nissues, in this paper, we propose a unified heterogeneous learning framework,\nwhich combines both the weighted unsupervised contrastive loss and the weighted\nsupervised contrastive loss to model multiple types of heterogeneity. We first\nprovide a theoretical analysis showing that the vanilla contrastive learning\nloss easily leads to the sub-optimal solution in the presence of false negative\npairs, whereas the proposed weighted loss could automatically adjust the weight\nbased on the similarity of the learned representations to mitigate this issue.\nExperimental results on real-world data sets demonstrate the effectiveness and\nthe efficiency of the proposed framework modeling multiple types of\nheterogeneity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yada Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transforming the Latent Space of StyleGAN for Real Face Editing. (arXiv:2105.14230v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14230","description":"<p>Despite recent advances in semantic manipulation using StyleGAN, semantic\nediting of real faces remains challenging. The gap between the $W$ space and\nthe $W$+ space demands an undesirable trade-off between reconstruction quality\nand editing quality. To solve this problem, we propose to expand the latent\nspace by replacing fully-connected layers in the StyleGAN's mapping network\nwith attention-based transformers. This simple and effective technique\nintegrates the aforementioned two spaces and transforms them into one new\nlatent space called $W$++. Our modified StyleGAN maintains the state-of-the-art\ngeneration quality of the original StyleGAN with moderately better diversity.\nBut more importantly, the proposed $W$++ space achieves superior performance in\nboth reconstruction quality and editing quality. Despite these significant\nadvantages, our $W$++ space supports existing inversion algorithms and editing\nmethods with only negligible modifications thanks to its structural similarity\nwith the $W/W$+ space. Extensive experiments on the FFHQ dataset prove that our\nproposed $W$++ space is evidently more preferable than the previous $W/W$+\nspace for real face editing. The code is publicly available for research\npurposes at https://github.com/AnonSubm2021/TransStyleGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Heyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinlong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yunzhi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huayan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1\">Klaus Mueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Knowledge-Transfer for Learned Image Reconstruction. (arXiv:2107.02572v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.02572","description":"<p>Deep learning-based image reconstruction approaches have demonstrated\nimpressive empirical performance in many imaging modalities. These approaches\nusually require a large amount of high-quality paired training data, which is\noften not available in medical imaging. To circumvent this issue we develop a\nnovel unsupervised knowledge-transfer paradigm for learned reconstruction\nwithin a Bayesian framework. The proposed approach learns a reconstruction\nnetwork in two phases. The first phase trains a reconstruction network with a\nset of ordered pairs comprising of ground truth images of ellipses and the\ncorresponding simulated measurement data. The second phase fine-tunes the\npretrained network to more realistic measurement data without supervision. By\nconstruction, the framework is capable of delivering predictive uncertainty\ninformation over the reconstructed image. We present extensive experimental\nresults on low-dose and sparse-view computed tomography showing that the\napproach is competitive with several state-of-the-art supervised and\nunsupervised reconstruction techniques. Moreover, for test data distributed\ndifferently from the training data, the proposed framework can significantly\nimprove reconstruction quality not only visually, but also quantitatively in\nterms of PSNR and SSIM, when compared with learned methods trained on the\nsynthetic dataset only.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Barbano_R/0/1/0/all/0/1\">Riccardo Barbano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kereta_Z/0/1/0/all/0/1\">Zeljko Kereta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hauptmann_A/0/1/0/all/0/1\">Andreas Hauptmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arridge_S/0/1/0/all/0/1\">Simon R. Arridge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_B/0/1/0/all/0/1\">Bangti Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DISP6D: Disentangled Implicit Shape and Pose Learning for Scalable 6D Pose Estimation. (arXiv:2107.12549v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12549","description":"<p>Scalable 6D pose estimation for rigid objects from RGB images aims at\nhandling multiple objects and generalizing to novel objects. Building on a\nwell-known auto-encoding framework to cope with object symmetry and the lack of\nlabeled training data, we achieve scalability by disentangling the latent\nrepresentation of auto-encoder into shape and pose sub-spaces. The latent shape\nspace models the similarity of different objects through contrastive metric\nlearning, and the latent pose code is compared with canonical rotations for\nrotation retrieval. Because different object symmetries induce inconsistent\nlatent pose spaces, we re-entangle the shape representation with canonical\nrotations to generate shape-dependent pose codebooks for rotation retrieval. We\nshow state-of-the-art performance on two benchmarks containing textureless CAD\nobjects without category and daily objects with categories respectively, and\nfurther demonstrate improved scalability by extending to a more challenging\nsetting of daily objects across categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yilin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Image Representations for Multi-Image Fusion and Layer Separation. (arXiv:2108.01199v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01199","description":"<p>We propose a framework for aligning and fusing multiple images into a single\nview using neural image representations (NIRs), also known as implicit or\ncoordinate-based neural representations. Our framework targets burst images\nthat exhibit camera ego motion and potential changes in the scene. We describe\ndifferent strategies for alignment depending on the nature of the scene motion\n-- namely, perspective planar (i.e., homography), optical flow with minimal\nscene change, and optical flow with notable occlusion and disocclusion. With\nthe neural image representation, our framework effectively combines multiple\ninputs into a single canonical view without the need for selecting one of the\nimages as a reference frame. We demonstrate how to use this multi-frame fusion\nframework for various layer separation tasks. The code and results are\navailable at https://shnnam.github.io/research/nir.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1\">Seonghyeon Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Gaze Analysis: A Survey of Deep Learning based Approaches. (arXiv:2108.05479v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05479","description":"<p>Eye gaze analysis is an important research problem in the field of Computer\nVision and Human-Computer Interaction. Even with notable progress in the last\n10 years, automatic gaze analysis still remains challenging due to the\nuniqueness of eye appearance, eye-head interplay, occlusion, image quality, and\nillumination conditions. There are several open questions, including what are\nthe important cues to interpret gaze direction in an unconstrained environment\nwithout prior knowledge and how to encode them in real-time. We review the\nprogress across a range of gaze analysis tasks and applications to elucidate\nthese fundamental questions, identify effective methods in gaze analysis, and\nprovide possible future directions. We analyze recent gaze estimation and\nsegmentation methods, especially in the unsupervised and weakly supervised\ndomain, based on their advantages and reported evaluation metrics. Our analysis\nshows that the development of a robust and generic gaze analysis method still\nneeds to address real-world challenges such as unconstrained setup and learning\nwith less supervision. We conclude by discussing future research directions for\ndesigning a real-world gaze analysis system that can propagate to other domains\nincluding Computer Vision, Augmented Reality (AR), Virtual Reality (VR), and\nHuman Computer Interaction (HCI). Project Page:\nhttps://github.com/i-am-shreya/EyeGazeSurvey}{https://github.com/i-am-shreya/EyeGazeSurvey\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shreya Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhall_A/0/1/0/all/0/1\">Abhinav Dhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knibbe_J/0/1/0/all/0/1\">Jarrod Knibbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1\">Qiang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying partial mouse brain microscopy images from Allen reference atlas using a contrastively learned semantic space. (arXiv:2109.06662v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06662","description":"<p>Precise identification of mouse brain microscopy images is a crucial first\nstep when anatomical structures in the mouse brain are to be registered to a\nreference atlas. Practitioners usually rely on manual comparison of images or\ntools that assume the presence of complete images. This work explores Siamese\nNetworks as the method for finding corresponding 2D reference atlas plates for\ngiven partial 2D mouse brain images. Siamese networks are a class of\nconvolutional neural networks (CNNs) that use weight-shared paths to obtain low\ndimensional embeddings of pairs of input images. The correspondence between the\npartial mouse brain image and reference atlas plate is determined based on the\ndistance between low dimensional embeddings of brain slices and atlas plates\nthat are obtained from Siamese networks using contrastive learning. Experiments\nshowed that Siamese CNNs can precisely identify brain slices using the Allen\nmouse brain atlas when training and testing images come from the same source.\nThey achieved TOP-1 and TOP-5 accuracy of 25% and 100%, respectively, taking\nonly 7.2 seconds to identify 29 images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antanavicius_J/0/1/0/all/0/1\">Justinas Antanavicius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leiras_R/0/1/0/all/0/1\">Roberto Leiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvan_R/0/1/0/all/0/1\">Raghavendra Selvan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Autonomous Visual Navigation in Arable Fields. (arXiv:2109.11936v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.11936","description":"<p>Autonomous navigation of a robot in agricultural fields is essential for\nevery task from crop monitoring to weed management and fertilizer application.\nMany current approaches rely on accurate GPS, however, such technology is\nexpensive and also prone to failure (e.g. through lack of coverage). As such,\nautonomous navigation through sensors that can interpret their environment\n(such as cameras) is important to achieve the goal of autonomy in agriculture.\nIn this paper, we introduce a purely vision-based navigation scheme that is\nable to reliably guide the robot through row-crop fields without manual\nintervention. Independent of any global localization or mapping, this approach\nis able to accurately follow the crop-rows and switch between the rows, only\nusing onboard cameras. With the help of a novel crop-row detection and a novel\ncrop-row switching technique, our navigation scheme can be deployed in a wide\nrange of fields with different canopy types in various growth stages with\nlimited parameter tuning, creating a crop agnostic navigation approach. We have\nextensively evaluated our approach in three different fields under various\nillumination conditions using our agricultural robotic platform (BonnBot-I).\nFor navigation, our approach is evaluated on five crop types and achieves an\naverage navigation accuracy of 3.82cm relative to manual teleoperation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_A/0/1/0/all/0/1\">Alireza Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halstead_M/0/1/0/all/0/1\">Michael Halstead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCool_C/0/1/0/all/0/1\">Chris McCool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Pedestrian Attribute Recognition Using Group Sparsity for Occlusion Videos. (arXiv:2110.08708v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08708","description":"<p>Occlusion processing is a key issue in pedestrian attribute recognition\n(PAR). Nevertheless, several existing video-based PAR methods have not yet\nconsidered occlusion handling in depth. In this paper, we formulate finding\nnon-occluded frames as sparsity-based temporal attention of a crowded video. In\nthis manner, a model is guided not to pay attention to the occluded frame.\nHowever, temporal sparsity cannot include a correlation between attributes when\nocclusion occurs. For example, \"boots\" and \"shoe color\" cannot be recognized\nwhen the foot is invisible. To solve the uncorrelated attention issue, we also\npropose a novel group sparsity-based temporal attention module. Group sparsity\nis applied across attention weights in correlated attributes. Thus, attention\nweights in a group are forced to pay attention to the same frames. Experimental\nresults showed that the proposed method achieved a higher F1-score than the\nstate-of-the-art methods on two video-based PAR datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Geonu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_K/0/1/0/all/0/1\">Kimin Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jungchan Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithmic encoding of protected characteristics in image-based models for disease detection. (arXiv:2110.14755v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.14755","description":"<p>It has been rightfully emphasized that the use of AI for clinical decision\nmaking could amplify health disparities. An algorithm may encode protected\ncharacteristics, and then use this information for making predictions due to\nundesirable correlations in the (historical) training data. It remains unclear\nhow we can establish whether such information is actually used. Besides the\nscarcity of data from underserved populations, very little is known about how\ndataset biases manifest in predictive models and how this may result in\ndisparate performance. This article aims to shed some light on these issues by\nexploring new methodology for subgroup analysis in image-based disease\ndetection models. We utilize two publicly available chest X-ray datasets,\nCheXpert and MIMIC-CXR, to study performance disparities across race and\nbiological sex in deep learning models. We explore test set resampling,\ntransfer learning, multitask learning, and model inspection to assess the\nrelationship between the encoding of protected characteristics and disease\ndetection performance across subgroups. We confirm subgroup disparities in\nterms of shifted true and false positive rates which are partially removed\nafter correcting for population and prevalence shifts in the test sets. We\nfurther find a previously used transfer learning method to be insufficient for\nestablishing whether specific patient information is used for making\npredictions. The proposed combination of test-set resampling, multitask\nlearning, and model inspection reveals valuable new insights about the way\nprotected characteristics are encoded in the feature representations of deep\nneural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1\">Charles Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernhardt_M/0/1/0/all/0/1\">Melanie Bernhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winzeck_S/0/1/0/all/0/1\">Stefan Winzeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP2TV: Align, Match and Distill for Video-Text Retrieval. (arXiv:2111.05610v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.05610","description":"<p>Modern video-text retrieval frameworks basically consist of three parts:\nvideo encoder, text encoder and the similarity head. With the success on both\nvisual and textual representation learning, transformer based encoders and\nfusion methods have also been adopted in the field of video-text retrieval. In\nthis report, we present CLIP2TV, aiming at exploring where the critical\nelements lie in transformer based methods. To achieve this, We first revisit\nsome recent works on multi-modal learning, then introduce some techniques into\nvideo-text retrieval, finally evaluate them through extensive experiments in\ndifferent configurations. Notably, CLIP2TV achieves 52.9@R1 on MSR-VTT dataset,\noutperforming the previous SOTA result by 4.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zijian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dedan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lili Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnimeCeleb: Large-Scale Animation CelebHeads Dataset for Head Reenactment. (arXiv:2111.07640v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2111.07640","description":"<p>We present a novel Animation CelebHeads dataset (AnimeCeleb) to address an\nanimation head reenactment. Different from previous animation head datasets, we\nutilize 3D animation models as the controllable image samplers, which can\nprovide a large amount of head images with their corresponding detailed pose\nannotations. To facilitate a data creation process, we build a semi-automatic\npipeline leveraging an open 3D computer graphics software with a developed\nannotation system. After training with the AnimeCeleb, recent head reenactment\nmodels produce high-quality animation head reenactment results, which are not\nachievable with existing datasets. Furthermore, motivated by metaverse\napplication, we propose a novel pose mapping method and architecture to tackle\na cross-domain head reenactment task. During inference, a user can easily\ntransfer one's motion to an arbitrary animation head. Experiments demonstrate\nthe usefulness of the AnimeCeleb to train animation head reenactment models,\nand the superiority of our cross-domain head reenactment model compared to\nstate-of-the-art methods. Our dataset and code are available at\nhttps://github.com/kangyeolk/AnimeCeleb.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kangyeol Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaeseong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Sunghyo Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layered Controllable Video Generation. (arXiv:2111.12747v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12747","description":"<p>We introduce layered controllable video generation, where we, without any\nsupervision, decompose the initial frame of a video into foreground and\nbackground layers, with which the user can control the video generation process\nby simply manipulating the foreground mask. The key challenges are the\nunsupervised foreground-background separation, which is ambiguous, and ability\nto anticipate user manipulations with access to only raw video sequences. We\naddress these challenges by proposing a two-stage learning procedure. In the\nfirst stage, with the rich set of losses and dynamic foreground size prior, we\nlearn how to separate the frame into foreground and background layers and,\nconditioned on these layers, how to generate the next frame using VQ-VAE\ngenerator. In the second stage, we fine-tune this network to anticipate edits\nto the mask, by fitting (parameterized) control to the mask from future frame.\nWe demonstrate the effectiveness of this learning and the more granular control\nmechanism, while illustrating state-of-the-art performance on two benchmark\ndatasets. We provide a video abstract as well as some video results on\nhttps://gabriel-huang.github.io/layered_controllable_video_generation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiahui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuhe Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kwang Moo Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1\">Leonid Sigal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Invariants to Understand Unsupervised Out-of-Distribution Detection. (arXiv:2111.13362v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13362","description":"<p>Unsupervised out-of-distribution (U-OOD) detection has recently attracted\nmuch attention due its importance in mission-critical systems and broader\napplicability over its supervised counterpart. Despite this increase in\nattention, U-OOD methods suffer from important shortcomings. By performing a\nlarge-scale evaluation on different benchmarks and image modalities, we show in\nthis work that most popular state-of-the-art methods are unable to consistently\noutperform a simple anomaly detector based on pre-trained features and the\nMahalanobis distance (MahaAD). A key reason for the inconsistencies of these\nmethods is the lack of a formal description of U-OOD. Motivated by a simple\nthought experiment, we propose a characterization of U-OOD based on the\ninvariants of the training dataset. We show how this characterization is\nunknowingly embodied in the top-scoring MahaAD method, thereby explaining its\nquality. Furthermore, our approach can be used to interpret predictions of\nU-OOD detectors and provides insights into good practices for evaluating future\nU-OOD methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doorenbos_L/0/1/0/all/0/1\">Lars Doorenbos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1\">Raphael Sznitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquez_Neila_P/0/1/0/all/0/1\">Pablo M&#xe1;rquez-Neila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Fit Morphable Models. (arXiv:2111.14824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14824","description":"<p>Fitting parametric models of human bodies, hands or faces to sparse input\nsignals in an accurate, robust, and fast manner has the promise of\nsignificantly improving immersion in AR and VR scenarios. A common first step\nin systems that tackle these problems is to regress the parameters of the\nparametric model directly from the input data. This approach is fast, robust,\nand is a good starting point for an iterative minimization algorithm. The\nlatter searches for the minimum of an energy function, typically composed of a\ndata term and priors that encode our knowledge about the problem's structure.\nWhile this is undoubtedly a very successful recipe, priors are often hand\ndefined heuristics and finding the right balance between the different terms to\nachieve high quality results is a non-trivial task. Furthermore, converting and\noptimizing these systems to run in a performant way requires custom\nimplementations that demand significant time investments from both engineers\nand domain experts. In this work, we build upon recent advances in learned\noptimization and propose an update rule inspired by the classic\nLevenberg-Marquardt algorithm. We show the effectiveness of the proposed neural\noptimizer on three problems, 3D body estimation from a head-mounted device, 3D\nbody estimation from sparse 2D keypoints and face surface estimation from dense\n2D landmarks. Our method can easily be applied to new model fitting problems\nand offers a competitive alternative to well-tuned 'traditional' model fitting\npipelines, both in terms of accuracy and speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choutas_V/0/1/0/all/0/1\">Vasileios Choutas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogo_F/0/1/0/all/0/1\">Federica Bogo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jingjing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentin_J/0/1/0/all/0/1\">Julien Valentin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdiBERT, a generative model for image editing. (arXiv:2111.15264v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15264","description":"<p>Advances in computer vision are pushing the limits of im-age manipulation,\nwith generative models sampling detailed images on various tasks. However, a\nspecialized model is often developed and trained for each specific task, even\nthough many image edition tasks share similarities. In denoising, inpainting,\nor image compositing, one always aims at generating a realistic image from a\nlow-quality one. In this paper, we aim at making a step towards a unified\napproach for image editing. To do so, we propose EdiBERT, a bi-directional\ntransformer trained in the discrete latent space built by a vector-quantized\nauto-encoder. We argue that such a bidirectional model is suited for image\nmanipulation since any patch can be re-sampled conditionally to the whole\nimage. Using this unique and straightforward training objective, we show that\nthe resulting model matches state-of-the-art performances on a wide variety of\ntasks: image denoising, image completion, and image composition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Issenhuth_T/0/1/0/all/0/1\">Thibaut Issenhuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanielian_U/0/1/0/all/0/1\">Ugo Tanielian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mary_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;mie Mary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1\">David Picard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRF-SR: High-Quality Neural Radiance Fields using Supersampling. (arXiv:2112.01759v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01759","description":"<p>We present NeRF-SR, a solution for high-resolution (HR) novel view synthesis\nwith mostly low-resolution (LR) inputs. Our method is built upon Neural\nRadiance Fields (NeRF) that predicts per-point density and color with a\nmulti-layer perceptron. While producing images at arbitrary scales, NeRF\nstruggles with resolutions that go beyond observed images. Our key insight is\nthat NeRF benefits from 3D consistency, which means an observed pixel absorbs\ninformation from nearby views. We first exploit it by a supersampling strategy\nthat shoots multiple rays at each image pixel, which further enforces\nmulti-view constraint at a sub-pixel level. Then, we show that NeRF-SR can\nfurther boost the performance of supersampling by a refinement network that\nleverages the estimated depth at hand to hallucinate details from related\npatches on only one HR reference image. Experiment results demonstrate that\nNeRF-SR generates high-quality results for novel view synthesis at HR on both\nsynthetic and real-world datasets without any external information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuan-Chen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Song-Hai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi-Min Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIVE: Evaluating the Human Interpretability of Visual Explanations. (arXiv:2112.03184v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03184","description":"<p>As AI technology is increasingly applied to high-impact, high-risk domains,\nthere have been a number of new methods aimed at making AI models more human\ninterpretable. Despite the recent growth of interpretability work, there is a\nlack of systematic evaluation of proposed techniques. In this work, we\nintroduce HIVE (Human Interpretability of Visual Explanations), a novel human\nevaluation framework that assesses the utility of explanations to human users\nin AI-assisted decision making scenarios, and enables falsifiable hypothesis\ntesting, cross-method comparison, and human-centered evaluation of visual\ninterpretability methods. To the best of our knowledge, this is the first work\nof its kind. Using HIVE, we conduct IRB-approved human studies with nearly 1000\nparticipants and evaluate four methods that represent the diversity of computer\nvision interpretability works: GradCAM, BagNet, ProtoPNet, and ProtoTree. Our\nresults suggest that explanations engender human trust, even for incorrect\npredictions, yet are not distinct enough for users to distinguish between\ncorrect and incorrect predictions. We open-source HIVE to enable future studies\nand encourage more human-centered approaches to interpretability research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunnie S. Y. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_N/0/1/0/all/0/1\">Nicole Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaswamy_V/0/1/0/all/0/1\">Vikram V. Ramaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_R/0/1/0/all/0/1\">Ruth Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Global and Local Hierarchical Priors for Learned Image Compression. (arXiv:2112.04487v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.04487","description":"<p>Recently, learned image compression methods have outperformed traditional\nhand-crafted ones including BPG. One of the keys to this success is learned\nentropy models that estimate the probability distribution of the quantized\nlatent representation. Like other vision tasks, most recent learned entropy\nmodels are based on convolutional neural networks (CNNs). However, CNNs have a\nlimitation in modeling long-range dependencies due to their nature of local\nconnectivity, which can be a significant bottleneck in image compression where\nreducing spatial redundancy is a key point. To overcome this issue, we propose\na novel entropy model called Information Transformer (Informer) that exploits\nboth global and local information in a content-dependent manner using an\nattention mechanism. Our experiments show that Informer improves\nrate--distortion performance over the state-of-the-art methods on the Kodak and\nTecnick datasets without the quadratic computational complexity problem. Our\nsource code is available at https://github.com/naver-ai/informer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Jun-Hyuk Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heo_B/0/1/0/all/0/1\">Byeongho Heo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Seok Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRF for Outdoor Scene Relighting. (arXiv:2112.05140v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05140","description":"<p>Photorealistic editing of outdoor scenes from photographs requires a profound\nunderstanding of the image formation process and an accurate estimation of the\nscene geometry, reflectance and illumination. A delicate manipulation of the\nlighting can then be performed while keeping the scene albedo and geometry\nunaltered. We present NeRF-OSR, i.e., the first approach for outdoor scene\nrelighting based on neural radiance fields. In contrast to the prior art, our\ntechnique allows simultaneous editing of both scene illumination and camera\nviewpoint using only a collection of outdoor photos shot in uncontrolled\nsettings. Moreover, it enables direct control over the scene illumination, as\ndefined through a spherical harmonics model. For evaluation, we collect a new\nbenchmark dataset of several outdoor sites photographed from multiple\nviewpoints and at different times. For each time, a 360 degree environment map\nis captured together with a colour-calibration chequerboard to allow accurate\nnumerical evaluations on real data against ground truth. Comparisons against\nSoTA show that NeRF-OSR enables controllable lighting and viewpoint editing at\nhigher quality and with realistic self-shadowing reproduction. Our method and\nthe dataset are publicly available at https://4dqv.mpi-inf.mpg.de/NeRF-OSR/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rudnev_V/0/1/0/all/0/1\">Viktor Rudnev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1\">Mohamed Elgharib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1\">William Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Triangle Attack: A Query-efficient Decision-based Adversarial Attack. (arXiv:2112.06569v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06569","description":"<p>Decision-based attack poses a severe threat to real-world applications since\nit regards the target model as a black box and only accesses the hard\nprediction label. Great efforts have been made recently to decrease the number\nof queries; however, existing decision-based attacks still require thousands of\nqueries in order to generate good quality adversarial examples. In this work,\nwe find that a benign sample, the current and the next adversarial examples can\nnaturally construct a triangle in a subspace for any iterative attacks. Based\non the law of sines, we propose a novel Triangle Attack (TA) to optimize the\nperturbation by utilizing the geometric information that the longer side is\nalways opposite the larger angle in any triangle. However, directly applying\nsuch information on the input image is ineffective because it cannot thoroughly\nexplore the neighborhood of the input sample in the high dimensional space. To\naddress this issue, TA optimizes the perturbation in the low frequency space\nfor effective dimensionality reduction owing to the generality of such\ngeometric property. Extensive evaluations on ImageNet dataset show that TA\nachieves a much higher attack success rate within 1,000 queries and needs a\nmuch less number of queries to achieve the same attack success rate under\nvarious perturbation budgets than existing decision-based attacks. With such\nhigh efficiency, we further validate the applicability of TA on real-world API,\ni.e., Tencent Cloud API.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zeliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_K/0/1/0/all/0/1\">Kangheng Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dihong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeqFormer: Sequential Transformer for Video Instance Segmentation. (arXiv:2112.08275v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08275","description":"<p>In this work, we present SeqFormer for video instance segmentation. SeqFormer\nfollows the principle of vision transformer that models instance relationships\namong video frames. Nevertheless, we observe that a stand-alone instance query\nsuffices for capturing a time sequence of instances in a video, but attention\nmechanisms shall be done with each frame independently. To achieve this,\nSeqFormer locates an instance in each frame and aggregates temporal information\nto learn a powerful representation of a video-level instance, which is used to\npredict the mask sequences on each frame dynamically. Instance tracking is\nachieved naturally without tracking branches or post-processing. On\nYouTube-VIS, SeqFormer achieves 47.4 AP with a ResNet-50 backbone and 49.0 AP\nwith a ResNet-101 backbone without bells and whistles. Such achievement\nsignificantly exceeds the previous state-of-the-art performance by 4.6 and 4.4,\nrespectively. In addition, integrated with the recently-proposed Swin\ntransformer, SeqFormer achieves a much higher AP of 59.3. We hope SeqFormer\ncould be a strong baseline that fosters future research in video instance\nsegmentation, and in the meantime, advances this field with a more robust,\naccurate, neat model. The code is available at\nhttps://github.com/wjf5203/SeqFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junfeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mimic Embedding via Adaptive Aggregation: Learning Generalizable Person Re-identification. (arXiv:2112.08684v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08684","description":"<p>Domain generalizable (DG) person re-identification (ReID) aims to test across\nunseen domains without access to the target domain data at training time, which\nis a realistic but challenging problem. In contrast to methods assuming an\nidentical model for different domains, Mixture of Experts (MoE) exploits\nmultiple domain-specific networks for leveraging complementary information\nbetween domains, obtaining impressive results. However, prior MoE-based DG ReID\nmethods suffer from a large model size with the increase of the number of\nsource domains, and most of them overlook the exploitation of domain-invariant\ncharacteristics. To handle the two issues above, this paper presents a new\napproach called Mimic Embedding via adapTive Aggregation (META) for DG person\nReID. To avoid the large model size, experts in META do not adopt a branch\nnetwork for each source domain but share all the parameters except for the\nbatch normalization layers. Besides multiple experts, META leverages Instance\nNormalization (IN) and introduces it into a global branch to pursue invariant\nfeatures across domains. Meanwhile, META considers the relevance of an unseen\ntarget sample and source domains via normalization statistics and develops an\naggregation module to adaptively integrate multiple experts for mimicking\nunseen target domain. Benefiting from a proposed consistency loss and an\nepisodic training algorithm, META is expected to mimic embedding for a truly\nunseen target domain. Extensive experiments verify that META surpasses\nstate-of-the-art DG person ReID methods by a large margin. Our code is\navailable at https://github.com/xbq1994/META.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Boqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lingxiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DProST: Dynamic Projective Spatial Transformer Network for 6D Pose Estimation. (arXiv:2112.08775v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08775","description":"<p>Predicting the object's 6D pose from a single RGB image is a fundamental\ncomputer vision task. Generally, the distance between transformed object\nvertices is employed as an objective function for pose estimation methods.\nHowever, projective geometry in the camera space is not considered in those\nmethods and causes performance degradation. In this regard, we propose a new\npose estimation system based on a projective grid instead of object vertices.\nOur pose estimation method, dynamic projective spatial transformer network\n(DProST), localizes the region of interest grid on the rays in camera space and\ntransforms the grid to object space by estimated pose. The transformed grid is\nused as both a sampling grid and a new criterion of the estimated pose.\nAdditionally, because DProST does not require object vertices, our method can\nbe used in a mesh-less setting by replacing the mesh with a reconstructed\nfeature. Experimental results show that mesh-less DProST outperforms the\nstate-of-the-art mesh-based methods on the LINEMOD and LINEMOD-OCCLUSION\ndataset, and shows competitive performance on the YCBV dataset with mesh data.\nThe source code is available at https://github.com/parkjaewoo0611/DProST\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaewoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds. (arXiv:2112.08879v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08879","description":"<p>Most models tasked to ground referential utterances in 2D and 3D scenes learn\nto select the referred object from a pool of object proposals provided by a\npre-trained detector. This is limiting because an utterance may refer to visual\nentities at various levels of granularity, such as the chair, the leg of the\nchair, or the tip of the front leg of the chair, which may be missed by the\ndetector. We propose a language grounding model that attends on the referential\nutterance and on the object proposal pool computed from a pre-trained detector\nto decode referenced objects with a detection head, without selecting them from\nthe pool. In this way, it is helped by powerful pre-trained object detectors\nwithout being restricted by their misses. We call our model Bottom Up Top Down\nDEtection TRansformers (BUTD-DETR) because it uses both language guidance (top\ndown) and objectness guidance (bottom-up) to ground referential utterances in\nimages and point clouds. Moreover, BUTD-DETR casts object detection as\nreferential grounding and uses object labels as language prompts to be grounded\nin the visual scene, augmenting supervision for the referential grounding task\nin this way. The proposed model sets a new state-of-the-art across popular 3D\nlanguage grounding benchmarks with significant performance gains over previous\n3D approaches (12.6% on SR3D, 11.6% on NR3D and 6.3% on ScanRefer). When\napplied in 2D images, it performs on par with the previous state of the art. We\nablate the design choices of our model and quantify their contribution to\nperformance. Our code and checkpoints can be found at the project website\nhttps://butd-detr.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1\">Nikolaos Gkanatsios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mediratta_I/0/1/0/all/0/1\">Ishita Mediratta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleSwin: Transformer-based GAN for High-resolution Image Generation. (arXiv:2112.10762v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10762","description":"<p>Despite the tantalizing success in a broad of vision tasks, transformers have\nnot yet demonstrated on-par ability as ConvNets in high-resolution image\ngenerative modeling. In this paper, we seek to explore using pure transformers\nto build a generative adversarial network for high-resolution image synthesis.\nTo this end, we believe that local attention is crucial to strike the balance\nbetween computational efficiency and modeling capacity. Hence, the proposed\ngenerator adopts Swin transformer in a style-based architecture. To achieve a\nlarger receptive field, we propose double attention which simultaneously\nleverages the context of the local and the shifted windows, leading to improved\ngeneration quality. Moreover, we show that offering the knowledge of the\nabsolute position that has been lost in window-based transformers greatly\nbenefits the generation quality. The proposed StyleSwin is scalable to high\nresolutions, with both the coarse geometry and fine structures benefit from the\nstrong expressivity of transformers. However, blocking artifacts occur during\nhigh-resolution synthesis because performing the local attention in a\nblock-wise manner may break the spatial coherency. To solve this, we\nempirically investigate various solutions, among which we find that employing a\nwavelet discriminator to examine the spectral discrepancy effectively\nsuppresses the artifacts. Extensive experiments show the superiority over prior\ntransformer-based GANs, especially on high resolutions, e.g., 1024x1024. The\nStyleSwin, without complex training strategies, excels over StyleGAN on\nCelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the\npromise of using transformers for high-resolution image generation. The code\nand models will be available at https://github.com/microsoft/StyleSwin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Open-Vocabulary Image Segmentation with Image-Level Labels. (arXiv:2112.12143v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12143","description":"<p>We design an open-vocabulary image segmentation model to organize an image\ninto meaningful regions indicated by arbitrary texts. Recent works (CLIP and\nALIGN), despite attaining impressive open-vocabulary classification accuracy\nwith image-level caption labels, are unable to segment visual concepts with\npixels. We argue that these models miss an important step of visual grouping,\nwhich organizes pixels into groups before learning visual-semantic alignments.\nWe propose OpenSeg to address the above issue while still making use of\nscalable image-level supervision of captions. First, it learns to propose\nsegmentation masks for possible organizations. Then it learns visual-semantic\nalignments by aligning each word in a caption to one or a few predicted masks.\nWe find the mask representations are the key to support learning image\nsegmentation from captions, making it possible to scale up the dataset and\nvocabulary sizes. OpenSeg significantly outperforms the recent open-vocabulary\nmethod of LSeg by +19.9 mIoU on PASCAL dataset, thanks to its scalability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1\">Golnaz Ghiasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiuye Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos. (arXiv:2112.13715v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13715","description":"<p>When analyzing human motion videos, the output jitters from existing pose\nestimators are highly-unbalanced with varied estimation errors across frames.\nMost frames in a video are relatively easy to estimate and only suffer from\nslight jitters. In contrast, for rarely seen or occluded actions, the estimated\npositions of multiple joints largely deviate from the ground truth values for a\nconsecutive sequence of frames, rendering significant jitters on them. To\ntackle this problem, we propose to attach a dedicated temporal-only refinement\nnetwork to existing pose estimators for jitter mitigation, named SmoothNet.\nUnlike existing learning-based solutions that employ spatio-temporal models to\nco-optimize per-frame precision and temporal smoothness at all the joints,\nSmoothNet models the natural smoothness characteristics in body movements by\nlearning the long-range temporal relations of every joint without considering\nthe noisy correlations among joints. With a simple yet effective motion-aware\nfully-connected network, SmoothNet improves the temporal smoothness of existing\npose estimators significantly and enhances the estimation accuracy of those\nchallenging frames as a side-effect. Moreover, as a temporal-only model, a\nunique advantage of SmoothNet is its strong transferability across various\ntypes of estimators and datasets. Comprehensive experiments on five datasets\nwith eleven popular backbone networks across 2D and 3D pose estimation and body\nrecovery tasks demonstrate the efficacy of the proposed solution. Code is\navailable at https://github.com/cure-lab/SmoothNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_X/0/1/0/all/0/1\">Xuan Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Query Video Retrieval. (arXiv:2201.03639v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03639","description":"<p>Retrieving target videos based on text descriptions is a task of great\npractical value and has received increasing attention over the past few years.\nDespite recent progress, imperfect annotations in existing video retrieval\ndatasets have posed significant challenges on model evaluation and development.\nIn this paper, we tackle this issue by focusing on the less-studied setting of\nmulti-query video retrieval, where multiple descriptions are provided to the\nmodel for searching over the video archive. We first show that multi-query\nretrieval task effectively mitigates the dataset noise introduced by imperfect\nannotations and better correlates with human judgement on evaluating retrieval\nabilities of current models. We then investigate several methods which leverage\nmultiple queries at training time, and demonstrate that the multi-query\ninspired training can lead to superior performance and better generalization.\nWe hope further investigation in this direction can bring new insights on\nbuilding systems that perform better in real-world video retrieval\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Robustness by Enhancing Weak Subnets. (arXiv:2201.12765v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12765","description":"<p>Despite their success, deep networks have been shown to be highly susceptible\nto perturbations, often causing significant drops in accuracy. In this paper,\nwe investigate model robustness on perturbed inputs by studying the performance\nof internal sub-networks (subnets). Interestingly, we observe that most subnets\nshow particularly poor robustness against perturbations. More importantly,\nthese weak subnets are correlated with the overall lack of robustness. Tackling\nthis phenomenon, we propose a new training procedure that identifies and\nenhances weak subnets (EWS) to improve robustness. Specifically, we develop a\nsearch algorithm to find particularly weak subnets and explicitly strengthen\nthem via knowledge distillation from the full network. We show that EWS greatly\nimproves both robustness against corrupted images as well as accuracy on clean\ndata. Being complementary to popular data augmentation methods, EWS\nconsistently improves robustness when combined with these approaches. To\nhighlight the flexibility of our approach, we combine EWS also with popular\nadversarial training methods resulting in improved adversarial robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1\">David Stutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Webly Supervised Concept Expansion for General Purpose Vision Models. (arXiv:2202.02317v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02317","description":"<p>General Purpose Vision (GPV) systems are models that are designed to solve a\nwide array of visual tasks without requiring architectural changes. Today, GPVs\nprimarily learn both skills and concepts from large fully supervised datasets.\nScaling GPVs to tens of thousands of concepts by acquiring data to learn each\nconcept for every skill quickly becomes prohibitive. This work presents an\neffective and inexpensive alternative: learn skills from supervised datasets,\nlearn concepts from web image search, and leverage a key characteristic of\nGPVs: the ability to transfer visual knowledge across skills. We use a dataset\nof 1M+ images spanning 10k+ visual concepts to demonstrate webly-supervised\nconcept expansion for two existing GPVs (GPV-1 and VL-T5) on 3 benchmarks: 5\nCOCO-based datasets (80 primary concepts), a newly curated series of 5 datasets\nbased on the OpenImages and VisualGenome repositories (~500 concepts), and the\nWeb-derived dataset (10k+ concepts). We also propose a new architecture, GPV-2\nthat supports a variety of tasks -- from vision tasks like classification and\nlocalization to vision+language tasks like QA and captioning, to more niche\nones like human-object interaction detection. GPV-2 benefits hugely from web\ndata and outperforms GPV-1 and VL-T5 across these benchmarks. Our data, code,\nand web demo are available at https://prior.allenai.org/projects/gpv2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Amita Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_C/0/1/0/all/0/1\">Christopher Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanmay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolve_E/0/1/0/all/0/1\">Eric Kolve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1\">Derek Hoiem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realistic Blur Synthesis for Learning Image Deblurring. (arXiv:2202.08771v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08771","description":"<p>Training learning-based deblurring methods demands a tremendous amount of\nblurred and sharp image pairs. Unfortunately, existing synthetic datasets are\nnot realistic enough, and deblurring models trained on them cannot handle real\nblurred images effectively. While real datasets have recently been proposed,\nthey provide limited diversity of scenes and camera settings, and capturing\nreal datasets for diverse settings is still challenging. To resolve this, this\npaper analyzes various factors that introduce differences between real and\nsynthetic blurred images. To this end, we present RSBlur, a novel dataset with\nreal blurred images and the corresponding sharp image sequences to enable a\ndetailed analysis of the difference between real and synthetic blur. With the\ndataset, we reveal the effects of different factors in the blur generation\nprocess. Based on the analysis, we also present a novel blur synthesis pipeline\nto synthesize more realistic blur. We show that our synthesis pipeline can\nimprove the deblurring performance on real blurred images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rim_J/0/1/0/all/0/1\">Jaesung Rim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geonung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jungeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FS-COCO: Towards Understanding of Freehand Sketches of Common Objects in Context. (arXiv:2203.02113v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02113","description":"<p>We advance sketch research to scenes with the first dataset of freehand scene\nsketches, FS-COCO. With practical applications in mind, we collect sketches\nthat convey scene content well but can be sketched within a few minutes by a\nperson with any sketching skills. Our dataset comprises 10,000 freehand scene\nvector sketches with per point space-time information by 100 non-expert\nindividuals, offering both object- and scene-level abstraction. Each sketch is\naugmented with its text description. Using our dataset, we study for the first\ntime the problem of fine-grained image retrieval from freehand scene sketches\nand sketch captions. We draw insights on: (i) Scene salience encoded in\nsketches using the strokes temporal order; (ii) Performance comparison of image\nretrieval from a scene sketch and an image caption; (iii) Complementarity of\ninformation in sketches and image captions, as well as the potential benefit of\ncombining the two modalities. In addition, we extend a popular vector sketch\nLSTM-based encoder to handle sketches with larger complexity than was supported\nby previous work. Namely, we propose a hierarchical sketch decoder, which we\nleverage at a sketch-specific \"pre-text\" task. Our dataset enables for the\nfirst time research on freehand scene sketch understanding and its practical\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Nath Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1\">Aneeshan Sain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gryaditskaya_Y/0/1/0/all/0/1\">Yulia Gryaditskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminability-Transferability Trade-Off: An Information-Theoretic Perspective. (arXiv:2203.03871v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03871","description":"<p>This work simultaneously considers the discriminability and transferability\nproperties of deep representations in the typical supervised learning task,\ni.e., image classification. By a comprehensive temporal analysis, we observe a\ntrade-off between these two properties. The discriminability keeps increasing\nwith the training progressing while the transferability intensely diminishes in\nthe later training period.\n</p>\n<p>From the perspective of information-bottleneck theory, we reveal that the\nincompatibility between discriminability and transferability is attributed to\nthe over-compression of input information. More importantly, we investigate why\nand how the InfoNCE loss can alleviate the over-compression, and further\npresent a learning framework, named contrastive temporal coding~(CTC), to\ncounteract the over-compression and alleviate the incompatibility. Extensive\nexperiments validate that CTC successfully mitigates the incompatibility,\nyielding discriminative and transferable representations. Noticeable\nimprovements are achieved on the image classification task and challenging\ntransfer learning tasks. We hope that this work will raise the significance of\nthe transferability property in the conventional supervised learning setting.\nCode is available at https://github.com/DTennant/dt-tradeoff.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Quan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhao-Min Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Borui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Renjie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiajun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Boyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshie_O/0/1/0/all/0/1\">Osamu Yoshie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClearPose: Large-scale Transparent Object Dataset and Benchmark. (arXiv:2203.03890v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03890","description":"<p>Transparent objects are ubiquitous in household settings and pose distinct\nchallenges for visual sensing and perception systems. The optical properties of\ntransparent objects leave conventional 3D sensors alone unreliable for object\ndepth and pose estimation. These challenges are highlighted by the shortage of\nlarge-scale RGB-Depth datasets focusing on transparent objects in real-world\nsettings. In this work, we contribute a large-scale real-world RGB-Depth\ntransparent object dataset named ClearPose to serve as a benchmark dataset for\nsegmentation, scene-level depth completion and object-centric pose estimation\ntasks. The ClearPose dataset contains over 350K labeled real-world RGB-Depth\nframes and 5M instance annotations covering 63 household objects. The dataset\nincludes object categories commonly used in daily life under various lighting\nand occluding conditions as well as challenging test scenarios such as cases of\nocclusion by opaque or translucent objects, non-planar orientations, presence\nof liquids, etc. We benchmark several state-of-the-art depth completion and\nobject pose estimation deep neural networks on ClearPose. The dataset and\nbenchmarking source code is available at https://github.com/opipari/ClearPose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaotong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zeren Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opipari_A/0/1/0/all/0/1\">Anthony Opipari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_O/0/1/0/all/0/1\">Odest Chadwicke Jenkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Multimodal Guidance for Medical Image Classification. (arXiv:2203.05683v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05683","description":"<p>Medical imaging is a cornerstone of therapy and diagnosis in modern medicine.\nHowever, the choice of imaging modality for a particular theranostic task\ntypically involves trade-offs between the feasibility of using a particular\nmodality (e.g., short wait times, low cost, fast acquisition, reduced\nradiation/invasiveness) and the expected performance on a clinical task (e.g.,\ndiagnostic accuracy, efficacy of treatment planning and guidance). In this\nwork, we aim to apply the knowledge learned from the less feasible but\nbetter-performing (superior) modality to guide the utilization of the\nmore-feasible yet under-performing (inferior) modality and steer it towards\nimproved performance. We focus on the application of deep learning for\nimage-based diagnosis. We develop a light-weight guidance model that leverages\nthe latent representation learned from the superior modality, when training a\nmodel that consumes only the inferior modality. We examine the advantages of\nour method in the context of two clinical applications: multi-task skin lesion\nclassification from clinical and dermoscopic images and brain tumor\nclassification from multi-sequence magnetic resonance imaging (MRI) and\nhistopathology images. For both these scenarios we show a boost in diagnostic\nperformance of the inferior modality without requiring the superior modality.\nFurthermore, in the case of brain tumor classification, our method outperforms\nthe model trained on the superior modality while producing comparable results\nto the model that uses both modalities during inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mallya_M/0/1/0/all/0/1\">Mayur Mallya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PC-SwinMorph: Patch Representation for Unsupervised Medical Image Registration and Segmentation. (arXiv:2203.05684v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05684","description":"<p>Medical image registration and segmentation are critical tasks for several\nclinical procedures. Manual realisation of those tasks is time-consuming and\nthe quality is highly dependent on the level of expertise of the physician. To\nmitigate that laborious task, automatic tools have been developed where the\nmajority of solutions are supervised techniques. However, in medical domain,\nthe strong assumption of having a well-representative ground truth is far from\nbeing realistic. To overcome this challenge, unsupervised techniques have been\ninvestigated. However, they are still limited in performance and they fail to\nproduce plausible results. In this work, we propose a novel unified\nunsupervised framework for image registration and segmentation that we called\nPC-SwinMorph. The core of our framework is two patch-based strategies, where we\ndemonstrate that patch representation is key for performance gain. We first\nintroduce a patch-based contrastive strategy that enforces locality conditions\nand richer feature representation. Secondly, we utilise a 3D\nwindow/shifted-window multi-head self-attention module as a patch stitching\nstrategy to eliminate artifacts from the patch splitting. We demonstrate,\nthrough a set of numerical and visual results, that our technique outperforms\ncurrent state-of-the-art unsupervised techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhening Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica I. Aviles-Rivero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit field supervision for robust non-rigid shape matching. (arXiv:2203.07694v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07694","description":"<p>Establishing a correspondence between two non-rigidly deforming shapes is one\nof the most fundamental problems in visual computing. Existing methods often\nshow weak resilience when presented with challenges innate to real-world data\nsuch as noise, outliers, self-occlusion etc. On the other hand, auto-decoders\nhave demonstrated strong expressive power in learning geometrically meaningful\nlatent embeddings. However, their use in \\emph{shape analysis} has been\nlimited. In this paper, we introduce an approach based on an auto-decoder\nframework, that learns a continuous shape-wise deformation field over a fixed\ntemplate. By supervising the deformation field for points on-surface and\nregularising for points off-surface through a novel \\emph{Signed Distance\nRegularisation} (SDR), we learn an alignment between the template and shape\n\\emph{volumes}. Trained on clean water-tight meshes, \\emph{without} any\ndata-augmentation, we demonstrate compelling performance on compromised data\nand real-world scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundararaman_R/0/1/0/all/0/1\">Ramana Sundararaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_G/0/1/0/all/0/1\">Gautam Pai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation. (arXiv:2203.08713v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08713","description":"<p>This paper proposes a simple baseline framework for video-based 2D/3D human\npose estimation that can achieve 10 times efficiency improvement over existing\nworks without any performance degradation, named DeciWatch. Unlike current\nsolutions that estimate each frame in a video, DeciWatch introduces a simple\nyet effective sample-denoise-recover framework that only watches sparsely\nsampled frames, taking advantage of the continuity of human motions and the\nlightweight pose representation. Specifically, DeciWatch uniformly samples less\nthan 10% video frames for detailed estimation, denoises the estimated 2D/3D\nposes with an efficient Transformer architecture, and then accurately recovers\nthe rest of the frames using another Transformer-based network. Comprehensive\nexperimental results on three video-based human pose estimation and body mesh\nrecovery tasks with four datasets validate the efficiency and effectiveness of\nDeciWatch. Code is available at https://github.com/cure-lab/DeciWatch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_X/0/1/0/all/0/1\">Xuan Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruiyuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knee arthritis severity measurement using deep learning: a publicly available algorithm with a multi-institutional validation showing radiologist-level performance. (arXiv:2203.08914v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.08914","description":"<p>The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a\ncentral criteria for the use of total knee arthroplasty. However, this\nassessment suffers from imprecise standards and a remarkably high inter-reader\nvariability. An algorithmic, automated assessment of KOA severity could improve\noverall outcomes of knee replacement procedures by increasing the\nappropriateness of its use. We propose a novel deep learning-based five-step\nalgorithm to automatically grade KOA from posterior-anterior (PA) views of\nradiographs: (1) image preprocessing (2) localization of knees joints in the\nimage using the YOLO v3-Tiny model, (3) initial assessment of the severity of\nosteoarthritis using a convolutional neural network-based classifier, (4)\nsegmentation of the joints and calculation of the joint space narrowing (JSN),\nand (5), a combination of the JSN and the initial assessment to determine a\nfinal Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation\nmasks used to make the assessment, our algorithm demonstrates a higher degree\nof transparency compared to typical \"black box\" deep learning classifiers. We\nperform a comprehensive evaluation using two public datasets and one dataset\nfrom our institution, and show that our algorithm reaches state-of-the art\nperformance. Moreover, we also collected ratings from multiple radiologists at\nour institution and showed that our algorithm performs at the radiologist\nlevel.\n</p>\n<p>The software has been made publicly available at\nhttps://github.com/MaciejMazurowski/osteoarthritis-classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gu_H/0/1/0/all/0/1\">Hanxue Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Keyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colglazier_R/0/1/0/all/0/1\">Roy J. Colglazier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jichen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lebhar_M/0/1/0/all/0/1\">Michael Lebhar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ODonnell_J/0/1/0/all/0/1\">Jonathan O&#x27;Donnell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiranek_W/0/1/0/all/0/1\">William A. Jiranek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mather_R/0/1/0/all/0/1\">Richard C. Mather</a>, <a href=\"http://arxiv.org/find/eess/1/au:+French_R/0/1/0/all/0/1\">Rob J. French</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Said_N/0/1/0/all/0/1\">Nicholas Said</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jikai Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_C/0/1/0/all/0/1\">Christine Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazurowski_M/0/1/0/all/0/1\">Maciej A. Mazurowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers. (arXiv:2203.10157v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10157","description":"<p>Novel view synthesis is a long-standing problem. In this work, we consider a\nvariant of the problem where we are given only a few context views sparsely\ncovering a scene or an object. The goal is to predict novel viewpoints in the\nscene, which requires learning priors. The current state of the art is based on\nNeural Radiance Field (NeRF), and while achieving impressive results, the\nmethods suffer from long training times as they require evaluating millions of\n3D point samples via a neural network for each image. We propose a 2D-only\nmethod that maps multiple context views and a query pose to a new image in a\nsingle pass of a neural network. Our model uses a two-stage architecture\nconsisting of a codebook and a transformer model. The codebook is used to embed\nindividual images into a smaller latent space, and the transformer solves the\nview synthesis task in this more compact space. To train our model efficiently,\nwe introduce a novel branching attention mechanism that allows us to use the\nsame model not only for neural rendering but also for camera pose estimation.\nExperimental results on real-world scenes show that our approach is competitive\ncompared to NeRF-based methods while not reasoning explicitly in 3D, and it is\nfaster to train.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulhanek_J/0/1/0/all/0/1\">Jon&#xe1;&#x161; Kulh&#xe1;nek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derner_E/0/1/0/all/0/1\">Erik Derner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1\">Torsten Sattler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1\">Robert Babu&#x161;ka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields. (arXiv:2203.10821v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10821","description":"<p>Image translation and manipulation have gain increasing attention along with\nthe rapid development of deep generative models. Although existing approaches\nhave brought impressive results, they mainly operated in 2D space. In light of\nrecent advances in NeRF-based 3D-aware generative models, we introduce a new\ntask, Semantic-to-NeRF translation, that aims to reconstruct a 3D scene\nmodelled by NeRF, conditioned on one single-view semantic mask as input. To\nkick-off this novel task, we propose the Sem2NeRF framework. In particular,\nSem2NeRF addresses the highly challenging task by encoding the semantic mask\ninto the latent code that controls the 3D scene representation of a pre-trained\ndecoder. To further improve the accuracy of the mapping, we integrate a new\nregion-aware learning strategy into the design of both the encoder and the\ndecoder. We verify the efficacy of the proposed Sem2NeRF and demonstrate that\nit outperforms several strong baselines on two benchmark datasets. Code and\nvideo are available at https://donydchen.github.io/sem2nerf/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuedong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanxia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cham_T/0/1/0/all/0/1\">Tat-Jen Cham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Network for Future Hand Segmentation from Egocentric Video. (arXiv:2203.11305v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11305","description":"<p>We introduce the novel problem of anticipating a time series of future hand\nmasks from egocentric video. A key challenge is to model the stochasticity of\nfuture head motions, which globally impact the head-worn camera video analysis.\nTo this end, we propose a novel deep generative model -- EgoGAN, which uses a\n3D Fully Convolutional Network to learn a spatio-temporal video representation\nfor pixel-wise visual anticipation, generates future head motion using\nGenerative Adversarial Network (GAN), and then predicts the future hand masks\nbased on the video representation and the generated future head motion. We\nevaluate our method on both the EPIC-Kitchens and the EGTEA Gaze+ datasets. We\nconduct detailed ablation studies to validate the design choices of our\napproach. Furthermore, we compare our method with previous state-of-the-art\nmethods on future image segmentation and show that our method can more\naccurately predict future hand masks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wenqi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Patch Exiting for Scalable Single Image Super-Resolution. (arXiv:2203.11589v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11589","description":"<p>Since the future of computing is heterogeneous, scalability is a crucial\nproblem for single image super-resolution. Recent works try to train one\nnetwork, which can be deployed on platforms with different capacities. However,\nthey rely on the pixel-wise sparse convolution, which is not hardware-friendly\nand achieves limited practical speedup. As image can be divided into patches,\nwhich have various restoration difficulties, we present a scalable method based\non Adaptive Patch Exiting (APE) to achieve more practical speedup.\nSpecifically, we propose to train a regressor to predict the incremental\ncapacity of each layer for the patch. Once the incremental capacity is below\nthe threshold, the patch can exit at the specific layer. Our method can easily\nadjust the trade-off between performance and efficiency by changing the\nthreshold of incremental capacity. Furthermore, we propose a novel strategy to\nenable the network training of our method. We conduct extensive experiments\nacross various backbones, datasets and scaling factors to demonstrate the\nadvantages of our method. Code is available at\nhttps://github.com/littlepure2333/APE\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shizun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-enabled Assessment of Cardiac Systolic and Diastolic Function from Echocardiography. (arXiv:2203.11726v2 [physics.med-ph] UPDATED)","link":"http://arxiv.org/abs/2203.11726","description":"<p>Left ventricular (LV) function is an important factor in terms of patient\nmanagement, outcome, and long-term survival of patients with heart disease. The\nmost recently published clinical guidelines for heart failure recognise that\nover reliance on only one measure of cardiac function (LV ejection fraction) as\na diagnostic and treatment stratification biomarker is suboptimal. Recent\nadvances in AI-based echocardiography analysis have shown excellent results on\nautomated estimation of LV volumes and LV ejection fraction. However, from\ntime-varying 2-D echocardiography acquisition, a richer description of cardiac\nfunction can be obtained by estimating functional biomarkers from the complete\ncardiac cycle. In this work we propose for the first time an AI approach for\nderiving advanced biomarkers of systolic and diastolic LV function from 2-D\nechocardiography based on segmentations of the full cardiac cycle. These\nbiomarkers will allow clinicians to obtain a much richer picture of the heart\nin health and disease. The AI model is based on the 'nn-Unet' framework and was\ntrained and tested using four different databases. Results show excellent\nagreement between manual and automated analysis and showcase the potential of\nthe advanced systolic and diastolic biomarkers for patient stratification.\nFinally, for a subset of 50 cases, we perform a correlation analysis between\nclinical biomarkers derived from echocardiography and CMR and we show excellent\nagreement between the two modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Puyol_Anton_E/0/1/0/all/0/1\">Esther Puyol-Ant&#xf3;n</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ruijsink_B/0/1/0/all/0/1\">Bram Ruijsink</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sidhu_B/0/1/0/all/0/1\">Baldeep S. Sidhu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gould_J/0/1/0/all/0/1\">Justin Gould</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Porter_B/0/1/0/all/0/1\">Bradley Porter</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Elliott_M/0/1/0/all/0/1\">Mark K. Elliott</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mehta_V/0/1/0/all/0/1\">Vishal Mehta</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gu_H/0/1/0/all/0/1\">Haotian Gu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Xochicale_M/0/1/0/all/0/1\">Miguel Xochicale</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gomez_A/0/1/0/all/0/1\">Alberto Gomez</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rinaldi_C/0/1/0/all/0/1\">Christopher A. Rinaldi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cowie_M/0/1/0/all/0/1\">Martin Cowie</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chowienczyk_P/0/1/0/all/0/1\">Phil Chowienczyk</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+King_A/0/1/0/all/0/1\">Andrew P. King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Broad Study of Pre-training for Domain Generalization and Adaptation. (arXiv:2203.11819v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11819","description":"<p>Deep models must learn robust and transferable representations in order to\nperform well on new domains. While domain transfer methods (e.g., domain\nadaptation, domain generalization) have been proposed to learn transferable\nrepresentations across domains, they are typically applied to ResNet backbones\npre-trained on ImageNet. Thus, existing works pay little attention to the\neffects of pre-training on domain transfer tasks. In this paper, we provide a\nbroad study and in-depth analysis of pre-training for domain adaptation and\ngeneralization, namely: network architectures, size, pre-training loss, and\ndatasets. We observe that simply using a state-of-the-art backbone outperforms\nexisting state-of-the-art domain adaptation baselines and set new baselines on\nOffice-Home and DomainNet improving by 10.7\\% and 5.5\\%. We hope that this work\ncan provide more insights for future domain transfer research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Generalization in Federated Learning by Seeking Flat Minima. (arXiv:2203.11834v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.11834","description":"<p>Models trained in federated settings often suffer from degraded performances\nand fail at generalizing, especially when facing heterogeneous scenarios. In\nthis work, we investigate such behavior through the lens of geometry of the\nloss and Hessian eigenspectrum, linking the model's lack of generalization\ncapacity to the sharpness of the solution. Motivated by prior studies\nconnecting the sharpness of the loss surface and the generalization gap, we\nshow that i) training clients locally with Sharpness-Aware Minimization (SAM)\nor its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on\nthe server-side can substantially improve generalization in Federated Learning\nand help bridging the gap with centralized models. By seeking parameters in\nneighborhoods having uniform low loss, the model converges towards flatter\nminima and its generalization significantly improves in both homogeneous and\nheterogeneous scenarios. Empirical results demonstrate the effectiveness of\nthose optimizers across a variety of benchmark vision datasets (e.g.\nCIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,\nsemantic segmentation, domain generalization).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caldarola_D/0/1/0/all/0/1\">Debora Caldarola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1\">Marco Ciccone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CM-GAN: Image Inpainting with Cascaded Modulation GAN and Object-Aware Training. (arXiv:2203.11947v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11947","description":"<p>Recent image inpainting methods have made great progress but often struggle\nto generate plausible image structures when dealing with large holes in complex\nimages. This is partially due to the lack of effective network structures that\ncan capture both the long-range dependency and high-level semantics of an\nimage. We propose cascaded modulation GAN (CM-GAN), a new network design\nconsisting of an encoder with Fourier convolution blocks that extract\nmulti-scale feature representations from the input image with holes and a\ndual-stream decoder with a novel cascaded global-spatial modulation block at\neach scale level. In each decoder block, global modulation is first applied to\nperform coarse and semantic-aware structure synthesis, followed by spatial\nmodulation to further adjust the feature map in a spatially adaptive fashion.\nIn addition, we design an object-aware training scheme to prevent the network\nfrom hallucinating new objects inside holes, fulfilling the needs of object\nremoval tasks in real-world scenarios. Extensive experiments are conducted to\nshow that our method significantly outperforms existing methods in both\nquantitative and qualitative evaluation. Please refer to the project page:\n\\url{https://github.com/htzheng/CM-GAN-Inpainting}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingwan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Scott Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_C/0/1/0/all/0/1\">Connelly Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirghodsi_S/0/1/0/all/0/1\">Sohrab Amirghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R-DFCIL: Relation-Guided Representation Learning for Data-Free Class Incremental Learning. (arXiv:2203.13104v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13104","description":"<p>Class-Incremental Learning (CIL) struggles with catastrophic forgetting when\nlearning new knowledge, and Data-Free CIL (DFCIL) is even more challenging\nwithout access to the training data of previously learned classes. Though\nrecent DFCIL works introduce techniques such as model inversion to synthesize\ndata for previous classes, they fail to overcome forgetting due to the severe\ndomain gap between the synthetic and real data. To address this issue, this\npaper proposes relation-guided representation learning (RRL) for DFCIL, dubbed\nR-DFCIL. In RRL, we introduce relational knowledge distillation to flexibly\ntransfer the structural relation of new data from the old model to the current\nmodel. Our RRL-boosted DFCIL can guide the current model to learn\nrepresentations of new classes better compatible with representations of\nprevious classes, which greatly reduces forgetting while improving plasticity.\nTo avoid the mutual interference between representation and classifier\nlearning, we employ local rather than global classification loss during RRL.\nAfter RRL, the classification head is refined with global class-balanced\nclassification loss to address the data imbalance issue as well as learn the\ndecision boundaries between new and previous classes. Extensive experiments on\nCIFAR100, Tiny-ImageNet200, and ImageNet100 demonstrate that our R-DFCIL\nsignificantly surpasses previous approaches and achieves a new state-of-the-art\nperformance for DFCIL. Code is available at\nhttps://github.com/jianzhangcs/R-DFCIL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiankun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlowFormer: A Transformer Architecture for Optical Flow. (arXiv:2203.16194v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16194","description":"<p>We introduce optical Flow transFormer, dubbed as FlowFormer, a\ntransformer-based neural network architecture for learning optical flow.\nFlowFormer tokenizes the 4D cost volume built from an image pair, encodes the\ncost tokens into a cost memory with alternate-group transformer (AGT) layers in\na novel latent space, and decodes the cost memory via a recurrent transformer\ndecoder with dynamic positional cost queries. On the Sintel benchmark,\nFlowFormer achieves 1.144 and 2.183 average end-ponit-error (AEPE) on the clean\nand final pass, a 17.6% and 11.6% error reduction from the best published\nresult (1.388 and 2.47). Besides, FlowFormer also achieves strong\ngeneralization performance. Without being trained on Sintel, FlowFormer\nachieves 0.95 AEPE on the Sintel training set clean pass, outperforming the\nbest published result (1.29) by 26.9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaoyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_K/0/1/0/all/0/1\">Ka Chun Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hongwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Semantic Segmentation. (arXiv:2204.01599v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01599","description":"<p>Deep learning approaches achieve prominent success in 3D semantic\nsegmentation. However, collecting densely annotated real-world 3D datasets is\nextremely time-consuming and expensive. Training models on synthetic data and\ngeneralizing on real-world scenarios becomes an appealing alternative, but\nunfortunately suffers from notorious domain shifts. In this work, we propose a\nData-Oriented Domain Adaptation (DODA) framework to mitigate pattern and\ncontext gaps caused by different sensing mechanisms and layout placements\nacross domains. Our DODA encompasses virtual scan simulation to imitate\nreal-world point cloud patterns and tail-aware cuboid mixing to alleviate the\ninterior context gap with a cuboid-based intermediate domain. The first\nunsupervised sim-to-real adaptation benchmark on 3D indoor semantic\nsegmentation is also built on 3D-FRONT, ScanNet and S3DIS along with 7 popular\nUnsupervised Domain Adaptation (UDA) methods. Our DODA surpasses existing UDA\napproaches by over 13% on both 3D-FRONT -&gt; ScanNet and 3D-FRONT -&gt; S3DIS. Code\nis available at https://github.com/CVMI-Lab/DODA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Runyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Movie Clip Classification with State-Space Video Models. (arXiv:2204.01692v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01692","description":"<p>Most modern video recognition models are designed to operate on short video\nclips (e.g., 5-10s in length). Thus, it is challenging to apply such models to\nlong movie understanding tasks, which typically require sophisticated\nlong-range temporal reasoning. The recently introduced video transformers\npartially address this issue by using long-range temporal self-attention.\nHowever, due to the quadratic cost of self-attention, such models are often\ncostly and impractical to use. Instead, we propose ViS4mer, an efficient\nlong-range video model that combines the strengths of self-attention and the\nrecently introduced structured state-space sequence (S4) layer. Our model uses\na standard Transformer encoder for short-range spatiotemporal feature\nextraction, and a multi-scale temporal S4 decoder for subsequent long-range\ntemporal reasoning. By progressively reducing the spatiotemporal feature\nresolution and channel dimension at each decoder layer, ViS4mer learns complex\nlong-range spatiotemporal dependencies in a video. Furthermore, ViS4mer is\n$2.63\\times$ faster and requires $8\\times$ less GPU memory than the\ncorresponding pure self-attention-based model. Additionally, ViS4mer achieves\nstate-of-the-art results in $6$ out of $9$ long-form movie video classification\ntasks on the Long Video Understanding (LVU) benchmark. Furthermore, we show\nthat our approach successfully generalizes to other domains, achieving\ncompetitive results on the Breakfast and the COIN procedural activity datasets.\nThe code is publicly available at: https://github.com/md-mohaiminul/ViS4mer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Mohaiminul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Prediction of Future Lesion Activity and Treatment Effect in Multiple Sclerosis from Baseline MRI. (arXiv:2204.01702v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.01702","description":"<p>Precision medicine for chronic diseases such as multiple sclerosis (MS)\ninvolves choosing a treatment which best balances efficacy and side\neffects/preferences for individual patients. Making this choice as early as\npossible is important, as delays in finding an effective therapy can lead to\nirreversible disability accrual. To this end, we present the first deep neural\nnetwork model for individualized treatment decisions from baseline magnetic\nresonance imaging (MRI) (with clinical information if available) for MS\npatients. Our model (a) predicts future new and enlarging T2 weighted (NE-T2)\nlesion counts on follow-up MRI on multiple treatments and (b) estimates the\nconditional average treatment effect (CATE), as defined by the predicted future\nsuppression of NE-T2 lesions, between different treatment options relative to\nplacebo. Our model is validated on a proprietary federated dataset of 1817\nmulti-sequence MRIs acquired from MS patients during four multi-centre\nrandomized clinical trials. Our framework achieves high average precision in\nthe binarized regression of future NE-T2 lesions on five different treatments,\nidentifies heterogeneous treatment effects, and provides a personalized\ntreatment recommendation that accounts for treatment-associated risk (e.g. side\neffects, patient preference, administration difficulties).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Durso_Finley_J/0/1/0/all/0/1\">Joshua Durso-Finley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Falet_J/0/1/0/all/0/1\">Jean-Pierre R. Falet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D face reconstruction with dense landmarks. (arXiv:2204.02776v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02776","description":"<p>Landmarks often play a key role in face analysis, but many aspects of\nidentity or expression cannot be represented by sparse landmarks alone. Thus,\nin order to reconstruct faces more accurately, landmarks are often combined\nwith additional signals like depth images or techniques like differentiable\nrendering. Can we keep things simple by just using more landmarks? In answer,\nwe present the first method that accurately predicts 10x as many landmarks as\nusual, covering the whole head, including the eyes and teeth. This is\naccomplished using synthetic training data, which guarantees perfect landmark\nannotations. By fitting a morphable model to these dense landmarks, we achieve\nstate-of-the-art results for monocular 3D face reconstruction in the wild. We\nshow that dense landmarks are an ideal signal for integrating face shape\ninformation across frames by demonstrating accurate and expressive facial\nperformance capture in both monocular and multi-view scenarios. This approach\nis also highly efficient: we can predict dense landmarks and fit our 3D face\nmodel at over 150FPS on a single CPU thread. Please see our website:\nhttps://microsoft.github.io/DenseLandmarks/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wood_E/0/1/0/all/0/1\">Erroll Wood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltrusaitis_T/0/1/0/all/0/1\">Tadas Baltrusaitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_C/0/1/0/all/0/1\">Charlie Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Matthew Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jingjing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milosavljevic_N/0/1/0/all/0/1\">Nikola Milosavljevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilde_D/0/1/0/all/0/1\">Daniel Wilde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garbin_S/0/1/0/all/0/1\">Stephan Garbin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_C/0/1/0/all/0/1\">Chirag Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shotton_J/0/1/0/all/0/1\">Jamie Shotton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharp_T/0/1/0/all/0/1\">Toby Sharp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stojiljkovic_I/0/1/0/all/0/1\">Ivan Stojiljkovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cashman_T/0/1/0/all/0/1\">Tom Cashman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentin_J/0/1/0/all/0/1\">Julien Valentin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. (arXiv:2204.02874v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02874","description":"<p>We introduce an audiovisual method for long-range text-to-video retrieval.\nUnlike previous approaches designed for short video retrieval (e.g., 5-15\nseconds in duration), our approach aims to retrieve minute-long videos that\ncapture complex human actions. One challenge of standard video-only approaches\nis the large computational cost associated with processing hundreds of densely\nextracted frames from such long videos. To address this issue, we propose to\nreplace parts of the video with compact audio cues that succinctly summarize\ndynamic audio events and are cheap to process. Our method, named ECLIPSE\n(Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an\naudiovisual video setting, by adding a unified audiovisual transformer block\nthat captures complementary cues from the video and audio streams. In addition\nto being 2.92x faster and 2.34x memory-efficient than long-range video-only\napproaches, our method also achieves better text-to-video retrieval accuracy on\nseveral diverse long-range video datasets such as ActivityNet, QVHighlights,\nYouCook2, DiDeMo and Charades.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yan-Bo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSGN++: Exploiting Visual-Spatial Relation for Stereo-based 3D Detectors. (arXiv:2204.03039v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03039","description":"<p>Camera-based 3D object detectors are welcome due to their wider deployment\nand lower price than LiDAR sensors. We first revisit the prior stereo detector\nDSGN for its stereo volume construction ways for representing both 3D geometry\nand semantics. We polish the stereo modeling and propose the advanced version,\nDSGN++, aiming to enhance effective information flow throughout the 2D-to-3D\npipeline in three main aspects. First, to effectively lift the 2D information\nto stereo volume, we propose depth-wise plane sweeping (DPS) that allows denser\nconnections and extracts depth-guided features. Second, for grasping\ndifferently spaced features, we present a novel stereo volume -- Dual-view\nStereo Volume (DSV) that integrates front-view and top-view features and\nreconstructs sub-voxel depth in the camera frustum. Third, as the foreground\nregion becomes less dominant in 3D space, we propose a multi-modal data editing\nstrategy -- Stereo-LiDAR Copy-Paste, which ensures cross-modal alignment and\nimproves data efficiency. Without bells and whistles, extensive experiments in\nvarious modality setups on the popular KITTI benchmark show that our method\nconsistently outperforms other camera-based 3D detectors for all categories.\nCode is available at https://github.com/chenyilun95/DSGN2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shijia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking. (arXiv:2204.07049v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2204.07049","description":"<p>In this paper, we propose an iterative self-training framework for\nsim-to-real 6D object pose estimation to facilitate cost-effective robotic\ngrasping. Given a bin-picking scenario, we establish a photo-realistic\nsimulator to synthesize abundant virtual data, and use this to train an initial\npose estimation network. This network then takes the role of a teacher model,\nwhich generates pose predictions for unlabeled real data. With these\npredictions, we further design a comprehensive adaptive selection scheme to\ndistinguish reliable results, and leverage them as pseudo labels to update a\nstudent model for pose estimation on real data. To continuously improve the\nquality of pseudo labels, we iterate the above steps by taking the trained\nstudent model as a new teacher and re-label real data using the refined teacher\nmodel. We evaluate our method on a public benchmark and our newly-released\ndataset, achieving an ADD(-S) improvement of 11.49% and 22.62% respectively.\nOur method is also able to improve robotic bin-picking success by 19.54%,\ndemonstrating the potential of iterative sim-to-real solutions for robotic\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yichuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun-Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Level Set Theory for Neural Implicit Evolution under Explicit Flows. (arXiv:2204.07159v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07159","description":"<p>Coordinate-based neural networks parameterizing implicit surfaces have\nemerged as efficient representations of geometry. They effectively act as\nparametric level sets with the zero-level set defining the surface of interest.\nWe present a framework that allows applying deformation operations defined for\ntriangle meshes onto such implicit surfaces. Several of these operations can be\nviewed as energy-minimization problems that induce an instantaneous flow field\non the explicit surface. Our method uses the flow field to deform parametric\nimplicit surfaces by extending the classical theory of level sets. We also\nderive a consolidated view for existing methods on differentiable surface\nextraction and rendering, by formalizing connections to the level-set theory.\nWe show that these methods drift from the theory and that our approach exhibits\nimprovements for applications like surface smoothing, mean-curvature flow,\ninverse rendering and user-defined editing on implicit geometry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_I/0/1/0/all/0/1\">Ishit Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1\">Ravi Ramamoorthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation. (arXiv:2204.07733v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07733","description":"<p>Birds-eye-view (BEV) semantic segmentation is critical for autonomous driving\nfor its powerful spatial representation ability. It is challenging to estimate\nthe BEV semantic maps from monocular images due to the spatial gap, since it is\nimplicitly required to realize both the perspective-to-BEV transformation and\nsegmentation. We present a novel two-stage Geometry Prior-based Transformation\nframework named GitNet, consisting of (i) the geometry-guided pre-alignment and\n(ii) ray-based transformer. In the first stage, we decouple the BEV\nsegmentation into the perspective image segmentation and geometric prior-based\nmapping, with explicit supervision by projecting the BEV semantic labels onto\nthe image plane to learn visibility-aware features and learnable geometry to\ntranslate into BEV space. Second, the pre-aligned coarse BEV features are\nfurther deformed by ray-based transformers to take visibility knowledge into\naccount. GitNet achieves the leading performance on the challenging nuScenes\nand Argoverse Datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shi Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09817","description":"<p>Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision--language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1\">Stephanie Hyland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1\">Maria Wetscherek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Split for Automatic Bias Detection. (arXiv:2204.13749v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.13749","description":"<p>Classifiers are biased when trained on biased datasets. As a remedy, we\npropose Learning to Split (ls), an algorithm for automatic bias detection.\nGiven a dataset with input-label pairs, ls learns to split this dataset so that\npredictors trained on the training split cannot generalize to the testing\nsplit. This performance gap suggests that the testing split is\nunder-represented in the dataset, which is a signal of potential bias.\nIdentifying non-generalizable splits is challenging since we have no\nannotations about the bias. In this work, we show that the prediction\ncorrectness of each example in the testing split can be used as a source of\nweak supervision: generalization performance will drop if we move examples that\nare predicted correctly away from the testing split, leaving only those that\nare mis-predicted. ls is task-agnostic and can be applied to any supervised\nlearning problem, ranging from natural language understanding and image\nclassification to molecular property prediction. Empirical results show that ls\nis able to generate astonishingly challenging splits that correlate with\nhuman-identified biases. Moreover, we demonstrate that combining robust\nlearning algorithms (such as group DRO) with splits identified by ls enables\nautomatic de-biasing. Compared to previous state-of-the-art, we substantially\nimprove the worst-group performance (23.4% on average) when the source of\nbiases is unknown during training and validation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking. (arXiv:2205.02301v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02301","description":"<p>Estimating human motion from video is an active research area due to its many\npotential applications. Most state-of-the-art methods predict human shape and\nposture estimates for individual images and do not leverage the temporal\ninformation available in video. Many \"in the wild\" sequences of human motion\nare captured by a moving camera, which adds the complication of conflated\ncamera and human motion to the estimation. We therefore present BodySLAM, a\nmonocular SLAM system that jointly estimates the position, shape, and posture\nof human bodies, as well as the camera trajectory. We also introduce a novel\nhuman motion model to constrain sequential body postures and observe the scale\nof the scene. Through a series of experiments on video sequences of human\nmotion captured by a moving monocular camera, we demonstrate that BodySLAM\nimproves estimates of all human body parameters and camera poses when compared\nto estimating these separately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henning_D/0/1/0/all/0/1\">Dorian F. Henning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1\">Tristan Laidlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leutenegger_S/0/1/0/all/0/1\">Stefan Leutenegger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints. (arXiv:2205.04992v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04992","description":"<p>Image-based volumetric humans using pixel-aligned features promise\ngeneralization to unseen poses and identities. Prior work leverages global\nspatial encodings and multi-view geometric consistency to reduce spatial\nambiguity. However, global encodings often suffer from overfitting to the\ndistribution of the training data, and it is difficult to learn multi-view\nconsistent reconstruction from sparse views. In this work, we investigate\ncommon issues with existing spatial encodings and propose a simple yet highly\neffective approach to modeling high-fidelity volumetric humans from sparse\nviews. One of the key ideas is to encode relative spatial 3D information via\nsparse 3D keypoints. This approach is robust to the sparsity of viewpoints and\ncross-dataset domain gap. Our approach outperforms state-of-the-art methods for\nhead reconstruction. On human body reconstruction for unseen subjects, we also\nachieve performance comparable to prior work that uses a parametric human body\nmodel and temporal feature aggregation. Our experiments show that a majority of\nerrors in prior work stem from an inappropriate choice of spatial encoding and\nthus we suggest a new direction for high-fidelity image-based human modeling.\nhttps://markomih.github.io/KeypointNeRF\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihajlovic_M/0/1/0/all/0/1\">Marko Mihajlovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Aayush Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TOCH: Spatio-Temporal Object-to-Hand Correspondence for Motion Refinement. (arXiv:2205.07982v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.07982","description":"<p>We present TOCH, a method for refining incorrect 3D hand-object interaction\nsequences using a data prior. Existing hand trackers, especially those that\nrely on very few cameras, often produce visually unrealistic results with\nhand-object intersection or missing contacts. Although correcting such errors\nrequires reasoning about temporal aspects of interaction, most previous works\nfocus on static grasps and contacts. The core of our method are TOCH fields, a\nnovel spatio-temporal representation for modeling correspondences between hands\nand objects during interaction. TOCH fields are a point-wise, object-centric\nrepresentation, which encode the hand position relative to the object.\nLeveraging this novel representation, we learn a latent manifold of plausible\nTOCH fields with a temporal denoising auto-encoder. Experiments demonstrate\nthat TOCH outperforms state-of-the-art 3D hand-object interaction models, which\nare limited to static grasps and contacts. More importantly, our method\nproduces smooth interactions even before and after contact. Using a single\ntrained TOCH model, we quantitatively and qualitatively demonstrate its\nusefulness for correcting erroneous sequences from off-the-shelf RGB/RGB-D\nhand-object reconstruction methods and transferring grasps across objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Keyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar_B/0/1/0/all/0/1\">Bharat Lal Bhatnagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenssen_J/0/1/0/all/0/1\">Jan Eric Lenssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrasting quadratic assignments for set-based representation learning. (arXiv:2205.15814v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15814","description":"<p>The standard approach to contrastive learning is to maximize the agreement\nbetween different views of the data. The views are ordered in pairs, such that\nthey are either positive, encoding different views of the same object, or\nnegative, corresponding to views of different objects. The supervisory signal\ncomes from maximizing the total similarity over positive pairs, while the\nnegative pairs are needed to avoid collapse. In this work, we note that the\napproach of considering individual pairs cannot account for both intra-set and\ninter-set similarities when the sets are formed from the views of the data. It\nthus limits the information content of the supervisory signal available to\ntrain representations. We propose to go beyond contrasting individual pairs of\nobjects by focusing on contrasting objects as sets. For this, we use\ncombinatorial quadratic assignment theory designed to evaluate set and graph\nsimilarities and derive set-contrastive objective as a regularizer for\ncontrastive learning methods. We conduct experiments and demonstrate that our\nmethod improves learned representations for the tasks of metric learning and\nself-supervised classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moskalev_A/0/1/0/all/0/1\">Artem Moskalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1\">Ivan Sosnovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_V/0/1/0/all/0/1\">Volker Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1\">Arnold Smeulders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes. (arXiv:2206.04382v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04382","description":"<p>We propose CLIP-Actor, a text-driven motion recommendation and neural mesh\nstylization system for human mesh animation. CLIP-Actor animates a 3D human\nmesh to conform to a text prompt by recommending a motion sequence and\noptimizing mesh style attributes. We build a text-driven human motion\nrecommendation system by leveraging a large-scale human motion dataset with\nlanguage labels. Given a natural language prompt, CLIP-Actor suggests a\ntext-conforming human motion in a coarse-to-fine manner. Then, our novel\nzero-shot neural style optimization detailizes and texturizes the recommended\nmesh sequence to conform to the prompt in a temporally-consistent and\npose-agnostic manner. This is distinctive in that prior work fails to generate\nplausible results when the pose of an artist-designed mesh does not conform to\nthe text from the beginning. We further propose the spatio-temporal view\naugmentation and mask-weighted embedding attention, which stabilize the\noptimization process by leveraging multi-frame human motion and rejecting\npoorly rendered views. We demonstrate that CLIP-Actor produces plausible and\nhuman-recognizable style 3D human mesh in motion with detailed geometry and\ntexture solely from a natural language prompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Youwang_K/0/1/0/all/0/1\">Kim Youwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Yeon_K/0/1/0/all/0/1\">Kim Ji-Yeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Implicit Attention: Guided Attention by The Model Itself. (arXiv:2206.07434v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07434","description":"<p>We propose Self-Supervised Implicit Attention (SSIA), a new approach that\nadaptively guides deep neural network models to gain attention by exploiting\nthe properties of the models themselves. SSIA is a novel attention mechanism\nthat does not require any extra parameters, computation, or memory access costs\nduring inference, which is in contrast to existing attention mechanism. In\nshort, by considering attention weights as higher-level semantic information,\nwe reconsidered the implementation of existing attention mechanisms and further\npropose generating supervisory signals from higher network layers to guide\nlower network layers for parameter updates. We achieved this by building a\nself-supervised learning task using the hierarchical features of the network\nitself, which only works at the training stage. To verify the effectiveness of\nSSIA, we performed a particular implementation (called an SSIA block) in\nconvolutional neural network models and validated it on several image\nclassification datasets. The experimental results show that an SSIA block can\nsignificantly improve the model performance, even outperforms many popular\nattention methods that require additional parameters and computation costs,\nsuch as Squeeze-and-Excitation and Convolutional Block Attention Module. Our\nimplementation will be available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jinyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhemin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Segmentation of LiDAR Sequences: Dataset and Algorithm. (arXiv:2206.08194v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08194","description":"<p>Roof-mounted spinning LiDAR sensors are widely used by autonomous vehicles.\nHowever, most semantic datasets and algorithms used for LiDAR sequence\nsegmentation operate on $360^\\circ$ frames, causing an acquisition latency\nincompatible with real-time applications. To address this issue, we first\nintroduce HelixNet, a $10$ billion point dataset with fine-grained labels,\ntimestamps, and sensor rotation information necessary to accurately assess the\nreal-time readiness of segmentation algorithms. Second, we propose Helix4D, a\ncompact and efficient spatio-temporal transformer architecture specifically\ndesigned for rotating LiDAR sequences. Helix4D operates on acquisition slices\ncorresponding to a fraction of a full sensor rotation, significantly reducing\nthe total latency. Helix4D reaches accuracy on par with the best segmentation\nalgorithms on HelixNet and SemanticKITTI with a reduction of over $5\\times$ in\nterms of latency and $50\\times$ in model size. The code and data are available\nat: https://romainloiseau.fr/helixnet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loiseau_R/0/1/0/all/0/1\">Romain Loiseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1\">Mathieu Aubry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Lo&#xef;c Landrieu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VectorMapNet: End-to-end Vectorized HD Map Learning. (arXiv:2206.08920v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08920","description":"<p>Autonomous driving systems require a good understanding of surrounding\nenvironments, including moving obstacles and static High-Definition (HD)\nsemantic map elements. Existing methods approach the semantic map problem by\noffline manual annotation, which suffers from serious scalability issues.\nRecent learning-based methods produce dense rasterized segmentation predictions\nto construct maps. However, these predictions do not include instance\ninformation of individual map elements and require heuristic post-processing,\nthat involves many hand-designed components, to obtain vectorized maps. To that\nend, we introduce an end-to-end vectorized HD map learning pipeline, termed\nVectorMapNet. VectorMapNet takes onboard sensor observations and predicts a\nsparse set of polylines primitives in the bird's-eye view to model the geometry\nof HD maps. This pipeline can explicitly model the spatial relation between map\nelements and generate vectorized maps that are friendly to downstream\nautonomous driving tasks without the need for post-processing. In our\nexperiments, VectorMapNet achieves strong HD map learning performance on\nnuScenes dataset, surpassing previous state-of-the-art methods by 14.2 mAP.\nQualitatively, we also show that VectorMapNet is capable of generating\ncomprehensive maps and capturing more fine-grained details of road geometry. To\nthe best of our knowledge, VectorMapNet is the first work designed toward\nend-to-end vectorized HD map learning problems. Our project website is\navailable at https://tsinghua-mars-lab.github.io/vectormapnet/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GNN-PMB: A Simple but Effective Online 3D Multi-Object Tracker without Bells and Whistles. (arXiv:2206.10255v4 [eess.SY] UPDATED)","link":"http://arxiv.org/abs/2206.10255","description":"<p>Multi-object tracking (MOT) is among crucial applications in modern advanced\ndriver assistance systems (ADAS) and autonomous driving (AD) systems. The\nglobal nearest neighbor (GNN) filter, as the earliest random vector-based\nBayesian tracking framework, has been adopted in most of state-of-the-arts\ntrackers and widely accepted in the automotive industry. With the development\nof random finite set (RFS) theory, which facilitates a mathematically rigorous\ntreatment of the MOT problem, different variants of RFS-based Bayesian filters\nhave been developed. However, their usefulness in the real traffic for ADAS and\nAD application is still open to doubt. In this paper, it is first demonstrated\nthat the latest RFS-based Bayesian tracking framework could be superior to\ntypical random vector-based Bayesian tracking framework like GNN, via a\nsystematic comparative study of both traditional random vector-based Bayesian\nfilters with rule-based heuristic track maintenance and RFS-based Bayesian\nfilters on the nuScenes validation dataset. Then, an RFS-based tracker, namely\nPoisson multi-Bernoulli filter using the global nearest neighbor (GNN-PMB), is\nproposed to LiDAR-based MOT tasks. This GNN-PMB tracker is simple to use but\ncan achieve competitive results on the nuScenes dataset. Specifically, the\nproposed GNN-PMB tracker outperforms most of the state-of-the-art LiDAR-only\ntrackers and LiDAR and camera fusion-based trackers, ranking the 3rd among all\nLiDAR-only trackers on nuScenes 3D tracking challenge leader board1 at the time\nof submission. Our code is available at here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_L/0/1/0/all/0/1\">Liping Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxuan Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_B/0/1/0/all/0/1\">Bing Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_Q/0/1/0/all/0/1\">Qing-Long Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated GI tract segmentation using deep learning. (arXiv:2206.11048v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.11048","description":"<p>The job of Radiation oncologists is to deliver x-ray beams pointed toward the\ntumor and at the same time avoid the stomach and intestines. With MR-Linacs\n(magnetic resonance imaging and linear accelerator systems), oncologists can\nvisualize the position of the tumor and allow for precise dose according to\ntumor cell presence which can vary from day to day. The current job of\noutlining the position of the stomach and intestines to adjust the X-ray beams\ndirection for the dose delivery to the tumor while avoiding the organs. This is\na time-consuming and labor-intensive process that can easily prolong treatments\nfrom 15 minutes to an hour a day unless deep learning methods can automate the\nsegmentation process. This paper discusses an automated segmentation process\nusing deep learning to make this process faster and allow more patients to get\neffective treatment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sharma_M/0/1/0/all/0/1\">Manhar Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Activity Localisation with Uncertainties in Temporal Boundary. (arXiv:2206.12923v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.12923","description":"<p>Current methods for video activity localisation over time assume implicitly\nthat activity temporal boundaries labelled for model training are determined\nand precise. However, in unscripted natural videos, different activities mostly\ntransit smoothly, so that it is intrinsically ambiguous to determine in\nlabelling precisely when an activity starts and ends over time. Such\nuncertainties in temporal labelling are currently ignored in model training,\nresulting in learning mis-matched video-text correlation with poor\ngeneralisation in test. In this work, we solve this problem by introducing\nElastic Moment Bounding (EMB) to accommodate flexible and adaptive activity\ntemporal boundaries towards modelling universally interpretable video-text\ncorrelation with tolerance to underlying temporal uncertainties in pre-fixed\nannotations. Specifically, we construct elastic boundaries adaptively by mining\nand discovering frame-wise temporal endpoints that can maximise the alignment\nbetween video segments and query sentences. To enable both more accurate\nmatching (segment content attention) and more robust localisation (segment\nelastic boundaries), we optimise the selection of frame-wise endpoints subject\nto segment-wise contents by a novel Guided Attention mechanism. Extensive\nexperiments on three video activity localisation benchmarks demonstrate\ncompellingly the EMB's advantages over existing methods without modelling\nuncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiabo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shaogang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Lottery Ticket Hypothesis in Spiking Neural Networks. (arXiv:2207.01382v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2207.01382","description":"<p>Spiking Neural Networks (SNNs) have recently emerged as a new generation of\nlow-power deep neural networks, which is suitable to be implemented on\nlow-power mobile/edge devices. As such devices have limited memory storage,\nneural pruning on SNNs has been widely explored in recent years. Most existing\nSNN pruning works focus on shallow SNNs (2~6 layers), however, deeper SNNs (&gt;16\nlayers) are proposed by state-of-the-art SNN works, which is difficult to be\ncompatible with the current SNN pruning work. To scale up a pruning technique\ntowards deep SNNs, we investigate Lottery Ticket Hypothesis (LTH) which states\nthat dense networks contain smaller subnetworks (i.e., winning tickets) that\nachieve comparable performance to the dense networks. Our studies on LTH reveal\nthat the winning tickets consistently exist in deep SNNs across various\ndatasets and architectures, providing up to 97% sparsity without huge\nperformance degradation. However, the iterative searching process of LTH brings\na huge training computational cost when combined with the multiple timesteps of\nSNNs. To alleviate such heavy searching cost, we propose Early-Time (ET) ticket\nwhere we find the important weight connectivity from a smaller number of\ntimesteps. The proposed ET ticket can be seamlessly combined with a common\npruning techniques for finding winning tickets, such as Iterative Magnitude\nPruning (IMP) and Early-Bird (EB) tickets. Our experiment results show that the\nproposed ET ticket reduces search time by up to 38% compared to IMP or EB\nmethods. Code is available at Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyoungseob Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesha_Y/0/1/0/all/0/1\">Yeshwanth Venkatesha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_R/0/1/0/all/0/1\">Ruokai Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1\">Priyadarshini Panda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brain-Aware Replacements for Supervised Contrastive Learning in Detection of Alzheimer's Disease. (arXiv:2207.04574v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04574","description":"<p>We propose a novel framework for Alzheimer's disease (AD) detection using\nbrain MRIs. The framework starts with a data augmentation method called\nBrain-Aware Replacements (BAR), which leverages a standard brain parcellation\nto replace medically-relevant 3D brain regions in an anchor MRI from a randomly\npicked MRI to create synthetic samples. Ground truth \"hard\" labels are also\nlinearly mixed depending on the replacement ratio in order to create \"soft\"\nlabels. BAR produces a great variety of realistic-looking synthetic MRIs with\nhigher local variability compared to other mix-based methods, such as CutMix.\nOn top of BAR, we propose using a soft-label-capable supervised contrastive\nloss, aiming to learn the relative similarity of representations that reflect\nhow mixed are the synthetic MRIs using our soft labels. This way, we do not\nfully exhaust the entropic capacity of our hard labels, since we only use them\nto create soft labels and synthetic MRIs through BAR. We show that a model\npre-trained using our framework can be further fine-tuned with a cross-entropy\nloss using the hard labels that were used to create the synthetic samples. We\nvalidated the performance of our framework in a binary AD detection task\nagainst both from-scratch supervised training and state-of-the-art\nself-supervised training plus fine-tuning approaches. Then we evaluated BAR's\nindividual performance compared to another mix-based method CutMix by\nintegrating it within our framework. We show that our framework yields superior\nresults in both precision and recall for the AD detection task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seyfioglu_M/0/1/0/all/0/1\">Mehmet Sayg&#x131;n Seyfio&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_P/0/1/0/all/0/1\">Pranav Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangolli_S/0/1/0/all/0/1\">Sadjyot Gangolli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabowski_T/0/1/0/all/0/1\">Thomas Grabowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapiro_L/0/1/0/all/0/1\">Linda Shapiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Scale-Aware, Robust, and Generalizable Unsupervised Monocular Depth Estimation by Integrating IMU Motion Dynamics. (arXiv:2207.04680v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04680","description":"<p>Unsupervised monocular depth and ego-motion estimation has drawn extensive\nresearch attention in recent years. Although current methods have reached a\nhigh up-to-scale accuracy, they usually fail to learn the true scale metric due\nto the inherent scale ambiguity from training with monocular sequences. In this\nwork, we tackle this problem and propose DynaDepth, a novel scale-aware\nframework that integrates information from vision and IMU motion dynamics.\nSpecifically, we first propose an IMU photometric loss and a cross-sensor\nphotometric consistency loss to provide dense supervision and absolute scales.\nTo fully exploit the complementary information from both sensors, we further\ndrive a differentiable camera-centric extended Kalman filter (EKF) to update\nthe IMU preintegrated motions when observing visual measurements. In addition,\nthe EKF formulation enables learning an ego-motion uncertainty measure, which\nis non-trivial for unsupervised methods. By leveraging IMU during training,\nDynaDepth not only learns an absolute scale, but also provides a better\ngeneralization ability and robustness against vision degradation such as\nillumination change and moving objects. We validate the effectiveness of\nDynaDepth by conducting extensive experiments and simulations on the KITTI and\nMake3D datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Graph Transformer for Video Question Answering. (arXiv:2207.05342v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05342","description":"<p>This paper proposes a Video Graph Transformer (VGT) model for Video Quetion\nAnswering (VideoQA). VGT's uniqueness are two-fold: 1) it designs a dynamic\ngraph transformer module which encodes video by explicitly capturing the visual\nobjects, their relations, and dynamics for complex spatio-temporal reasoning;\nand 2) it exploits disentangled video and text Transformers for relevance\ncomparison between the video and text to perform QA, instead of entangled\ncross-modal Transformer for answer classification. Vision-text communication is\ndone by additional cross-modal interaction modules. With more reasonable video\nencoding and QA solution, we show that VGT can achieve much better performances\non VideoQA tasks that challenge dynamic relation reasoning than prior arts in\nthe pretraining-free scenario. Its performances even surpass those models that\nare pretrained with millions of external data. We further show that VGT can\nalso benefit a lot from self-supervised cross-modal pretraining, yet with\norders of magnitude smaller data. These results clearly demonstrate the\neffectiveness and superiority of VGT, and reveal its potential for more\ndata-efficient pretraining. With comprehensive analyses and some heuristic\nobservations, we hope that VGT can promote VQA research beyond coarse\nrecognition/description towards fine-grained relation reasoning in realistic\nvideos. Our code is available at https://github.com/sail-sg/VGT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Junbin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image and Model Transformation with Secret Key for Vision Transformer. (arXiv:2207.05366v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05366","description":"<p>In this paper, we propose a combined use of transformed images and vision\ntransformer (ViT) models transformed with a secret key. We show for the first\ntime that models trained with plain images can be directly transformed to\nmodels trained with encrypted images on the basis of the ViT architecture, and\nthe performance of the transformed models is the same as models trained with\nplain images when using test images encrypted with the key. In addition, the\nproposed scheme does not require any specially prepared data for training\nmodels or network modification, so it also allows us to easily update the\nsecret key. In an experiment, the effectiveness of the proposed scheme is\nevaluated in terms of performance degradation and model protection performance\nin an image classification task on the CIFAR-10 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iijima_R/0/1/0/all/0/1\">Ryota Iijima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aprilpyone_M/0/1/0/all/0/1\">MaungMaung Aprilpyone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinoshita_Y/0/1/0/all/0/1\">Yuma Kinoshita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Lightweight Super-Resolution with Dual Regression Learning. (arXiv:2207.07929v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07929","description":"<p>Deep neural networks have exhibited remarkable performance in image\nsuper-resolution (SR) tasks by learning a mapping from low-resolution (LR)\nimages to high-resolution (HR) images. However, the SR problem is typically an\nill-posed problem and existing methods would come with several limitations.\nFirst, the possible mapping space of SR can be extremely large since there may\nexist many different HR images that can be downsampled to the same LR image. As\na result, it is hard to directly learn a promising SR mapping from such a large\nspace. Second, it is often inevitable to develop very large models with\nextremely high computational cost to yield promising SR performance. In\npractice, one can use model compression techniques to obtain compact models by\nreducing model redundancy. Nevertheless, it is hard for existing model\ncompression methods to accurately identify the redundant components due to the\nextremely large SR mapping space. To alleviate the first challenge, we propose\na dual regression learning scheme to reduce the space of possible SR mappings.\nSpecifically, in addition to the mapping from LR to HR images, we learn an\nadditional dual regression mapping to estimate the downsampling kernel and\nreconstruct LR images. In this way, the dual mapping acts as a constraint to\nreduce the space of possible mappings. To address the second challenge, we\npropose a lightweight dual regression compression method to reduce model\nredundancy in both layer-level and channel-level based on channel pruning.\nSpecifically, we first develop a channel number search method that minimizes\nthe dual regression loss to determine the redundancy of each layer. Given the\nsearched channel numbers, we further exploit the dual regression manner to\nevaluate the importance of channels and prune the redundant ones. Extensive\nexperiments show the effectiveness of our method in obtaining accurate and\nefficient SR models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiezhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zeshuai Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding The Semidefinite Relaxations of Truncated Least-Squares in Robust Rotation Search. (arXiv:2207.08350v2 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2207.08350","description":"<p>The rotation search problem aims to find a 3D rotation that best aligns a\ngiven number of point pairs. To induce robustness against outliers for rotation\nsearch, prior work considers truncated least-squares (TLS), which is a\nnon-convex optimization problem, and its semidefinite relaxation (SDR) as a\ntractable alternative. Whether this SDR is theoretically tight in the presence\nof noise, outliers, or both has remained largely unexplored. We derive\nconditions that characterize the tightness of this SDR, showing that the\ntightness depends on the noise level, the truncation parameters of TLS, and the\noutlier distribution (random or clustered). In particular, we give a short\nproof for the tightness in the noiseless and outlier-free case, as opposed to\nthe lengthy analysis of prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Peng_L/0/1/0/all/0/1\">Liangzu Peng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fazlyab_M/0/1/0/all/0/1\">Mahyar Fazlyab</a>, <a href=\"http://arxiv.org/find/math/1/au:+Vidal_R/0/1/0/all/0/1\">Ren&#xe9; Vidal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latency-Aware Collaborative Perception. (arXiv:2207.08560v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08560","description":"<p>Collaborative perception has recently shown great potential to improve\nperception capabilities over single-agent perception. Existing collaborative\nperception methods usually consider an ideal communication environment.\nHowever, in practice, the communication system inevitably suffers from latency\nissues, causing potential performance degradation and high risks in\nsafety-critical applications, such as autonomous driving. To mitigate the\neffect caused by the inevitable latency, from a machine learning perspective,\nwe present the first latency-aware collaborative perception system, which\nactively adapts asynchronous perceptual features from multiple agents to the\nsame time stamp, promoting the robustness and effectiveness of collaboration.\nTo achieve such a feature-level synchronization, we propose a novel latency\ncompensation module, called SyncNet, which leverages feature-attention\nsymbiotic estimation and time modulation techniques. Experiments results show\nthat the proposed latency aware collaborative perception system with SyncNet\ncan outperforms the state-of-the-art collaborative perception method by 15.6%\nin the communication latency scenario and keep collaborative perception being\nsuperior to single agent perception under severe latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zixing Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shunli Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-Aware Reference Synthesis for Multi-View Image Super-Resolution. (arXiv:2207.08601v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08601","description":"<p>Recent multi-view multimedia applications struggle between high-resolution\n(HR) visual experience and storage or bandwidth constraints. Therefore, this\npaper proposes a Multi-View Image Super-Resolution (MVISR) task. It aims to\nincrease the resolution of multi-view images captured from the same scene. One\nsolution is to apply image or video super-resolution (SR) methods to\nreconstruct HR results from the low-resolution (LR) input view. However, these\nmethods cannot handle large-angle transformations between views and leverage\ninformation in all multi-view images. To address these problems, we propose the\nMVSRnet, which uses geometry information to extract sharp details from all LR\nmulti-view to support the SR of the LR input view. Specifically, the proposed\nGeometry-Aware Reference Synthesis module in MVSRnet uses geometry information\nand all multi-view LR images to synthesize pixel-aligned HR reference images.\nThen, the proposed Dynamic High-Frequency Search network fully exploits the\nhigh-frequency textural details in reference images for SR. Extensive\nexperiments on several benchmarks show that our method significantly improves\nover the state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ri Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weimin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chenxi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Partition Implicit with Surface Codes for 3D Representation. (arXiv:2207.08631v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08631","description":"<p>Deep implicit functions have shown remarkable shape modeling ability in\nvarious 3D computer vision tasks. One drawback is that it is hard for them to\nrepresent a 3D shape as multiple parts. Current solutions learn various\nprimitives and blend the primitives directly in the spatial space, which still\nstruggle to approximate the 3D shape accurately. To resolve this problem, we\nintroduce a novel implicit representation to represent a single 3D shape as a\nset of parts in the latent space, towards both highly accurate and plausibly\ninterpretable shape modeling. Our insight here is that both the part learning\nand the part blending can be conducted much easier in the latent space than in\nthe spatial space. We name our method Latent Partition Implicit (LPI), because\nof its ability of casting the global shape modeling into multiple local part\nmodeling, which partitions the global shape unity. LPI represents a shape as\nSigned Distance Functions (SDFs) using surface codes. Each surface code is a\nlatent code representing a part whose center is on the surface, which enables\nus to flexibly employ intrinsic attributes of shapes or additional surface\nproperties. Eventually, LPI can reconstruct both the shape and the parts on the\nshape, both of which are plausible meshes. LPI is a multi-level representation,\nwhich can partition a shape into different numbers of parts after training. LPI\ncan be learned without ground truth signed distances, point normals or any\nsupervision for part partition. LPI outperforms the latest methods under the\nwidely used benchmarks in terms of reconstruction accuracy and modeling\ninterpretability. Our code, data and models are available at\nhttps://github.com/chenchao15/LPI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Action Affinity and Continuity for Semi-supervised Temporal Action Segmentation. (arXiv:2207.08653v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08653","description":"<p>We present a semi-supervised learning approach to the temporal action\nsegmentation task. The goal of the task is to temporally detect and segment\nactions in long, untrimmed procedural videos, where only a small set of videos\nare densely labelled, and a large collection of videos are unlabelled. To this\nend, we propose two novel loss functions for the unlabelled data: an action\naffinity loss and an action continuity loss. The action affinity loss guides\nthe unlabelled samples learning by imposing the action priors induced from the\nlabelled set. Action continuity loss enforces the temporal continuity of\nactions, which also provides frame-wise classification supervision. In\naddition, we propose an Adaptive Boundary Smoothing (ABS) approach to build\ncoarser action boundaries for more robust and reliable learning. The proposed\nloss functions and ABS were evaluated on three benchmarks. Results show that\nthey significantly improved action segmentation performance with a low amount\n(5% and 10%) of labelled data and achieved comparable results to full\nsupervision with 50% labelled data. Furthermore, ABS succeeded in boosting\nperformance when integrated into fully-supervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guodong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video. (arXiv:2207.08920v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08920","description":"<p>Introduction: Hand function is a central determinant of independence after\nstroke. Measuring hand use in the home environment is necessary to evaluate the\nimpact of new interventions, and calls for novel wearable technologies.\nEgocentric video can capture hand-object interactions in context, as well as\nshow how more-affected hands are used during bilateral tasks (for stabilization\nor manipulation). Automated methods are required to extract this information.\nObjective: To use artificial intelligence-based computer vision to classify\nhand use and hand role from egocentric videos recorded at home after stroke.\nMethods: Twenty-one stroke survivors participated in the study. A random forest\nclassifier, a SlowFast neural network, and the Hand Object Detector neural\nnetwork were applied to identify hand use and hand role at home.\nLeave-One-Subject-Out-Cross-Validation (LOSOCV) was used to evaluate the\nperformance of the three models. Between-group differences of the models were\ncalculated based on the Mathews correlation coefficient (MCC). Results: For\nhand use detection, the Hand Object Detector had significantly higher\nperformance than the other models. The macro average MCCs using this model in\nthe LOSOCV were 0.50 +- 0.23 for the more-affected hands and 0.58 +- 0.18 for\nthe less-affected hands. Hand role classification had macro average MCCs in the\nLOSOCV that were close to zero for all models. Conclusion: Using egocentric\nvideo to capture the hand use of stroke survivors at home is feasible. Pose\nestimation to track finger movements may be beneficial to classifying hand\nroles in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_M/0/1/0/all/0/1\">Meng-Fen Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rosalie H. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zariffa_J/0/1/0/all/0/1\">Jo&#x15b;e Zariffa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Interactive Object Segmentation Through a Singulation-and-Grasping Approach. (arXiv:2207.09314v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2207.09314","description":"<p>Instance segmentation with unseen objects is a challenging problem in\nunstructured environments. To solve this problem, we propose a robot learning\napproach to actively interact with novel objects and collect each object's\ntraining label for further fine-tuning to improve the segmentation model\nperformance, while avoiding the time-consuming process of manually labeling a\ndataset. The Singulation-and-Grasping (SaG) policy is trained through\nend-to-end reinforcement learning. Given a cluttered pile of objects, our\napproach chooses pushing and grasping motions to break the clutter and conducts\nobject-agnostic grasping for which the SaG policy takes as input the visual\nobservations and imperfect segmentation. We decompose the problem into three\nsubtasks: (1) the object singulation subtask aims to separate the objects from\neach other, which creates more space that alleviates the difficulty of (2) the\ncollision-free grasping subtask; (3) the mask generation subtask to obtain the\nself-labeled ground truth masks by using an optical flow-based binary\nclassifier and motion cue post-processing for transfer learning. Our system\nachieves 70% singulation success rate in simulated cluttered scenes. The\ninteractive segmentation of our system achieves 87.8%, 73.9%, and 69.3% average\nprecision for toy blocks, YCB objects in simulation and real-world novel\nobjects, respectively, which outperforms several baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Houjian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Changhyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoserNet: Refining Relative Camera Poses Exploiting Object Detections. (arXiv:2207.09445v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09445","description":"<p>The estimation of the camera poses associated with a set of images commonly\nrelies on feature matches between the images. In contrast, we are the first to\naddress this challenge by using objectness regions to guide the pose estimation\nproblem rather than explicit semantic object detections. We propose Pose\nRefiner Network (PoserNet) a light-weight Graph Neural Network to refine the\napproximate pair-wise relative camera poses. PoserNet exploits associations\nbetween the objectness regions - concisely expressed as bounding boxes - across\nmultiple views to globally refine sparsely connected view graphs. We evaluate\non the 7-Scenes dataset across varied sizes of graphs and show how this process\ncan be beneficial to optimisation-based Motion Averaging algorithms improving\nthe median error on the rotation by 62 degrees with respect to the initial\nestimates obtained based on bounding boxes. Code and data are available at\nhttps://github.com/IIT-PAVIS/PoserNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taiana_M/0/1/0/all/0/1\">Matteo Taiana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toso_M/0/1/0/all/0/1\">Matteo Toso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stuart James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1\">Alessio Del Bue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSE-NN Team at the 4th ABAW Competition: Multi-task Emotion Recognition and Learning from Synthetic Images. (arXiv:2207.09508v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09508","description":"<p>In this paper, we present the results of the HSE-NN team in the 4th\ncompetition on Affective Behavior Analysis in-the-wild (ABAW). The novel\nmulti-task EfficientNet model is trained for simultaneous recognition of facial\nexpressions and prediction of valence and arousal on static photos. The\nresulting MT-EmotiEffNet extracts visual features that are fed into simple\nfeed-forward neural networks in the multi-task learning challenge. We obtain\nperformance measure 1.3 on the validation set, which is significantly greater\nwhen compared to either performance of baseline (0.3) or existing models that\nare trained only on the s-Aff-Wild2 database. In the learning from synthetic\ndata challenge, the quality of the original synthetic training set is increased\nby using the super-resolution techniques, such as Real-ESRGAN. Next, the\nMT-EmotiEffNet is fine-tuned on the new training set. The final prediction is a\nsimple blending ensemble of pre-trained and fine-tuned MT-EmotiEffNets. Our\naverage validation F1 score is 18% greater than the baseline convolutional\nneural network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savchenko_A/0/1/0/all/0/1\">Andrey V. Savchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of 3D Dental Images Using Deep Learning. (arXiv:2207.09582v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.09582","description":"<p>3D image segmentation is a recent and crucial step in many medical analysis\nand recognition schemes. In fact, it represents a relevant research subject and\na fundamental challenge due to its importance and influence. This paper\nprovides a multi-phase Deep Learning-based system that hybridizes various\nefficient methods in order to get the best 3D segmentation output. First, to\nreduce the amount of data and accelerate the processing time, the application\nof Decimate compression technique is suggested and justified. We then use a CNN\nmodel to segment dental images into fifteen separated classes. In the end, a\nspecial KNN-based transformation is applied for the purpose of removing\nisolated meshes and of correcting dental forms. Experimentations demonstrate\nthe precision and the robustness of the selected framework applied to 3D dental\nimages within a private clinical benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Boudraa_O/0/1/0/all/0/1\">Omar Boudraa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perspective Phase Angle Model for Polarimetric 3D Reconstruction. (arXiv:2207.09629v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09629","description":"<p>Current polarimetric 3D reconstruction methods, including those in the\nwell-established shape from polarization literature, are all developed under\nthe orthographic projection assumption. In the case of a large field of view,\nhowever, this assumption does not hold and may result in significant\nreconstruction errors in methods that make this assumption. To address this\nproblem, we present the perspective phase angle (PPA) model that is applicable\nto perspective cameras. Compared with the orthographic model, the proposed PPA\nmodel accurately describes the relationship between polarization phase angle\nand surface normal under perspective projection. In addition, the PPA model\nmakes it possible to estimate surface normals from only one single-view phase\nangle map and does not suffer from the so-called $\\pi$-ambiguity problem.\nExperiments on real data show that the PPA model is more accurate for surface\nnormal estimation with a perspective camera than the orthographic model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangcheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Li He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yisheng Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HTNet: Anchor-free Temporal Action Localization with Hierarchical Transformers. (arXiv:2207.09662v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09662","description":"<p>Temporal action localization (TAL) is a task of identifying a set of actions\nin a video, which involves localizing the start and end frames and classifying\neach action instance. Existing methods have addressed this task by using\npredefined anchor windows or heuristic bottom-up boundary-matching strategies,\nwhich are major bottlenecks in inference time. Additionally, the main challenge\nis the inability to capture long-range actions due to a lack of global\ncontextual information. In this paper, we present a novel anchor-free\nframework, referred to as HTNet, which predicts a set of &lt;start time, end time,\nclass&gt; triplets from a video based on a Transformer architecture. After the\nprediction of coarse boundaries, we refine it through a background feature\nsampling (BFS) module and hierarchical Transformers, which enables our model to\naggregate global contextual information and effectively exploit the inherent\nsemantic relationships in a video. We demonstrate how our method localizes\naccurate action instances and achieves state-of-the-art performance on two TAL\nbenchmark datasets: THUMOS14 and ActivityNet 1.3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1\">Tae-Kyung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gun-Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERA: Expert Retrieval and Assembly for Early Action Prediction. (arXiv:2207.09675v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09675","description":"<p>Early action prediction aims to successfully predict the class label of an\naction before it is completely performed. This is a challenging task because\nthe beginning stages of different actions can be very similar, with only minor\nsubtle differences for discrimination. In this paper, we propose a novel Expert\nRetrieval and Assembly (ERA) module that retrieves and assembles a set of\nexperts most specialized at using discriminative subtle differences, to\ndistinguish an input sample from other highly similar samples. To encourage our\nmodel to effectively use subtle differences for early action prediction, we\npush experts to discriminate exclusively between samples that are highly\nsimilar, forcing these experts to learn to use subtle differences that exist\nbetween those samples. Additionally, we design an effective Expert Learning\nRate Optimization method that balances the experts' optimization and leads to\nbetter performance. We evaluate our ERA module on four public action datasets\nand achieve state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foo_L/0/1/0/all/0/1\">Lin Geng Foo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianjiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qiuhong Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Anatomy of Video Editing: A Dataset and Benchmark Suite for AI-Assisted Video Editing. (arXiv:2207.09812v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09812","description":"<p>Machine learning is transforming the video editing industry. Recent advances\nin computer vision have leveled-up video editing tasks such as intelligent\nreframing, rotoscoping, color grading, or applying digital makeups. However,\nmost of the solutions have focused on video manipulation and VFX. This work\nintroduces the Anatomy of Video Editing, a dataset, and benchmark, to foster\nresearch in AI-assisted video editing. Our benchmark suite focuses on video\nediting tasks, beyond visual effects, such as automatic footage organization\nand assisted video assembling. To enable research on these fronts, we annotate\nmore than 1.5M tags, with relevant concepts to cinematography, from 196176\nshots sampled from movie scenes. We establish competitive baseline methods and\ndetailed analyses for each of the tasks. We hope our work sparks innovative\nresearch towards underexplored areas of AI-assisted video editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Argaw_D/0/1/0/all/0/1\">Dawit Mureja Argaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joon-Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodson_M/0/1/0/all/0/1\">Markus Woodson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Landmark-based Stent Tracking in X-ray Fluoroscopy. (arXiv:2207.09933v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09933","description":"<p>In clinical procedures of angioplasty (i.e., open clogged coronary arteries),\ndevices such as balloons and stents need to be placed and expanded in arteries\nunder the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose,\nthe resulting images are often noisy. To check the correct placement of these\ndevices, typically multiple motion-compensated frames are averaged to enhance\nthe view. Therefore, device tracking is a necessary procedure for this purpose.\nEven though angioplasty devices are designed to have radiopaque markers for the\nease of tracking, current methods struggle to deliver satisfactory results due\nto the small marker size and complex scenes in angioplasty. In this paper, we\npropose an end-to-end deep learning framework for single stent tracking, which\nconsists of three hierarchical modules: U-Net based landmark detection, ResNet\nbased stent proposal and feature extraction, and graph convolutional neural\nnetwork (GCN) based stent tracking that temporally aggregates both spatial\ninformation and appearance features. The experiments show that our method\nperforms significantly better in detection compared with the state-of-the-art\npoint-based tracking models. In addition, its fast inference speed satisfies\nclinical requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Luojie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Eric Z. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shanhui Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Secrets of Event-Based Optical Flow. (arXiv:2207.10022v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10022","description":"<p>Event cameras respond to scene dynamics and offer advantages to estimate\nmotion. Following recent image-based deep-learning achievements, optical flow\nestimation methods for event cameras have rushed to combine those image-based\nmethods with event data. However, it requires several adaptations (data\nconversion, loss function, etc.) as they have very different properties. We\ndevelop a principled method to extend the Contrast Maximization framework to\nestimate optical flow from events alone. We investigate key elements: how to\ndesign the objective function to prevent overfitting, how to warp events to\ndeal better with occlusions, and how to improve convergence with multi-scale\nraw events. With these key elements, our method ranks first among unsupervised\nmethods on the MVSEC benchmark, and is competitive on the DSEC benchmark.\nMoreover, our method allows us to expose the issues of the ground truth flow in\nthose benchmarks, and produces remarkable results when it is transferred to\nunsupervised learning settings. Our code is available at\nhttps://github.com/tub-rip/event_based_optical_flow\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shiba_S/0/1/0/all/0/1\">Shintaro Shiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoki_Y/0/1/0/all/0/1\">Yoshimitsu Aoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Guillermo Gallego</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks. (arXiv:2207.10025v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10025","description":"<p>Facial expression in-the-wild is essential for various interactive computing\ndomains. Especially, \"Learning from Synthetic Data\" (LSD) is an important topic\nin the facial expression recognition task. In this paper, we propose a\nmulti-task learning-based facial expression recognition approach which consists\nof emotion and appearance learning branches that can share all face\ninformation, and present preliminary results for the LSD challenge introduced\nin the 4th affective behavior analysis in-the-wild (ABAW) competition. Our\nmethod achieved the mean F1 score of 0.71.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jae-Yeop Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yeong-Gi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">JiYeon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sumin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jin-Woo Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">Yuchul Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Densely Constrained Depth Estimator for Monocular 3D Object Detection. (arXiv:2207.10047v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10047","description":"<p>Estimating accurate 3D locations of objects from monocular images is a\nchallenging problem because of lacking depth. Previous work shows that\nutilizing the object's keypoint projection constraints to estimate multiple\ndepth candidates boosts the detection performance. However, the existing\nmethods can only utilize vertical edges as projection constraints for depth\nestimation. So these methods only use a small number of projection constraints\nand produce insufficient depth candidates, leading to inaccurate depth\nestimation. In this paper, we propose a method that utilizes dense projection\nconstraints from edges of any direction. In this way, we employ much more\nprojection constraints and produce considerable depth candidates. Besides, we\npresent a graph matching weighting module to merge the depth candidates. The\nproposed method DCD (Densely Constrained Detector) achieves state-of-the-art\nperformance on the KITTI and WOD benchmarks. Code is released at\nhttps://github.com/BraveGroup/DCD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuntao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiawei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}