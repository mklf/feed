{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-09T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Text-Free Prosody-Aware Generative Spoken Language Modeling. (arXiv:2109.03264v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03264","description":"<p>Speech pre-training has primarily demonstrated efficacy on classification\ntasks, while its capability of generating novel speech, similar to how GPT-2\ncan generate coherent paragraphs, has barely been explored. Generative Spoken\nLanguage Modeling (GSLM) (Lakhotia et al., 2021) is the only prior work\naddressing the generative aspects of speech pre-training, which replaces text\nwith discovered phone-like units for language modeling and shows the ability to\ngenerate meaningful novel sentences. Unfortunately, despite eliminating the\nneed of text, the units used in GSLM discard most of the prosodic information.\nHence, GSLM fails to leverage prosody for better comprehension, and does not\ngenerate expressive speech. In this work, we present a prosody-aware generative\nspoken language model (pGSLM). It is composed of a multi-stream transformer\nlanguage model (MS-TLM) of speech, represented as discovered unit and prosodic\nfeature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to\nwaveforms. We devise a series of metrics for prosody modeling and generation,\nand re-use metrics from GSLM for content modeling. Experimental results show\nthat the pGSLM can utilize prosody to improve both prosody and content\nmodeling, and also generate natural, meaningful, and coherent speech given a\nspoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu-Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Rivi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual-Decoder Conformer for Multilingual Speech Recognition. (arXiv:2109.03277v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03277","description":"<p>Transformer-based models have recently become very popular for\nsequence-to-sequence applications such as machine translation and speech\nrecognition. This work proposes a dual-decoder transformer model for\nlow-resource multilingual speech recognition for Indian languages. Our proposed\nmodel consists of a Conformer [1] encoder, two parallel transformer decoders,\nand a language classifier. We use a phoneme decoder (PHN-DEC) for the phoneme\nrecognition task and a grapheme decoder (GRP-DEC) to predict grapheme sequence\nalong with language information. We consider phoneme recognition and language\nidentification as auxiliary tasks in the multi-task learning framework. We\njointly optimize the network for phoneme recognition, grapheme recognition, and\nlanguage identification tasks with Joint CTC-Attention [2] training. Our\nexperiments show that we can obtain a significant reduction in WER over the\nbaseline approaches. We also show that our dual-decoder approach obtains\nsignificant improvement over the single decoder approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+N_K/0/1/0/all/0/1\">Krishna D N</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models. (arXiv:2109.03300v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03300","description":"<p>All AI models are susceptible to learning biases in data that they are\ntrained on. For generative dialogue models, being trained on real human\nconversations containing unbalanced gender and race/ethnicity references can\nlead to models that display learned biases, which we define here broadly as any\nmeasurable differences in the distributions of words or semantic content of\nconversations based on demographic groups. We measure the strength of such\nbiases by producing artificial conversations between two copies of a dialogue\nmodel, conditioning one conversational partner to state a name commonly\nassociated with a certain gender and/or race/ethnicity. We find that larger\ncapacity models tend to exhibit more gender bias and greater stereotyping of\noccupations by gender. We show that several methods of tuning these dialogue\nmodels, specifically name scrambling, controlled generation, and unlikelihood\ntraining, are effective in reducing bias in conversation, including on a\ndownstream conversational task. Name scrambling is also effective in lowering\ndifferences in token usage across conversations where partners have names\nassociated with different genders or races/ethnicities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Eric Michael Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Corpus-based Open-Domain Event Type Induction. (arXiv:2109.03322v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03322","description":"<p>Traditional event extraction methods require predefined event types and their\ncorresponding annotations to learn event extractors. These prerequisites are\noften hard to be satisfied in real-world applications. This work presents a\ncorpus-based open-domain event type induction method that automatically\ndiscovers a set of event types from a given corpus. As events of the same type\ncould be expressed in multiple ways, we propose to represent each event type as\na cluster of &lt;predicate sense, object head&gt; pairs. Specifically, our method (1)\nselects salient predicates and object heads, (2) disambiguates predicate senses\nusing only a verb sense dictionary, and (3) obtains event types by jointly\nembedding and clustering &lt;predicate sense, object head&gt; pairs in a latent\nspherical space. Our experiments, on three datasets from different domains,\nshow our method can discover salient and high-quality event types, according to\nboth automatic and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings. (arXiv:2109.03334v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03334","description":"<p>Building compositional explanations requires models to combine two or more\nfacts that, together, describe why the answer to a question is correct.\nTypically, these \"multi-hop\" explanations are evaluated relative to one (or a\nsmall number of) gold explanations. In this work, we show these evaluations\nsubstantially underestimate model performance, both in terms of the relevance\nof included facts, as well as the completeness of model-generated explanations,\nbecause models regularly discover and produce valid explanations that are\ndifferent than gold explanations. To address this, we construct a large corpus\nof 126k domain-expert (science teacher) relevance ratings that augment a corpus\nof explanations to standardized science exam questions, discovering 80k\nadditional relevant facts not rated as gold. We build three strong models based\non different methodologies (generation, ranking, and schemas), and empirically\nshow that while expert-augmented ratings provide better estimates of\nexplanation quality, both original (gold) and expert-augmented automatic\nevaluations still substantially underestimate performance by up to 36% when\ncompared with full manual expert judgements, with different models being\ndisproportionately affected. This poses a significant methodological challenge\nto accurately evaluating explanations produced by compositional reasoning\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kelly Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_D/0/1/0/all/0/1\">Dan Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_H/0/1/0/all/0/1\">Huitzilin Ortiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering. (arXiv:2109.03381v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03381","description":"<p>Spoken question answering (SQA) requires fine-grained understanding of both\nspoken documents and questions for the optimal answer prediction. In this\npaper, we propose novel training schemes for spoken question answering with a\nself-supervised training stage and a contrastive representation learning stage.\nIn the self-supervised stage, we propose three auxiliary self-supervised tasks,\nincluding utterance restoration, utterance insertion, and question\ndiscrimination, and jointly train the model to capture consistency and\ncoherence among speech documents without any additional data or annotations. We\nthen propose to learn noise-invariant utterance representations in a\ncontrastive objective by adopting multiple augmentation strategies, including\nspan deletion and span substitution. Besides, we design a Temporal-Alignment\nattention to semantically align the speech-text clues in the learned common\nspace and benefit the SQA tasks. By this means, the training schemes can more\neffectively guide the generation model to predict more proper answers.\nExperimental results show that our model achieves state-of-the-art results on\nthree SQA benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepZensols: Deep Natural Language Processing Framework. (arXiv:2109.03383v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03383","description":"<p>Reproducing results in publications by distributing publicly available source\ncode is becoming ever more popular. Given the difficulty of reproducing machine\nlearning (ML) experiments, there have been significant efforts in reducing the\nvariance of these results. As in any science, the ability to consistently\nreproduce results effectively strengthens the underlying hypothesis of the\nwork, and thus, should be regarded as important as the novel aspect of the\nresearch itself. The contribution of this work is a framework that is able to\nreproduce consistent results and provides a means of easily creating, training,\nand evaluating natural language processing (NLP) deep learning (DL) models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landes_P/0/1/0/all/0/1\">Paul Landes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eugenio_B/0/1/0/all/0/1\">Barbara Di Eugenio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixup Decoding for Diverse Machine Translation. (arXiv:2109.03402v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03402","description":"<p>Diverse machine translation aims at generating various target language\ntranslations for a given source language sentence. Leveraging the linear\nrelationship in the sentence latent space introduced by the mixup training, we\npropose a novel method, MixDiversity, to generate different translations for\nthe input sentence by linearly interpolating it with different sentence pairs\nsampled from the training corpus when decoding. To further improve the\nfaithfulness and diversity of the translations, we propose two simple but\neffective approaches to select diverse sentence pairs in the training corpus\nand adjust the interpolation weight for each pair correspondingly. Moreover, by\ncontrolling the interpolation weight, our method can achieve the trade-off\nbetween faithfulness and diversity without any additional training, which is\nrequired in most of the previous methods. Experiments on WMT'16 en-ro, WMT'14\nen-de, and WMT'17 zh-en are conducted to show that our method substantially\noutperforms all previous diverse machine translation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengzhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuanfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhongjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models. (arXiv:2109.03415v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03415","description":"<p>Multimodal machine translation (MMT) systems have been shown to outperform\ntheir text-only neural machine translation (NMT) counterparts when visual\ncontext is available. However, recent studies have also shown that the\nperformance of MMT models is only marginally impacted when the associated image\nis replaced with an unrelated image or noise, which suggests that the visual\ncontext might not be exploited by the model at all. We hypothesize that this\nmight be caused by the nature of the commonly used evaluation benchmark, also\nknown as Multi30K, where the translations of image captions were prepared\nwithout actually showing the images to human translators. In this paper, we\npresent a qualitative study that examines the role of datasets in stimulating\nthe leverage of visual modality and we propose methods to highlight the\nimportance of visual signals in the datasets which demonstrate improvements in\nreliance of models on the source images. Our findings suggest the research on\neffective MMT architectures is currently impaired by the lack of suitable\ndatasets and careful consideration must be taken in creation of future MMT\ndatasets, for which we also provide useful insights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaoda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataman_D/0/1/0/all/0/1\">Duygu Ataman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It is AI's Turn to Ask Human a Question: Question and Answer Pair Generation for Children Storybooks in FairytaleQA Dataset. (arXiv:2109.03423v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03423","description":"<p>Existing question answering (QA) datasets are created mainly for the\napplication of having AI to be able to answer questions asked by humans. But in\neducational applications, teachers and parents sometimes may not know what\nquestions they should ask a child that can maximize their language learning\nresults. With a newly released book QA dataset (FairytaleQA), which educational\nexperts labeled on 46 fairytale storybooks for early childhood readers, we\ndeveloped an automated QA generation model architecture for this novel\napplication. Our model (1) extracts candidate answers from a given storybook\npassage through carefully designed heuristics based on a pedagogical framework;\n(2) generates appropriate questions corresponding to each extracted answer\nusing a language model; and, (3) uses another QA model to rank top QA-pairs.\nAutomatic and human evaluations show that our model outperforms baselines. We\nalso demonstrate that our method can help with the scarcity issue of the\nchildren's book QA dataset via data augmentation on 200 unlabeled storybooks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Tran Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Branda Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Toby Jia-Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArchivalQA: A Large-scale Benchmark Dataset for Open Domain Question Answering over Archival News Collections. (arXiv:2109.03438v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03438","description":"<p>In the last few years, open-domain question answering (ODQA) has advanced\nrapidly due to the development of deep learning techniques and the availability\nof large-scale QA datasets. However, the current datasets are essentially\ndesigned for synchronic document collections (e.g., Wikipedia). Temporal news\ncollections such as long-term news archives spanning several decades, are\nrarely used in training the models despite they are quite valuable for our\nsociety. In order to foster the research in the field of ODQA on such\nhistorical collections, we present ArchivalQA, a large question answering\ndataset consisting of 1,067,056 question-answer pairs which is designed for\ntemporal news QA. In addition, we create four subparts of our dataset based on\nthe question difficulty levels and the containment of temporal expressions,\nwhich we believe could be useful for training or testing ODQA systems\ncharacterized by different strengths and abilities. The novel QA\ndataset-constructing framework that we introduce can be also applied to create\ndatasets over other types of collections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1\">Masatoshi Yoshikawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Referee: Towards reference-free cross-speaker style transfer with low-quality data for expressive speech synthesis. (arXiv:2109.03439v1 [eess.AS])","link":"http://arxiv.org/abs/2109.03439","description":"<p>Cross-speaker style transfer (CSST) in text-to-speech (TTS) synthesis aims at\ntransferring a speaking style to the synthesised speech in a target speaker's\nvoice. Most previous CSST approaches rely on expensive high-quality data\ncarrying desired speaking style during training and require a reference\nutterance to obtain speaking style descriptors as conditioning on the\ngeneration of a new sentence. This work presents Referee, a robust\nreference-free CSST approach for expressive TTS, which fully leverages\nlow-quality data to learn speaking styles from text. Referee is built by\ncascading a text-to-style (T2S) model with a style-to-wave (S2W) model.\nPhonetic PosteriorGram (PPG), phoneme-level pitch and energy contours are\nadopted as fine-grained speaking style descriptors, which are predicted from\ntext using the T2S model. A novel pretrain-refinement method is adopted to\nlearn a robust T2S model by only using readily accessible low-quality data. The\nS2W model is trained with high-quality target data, which is adopted to\neffectively aggregate style descriptors and generate high-fidelity speech in\nthe target speaker's voice. Experimental results are presented, showing that\nReferee outperforms a global-style-token (GST)-based baseline approach in CSST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Songxiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Level Contrastive Learning for Text Summarization. (arXiv:2109.03481v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03481","description":"<p>Contrastive learning models have achieved great success in unsupervised\nvisual representation learning, which maximize the similarities between feature\nrepresentations of different views of the same image, while minimize the\nsimilarities between feature representations of views of different images. In\ntext summarization, the output summary is a shorter form of the input document\nand they have similar meanings. In this paper, we propose a contrastive\nlearning model for supervised abstractive text summarization, where we view a\ndocument, its gold summary and its model generated summaries as different views\nof the same mean representation and maximize the similarities between them\nduring training. We improve over a strong sequence-to-sequence text generation\nmodel (i.e., BART) on three different summarization datasets. Human evaluation\nalso shows that our model achieves better faithfulness ratings compared to its\ncounterpart without contrastive objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shusheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Analysis of Young Basque Speaking Communities in Twitter. (arXiv:2109.03487v1 [cs.CY])","link":"http://arxiv.org/abs/2109.03487","description":"<p>In this paper we take into account both social and linguistic aspects to\nperform demographic analysis by processing a large amount of tweets in Basque\nlanguage. The study of demographic characteristics and social relationships are\napproached by applying machine learning and modern deep-learning Natural\nLanguage Processing (NLP) techniques, combining social sciences with automatic\ntext processing. More specifically, our main objective is to combine\ndemographic inference and social analysis in order to detect young Basque\nTwitter users and to identify the communities that arise from their\nrelationships or shared content. This social and demographic analysis will be\nentirely based on the~automatically collected tweets using NLP to convert\nunstructured textual information into interpretable knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landa_J/0/1/0/all/0/1\">J. Fernandez de Landa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">R. Agerri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spelling provides a precise (but sometimes misplaced) phonological target. Orthography and acoustic variability in second language word learning. (arXiv:2109.03490v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03490","description":"<p>L1 French participants learned novel L2 English words over two days of\nlearning sessions, with half of the words presented with their orthographic\nforms (Audio-Ortho) and half without (Audio only). One group heard the words\npronounced by a single talker, while another group heard them pronounced by\nmultiple talkers. On the third day, they completed a variety of tasks to\nevaluate their learning. Our results show a robust influence of orthography,\nwith faster response times in both production (picture naming) and recognition\n(picture mapping) tasks for words learned in the Audio-Ortho condition.\nMoreover, formant analyses of the picture naming responses show that\northographic input pulls pronunciations of English novel words towards a\nnon-native (French) phonological target. Words learned with their orthographic\nforms were pronounced more precisely (with smaller Dispersion Scores), but were\nmisplaced in the vowel space (as reflected by smaller Euclidian distances with\nrespect to French vowels). Finally, we found only limited evidence of an effect\nof talker-based acoustic variability: novel words learned with multiple talkers\nshowed faster responses times in the picture naming task, but only in the\nAudio-only condition, which suggests that orthographic information may have\noverwhelmed any advantage of talker-based acoustic variability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Welby_P/0/1/0/all/0/1\">Pauline Welby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spinelli_E/0/1/0/all/0/1\">Elsa Spinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burki_A/0/1/0/all/0/1\">Audrey B&#xfc;rki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R2-D2: A Modular Baseline for Open-Domain Question Answering. (arXiv:2109.03502v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03502","description":"<p>This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank\ntwice, reaD twice). The pipeline is composed of a retriever, passage reranker,\nextractive reader, generative reader and a mechanism that aggregates the final\nprediction from all system's components. We demonstrate its strength across\nthree open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA,\nsurpassing state-of-the-art on the first two. Our analysis demonstrates that:\n(i) combining extractive and generative reader yields absolute improvements up\nto 5 exact match and it is at least twice as effective as the posterior\naveraging ensemble of the same models with different parameters, (ii) the\nextractive reader with fewer parameters can match the performance of the\ngenerative reader on extractive QA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Docekal_M/0/1/0/all/0/1\">Martin Docekal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondrej_K/0/1/0/all/0/1\">Karel Ondrej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RefineCap: Concept-Aware Refinement for Image Captioning. (arXiv:2109.03529v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03529","description":"<p>Automatically translating images to texts involves image scene understanding\nand language modeling. In this paper, we propose a novel model, termed\nRefineCap, that refines the output vocabulary of the language decoder using\ndecoder-guided visual semantics, and implicitly learns the mapping between\nvisual tag words and images. The proposed Visual-Concept Refinement method can\nallow the generator to attend to semantic details in the image, thereby\ngenerating more semantically descriptive captions. Our model achieves superior\nperformance on the MS-COCO dataset in comparison with previous visual-concept\nbased models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yekun Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shuo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Junliang Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets. (arXiv:2109.03537v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03537","description":"<p>Pre-training language models (LMs) on large-scale unlabeled text data makes\nthe model much easier to achieve exceptional downstream performance than their\ncounterparts directly trained on the downstream tasks. In this work, we study\nwhat specific traits in the pre-training data, other than the semantics, make a\npre-trained LM superior to their counterparts trained from scratch on\ndownstream tasks. We propose to use artificially constructed datasets as the\npre-training data to exclude the effect of semantics, and further control what\ncharacteristics the pre-training corpora have. By fine-tuning the pre-trained\nmodels on GLUE benchmark, we can learn how beneficial it is to transfer the\nknowledge from the model trained on the dataset possessing that specific trait.\nWe define and discuss three different characteristics in the artificial\ndataset: 1) matching the token's uni-gram or bi-gram distribution between\npre-training and downstream fine-tuning, 2) the presence of the explicit\ndependencies among the tokens in a sequence, 3) the length of the implicit\ndependencies among the tokens in a sequence. Our experiments show that the\nexplicit dependencies in the sequences of the pre-training data are critical to\nthe downstream performance. Our results also reveal that models achieve better\ndownstream performance when pre-trained on a dataset with a longer range of\nimplicit dependencies. Based on our analysis, we find that models pre-trained\nwith artificial datasets are prone to learn spurious correlation in downstream\ntasks. Our work reveals that even if the LMs are not pre-trained on natural\nlanguage, they still gain transferability on certain human language downstream\ntasks once the LMs learn to model the token dependencies in the sequences. This\nresult helps us understand the exceptional transferability of pre-trained LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1\">Cheng-Han Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion. (arXiv:2109.03551v1 [cs.SD])","link":"http://arxiv.org/abs/2109.03551","description":"<p>Voice conversion (VC) is an effective approach to electrolaryngeal (EL)\nspeech enhancement, a task that aims to improve the quality of the artificial\nvoice from an electrolarynx device. In frame-based VC methods, time alignment\nneeds to be performed prior to model training, and the dynamic time warping\n(DTW) algorithm is widely adopted to compute the best time alignment between\neach utterance pair. The validity is based on the assumption that the same\nphonemes of the speakers have similar features and can be mapped by measuring a\npre-defined distance between speech frames of the source and the target.\nHowever, the special characteristics of the EL speech can break the assumption,\nresulting in a sub-optimal DTW alignment. In this work, we propose to use lip\nimages for time alignment, as we assume that the lip movements of laryngectomee\nremain normal compared to healthy people. We investigate two naive lip\nrepresentations and distance metrics, and experimental results demonstrate that\nthe proposed method can significantly outperform the audio-only alignment in\nterms of objective and subjective evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liou_Y/0/1/0/all/0/1\">Yi-Syuan Liou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_M/0/1/0/all/0/1\">Ming-Chi Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1\">Shu-Wei Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yu-Huai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Offensive Language Identification for Low Resource Languages: The Case of Marathi. (arXiv:2109.03552v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03552","description":"<p>The widespread presence of offensive language on social media motivated the\ndevelopment of systems capable of recognizing such content automatically. Apart\nfrom a few notable exceptions, most research on automatic offensive language\nidentification has dealt with English. To address this shortcoming, we\nintroduce MOLD, the Marathi Offensive Language Dataset. MOLD is the first\ndataset of its kind compiled for Marathi, thus opening a new domain for\nresearch in low-resource Indo-Aryan languages. We present results from several\nmachine learning experiments on this dataset, including zero-short and other\ntransfer learning experiments on state-of-the-art cross-lingual transformers\nfrom existing data in Bengali, English, and Hindi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaikwad_S/0/1/0/all/0/1\">Saurabh Gaikwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homan_C/0/1/0/all/0/1\">Christopher M. Homan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction. (arXiv:2109.03564v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03564","description":"<p>Using prompts to utilize language models to perform various downstream tasks,\nalso known as prompt-based learning or prompt-learning, has lately gained\nsignificant success in comparison to the pre-train and fine-tune paradigm.\nNonetheless, virtually all prompt-based methods are token-level, meaning they\nall utilize GPT's left-to-right language model or BERT's masked language model\nto perform cloze-style tasks. In this paper, we attempt to accomplish several\nNLP tasks in the zero-shot scenario using a BERT original pre-training task\nabandoned by RoBERTa and other models--Next Sentence Prediction (NSP). Unlike\ntoken-level techniques, our sentence-level prompt-based method NSP-BERT does\nnot need to fix the length of the prompt or the position to be predicted,\nallowing it to handle tasks such as entity linking with ease. Based on the\ncharacteristics of NSP-BERT, we offer several quick building templates for\nvarious downstream tasks. We suggest a two-stage prompt method for word sense\ndisambiguation tasks in particular. Our strategies for mapping the labels\nsignificantly enhance the model's performance on sentence pair tasks. On the\nFewCLUE benchmark, our NSP-BERT outperforms other zero-shot methods on most of\nthese tasks and comes close to the few-shot methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Chao Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Hangping Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario. (arXiv:2109.03570v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03570","description":"<p>This work presents biomedical and clinical language models for Spanish by\nexperimenting with different pretraining choices, such as masking at word and\nsubword level, varying the vocabulary size and testing with domain data,\nlooking for better language representations. Interestingly, in the absence of\nenough clinical data to train a model from scratch, we applied mixed-domain\npretraining and cross-domain transfer approaches to generate a performant\nbio-clinical model suitable for real-world clinical data. We evaluated our\nmodels on Named Entity Recognition (NER) tasks for biomedical documents and\nchallenging hospital discharge reports. When compared against the competitive\nmBERT and BETO models, we outperform them in all NER tasks by a significant\nmargin. Finally, we studied the impact of the model's vocabulary on the NER\nperformances by offering an interesting vocabulary-centric analysis. The\nresults confirm that domain-specific pretraining is fundamental to achieving\nhigher performances in downstream NER tasks, even within a mid-resource\nscenario. To the best of our knowledge, we provide the first biomedical and\nclinical transformer-based pretrained language models for Spanish, intending to\nboost native Spanish NLP applications in biomedicine. Our models will be made\nfreely available after publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrollsWithOpinion: A Dataset for Predicting Domain-specific Opinion Manipulation in Troll Memes. (arXiv:2109.03571v1 [cs.SI])","link":"http://arxiv.org/abs/2109.03571","description":"<p>Research into the classification of Image with Text (IWT) troll memes has\nrecently become popular. Since the online community utilizes the refuge of\nmemes to express themselves, there is an abundance of data in the form of\nmemes. These memes have the potential to demean, harras, or bully targeted\nindividuals. Moreover, the targeted individual could fall prey to opinion\nmanipulation. To comprehend the use of memes in opinion manipulation, we define\nthree specific domains (product, political or others) which we classify into\ntroll or not-troll, with or without opinion manipulation. To enable this\nanalysis, we enhanced an existing dataset by annotating the data with our\ndefined classes, resulting in a dataset of 8,881 IWT or multimodal memes in the\nEnglish language (TrollsWithOpinion dataset). We perform baseline experiments\non the annotated dataset, and our result shows that existing state-of-the-art\ntechniques could only reach a weighted-average F1-score of 0.37. This shows the\nneed for a development of a specific technique to deal with multimodal troll\nmemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suryawanshi_S/0/1/0/all/0/1\">Shardul Suryawanshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcan_M/0/1/0/all/0/1\">Mihael Arcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Little_S/0/1/0/all/0/1\">Suzanne Little</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buitelaar_P/0/1/0/all/0/1\">Paul Buitelaar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict. (arXiv:2109.03587v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03587","description":"<p>Sarcasm employs ambivalence, where one says something positive but actually\nmeans negative, and vice versa. Due to the sophisticated and obscure sentiment,\nsarcasm brings in great challenges to sentiment analysis. In this paper, we\nshow up the essence of sarcastic text is that the literal sentiment (expressed\nby the surface form of the text) is opposite to the deep sentiment (expressed\nby the actual meaning of the text). To this end, we propose a Dual-Channel\nFramework by modeling both literal and deep sentiments to recognize the\nsentiment conflict. Specifically, the proposed framework is capable of\ndetecting the sentiment conflict between the literal and deep meanings of the\ninput text. Experiments on the political debates and the Twitter datasets show\nthat our framework achieves the best performance on sarcasm recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yequan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuying Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formal Query Building with Query Structure Prediction for Complex Question Answering over Knowledge Base. (arXiv:2109.03614v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03614","description":"<p>Formal query building is an important part of complex question answering over\nknowledge bases. It aims to build correct executable queries for questions.\nRecent methods try to rank candidate queries generated by a state-transition\nstrategy. However, this candidate generation strategy ignores the structure of\nqueries, resulting in a considerable number of noisy queries. In this paper, we\npropose a new formal query building approach that consists of two stages. In\nthe first stage, we predict the query structure of the question and leverage\nthe structure to constrain the generation of the candidate queries. We propose\na novel graph generation framework to handle the structure prediction task and\ndesign an encoder-decoder model to predict the argument of the predetermined\noperation in each generative step. In the second stage, we follow the previous\nmethods to rank the candidate queries. The experimental results show that our\nformal query building approach outperforms existing methods on complex\nquestions while staying competitive on simple questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yuncheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discrete and Soft Prompting for Multilingual Models. (arXiv:2109.03630v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03630","description":"<p>It has been shown for English that discrete and soft prompting perform\nstrongly in few-shot learning with pretrained language models (PLMs). In this\npaper, we show that discrete and soft prompting perform better than finetuning\nin multilingual cases: Crosslingual transfer and in-language training of\nmultilingual natural language inference. For example, with 48 English training\nexamples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely\nsurpassing the majority baseline (33.33%). In contrast, discrete and soft\nprompting outperform finetuning, achieving 36.43% and 38.79%. We also\ndemonstrate good performance of prompting with training data in multiple\nlanguages other than English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengjie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach. (arXiv:2109.03645v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03645","description":"<p>In the context of neural machine translation, data augmentation (DA)\ntechniques may be used for generating additional training samples when the\navailable parallel data are scarce. Many DA approaches aim at expanding the\nsupport of the empirical data distribution by generating new sentence pairs\nthat contain infrequent words, thus making it closer to the true data\ndistribution of parallel sentences. In this paper, we propose to follow a\ncompletely different approach and present a multi-task DA approach in which we\ngenerate new sentence pairs with transformations, such as reversing the order\nof the target sentence, which produce unfluent target sentences. During\ntraining, these augmented sentences are used as auxiliary tasks in a multi-task\nframework with the aim of providing new contexts where the target prefix is not\ninformative enough to predict the next word. This strengthens the encoder and\nforces the decoder to pay more attention to the source representations of the\nencoder. Experiments carried out on six low-resource translation tasks show\nconsistent improvements over the baseline and over DA methods aiming at\nextending the support of the empirical data distribution. The systems trained\nwith our approach rely more on the source tokens, are more robust against\ndomain shift and suffer less hallucinations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Cartagena_V/0/1/0/all/0/1\">V&#xed;ctor M. S&#xe1;nchez-Cartagena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espla_Gomis_M/0/1/0/all/0/1\">Miquel Espl&#xe0;-Gomis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Ortiz_J/0/1/0/all/0/1\">Juan Antonio P&#xe9;rez-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Martinez_F/0/1/0/all/0/1\">Felipe S&#xe1;nchez-Mart&#xed;nez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sustainable Modular Debiasing of Language Models. (arXiv:2109.03646v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03646","description":"<p>Unfair stereotypical biases (e.g., gender, racial, or religious biases)\nencoded in modern pretrained language models (PLMs) have negative ethical\nimplications for widespread adoption of state-of-the-art language technology.\nTo remedy for this, a wide range of debiasing techniques have recently been\nintroduced to remove such stereotypical biases from PLMs. Existing debiasing\nmethods, however, directly modify all of the PLMs parameters, which -- besides\nbeing computationally expensive -- comes with the inherent risk of\n(catastrophic) forgetting of useful language knowledge acquired in pretraining.\nIn this work, we propose a more sustainable modular debiasing approach based on\ndedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter\nmodules into the original PLM layers and (2) update only the adapters (i.e., we\nkeep the original PLM parameters frozen) via language modeling training on a\ncounterfactually augmented corpus. We showcase ADELE, in gender debiasing of\nBERT: our extensive evaluation, encompassing three intrinsic and two extrinsic\nbias measures, renders ADELE, very effective in bias mitigation. We further\nshow that -- due to its modular nature -- ADELE, coupled with task adapters,\nretains fairness even after large-scale downstream training. Finally, by means\nof multilingual BERT, we successfully transfer ADELE, to six target languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luken_T/0/1/0/all/0/1\">Tobias L&#xfc;ken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction. (arXiv:2109.03659v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03659","description":"<p>Relation extraction systems require large amounts of labeled examples which\nare costly to annotate. In this work we reformulate relation extraction as an\nentailment task, with simple, hand-made, verbalizations of relations produced\nin less than 15 min per relation. The system relies on a pretrained textual\nentailment engine which is run as-is (no training examples, zero-shot) or\nfurther fine-tuned on labeled examples (few-shot or fully trained). In our\nexperiments on TACRED we attain 63% F1 zero-shot, 69% with 16 examples per\nrelation (17% points better than the best supervised system on the same\nconditions), and only 4 points short to the state-of-the-art (which uses 20\ntimes more training data). We also show that the performance can be improved\nsignificantly with larger entailment models, up to 12 points in zero-shot,\nallowing to report the best results to date on TACRED when fully trained. The\nanalysis shows that our few-shot systems are specially effective when\ndiscriminating between relations, and that the performance difference in low\ndata regimes comes mainly from identifying no-relation cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sainz_O/0/1/0/all/0/1\">Oscar Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1\">Oier Lopez de Lacalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1\">Gorka Labaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrena_A/0/1/0/all/0/1\">Ander Barrena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Aspect Target Sentiment Classification with Natural Language Prompts. (arXiv:2109.03685v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03685","description":"<p>For many business applications, we often seek to analyze sentiments\nassociated with any arbitrary aspects of commercial products, despite having a\nvery limited amount of labels or even without any labels at all. However,\nexisting aspect target sentiment classification (ATSC) models are not trainable\nif annotated datasets are not available. Even with labeled data, they fall\nshort of reaching satisfactory performance. To address this, we propose simple\napproaches that better solve ATSC with natural language prompts, enabling the\ntask under zero-shot cases and enhancing supervised settings, especially for\nfew-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop\ndomain, our method of reformulating ATSC as an NLI task outperforms supervised\nSOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points.\nMoreover, we demonstrate that our prompts could handle implicitly stated\naspects as well: our models reach about 77% accuracy on detecting sentiments\nfor aspect categories (e.g., food), which do not necessarily appear within the\ntext, even though we trained the models only with explicitly mentioned aspect\nterms (e.g., fajitas) from just 16 reviews - while the accuracy of the\nno-prompt baseline is only around 65%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seoh_R/0/1/0/all/0/1\">Ronald Seoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birle_I/0/1/0/all/0/1\">Ian Birle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tak_M/0/1/0/all/0/1\">Mrinal Tak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haw-Shiuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinette_B/0/1/0/all/0/1\">Brian Pinette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hough_A/0/1/0/all/0/1\">Alfred Hough</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Entailment Patterns for Lexical Inference in Context. (arXiv:2109.03695v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03695","description":"<p>Combining a pretrained language model (PLM) with textual patterns has been\nshown to help in both zero- and few-shot settings. For zero-shot performance,\nit makes sense to design patterns that closely resemble the text seen during\nself-supervised pretraining because the model has never seen anything else.\nSupervised training allows for more flexibility. If we allow for tokens outside\nthe PLM's vocabulary, patterns can be adapted more flexibly to a PLM's\nidiosyncrasies. Contrasting patterns where a \"token\" can be any continuous\nvector vs. those where a discrete choice between vocabulary elements has to be\nmade, we call our method CONtinuous pAtterNs (CONAN). We evaluate CONAN on two\nestablished benchmarks for lexical inference in context (LIiC) a.k.a. predicate\nentailment, a challenging natural language understanding task with relatively\nsmall training sets. In a direct comparison with discrete patterns, CONAN\nconsistently leads to improved performance, setting a new state of the art. Our\nexperiments give valuable insights into the kind of pattern that enhances a\nPLM's performance on LIiC and raise important questions regarding our\nunderstanding of PLMs using text patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Martin Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Policy Compliance Detection via Question Answering. (arXiv:2109.03731v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03731","description":"<p>Policy compliance detection is the task of ensuring that a scenario conforms\nto a policy (e.g. a claim is valid according to government rules or a post in\nan online platform conforms to community guidelines). This task has been\npreviously instantiated as a form of textual entailment, which results in poor\naccuracy due to the complexity of the policies. In this paper we propose to\naddress policy compliance detection via decomposing it into question answering,\nwhere questions check whether the conditions stated in the policy apply to the\nscenario, and an expression tree combines the answers to obtain the label.\nDespite the initial upfront annotation cost, we demonstrate that this approach\nresults in better accuracy, especially in the cross-policy setup where the\npolicies during testing are unseen in training. In addition, it allows us to\nuse existing question answering models pre-trained on existing large datasets.\nFinally, it explicitly identifies the information missing from a scenario in\ncase policy compliance cannot be determined. We conduct our experiments using a\nrecent dataset consisting of government policies, which we augment with expert\nannotations and find that the cost of annotating question answering\ndecomposition is largely offset by improved inter-annotator agreement and\nspeed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1\">Marzieh Saeidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1\">Majid Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories. (arXiv:2109.03754v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03754","description":"<p>Measuring event salience is essential in the understanding of stories. This\npaper takes a recent unsupervised method for salience detection derived from\nBarthes Cardinal Functions and theories of surprise and applies it to longer\nnarrative forms. We improve the standard transformer language model by\nincorporating an external knowledgebase (derived from Retrieval Augmented\nGeneration) and adding a memory mechanism to enhance performance on longer\nworks. We use a novel approach to derive salience annotation using\nchapter-aligned summaries from the Shmoop corpus for classic literary works.\nOur evaluation against this data demonstrates that our salience detection model\nimproves performance over and above a non-knowledgebase and memory augmented\nlanguage model, both of which are crucial to this improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilmot_D/0/1/0/all/0/1\">David Wilmot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning by Acquiring Contrastive Examples. (arXiv:2109.03764v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03764","description":"<p>Common acquisition functions for active learning use either uncertainty or\ndiversity sampling, aiming to select difficult and diverse data points from the\npool of unlabeled data, respectively. In this work, leveraging the best of both\nworlds, we propose an acquisition function that opts for selecting\n\\textit{contrastive examples}, i.e. data points that are similar in the model\nfeature space and yet the model outputs maximally different predictive\nlikelihoods. We compare our approach, CAL (Contrastive Active Learning), with a\ndiverse set of acquisition functions in four natural language understanding\ntasks and seven datasets. Our experiments show that CAL performs consistently\nbetter or equal than the best performing baseline across all tasks, on both\nin-domain and out-of-domain data. We also conduct an extensive ablation study\nof our method and we further analyze all actively acquired datasets showing\nthat CAL achieves a better trade-off between uncertainty and diversity compared\nto other strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1\">Katerina Margatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vernikos_G/0/1/0/all/0/1\">Giorgos Vernikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension. (arXiv:2109.03772v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03772","description":"<p>Multi-party dialogue machine reading comprehension (MRC) brings tremendous\nchallenge since it involves multiple speakers at one dialogue, resulting in\nintricate speaker information flows and noisy dialogue contexts. To alleviate\nsuch difficulties, previous models focus on how to incorporate these\ninformation using complex graph-based modules and additional manually labeled\ndata, which is usually rare in real scenarios. In this paper, we design two\nlabour-free self- and pseudo-self-supervised prediction tasks on speaker and\nkey-utterance to implicitly model the speaker information flows, and capture\nsalient clues in a long dialogue. Experimental results on two benchmark\ndatasets have justified the effectiveness of our method over competitive\nbaselines and current state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forget me not: A Gentle Reminder to Mind the Simple Multi-Layer Perceptron Baseline for Text Classification. (arXiv:2109.03777v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03777","description":"<p>Graph neural networks have triggered a resurgence of graph-based text\nclassification. We show that already a simple MLP baseline achieves comparable\nperformance on benchmark datasets, questioning the importance of synthetic\ngraph structures. When considering an inductive scenario, i. e., when adding\nnew documents to a corpus, a simple MLP even outperforms most graph-based\nmodels. We further fine-tune DistilBERT for comparison and find that it\noutperforms all state-of-the-art models. We suggest that future studies use at\nleast an MLP baseline to contextualize the results. We provide recommendations\nfor the design and training of such a baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highly Parallel Autoregressive Entity Linking with Discriminative Correction. (arXiv:2109.03792v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03792","description":"<p>Generative approaches have been recently shown to be effective for both\nEntity Disambiguation and Entity Linking (i.e., joint mention detection and\ndisambiguation). However, the previously proposed autoregressive formulation\nfor EL suffers from i) high computational cost due to a complex (deep) decoder,\nii) non-parallelizable decoding that scales with the source sequence length,\nand iii) the need for training on a large amount of data. In this work, we\npropose a very efficient approach that parallelizes autoregressive linking\nacross all potential mentions and relies on a shallow and efficient decoder.\nMoreover, we augment the generative objective with an extra discriminative\ncomponent, i.e., a correction term which lets us directly optimize the\ngenerator's ranking. When taken together, these techniques tackle all the above\nissues: our model is &gt;70 times faster and more accurate than the previous\ngenerative method, outperforming state-of-the-art approaches on the standard\nEnglish dataset AIDA-CoNLL. Source code available at\nhttps://github.com/nicola-decao/efficient-autoregressive-EL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1\">Nicola De Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smelting Gold and Silver for Improved Multilingual AMR-to-Text Generation. (arXiv:2109.03808v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03808","description":"<p>Recent work on multilingual AMR-to-text generation has exclusively focused on\ndata augmentation strategies that utilize silver AMR. However, this assumes a\nhigh quality of generated AMRs, potentially limiting the transferability to the\ntarget task. In this paper, we investigate different techniques for\nautomatically generating AMR annotations, where we aim to study which source of\ninformation yields better multilingual results. Our models trained on gold AMR\nwith silver (machine translated) sentences outperform approaches which leverage\ngenerated silver AMR. We find that combining both complementary sources of\ninformation further improves multilingual AMR-to-text generation. Our models\nsurpass the previous state of the art for German, Italian, Spanish, and Chinese\nby a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Common Semantic Space for Monolingual and Cross-Lingual Meta-Embeddings. (arXiv:2001.06381v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2001.06381","description":"<p>This paper presents a new technique for creating monolingual and\ncross-lingual meta-embeddings. Our method integrates multiple word embeddings\ncreated from complementary techniques, textual sources, knowledge bases and\nlanguages. Existing word vectors are projected to a common semantic space using\nlinear transformations and averaging. With our method the resulting\nmeta-embeddings maintain the dimensionality of the original embeddings without\nlosing information while dealing with the out-of-vocabulary problem. An\nextensive empirical evaluation demonstrates the effectiveness of our technique\nwith respect to previous work on various intrinsic and extrinsic multilingual\nevaluations, obtaining competitive results for Semantic Textual Similarity and\nstate-of-the-art performance for word similarity and POS tagging (English and\nSpanish). The resulting cross-lingual meta-embeddings also exhibit excellent\ncross-lingual transfer learning capabilities. In other words, we can leverage\npre-trained source embeddings from a resource-rich language in order to improve\nthe word representations for under-resourced languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Ferrero_I/0/1/0/all/0/1\">Iker Garc&#xed;a-Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigau_G/0/1/0/all/0/1\">German Rigau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Chart-based Constituency Parse Extraction from Pre-trained Language Models. (arXiv:2004.13805v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.13805","description":"<p>As it has been unveiled that pre-trained language models (PLMs) are to some\nextent capable of recognizing syntactic concepts in natural language, much\neffort has been made to develop a method for extracting complete (binary)\nparses from PLMs without training separate parsers. We improve upon this\nparadigm by proposing a novel chart-based method and an effective top-K\nensemble technique. Moreover, we demonstrate that we can broaden the scope of\napplication of the approach into multilingual settings. Specifically, we show\nthat by applying our method on multilingual PLMs, it becomes possible to induce\nnon-trivial parses for sentences from nine languages in an integrated and\nlanguage-agnostic manner, attaining performance superior or comparable to that\nof unsupervised PCFGs. We also verify that our approach is robust to\ncross-lingual transfer. Finally, we provide analyses on the inner workings of\nour method. For instance, we discover universal attention heads which are\nconsistently sensitive to syntactic information irrespective of the input\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Equations: Inherently Interpretable Sparse Word Embeddingsthrough Sparse Coding. (arXiv:2004.13847v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.13847","description":"<p>Word embeddings are a powerful natural lan-guage processing technique, but\nthey are ex-tremely difficult to interpret. To enable inter-pretable NLP\nmodels, we create vectors whereeach dimension isinherently interpretable.\nByinherently interpretable, we mean a systemwhere each dimension is associated\nwith somehuman-understandablehintthat can describethe meaning of that\ndimension. In order tocreate more interpretable word embeddings,we transform\npretrained dense word embed-dings into sparse embeddings. These new em-beddings\nare inherently interpretable: each oftheir dimensions is created from and\nrepre-sents a natural language word or specific gram-matical concept. We\nconstruct these embed-dings through sparse coding, where each vec-tor in the\nbasis set is itself a word embedding.Therefore, each dimension of our sparse\nvec-tors corresponds to a natural language word.We also show that models\ntrained using thesesparse embeddings can achieve good perfor-mance and are more\ninterpretable in practice,including through human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Templeton_A/0/1/0/all/0/1\">Adly Templeton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2010.09313v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.09313","description":"<p>Probing complex language models has recently revealed several insights into\nlinguistic and semantic patterns found in the learned representations. In this\npaper, we probe BERT specifically to understand and measure the relational\nknowledge it captures. We utilize knowledge base completion tasks to probe\nevery layer of pre-trained as well as fine-tuned BERT (ranking, question\nanswering, NER). Our findings show that knowledge is not just contained in\nBERT's final layers. Intermediate layers contribute a significant amount\n(17-60%) to the total knowledge found. Probing intermediate layers also reveals\nhow different types of knowledge emerge at varying rates. When BERT is\nfine-tuned, relational knowledge is forgotten but the extent of forgetting is\nimpacted by the fine-tuning objective but not the size of the dataset. We found\nthat ranking models forget the least and retain more knowledge in their final\nlayer. We release our code on github to repeat the experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallat_J/0/1/0/all/0/1\">Jonas Wallat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaspreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00234","description":"<p>Transformers have shown improved performance when compared to previous\narchitectures for sequence processing such as RNNs. Despite their sizeable\nperformance gains, as recently suggested, the model is computationally\nexpensive to train and with a high parameter budget. In light of this, we\nexplore parameter-sharing methods in Transformers with a specific focus on\ngenerative models. We perform an analysis of different parameter\nsharing/reduction methods and develop the Subformer. Our model combines\nsandwich-style parameter sharing, which overcomes naive cross-layer parameter\nsharing in generative models, and self-attentive embedding factorization\n(SAFE). Experiments on machine translation, abstractive summarization and\nlanguage modeling show that the Subformer can outperform the Transformer even\nwhen using significantly fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marrese_Taylor_E/0/1/0/all/0/1\">Edison Marrese-Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimum projective linearizations of trees in linear time. (arXiv:2102.03277v4 [cs.DS] UPDATED)","link":"http://arxiv.org/abs/2102.03277","description":"<p>The Minimum Linear Arrangement problem (MLA) consists of finding a mapping\n$\\pi$ from vertices of a graph to distinct integers that minimizes\n$\\sum_{\\{u,v\\}\\in E}|\\pi(u) - \\pi(v)|$. In that setting, vertices are often\nassumed to lie on a horizontal line and edges are drawn as semicircles above\nsaid line. For trees, various algorithms are available to solve the problem in\npolynomial time in $n=|V|$. There exist variants of the MLA in which the\narrangements are constrained. Iordanskii, and later Hochberg and Stallmann\n(HS), put forward $O(n)$-time algorithms that solve the problem when\narrangements are constrained to be planar (also known as one-page book\nembeddings). We also consider linear arrangements of rooted trees that are\nconstrained to be projective (planar embeddings where the root is not covered\nby any edge). Gildea and Temperley (GT) sketched an algorithm for projective\narrangements which they claimed runs in $O(n)$ but did not provide any\njustification of its cost. In contrast, Park and Levy claimed that GT's\nalgorithm runs in $O(n \\log d_{max})$ where $d_{max}$ is the maximum degree but\ndid not provide sufficient detail. Here we correct an error in HS's algorithm\nfor the planar case, show its relationship with the projective case, and derive\nsimple algorithms for the projective and planar cases that run without a doubt\nin $O(n)$ time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Juan Luis Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structural Adapters in Pretrained Language Models for AMR-to-text Generation. (arXiv:2103.09120v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.09120","description":"<p>Pretrained language models (PLM) have recently advanced graph-to-text\ngeneration, where the input graph is linearized into a sequence and fed into\nthe PLM to obtain its representation. However, efficiently encoding the graph\nstructure in PLMs is challenging because such models were pretrained on natural\nlanguage, and modeling structured data may lead to catastrophic forgetting of\ndistributional knowledge. In this paper, we propose StructAdapt, an adapter\nmethod to encode graph structure into PLMs. Contrary to prior work, StructAdapt\neffectively models interactions among the nodes based on the graph\nconnectivity, only training graph structure-aware adapter parameters. In this\nway, we incorporate task-specific knowledge while maintaining the topological\nstructure of the graph. We empirically show the benefits of explicitly encoding\ngraph structure into PLMs using StructAdapt, outperforming the state of the art\non two AMR-to-text datasets, training only 5.1% of the PLM parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfExplain: A Self-Explaining Architecture for Neural Text Classifiers. (arXiv:2103.12279v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.12279","description":"<p>We introduce SelfExplain, a novel self-explaining model that explains a text\nclassifier's predictions using phrase-based concepts. SelfExplain augments\nexisting neural classifiers by adding (1) a globally interpretable layer that\nidentifies the most influential concepts in the training set for a given sample\nand (2) a locally interpretable layer that quantifies the contribution of each\nlocal input concept by computing a relevance score relative to the predicted\nlabel. Experiments across five text-classification datasets show that\nSelfExplain facilitates interpretability without sacrificing performance. Most\nimportantly, explanations from SelfExplain show sufficiency for model\npredictions and are perceived as adequate, trustworthy and understandable by\nhuman judges compared to existing widely-used baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1\">Dheeraj Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training. (arXiv:2104.01027v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2104.01027","description":"<p>Self-supervised learning of speech representations has been a very active\nresearch area but most work is focused on a single domain such as read audio\nbooks for which there exist large quantities of labeled and unlabeled data. In\nthis paper, we explore more general setups where the domain of the unlabeled\ndata for pre-training data differs from the domain of the labeled data for\nfine-tuning, which in turn may differ from the test data domain. Our\nexperiments show that using target domain data during pre-training leads to\nlarge performance improvements across a variety of setups. On a large-scale\ncompetitive setup, we show that pre-training on unlabeled in-domain data\nreduces the gap between models trained on in-domain and out-of-domain labeled\ndata by 66%-73%. This has obvious practical implications since it is much\neasier to obtain unlabeled target domain data than labeled data. Moreover, we\nfind that pre-training on multiple domains improves generalization performance\non domains not seen during training. Code and models will be made available at\nhttps://github.com/pytorch/fairseq.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_A/0/1/0/all/0/1\">Anuroop Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratap_V/0/1/0/all/0/1\">Vineel Pratap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. (arXiv:2104.04670v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04670","description":"<p>Large pre-trained language models (LMs) such as GPT-3 have acquired a\nsurprising ability to perform zero-shot learning. For example, to classify\nsentiment without any training examples, we can \"prompt\" the LM with the review\nand the label description \"Does the user like this movie?\", and ask whether the\nnext word is \"yes\" or \"no\". However, the next word prediction training\nobjective is still misaligned with the target zero-shot learning objective. To\naddress this weakness, we propose meta-tuning, which directly optimizes the\nzero-shot learning objective by fine-tuning pre-trained language models on a\ncollection of datasets. We focus on classification tasks, and construct the\nmeta-dataset by aggregating 43 existing datasets and annotating 441 label\ndescriptions in a question-answering (QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-sized QA model and the previous SOTA\nzero-shot learning system based on natural language inference. Additionally,\nincreasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,\nand we forecast that even larger models would perform better. Therefore,\nmeasuring zero-shot learning performance on language models out-of-the-box\nmight underestimate their true potential, and community-wide efforts on\naggregating datasets and unifying their formats can help build models that\nanswer prompts better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kristy Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media. (arXiv:2104.08116v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08116","description":"<p>Language use differs between domains and even within a domain, language use\nchanges over time. For pre-trained language models like BERT, domain adaptation\nthrough continued pre-training has been shown to improve performance on\nin-domain downstream tasks. In this article, we investigate whether temporal\nadaptation can bring additional benefits. For this purpose, we introduce a\ncorpus of social media comments sampled over three years. It contains\nunlabelled data for adaptation and evaluation on an upstream masked language\nmodelling task as well as labelled data for fine-tuning and evaluation on a\ndownstream document classification task. We find that temporality matters for\nboth tasks: temporal adaptation improves upstream and temporal fine-tuning\ndownstream task performance. Time-specific models generally perform better on\npast than on future test sets, which matches evidence on the bursty usage of\ntopical words. However, adapting BERT to time and domain does not improve\nperformance on the downstream task over only adapting to domain. Token-level\nanalysis shows that temporal adaptation captures event-driven changes in\nlanguage use in the downstream task, but not those changes that are actually\nrelevant to task performance. Based on our findings, we discuss when temporal\nadaptation may be more effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1\">Janet B. Pierrehumbert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Noisy Labels for Entity-Centric Information Extraction. (arXiv:2104.08656v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08656","description":"<p>Recent information extraction approaches have relied on training deep neural\nmodels. However, such models can easily overfit noisy labels and suffer from\nperformance degradation. While it is very costly to filter noisy labels in\nlarge learning resources, recent studies show that such labels take more\ntraining steps to be memorized and are more frequently forgotten than clean\nlabels, therefore are identifiable in training. Motivated by such properties,\nwe propose a simple co-regularization framework for entity-centric information\nextraction, which consists of several neural models with identical structures\nbut different parameter initialization. These models are jointly optimized with\nthe task-specific losses and are regularized to generate similar predictions\nbased on an agreement loss, which prevents overfitting on noisy labels.\nExtensive experiments on two widely used but noisy benchmarks for information\nextraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework.\nWe release our code to the community for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. (arXiv:2104.08663v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2104.08663","description":"<p>Existing neural information retrieval (IR) models have often been studied in\nhomogeneous and narrow settings, which has considerably limited insights into\ntheir out-of-distribution (OOD) generalization capabilities. To address this,\nand to facilitate researchers to broadly evaluate the effectiveness of their\nmodels, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous\nevaluation benchmark for information retrieval. We leverage a careful selection\nof 18 publicly available datasets from diverse text retrieval tasks and domains\nand evaluate 10 state-of-the-art retrieval systems including lexical, sparse,\ndense, late-interaction and re-ranking architectures on the BEIR benchmark. Our\nresults show BM25 is a robust baseline and re-ranking and\nlate-interaction-based models on average achieve the best zero-shot\nperformances, however, at high computational costs. In contrast, dense and\nsparse-retrieval models are computationally more efficient but often\nunderperform other approaches, highlighting the considerable room for\nimprovement in their generalization capabilities. We hope this framework allows\nus to better evaluate and understand existing retrieval systems, and\ncontributes to accelerating progress towards better robust and generalizable\nsystems in the future. BEIR is publicly available at\nhttps://github.com/UKPLab/beir.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nandan Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Abhishek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Out-of-Distribution Detection for Pretrained Transformers. (arXiv:2104.08812v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08812","description":"<p>Pretrained Transformers achieve remarkable performance when training and test\ndata are from the same distribution. However, in real-world scenarios, the\nmodel often faces out-of-distribution (OOD) instances that can cause severe\nsemantic shift problems at inference time. Therefore, in practice, a reliable\nmodel should identify such instances, and then either reject them during\ninference or pass them over to models that handle another distribution. In this\npaper, we develop an unsupervised OOD detection method, in which only the\nin-distribution (ID) data are used in training. We propose to fine-tune the\nTransformers with a contrastive loss, which improves the compactness of\nrepresentations, such that OOD instances can be better differentiated from ID\nones. These OOD instances can then be accurately detected using the Mahalanobis\ndistance in the model's penultimate layer. We experiment with comprehensive\nsettings and achieve near-perfect OOD detection performance, outperforming\nbaselines drastically. We further investigate the rationales behind the\nimprovement, finding that more compact representations through margin-based\ncontrastive learning bring the improvement. We release our code to the\ncommunity for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stream-level Latency Evaluation for Simultaneous Machine Translation. (arXiv:2104.08817v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08817","description":"<p>Simultaneous machine translation has recently gained traction thanks to\nsignificant quality improvements and the advent of streaming applications.\nSimultaneous translation systems need to find a trade-off between translation\nquality and response time, and with this purpose multiple latency measures have\nbeen proposed. However, latency evaluations for simultaneous translation are\nestimated at the sentence level, not taking into account the sequential nature\nof a streaming scenario. Indeed, these sentence-level latency measures are not\nwell suited for continuous stream translation resulting in figures that are not\ncoherent with the simultaneous translation policy of the system being assessed.\nThis work proposes a stream-level adaptation of the current latency measures\nbased on a re-segmentation approach applied to the output translation, that is\nsuccessfully evaluated on streaming conditions for a reference IWSLT task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iranzo_Sanchez_J/0/1/0/all/0/1\">Javier Iranzo-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Jorge Civera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_A/0/1/0/all/0/1\">Alfons Juan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2106.02902v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.02902","description":"<p>Probing complex language models has recently revealed several insights into\nlinguistic and semantic patterns found in the learned representations. In this\narticle, we probe BERT specifically to understand and measure the relational\nknowledge it captures in its parametric memory. While probing for linguistic\nunderstanding is commonly applied to all layers of BERT as well as fine-tuned\nmodels, this has not been done for factual knowledge. We utilize existing\nknowledge base completion tasks (LAMA) to probe every layer of pre-trained as\nwell as fine-tuned BERT models(ranking, question answering, NER). Our findings\nshow that knowledge is not just contained in BERT's final layers. Intermediate\nlayers contribute a significant amount (17-60%) to the total knowledge found.\nProbing intermediate layers also reveals how different types of knowledge\nemerge at varying rates. When BERT is fine-tuned, relational knowledge is\nforgotten. The extent of forgetting is impacted by the fine-tuning objective\nand the training data. We found that ranking models forget the least and retain\nmore knowledge in their final layer compared to masked language modeling and\nquestion-answering. However, masked language modeling performed the best at\nacquiring new knowledge from the training data. When it comes to learning\nfacts, we found that capacity and fact density are key factors. We hope this\ninitial work will spur further research into understanding the parametric\nmemory of language models and the effect of training objectives on factual\nknowledge. The code to repeat the experiments is publicly available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallat_J/0/1/0/all/0/1\">Jonas Wallat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaspreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JNLP Team: Deep Learning Approaches for Legal Processing Tasks in COLIEE 2021. (arXiv:2106.13405v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.13405","description":"<p>COLIEE is an annual competition in automatic computerized legal text\nprocessing. Automatic legal document processing is an ambitious goal, and the\nstructure and semantics of the law are often far more complex than everyday\nlanguage. In this article, we survey and report our methods and experimental\nresults in using deep learning in legal document processing. The results show\nthe difficulties as well as potentials in this family of approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1\">Thi-Hai-Yen Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1\">Quan Minh Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chau Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1\">Binh Tran Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Le Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1\">Ken Satoh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents. (arXiv:2108.04539v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04539","description":"<p>Understanding documents from their visual snapshots is an emerging problem\nthat requires both advanced computer vision and NLP methods. The recent advance\nin OCR enables the accurate recognition of text blocks, yet it is still\nchallenging to extract key information from documents due to the diversity of\ntheir layouts. Although recent studies on pre-trained language models show the\nimportance of incorporating layout information on this task, the conjugation of\ntexts and their layouts still follows the style of BERT optimized for\nunderstanding the 1D text. This implies there is room for further improvement\nconsidering the 2D nature of text layouts. This paper introduces a pre-trained\nlanguage model, BERT Relying On Spatiality (BROS), which effectively utilizes\nthe information included in individual text blocks and their layouts.\nSpecifically, BROS encodes spatial information by utilizing relative positions\nand learns spatial dependencies between OCR blocks with a novel area-masking\nstrategy. These two novel approaches lead to an efficient encoding of spatial\nlayout information highlighted by the robust performance of BROS under\nlow-resource environments. We also introduce a general-purpose parser that can\nbe combined with BROS to extract key information even when there is no order\ninformation between text blocks. BROS shows its superiority on four public\nbenchmarks -- FUNSD, SROIE*, CORD, and SciTSR -- and its robustness in\npractical cases where order information of text blocks is not available.\nFurther experiments with a varying number of training examples demonstrate the\nhigh training efficiency of our approach. Our code will be open to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1\">Teakgyu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1\">Mingi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daehyun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features is dependent\nupon each other. Experiment results on six public datasets show that our model\nperforms significantly better than previous approaches. In addition, contrary\nto what previous work has claimed, our auxiliary experiments suggest that\nrelation prediction is contributory to named entity prediction in a\nnon-negligible way. The source code can be found at\nhttps://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12229","description":"<p>The ability to detect Out-of-Domain (OOD) inputs has been a critical\nrequirement in many real-world NLP applications since the inclusion of\nunsupported OOD inputs may lead to catastrophic failure of systems. However, it\nremains an empirical question whether current algorithms can tackle such\nproblem reliably in a realistic scenario where zero OOD training data is\navailable. In this study, we propose ProtoInfoMax, a new architecture that\nextends Prototypical Networks to simultaneously process In-Domain (ID) and OOD\nsentences via Mutual Information Maximization (InfoMax) objective. Experimental\nresults show that our proposed method can substantially improve performance up\nto 20% for OOD detection in low resource settings of text classification. We\nalso show that ProtoInfoMax is less prone to typical over-confidence Error of\nNeural Networks, leading to more reliable ID and OOD prediction outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nimah_I/0/1/0/all/0/1\">Iftitahu Ni&#x27;mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation. (arXiv:2108.13134v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13134","description":"<p>Despite significant progress has been achieved in text summarization, factual\ninconsistency in generated summaries still severely limits its practical\napplications. Among the key factors to ensure factual consistency, a reliable\nautomatic evaluation metric is the first and the most crucial one. However,\nexisting metrics either neglect the intrinsic cause of the factual\ninconsistency or rely on auxiliary tasks, leading to an unsatisfied correlation\nwith human judgments or increasing the inconvenience of usage in practice. In\nlight of these challenges, we propose a novel metric to evaluate the factual\nconsistency in text summarization via counterfactual estimation, which\nformulates the causal relationship among the source document, the generated\nsummary, and the language prior. We remove the effect of language prior, which\ncan cause factual inconsistency, from the total causal effect on the generated\nsummary, and provides a simple yet effective way to evaluate consistency\nwithout relying on other auxiliary tasks. We conduct a series of experiments on\nthree public abstractive text summarization datasets, and demonstrate the\nadvantages of the proposed metric in both improving the correlation with human\njudgments and the convenience of usage. The source code is available at\nhttps://github.com/xieyxclack/factual_coco.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuexiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Sequence-to-Sequence Dialogue State Tracking. (arXiv:2108.13990v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13990","description":"<p>Sequence-to-sequence models have been applied to a wide variety of NLP tasks,\nbut how to properly use them for dialogue state tracking has not been\nsystematically investigated. In this paper, we study this problem from the\nperspectives of pre-training objectives as well as the formats of context\nrepresentations. We demonstrate that the choice of pre-training objective makes\na significant difference to the state tracking quality. In particular, we find\nthat masked span prediction is more effective than auto-regressive language\nmodeling. We also explore using Pegasus, a span prediction-based pre-training\nobjective for text summarization, for the state tracking model. We found that\npre-training for the seemingly distant summarization task works surprisingly\nwell for dialogue state tracking. In addition, we found that while recurrent\nstate context representation works also reasonably well, the model may have a\nhard time recovering from earlier mistakes. We conducted experiments on the\nMultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdieh_M/0/1/0/all/0/1\">Mahdis Mahdieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ye Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MergeBERT: Program Merge Conflict Resolution via Neural Transformers. (arXiv:2109.00084v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2109.00084","description":"<p>Collaborative software development is an integral part of the modern software\ndevelopment life cycle, essential to the success of large-scale software\nprojects. When multiple developers make concurrent changes around the same\nlines of code, a merge conflict may occur. Such conflicts stall pull requests\nand continuous integration pipelines for hours to several days, seriously\nhurting developer productivity.\n</p>\n<p>In this paper, we introduce MergeBERT, a novel neural program merge framework\nbased on the token-level three-way differencing and a transformer encoder\nmodel. Exploiting restricted nature of merge conflict resolutions, we\nreformulate the task of generating the resolution sequence as a classification\ntask over a set of primitive merge patterns extracted from real-world merge\ncommit data.\n</p>\n<p>Our model achieves 64--69% precision of merge resolution synthesis, yielding\nnearly a 2x performance improvement over existing structured and neural program\nmerge tools. Finally, we demonstrate versatility of our model, which is able to\nperform program merge in a multilingual setting with Java, JavaScript,\nTypeScript, and C# programming languages, generalizing zero-shot to unseen\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mytkowicz_T/0/1/0/all/0/1\">Todd Mytkowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_N/0/1/0/all/0/1\">Negar Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhoury_S/0/1/0/all/0/1\">Sarah Fakhoury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinella_E/0/1/0/all/0/1\">Elizabeth Dinella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bird_C/0/1/0/all/0/1\">Christian Bird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_S/0/1/0/all/0/1\">Shuvendu Lahiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00430","description":"<p>Medical dialogue systems (MDSs) aim to assist doctors and patients with a\nrange of professional medical services, i.e., diagnosis, consultation, and\ntreatment. However, one-stop MDS is still unexplored because: (1) no dataset\nhas so large-scale dialogues contains both multiple medical services and\nfine-grained medical labels (i.e., intents, slots, values); (2) no model has\naddressed a MDS based on multiple-service conversations in a unified framework.\nIn this work, we first build a Multiple-domain Multiple-service medical\ndialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between\ndoctors and patients, covering 276 types of diseases, 2,468 medical entities,\nand 3 specialties of medical services. To the best of our knowledge, it is the\nonly medical dialogue dataset that includes both multiple medical services and\nfine-grained medical labels. Then, we formulate a one-stop MDS as a\nsequence-to-sequence generation problem. We unify a MDS with causal language\nmodeling and conditional causal language modeling, respectively. Specifically,\nwe employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)\nand their variants to get benchmarks on M^2-MedDialog dataset. We also propose\npseudo labeling and natural perturbation methods to expand M2-MedDialog dataset\nand enhance the state-of-the-art pretrained models. We demonstrate the results\nachieved by the benchmarks so far through extensive experiments on\nM2-MedDialog. We release the dataset, the code, as well as the evaluation\nscripts to facilitate future research in this important research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guojun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiahuan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huasheng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PermuteFormer: Efficient Relative Position Encoding for Long Sequences. (arXiv:2109.02377v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02377","description":"<p>A recent variation of Transformer, Performer, scales Transformer to longer\nsequences with a linear attention mechanism. However, it is not compatible with\nrelative position encoding, which has advantages over absolute position\nencoding. In this paper, we discuss possible ways to add relative position\nencoding to Performer. Based on the analysis, we propose PermuteFormer, a\nPerformer-based model with relative position encoding that scales linearly on\nlong sequences. PermuteFormer applies position-dependent transformation on\nqueries and keys to encode positional information into the attention module.\nThis transformation is carefully crafted so that the final output of\nself-attention is not affected by absolute positions of tokens. PermuteFormer\nintroduces negligible computational overhead by design that it runs as fast as\nPerformer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long\nsequences, as well as WikiText-103, a language modeling dataset. The\nexperiments show that PermuteFormer uniformly improves the performance of\nPerformer with almost no computational overhead and outperforms vanilla\nTransformer on most of the tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization. (arXiv:2109.02401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02401","description":"<p>Multimodal abstractive summarization (MAS) models that summarize videos\n(vision modality) and their corresponding transcripts (text modality) are able\nto extract the essential information from massive multimodal data on the\nInternet. Recently, large-scale generative pre-trained language models (GPLMs)\nhave been shown to be effective in text generation tasks. However, existing MAS\nmodels cannot leverage GPLMs' powerful generation ability. To fill this\nresearch gap, we aim to study two research questions: 1) how to inject visual\ninformation into GPLMs without hurting their generation ability; and 2) where\nis the optimal place in GPLMs to inject the visual information? In this paper,\nwe present a simple yet effective method to construct vision guided (VG) GPLMs\nfor the MAS task using attention-based add-on layers to incorporate visual\ninformation while maintaining their original text generation ability. Results\nshow that our best model significantly surpasses the prior state-of-the-art\nmodel by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,\nand our visual guidance method contributes 83.6% of the overall improvement.\nFurthermore, we conduct thorough ablation studies to analyze the effectiveness\nof various modality fusion methods and fusion locations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient conformer: Progressive downsampling and grouped attention for automatic speech recognition. (arXiv:2109.01163v2 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2109.01163","description":"<p>The recently proposed Conformer architecture has shown state-of-the-art\nperformances in Automatic Speech Recognition by combining convolution with\nattention to model both local and global dependencies. In this paper, we study\nhow to reduce the Conformer architecture complexity with a limited computing\nbudget, leading to a more efficient architecture design that we call Efficient\nConformer. We introduce progressive downsampling to the Conformer encoder and\npropose a novel attention mechanism named grouped attention, allowing us to\nreduce attention complexity from $O(n^{2}d)$ to $O(n^{2}d / g)$ for sequence\nlength $n$, hidden dimension $d$ and group size parameter $g$. We also\nexperiment the use of strided multi-head self-attention as a global\ndownsampling operation. Our experiments are performed on the LibriSpeech\ndataset with CTC and RNN-Transducer losses. We show that within the same\ncomputing budget, the proposed architecture achieves better performances with\nfaster training and decoding compared to the Conformer. Our 13M parameters CTC\nmodel achieves competitive WERs of 3.6%/9.0% without using a language model and\n2.7%/6.7% with an external n-gram language model on the test-clean/test-other\nsets while being 29% faster than our CTC Conformer baseline at inference and\n36% faster to train.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Burchi_M/0/1/0/all/0/1\">Maxime Burchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vielzeuf_V/0/1/0/all/0/1\">Valentin Vielzeuf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"MRI Reconstruction Using Deep Energy-Based Model. (arXiv:2109.03237v1 [eess.IV])","link":"http://arxiv.org/abs/2109.03237","description":"<p>Purpose: Although recent deep energy-based generative models (EBMs) have\nshown encouraging results in many image generation tasks, how to take advantage\nof the self-adversarial cogitation in deep EBMs to boost the performance of\nMagnetic Resonance Imaging (MRI) reconstruction is still desired.\n</p>\n<p>Methods: With the successful application of deep learning in a wide range of\nMRI reconstruction, a line of emerging research involves formulating an\noptimization-based reconstruction method in the space of a generative model.\nLeveraging this, a novel regularization strategy is introduced in this article\nwhich takes advantage of self-adversarial cogitation of the deep energy-based\nmodel. More precisely, we advocate for alternative learning a more powerful\nenergy-based model with maximum likelihood estimation to obtain the deep\nenergy-based information, represented as image prior. Simultaneously, implicit\ninference with Langevin dynamics is a unique property of re-construction. In\ncontrast to other generative models for reconstruction, the proposed method\nutilizes deep energy-based information as the image prior in reconstruction to\nimprove the quality of image.\n</p>\n<p>Results: Experiment results that imply the proposed technique can obtain\nremarkable performance in terms of high reconstruction accuracy that is\ncompetitive with state-of-the-art methods, and does not suffer from mode\ncollapse.\n</p>\n<p>Conclusion: Algorithmically, an iterative approach was presented to\nstrengthen EBM training with the gradient of energy network. The robustness and\nthe reproducibility of the algorithm were also experimentally validated. More\nimportantly, the proposed reconstruction framework can be generalized for most\nMRI reconstruction scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_Z/0/1/0/all/0/1\">Zongjiang Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Video Generation using Neural ODEs. (arXiv:2109.03292v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03292","description":"<p>Despite having been studied to a great extent, the task of conditional\ngeneration of sequences of frames, or videos, remains extremely challenging. It\nis a common belief that a key step towards solving this task resides in\nmodelling accurately both spatial and temporal information in video signals. A\npromising direction to do so has been to learn latent variable models that\npredict the future in latent space and project back to pixels, as suggested in\nrecent literature. Following this line of work and building on top of a family\nof models introduced in prior work, Neural ODE, we investigate an approach that\nmodels time-continuous dynamics over a continuous latent space with a\ndifferential equation with respect to time. The intuition behind this approach\nis that these trajectories in latent space could then be extrapolated to\ngenerate video frames beyond the time steps for which the model is trained. We\nshow that our approach yields promising results in the task of future frame\nprediction on the Moving MNIST dataset with 1 and 2 digits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanaa_D/0/1/0/all/0/1\">David Kanaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1\">Samira Ebrahimi Kahou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Representation Learning using Visual Field Expansion on Digital Pathology. (arXiv:2109.03299v1 [eess.IV])","link":"http://arxiv.org/abs/2109.03299","description":"<p>The examination of histopathology images is considered to be the gold\nstandard for the diagnosis and stratification of cancer patients. A key\nchallenge in the analysis of such images is their size, which can run into the\ngigapixels and can require tedious screening by clinicians. With the recent\nadvances in computational medicine, automatic tools have been proposed to\nassist clinicians in their everyday practice. Such tools typically process\nthese large images by slicing them into tiles that can then be encoded and\nutilized for different clinical models. In this study, we propose a novel\ngenerative framework that can learn powerful representations for such tiles by\nlearning to plausibly expand their visual field. In particular, we developed a\nprogressively grown generative model with the objective of visual field\nexpansion. Thus trained, our model learns to generate different tissue types\nwith fine details, while simultaneously learning powerful representations that\ncan be used for different clinical endpoints, all in a self-supervised way. To\nevaluate the performance of our model, we conducted classification experiments\non CAMELYON17 and CRC benchmark datasets, comparing favorably to other\nself-supervised and pre-trained strategies that are commonly used in digital\npathology. Our code is available at https://github.com/jcboyd/cdpath21-gan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Boyd_J/0/1/0/all/0/1\">Joseph Boyd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liashuha_M/0/1/0/all/0/1\">Mykola Liashuha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deutsch_E/0/1/0/all/0/1\">Eric Deutsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christodoulidis_S/0/1/0/all/0/1\">Stergios Christodoulidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Melatect: A Machine Learning Model Approach For Identifying Malignant Melanoma in Skin Growths. (arXiv:2109.03310v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03310","description":"<p>Malignant melanoma is a common skin cancer that is mostly curable before\nmetastasis, where melanoma growths spawn in organs away from the original site.\nMelanoma is the most dangerous type of skin cancer if left untreated due to the\nhigh chance of metastasis. This paper presents Melatect, a machine learning\nmodel that identifies potential malignant melanoma. A recursive computer image\nanalysis algorithm was used to create a machine learning model which is capable\nof detecting likely melanoma. The comparison is performed using 20,000 raw\nimages of benign and malignant lesions from the International Skin Imaging\nCollaboration (ISIC) archive that were augmented to 60,000 images. Tests of the\nalgorithm using subsets of the ISIC images suggest it accurately classifies\nlesions as malignant or benign over 95% of the time with no apparent bias or\noverfitting. The Melatect iOS app was later created (unpublished), in which the\nmachine learning model was embedded. With the app, users have the ability to\ntake pictures of skin lesions (moles) using the app, which are then processed\nthrough the machine learning model, and users are notified whether their lesion\ncould be abnormal or not. Melatect provides a convenient way to get free advice\non lesions and track these lesions over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meel_V/0/1/0/all/0/1\">Vidushi Meel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodepudi_A/0/1/0/all/0/1\">Asritha Bodepudi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstructing High-resolution Turbulent Flows Using Physics-Guided Neural Networks. (arXiv:2109.03327v1 [physics.flu-dyn])","link":"http://arxiv.org/abs/2109.03327","description":"<p>Direct numerical simulation (DNS) of turbulent flows is computationally\nexpensive and cannot be applied to flows with large Reynolds numbers. Large\neddy simulation (LES) is an alternative that is computationally less demanding,\nbut is unable to capture all of the scales of turbulent transport accurately.\nOur goal in this work is to build a new data-driven methodology based on\nsuper-resolution techniques to reconstruct DNS data from LES predictions. We\nleverage the underlying physical relationships to regularize the relationships\namongst different physical variables. We also introduce a hierarchical\ngenerative process and a reverse degradation process to fully explore the\ncorrespondence between DNS and LES data. We demonstrate the effectiveness of\nour method through a single-snapshot experiment and a cross-time experiment.\nThe results confirm that our method can better reconstruct high-resolution DNS\ndata over space and over time in terms of pixel-wise reconstruction error and\nstructural similarity. Visual comparisons show that our method performs much\nbetter in capturing fine-level flow dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Chen_S/0/1/0/all/0/1\">Shengyu Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sammak_S/0/1/0/all/0/1\">Shervin Sammak</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Givi_P/0/1/0/all/0/1\">Peyman Givi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yurko1_J/0/1/0/all/0/1\">Joseph P.Yurko1</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-World Adversarial Examples involving Makeup Application. (arXiv:2109.03329v1 [cs.CR])","link":"http://arxiv.org/abs/2109.03329","description":"<p>Deep neural networks have developed rapidly and have achieved outstanding\nperformance in several tasks, such as image classification and natural language\nprocessing. However, recent studies have indicated that both digital and\nphysical adversarial examples can fool neural networks. Face-recognition\nsystems are used in various applications that involve security threats from\nphysical adversarial examples. Herein, we propose a physical adversarial attack\nwith the use of full-face makeup. The presence of makeup on the human face is a\nreasonable possibility, which possibly increases the imperceptibility of\nattacks. In our attack framework, we combine the cycle-adversarial generative\nnetwork (cycle-GAN) and a victimized classifier. The Cycle-GAN is used to\ngenerate adversarial makeup, and the architecture of the victimized classifier\nis VGG 16. Our experimental results show that our attack can effectively\novercome manual errors in makeup application, such as color and\nposition-related errors. We also demonstrate that the approaches used to train\nthe models can influence physical attacks; the adversarial perturbations\ncrafted from the pre-trained model are affected by the corresponding training\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chang-Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chia-Yi Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chia-Mu Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Branch Deep Radial Basis Function Networks for Facial Emotion Recognition. (arXiv:2109.03336v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03336","description":"<p>Emotion recognition (ER) from facial images is one of the landmark tasks in\naffective computing with major developments in the last decade. Initial efforts\non ER relied on handcrafted features that were used to characterize facial\nimages and then feed to standard predictive models. Recent methodologies\ncomprise end-to-end trainable deep learning methods that simultaneously learn\nboth, features and predictive model. Perhaps the most successful models are\nbased on convolutional neural networks (CNNs). While these models have excelled\nat this task, they still fail at capturing local patterns that could emerge in\nthe learning process. We hypothesize these patterns could be captured by\nvariants based on locally weighted learning. Specifically, in this paper we\npropose a CNN based architecture enhanced with multiple branches formed by\nradial basis function (RBF) units that aims at exploiting local information at\nthe final stage of the learning process. Intuitively, these RBF units capture\nlocal patterns shared by similar instances using an intermediate\nrepresentation, then the outputs of the RBFs are feed to a softmax layer that\nexploits this information to improve the predictive performance of the model.\nThis feature could be particularly advantageous in ER as cultural / ethnicity\ndifferences may be identified by the local units. We evaluate the proposed\nmethod in several ER datasets and show the proposed methodology achieves\nstate-of-the-art in some of them, even when we adopt a pre-trained VGG-Face\nmodel as backbone. We show it is the incorporation of local information what\nmakes the proposed model competitive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Luquin_F/0/1/0/all/0/1\">Fernanda Hern&#xe1;ndez-Luquin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalante_H/0/1/0/all/0/1\">Hugo Jair Escalante</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Certifiable Outlier-Robust Geometric Perception: Exact Semidefinite Relaxations and Scalable Global Optimization. (arXiv:2109.03349v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03349","description":"<p>We propose the first general and scalable framework to design certifiable\nalgorithms for robust geometric perception in the presence of outliers. Our\nfirst contribution is to show that estimation using common robust costs, such\nas truncated least squares (TLS), maximum consensus, Geman-McClure, Tukey's\nbiweight, among others, can be reformulated as polynomial optimization problems\n(POPs). By focusing on the TLS cost, our second contribution is to exploit\nsparsity in the POP and propose a sparse semidefinite programming (SDP)\nrelaxation that is much smaller than the standard Lasserre's hierarchy while\npreserving exactness, i.e., the SDP recovers the optimizer of the nonconvex POP\nwith an optimality certificate. Our third contribution is to solve the SDP\nrelaxations at an unprecedented scale and accuracy by presenting STRIDE, a\nsolver that blends global descent on the convex SDP with fast local search on\nthe nonconvex POP. Our fourth contribution is an evaluation of the proposed\nframework on six geometric perception problems including single and multiple\nrotation averaging, point cloud and mesh registration, absolute pose\nestimation, and category-level object pose and shape estimation. Our\nexperiments demonstrate that (i) our sparse SDP relaxation is exact with up to\n60%-90% outliers across applications; (ii) while still being far from\nreal-time, STRIDE is up to 100 times faster than existing SDP solvers on\nmedium-scale problems, and is the only solver that can solve large-scale SDPs\nwith hundreds of thousands of constraints to high accuracy; (iii) STRIDE\nprovides a safeguard to existing fast heuristics for robust estimation (e.g.,\nRANSAC or Graduated Non-Convexity), i.e., it certifies global optimality if the\nheuristic estimates are optimal, or detects and allows escaping local optima\nwhen the heuristic estimates are suboptimal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing the objects of vision with neural networks. (arXiv:2109.03351v1 [q-bio.NC])","link":"http://arxiv.org/abs/2109.03351","description":"<p>Human visual perception carves a scene at its physical joints, decomposing\nthe world into objects, which are selectively attended, tracked, and predicted\nas we engage our surroundings. Object representations emancipate perception\nfrom the sensory input, enabling us to keep in mind that which is out of sight\nand to use perceptual content as a basis for action and symbolic cognition.\nHuman behavioral studies have documented how object representations emerge\nthrough grouping, amodal completion, proto-objects, and object files. Deep\nneural network (DNN) models of visual object recognition, by contrast, remain\nlargely tethered to the sensory input, despite achieving human-level\nperformance at labeling objects. Here, we review related work in both fields\nand examine how these fields can help each other. The cognitive literature\nprovides a starting point for the development of new experimental tasks that\nreveal mechanisms of human object perception and serve as benchmarks driving\ndevelopment of deep neural network models that will put the object into object\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Peters_B/0/1/0/all/0/1\">Benjamin Peters</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kriegeskorte_N/0/1/0/all/0/1\">Nikolaus Kriegeskorte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoadAtlas: Intelligent Platform for Automated Road Defect Detection and Asset Management. (arXiv:2109.03385v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03385","description":"<p>With the rapid development of intelligent detection algorithms based on deep\nlearning, much progress has been made in automatic road defect recognition and\nroad marking parsing. This can effectively address the issue of an expensive\nand time-consuming process for professional inspectors to review the street\nmanually. Towards this goal, we present RoadAtlas, a novel end-to-end\nintegrated system that can support 1) road defect detection, 2) road marking\nparsing, 3) a web-based dashboard for presenting and inputting data by users,\nand 4) a backend containing a well-structured database and developed APIs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuoxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jinjiang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Southon_A/0/1/0/all/0/1\">Anthony Southon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Discriminate Information for Online Action Detection: Analysis and Application. (arXiv:2109.03393v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03393","description":"<p>Online action detection, which aims to identify an ongoing action from a\nstreaming video, is an important subject in real-world applications. For this\ntask, previous methods use recurrent neural networks for modeling temporal\nrelations in an input sequence. However, these methods overlook the fact that\nthe input image sequence includes not only the action of interest but\nbackground and irrelevant actions. This would induce recurrent units to\naccumulate unnecessary information for encoding features on the action of\ninterest. To overcome this problem, we propose a novel recurrent unit, named\nInformation Discrimination Unit (IDU), which explicitly discriminates the\ninformation relevancy between an ongoing action and others to decide whether to\naccumulate the input information. This enables learning more discriminative\nrepresentations for identifying an ongoing action. In this paper, we further\npresent a new recurrent unit, called Information Integration Unit (IIU), for\naction anticipation. Our IIU exploits the outputs from IDU as pseudo action\nlabels as well as RGB frames to learn enriched features of observed actions\neffectively. In experiments on TVSeries and THUMOS-14, the proposed methods\noutperform state-of-the-art methods by a significant margin in online action\ndetection and action anticipation. Moreover, we demonstrate the effectiveness\nof the proposed units by conducting comprehensive ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sumin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eun_H/0/1/0/all/0/1\">Hyunjun Eun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jinyoung Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seokeon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonhyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1\">Chanho Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changick Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Master Face Attacks on Face Recognition Systems. (arXiv:2109.03398v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03398","description":"<p>Face authentication is now widely used, especially on mobile devices, rather\nthan authentication using a personal identification number or an unlock\npattern, due to its convenience. It has thus become a tempting target for\nattackers using a presentation attack. Traditional presentation attacks use\nfacial images or videos of the victim. Previous work has proven the existence\nof master faces, i.e., faces that match multiple enrolled templates in face\nrecognition systems, and their existence extends the ability of presentation\nattacks. In this paper, we perform an extensive study on latent variable\nevolution (LVE), a method commonly used to generate master faces. We run an LVE\nalgorithm for various scenarios and with more than one database and/or face\nrecognition system to study the properties of the master faces and to\nunderstand in which conditions strong master faces could be generated.\nMoreover, through analysis, we hypothesize that master faces come from some\ndense areas in the embedding spaces of the face recognition systems. Last but\nnot least, simulated presentation attacks using generated master faces\ngenerally preserve the false-matching ability of their original digital forms,\nthus demonstrating that the existence of master faces poses an actual threat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1\">S&#xe9;bastien Marcel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GTT-Net: Learned Generalized Trajectory Triangulation. (arXiv:2109.03408v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03408","description":"<p>We present GTT-Net, a supervised learning framework for the reconstruction of\nsparse dynamic 3D geometry. We build on a graph-theoretic formulation of the\ngeneralized trajectory triangulation problem, where non-concurrent multi-view\nimaging geometry is known but global image sequencing is not provided. GTT-Net\nlearns pairwise affinities modeling the spatio-temporal relationships among our\ninput observations and leverages them to determine 3D geometry estimates.\nExperiments reconstructing 3D motion-capture sequences show GTT-Net outperforms\nthe state of the art in terms of accuracy and robustness. Within the context of\narticulated motion reconstruction, our proposed architecture is 1) able to\nlearn and enforce semantic 3D motion priors for shared training and test\ndomains, while being 2) able to generalize its performance across different\ntraining and test domains. Moreover, GTT-Net provides a computationally\nstreamlined framework for trajectory triangulation with applications to\nmulti-instance reconstruction and event segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_E/0/1/0/all/0/1\">Enrique Dunn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YouRefIt: Embodied Reference Understanding with Language and Gesture. (arXiv:2109.03413v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03413","description":"<p>We study the understanding of embodied reference: One agent uses both\nlanguage and gesture to refer to an object to another agent in a shared\nphysical environment. Of note, this new visual task requires understanding\nmultimodal cues with perspective-taking to identify which object is being\nreferred to. To tackle this problem, we introduce YouRefIt, a new crowd-sourced\ndataset of embodied reference collected in various physical scenes; the dataset\ncontains 4,195 unique reference clips in 432 indoor scenes. To the best of our\nknowledge, this is the first embodied reference dataset that allows us to study\nreferring expressions in daily physical scenes to understand referential\nbehavior, human communication, and human-robot interaction. We further devise\ntwo benchmarks for image-based and video-based embodied reference\nunderstanding. Comprehensive baselines and extensive experiments provide the\nvery first result of machine perception on how the referring expressions and\ngestures affect the embodied reference understanding. Our results provide\nessential evidence that gestural cues are as critical as language cues in\nunderstanding the embodied reference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deqian Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kei_Y/0/1/0/all/0/1\">Yik Lun Kei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGB-D Salient Object Detection with Ubiquitous Target Awareness. (arXiv:2109.03425v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03425","description":"<p>Conventional RGB-D salient object detection methods aim to leverage depth as\ncomplementary information to find the salient regions in both modalities.\nHowever, the salient object detection results heavily rely on the quality of\ncaptured depth data which sometimes are unavailable. In this work, we make the\nfirst attempt to solve the RGB-D salient object detection problem with a novel\ndepth-awareness framework. This framework only relies on RGB data in the\ntesting phase, utilizing captured depth data as supervision for representation\nlearning. To construct our framework as well as achieving accurate salient\ndetection results, we propose a Ubiquitous Target Awareness (UTA) network to\nsolve three important challenges in RGB-D SOD task: 1) a depth awareness module\nto excavate depth information and to mine ambiguous regions via adaptive\ndepth-error weights, 2) a spatial-aware cross-modal interaction and a\nchannel-aware cross-level interaction, exploiting the low-level boundary cues\nand amplifying high-level salient channels, and 3) a gated multi-scale\npredictor module to perceive the object saliency in different contextual\nscales. Besides its high performance, our proposed UTA network is depth-free\nfor inference and runs in real-time with 43 FPS. Experimental evidence\ndemonstrates that our proposed network not only surpasses the state-of-the-art\nmethods on five public RGB-D SOD benchmarks by a large margin, but also\nverifies its extensibility on five public RGB SOD benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiawei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaowu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask is All You Need: Rethinking Mask R-CNN for Dense and Arbitrary-Shaped Scene Text Detection. (arXiv:2109.03426v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03426","description":"<p>Due to the large success in object detection and instance segmentation, Mask\nR-CNN attracts great attention and is widely adopted as a strong baseline for\narbitrary-shaped scene text detection and spotting. However, two issues remain\nto be settled. The first is dense text case, which is easy to be neglected but\nquite practical. There may exist multiple instances in one proposal, which\nmakes it difficult for the mask head to distinguish different instances and\ndegrades the performance. In this work, we argue that the performance\ndegradation results from the learning confusion issue in the mask head. We\npropose to use an MLP decoder instead of the \"deconv-conv\" decoder in the mask\nhead, which alleviates the issue and promotes robustness significantly. And we\npropose instance-aware mask learning in which the mask head learns to predict\nthe shape of the whole instance rather than classify each pixel to text or\nnon-text. With instance-aware mask learning, the mask branch can learn\nseparated and compact masks. The second is that due to large variations in\nscale and aspect ratio, RPN needs complicated anchor settings, making it hard\nto maintain and transfer across different datasets. To settle this issue, we\npropose an adaptive label assignment in which all instances especially those\nwith extreme aspect ratios are guaranteed to be associated with enough anchors.\nEquipped with these components, the proposed method named MAYOR achieves\nstate-of-the-art performance on five benchmarks including DAST1500, MSRA-TD500,\nICDAR2015, CTW1500, and Total-Text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xugong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Youhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dayan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhihong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Ning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongbin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSEGEP: Small SEGment Emphasized Performance evaluation metric for medical image segmentation. (arXiv:2109.03435v1 [eess.IV])","link":"http://arxiv.org/abs/2109.03435","description":"<p>Automatic image segmentation is a critical component of medical image\nanalysis, and hence quantifying segmentation performance is crucial. Challenges\nin medical image segmentation are mainly due to spatial variations of regions\nto be segmented and imbalance in distribution of classes. Commonly used metrics\ntreat all detected pixels, indiscriminately. However, pixels in smaller\nsegments must be treated differently from pixels in larger segments, as\ndetection of smaller ones aid in early treatment of associated disease and are\nalso easier to miss. To address this, we propose a novel evaluation metric for\nsegmentation performance, emphasizing smaller segments, by assigning higher\nweightage to smaller segment pixels. Weighted false positives are also\nconsidered in deriving the new metric named, \"SSEGEP\"(Small SEGment Emphasized\nPerformance evaluation metric), (range : 0(Bad) to 1(Good)). The experiments\nwere performed on diverse anatomies(eye, liver, pancreas and breast) from\npublicly available datasets to show applicability of the proposed metric across\ndifferent imaging techniques. Mean opinion score (MOS) and statistical\nsignificance testing is used to quantify the relevance of proposed approach.\nAcross 33 fundus images, where the largest exudate is 1.41%, and the smallest\nis 0.0002% of the image, the proposed metric is 30% closer to MOS, as compared\nto Dice Similarity Coefficient (DSC). Statistical significance testing resulted\nin promising p-value of order 10^{-18} with SSEGEP for hepatic tumor compared\nto DSC. The proposed metric is found to perform better for the images having\nmultiple segments for a single label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+R_A/0/1/0/all/0/1\">Ammu R</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinha_N/0/1/0/all/0/1\">Neelam Sinha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unfolding Taylor's Approximations for Image Restoration. (arXiv:2109.03442v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03442","description":"<p>Deep learning provides a new avenue for image restoration, which demands a\ndelicate balance between fine-grained details and high-level contextualized\ninformation during recovering the latent clear image. In practice, however,\nexisting methods empirically construct encapsulated end-to-end mapping networks\nwithout deepening into the rationality, and neglect the intrinsic prior\nknowledge of restoration task. To solve the above problems, inspired by\nTaylor's Approximations, we unfold Taylor's Formula to construct a novel\nframework for image restoration. We find the main part and the derivative part\nof Taylor's Approximations take the same effect as the two competing goals of\nhigh-level contextualized information and spatial details of image restoration\nrespectively. Specifically, our framework consists of two steps,\ncorrespondingly responsible for the mapping and derivative functions. The\nformer first learns the high-level contextualized information and the later\ncombines it with the degraded input to progressively recover local high-order\nspatial details. Our proposed framework is orthogonal to existing methods and\nthus can be easily integrated with them for further improvement, and extensive\nexperiments demonstrate the effectiveness and scalability of our proposed\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Man Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zeyu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xueyang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Real-World Super-Resolution via Adaptive Downsampling Models. (arXiv:2109.03444v1 [eess.IV])","link":"http://arxiv.org/abs/2109.03444","description":"<p>Most image super-resolution (SR) methods are developed on synthetic\nlow-resolution (LR) and high-resolution (HR) image pairs that are constructed\nby a predetermined operation, e.g., bicubic downsampling. As existing methods\ntypically learn an inverse mapping of the specific function, they produce\nblurry results when applied to real-world images whose exact formulation is\ndifferent and unknown. Therefore, several methods attempt to synthesize much\nmore diverse LR samples or learn a realistic downsampling model. However, due\nto restrictive assumptions on the downsampling process, they are still biased\nand less generalizable. This study proposes a novel method to simulate an\nunknown downsampling process without imposing restrictive prior knowledge. We\npropose a generalizable low-frequency loss (LFL) in the adversarial training\nframework to imitate the distribution of target LR images without using any\npaired examples. Furthermore, we design an adaptive data loss (ADL) for the\ndownsampler, which can be adaptively learned and updated from the data during\nthe training loops. Extensive experiments validate that our downsampling model\ncan facilitate existing SR methods to perform more accurate reconstructions on\nvarious synthetic and real-world examples than the conventional approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Son_S/0/1/0/all/0/1\">Sanghyun Son</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Jaeha Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_W/0/1/0/all/0/1\">Wei-Sheng Lai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Husan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which and Where to Focus: A Simple yet Accurate Framework for Arbitrary-Shaped Nearby Text Detection in Scene Images. (arXiv:2109.03451v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03451","description":"<p>Scene text detection has drawn the close attention of researchers. Though\nmany methods have been proposed for horizontal and oriented texts, previous\nmethods may not perform well when dealing with arbitrary-shaped texts such as\ncurved texts. In particular, confusion problem arises in the case of nearby\ntext instances. In this paper, we propose a simple yet effective method for\naccurate arbitrary-shaped nearby scene text detection. Firstly, a One-to-Many\nTraining Scheme (OMTS) is designed to eliminate confusion and enable the\nproposals to learn more appropriate groundtruths in the case of nearby text\ninstances. Secondly, we propose a Proposal Feature Attention Module (PFAM) to\nexploit more effective features for each proposal, which can better adapt to\narbitrary-shaped text instances. Finally, we propose a baseline that is based\non Faster R-CNN and outputs the curve representation directly. Equipped with\nPFAM and OMTS, the detector can achieve state-of-the-art or competitive\nperformance on several challenging benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Youhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xugong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recalibrating the KITTI Dataset Camera Setup for Improved Odometry Accuracy. (arXiv:2109.03462v1 [cs.RO])","link":"http://arxiv.org/abs/2109.03462","description":"<p>Over the last decade, one of the most relevant public datasets for evaluating\nodometry accuracy is the KITTI dataset. Beside the quality and rich sensor\nsetup, its success is also due to the online evaluation tool, which enables\nresearchers to benchmark and compare algorithms. The results are evaluated on\nthe test subset solely, without any knowledge about the ground truth, yielding\nunbiased, overfit free and therefore relevant validation for robot localization\nbased on cameras, 3D laser or combination of both. However, as any sensor\nsetup, it requires prior calibration and rectified stereo images are provided,\nintroducing dependence on the default calibration parameters. Given that, a\nnatural question arises if a better set of calibration parameters can be found\nthat would yield higher odometry accuracy. In this paper, we propose a new\napproach for one shot calibration of the KITTI dataset multiple camera setup.\nThe approach yields better calibration parameters, both in the sense of lower\ncalibration reprojection errors and lower visual odometry error. We conducted\nexperiments where we show for three different odometry algorithms, namely\nSOFT2, ORB-SLAM2 and VISO2, that odometry accuracy is significantly improved\nwith the proposed calibration parameters. Moreover, our odometry, SOFT2, in\nconjunction with the proposed calibration method achieved the highest accuracy\non the official KITTI scoreboard with 0.53% translational and 0.0009 deg/m\nrotational error, outperforming even 3D laser-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cvisic_I/0/1/0/all/0/1\">Igor Cvi&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markovic_I/0/1/0/all/0/1\">Ivan Markovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrovic_I/0/1/0/all/0/1\">Ivan Petrovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Level Set Binocular Stereo with Occlusions. (arXiv:2109.03464v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03464","description":"<p>Localizing stereo boundaries and predicting nearby disparities are difficult\nbecause stereo boundaries induce occluded regions where matching cues are\nabsent. Most modern computer vision algorithms treat occlusions secondarily\n(e.g., via left-right consistency checks after matching) or rely on high-level\ncues to improve nearby disparities (e.g., via deep networks and large training\nsets). They ignore the geometry of stereo occlusions, which dictates that the\nspatial extent of occlusion must equal the amplitude of the disparity jump that\ncauses it. This paper introduces an energy and level-set optimizer that\nimproves boundaries by encoding occlusion geometry. Our model applies to\ntwo-layer, figure-ground scenes, and it can be implemented cooperatively using\nmessages that pass predominantly between parents and children in an undecimated\nhierarchy of multi-scale image patches. In a small collection of figure-ground\nscenes curated from Middlebury and Falling Things stereo datasets, our model\nprovides more accurate boundaries than previous occlusion-handling stereo\ntechniques. This suggests new directions for creating cooperative stereo\nsystems that incorporate occlusion cues in a human-like manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zickler_T/0/1/0/all/0/1\">Todd Zickler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Site Severity Assessment of COVID-19 from CT Images via Domain Adaptation. (arXiv:2109.03478v1 [eess.IV])","link":"http://arxiv.org/abs/2109.03478","description":"<p>Early and accurate severity assessment of Coronavirus disease 2019 (COVID-19)\nbased on computed tomography (CT) images offers a great help to the estimation\nof intensive care unit event and the clinical decision of treatment planning.\nTo augment the labeled data and improve the generalization ability of the\nclassification model, it is necessary to aggregate data from multiple sites.\nThis task faces several challenges including class imbalance between mild and\nsevere infections, domain distribution discrepancy between sites, and presence\nof heterogeneous features. In this paper, we propose a novel domain adaptation\n(DA) method with two components to address these problems. The first component\nis a stochastic class-balanced boosting sampling strategy that overcomes the\nimbalanced learning problem and improves the classification performance on\npoorly-predicted classes. The second component is a representation learning\nthat guarantees three properties: 1) domain-transferability by prototype\ntriplet loss, 2) discriminant by conditional maximum mean discrepancy loss, and\n3) completeness by multi-view reconstruction loss. Particularly, we propose a\ndomain translator and align the heterogeneous data to the estimated class\nprototypes (i.e., class centers) in a hyper-sphere manifold. Experiments on\ncross-site severity assessment of COVID-19 from CT images show that the\nproposed method can effectively tackle the imbalanced learning problem and\noutperform recent DA approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_G/0/1/0/all/0/1\">Geng-Xin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_Z/0/1/0/all/0/1\">Zhongxiang Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_F/0/1/0/all/0/1\">Feng Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_M/0/1/0/all/0/1\">Man Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoming Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1\">Ying Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yaozong Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_C/0/1/0/all/0/1\">Chuan-Xian Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose-guided Inter- and Intra-part Relational Transformer for Occluded Person Re-Identification. (arXiv:2109.03483v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03483","description":"<p>Person Re-Identification (Re-Id) in occlusion scenarios is a challenging\nproblem because a pedestrian can be partially occluded. The use of local\ninformation for feature extraction and matching is still necessary. Therefore,\nwe propose a Pose-guided inter-and intra-part relational transformer (Pirt) for\noccluded person Re-Id, which builds part-aware long-term correlations by\nintroducing transformers. In our framework, we firstly develop a pose-guided\nfeature extraction module with regional grouping and mask construction for\nrobust feature representations. The positions of a pedestrian in the image\nunder surveillance scenarios are relatively fixed, hence we propose an\nintra-part and inter-part relational transformer. The intra-part module creates\nlocal relations with mask-guided features, while the inter-part relationship\nbuilds correlations with transformers, to develop cross relationships between\npart nodes. With the collaborative learning inter- and intra-part\nrelationships, experiments reveal that our proposed Pirt model achieves a new\nstate of the art on the public occluded dataset, and further extensions on\nstandard non-occluded person Re-Id datasets also reveal our comparable\nperformances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhongxing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shuffled Patch-Wise Supervision for Presentation Attack Detection. (arXiv:2109.03484v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03484","description":"<p>Face anti-spoofing is essential to prevent false facial verification by using\na photo, video, mask, or a different substitute for an authorized person's\nface. Most of the state-of-the-art presentation attack detection (PAD) systems\nsuffer from overfitting, where they achieve near-perfect scores on a single\ndataset but fail on a different dataset with more realistic data. This problem\ndrives researchers to develop models that perform well under real-world\nconditions. This is an especially challenging problem for frame-based\npresentation attack detection systems that use convolutional neural networks\n(CNN). To this end, we propose a new PAD approach, which combines pixel-wise\nbinary supervision with patch-based CNN. We believe that training a CNN with\nface patches allows the model to distinguish spoofs without learning background\nor dataset-specific traces. We tested the proposed method both on the standard\nbenchmark datasets -- Replay-Mobile, OULU-NPU -- and on a real-world dataset.\nThe proposed approach shows its superiority on challenging experimental setups.\nNamely, it achieves higher performance on OULU-NPU protocol 3, 4 and on\ninter-dataset real-world experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kantarci_A/0/1/0/all/0/1\">Alperen Kantarc&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dertli_H/0/1/0/all/0/1\">Hasan Dertli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Haz&#x131;m Kemal Ekenel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceCook: Face Generation Based on Linear Scaling Factors. (arXiv:2109.03492v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03492","description":"<p>With the excellent disentanglement properties of state-of-the-art generative\nmodels, image editing has been the dominant approach to control the attributes\nof synthesised face images. However, these edited results often suffer from\nartifacts or incorrect feature rendering, especially when there is a large\ndiscrepancy between the image to be edited and the desired feature set.\nTherefore, we propose a new approach to mapping the latent vectors of the\ngenerative model to the scaling factors through solving a set of multivariate\nlinear equations. The coefficients of the equations are the eigenvectors of the\nweight parameters of the pre-trained model, which form the basis of a hyper\ncoordinate system. The qualitative and quantitative results both show that the\nproposed method outperforms the baseline in terms of image diversity. In\naddition, the method is much more time-efficient because you can obtain\nsynthesised images with desirable features directly from the latent vectors,\nrather than the former process of editing randomly generated images requiring\nmany processing steps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Can Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Teng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovell_B/0/1/0/all/0/1\">Brian Lovell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal RoI Align for Video Object Recognition. (arXiv:2109.03495v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03495","description":"<p>Video object detection is challenging in the presence of appearance\ndeterioration in certain video frames. Therefore, it is a natural choice to\naggregate temporal information from other frames of the same video into the\ncurrent frame. However, RoI Align, as one of the most core procedures of video\ndetectors, still remains extracting features from a single-frame feature map\nfor proposals, making the extracted RoI features lack temporal information from\nvideos. In this work, considering the features of the same object instance are\nhighly similar among frames in a video, a novel Temporal RoI Align operator is\nproposed to extract features from other frames feature maps for current frame\nproposals by utilizing feature similarity. The proposed Temporal RoI Align\noperator can extract temporal information from the entire video for proposals.\nWe integrate it into single-frame video detectors and other state-of-the-art\nvideo detectors, and conduct quantitative experiments to demonstrate that the\nproposed Temporal RoI Align operator can consistently and significantly boost\nthe performance. Besides, the proposed Temporal RoI Align can also be applied\ninto video instance segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Huamin Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Elastic Significant Bit Quantization and Acceleration for Deep Neural Networks. (arXiv:2109.03513v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03513","description":"<p>Quantization has been proven to be a vital method for improving the inference\nefficiency of deep neural networks (DNNs). However, it is still challenging to\nstrike a good balance between accuracy and efficiency while quantizing DNN\nweights or activation values from high-precision formats to their quantized\ncounterparts. We propose a new method called elastic significant bit\nquantization (ESB) that controls the number of significant bits of quantized\nvalues to obtain better inference accuracy with fewer resources. We design a\nunified mathematical formula to constrain the quantized values of the ESB with\na flexible number of significant bits. We also introduce a distribution\ndifference aligner (DDA) to quantitatively align the distributions between the\nfull-precision weight or activation values and quantized values. Consequently,\nESB is suitable for various bell-shaped distributions of weights and activation\nof DNNs, thus maintaining a high inference accuracy. Benefitting from fewer\nsignificant bits of quantized values, ESB can reduce the multiplication\ncomplexity. We implement ESB as an accelerator and quantitatively evaluate its\nefficiency on FPGAs. Extensive experimental results illustrate that ESB\nquantization consistently outperforms state-of-the-art methods and achieves\naverage accuracy improvements of 4.78%, 1.92%, and 3.56% over AlexNet,\nResNet18, and MobileNetV2, respectively. Furthermore, ESB as an accelerator can\nachieve 10.95 GOPS peak performance of 1k LUTs without DSPs on the Xilinx\nZCU102 FPGA platform. Compared with CPU, GPU, and state-of-the-art accelerators\non FPGAs, the ESB accelerator can improve the energy efficiency by up to 65x,\n11x, and 26x, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Cheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Ye Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Kunpeng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zongming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion. (arXiv:2109.03551v1 [cs.SD])","link":"http://arxiv.org/abs/2109.03551","description":"<p>Voice conversion (VC) is an effective approach to electrolaryngeal (EL)\nspeech enhancement, a task that aims to improve the quality of the artificial\nvoice from an electrolarynx device. In frame-based VC methods, time alignment\nneeds to be performed prior to model training, and the dynamic time warping\n(DTW) algorithm is widely adopted to compute the best time alignment between\neach utterance pair. The validity is based on the assumption that the same\nphonemes of the speakers have similar features and can be mapped by measuring a\npre-defined distance between speech frames of the source and the target.\nHowever, the special characteristics of the EL speech can break the assumption,\nresulting in a sub-optimal DTW alignment. In this work, we propose to use lip\nimages for time alignment, as we assume that the lip movements of laryngectomee\nremain normal compared to healthy people. We investigate two naive lip\nrepresentations and distance metrics, and experimental results demonstrate that\nthe proposed method can significantly outperform the audio-only alignment in\nterms of objective and subjective evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liou_Y/0/1/0/all/0/1\">Yi-Syuan Liou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_M/0/1/0/all/0/1\">Ming-Chi Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1\">Shu-Wei Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yu-Huai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR. (arXiv:2109.03569v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03569","description":"<p>Vision-based depth estimation is a key feature in autonomous systems, which\noften relies on a single camera or several independent ones. In such a\nmonocular setup, dense depth is obtained with either additional input from one\nor several expensive LiDARs, e.g., with 64 beams, or camera-only methods, which\nsuffer from scale-ambiguity and infinite-depth problems. In this paper, we\npropose a new alternative of densely estimating metric depth by combining a\nmonocular camera with a light-weight LiDAR, e.g., with 4 beams, typical of\ntoday's automotive-grade mass-produced laser scanners. Inspired by recent\nself-supervised methods, we introduce a novel framework, called LiDARTouch, to\nestimate dense depth maps from monocular images with the help of ``touches'' of\nLiDAR, i.e., without the need for dense ground-truth depth. In our setup, the\nminimal LiDAR input contributes on three different levels: as an additional\nmodel's input, in a self-supervised LiDAR reconstruction objective function,\nand to estimate changes of pose (a key component of self-supervised depth\nestimation architectures). Our LiDARTouch framework achieves new state of the\nart in self-supervised depth estimation on the KITTI dataset, thus supporting\nour choices of integrating the very sparse LiDAR signal with other visual\nfeatures. Moreover, we show that the use of a few-beam LiDAR alleviates scale\nambiguity and infinite-depth issues that camera-only methods suffer from. We\nalso demonstrate that methods from the fully-supervised depth-completion\nliterature can be adapted to a self-supervised regime with a minimal LiDAR\nsignal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartoccioni_F/0/1/0/all/0/1\">Florent Bartoccioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zablocki_E/0/1/0/all/0/1\">&#xc9;loi Zablocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deriving Explanation of Deep Visual Saliency Models. (arXiv:2109.03575v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03575","description":"<p>Deep neural networks have shown their profound impact on achieving human\nlevel performance in visual saliency prediction. However, it is still unclear\nhow they learn the task and what it means in terms of understanding human\nvisual system. In this work, we develop a technique to derive explainable\nsaliency models from their corresponding deep neural architecture based\nsaliency models by applying human perception theories and the conventional\nconcepts of saliency. This technique helps us understand the learning pattern\nof the deep network at its intermediate layers through their activation maps.\nInitially, we consider two state-of-the-art deep saliency models, namely UNISAL\nand MSI-Net for our interpretation. We use a set of biologically plausible\nlog-gabor filters for identifying and reconstructing the activation maps of\nthem using our explainable saliency model. The final saliency map is generated\nusing these reconstructed activation maps. We also build our own deep saliency\nmodel named cross-concatenated multi-scale residual block based network\n(CMRNet) for saliency prediction. Then, we evaluate and compare the performance\nof the explainable models derived from UNISAL, MSI-Net and CMRNet on three\nbenchmark datasets with other state-of-the-art methods. Hence, we propose that\nthis approach of explainability can be applied to any deep visual saliency\nmodel for interpretation which makes it a generic one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malladi_S/0/1/0/all/0/1\">Sai Phani Kumar Malladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_J/0/1/0/all/0/1\">Jayanta Mukhopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larabi_C/0/1/0/all/0/1\">Chaker Larabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhury_S/0/1/0/all/0/1\">Santanu Chaudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching in the Dark: A Dataset for Matching Image Pairs of Low-light Scenes. (arXiv:2109.03585v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03585","description":"<p>This paper considers matching images of low-light scenes, aiming to widen the\nfrontier of SfM and visual SLAM applications. Recent image sensors can record\nthe brightness of scenes with more than eight-bit precision, available in their\nRAW-format image. We are interested in making full use of such high-precision\ninformation to match extremely low-light scene images that conventional methods\ncannot handle. For extreme low-light scenes, even if some of their brightness\ninformation exists in the RAW format images' low bits, the standard raw image\nprocessing on cameras fails to utilize them properly. As was recently shown by\nChen et al., CNNs can learn to produce images with a natural appearance from\nsuch RAW-format images. To consider if and how well we can utilize such\ninformation stored in RAW-format images for image matching, we have created a\nnew dataset named MID (matching in the dark). Using it, we experimentally\nevaluated combinations of eight image-enhancing methods and eleven image\nmatching methods consisting of classical/neural local descriptors and\nclassical/neural initial point-matching methods. The results show the advantage\nof using the RAW-format images and the strengths and weaknesses of the above\ncomponent methods. They also imply there is room for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">W. Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1\">M. Suganuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">X. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimobayashi_N/0/1/0/all/0/1\">N. Shimobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maruta_D/0/1/0/all/0/1\">D. Maruta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">T. Okatani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification of Social-Media Platform of Videos through the Use of Shared Features. (arXiv:2109.03598v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03598","description":"<p>Videos have become a powerful tool for spreading illegal content such as\nmilitary propaganda, revenge porn, or bullying through social networks. To\ncounter these illegal activities, it has become essential to try new methods to\nverify the origin of videos from these platforms. However, collecting datasets\nlarge enough to train neural networks for this task has become difficult\nbecause of the privacy regulations that have been enacted in recent years. To\nmitigate this limitation, in this work we propose two different solutions based\non transfer learning and multitask learning to determine whether a video has\nbeen uploaded from or downloaded to a specific social platform through the use\nof shared features with images trained on the same task. By transferring\nfeatures from the shallowest to the deepest levels of the network from the\nimage task to videos, we measure the amount of information shared between these\ntwo tasks. Then, we introduce a model based on multitask learning, which learns\nfrom both tasks simultaneously. The promising experimental results show, in\nparticular, the effectiveness of the multitask approach. According to our\nknowledge, this is the first work that addresses the problem of social media\nplatform identification of videos through the use of shared features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maiano_L/0/1/0/all/0/1\">Luca Maiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amerini_I/0/1/0/all/0/1\">Irene Amerini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celsi_L/0/1/0/all/0/1\">Lorenzo Ricciardi Celsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anagnostopoulos_A/0/1/0/all/0/1\">Aris Anagnostopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tactile Image-to-Image Disentanglement of Contact Geometry from Motion-Induced Shear. (arXiv:2109.03615v1 [cs.RO])","link":"http://arxiv.org/abs/2109.03615","description":"<p>Robotic touch, particularly when using soft optical tactile sensors, suffers\nfrom distortion caused by motion-dependent shear. The manner in which the\nsensor contacts a stimulus is entangled with the tactile information about the\ngeometry of the stimulus. In this work, we propose a supervised convolutional\ndeep neural network model that learns to disentangle, in the latent space, the\ncomponents of sensor deformations caused by contact geometry from those due to\nsliding-induced shear. The approach is validated by reconstructing unsheared\ntactile images from sheared images and showing they match unsheared tactile\nimages collected with no sliding motion. In addition, the unsheared tactile\nimages give a faithful reconstruction of the contact geometry that is not\npossible from the sheared data, and robust estimation of the contact pose that\ncan be used for servo control sliding around various 2D shapes. Finally, the\ncontact geometry reconstruction in conjunction with servo control sliding were\nused for faithful full object reconstruction of various 2D shapes. The methods\nhave broad applicability to deep learning models for robots with a\nshear-sensitive sense of touch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anupam K. Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aitchison_L/0/1/0/all/0/1\">Laurence Aitchison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepora_N/0/1/0/all/0/1\">Nathan F. Lepora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Local-Global Contextual Adaptation for Fully End-to-End Bottom-Up Human Pose Estimation. (arXiv:2109.03622v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03622","description":"<p>This paper presents a method of learning Local-GlObal Contextual Adaptation\nfor fully end-to-end and fast bottom-up human Pose estimation, dubbed as\nLOGO-CAP. It is built on the conceptually simple center-offset formulation that\nlacks inaccuracy for pose estimation. When revisiting the bottom-up human pose\nestimation with the thought of \"thinking, fast and slow\" by D. Kahneman, we\nintroduce a \"slow keypointer\" to remedy the lack of sufficient accuracy of the\n\"fast keypointer\". In learning the \"slow keypointer\", the proposed LOGO-CAP\nlifts the initial \"fast\" keypoints by offset predictions to keypoint expansion\nmaps (KEMs) to counter their uncertainty in two modules. Firstly, the local\nKEMs (e.g., 11x11) are extracted from a low-dimensional feature map. A proposed\nconvolutional message passing module learns to \"re-focus\" the local KEMs to the\nkeypoint attraction maps (KAMs) by accounting for the structured output\nprediction nature of human pose estimation, which is directly supervised by the\nobject keypoint similarity (OKS) loss in training. Secondly, the global KEMs\nare extracted, with a sufficiently large region-of-interest (e.g., 97x97), from\nthe keypoint heatmaps that are computed by a direct map-to-map regression.\nThen, a local-global contextual adaptation module is proposed to convolve the\nglobal KEMs using the learned KAMs as the kernels. This convolution can be\nunderstood as the learnable offsets guided deformable and dynamic convolution\nin a pose-sensitive way. The proposed method is end-to-end trainable with near\nreal-time inference speed, obtaining state-of-the-art performance on the COCO\nkeypoint benchmark for bottom-up human pose estimation. With the COCO trained\nmodel, our LOGO-CAP also outperforms prior arts by a large margin on the\nchallenging OCHuman dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Recognizing Occluded Faces in the Wild. (arXiv:2109.03672v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03672","description":"<p>Facial appearance variations due to occlusion has been one of the main\nchallenges for face recognition systems. To facilitate further research in this\narea, it is necessary and important to have occluded face datasets collected\nfrom real-world, as synthetically generated occluded faces cannot represent the\nnature of the problem. In this paper, we present the Real World Occluded Faces\n(ROF) dataset, that contains faces with both upper face occlusion, due to\nsunglasses, and lower face occlusion, due to masks. We propose two evaluation\nprotocols for this dataset. Benchmark experiments on the dataset have shown\nthat no matter how powerful the deep face representation models are, their\nperformance degrades significantly when they are tested on real-world occluded\nfaces. It is observed that the performance drop is far less when the models are\ntested on synthetically generated occluded faces. The ROF dataset and the\nassociated evaluation protocols are publicly available at the following link\nhttps://github.com/ekremerakin/RealWorldOccludedFaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Erakin_M/0/1/0/all/0/1\">Mustafa Ekrem Erak&#x131;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1\">U&#x11f;ur Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Haz&#x131;m Kemal Ekenel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised clothing change adaptive person ReID. (arXiv:2109.03702v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03702","description":"<p>Clothing changes and lack of data labels are both crucial challenges in\nperson ReID. For the former challenge, people may occur multiple times at\ndifferent locations wearing different clothing. However, most of the current\nperson ReID research works focus on the benchmarks in which a person's clothing\nis kept the same all the time. For the last challenge, some researchers try to\nmake model learn information from a labeled dataset as a source to an unlabeled\ndataset. Whereas purely unsupervised training is less used. In this paper, we\naim to solve both problems at the same time. We design a novel unsupervised\nmodel, Sync-Person-Cloud ReID, to solve the unsupervised clothing change person\nReID problem. We developer a purely unsupervised clothing change person ReID\npipeline with person sync augmentation operation and same person feature\nrestriction. The person sync augmentation is to supply additional same person\nresources. These same person's resources can be used as part supervised input\nby same person feature restriction. The extensive experiments on clothing\nchange ReID datasets show the out-performance of our methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Congzhentao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Richard YiDa Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Alzheimer's disease neurodegeneration from typical brain aging using machine learning. (arXiv:2109.03723v1 [cs.LG])","link":"http://arxiv.org/abs/2109.03723","description":"<p>Neuroimaging biomarkers that distinguish between typical brain aging and\nAlzheimer's disease (AD) are valuable for determining how much each contributes\nto cognitive decline. Machine learning models can derive multi-variate brain\nchange patterns related to the two processes, including the SPARE-AD (Spatial\nPatterns of Atrophy for Recognition of Alzheimer's Disease) and SPARE-BA (of\nBrain Aging) investigated herein. However, substantial overlap between brain\nregions affected in the two processes confounds measuring them independently.\nWe present a methodology toward disentangling the two. T1-weighted MRI images\nof 4,054 participants (48-95 years) with AD, mild cognitive impairment (MCI),\nor cognitively normal (CN) diagnoses from the iSTAGING (Imaging-based\ncoordinate SysTem for AGIng and NeurodeGenerative diseases) consortium were\nanalyzed. First, a subset of AD patients and CN adults were selected based\npurely on clinical diagnoses to train SPARE-BA1 (regression of age using CN\nindividuals) and SPARE-AD1 (classification of CN versus AD). Second, analogous\ngroups were selected based on clinical and molecular markers to train SPARE-BA2\nand SPARE-AD2: amyloid-positive (A+) AD continuum group (consisting of A+AD,\nA+MCI, and A+ and tau-positive CN individuals) and amyloid-negative (A-) CN\ngroup. Finally, the combined group of the AD continuum and A-/CN individuals\nwas used to train SPARE-BA3, with the intention to estimate brain age\nregardless of AD-related brain changes. Disentangled SPARE models derived brain\npatterns that were more specific to the two types of the brain changes.\nCorrelation between the SPARE-BA and SPARE-AD was significantly reduced.\nCorrelation of disentangled SPARE-AD was non-inferior to the molecular\nmeasurements and to the number of APOE4 alleles, but was less to AD-related\npsychometric test scores, suggesting contribution of advanced brain aging to\nthese scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1\">Gyujoon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulkadir_A/0/1/0/all/0/1\">Ahmed Abdulkadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erus_G/0/1/0/all/0/1\">Guray Erus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habes_M/0/1/0/all/0/1\">Mohamad Habes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomponio_R/0/1/0/all/0/1\">Raymond Pomponio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_H/0/1/0/all/0/1\">Haochang Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_J/0/1/0/all/0/1\">Jimit Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamourian_E/0/1/0/all/0/1\">Elizabeth Mamourian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1\">Tanweer Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilgel_M/0/1/0/all/0/1\">Murat Bilgel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotiras_A/0/1/0/all/0/1\">Aristeidis Sotiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_D/0/1/0/all/0/1\">Dhivya Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_J/0/1/0/all/0/1\">John C. Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_D/0/1/0/all/0/1\">Daniel Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albert_M/0/1/0/all/0/1\">Marilyn S. Albert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryan_N/0/1/0/all/0/1\">Nick R. Bryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resnick_S/0/1/0/all/0/1\">Susan M. Resnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrallah_I/0/1/0/all/0/1\">Ilya M. Nasrallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davatzikos_C/0/1/0/all/0/1\">Christos Davatzikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolk_D/0/1/0/all/0/1\">David A. Wolk</a> (from the iSTAGING consortium, for the ADNI)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Axial multi-layer perceptron architecture for automatic segmentation of choroid plexus in multiple sclerosis. (arXiv:2109.03778v1 [eess.IV])","link":"http://arxiv.org/abs/2109.03778","description":"<p>Choroid plexuses (CP) are structures of the ventricles of the brain which\nproduce most of the cerebrospinal fluid (CSF). Several postmortem and in vivo\nstudies have pointed towards their role in the inflammatory process in multiple\nsclerosis (MS). Automatic segmentation of CP from MRI thus has high value for\nstudying their characteristics in large cohorts of patients. To the best of our\nknowledge, the only freely available tool for CP segmentation is FreeSurfer but\nits accuracy for this specific structure is poor. In this paper, we propose to\nautomatically segment CP from non-contrast enhanced T1-weighted MRI. To that\nend, we introduce a new model called \"Axial-MLP\" based on an assembly of Axial\nmulti-layer perceptrons (MLPs). This is inspired by recent works which showed\nthat the self-attention layers of Transformers can be replaced with MLPs. This\napproach is systematically compared with a standard 3D U-Net, nnU-Net,\nFreesurfer and FastSurfer. For our experiments, we make use of a dataset of 141\nsubjects (44 controls and 97 patients with MS). We show that all the tested\ndeep learning (DL) methods outperform FreeSurfer (Dice around 0.7 for DL vs\n0.33 for FreeSurfer). Axial-MLP is competitive with U-Nets even though it is\nslightly less accurate. The conclusions of our paper are two-fold: 1) the\nstudied deep learning methods could be useful tools to study CP in large\ncohorts of MS patients; 2)~Axial-MLP is a potentially viable alternative to\nconvolutional neural networks for such tasks, although it could benefit from\nfurther improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_Mengin_M/0/1/0/all/0/1\">Marius Schmidt-Mengin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ricigliano_V/0/1/0/all/0/1\">Vito A.G. Ricigliano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bodini_B/0/1/0/all/0/1\">Benedetta Bodini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morena_E/0/1/0/all/0/1\">Emanuele Morena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colombi_A/0/1/0/all/0/1\">Annalisa Colombi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamzaoui_M/0/1/0/all/0/1\">Mariem Hamzaoui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Panah_A/0/1/0/all/0/1\">Arya Yazdan Panah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stankoff_B/0/1/0/all/0/1\">Bruno Stankoff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colliot_O/0/1/0/all/0/1\">Olivier Colliot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Egocentric View Hand Action Recognition by Leveraging Hand Surface and Hand Grasp Type. (arXiv:2109.03783v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03783","description":"<p>We introduce a multi-stage framework that uses mean curvature on a hand\nsurface and focuses on learning interaction between hand and object by\nanalyzing hand grasp type for hand action recognition in egocentric videos. The\nproposed method does not require 3D information of objects including 6D object\nposes which are difficult to annotate for learning an object's behavior while\nit interacts with hands. Instead, the framework synthesizes the mean curvature\nof the hand mesh model to encode the hand surface geometry in 3D space.\nAdditionally, our method learns the hand grasp type which is highly correlated\nwith the hand action. From our experiment, we notice that using hand grasp type\nand mean curvature of hand increases the performance of the hand action\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangpil Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1\">Jihyun Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1\">Hyunggun Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sunghee Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_B/0/1/0/all/0/1\">Byoung Soo Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramani_K/0/1/0/all/0/1\">Karthik Ramani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIDNet: LiDAR Point Cloud Semantic Segmentation with Fully Interpolation Decoding. (arXiv:2109.03787v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03787","description":"<p>Projecting the point cloud on the 2D spherical range image transforms the\nLiDAR semantic segmentation to a 2D segmentation task on the range image.\nHowever, the LiDAR range image is still naturally different from the regular 2D\nRGB image; for example, each position on the range image encodes the unique\ngeometry information. In this paper, we propose a new projection-based LiDAR\nsemantic segmentation pipeline that consists of a novel network structure and\nan efficient post-processing step. In our network structure, we design a FID\n(fully interpolation decoding) module that directly upsamples the\nmulti-resolution feature maps using bilinear interpolation. Inspired by the 3D\ndistance interpolation used in PointNet++, we argue this FID module is a 2D\nversion distance interpolation on $(\\theta, \\phi)$ space. As a parameter-free\ndecoding module, the FID largely reduces the model complexity by maintaining\ngood performance. Besides the network structure, we empirically find that our\nmodel predictions have clear boundaries between different semantic classes.\nThis makes us rethink whether the widely used K-nearest-neighbor\npost-processing is still necessary for our pipeline. Then, we realize the\nmany-to-one mapping causes the blurring effect that some points are mapped into\nthe same pixel and share the same label. Therefore, we propose to process those\noccluded points by assigning the nearest predicted label to them. This NLA\n(nearest label assignment) post-processing step shows a better performance than\nKNN with faster inference speed in the ablation study. On the SemanticKITTI\ndataset, our pipeline achieves the best performance among all projection-based\nmethods with $64 \\times 2048$ resolution and all point-wise solutions. With a\nResNet-34 as the backbone, both the training and testing of our model can be\nfinished on a single RTX 2080 Ti with 11G memory. The code is released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Few-Shot Learning PoC Ultrasound COVID-19 Diagnostic System. (arXiv:2109.03793v1 [eess.IV])","link":"http://arxiv.org/abs/2109.03793","description":"<p>This paper presents a novel ultrasound imaging point-of-care (PoC) COVID-19\ndiagnostic system. The adaptive visual diagnostics utilize few-shot learning\n(FSL) to generate encoded disease state models that are stored and classified\nusing a dictionary of knowns. The novel vocabulary based feature processing of\nthe pipeline adapts the knowledge of a pretrained deep neural network to\ncompress the ultrasound images into discrimative descriptions. The\ncomputational efficiency of the FSL approach enables high diagnostic deep\nlearning performance in PoC settings, where training data is limited and the\nannotation process is not strictly controlled. The algorithm performance is\nevaluated on the open source COVID-19 POCUS Dataset to validate the system's\nability to distinguish COVID-19, pneumonia, and healthy disease states. The\nresults of the empirical analyses demonstrate the appropriate efficiency and\naccuracy for scalable PoC use. The code for this work will be made publicly\navailable on GitHub upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Karnes_M/0/1/0/all/0/1\">Michael Karnes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Perera_S/0/1/0/all/0/1\">Shehan Perera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adhikari_S/0/1/0/all/0/1\">Srikar Adhikari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digitize-PID: Automatic Digitization of Piping and Instrumentation Diagrams. (arXiv:2109.03794v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03794","description":"<p>Digitization of scanned Piping and Instrumentation diagrams(P&amp;ID), widely\nused in manufacturing or mechanical industries such as oil and gas over several\ndecades, has become a critical bottleneck in dynamic inventory management and\ncreation of smart P&amp;IDs that are compatible with the latest CAD tools.\nHistorically, P&amp;ID sheets have been manually generated at the design stage,\nbefore being scanned and stored as PDFs. Current digitization initiatives\ninvolve manual processing and are consequently very time consuming, labour\nintensive and error-prone.Thanks to advances in image processing, machine and\ndeep learning techniques there are emerging works on P&amp;ID digitization.\nHowever, existing solutions face several challenges owing to the variation in\nthe scale, size and noise in the P&amp;IDs, sheer complexity and crowdedness within\ndrawings, domain knowledge required to interpret the drawings. This motivates\nour current solution called Digitize-PID which comprises of an end-to-end\npipeline for detection of core components from P&amp;IDs like pipes, symbols and\ntextual information, followed by their association with each other and\neventually, the validation and correction of output data based on inherent\ndomain knowledge. A novel and efficient kernel-based line detection and a\ntwo-step method for detection of complex symbols based on a fine-grained deep\nrecognition technique is presented in the paper. In addition, we have created\nan annotated synthetic dataset, Dataset-P&amp;ID, of 500 P&amp;IDs by incorporating\ndifferent types of noise and complex symbols which is made available for public\nuse (currently there exists no public P&amp;ID dataset). We evaluate our proposed\nmethod on this synthetic dataset and a real-world anonymized private dataset of\n12 P&amp;ID sheets. Results show that Digitize-PID outperforms the existing\nstate-of-the-art for P&amp;ID digitization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paliwal_S/0/1/0/all/0/1\">Shubham Paliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Arushi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Monika Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_L/0/1/0/all/0/1\">Lovekesh Vig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking. (arXiv:2109.03805v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03805","description":"<p>Panoptic scene understanding and tracking of dynamic agents are essential for\nrobots and automated vehicles to navigate in urban environments. As LiDARs\nprovide accurate illumination-independent geometric depictions of the scene,\nperforming these tasks using LiDAR point clouds provides reliable predictions.\nHowever, existing datasets lack diversity in the type of urban scenes and have\na limited number of dynamic object instances which hinders both learning of\nthese tasks as well as credible benchmarking of the developed methods. In this\npaper, we introduce the large-scale Panoptic nuScenes benchmark dataset that\nextends our popular nuScenes dataset with point-wise groundtruth annotations\nfor semantic segmentation, panoptic segmentation, and panoptic tracking tasks.\nTo facilitate comparison, we provide several strong baselines for each of these\ntasks on our proposed dataset. Moreover, we analyze the drawbacks of the\nexisting metrics for the panoptic tracking problem and propose a novel\ninstance-centric metric that addresses the concerns. We present extensive\nexperiments that demonstrate the utility of Panoptic nuScenes compared to\nexisting datasets and make the online evaluation server available at\n\\url{nuScenes.org}. We believe that this extension will accelerate the research\nof novel methods for scene understanding of dynamic urban environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1\">Whye Kit Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_R/0/1/0/all/0/1\">Rohit Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1\">Juana Valeria Hurtado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lubing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1\">Holger Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaled ReLU Matters for Training Vision Transformers. (arXiv:2109.03810v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03810","description":"<p>Vision transformers (ViTs) have been an alternative design paradigm to\nconvolutional neural networks (CNNs). However, the training of ViTs is much\nharder than CNNs, as it is sensitive to the training parameters, such as\nlearning rate, optimizer and warmup epoch. The reasons for training difficulty\nare empirically analysed in ~\\cite{xiao2021early}, and the authors conjecture\nthat the issue lies with the \\textit{patchify-stem} of ViT models and propose\nthat early convolutions help transformers see better. In this paper, we further\ninvestigate this problem and extend the above conclusion: only early\nconvolutions do not help for stable training, but the scaled ReLU operation in\nthe \\textit{convolutional stem} (\\textit{conv-stem}) matters. We verify, both\ntheoretically and empirically, that scaled ReLU in \\textit{conv-stem} not only\nimproves training stabilization, but also increases the diversity of patch\ntokens, thus boosting peak performance with a large margin via adding few\nparameters and flops. In addition, extensive experiments are conducted to\ndemonstrate that previous ViTs are far from being well trained, further showing\nthat ViTs have great potential to be a better substitute of CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingkai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"fastMRI+: Clinical Pathology Annotations for Knee and Brain Fully Sampled Multi-Coil MRI Data. (arXiv:2109.03812v1 [eess.IV])","link":"http://arxiv.org/abs/2109.03812","description":"<p>Improving speed and image quality of Magnetic Resonance Imaging (MRI) via\nnovel reconstruction approaches remains one of the highest impact applications\nfor deep learning in medical imaging. The fastMRI dataset, unique in that it\ncontains large volumes of raw MRI data, has enabled significant advances in\naccelerating MRI using deep learning-based reconstruction methods. While the\nimpact of the fastMRI dataset on the field of medical imaging is unquestioned,\nthe dataset currently lacks clinical expert pathology annotations, critical to\naddressing clinically relevant reconstruction frameworks and exploring\nimportant questions regarding rendering of specific pathology using such novel\napproaches. This work introduces fastMRI+, which consists of 16154\nsubspecialist expert bounding box annotations and 13 study-level labels for 22\ndifferent pathology categories on the fastMRI knee dataset, and 7570\nsubspecialist expert bounding box annotations and 643 study-level labels for 30\ndifferent pathology categories for the fastMRI brain dataset. The fastMRI+\ndataset is open access and aims to support further research and advancement of\nmedical imaging in MRI reconstruction and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_R/0/1/0/all/0/1\">Ruiyang Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaman_B/0/1/0/all/0/1\">Burhaneddin Yaman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stewart_R/0/1/0/all/0/1\">Russell Stewart</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dixon_A/0/1/0/all/0/1\">Austin Dixon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knoll_F/0/1/0/all/0/1\">Florian Knoll</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengnan Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lui_Y/0/1/0/all/0/1\">Yvonne W. Lui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_M/0/1/0/all/0/1\">Michael S. Hansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic SegFormer. (arXiv:2109.03814v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03814","description":"<p>We present Panoptic SegFormer, a general framework for end-to-end panoptic\nsegmentation with Transformers. The proposed method extends Deformable DETR\nwith a unified mask prediction workflow for both things and stuff, making the\npanoptic segmentation pipeline concise and effective. With a ResNet-50\nbackbone, our method achieves 50.0\\% PQ on the COCO test-dev split, surpassing\nprevious state-of-the-art methods by significant margins without bells and\nwhistles. Using a more powerful PVTv2-B5 backbone, Panoptic-SegFormer achieves\na new record of 54.1\\%PQ and 54.4\\% PQ on the COCO val and test-dev splits with\nsingle scale input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptive Ensemble Learning. (arXiv:2003.07325v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.07325","description":"<p>The problem of generalizing deep neural networks from multiple source domains\nto a target one is studied under two settings: When unlabeled target data is\navailable, it is a multi-source unsupervised domain adaptation (UDA) problem,\notherwise a domain generalization (DG) problem. We propose a unified framework\ntermed domain adaptive ensemble learning (DAEL) to address both problems. A\nDAEL model is composed of a CNN feature extractor shared across domains and\nmultiple classifier heads each trained to specialize in a particular source\ndomain. Each such classifier is an expert to its own domain and a non-expert to\nothers. DAEL aims to learn these experts collaboratively so that when forming\nan ensemble, they can leverage complementary information from each other to be\nmore effective for an unseen target domain. To this end, each source domain is\nused in turn as a pseudo-target-domain with its own expert providing\nsupervisory signal to the ensemble of non-experts learned from the other\nsources. For unlabeled target data under the UDA setting where real expert does\nnot exist, DAEL uses pseudo-label to supervise the ensemble learning. Extensive\nexperiments on three multi-source UDA datasets and two DG datasets show that\nDAEL improves the state of the art on both problems, often by significant\nmargins. The code is released at\n\\url{https://github.com/KaiyangZhou/Dassl.pytorch}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quasi-Dense Similarity Learning for Multiple Object Tracking. (arXiv:2006.06664v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.06664","description":"<p>Similarity learning has been recognized as a crucial step for object\ntracking. However, existing multiple object tracking methods only use sparse\nground truth matching as the training objective, while ignoring the majority of\nthe informative regions on the images. In this paper, we present Quasi-Dense\nSimilarity Learning, which densely samples hundreds of region proposals on a\npair of images for contrastive learning. We can directly combine this\nsimilarity learning with existing detection methods to build Quasi-Dense\nTracking (QDTrack) without turning to displacement regression or motion priors.\nWe also find that the resulting distinctive feature space admits a simple\nnearest neighbor search at the inference time. Despite its simplicity, QDTrack\noutperforms all existing methods on MOT, BDD100K, Waymo, and TAO tracking\nbenchmarks. It achieves 68.7 MOTA at 20.3 FPS on MOT17 without using external\ntraining data. Compared to methods with similar detectors, it boosts almost 10\npoints of MOTA and significantly decreases the number of ID switches on BDD100K\nand Waymo datasets. Our code and trained models are available at\n<a href=\"http://vis.xyz/pub/qdtrack.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Linlu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haofeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self supervised contrastive learning for digital histopathology. (arXiv:2011.13971v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2011.13971","description":"<p>Unsupervised learning has been a long-standing goal of machine learning and\nis especially important for medical image analysis, where the learning can\ncompensate for the scarcity of labeled datasets. A promising subclass of\nunsupervised learning is self-supervised learning, which aims to learn salient\nfeatures using the raw input as the learning signal. In this paper, we use a\ncontrastive self-supervised learning method called SimCLR that achieved\nstate-of-the-art results on natural-scene images and apply this method to\ndigital histopathology by collecting and pretraining on 57 histopathology\ndatasets without any labels. We find that combining multiple multi-organ\ndatasets with different types of staining and resolution properties improves\nthe quality of the learned features. Furthermore, we find using more images for\npretraining leads to a better performance in multiple downstream tasks. Linear\nclassifiers trained on top of the learned features show that networks\npretrained on digital histopathology datasets perform better than ImageNet\npretrained networks, boosting task performances by more than 28% in F1 scores\non average. These findings may also be useful when applying newer contrastive\ntechniques to histopathology data. Pretrained PyTorch models are made publicly\navailable at https://github.com/ozanciga/self-supervised-histopathology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ciga_O/0/1/0/all/0/1\">Ozan Ciga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_T/0/1/0/all/0/1\">Tony Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-based Plant Disease Diagnosis with Unsupervised Anomaly Detection Based on Reconstructability of Colors. (arXiv:2011.14306v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14306","description":"<p>This paper proposes an unsupervised anomaly detection technique for\nimage-based plant disease diagnosis. The construction of large and publicly\navailable datasets containing labeled images of healthy and diseased crop\nplants led to growing interest in computer vision techniques for automatic\nplant disease diagnosis. Although supervised image classifiers based on deep\nlearning can be a powerful tool for plant disease diagnosis, they require a\nhuge amount of labeled data. The data mining technique of anomaly detection\nincludes unsupervised approaches that do not require rare samples for training\nclassifiers. We propose an unsupervised anomaly detection technique for\nimage-based plant disease diagnosis that is based on the reconstructability of\ncolors; a deep encoder-decoder network trained to reconstruct the colors of\n\\textit{healthy} plant images should fail to reconstruct colors of symptomatic\nregions. Our proposed method includes a new image-based framework for plant\ndisease detection that utilizes a conditional adversarial network called\npix2pix and a new anomaly score based on CIEDE2000 color difference.\nExperiments with PlantVillage dataset demonstrated the superiority of our\nproposed method compared to an existing anomaly detector at identifying\ndiseased crop images in terms of accuracy, interpretability and computational\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katafuchi_R/0/1/0/all/0/1\">Ryoya Katafuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokunaga_T/0/1/0/all/0/1\">Terumasa Tokunaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Improved Iterative Neural Network for High-Quality Image-Domain Material Decomposition in Dual-Energy CT. (arXiv:2012.01986v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.01986","description":"<p>Dual-energy computed tomography (DECT) has been widely used in many\napplications that need material decomposition. Image-domain methods directly\ndecompose material images from high- and low-energy attenuation images, and\nthus, are susceptible to noise and artifacts on attenuation images. The purpose\nof this study is to develop an improved iterative neural network (INN) for\nhigh-quality image-domain material decomposition in DECT, and to study its\nproperties. We propose a new INN architecture for DECT material decomposition.\nThe proposed INN architecture uses distinct cross-material convolutional neural\nnetwork (CNN) in image refining modules, and uses image decomposition physics\nin image reconstruction modules. The distinct cross-material CNN refiners\nincorporate distinct encoding-decoding filters and cross-material model that\ncaptures correlations between different materials. We study the distinct\ncross-material CNN refiner with patch-based reformulation and tight-frame\ncondition. Numerical experiments with extended cardiactorso (XCAT) phantom and\nclinical data show that the proposed INN significantly improves the image\nquality over several image-domain material decomposition methods, including a\nconventional model-based image decomposition (MBID) method using an\nedge-preserving regularizer, a recent MBID method using pre-learned\nmaterial-wise sparsifying transforms, and a noniterative deep CNN method. Our\nstudy with patch-based reformulations reveals that learned filters of distinct\ncross-material CNN refiners can approximately satisfy the tight-frame\ncondition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhipeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1\">Yong Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chun_I/0/1/0/all/0/1\">Il Yong Chun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification. (arXiv:2012.03173v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.03173","description":"<p>Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural\nnetwork by maximizing the AUC score of the model on a dataset. Most previous\nworks of AUC maximization focus on the perspective of optimization by designing\nefficient stochastic algorithms, and studies on generalization performance of\nlarge-scale DAM on difficult tasks are missing. In this work, we aim to make\nDAM more practical for interesting real-world applications (e.g., medical image\nclassification). First, we propose a new margin-based min-max surrogate loss\nfunction for the AUC score (named as AUC min-max-margin loss or simply AUC\nmargin loss for short). It is more robust than the commonly used AUC square\nloss, while enjoying the same advantage in terms of large-scale stochastic\noptimization. Second, we conduct extensive empirical studies of our DAM method\non four difficult medical image classification tasks, namely (i) classification\nof chest x-ray images for identifying many threatening diseases, (ii)\nclassification of images of skin lesions for identifying melanoma, (iii)\nclassification of mammogram for breast cancer screening, and (iv)\nclassification of microscopic images for identifying tumor tissue. Our studies\ndemonstrate that the proposed DAM method improves the performance of optimizing\ncross-entropy loss by a large margin, and also achieves better performance than\noptimizing the existing AUC square loss on these medical image classification\ntasks. Specifically, our DAM method has achieved the 1st place on Stanford\nCheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is\nthe first work that makes DAM succeed on large-scale medical image datasets. We\nalso conduct extensive ablation studies to demonstrate the advantages of the\nnew AUC margin loss over the AUC square loss on benchmark datasets. The\nproposed method is implemented in our open-sourced library LibAUC\n(www.libauc.org).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhuoning Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonka_M/0/1/0/all/0/1\">Milan Sonka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers in Vision: A Survey. (arXiv:2101.01169v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.01169","description":"<p>Astounding results from Transformer models on natural language tasks have\nintrigued the vision community to study their application to computer vision\nproblems. Among their salient benefits, Transformers enable modeling long\ndependencies between input sequence elements and support parallel processing of\nsequence as compared to recurrent networks e.g., Long short-term memory (LSTM).\nDifferent from convolutional networks, Transformers require minimal inductive\nbiases for their design and are naturally suited as set-functions. Furthermore,\nthe straightforward design of Transformers allows processing multiple\nmodalities (e.g., images, videos, text and speech) using similar processing\nblocks and demonstrates excellent scalability to very large capacity networks\nand huge datasets. These strengths have led to exciting progress on a number of\nvision tasks using Transformer networks. This survey aims to provide a\ncomprehensive overview of the Transformer models in the computer vision\ndiscipline. We start with an introduction to fundamental concepts behind the\nsuccess of Transformers i.e., self-attention, large-scale pre-training, and\nbidirectional encoding. We then cover extensive applications of transformers in\nvision including popular recognition tasks (e.g., image classification, object\ndetection, action recognition, and segmentation), generative modeling,\nmulti-modal tasks (e.g., visual-question answering, visual reasoning, and\nvisual grounding), video processing (e.g., activity recognition, video\nforecasting), low-level vision (e.g., image super-resolution, image\nenhancement, and colorization) and 3D analysis (e.g., point cloud\nclassification and segmentation). We compare the respective advantages and\nlimitations of popular techniques both in terms of architectural design and\ntheir experimental value. Finally, we provide an analysis on open research\ndirections and possible future works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1\">Muzammal Naseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_S/0/1/0/all/0/1\">Syed Waqas Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Object Detection via Integrity Learning. (arXiv:2101.07663v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07663","description":"<p>Albeit current salient object detection (SOD) works have achieved fantastic\nprogress, they are cast into the shade when it comes to the integrity of the\npredicted salient regions. We define the concept of integrity at both the micro\nand macro level. Specifically, at the micro level, the model should highlight\nall parts that belong to a certain salient object, while at the macro level,\nthe model needs to discover all salient objects from the given image scene. To\nfacilitate integrity learning for salient object detection, we design a novel\nIntegrity Cognition Network (ICON), which explores three important components\nto learn strong integrity features. 1) Unlike the existing models that focus\nmore on feature discriminability, we introduce a diverse feature aggregation\n(DFA) component to aggregate features with various receptive fields (i.e.,,\nkernel shape and context) and increase the feature diversity. Such diversity is\nthe foundation for mining the integral salient objects. 2) Based on the DFA\nfeatures, we introduce the integrity channel enhancement (ICE) component with\nthe goal of enhancing feature channels that highlight the integral salient\nobjects at the macro level, while suppressing the other distracting ones. 3)\nAfter extracting the enhanced features, the part-whole verification (PWV)\nmethod is employed to determine whether the part and whole object features have\nstrong agreement. Such part-whole agreements can further improve the\nmicro-level integrity for each salient object. To demonstrate the effectiveness\nof ICON, comprehensive experiments are conducted on seven challenging\nbenchmarks, where promising results are achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuge_M/0/1/0/all/0/1\">Mingchen Zhuge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor Neural Network. (arXiv:2103.08160v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08160","description":"<p>Few-shot learning (FSL) aims to classify images under low-data regimes, where\nthe conventional pooled global feature is likely to lose useful local\ncharacteristics. Recent work has achieved promising performances by using deep\ndescriptors. They generally take all deep descriptors from neural networks into\nconsideration while ignoring that some of them are useless in classification\ndue to their limited receptive field, e.g., task-irrelevant descriptors could\nbe misleading and multiple aggregative descriptors from background clutter\ncould even overwhelm the object's presence. In this paper, we argue that a\nMutual Nearest Neighbor (MNN) relation should be established to explicitly\nselect the query descriptors that are most relevant to each task and discard\nless relevant ones from aggregative clutters in FSL. Specifically, we propose\nDiscriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive\nexperiments demonstrate that our method outperforms the existing\nstate-of-the-arts on both fine-grained and generalized datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Suppress-and-Refine Framework for End-to-End 3D Object Detection. (arXiv:2103.10042v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.10042","description":"<p>3D object detector based on Hough voting achieves great success and derives\nmany follow-up works. Despite constantly refreshing the detection accuracy,\nthese works suffer from handcrafted components used to eliminate redundant\nboxes, and thus are non-end-to-end and time-consuming. In this work, we propose\na suppress-and-refine framework to remove these handcrafted components. To\nfully utilize full-resolution information and achieve real-time speed, it\ndirectly consumes feature points and redundant 3D proposals. Specifically, it\nfirst suppresses noisy 3D feature points and then feeds them to 3D proposals\nfor the following RoI-aware refinement. With the gating mechanism to build fine\nproposal features and the self-attention mechanism to model relationships, our\nmethod can produce high-quality predictions with a small computation budget in\nan end-to-end manner. To this end, we present the first fully end-to-end 3D\ndetector, SRDet, on the basis of VoteNet. It achieves state-of-the-art\nperformance on the challenging ScanNetV2 and SUN RGB-D datasets with the\nfastest speed ever. Our code will be available at\nhttps://github.com/ZJULearning/SRDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zili Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guodong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Honghui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kuoliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling-up Disentanglement for Image Translation. (arXiv:2103.14017v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14017","description":"<p>Image translation methods typically aim to manipulate a set of labeled\nattributes (given as supervision at training time e.g. domain label) while\nleaving the unlabeled attributes intact. Current methods achieve either: (i)\ndisentanglement, which exhibits low visual fidelity and can only be satisfied\nwhere the attributes are perfectly uncorrelated. (ii) visually-plausible\ntranslations, which are clearly not disentangled. In this work, we propose\nOverLORD, a single framework for disentangling labeled and unlabeled attributes\nas well as synthesizing high-fidelity images, which is composed of two stages;\n(i) Disentanglement: Learning disentangled representations with latent\noptimization. Differently from previous approaches, we do not rely on\nadversarial training or any architectural biases. (ii) Synthesis: Training\nfeed-forward encoders for inferring the learned attributes and tuning the\ngenerator in an adversarial manner to increase the perceptual quality. When the\nlabeled and unlabeled attributes are correlated, we model an additional\nrepresentation that accounts for the correlated attributes and improves\ndisentanglement. We highlight that our flexible framework covers multiple\nsettings as disentangling labeled attributes, pose and appearance, localized\nconcepts, and shape and texture. We present significantly better\ndisentanglement with higher translation quality and greater output diversity\nthan state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabbay_A/0/1/0/all/0/1\">Aviv Gabbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lidar Point Cloud Guided Monocular 3D Object Detection. (arXiv:2104.09035v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09035","description":"<p>Monocular 3D detection currently struggles with extremely lower detection\nrates compared to LiDAR-based methods. The poor accuracy is mainly caused by\nthe absence of accurate location cues due to the ill-posed nature of monocular\nimagery. LiDAR point clouds, which provide precise spatial measurement, can\noffer beneficial information for the training of monocular methods. To make use\nof LiDAR point clouds, prior works project them to form depth map labels,\nsubsequently training a dense depth estimator to extract explicit location\nfeatures. This indirect and complicated way introduces intermediate products,\ni.e., depth map predictions, taking much computation costs as well as leading\nto suboptimal performances. In this paper, we propose LPCG (LiDAR point cloud\nguided monocular 3D object detection), which is a general framework for guiding\nthe training of monocular 3D detectors with LiDAR point clouds. Specifically,\nwe use LiDAR point clouds to generate pseudo labels, allowing monocular 3D\ndetectors to benefit from easy-collected massive unlabeled data. LPCG works\nwell under both supervised and unsupervised setups. Thanks to a general design,\nLPCG can be plugged into any monocular 3D detector, significantly boosting the\nperformance. As a result, we take the first place on KITTI monocular 3D/BEV\n(bird's-eye-view) detection benchmark with a considerable margin. The code will\nbe made publicly available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengxu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Senbo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Dan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAGAN: Text-To-Image Generation with Combined Attention GANs. (arXiv:2104.12663v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12663","description":"<p>Generating images according to natural language descriptions is a challenging\ntask. Prior research has mainly focused to enhance the quality of generation by\ninvestigating the use of spatial attention and/or textual attention thereby\nneglecting the relationship between channels. In this work, we propose the\nCombined Attention Generative Adversarial Network (CAGAN) to generate\nphoto-realistic images according to textual descriptions. The proposed CAGAN\nutilises two attention models: word attention to draw different sub-regions\nconditioned on related words; and squeeze-and-excitation attention to capture\nnon-linear interaction among channels. With spectral normalisation to stabilise\ntraining, our proposed CAGAN improves the state of the art on the IS and FID on\nthe CUB dataset and the FID on the more challenging COCO dataset. Furthermore,\nwe demonstrate that judging a model by a single evaluation metric can be\nmisleading by developing an additional model adding local self-attention which\nscores a higher IS, outperforming the state of the art on the CUB dataset, but\ngenerates unrealistic images through feature repetition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schulze_H/0/1/0/all/0/1\">Henning Schulze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1\">Dogucan Yaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Information Extraction by Character-Level Embedding and Multi-Stage Attentional U-Net. (arXiv:2106.00952v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00952","description":"<p>Information extraction from document images has received a lot of attention\nrecently, due to the need for digitizing a large volume of unstructured\ndocuments such as invoices, receipts, bank transfers, etc. In this paper, we\npropose a novel deep learning architecture for end-to-end information\nextraction on the 2D character-grid embedding of the document, namely the\n\\textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and\nspatial relations between 2D elements, our model leverages a specialized\nmulti-stage encoder-decoders design, in conjunction with efficient uses of the\nself-attention mechanism and the box convolution. Experimental results on\ndifferent datasets show that our model outperforms the baseline U-Net\narchitecture by a large margin while using 40\\% fewer parameters. Moreover, it\nalso significantly improved the baseline in erroneous OCR and limited training\ndata scenario, thus becomes practical for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tuan-Anh Nguyen Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat-Thanh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSI: Temporal Saliency Integration for Video Action Recognition. (arXiv:2106.01088v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01088","description":"<p>Efficient spatiotemporal modeling is an important yet challenging problem for\nvideo action recognition. Existing state-of-the-art methods exploit motion\nclues to assist in short-term temporal modeling through temporal difference\nover consecutive frames. However, insignificant noises will be inevitably\nintroduced due to the camera movement. Besides, movements of different actions\ncan vary greatly. In this paper, we propose a Temporal Saliency Integration\n(TSI) block, which mainly contains a Salient Motion Excitation (SME) module and\na Cross-scale Temporal Integration (CTI) module. Specifically, SME aims to\nhighlight the motion-sensitive area through local-global motion modeling, where\nthe saliency alignment and pyramidal feature difference are conducted\nsuccessively between neighboring frames to capture motion dynamics with less\nnoises caused by misaligned background. CTI is designed to perform multi-scale\ntemporal modeling through a group of separate 1D convolutions respectively.\nMeanwhile, temporal interactions across different scales are integrated with\nattention mechanism. Through these two modules, long short-term temporal\nrelationships can be encoded efficiently by introducing limited additional\nparameters. Extensive experiments are conducted on several popular benchmarks\n(i.e., Something-Something V1 &amp; V2, Kinetics-400, UCF-101, and HMDB-51), which\ndemonstrate the effectiveness and superiority of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Haisheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jinyuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Weihao Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. (arXiv:2106.05953v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05953","description":"<p>Encouraged by the success of contrastive learning on image classification\ntasks, we propose a new self-supervised method for the structured regression\ntask of 3D hand pose estimation. Contrastive learning makes use of unlabeled\ndata for the purpose of representation learning via a loss formulation that\nencourages the learned feature representations to be invariant under any image\ntransformation. For 3D hand pose estimation, it too is desirable to have\ninvariance to appearance transformation such as color jitter. However, the task\nrequires equivariance under affine transformations, such as rotation and\ntranslation. To address this issue, we propose an equivariant contrastive\nobjective and demonstrate its effectiveness in the context of 3D hand pose\nestimation. We experimentally investigate the impact of invariant and\nequivariant contrastive objectives and show that learning equivariant features\nleads to better representations for the task of 3D hand pose estimation.\nFurthermore, we show that standard ResNets with sufficient depth, trained on\nadditional unlabeled data, attain improvements of up to 14.5% in PA-EPE on\nFreiHAND and thus achieves state-of-the-art performance without any task\nspecific, specialized architectures. Code and models are available at\nhttps://ait.ethz.ch/projects/2021/PeCLR/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spurr_A/0/1/0/all/0/1\">Adrian Spurr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_A/0/1/0/all/0/1\">Aneesh Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xucong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plant Disease Detection Using Image Processing and Machine Learning. (arXiv:2106.10698v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10698","description":"<p>One of the important and tedious task in agricultural practices is the\ndetection of the disease on crops. It requires huge time as well as skilled\nlabor. This paper proposes a smart and efficient technique for detection of\ncrop disease which uses computer vision and machine learning techniques. The\nproposed system is able to detect 20 different diseases of 5 common plants with\n93% accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1\">Pranesh Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karwande_A/0/1/0/all/0/1\">Atharva Karwande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolhe_T/0/1/0/all/0/1\">Tejas Kolhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamble_S/0/1/0/all/0/1\">Soham Kamble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Akshay Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wyawahare_M/0/1/0/all/0/1\">Medha Wyawahare</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.09391","description":"<p>We focus on building robustness in the convolutions of neural visual\nclassifiers, especially against natural perturbations like elastic\ndeformations, occlusions and Gaussian noise. Existing CNNs show outstanding\nperformance on clean images, but fail to tackle naturally occurring\nperturbations. In this paper, we start from elastic perturbations, which\napproximate (local) view-point changes of the object. We present\nelastically-augmented convolutions (EAConv) by parameterizing filters as a\ncombination of fixed elastically-perturbed bases functions and trainable\nweights for the purpose of integrating unseen viewpoints in the CNN. We show on\nCIFAR-10 and STL-10 datasets that the general robustness of our method on\nunseen occlusion, zoom, rotation, image cut and Gaussian perturbations\nimproves, while significantly improving the performance on clean images without\nany data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gulshad_S/0/1/0/all/0/1\">Sadaf Gulshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1\">Ivan Sosnovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1\">Arnold Smeulders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations. (arXiv:2107.14483v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.14483","description":"<p>Object manipulation from 3D visual inputs poses many challenges on building\ngeneralizable perception and policy models. However, 3D assets in existing\nbenchmarks mostly lack the diversity of 3D shapes that align with real-world\nintra-class complexity in topology and geometry. Here we propose SAPIEN\nManipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over\ndiverse objects in a full-physics simulator. 3D assets in ManiSkill include\nlarge intra-class topological and geometric variations. Tasks are carefully\nchosen to cover distinct types of manipulation challenges. Latest progress in\n3D vision also makes us believe that we should customize the benchmark so that\nthe challenge is inviting to researchers working on 3D deep learning. To this\nend, we simulate a moving panoramic camera that returns ego-centric point\nclouds or RGB-D images. In addition, we would like ManiSkill to serve a broad\nset of researchers interested in manipulation research. Besides supporting the\nlearning of policies from interactions, we also support\nlearning-from-demonstrations (LfD) methods, by providing a large number of\nhigh-quality demonstrations (~36,000 successful trajectories, ~1.5M point\ncloud/RGB-D frames in total). We provide baselines using 3D deep learning and\nLfD algorithms. All code of our benchmark (simulator, environment, SDK, and\nbaselines) is open-sourced, and a challenge facing interdisciplinary\nresearchers will be held based on the benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParamCrop: Parametric Cubic Cropping for Video Contrastive Learning. (arXiv:2108.10501v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10501","description":"<p>The central idea of contrastive learning is to discriminate between different\ninstances and force different views of the same instance to share the same\nrepresentation. To avoid trivial solutions, augmentation plays an important\nrole in generating different views, among which random cropping is shown to be\neffective for the model to learn a strong and generalized representation.\nCommonly used random crop operation keeps the difference between two views\nstatistically consistent along the training process. In this work, we challenge\nthis convention by showing that adaptively controlling the disparity between\ntwo augmented views along the training process enhances the quality of the\nlearnt representation. Specifically, we present a parametric cubic cropping\noperation, ParamCrop, for video contrastive learning, which automatically crops\na 3D cubic from the video by differentiable 3D affine transformations.\nParamCrop is trained simultaneously with the video backbone using an\nadversarial objective and learns an optimal cropping strategy from the data.\nThe visualizations show that the center distance and the IoU between two\naugmented views are adaptively controlled by ParamCrop and the learned change\nin the disparity along the training process is beneficial to learning a strong\nrepresentation. Extensive ablation studies demonstrate the effectiveness of the\nproposed ParamCrop on multiple contrastive learning frameworks and video\nbackbones. With ParamCrop, we improve the state-of-the-art performance on both\nHMDB51 and UCF101 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H. Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wanderlust: Online Continual Object Detection in the Real World. (arXiv:2108.11005v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11005","description":"<p>Online continual learning from data streams in dynamic environments is a\ncritical direction in the computer vision field. However, realistic benchmarks\nand fundamental studies in this line are still missing. To bridge the gap, we\npresent a new online continual object detection benchmark with an egocentric\nvideo dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos,\nan ego-centric video stream collected over nine months by a graduate student.\nOAK provides exhaustive bounding box annotations of 80 video snippets (~17.5\nhours) for 105 object categories in outdoor scenes. The emergence of new object\ncategories in our benchmark follows a pattern similar to what a single person\nmight see in their day-to-day life. The dataset also captures the natural\ndistribution shifts as the person travels to different places. These egocentric\nlong-running videos provide a realistic playground for continual learning\nalgorithms, especially in online embodied settings. We also introduce new\nevaluation metrics to evaluate the model performance and catastrophic\nforgetting and provide baseline studies for online continual object detection.\nWe believe this benchmark will pose new exciting challenges for learning from\nnon-stationary data in continual learning. The OAK dataset and the associated\nbenchmark are released at https://oakdata.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Guan_Y/0/1/0/all/0/1\">Yue Shang-Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Aligned and Misaligned Features in One-stage Object Detection. (arXiv:2108.12176v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12176","description":"<p>One-stage object detectors rely on a point feature to predict the detection\nresults. However, the point feature often lacks the information of the whole\nobject, thereby leading to a misalignment between the object and the point\nfeature. Meanwhile, the classification and regression tasks are sensitive to\ndifferent object regions, but their features are spatially aligned. Both of\nthese two problems hinder the detection performance. In order to solve these\ntwo problems, we propose a simple and plug-in operator that can generate\naligned and disentangled features for each task, respectively, without breaking\nthe fully convolutional manner. By predicting two task-aware point sets that\nare located in each sensitive region, the proposed operator can align the point\nfeature with the object and disentangle the two tasks from the spatial\ndimension. We also reveal an interesting finding of the opposite effect of the\nlong-range skip connection for classification and regression. On the basis of\nthe Object-Aligned and Task-disentangled operator (OAT), we propose OAT-Net,\nwhich explicitly exploits point-set features for accurate detection results.\nExtensive experiments on the MS-COCO dataset show that OAT can consistently\nboost different state-of-the-art one-stage detectors by $\\sim$2 AP. Notably,\nOAT-Net with Res2Net-101-DCN backbone achieves 53.7 AP on the COCO test-dev.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zihao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digging into Uncertainty in Self-supervised Multi-view Stereo. (arXiv:2108.12966v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12966","description":"<p>Self-supervised Multi-view stereo (MVS) with a pretext task of image\nreconstruction has achieved significant progress recently. However, previous\nmethods are built upon intuitions, lacking comprehensive explanations about the\neffectiveness of the pretext task in self-supervised MVS. To this end, we\npropose to estimate epistemic uncertainty in self-supervised MVS, accounting\nfor what the model ignores. Specially, the limitations can be categorized into\ntwo types: ambiguious supervision in foreground and invalid supervision in\nbackground. To address these issues, we propose a novel Uncertainty reduction\nMulti-view Stereo (UMVS) framework for self-supervised learning. To alleviate\nambiguous supervision in foreground, we involve extra correspondence prior with\na flow-depth consistency loss. The dense 2D correspondence of optical flows is\nused to regularize the 3D stereo correspondence in MVS. To handle the invalid\nsupervision in background, we use Monte-Carlo Dropout to acquire the\nuncertainty map and further filter the unreliable supervision signals on\ninvalid regions. Extensive experiments on DTU and Tank&amp;Temples benchmark show\nthat our U-MVS framework achieves the best performance among unsupervised MVS\nmethods, with competitive performance with its supervised opponents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Wenxiong Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anatomical-Guided Attention Enhances Unsupervised PET Image Denoising Performance. (arXiv:2109.00802v2 [physics.med-ph] UPDATED)","link":"http://arxiv.org/abs/2109.00802","description":"<p>Although supervised convolutional neural networks (CNNs) often outperform\nconventional alternatives for denoising positron emission tomography (PET)\nimages, they require many low- and high-quality reference PET image pairs.\nHerein, we propose an unsupervised 3D PET image denoising method based on an\nanatomical information-guided attention mechanism. The proposed magnetic\nresonance-guided deep decoder (MR-GDD) utilizes the spatial details and\nsemantic features of MR-guidance image more effectively by introducing\nencoder-decoder and deep decoder subnetworks. Moreover, the specific shapes and\npatterns of the guidance image do not affect the denoised PET image, because\nthe guidance image is input to the network through an attention gate. In a\nMonte Carlo simulation of [$^{18}$F]fluoro-2-deoxy-D-glucose (FDG), the\nproposed method achieved the highest peak signal-to-noise ratio and structural\nsimilarity (27.92 $\\pm$ 0.44 dB/0.886 $\\pm$ 0.007), as compared with Gaussian\nfiltering (26.68 $\\pm$ 0.10 dB/0.807 $\\pm$ 0.004), image guided filtering\n(27.40 $\\pm$ 0.11 dB/0.849 $\\pm$ 0.003), deep image prior (DIP) (24.22 $\\pm$\n0.43 dB/0.737 $\\pm$ 0.017), and MR-DIP (27.65 $\\pm$ 0.42 dB/0.879 $\\pm$ 0.007).\nFurthermore, we experimentally visualized the behavior of the optimization\nprocess, which is often unknown in unsupervised CNN-based restoration problems.\nFor preclinical (using [$^{18}$F]FDG and [$^{11}$C]raclopride) and clinical\n(using [$^{18}$F]florbetapir) studies, the proposed method demonstrates\nstate-of-the-art denoising performance while retaining spatial resolution and\nquantitative accuracy, despite using a common network architecture for various\nnoisy PET images with 1/10th of the full counts. These results suggest that the\nproposed MR-GDD can reduce PET scan times and PET tracer doses considerably\nwithout impacting patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Onishi_Y/0/1/0/all/0/1\">Yuya Onishi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hashimoto_F/0/1/0/all/0/1\">Fumio Hashimoto</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ote_K/0/1/0/all/0/1\">Kibo Ote</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ohba_H/0/1/0/all/0/1\">Hiroyuki Ohba</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ota_R/0/1/0/all/0/1\">Ryosuke Ota</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yoshikawa_E/0/1/0/all/0/1\">Etsuji Yoshikawa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ouchi_Y/0/1/0/all/0/1\">Yasuomi Ouchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization. (arXiv:2109.02220v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02220","description":"<p>Model compression techniques are recently gaining explosive attention for\nobtaining efficient AI models for various real-time applications. Channel\npruning is one important compression strategy and is widely used in slimming\nvarious DNNs. Previous gate-based or importance-based pruning methods aim to\nremove channels whose importance is smallest. However, it remains unclear what\ncriteria the channel importance should be measured on, leading to various\nchannel selection heuristics. Some other sampling-based pruning methods deploy\nsampling strategies to train sub-nets, which often causes the training\ninstability and the compressed model's degraded performance. In view of the\nresearch gaps, we present a new module named Gates with Differentiable\nPolarization (GDP), inspired by principled optimization ideas. GDP can be\nplugged before convolutional layers without bells and whistles, to control the\non-and-off of each channel or whole layer block. During the training process,\nthe polarization effect will drive a subset of gates to smoothly decrease to\nexact zero, while other gates gradually stay away from zero by a large margin.\nWhen training terminates, those zero-gated channels can be painlessly removed,\nwhile other non-zero gates can be absorbed into the succeeding convolution\nkernel, causing completely no interruption to training nor damage to the\ntrained model. Experiments conducted over CIFAR-10 and ImageNet datasets show\nthat the proposed GDP algorithm achieves the state-of-the-art performance on\nvarious benchmark DNNs at a broad range of pruning ratios. We also apply GDP to\nDeepLabV3Plus-ResNet50 on the challenging Pascal VOC segmentation task, whose\ntest performance sees no drop (even slightly improved) with over 60% FLOPs\nsaving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Huan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jianchao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-Based Neural Rendering with Per-View Optimization. (arXiv:2109.02369v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02369","description":"<p>There has recently been great interest in neural rendering methods. Some\napproaches use 3D geometry reconstructed with Multi-View Stereo (MVS) but\ncannot recover from the errors of this process, while others directly learn a\nvolumetric neural representation, but suffer from expensive training and\ninference. We introduce a general approach that is initialized with MVS, but\nallows further optimization of scene properties in the space of input views,\nincluding depth and reprojected features, resulting in improved novel-view\nsynthesis. A key element of our approach is our new differentiable point-based\npipeline, based on bi-directional Elliptical Weighted Average splatting, a\nprobabilistic depth test and effective camera selection. We use these elements\ntogether in our neural renderer, that outperforms all previous methods both in\nquality and speed in almost all scenes we tested. Our pipeline can be applied\nto multi-view harmonization and stylization in addition to novel-view\nsynthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kopanas_G/0/1/0/all/0/1\">Georgios Kopanas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1\">Julien Philip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leimkuhler_T/0/1/0/all/0/1\">Thomas Leimk&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drettakis_G/0/1/0/all/0/1\">George Drettakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Animation Transformer: Visual Correspondence via Segment Matching. (arXiv:2109.02614v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02614","description":"<p>Visual correspondence is a fundamental building block on the way to building\nassistive tools for hand-drawn animation. However, while a large body of work\nhas focused on learning visual correspondences at the pixel-level, few\napproaches have emerged to learn correspondence at the level of line enclosures\n(segments) that naturally occur in hand-drawn animation. Exploiting this\nstructure in animation has numerous benefits: it avoids the intractable memory\ncomplexity of attending to individual pixels in high resolution images and\nenables the use of real-world animation datasets that contain correspondence\ninformation at the level of per-segment colors. To that end, we propose the\nAnimation Transformer (AnT) which uses a transformer-based architecture to\nlearn the spatial and visual relationships between segments across a sequence\nof images. AnT enables practical ML-assisted colorization for professional\nanimation workflows and is publicly accessible as a creative tool in Cadmium.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casey_E/0/1/0/all/0/1\">Evan Casey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_V/0/1/0/all/0/1\">V&#xed;ctor P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teitelman_H/0/1/0/all/0/1\">Harry Teitelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyajian_N/0/1/0/all/0/1\">Nick Boyajian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulver_T/0/1/0/all/0/1\">Tim Pulver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manh_M/0/1/0/all/0/1\">Mike Manh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grisaitis_W/0/1/0/all/0/1\">William Grisaitis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GCsT: Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02860","description":"<p>Graph convolutional networks (GCNs) achieve promising performance for\nskeleton-based action recognition. However, in most GCN-based methods, the\nspatial-temporal graph convolution is strictly restricted by the graph topology\nwhile only captures the short-term temporal context, thus lacking the\nflexibility of feature extraction. In this work, we present a novel\narchitecture, named Graph Convolutional skeleton Transformer (GCsT), which\naddresses limitations in GCNs by introducing Transformer. Our GCsT employs all\nthe benefits of Transformer (i.e. dynamical attention and global context) while\nkeeps the advantages of GCNs (i.e. hierarchy and local topology structure). In\nGCsT, the spatial-temporal GCN forces the capture of local dependencies while\nTransformer dynamically extracts global spatial-temporal relationships.\nFurthermore, the proposed GCsT shows stronger expressive capability by adding\nadditional information present in skeleton sequences. Incorporating the\nTransformer allows that information to be introduced into the model almost\neffortlessly. We validate the proposed GCsT by conducting extensive\nexperiments, which achieves the state-of-the-art performance on NTU RGB+D, NTU\nRGB+D 120 and Northwestern-UCLA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_R/0/1/0/all/0/1\">Ruwen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengfa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Miao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpaired Adversarial Learning for Single Image Deraining with Rain-Space Contrastive Constraints. (arXiv:2109.02973v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02973","description":"<p>Deep learning-based single image deraining (SID) with unpaired information is\nof immense importance, as relying on paired synthetic data often limits their\ngenerality and scalability in real-world applications. However, we noticed that\ndirect employ of unpaired adversarial learning and cycle-consistency\nconstraints in the SID task is insufficient to learn the underlying\nrelationship from rainy input to clean outputs, since the domain knowledge\nbetween rainy and rain-free images is asymmetrical. To address such limitation,\nwe develop an effective unpaired SID method which explores mutual properties of\nthe unpaired exemplars by a contrastive learning manner in a GAN framework,\nnamed as CDR-GAN. The proposed method mainly consists of two cooperative\nbranches: Bidirectional Translation Branch (BTB) and Contrastive Guidance\nBranch (CGB). Specifically, BTB takes full advantage of the circulatory\narchitecture of adversarial consistency to exploit latent feature distributions\nand guide transfer ability between two domains by equipping it with\nbidirectional mapping. Simultaneously, CGB implicitly constrains the embeddings\nof different exemplars in rain space by encouraging the similar feature\ndistributions closer while pushing the dissimilar further away, in order to\nbetter help rain removal and image restoration. During training, we explore\nseveral loss functions to further constrain the proposed CDR-GAN. Extensive\nexperiments show that our method performs favorably against existing unpaired\nderaining approaches on both synthetic and real-world datasets, even\noutperforms several fully-supervised or semi-supervised models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinshan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Caihua Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Longgang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair Comparison: Quantifying Variance in Resultsfor Fine-grained Visual Categorization. (arXiv:2109.03156v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03156","description":"<p>For the task of image classification, researchers work arduously to develop\nthe next state-of-the-art (SOTA) model, each bench-marking their own\nperformance against that of their predecessors and of their peers.\nUnfortunately, the metric used most frequently to describe a model's\nperformance, average categorization accuracy, is often used in isolation. As\nthe number of classes increases, such as in fine-grained visual categorization\n(FGVC), the amount of information conveyed by average accuracy alone dwindles.\nWhile its most glaring weakness is its failure to describe the model's\nperformance on a class-by-class basis, average accuracy also fails to describe\nhow performance may vary from one trained model of the same architecture, on\nthe same dataset, to another (both averaged across all categories and at the\nper-class level). We first demonstrate the magnitude of these variations across\nmodels and across class distributions based on attributes of the data,\ncomparing results on different visual domains and different per-class image\ndistributions, including long-tailed distributions and few-shot subsets. We\nthen analyze the impact various FGVC methods have on overall and per-class\nvariance. From this analysis, we both highlight the importance of reporting and\ncomparing methods based on information beyond overall accuracy, as well as\npoint out techniques that mitigate variance in FGVC results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gwilliam_M/0/1/0/all/0/1\">Matthew Gwilliam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teuscher_A/0/1/0/all/0/1\">Adam Teuscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_C/0/1/0/all/0/1\">Connor Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farrell_R/0/1/0/all/0/1\">Ryan Farrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NnFormer: Interleaved Transformer for Volumetric Segmentation. (arXiv:2109.03201v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03201","description":"<p>Transformers, the default model of choices in natural language processing,\nhave drawn scant attention from the medical imaging community. Given the\nability to exploit long-term dependencies, transformers are promising to help\natypical convolutional neural networks (convnets) to overcome its inherent\nshortcomings of spatial inductive bias. However, most of recently proposed\ntransformer-based segmentation approaches simply treated transformers as\nassisted modules to help encode global context into convolutional\nrepresentations without investigating how to optimally combine self-attention\n(i.e., the core of transformers) with convolution. To address this issue, in\nthis paper, we introduce nnFormer (i.e., Not-aNother transFormer), a powerful\nsegmentation model with an interleaved architecture based on empirical\ncombination of self-attention and convolution. In practice, nnFormer learns\nvolumetric representations from 3D local volumes. Compared to the naive\nvoxel-level self-attention implementation, such volume-based operations help to\nreduce the computational complexity by approximate 98% and 99.5% on Synapse and\nACDC datasets, respectively. In comparison to prior-art network configurations,\nnnFormer achieves tremendous improvements over previous transformer-based\nmethods on two commonly used datasets Synapse and ACDC. For instance, nnFormer\noutperforms Swin-UNet by over 7 percents on Synapse. Even when compared to\nnnUNet, currently the best performing fully-convolutional medical segmentation\nnetwork, nnFormer still provides slightly better performance on Synapse and\nACDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiansen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets. (arXiv:2109.03229v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03229","description":"<p>Many existing works have made great strides towards reducing racial bias in\nface recognition. However, most of these methods attempt to rectify bias that\nmanifests in models during training instead of directly addressing a major\nsource of the bias, the dataset itself. Exceptions to this are\nBUPT-Balancedface/RFW and Fairface, but these works assume that primarily\ntraining on a single race or not racially balancing the dataset are inherently\ndisadvantageous. We demonstrate that these assumptions are not necessarily\nvalid. In our experiments, training on only African faces induced less bias\nthan training on a balanced distribution of faces and distributions skewed to\ninclude more African faces produced more equitable models. We additionally\nnotice that adding more images of existing identities to a dataset in place of\nadding new identities can lead to accuracy boosts across racial categories. Our\ncode is available at\nhttps://github.com/j-alex-hanson/rethinking-race-face-datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gwilliam_M/0/1/0/all/0/1\">Matthew Gwilliam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1\">Srinidhi Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinubu_L/0/1/0/all/0/1\">Lade Tinubu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanson_A/0/1/0/all/0/1\">Alex Hanson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly supervised semantic segmentation of tomographic images in the diagnosis of stroke. (arXiv:2109.01887v1 [eess.IV] CROSS LISTED)","link":"http://arxiv.org/abs/2109.01887","description":"<p>This paper presents an automatic algorithm for the segmentation of areas\naffected by an acute stroke on the non-contrast computed tomography brain\nimages. The proposed algorithm is designed for learning in a weakly supervised\nscenario when some images are labeled accurately, and some images are labeled\ninaccurately. Wrong labels appear as a result of inaccuracy made by a\nradiologist in the process of manual annotation of computed tomography images.\nWe propose methods for solving the segmentation problem in the case of\ninaccurately labeled training data. We use the U-Net neural network\narchitecture with several modifications. Experiments on real computed\ntomography scans show that the proposed methods increase the segmentation\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dobshik_A/0/1/0/all/0/1\">Anna Dobshik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tulupov_A/0/1/0/all/0/1\">Andrey Tulupov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berikov_V/0/1/0/all/0/1\">Vladimir Berikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}