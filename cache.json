{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Improved Text Classification via Test-Time Augmentation. (arXiv:2206.13607v1 [cs.LG])","link":"http://arxiv.org/abs/2206.13607","description":"<p>Test-time augmentation -- the aggregation of predictions across transformed\nexamples of test inputs -- is an established technique to improve the\nperformance of image classification models. Importantly, TTA can be used to\nimprove model performance post-hoc, without additional training. Although\ntest-time augmentation (TTA) can be applied to any data modality, it has seen\nlimited adoption in NLP due in part to the difficulty of identifying\nlabel-preserving transformations. In this paper, we present augmentation\npolicies that yield significant accuracy improvements with language models. A\nkey finding is that augmentation policy design -- for instance, the number of\nsamples generated from a single, non-deterministic augmentation -- has a\nconsiderable impact on the benefit of TTA. Experiments across a binary\nclassification task and dataset show that test-time augmentation can deliver\nconsistent improvements over current state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Helen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_D/0/1/0/all/0/1\">Divya Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_H/0/1/0/all/0/1\">Harini Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1\">John Guttag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wav2Vec-Aug: Improved self-supervised training with limited data. (arXiv:2206.13654v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13654","description":"<p>Self-supervised learning (SSL) of speech representations has received much\nattention over the last few years but most work has focused on languages and\ndomains with an abundance of unlabeled data. However, for many languages there\nis a shortage even in the unlabeled data which limits the effectiveness of SSL.\nIn this work, we focus on the problem of applying SSL to domains with limited\navailable data by leveraging data augmentation for Wav2Vec 2.0 pretraining.\nFurther, we propose improvements to each component of the model which result in\na combined relative word error rate (WER) improvement of up to 13% compared to\nWav2Vec 2.0 on Librispeech test-clean / other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sriram_A/0/1/0/all/0/1\">Anuroop Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kwame for Science: An AI Teaching Assistant for Science Education in West Africa. (arXiv:2206.13703v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13703","description":"<p>Africa has a high student-to-teacher ratio which limits students' access to\nteachers. Consequently, students struggle to get answers to their questions. In\nthis work, we extended Kwame, our previous AI teaching assistant, adapted it\nfor science education, and deployed it as a web app. Kwame for Science answers\nquestions of students based on the Integrated Science subject of the West\nAfrican Senior Secondary Certificate Examination (WASSCE). Kwame for Science is\na Sentence-BERT-based question-answering web app that displays 3 paragraphs as\nanswers along with a confidence score in response to science questions.\nAdditionally, it displays the top 5 related past exam questions and their\nanswers in addition to the 3 paragraphs. Our preliminary evaluation of the\nKwame for Science with a 2.5-week real-world deployment showed a top 3 accuracy\nof 87.5% (n=56) with 190 users across 11 countries. Kwame for Science will\nenable the delivery of scalable, cost-effective, and quality remote education\nto millions of people across Africa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1\">George Boateng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+John_S/0/1/0/all/0/1\">Samuel John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glago_A/0/1/0/all/0/1\">Andrew Glago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boateng_S/0/1/0/all/0/1\">Samuel Boateng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumbol_V/0/1/0/all/0/1\">Victor Kumbol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Fine-Grained Entity Typing with Automatic Label Interpretation and Instance Generation. (arXiv:2206.13746v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13746","description":"<p>We study the problem of few-shot Fine-grained Entity Typing (FET), where only\na few annotated entity mentions with contexts are given for each entity type.\nRecently, prompt-based tuning has demonstrated superior performance to standard\nfine-tuning in few-shot scenarios by formulating the entity type classification\ntask as a ''fill-in-the-blank'' problem. This allows effective utilization of\nthe strong language modeling capability of Pre-trained Language Models (PLMs).\nDespite the success of current prompt-based tuning approaches, two major\nchallenges remain: (1) the verbalizer in prompts is either manually designed or\nconstructed from external knowledge bases, without considering the target\ncorpus and label hierarchy information, and (2) current approaches mainly\nutilize the representation power of PLMs, but have not explored their\ngeneration power acquired through extensive general-domain pre-training. In\nthis work, we propose a novel framework for few-shot FET consisting of two\nmodules: (1) an entity type label interpretation module automatically learns to\nrelate type labels to the vocabulary by jointly leveraging few-shot instances\nand the label hierarchy, and (2) a type-based contextualized instance generator\nproduces new instances based on given instances to enlarge the training set for\nbetter generalization. On three benchmark datasets, our model outperforms\nexisting methods by significant margins. Code can be found at\nhttps://github.com/teapot123/Fine-Grained-Entity-Typing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phrase Mining. (arXiv:2206.13748v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13748","description":"<p>Extracting frequent words from a collection of texts is performed on a great\nscale in many subjects. Extracting phrases, on the other hand, is not commonly\ndone due to inherent complications when extracting phrases, the most\nsignificant complication being that of double-counting, where words or phrases\nare counted when they appear inside longer phrases that themselves are also\ncounted. Several papers have been written on phrase mining that describe\nsolutions to this issue; however, they either require a list of so-called\nquality phrases to be available to the extracting process, or they require\nhuman interaction to identify those quality phrases during the process. We\npresent a method that eliminates double-counting without the need to identify\nlists of quality phrases. In the context of a set of texts, we define a\nprincipal phrase as a phrase that does not cross punctuation marks, does not\nstart with a stop word, with the exception of the stop words \"not\" and \"no\",\ndoes not end with a stop word, is frequent within those texts without being\ndouble counted, and is meaningful to the user. Our method can identify such\nprincipal phrases independently without human input, and enables their\nextraction from any texts. An R package called phm has been developed that\nimplements this method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Small_E/0/1/0/all/0/1\">Ellie Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabrera_J/0/1/0/all/0/1\">Javier Cabrera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Multi-view Rule Discovery for Weakly-Supervised Compatible Products Prediction. (arXiv:2206.13749v1 [cs.LG])","link":"http://arxiv.org/abs/2206.13749","description":"<p>On e-commerce platforms, predicting if two products are compatible with each\nother is an important functionality to achieve trustworthy product\nrecommendation and search experience for consumers. However, accurately\npredicting product compatibility is difficult due to the heterogeneous product\ndata and the lack of manually curated training data. We study the problem of\ndiscovering effective labeling rules that can enable weakly-supervised product\ncompatibility prediction. We develop AMRule, a multi-view rule discovery\nframework that can (1) adaptively and iteratively discover novel rulers that\ncan complement the current weakly-supervised model to improve compatibility\nprediction; (2) discover interpretable rules from both structured attribute\ntables and unstructured product descriptions. AMRule adaptively discovers\nlabeling rules from large-error instances via a boosting-style strategy, the\nhigh-quality rules can remedy the current model's weak spots and refine the\nmodel iteratively. For rule discovery from structured product attributes, we\ngenerate composable high-order rules from decision trees; and for rule\ndiscovery from unstructured product descriptions, we generate prompt-based\nrules from a pre-trained language model. Experiments on 4 real-world datasets\nshow that AMRule outperforms the baselines by 5.98% on average and improves\nrule quality and rule proposal efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Rebecca West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiquan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Impact of Noises in Crowd-Sourced Data for Speech Translation. (arXiv:2206.13756v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13756","description":"<p>Training speech translation (ST) models requires large and high-quality\ndatasets. MuST-C is one of the most widely used ST benchmark datasets. It\ncontains around 400 hours of speech-transcript-translation data for each of the\neight translation directions. This dataset passes several quality-control\nfilters during creation. However, we find that MuST-C still suffers from three\nmajor quality issues: audio-text misalignment, inaccurate translation, and\nunnecessary speaker's name. What are the impacts of these data quality issues\nfor model development and evaluation? In this paper, we propose an automatic\nmethod to fix or filter the above quality issues, using English-German (En-De)\ntranslation as an example. Our experiments show that ST models perform better\non clean test sets, and the rank of proposed models remains consistent across\ndifferent test sets. Besides, simply removing misaligned data points from the\ntraining set does not lead to a better ST model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siqi Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible text generation for counterfactual fairness probing. (arXiv:2206.13757v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13757","description":"<p>A common approach for testing fairness issues in text-based classifiers is\nthrough the use of counterfactuals: does the classifier output change if a\nsensitive attribute in the input is changed? Existing counterfactual generation\nmethods typically rely on wordlists or templates, producing simple\ncounterfactuals that don't take into account grammar, context, or subtle\nsensitive attribute references, and could miss issues that the wordlist\ncreators had not considered. In this paper, we introduce a task for generating\ncounterfactuals that overcomes these shortcomings, and demonstrate how large\nlanguage models (LLMs) can be leveraged to make progress on this task. We show\nthat this LLM-based method can produce complex counterfactuals that existing\nmethods cannot, comparing the performance of various counterfactual generation\nmethods on the Civil Comments dataset and showing their value in evaluating a\ntoxicity classifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fryer_Z/0/1/0/all/0/1\">Zee Fryer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axelrod_V/0/1/0/all/0/1\">Vera Axelrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Packer_B/0/1/0/all/0/1\">Ben Packer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1\">Alex Beutel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jilin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webster_K/0/1/0/all/0/1\">Kellie Webster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CC-Riddle: A Question Answering Dataset of Chinese Character Riddles. (arXiv:2206.13778v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13778","description":"<p>Chinese character riddle is a challenging riddle game which takes a single\ncharacter as the solution. The riddle describes the pronunciation, shape and\nmeaning of the solution character with rhetoric techniques. In this paper, we\npropose a Chinese character riddle dataset covering the majority of common\nsimplified Chinese characters by crawling riddles from the Web and generating\nbrand new ones. In the generation stage, we provide the Chinese phonetic\nalphabet, decomposition and explanation of the solution character for the\ngeneration model and get multiple riddle descriptions for each tested\ncharacter. Then the generated riddles are manually filtered and the final\ndataset, CC-Riddle is composed of both human-written riddles and filtered\ngenerated riddles. Furthermore, we build a character riddle QA system based on\nour dataset and find that the existing models struggle to solve such tricky\nquestions. CC-Riddle is now publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dependency Parsing with Backtracking using Deep Reinforcement Learning. (arXiv:2206.13914v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13914","description":"<p>Greedy algorithms for NLP such as transition based parsing are prone to error\npropagation. One way to overcome this problem is to allow the algorithm to\nbacktrack and explore an alternative solution in cases where new evidence\ncontradicts the solution explored so far. In order to implement such a\nbehavior, we use reinforcement learning and let the algorithm backtrack in\ncases where such an action gets a better reward than continuing to explore the\ncurrent solution. We test this idea on both POS tagging and dependency parsing\nand show that backtracking is an effective means to fight against error\npropagation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dary_F/0/1/0/all/0/1\">Franck Dary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petit_M/0/1/0/all/0/1\">Maxime Petit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasr_A/0/1/0/all/0/1\">Alexis Nasr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Range Language Modeling via Gated State Spaces. (arXiv:2206.13947v1 [cs.LG])","link":"http://arxiv.org/abs/2206.13947","description":"<p>State space models have shown to be effective at modeling long range\ndependencies, specially on sequence classification tasks. In this work we focus\non autoregressive sequence modeling over English books, Github source code and\nArXiv mathematics articles. Based on recent developments around the\neffectiveness of gated activation functions, we propose a new layer named Gated\nState Space (GSS) and show that it trains significantly faster than the\ndiagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several\nwell-tuned Transformer-based baselines and exhibits zero-shot generalization to\nlonger inputs while being straightforward to implement. Finally, we show that\nleveraging self-attention to model local dependencies improves the performance\nof GSS even further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1\">Ashok Cutkosky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Individual Conversational Volatility in Tandem Telecollaboration for Second Language Learning. (arXiv:2206.13965v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13965","description":"<p>Second language learning can be enabled by tandem collaboration where\nstudents are grouped into video conference calls while learning the native\nlanguage of other student(s) on the calls. This places students in an online\nenvironment where the more outgoing can actively contribute and engage in\ndialogue while those more shy and unsure of their second language skills can\nsit back and coast through the calls. We have built and deployed the L2L system\nwhich records timings of conversational utterances from all participants in a\ncall. We generate visualisations including participation rates and timelines\nfor each student in each call and present these on a dashboard. We have\nrecently developed a measure called personal conversational volatility for how\ndynamic has been each student's contribution to the dialogue in each call. We\npresent an analysis of conversational volatility measures for a sample of 19\nindividual English-speaking students from our University who are learning\nFrenchm, in each of 86 tandem telecollaboration calls over one teaching\nsemester. Our analysis shows there is a need to look into the nature of the\ninteractions and see if the choices of discussion topics assigned to them were\ntoo difficult for some students and that may have influenced their engagement\nin some way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1\">Alan F. Smeaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_Plissonneau_A/0/1/0/all/0/1\">Aparajita Dey-Plissonneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyowon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scriney_M/0/1/0/all/0/1\">Michael Scriney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MACSA: A Multimodal Aspect-Category Sentiment Analysis Dataset with Multimodal Fine-grained Aligned Annotations. (arXiv:2206.13969v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13969","description":"<p>Multimodal fine-grained sentiment analysis has recently attracted increasing\nattention due to its broad applications. However, the existing multimodal\nfine-grained sentiment datasets most focus on annotating the fine-grained\nelements in text but ignore those in images, which leads to the fine-grained\nelements in visual content not receiving the full attention they deserve. In\nthis paper, we propose a new dataset, the Multimodal Aspect-Category Sentiment\nAnalysis (MACSA) dataset, which contains more than 21K text-image pairs. The\ndataset provides fine-grained annotations for both textual and visual content\nand firstly uses the aspect category as the pivot to align the fine-grained\nelements between the two modalities. Based on our dataset, we propose the\nMultimodal ACSA task and a multimodal graph-based aligned model (MGAM), which\nadopts a fine-grained cross-modal fusion method. Experimental results show that\nour method can facilitate the baseline comparison for future research on this\ncorpus. We will make the dataset and code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Generator-Ranker Learning for Natural Language Generation. (arXiv:2206.13974v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13974","description":"<p>Due to exposure bias, most existing natural language generation (NLG) models\ntrained by maximizing the likelihood objective predict poor text results during\nthe inference stage. In this paper, to tackle this problem, we revisit the\ngenerate-then-rank framework and propose a joint generator-ranker (JGR)\ntraining algorithm for text generation tasks. In JGR, the generator model is\ntrained by maximizing two objectives: the likelihood of the training corpus and\nthe expected reward given by the ranker model. Meanwhile, the ranker model\ntakes input samples from the generator model and learns to distinguish good\nsamples from the generation pool. The generator and ranker models are\nalternately optimized till convergence. In the empirical study, the proposed\nJGR model achieves new state-of-the-art performance on five public benchmarks\ncovering three popular generation tasks: summarization, question generation,\nand response generation. We will make code, data, and models available at\nhttps://github.com/microsoft/AdvNLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Weizhou Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-enhanced Prototypical Network with Contrastive Learning for Multi-label Few-shot Aspect Category Detection. (arXiv:2206.13980v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13980","description":"<p>Multi-label aspect category detection allows a given review sentence to\ncontain multiple aspect categories, which is shown to be more practical in\nsentiment analysis and attracting increasing attention. As annotating large\namounts of data is time-consuming and labor-intensive, data scarcity occurs\nfrequently in real-world scenarios, which motivates multi-label few-shot aspect\ncategory detection. However, research on this problem is still in infancy and\nfew methods are available. In this paper, we propose a novel label-enhanced\nprototypical network (LPN) for multi-label few-shot aspect category detection.\nThe highlights of LPN can be summarized as follows. First, it leverages label\ndescription as auxiliary knowledge to learn more discriminative prototypes,\nwhich can retain aspect-relevant information while eliminating the harmful\neffect caused by irrelevant aspects. Second, it integrates with contrastive\nlearning, which encourages that the sentences with the same aspect label are\npulled together in embedding space while simultaneously pushing apart the\nsentences with different aspect labels. In addition, it introduces an adaptive\nmulti-label inference module to predict the aspect count in the sentence, which\nis simple yet effective. Extensive experimental results on three datasets\ndemonstrate that our proposed model LPN can consistently achieve\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaotong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Siyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Junjie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianchao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Ensemble for Fake News Detection: An attempt. (arXiv:2206.13981v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13981","description":"<p>Fake News Detection has been a challenging problem in the field of Machine\nLearning. Researchers have approached it via several techniques using old\nStatistical Classification models and modern Deep Learning. Today, with the\ngrowing amount of data, developments in the field of NLP and ML, and an\nincrease in the computation power at disposal, there are infinite permutations\nand combinations to approach this problem from a different perspective. In this\npaper, we try different methods to tackle Fake News, and try to build, and\npropose the possibilities of a Hybrid Ensemble combining the classical Machine\nLearning techniques with the modern Deep Learning Approaches\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_L/0/1/0/all/0/1\">Lovedeep Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Proposed Bi-LSTM Method to Fake News Detection. (arXiv:2206.13982v1 [cs.CL])","link":"http://arxiv.org/abs/2206.13982","description":"<p>Recent years have seen an explosion in social media usage, allowing people to\nconnect with others. Since the appearance of platforms such as Facebook and\nTwitter, such platforms influence how we speak, think, and behave. This problem\nnegatively undermines confidence in content because of the existence of fake\nnews. For instance, false news was a determining factor in influencing the\noutcome of the U.S. presidential election and other sites. Because this\ninformation is so harmful, it is essential to make sure we have the necessary\ntools to detect and resist it. We applied Bidirectional Long Short-Term Memory\n(Bi-LSTM) to determine if the news is false or real in order to showcase this\nstudy. A number of foreign websites and newspapers were used for data\ncollection. After creating &amp; running the model, the work achieved 84% model\naccuracy and 62.0 F1-macro scores with training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Taminul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosen_M/0/1/0/all/0/1\">MD Alamin Hosen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mony_A/0/1/0/all/0/1\">Akhi Mony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">MD Touhid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahan_I/0/1/0/all/0/1\">Israt Jahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_A/0/1/0/all/0/1\">Arindom Kundu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SINC: Service Information Augmented Open-Domain Conversation. (arXiv:2206.14000v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14000","description":"<p>Generative open-domain dialogue systems can benefit from external knowledge,\nbut the lack of external knowledge resources and the difficulty in finding\nrelevant knowledge limit the development of this technology. To this end, we\npropose a knowledge-driven dialogue task using dynamic service information.\nSpecifically, we use a large number of service APIs that can provide high\ncoverage and spatiotemporal sensitivity as external knowledge sources. The\ndialogue system generates queries to request external services along with user\ninformation, get the relevant knowledge, and generate responses based on this\nknowledge. To implement this method, we collect and release the first open\ndomain Chinese service knowledge dialogue dataset DuSinc. At the same time, we\nconstruct a baseline model PLATO-SINC, which realizes the automatic utilization\nof service information for dialogue. Both automatic evaluation and human\nevaluation show that our proposed new method can significantly improve the\neffect of open-domain conversation, and the session-level overall score in\nhuman evaluation is improved by 59.29% compared with the dialogue pre-training\nmodel PLATO-2. The dataset and benchmark model will be open sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenquan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhengyu Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Siqi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proton: Probing Schema Linking Information from Pre-trained Language Models for Text-to-SQL Parsing. (arXiv:2206.14017v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14017","description":"<p>The importance of building text-to-SQL parsers which can be applied to new\ndatabases has long been acknowledged, and a critical step to achieve this goal\nis schema linking, i.e., properly recognizing mentions of unseen columns or\ntables when generating SQLs. In this work, we propose a novel framework to\nelicit relational structures from large-scale pre-trained language models\n(PLMs) via a probing procedure based on Poincar\\'e distance metric, and use the\ninduced relations to augment current graph-based parsers for better schema\nlinking. Compared with commonly-used rule-based methods for schema linking, we\nfound that probing relations can robustly capture semantic correspondences,\neven when surface forms of mentions and entities differ. Moreover, our probing\nprocedure is entirely unsupervised and requires no additional parameters.\nExtensive experiments show that our framework sets new state-of-the-art\nperformance on three benchmarks. We empirically verify that our probing\nprocedure can indeed find desired relational structures through qualitative\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bowen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bengali Common Voice Speech Dataset for Automatic Speech Recognition. (arXiv:2206.14053v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14053","description":"<p>Bengali is one of the most spoken languages in the world with over 300\nmillion speakers globally. Despite its popularity, research into the\ndevelopment of Bengali speech recognition systems is hindered due to the lack\nof diverse open-source datasets. As a way forward, we have crowdsourced the\nBengali Common Voice Speech Dataset, which is a sentence-level automatic speech\nrecognition corpus. Collected on the Mozilla Common Voice platform, the dataset\nis part of an ongoing campaign that has led to the collection of over 400 hours\nof data in 2 months and is growing rapidly. Our analysis shows that our dataset\nhas more speaker, phoneme, and environmental diversity compared to the OpenSLR\nBengali ASR dataset, the largest existing open-source Bengali speech dataset.\nWe present insights obtained from the dataset and discuss key linguistic\nchallenges that need to be addressed in future versions. Additionally, we\nreport the current performance of a few Automatic Speech Recognition (ASR)\nalgorithms and set a benchmark for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Samiul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sushmit_A/0/1/0/all/0/1\">Asif Sushmit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_Z/0/1/0/all/0/1\">Zaowad Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakkhatra_S/0/1/0/all/0/1\">Shahrin Nakkhatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ansary_M/0/1/0/all/0/1\">MD. Nazmuddoha Ansary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossen_S/0/1/0/all/0/1\">Syed Mobassir Hossen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehnaz_S/0/1/0/all/0/1\">Sazia Morshed Mehnaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reasat_T/0/1/0/all/0/1\">Tahsin Reasat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humayun_A/0/1/0/all/0/1\">Ahmed Imtiaz Humayun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Lexical Gender Inference: A Scalable Methodology using Online Databases. (arXiv:2206.14055v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14055","description":"<p>This paper presents a new method for automatically detecting words with\nlexical gender in large-scale language datasets. Currently, the evaluation of\ngender bias in natural language processing relies on manually compiled lexicons\nof gendered expressions, such as pronouns ('he', 'she', etc.) and nouns with\nlexical gender ('mother', 'boyfriend', 'policewoman', etc.). However, manual\ncompilation of such lists can lead to static information if they are not\nperiodically updated and often involve value judgments by individual annotators\nand researchers. Moreover, terms not included in the list fall out of the range\nof analysis. To address these issues, we devised a scalable, dictionary-based\nmethod to automatically detect lexical gender that can provide a dynamic,\nup-to-date analysis with high coverage. Our approach reaches over 80% accuracy\nin determining the lexical gender of nouns retrieved randomly from a Wikipedia\nsample and when testing on a list of gendered words used in previous research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartl_M/0/1/0/all/0/1\">Marion Bartl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leavy_S/0/1/0/all/0/1\">Susan Leavy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Placing (Historical) Facts on a Timeline: A Classification cum Coref Resolution Approach. (arXiv:2206.14089v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14089","description":"<p>A timeline provides one of the most effective ways to visualize the important\nhistorical facts that occurred over a period of time, presenting the insights\nthat may not be so apparent from reading the equivalent information in textual\nform. By leveraging generative adversarial learning for important sentence\nclassification and by assimilating knowledge based tags for improving the\nperformance of event coreference resolution we introduce a two staged system\nfor event timeline generation from multiple (historical) text documents. We\ndemonstrate our results on two manually annotated historical text documents.\nOur results can be extremely helpful for historians, in advancing research in\nhistory and in understanding the socio-political landscape of a country as\nreflected in the writings of famous personas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adak_S/0/1/0/all/0/1\">Sayantan Adak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1\">Altaf Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Aditya Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplifying Dataflow Dialogue Design. (arXiv:2206.14125v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14125","description":"<p>In \\citep{andreas2020task-oriented}, a dataflow (DF) based dialogue system\nwas introduced, showing clear advantages compared to many commonly used current\nsystems. This was accompanied by the release of SMCalFlow, a practically\nrelevant, manually annotated dataset, more detailed and much larger than any\ncomparable dialogue dataset. Despite these remarkable contributions, the\ncommunity has not shown further interest in this direction. What are the\nreasons for this lack of interest? And how can the community be encouraged to\nengage in research in this direction?\n</p>\n<p>One explanation may be the perception that this approach is too complex -\nboth the the annotation and the system. This paper argues that this perception\nis wrong: 1) Suggestions for a simplified format for the annotation of the\ndataset are presented, 2) An implementation of the DF execution engine is\nreleased\\footnote{https://github.com/telepathylabsai/OpenDF}, which can serve\nas a sandbox allowing researchers to easily implement, and experiment with, new\nDF dialogue designs. The hope is that these contributions will help engage more\npractitioners in exploring new ideas and designs for DF based dialogue systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meron_J/0/1/0/all/0/1\">Joram Meron</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question Personalization in an Intelligent Tutoring System. (arXiv:2206.14145v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14145","description":"<p>This paper investigates personalization in the field of intelligent tutoring\nsystems (ITS). We hypothesize that personalization in the way questions are\nasked improves student learning outcomes. Previous work on dialogue-based ITS\npersonalization has yet to address question phrasing. We show that generating\nversions of the questions suitable for students at different levels of subject\nproficiency improves student learning gains, using variants written by a domain\nexpert and an experimental A/B test. This insight demonstrates that the\nlinguistic realization of questions in an ITS affects the learning outcomes for\nstudents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elkins_S/0/1/0/all/0/1\">Sabina Elkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belfer_R/0/1/0/all/0/1\">Robert Belfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochmar_E/0/1/0/all/0/1\">Ekaterina Kochmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serban_I/0/1/0/all/0/1\">Iulian Serban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie C.K. Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creation and Analysis of an International Corpus of Privacy Laws. (arXiv:2206.14169v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14169","description":"<p>The landscape of privacy laws and regulations around the world is complex and\never-changing. National and super-national laws, agreements, decrees, and other\ngovernment-issued rules form a patchwork that companies must follow to operate\ninternationally. To examine the status and evolution of this patchwork, we\nintroduce the Government Privacy Instructions Corpus, or GPI Corpus, of 1,043\nprivacy laws, regulations, and guidelines, covering 182 jurisdictions. This\ncorpus enables a large-scale quantitative and qualitative examination of legal\nfoci on privacy. We examine the temporal distribution of when GPIs were created\nand illustrate the dramatic increase in privacy legislation over the past 50\nyears, although a finer-grained examination reveals that the rate of increase\nvaries depending on the personal data types that GPIs address. Our exploration\nalso demonstrates that most privacy laws respectively address relatively few\npersonal data types, showing that comprehensive privacy legislation remains\nrare. Additionally, topic modeling results show the prevalence of common themes\nin GPIs, such as finance, healthcare, and telecommunications. Finally, we\nrelease the corpus to the research community to promote further study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poplavska_E/0/1/0/all/0/1\">Ellen Poplavska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OToole_N/0/1/0/all/0/1\">Nora O&#x27;Toole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norton_T/0/1/0/all/0/1\">Thomas Norton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeh_N/0/1/0/all/0/1\">Norman Sadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1\">Shomir Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The NLP Sandbox: an efficient model-to-data system to enable federated and unbiased evaluation of clinical NLP models. (arXiv:2206.14181v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14181","description":"<p>Objective The evaluation of natural language processing (NLP) models for\nclinical text de-identification relies on the availability of clinical notes,\nwhich is often restricted due to privacy concerns. The NLP Sandbox is an\napproach for alleviating the lack of data and evaluation frameworks for NLP\nmodels by adopting a federated, model-to-data approach. This enables unbiased\nfederated model evaluation without the need for sharing sensitive data from\nmultiple institutions. Materials and Methods We leveraged the Synapse\ncollaborative framework, containerization software, and OpenAPI generator to\nbuild the NLP Sandbox (nlpsandbox.io). We evaluated two state-of-the-art NLP\nde-identification focused annotation models, Philter and NeuroNER, using data\nfrom three institutions. We further validated model performance using data from\nan external validation site. Results We demonstrated the usefulness of the NLP\nSandbox through de-identification clinical model evaluation. The external\ndeveloper was able to incorporate their model into the NLP Sandbox template and\nprovide user experience feedback. Discussion We demonstrated the feasibility of\nusing the NLP Sandbox to conduct a multi-site evaluation of clinical text\nde-identification models without the sharing of data. Standardized model and\ndata schemas enable smooth model transfer and implementation. To generalize the\nNLP Sandbox, work is required on the part of data owners and model developers\nto develop suitable and standardized schemas and to adapt their data or model\nto fit the schemas. Conclusions The NLP Sandbox lowers the barrier to utilizing\nclinical data for NLP model evaluation and facilitates federated, multi-site,\nunbiased evaluation of NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Thomas Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muenzen_K/0/1/0/all/0/1\">Kathleen Muenzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyle_C/0/1/0/all/0/1\">Connor Boyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koslowski_G/0/1/0/all/0/1\">George Koslowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiaxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbins_N/0/1/0/all/0/1\">Nicholas Dobbins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essien_C/0/1/0/all/0/1\">Clement Essien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omberg_L/0/1/0/all/0/1\">Larsson Omberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yestigen_M/0/1/0/all/0/1\">Meliha Yestigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_B/0/1/0/all/0/1\">Bradley Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eddy_J/0/1/0/all/0/1\">James A Eddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guinney_J/0/1/0/all/0/1\">Justin Guinney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_S/0/1/0/all/0/1\">Sean Mooney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaffter_T/0/1/0/all/0/1\">Thomas Schaffter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cooperative Self-training of Machine Reading Comprehension. (arXiv:2103.07449v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07449","description":"<p>Pretrained language models have significantly improved the performance of\ndownstream language understanding tasks, including extractive question\nanswering, by providing high-quality contextualized word embeddings. However,\ntraining question answering models still requires large amounts of annotated\ndata for specific domains. In this work, we propose a cooperative self-training\nframework, RGX, for automatically generating more non-trivial question-answer\npairs to improve model performance. RGX is built upon a masked answer\nextraction task with an interactive learning environment containing an answer\nentity Recognizer, a question Generator, and an answer eXtractor. Given a\npassage with a masked entity, the generator generates a question around the\nentity, and the extractor is trained to extract the masked entity with the\ngenerated question and raw texts. The framework allows the training of question\ngeneration and answering models on any text corpora without annotation.\nExperiment results show that RGX outperforms the state-of-the-art (SOTA)\npretrained language models and transfer learning approaches on standard\nquestion-answering benchmarks, and yields the new SOTA performance under given\nmodel size and transfer learning settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hongyin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingye Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Seunghak Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark. (arXiv:2105.00071v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00071","description":"<p>Knowledge-grounded dialogue systems powered by large language models often\ngenerate responses that, while fluent, are not attributable to a relevant\nsource of information. Progress towards models that do not exhibit this issue\nrequires evaluation metrics that can quantify its prevalence. To this end, we\nintroduce the Benchmark for Evaluation of Grounded INteraction (BEGIN),\ncomprised of 12k dialogue turns generated by neural dialogue systems trained on\nthree knowledge-grounded dialogue corpora. We collect human annotations\nassessing the extent to which the models' responses can be attributed to the\ngiven background information. We then use BEGIN to analyze eight evaluation\nmetrics. We find that these metrics rely on spurious correlations, do not\nreliably distinguish attributable abstractive responses from unattributable\nones, and perform substantially worse when the knowledge source is longer. Our\nfindings underscore the need for more sophisticated and robust evaluation\nmetrics for knowledge-grounded dialogue. We make BEGIN publicly available at\nhttps://github.com/google/BEGIN-dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashkin_H/0/1/0/all/0/1\">Hannah Rashkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"gaBERT -- an Irish Language Model. (arXiv:2107.12930v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12930","description":"<p>The BERT family of neural language models have become highly popular due to\ntheir ability to provide sequences of text with rich context-sensitive token\nencodings which are able to generalise well to many NLP tasks. We introduce\ngaBERT, a monolingual BERT model for the Irish language. We compare our gaBERT\nmodel to multilingual BERT and the monolingual Irish WikiBERT, and we show that\ngaBERT provides better representations for a downstream parsing task. We also\nshow how different filtering criteria, vocabulary size and the choice of\nsubword tokenisation model affect downstream performance. We compare the\nresults of fine-tuning a gaBERT model with an mBERT model for the task of\nidentifying verbal multiword expressions, and show that the fine-tuned gaBERT\nmodel also performs better at this task. We release gaBERT and related code to\nthe community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barry_J/0/1/0/all/0/1\">James Barry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Joachim Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassidy_L/0/1/0/all/0/1\">Lauren Cassidy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowap_A/0/1/0/all/0/1\">Alan Cowap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynn_T/0/1/0/all/0/1\">Teresa Lynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walsh_A/0/1/0/all/0/1\">Abigail Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meachair_M/0/1/0/all/0/1\">M&#xed;che&#xe1;l J. &#xd3; Meachair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimpleTRON: Simple Transformer with O(N) Complexity. (arXiv:2111.15588v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.15588","description":"<p>In this paper, we propose that the dot product pairwise matching attention\nlayer, which is widely used in Transformer-based models, is redundant for the\nmodel performance. Attention, in its original formulation, has to be seen\nrather as a human-level tool to explore and/or visualize relevancy scores in\nsequential data. However, the way how it is constructed leads to significant\ncomputational complexity. Instead, we present SimpleTRON: Simple Transformer\nwith O(N) Complexity, a simple and fast alternative without any approximation\nthat, unlike other approximation models, does not have any architecture-related\noverhead and therefore can be seen as a purely linear Transformer-like model.\nThis architecture, to the best of our knowledge, outperforms existing\nsub-quadratic attention approximation models on several tasks from the\nLong-Range Arena benchmark. Moreover, we show, that SimpleTRON can benefit from\nweight transfer from pretrained large language models, as its parameters can be\nfully transferable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yorsh_U/0/1/0/all/0/1\">Uladzislau Yorsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalenko_A/0/1/0/all/0/1\">Alexander Kovalenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vancura_V/0/1/0/all/0/1\">Vojt&#x11b;ch Van&#x10d;ura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasata_D/0/1/0/all/0/1\">Daniel Va&#x161;ata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordik_P/0/1/0/all/0/1\">Pavel Kord&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolov_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Mikolov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Stop Asian Hate!\" : Refining Detection of Anti-Asian Hate Speech During the COVID-19 Pandemic. (arXiv:2112.02265v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.02265","description":"<p>Content warning: This work displays examples of explicit and/or strongly\noffensive language. Fueled by a surge of anti-Asian xenophobia and prejudice\nduring the COVID-19 pandemic, many have taken to social media to express these\nnegative sentiments. Identifying these posts is crucial for moderation and\nunderstanding the nature of hate in online spaces. In this paper, we create and\nannotate a corpus of tweets to explore anti-Asian hate speech with a finer\nlevel of granularity. Our analysis reveals that this emergent form of hate\nspeech often eludes established approaches. To address this challenge, we\ndevelop a model and an accompanied efficient training regimen that incorporates\nagreement between annotators. Our approach produces up to 8.8% improvement in\nmacro F1 scores over a strong established baseline, indicating its\neffectiveness even in settings where consensus among annotators is low. We\ndemonstrate that we are able to identify hate speech that is systematically\nmissed by established hate speech detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nghiem_H/0/1/0/all/0/1\">Huy Nghiem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting potentially harmful and protective suicide-related content on twitter: A machine learning approach. (arXiv:2112.04796v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.04796","description":"<p>Research shows that exposure to suicide-related news media content is\nassociated with suicide rates, with some content characteristics likely having\nharmful and others potentially protective effects. Although good evidence\nexists for a few selected characteristics, systematic large scale\ninvestigations are missing in general, and in particular for social media data.\nWe apply machine learning methods to classify large quantities of Twitter data\naccording to a novel annotation scheme that distinguishes 12 categories of\nsuicide-related tweets. We then trained a benchmark of machine learning models\nincluding a majority classifier, an approach based on word frequency (TF-IDF\nwith a linear SVM) and two state-of-the-art deep learning models (BERT, XLNet).\nThe two deep learning models achieved the best performance in two\nclassification tasks: In the first task, we classified six main content\ncategories, including personal stories about either suicidal ideation and\nattempts or coping, calls for action intending to spread either problem\nawareness or prevention-related information, reporting of suicide cases, and\nother tweets irrelevant to these categories. The deep learning models reached\naccuracy scores above 73% on average across the six categories, and F1-scores\nin between 0.70 and 0.85 for all but the suicidal ideation and attempts\ncategory (0.51-0.55). In the second task, separating tweets referring to actual\nsuicide from off-topic tweets, they correctly labeled around 88% of tweets,\nwith BERT achieving F1-scores of 0.93 and 0.74 for the two categories,\nrespectively. These classification performances are comparable to the\nstate-of-the-art on similar tasks. By making data labeling more efficient, this\nwork has enabled large-scale investigations on harmful and protective\nassociations of social media content with suicide rates and help-seeking\nbehavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Metzler_H/0/1/0/all/0/1\">Hannah Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baginski_H/0/1/0/all/0/1\">Hubert Baginski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niederkrotenthaler_T/0/1/0/all/0/1\">Thomas Niederkrotenthaler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_D/0/1/0/all/0/1\">David Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do Large Language Models Learn about Scripts?. (arXiv:2112.13834v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.13834","description":"<p>Script Knowledge (Schank and Abelson, 1975) has long been recognized as\ncrucial for language understanding as it can help in filling in unstated\ninformation in a narrative. However, such knowledge is expensive to produce\nmanually and difficult to induce from text due to reporting bias (Gordon and\nVan Durme, 2013). In this work, we are interested in the scientific question of\nwhether explicit script knowledge is present and accessible through pre-trained\ngenerative language models (LMs). To this end, we introduce the task of\ngenerating full event sequence descriptions (ESDs) given a scenario in the form\nof natural language prompts. In zero-shot probing experiments, we find that\ngenerative LMs produce poor ESDs with mostly omitted, irrelevant, repeated or\nmisordered events. To address this, we propose a pipeline-based script\ninduction framework (SIF) which can generate good quality ESDs for unseen\nscenarios (e.g., bake a cake). SIF is a two-staged framework that fine-tunes LM\non a small set of ESD examples in the first stage. In the second stage, ESD\ngenerated for an unseen scenario is post-processed using RoBERTa-based models\nto filter irrelevant events, remove repetitions, and reorder the temporally\nmisordered events. Through automatic and manual evaluations, we demonstrate\nthat SIF yields substantial improvements ($1$-$3$ BLUE points) over a\nfine-tuned LM. However, manual analysis shows that there is great room for\nimprovement, offering a new research direction for inducing script knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sancheti_A/0/1/0/all/0/1\">Abhilasha Sancheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1\">Rachel Rudinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Semantic Embeddings for Ontology Subsumption Prediction. (arXiv:2202.09791v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.09791","description":"<p>Automating ontology curation is a crucial task in knowledge engineering.\nPrediction by machine learning techniques such as semantic embedding is a\npromising direction, but the relevant research is still preliminary. In this\npaper, we present a class subsumption prediction method named BERTSubs, which\nuses the pre-trained language model BERT to compute contextual embeddings of\nthe class labels and customized input templates to incorporate contexts of\nsurrounding classes. The evaluation on two large-scale real-world ontologies\nhas shown its better performance than the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_Ruiz_E/0/1/0/all/0/1\">Ernesto Jimenez-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-constraint Optimal Transport for Entity Alignment with Dangling Cases. (arXiv:2203.05744v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05744","description":"<p>Entity alignment (EA) merges knowledge graphs (KGs) by identifying the\nequivalent entities in different graphs, which can effectively enrich knowledge\nrepresentations of KGs. However, in practice, different KGs often include\ndangling entities whose counterparts cannot be found in the other graph, which\nlimits the performance of EA methods. To improve EA with dangling entities, we\npropose an unsupervised method called Semi-constraint Optimal Transport for\nEntity Alignment in Dangling cases (SoTead). Our main idea is to model the\nentity alignment between two KGs as an optimal transport problem from one KG's\nentities to the others. First, we set pseudo entity pairs between KGs based on\npretrained word embeddings. Then, we conduct contrastive metric learning to\nobtain the transport cost between each entity pair. Finally, we introduce a\nvirtual entity for each KG to \"align\" the dangling entities from the other KGs,\nwhich relaxes the optimization constraints and leads to a semi-constraint\noptimal transport. In the experimental part, we first show the superiority of\nSoTead on a commonly-used entity alignment dataset. Besides, to analyze the\nability for dangling entity detection with other baselines, we construct a\nmedical cross-lingual knowledge graph dataset, MedED, where our SoTead also\nreaches state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengxuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot Filling. (arXiv:2203.12940v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12940","description":"<p>Zero-shot slot filling has received considerable attention to cope with the\nproblem of limited available data for the target domain. One of the important\nfactors in zero-shot learning is to make the model learn generalized and\nreliable representations. For this purpose, we present mcBERT, which stands for\nmomentum contrastive learning with BERT, to develop a robust zero-shot slot\nfilling model. mcBERT uses BERT to initialize the two encoders, the query\nencoder and key encoder, and is trained by applying momentum contrastive\nlearning. Our experimental results on the SNIPS benchmark show that mcBERT\nsubstantially outperforms the previous models, recording a new\nstate-of-the-art. Besides, we also show that each component composing mcBERT\ncontributes to the performance improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_S/0/1/0/all/0/1\">Seong-Hwan Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">WonKee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Hyeok Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation. (arXiv:2203.13339v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13339","description":"<p>End-to-end speech-to-speech translation (S2ST) without relying on\nintermediate text representations is a rapidly emerging frontier of research.\nRecent works have demonstrated that the performance of such direct S2ST systems\nis approaching that of conventional cascade S2ST when trained on comparable\ndatasets. However, in practice, the performance of direct S2ST is bounded by\nthe availability of paired S2ST training data. In this work, we explore\nmultiple approaches for leveraging much more widely available unsupervised and\nweakly-supervised speech and text data to improve the performance of direct\nS2ST based on Translatotron 2. With our most effective approaches, the average\ntranslation quality of direct S2ST on 21 language pairs on the CVSS-C corpus is\nimproved by +13.6 BLEU (or +113% relatively), as compared to the previous\nstate-of-the-art trained without additional data. The improvements on\nlow-resource language are even more significant (+398% relatively on average).\nOur comparative studies suggest future research directions for S2ST and speech\nrepresentation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morioka_N/0/1/0/all/0/1\">Nobuyuki Morioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Rare Word Recognition with LM-aware MWER Training. (arXiv:2204.07553v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07553","description":"<p>Language models (LMs) significantly improve the recognition accuracy of\nend-to-end (E2E) models on words rarely seen during training, when used in\neither the shallow fusion or the rescoring setups. In this work, we introduce\nLMs in the learning of hybrid autoregressive transducer (HAT) models in the\ndiscriminative training framework, to mitigate the training versus inference\ngap regarding the use of LMs. For the shallow fusion setup, we use LMs during\nboth hypotheses generation and loss computation, and the LM-aware MWER-trained\nmodel achieves 10\\% relative improvement over the model trained with standard\nMWER on voice search test sets containing rare words. For the rescoring setup,\nwe learn a small neural module to generate per-token fusion weights in a\ndata-dependent manner. This model achieves the same rescoring WER as regular\nMWER-trained model, but without the need for sweeping fusion weights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tongzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Variani_E/0/1/0/all/0/1\">Ehsan Variani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_N/0/1/0/all/0/1\">Neeraj Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavandadi_S/0/1/0/all/0/1\">Sepand Mavandadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyser_C/0/1/0/all/0/1\">Cal Peyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybach_D/0/1/0/all/0/1\">David Rybach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm Detection Using Generative-based and Mutation-based Data Augmentation. (arXiv:2204.08198v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08198","description":"<p>Sarcasm is a term that refers to the use of words to mock, irritate, or amuse\nsomeone. It is commonly used on social media. The metaphorical and creative\nnature of sarcasm presents a significant difficulty for sentiment analysis\nsystems based on affective computing. The methodology and results of our team,\nUTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in\nthis paper. We put different models, and data augmentation approaches to the\ntest and report on which one works best. The tests begin with traditional\nmachine learning models and progress to transformer-based and attention-based\nmodels. We employed data augmentation based on data mutation and data\ngeneration. Using RoBERTa and mutation-based data augmentation, our best\napproach achieved an F1-sarcastic of 0.38 in the competition's evaluation\nphase. After the competition, we fixed our model's flaws and achieved an\nF1-sarcastic of 0.414.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasouli_A/0/1/0/all/0/1\">Arash Rasouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeraati_T/0/1/0/all/0/1\">Tanin Zeraati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahrak_B/0/1/0/all/0/1\">Behnam Bahrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Unintended Memorization in Language-Model-Fused ASR. (arXiv:2204.09606v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09606","description":"<p>End-to-end (E2E) models are often being accompanied by language models (LMs)\nvia shallow fusion for boosting their overall quality as well as recognition of\nrare words. At the same time, several prior works show that LMs are susceptible\nto unintentionally memorizing rare or unique sequences in the training data. In\nthis work, we design a framework for detecting memorization of random textual\nsequences (which we call canaries) in the LM training data when one has only\nblack-box (query) access to LM-fused speech recognizer, as opposed to direct\naccess to the LM. On a production-grade Conformer RNN-T E2E model fused with a\nTransformer LM, we show that detecting memorization of singly-occurring\ncanaries from the LM training data of 300M examples is possible. Motivated to\nprotect privacy, we also show that such memorization gets significantly reduced\nby per-example gradient-clipped LM training without compromising overall\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Steve Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_O/0/1/0/all/0/1\">Om Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathews_R/0/1/0/all/0/1\">Rajiv Mathews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Neural Open Information Extraction: Current Status and Future Directions. (arXiv:2205.11725v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11725","description":"<p>Open Information Extraction (OpenIE) facilitates domain-independent discovery\nof relational facts from large corpora. The technique well suits many\nopen-world natural language understanding scenarios, such as automatic\nknowledge base construction, open-domain question answering, and explicit\nreasoning. Thanks to the rapid development in deep learning technologies,\nnumerous neural OpenIE architectures have been proposed and achieve\nconsiderable performance improvement. In this survey, we provide an extensive\noverview of the-state-of-the-art neural OpenIE models, their key design\ndecisions, strengths and weakness. Then, we discuss limitations of current\nsolutions and the open issues in OpenIE problem itself. Finally we list recent\ntrends that could help expand its scope and applicability, setting up promising\ndirections for future research in OpenIE. To our best knowledge, this paper is\nthe first review on this specific topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shaowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Cheng Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"AI-based computer-aided diagnostic system of chest digital tomography synthesis: Demonstrating comparative advantage with X-ray-based AI systems. (arXiv:2206.13504v1 [eess.IV])","link":"http://arxiv.org/abs/2206.13504","description":"<p>Compared with chest X-ray (CXR) imaging, which is a single image projected\nfrom the front of the patient, chest digital tomosynthesis (CDTS) imaging can\nbe more advantageous for lung lesion detection because it acquires multiple\nimages projected from multiple angles of the patient. Various clinical\ncomparative analysis and verification studies have been reported to demonstrate\nthis, but there were no artificial intelligence (AI)-based comparative analysis\nstudies. Existing AI-based computer-aided detection (CAD) systems for lung\nlesion diagnosis have been developed mainly based on CXR images; however,\nCAD-based on CDTS, which uses multi-angle images of patients in various\ndirections, has not been proposed and verified for its usefulness compared to\nCXR-based counterparts. This study develops/tests a CDTS-based AI CAD system to\ndetect lung lesions to demonstrate performance improvements compared to\nCXR-based AI CAD. We used multiple projection images as input for the\nCDTS-based AI model and a single-projection image as input for the CXR-based AI\nmodel to fairly compare and evaluate the performance between models. The\nproposed CDTS-based AI CAD system yielded sensitivities of 0.782 and 0.785 and\naccuracies of 0.895 and 0.837 for the performance of detecting tuberculosis and\npneumonia, respectively, against normal subjects. These results show higher\nperformance than sensitivities of 0.728 and 0.698 and accuracies of 0.874 and\n0.826 for detecting tuberculosis and pneumonia through the CXR-based AI CAD,\nwhich only uses a single projection image in the frontal direction. We found\nthat CDTS-based AI CAD improved the sensitivity of tuberculosis and pneumonia\nby 5.4% and 8.7% respectively, compared to CXR-based AI CAD without loss of\naccuracy. Therefore, we comparatively prove that CDTS-based AI CAD technology\ncan improve performance more than CXR, enhancing the clinical applicability of\nCDTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Su Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Ju Hwan Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oh_S/0/1/0/all/0/1\">Seong Je Oh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_M/0/1/0/all/0/1\">Myung Jin Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-Based Defect Classification and Detection in SEM Images. (arXiv:2206.13505v1 [eess.IV])","link":"http://arxiv.org/abs/2206.13505","description":"<p>This proposes a novel ensemble deep learning-based model to accurately\nclassify, detect and localize different defect categories for aggressive\npitches and thin resists (High NA applications).In particular, we train\nRetinaNet models using different ResNet, VGGNet architectures as backbone and\npresent the comparison between the accuracies of these models and their\nperformance analysis on SEM images with different types of defect patterns such\nas bridge, break and line collapses. Finally, we propose a preference-based\nensemble strategy to combine the output predictions from different models in\norder to achieve better performance on classification and detection of defects.\nAs CDSEM images inherently contain a significant level of noise, detailed\nfeature information is often shadowed by noise. For certain resist profiles,\nthe challenge is also to differentiate between a microbridge, footing, break,\nand zones of probable breaks. Therefore, we have applied an unsupervised\nmachine learning model to denoise the SEM images to remove the False-Positive\ndefects and optimize the effect of stochastic noise on structured pixels for\nbetter metrology and enhanced defect inspection. We repeated the defect\ninspection step with the same trained model and performed a comparative\nanalysis for \"robustness\" and \"accuracy\" metric with conventional approach for\nboth noisy/denoised image pair. The proposed ensemble method demonstrates\nimprovement of the average precision metric (mAP) of the most difficult defect\nclasses. In this work we have developed a novel robust supervised deep learning\ntraining scheme to accurately classify as well as localize different defect\ntypes in SEM images with high degree of accuracy. Our proposed approach\ndemonstrates its effectiveness both quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Deya_B/0/1/0/all/0/1\">Bappaditya Deya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goswamif_D/0/1/0/all/0/1\">Dipam Goswamif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haldera_S/0/1/0/all/0/1\">Sandip Haldera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khalilb_K/0/1/0/all/0/1\">Kasem Khalilb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leraya_P/0/1/0/all/0/1\">Philippe Leraya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bayoumi_M/0/1/0/all/0/1\">Magdy A. Bayoumi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Recovery Based on A Novel Non-convex Function Minimax Logarithmic Concave Penalty Function. (arXiv:2206.13506v1 [eess.IV])","link":"http://arxiv.org/abs/2206.13506","description":"<p>Non-convex relaxation methods have been widely used in tensor recovery\nproblems, and compared with convex relaxation methods, can achieve better\nrecovery results. In this paper, a new non-convex function, Minimax Logarithmic\nConcave Penalty (MLCP) function, is proposed, and some of its intrinsic\nproperties are analyzed, among which it is interesting to find that the\nLogarithmic function is an upper bound of the MLCP function. The proposed\nfunction is generalized to tensor cases, yielding tensor MLCP and weighted\ntensor $L\\gamma$-norm. Consider that its explicit solution cannot be obtained\nwhen applying it directly to the tensor recovery problem. Therefore, the\ncorresponding equivalence theorems to solve such problem are given, namely,\ntensor equivalent MLCP theorem and equivalent weighted tensor $L\\gamma$-norm\ntheorem. In addition, we propose two EMLCP-based models for classic tensor\nrecovery problems, namely low-rank tensor completion (LRTC) and tensor robust\nprincipal component analysis (TRPCA), and design proximal alternate\nlinearization minimization (PALM) algorithms to solve them individually.\nFurthermore, based on the Kurdyka-{\\L}ojasiwicz property, it is proved that the\nsolution sequence of the proposed algorithm has finite length and converges to\nthe critical point globally. Finally, Extensive experiments show that proposed\nalgorithm achieve good results, and it is confirmed that the MLCP function is\nindeed better than the Logarithmic function in the minimization problem, which\nis consistent with the analysis of theoretical properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_H/0/1/0/all/0/1\">Hongtao Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yajing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_X/0/1/0/all/0/1\">Xinyun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Image-to-Video Transfer Learning. (arXiv:2206.13559v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13559","description":"<p>Capitalizing on large pre-trained models for various downstream tasks of\ninterest have recently emerged with promising performance. Due to the\never-growing model size, the standard full fine-tuning based task adaptation\nstrategy becomes prohibitively costly in terms of model training and storage.\nThis has led to a new research direction in parameter-efficient transfer\nlearning. However, existing attempts typically focus on downstream tasks from\nthe same modality (e.g., image understanding) of the pre-trained model. This\ncreates a limit because in some specific modalities, (e.g., video\nunderstanding) such a strong pre-trained model with sufficient knowledge is\nless or not available. In this work, we investigate such a novel cross-modality\ntransfer learning setting, namely parameter-efficient image-to-video transfer\nlearning. To solve this problem, we propose a new Spatio-Temporal Adapter\n(ST-Adapter) for parameter-efficient fine-tuning per video task. With a\nbuilt-in spatio-temporal reasoning capability in a compact design, ST-Adapter\nenables a pre-trained image model without temporal knowledge to reason about\ndynamic video content at a small (~8%) per-task parameter cost, requiring\napproximately 20 times fewer updated parameters compared to previous work.\nExtensive experiments on video action recognition tasks show that our\nST-Adapter can match or even outperform the strong full fine-tuning strategy\nand state-of-the-art video models, whilst enjoying the advantage of parameter\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junting Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Ziyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A View Independent Classification Framework for Yoga Postures. (arXiv:2206.13577v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13577","description":"<p>Yoga is a globally acclaimed and widely recommended practice for a healthy\nliving. Maintaining correct posture while performing a Yogasana is of utmost\nimportance. In this work, we employ transfer learning from Human Pose\nEstimation models for extracting 136 key-points spread all over the body to\ntrain a Random Forest classifier which is used for estimation of the Yogasanas.\nThe results are evaluated on an in-house collected extensive yoga video\ndatabase of 51 subjects recorded from 4 different camera angles. We propose a 3\nstep scheme for evaluating the generalizability of a Yoga classifier by testing\nit on 1) unseen frames, 2) unseen subjects, and 3) unseen camera angles. We\nargue that for most of the applications, validation accuracies on unseen\nsubjects and unseen camera angles would be most important. We empirically\nanalyze over three public datasets, the advantage of transfer learning and the\npossibilities of target leakage. We further demonstrate that the classification\naccuracies critically depend on the cross validation method employed and can\noften be misleading. To promote further research, we have made key-points\ndataset and code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chasmai_M/0/1/0/all/0/1\">Mustafa Chasmai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1\">Nirjhar Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_A/0/1/0/all/0/1\">Aman Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1\">Rahul Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuRIS: Neural Reconstruction of Indoor Scenes Using Normal Priors. (arXiv:2206.13597v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13597","description":"<p>Reconstructing 3D indoor scenes from 2D images is an important task in many\ncomputer vision and graphics applications. A main challenge in this task is\nthat large texture-less areas in typical indoor scenes make existing methods\nstruggle to produce satisfactory reconstruction results. We propose a new\nmethod, named NeuRIS, for high quality reconstruction of indoor scenes. The key\nidea of NeuRIS is to integrate estimated normal of indoor scenes as a prior in\na neural rendering framework for reconstructing large texture-less shapes and,\nimportantly, to do this in an adaptive manner to also enable the reconstruction\nof irregular shapes with fine details. Specifically, we evaluate the\nfaithfulness of the normal priors on-the-fly by checking the multi-view\nconsistency of reconstruction during the optimization process. Only the normal\npriors accepted as faithful will be utilized for 3D reconstruction, which\ntypically happens in the regions of smooth shapes possibly with weak texture.\nHowever, for those regions with small objects or thin structures, for which the\nnormal priors are usually unreliable, we will only rely on visual features of\nthe input images, since such regions typically contain relatively rich visual\nfeatures (e.g., shade changes and boundary contours). Extensive experiments\nshow that NeuRIS significantly outperforms the state-of-the-art methods in\nterms of reconstruction quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiepeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1\">Xiaoxiao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Annotation Need in Self-Explanatory Models for Lung Nodule Diagnosis. (arXiv:2206.13608v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13608","description":"<p>Feature-based self-explanatory methods explain their classification in terms\nof human-understandable features. In the medical imaging community, this\nsemantic matching of clinical knowledge adds significantly to the\ntrustworthiness of the AI. However, the cost of additional annotation of\nfeatures remains a pressing issue. We address this problem by proposing\ncRedAnno, a data-/annotation-efficient self-explanatory approach for lung\nnodule diagnosis. cRedAnno considerably reduces the annotation need by\nintroducing self-supervised contrastive learning to alleviate the burden of\nlearning most parameters from annotation, replacing end-to-end training with\ntwo-stage training. When training with hundreds of nodule samples and only 1%\nof their annotations, cRedAnno achieves competitive accuracy in predicting\nmalignancy, meanwhile significantly surpassing most previous works in\npredicting nodule attributes. Visualisation of the learned space further\nindicates that the correlation between the clustering of malignancy and nodule\nattributes coincides with clinical knowledge. Our complete code is open-source\navailable: https://github.com/ludles/credanno.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiahao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erleben_K/0/1/0/all/0/1\">Kenny Erleben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_M/0/1/0/all/0/1\">Michael Bachmann Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1\">Sune Darkner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible-Rate Learned Hierarchical Bi-Directional Video Compression With Motion Refinement and Frame-Level Bit Allocation. (arXiv:2206.13613v1 [eess.IV])","link":"http://arxiv.org/abs/2206.13613","description":"<p>This paper presents improvements and novel additions to our recent work on\nend-to-end optimized hierarchical bi-directional video compression to further\nadvance the state-of-the-art in learned video compression. As an improvement,\nwe combine motion estimation and prediction modules and compress refined\nresidual motion vectors for improved rate-distortion performance. As novel\naddition, we adapted the gain unit proposed for image compression to\nflexible-rate video compression in two ways: first, the gain unit enables a\nsingle encoder model to operate at multiple rate-distortion operating points;\nsecond, we exploit the gain unit to control bit allocation among intra-coded\nvs. bi-directionally coded frames by fine tuning corresponding models for truly\nflexible-rate learned video coding. Experimental results demonstrate that we\nobtain state-of-the-art rate-distortion performance exceeding those of all\nprior art in learned video coding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cetin_E/0/1/0/all/0/1\">Eren Cetin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_M/0/1/0/all/0/1\">M. Akin Yilmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tekalp_A/0/1/0/all/0/1\">A. Murat Tekalp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch Selection for Melanoma Classification. (arXiv:2206.13626v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13626","description":"<p>In medical image processing, the most important information is often located\non small parts of the image. Patch-based approaches aim at using only the most\nrelevant parts of the image. Finding ways to automatically select the patches\nis a challenge. In this paper, we investigate two criteria to choose patches:\nentropy and a spectral similarity criterion. We perform experiments at\ndifferent levels of patch size. We train a Convolutional Neural Network on the\nsubsets of patches and analyze the training time. We find that, in addition to\nrequiring less preprocessing time, the classifiers trained on the datasets of\npatches selected based on entropy converge faster than on those selected based\non the spectral similarity criterion and, furthermore, lead to higher accuracy.\nMoreover, patches of high entropy lead to faster convergence and better\naccuracy than patches of low entropy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lachaud_G/0/1/0/all/0/1\">Guillaume Lachaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conde_Cespedes_P/0/1/0/all/0/1\">Patricia Conde-Cespedes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trocan_M/0/1/0/all/0/1\">Maria Trocan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale Network with Attentional Multi-resolution Fusion for Point Cloud Semantic Segmentation. (arXiv:2206.13628v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13628","description":"<p>In this paper, we present a comprehensive point cloud semantic segmentation\nnetwork that aggregates both local and global multi-scale information. First,\nwe propose an Angle Correlation Point Convolution (ACPConv) module to\neffectively learn the local shapes of points. Second, based upon ACPConv, we\nintroduce a local multi-scale split (MSS) block that hierarchically connects\nfeatures within one single block and gradually enlarges the receptive field\nwhich is beneficial for exploiting the local context. Third, inspired by HRNet\nwhich has excellent performance on 2D image vision tasks, we build an HRNet\ncustomized for point cloud to learn global multi-scale context. Lastly, we\nintroduce a point-wise attention fusion approach that fuses multi-resolution\npredictions and further improves point cloud semantic segmentation performance.\nOur experimental results and ablations on several benchmark datasets show that\nour proposed method is effective and able to achieve state-of-the-art\nperformances compared to existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Ye Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward an ImageNet Library of Functions for Global Optimization Benchmarking. (arXiv:2206.13630v1 [cs.AI])","link":"http://arxiv.org/abs/2206.13630","description":"<p>Knowledge of search-landscape features of BlackBox Optimization (BBO)\nproblems offers valuable information in light of the Algorithm Selection and/or\nConfiguration problems. Exploratory Landscape Analysis (ELA) models have gained\nsuccess in identifying predefined human-derived features and in facilitating\nportfolio selectors to address those challenges. Unlike ELA approaches, the\ncurrent study proposes to transform the identification problem into an image\nrecognition problem, with a potential to detect conception-free, machine-driven\nlandscape features. To this end, we introduce the notion of Landscape Images,\nwhich enables us to generate imagery instances per a benchmark function, and\nthen target the classification challenge over a diverse generalized dataset of\nfunctions. We address it as a supervised multi-class image recognition problem\nand apply basic artificial neural network models to solve it. The efficacy of\nour approach is numerically validated on the noise free BBOB and IOHprofiler\nbenchmarking suites. This evident successful learning is another step toward\nautomated feature extraction and local structure deduction of BBO problems. By\nusing this definition of landscape images, and by capitalizing on existing\ncapabilities of image recognition algorithms, we foresee the construction of an\nImageNet-like library of functions for training generalized detectors that rely\non machine-driven features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yazmir_B/0/1/0/all/0/1\">Boris Yazmir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shir_O/0/1/0/all/0/1\">Ofer M. Shir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omni-Seg+: A Scale-aware Dynamic Network for Pathological Image Segmentation. (arXiv:2206.13632v1 [eess.IV])","link":"http://arxiv.org/abs/2206.13632","description":"<p>Comprehensive semantic segmentation on renal pathological images is\nchallenging due to the heterogeneous scales of the objects. For example, on a\nwhole slide image (WSI), the cross-sectional areas of glomeruli can be 64 times\nlarger than that of the peritubular capillaries, making it impractical to\nsegment both objects on the same patch, at the same scale. To handle this\nscaling issue, prior studies have typically trained multiple segmentation\nnetworks in order to match the optimal pixel resolution of heterogeneous tissue\ntypes. This multi-network solution is resource-intensive and fails to model the\nspatial relationship between tissue types. In this paper, we propose the\nOmni-Seg+ network, a scale-aware dynamic neural network that achieves\nmulti-object (six tissue types) and multi-scale (5X to 40X scale) pathological\nimage segmentation via a single neural network. The contribution of this paper\nis three-fold: (1) a novel scale-aware controller is proposed to generalize the\ndynamic neural network from single-scale to multi-scale; (2) semi-supervised\nconsistency regularization of pseudo-labels is introduced to model the\ninter-scale correlation of unannotated tissue types into a single end-to-end\nlearning paradigm; and (3) superior scale-aware generalization is evidenced by\ndirectly applying a model trained on human kidney images to mouse kidney\nimages, without retraining. By learning from ~150,000 human pathological image\npatches from six tissue types at three different resolutions, our approach\nachieved superior segmentation performance according to human visual assessment\nand evaluation of image-omics (i.e., spatial transcriptomics). The official\nimplementation is available at https://github.com/ddrrnn123/Omni-Seg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Deng_R/0/1/0/all/0/1\">Ruining Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_T/0/1/0/all/0/1\">Tianyuan Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_J/0/1/0/all/0/1\">Jun Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asad_Z/0/1/0/all/0/1\">Zuhayr Asad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Womick_R/0/1/0/all/0/1\">R. Michael Womick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheyu Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fogo_A/0/1/0/all/0/1\">Agnes B. Fogo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Shilin Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Haichun Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Refinement to Improve High Resolution Image Inpainting. (arXiv:2206.13644v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13644","description":"<p>In this paper, we address the problem of degradation in inpainting quality of\nneural networks operating at high resolutions. Inpainting networks are often\nunable to generate globally coherent structures at resolutions higher than\ntheir training set. This is partially attributed to the receptive field\nremaining static, despite an increase in image resolution. Although downscaling\nthe image prior to inpainting produces coherent structure, it inherently lacks\ndetail present at higher resolutions. To get the best of both worlds, we\noptimize the intermediate featuremaps of a network by minimizing a multiscale\nconsistency loss at inference. This runtime optimization improves the\ninpainting results and establishes a new state-of-the-art for high resolution\ninpainting. Code is available at:\nhttps://github.com/geomagical/lama-with-refiner/tree/refinement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulshreshtha_P/0/1/0/all/0/1\">Prakhar Kulshreshtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugh_B/0/1/0/all/0/1\">Brian Pugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiddi_S/0/1/0/all/0/1\">Salma Jiddi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels. (arXiv:2206.13673v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13673","description":"<p>Event cameras continue to attract interest due to desirable characteristics\nsuch as high dynamic range, low latency, virtually no motion blur, and high\nenergy efficiency. One of the potential applications of event camera research\nlies in visual place recognition for robot localization, where a query\nobservation has to be matched to the corresponding reference place in the\ndatabase. In this letter, we explore the distinctiveness of event streams from\na small subset of pixels (in the tens or hundreds). We demonstrate that the\nabsolute difference in the number of events at those pixel locations\naccumulated into event frames can be sufficient for the place recognition task,\nwhen pixels that display large variations in the reference set are used. Using\nsuch sparse (over image coordinates) but varying (variance over the number of\nevents per pixel location) pixels enables frequent and computationally cheap\nupdates of the location estimates. Furthermore, when event frames contain a\nconstant number of events, our method takes full advantage of the event-driven\nnature of the sensory stream and displays promising robustness to changes in\nvelocity. We evaluate our proposed approach on the Brisbane-Event-VPR dataset\nin an outdoor driving scenario, as well as the newly contributed indoor\nQCR-Event-VPR dataset that was captured with a DAVIS346 camera mounted on a\nmobile robotic platform. Our results show that our approach achieves\ncompetitive performance when compared to several baseline methods on those\ndatasets, and is particularly well suited for compute- and energy-constrained\nplatforms such as interplanetary rovers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1\">Tobias Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Global-Scale Crowd+AI Techniques to Map and Assess Sidewalks for People with Disabilities. (arXiv:2206.13677v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13677","description":"<p>There is a lack of data on the location, condition, and accessibility of\nsidewalks across the world, which not only impacts where and how people travel\nbut also fundamentally limits interactive mapping tools and urban analytics. In\nthis paper, we describe initial work in semi-automatically building a sidewalk\nnetwork topology from satellite imagery using hierarchical multi-scale\nattention models, inferring surface materials from street-level images using\nactive learning-based semantic segmentation, and assessing sidewalk condition\nand accessibility features using Crowd+AI. We close with a call to create a\ndatabase of labeled satellite and streetscape scenes for sidewalks and sidewalk\naccessibility issues along with standardized benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Maryam Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saugstad_M/0/1/0/all/0/1\">Mikey Saugstad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miranda_F/0/1/0/all/0/1\">Fabio Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevtsuk_A/0/1/0/all/0/1\">Andres Sevtsuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1\">Claudio T. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Froehlich_J/0/1/0/all/0/1\">Jon E. Froehlich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POEM: Out-of-Distribution Detection with Posterior Sampling. (arXiv:2206.13687v1 [cs.LG])","link":"http://arxiv.org/abs/2206.13687","description":"<p>Out-of-distribution (OOD) detection is indispensable for machine learning\nmodels deployed in the open world. Recently, the use of an auxiliary outlier\ndataset during training (also known as outlier exposure) has shown promising\nperformance. As the sample space for potential OOD data can be prohibitively\nlarge, sampling informative outliers is essential. In this work, we propose a\nnovel posterior sampling-based outlier mining framework, POEM, which\nfacilitates efficient use of outlier data and promotes learning a compact\ndecision boundary between ID and OOD data for improved detection. We show that\nPOEM establishes state-of-the-art performance on common benchmarks. Compared to\nthe current best method that uses a greedy sampling strategy, POEM improves the\nrelative performance by 42.0% and 24.2% (FPR95) on CIFAR-10 and CIFAR-100,\nrespectively. We further provide theoretical insights on the effectiveness of\nPOEM for OOD detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yifei Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Ying Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Third Place Solution for CVPR2022 AVA Accessibility Vision and Autonomy Challenge. (arXiv:2206.13718v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13718","description":"<p>The goal of AVA challenge is to provide vision-based benchmarks and methods\nrelevant to accessibility. In this paper, we introduce the technical details of\nour submission to the CVPR2022 AVA Challenge. Firstly, we conducted some\nexperiments to help employ proper model and data augmentation strategy for this\ntask. Secondly, an effective training strategy was applied to improve the\nperformance. Thirdly, we integrated the results from two different segmentation\nframeworks to improve the performance further. Experimental results demonstrate\nthat our approach can achieve a competitive result on the AVA test set.\nFinally, our approach achieves 63.008\\%AP@0.50:0.95 on the test set of CVPR2022\nAVA Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Leilei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongbin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting R-CNN: Reweighting R-CNN Samples by RPN's Error for Underwater Object Detection. (arXiv:2206.13728v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13728","description":"<p>Complicated underwater environments bring new challenges to object detection,\nsuch as unbalanced light conditions, low contrast, occlusion, and mimicry of\naquatic organisms. Under these circumstances, the objects captured by the\nunderwater camera will become vague, and the generic detectors often fail on\nthese vague objects. This work aims to solve the problem from two perspectives:\nuncertainty modeling and hard example mining. We propose a two-stage underwater\ndetector named boosting R-CNN, which comprises three key components. First, a\nnew region proposal network named RetinaRPN is proposed, which provides\nhigh-quality proposals and considers objectness and IoU prediction for\nuncertainty to model the object prior probability. Second, the probabilistic\ninference pipeline is introduced to combine the first-stage prior uncertainty\nand the second-stage classification score to model the final detection score.\nFinally, we propose a new hard example mining method named boosting\nreweighting. Specifically, when the region proposal network miscalculates the\nobject prior probability for a sample, boosting reweighting will increase the\nclassification loss of the sample in the R-CNN head during training, while\nreducing the loss of easy samples with accurately estimated priors. Thus, a\nrobust detection head in the second stage can be obtained. During the inference\nstage, the R-CNN has the capability to rectify the error of the first stage to\nimprove the performance. Comprehensive experiments on two underwater datasets\nand two generic object detection datasets demonstrate the effectiveness and\nrobustness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_P/0/1/0/all/0/1\">Pinhao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Linhui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey on Deep Gait Recognition: Algorithms, Datasets and Challenges. (arXiv:2206.13732v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13732","description":"<p>Gait recognition aims at identifying a person at a distance through visual\ncameras. With the emergence of deep learning, significant advancements in gait\nrecognition have achieved inspiring success in many scenarios by utilizing deep\nlearning techniques. Nevertheless, the increasing need for video surveillance\nintroduces more challenges, including robust recognition under various\nvariances, modeling motion information in gait sequences, unfair performance\ncomparison due to protocol variances, biometrics security, and privacy\nprevention. This paper provides a comprehensive survey of deep learning for\ngait recognition. We first present the odyssey of gait recognition from\ntraditional algorithms to deep models, providing explicit knowledge of the\nwhole workflow of a gait recognition system. Then deep learning for gait\nrecognition is discussed from the perspective of deep representations and\narchitecture with an in-depth summary. Specifically, deep gait representations\nare categorized into static and dynamic features, while deep architectures\ninclude single-stream and multi-stream architecture. Following our proposed\ntaxonomy with novelty, it can be beneficial for providing inspiration and\npromoting the perception of deep gait recognition. Besides, we also present a\ncomprehensive summary of all vision-based gait datasets and the performance\nanalysis. Finally, the article discusses some open issues with significant\npotential prospects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chuanfu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shiqi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">George Q. Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Consistency for Single Domain Generalization in Medical Image Segmentation. (arXiv:2206.13737v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13737","description":"<p>An organ segmentation method that can generalize to unseen contrasts and\nscanner settings can significantly reduce the need for retraining of deep\nlearning models. Domain Generalization (DG) aims to achieve this goal. However,\nmost DG methods for segmentation require training data from multiple domains\nduring training. We propose a novel adversarial domain generalization method\nfor organ segmentation trained on data from a \\emph{single} domain. We\nsynthesize the new domains via learning an adversarial domain synthesizer (ADS)\nand presume that the synthetic domains cover a large enough area of plausible\ndistributions so that unseen domains can be interpolated from synthetic\ndomains. We propose a mutual information regularizer to enforce the semantic\nconsistency between images from the synthetic domains, which can be estimated\nby patch-level contrastive learning. We evaluate our method for various organ\nsegmentation for unseen modalities, scanning protocols, and scanner sites.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shaoan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds1_M/0/1/0/all/0/1\">Maxwell Reynolds1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragoza1_M/0/1/0/all/0/1\">Matthew Ragoza1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN-based Super-Resolution and Segmentation of Retinal Layers in Optical coherence tomography Scans. (arXiv:2206.13740v1 [eess.IV])","link":"http://arxiv.org/abs/2206.13740","description":"<p>In this paper, we design a Generative Adversarial Network (GAN)-based\nsolution for super-resolution and segmentation of optical coherence tomography\n(OCT) scans of the retinal layers. OCT has been identified as a non-invasive\nand inexpensive modality of imaging to discover potential biomarkers for the\ndiagnosis and progress determination of neurodegenerative diseases, such as\nAlzheimer's Disease (AD). Current hypotheses presume the thickness of the\nretinal layers, which are analyzable within OCT scans, can be effective\nbiomarkers. As a logical first step, this work concentrates on the challenging\ntask of retinal layer segmentation and also super-resolution for higher clarity\nand accuracy. We propose a GAN-based segmentation model and evaluate\nincorporating popular networks, namely, U-Net and ResNet, in the GAN\narchitecture with additional blocks of transposed convolution and sub-pixel\nconvolution for the task of upscaling OCT images from low to high resolution by\na factor of four. We also incorporate the Dice loss as an additional\nreconstruction loss term to improve the performance of this joint optimization\ntask. Our best model configuration empirically achieved the Dice coefficient of\n0.867 and mIOU of 0.765.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jeihouni_P/0/1/0/all/0/1\">Paria Jeihouni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dehzangi_O/0/1/0/all/0/1\">Omid Dehzangi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amireskandari_A/0/1/0/all/0/1\">Annahita Amireskandari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rezai_A/0/1/0/all/0/1\">Ali Rezai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Multi-Object Tracking with Differentiable Pose Estimation. (arXiv:2206.13785v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13785","description":"<p>We propose a novel approach for joint 3D multi-object tracking and\nreconstruction from RGB-D sequences in indoor environments. To this end, we\ndetect and reconstruct objects in each frame while predicting dense\ncorrespondences mappings into a normalized object space. We leverage those\ncorrespondences to inform a graph neural network to solve for the optimal,\ntemporally-consistent 7-DoF pose trajectories of all objects. The novelty of\nour method is two-fold: first, we propose a new graph-based approach for\ndifferentiable pose estimation over time to learn optimal pose trajectories;\nsecond, we present a joint formulation of reconstruction and pose estimation\nalong the time axis for robust and geometrically consistent multi-object\ntracking. In order to validate our approach, we introduce a new synthetic\ndataset comprising 2381 unique indoor sequences with a total of 60k rendered\nRGB-D images for multi-object tracking with moving objects and camera positions\nderived from the synthetic 3D-FRONT dataset. We demonstrate that our method\nimproves the accumulated MOTA score for all test sequences by 24.8% over\nexisting state-of-the-art methods. In several ablations on synthetic and\nreal-world sequences, we show that our graph-based, fully end-to-end-learnable\napproach yields a significant boost in tracking performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmauser_D/0/1/0/all/0/1\">Dominik Schmauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zeju Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_N/0/1/0/all/0/1\">Norman M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedRare: Federated Learning with Intra- and Inter-Client Contrast for Effective Rare Disease Classification. (arXiv:2206.13803v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13803","description":"<p>Federated learning (FL), enabling different medical institutions or clients\nto train a model collaboratively without data privacy leakage, has drawn great\nattention in medical imaging communities recently. Though inter-client data\nheterogeneity has been thoroughly studied, the class imbalance problem due to\nthe existence of rare diseases still is under-explored. In this paper, we\npropose a novel FL framework FedRare for medical image classification\nespecially on dealing with data heterogeneity with the existence of rare\ndiseases. In FedRare, each client trains a model locally to extract\nhighly-separable latent features for classification via intra-client supervised\ncontrastive learning. Considering the limited data on rare diseases, we build\npositive sample queues for augmentation (i.e. data re-sampling). The server in\nFedRare would collect the latent features from clients and automatically select\nthe most reliable latent features as guidance sent back to clients. Then, each\nclient is jointly trained by an inter-client contrastive loss to align its\nlatent features to the federated latent features of full classes. In this way,\nthe parameter/feature variances across clients are effectively minimized,\nleading to better convergence and performance improvements. Experimental\nresults on the publicly-available dataset for skin lesion diagnosis demonstrate\nFedRare's superior performance. Under the 10-client federated setting where\nfour clients have no rare disease samples, FedRare achieves an average increase\nof 9.60% and 5.90% in balanced accuracy compared to the baseline framework\nFedAvg and the state-of-the-art approach FedIRM respectively. Considering the\nboard existence of rare diseases in clinical scenarios, we believe FedRare\nwould benefit future FL framework design for medical image classification. The\nsource code of this paper is publicly available at\nhttps://github.com/wnn2000/FedRare.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Nannan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Li Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zengqiang Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Forgery Analysis of Vision Transformers and CNNs for Deepfake Image Detection. (arXiv:2206.13829v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13829","description":"<p>Deepfake Generation Techniques are evolving at a rapid pace, making it\npossible to create realistic manipulated images and videos and endangering the\nserenity of modern society. The continual emergence of new and varied\ntechniques brings with it a further problem to be faced, namely the ability of\ndeepfake detection models to update themselves promptly in order to be able to\nidentify manipulations carried out using even the most recent methods. This is\nan extremely complex problem to solve, as training a model requires large\namounts of data, which are difficult to obtain if the deepfake generation\nmethod is too recent. Moreover, continuously retraining a network would be\nunfeasible. In this paper, we ask ourselves if, among the various deep learning\ntechniques, there is one that is able to generalise the concept of deepfake to\nsuch an extent that it does not remain tied to one or more specific deepfake\ngeneration methods used in the training set. We compared a Vision Transformer\nwith an EfficientNetV2 on a cross-forgery context based on the ForgeryNet\ndataset. From our experiments, It emerges that EfficientNetV2 has a greater\ntendency to specialize often obtaining better results on training methods while\nVision Transformers exhibit a superior generalization ability that makes them\nmore competent even on images generated with new methodologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coccomini_D/0/1/0/all/0/1\">Davide Alessandro Coccomini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caldelli_R/0/1/0/all/0/1\">Roberto Caldelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1\">Fabrizio Falchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1\">Claudio Gennaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1\">Giuseppe Amato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation. (arXiv:2206.13850v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13850","description":"<p>Self-supervised deep learning methods for joint depth and ego-motion\nestimation can yield accurate trajectories without needing ground-truth\ntraining data. However, as they typically use photometric losses, their\nperformance can degrade significantly when the assumptions these losses make\n(e.g. temporal illumination consistency, a static scene, and the absence of\nnoise and occlusions) are violated. This limits their use for e.g. nighttime\nsequences, which tend to contain many point light sources (including on dynamic\nobjects) and low signal-to-noise ratio (SNR) in darker image regions. In this\npaper, we show how to use a combination of three techniques to allow the\nexisting photometric losses to work for both day and nighttime images. First,\nwe introduce a per-pixel neural intensity transformation to compensate for the\nlight changes that occur between successive frames. Second, we predict a\nper-pixel residual flow map that we use to correct the reprojection\ncorrespondences induced by the estimated ego-motion and depth from the\nnetworks. And third, we denoise the training images to improve the robustness\nand accuracy of our approach. These changes allow us to train a single model\nfor both day and nighttime images without needing separate encoders or extra\nfeature networks like existing methods. We perform extensive experiments and\nablation studies on the challenging Oxford RobotCar dataset to demonstrate the\nefficacy of our approach for both day and nighttime sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vankadari_M/0/1/0/all/0/1\">Madhu Vankadari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golodetz_S/0/1/0/all/0/1\">Stuart Golodetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Sangyun Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate and Real-time Pseudo Lidar Detection: Is Stereo Neural Network Really Necessary?. (arXiv:2206.13858v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13858","description":"<p>The proposal of Pseudo-Lidar representation has significantly narrowed the\ngap between visual-based and active Lidar-based 3D object detection. However,\ncurrent researches exclusively focus on pushing the accuracy improvement of\nPseudo-Lidar by taking the advantage of complex and time-consuming neural\nnetworks. Seldom explore the profound characteristics of Pseudo-Lidar\nrepresentation to obtain the promoting opportunities. In this paper, we dive\ndeep into the pseudo Lidar representation and argue that the performance of 3D\nobject detection is not fully dependent on the high precision stereo depth\nestimation. We demonstrate that even for the unreliable depth estimation, with\nproper data processing and refining, it can achieve comparable 3D object\ndetection accuracy. With this finding, we further show the possibility that\nutilizing fast but inaccurate stereo matching algorithms in the Pseudo-Lidar\nsystem to achieve low latency responsiveness. In the experiments, we develop a\nsystem with a less powerful stereo matching predictor and adopt the proposed\nrefinement schemes to improve the accuracy. The evaluation on the KITTI\nbenchmark shows that the presented system achieves competitive accuracy to the\nstate-of-the-art approaches with only 23 ms computing, showing it is a suitable\ncandidate for deploying to real car-hold applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Haitao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changcai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Embedding Spaces with Minimal Distributional Assumptions. (arXiv:2206.13872v1 [stat.ML])","link":"http://arxiv.org/abs/2206.13872","description":"<p>Interest in understanding and factorizing learned embedding spaces is\ngrowing. For instance, recent concept-based explanation techniques analyze a\nmachine learning model in terms of interpretable latent components. Such\ncomponents have to be discovered in the model's embedding space, e.g., through\nindependent component analysis (ICA) or modern disentanglement learning\ntechniques. While these unsupervised approaches offer a sound formal framework,\nthey either require access to a data generating function or impose rigid\nassumptions on the data distribution, such as independence of components, that\nare often violated in practice. In this work, we link conceptual explainability\nfor vision models with disentanglement learning and ICA. This enables us to\nprovide first theoretical results on how components can be identified without\nrequiring any distributional assumptions. From these insights, we derive the\ndisjoint attributions (DA) concept discovery method that is applicable to a\nbroader class of problems than current approaches but yet possesses a formal\nidentifiability guarantee. In an extensive comparison against component\nanalysis and over 300 state-of-the-art disentanglement models, DA stably\nmaintains superior performance, even under varying distributions and\ncorrelation strengths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Leemann_T/0/1/0/all/0/1\">Tobias Leemann</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kirchhof_M/0/1/0/all/0/1\">Michael Kirchhof</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rong_Y/0/1/0/all/0/1\">Yao Rong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kasneci_E/0/1/0/all/0/1\">Enkelejda Kasneci</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kasneci_G/0/1/0/all/0/1\">Gjergji Kasneci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Worst Case Visual Localization Coverage via Place-specific Sub-selection in Multi-camera Systems. (arXiv:2206.13883v1 [cs.RO])","link":"http://arxiv.org/abs/2206.13883","description":"<p>6-DoF visual localization systems utilize principled approaches rooted in 3D\ngeometry to perform accurate camera pose estimation of images to a map. Current\ntechniques use hierarchical pipelines and learned 2D feature extractors to\nimprove scalability and increase performance. However, despite gains in typical\nrecall@0.25m type metrics, these systems still have limited utility for\nreal-world applications like autonomous vehicles because of their `worst' areas\nof performance - the locations where they provide insufficient recall at a\ncertain required error tolerance. Here we investigate the utility of using\n`place specific configurations', where a map is segmented into a number of\nplaces, each with its own configuration for modulating the pose estimation\nstep, in this case selecting a camera within a multi-camera system. On the Ford\nAV benchmark dataset, we demonstrate substantially improved worst-case\nlocalization performance compared to using off-the-shelf pipelines - minimizing\nthe percentage of the dataset which has low recall at a certain error\ntolerance, as well as improved overall localization performance. Our proposed\napproach is particularly applicable to the crowdsharing model of autonomous\nvehicle deployment, where a fleet of AVs are regularly traversing a known\nroute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hausler_S/0/1/0/all/0/1\">Stephen Hausler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Ming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarty_P/0/1/0/all/0/1\">Punarjay Chakravarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_S/0/1/0/all/0/1\">Shubham Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vora_A/0/1/0/all/0/1\">Ankit Vora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating near-infrared facial expression datasets with dimensional affect labels. (arXiv:2206.13887v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13887","description":"<p>Facial expression analysis has long been an active research area of computer\nvision. Traditional methods mainly analyse images for prototypical discrete\nemotions; as a result, they do not provide an accurate depiction of the complex\nemotional states in humans. Furthermore, illumination variance remains a\nchallenge for face analysis in the visible light spectrum. To address these\nissues, we propose using a dimensional model based on valence and arousal to\nrepresent a wider range of emotions, in combination with near infra-red (NIR)\nimagery, which is more robust to illumination changes. Since there are no\nexisting NIR facial expression datasets with valence-arousal labels available,\nwe present two complementary data augmentation methods (face morphing and\nCycleGAN approach) to create NIR image datasets with dimensional emotion labels\nfrom existing categorical and/or visible-light datasets. Our experiments show\nthat these generated NIR datasets are comparable to existing datasets in terms\nof data quality and baseline prediction performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Calvin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1\">Stefan Winkler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AS-IntroVAE: Adversarial Similarity Distance Makes Robust IntroVAE. (arXiv:2206.13903v1 [eess.IV])","link":"http://arxiv.org/abs/2206.13903","description":"<p>Recently, introspective models like IntroVAE and S-IntroVAE have excelled in\nimage generation and reconstruction tasks. The principal characteristic of\nintrospective models is the adversarial learning of VAE, where the encoder\nattempts to distinguish between the real and the fake (i.e., synthesized)\nimages. However, due to the unavailability of an effective metric to evaluate\nthe difference between the real and the fake images, the posterior collapse and\nthe vanishing gradient problem still exist, reducing the fidelity of the\nsynthesized images. In this paper, we propose a new variation of IntroVAE\ncalled Adversarial Similarity Distance Introspective Variational Autoencoder\n(AS-IntroVAE). We theoretically analyze the vanishing gradient problem and\nconstruct a new Adversarial Similarity Distance (AS-Distance) using the\n2-Wasserstein distance and the kernel trick. With weight annealing on\nAS-Distance and KL-Divergence, the AS-IntroVAE are able to generate stable and\nhigh-quality images. The posterior collapse problem is addressed by making\nper-batch attempts to transform the image so that it better fits the prior\ndistribution in the latent space. Compared with the per-image approach, this\nstrategy fosters more diverse distributions in the latent space, allowing our\nmodel to produce images of great diversity. Comprehensive experiments on\nbenchmark datasets demonstrate the effectiveness of AS-IntroVAE on image\ngeneration and reconstruction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_C/0/1/0/all/0/1\">Changjie Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Shen Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dib_O/0/1/0/all/0/1\">Omar Dib</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_G/0/1/0/all/0/1\">Gaurav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discrete Morse Sandwich: Fast Computation of Persistence Diagrams for Scalar Data -- An Algorithm and A Benchmark. (arXiv:2206.13932v1 [cs.LG])","link":"http://arxiv.org/abs/2206.13932","description":"<p>This paper introduces an efficient algorithm for persistence diagram\ncomputation, given an input piecewise linear scalar field f defined on a\nd-dimensional simplicial complex K, with $d \\leq 3$. Our method extends the\nseminal \"PairCells\" algorithm by introducing three main accelerations. First,\nwe express this algorithm within the setting of discrete Morse theory, which\nconsiderably reduces the number of input simplices to consider. Second, we\nintroduce a stratification approach to the problem, that we call \"sandwiching\".\nSpecifically, minima-saddle persistence pairs ($D_0(f)$) and saddle-maximum\npersistence pairs ($D_{d-1}(f)$) are efficiently computed by respectively\nprocessing with a Union-Find the unstable sets of 1-saddles and the stable sets\nof (d-1)-saddles. This fast processing of the dimensions 0 and (d-1) further\nreduces, and drastically, the number of critical simplices to consider for the\ncomputation of $D_1(f)$, the intermediate layer of the sandwich. Third, we\ndocument several performance improvements via shared-memory parallelism. We\nprovide an open-source implementation of our algorithm for reproducibility\npurposes. We also contribute a reproducible benchmark package, which exploits\nthree-dimensional data from a public repository and compares our algorithm to a\nvariety of publicly available implementations. Extensive experiments indicate\nthat our algorithm improves by two orders of magnitude the time performance of\nthe seminal \"PairCells\" algorithm it extends. Moreover, it also improves memory\nfootprint and time performance over a selection of 14 competing approaches,\nwith a substantial gain over the fastest available approaches, while producing\na strictly identical output. We illustrate the utility of our contributions\nwith an application to the fast and robust extraction of persistent\n1-dimensional generators on surfaces, volume data and high-dimensional point\nclouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guillou_P/0/1/0/all/0/1\">Pierre Guillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1\">Jules Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tierny_J/0/1/0/all/0/1\">Julien Tierny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustifying Vision Transformer without Retraining from Scratch by Test-Time Class-Conditional Feature Alignment. (arXiv:2206.13951v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13951","description":"<p>Vision Transformer (ViT) is becoming more popular in image processing.\nSpecifically, we investigate the effectiveness of test-time adaptation (TTA) on\nViT, a technique that has emerged to correct its prediction during test-time by\nitself. First, we benchmark various test-time adaptation approaches on ViT-B16\nand ViT-L16. It is shown that the TTA is effective on ViT and the\nprior-convention (sensibly selecting modulation parameters) is not necessary\nwhen using proper loss function. Based on the observation, we propose a new\ntest-time adaptation method called class-conditional feature alignment (CFA),\nwhich minimizes both the class-conditional distribution differences and the\nwhole distribution differences of the hidden representation between the source\nand target in an online manner. Experiments of image classification tasks on\ncommon corruption (CIFAR-10-C, CIFAR-100-C, and ImageNet-C) and domain\nadaptation (digits datasets and ImageNet-Sketch) show that CFA stably\noutperforms the existing baselines on various datasets. We also verify that CFA\nis model agnostic by experimenting on ResNet, MLP-Mixer, and several ViT\nvariants (ViT-AugReg, DeiT, and BeiT). Using BeiT backbone, CFA achieves 19.8%\ntop-1 error rate on ImageNet-C, outperforming the existing test-time adaptation\nbaseline 44.0%. This is a state-of-the-art result among TTA methods that do not\nneed to alter training phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kojima_T/0/1/0/all/0/1\">Takeshi Kojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwasawa_Y/0/1/0/all/0/1\">Yusuke Iwasawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Prior Learning via Neural Architecture Search for Blind Face Restoration. (arXiv:2206.13962v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13962","description":"<p>Blind Face Restoration (BFR) aims to recover high-quality face images from\nlow-quality ones and usually resorts to facial priors for improving restoration\nperformance. However, current methods still suffer from two major difficulties:\n1) how to derive a powerful network architecture without extensive hand tuning;\n2) how to capture complementary information from multiple facial priors in one\nnetwork to improve restoration performance. To this end, we propose a Face\nRestoration Searching Network (FRSNet) to adaptively search the suitable\nfeature extraction architecture within our specified search space, which can\ndirectly contribute to the restoration quality. On the basis of FRSNet, we\nfurther design our Multiple Facial Prior Searching Network (MFPSNet) with a\nmulti-prior learning scheme. MFPSNet optimally extracts information from\ndiverse facial priors and fuses the information into image features, ensuring\nthat both external guidance and internal features are reserved. In this way,\nMFPSNet takes full advantage of semantic-level (parsing maps), geometric-level\n(facial heatmaps), reference-level (facial dictionaries) and pixel-level\n(degraded images) information and thus generates faithful and realistic images.\nQuantitative and qualitative experiments show that MFPSNet performs favorably\non both synthetic and real-world datasets against the state-of-the-art BFR\nmethods. The codes are publicly available at:\nhttps://github.com/YYJ1anG/MFPSNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yanjiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Puyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoren Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Primitive Graph Learning for Unified Vector Mapping. (arXiv:2206.13963v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13963","description":"<p>Large-scale vector mapping is important for transportation, city planning,\nand survey and census. We propose GraphMapper, a unified framework for\nend-to-end vector map extraction from satellite images. Our key idea is a novel\nunified representation of shapes of different topologies named \"primitive\ngraph\", which is a set of shape primitives and their pairwise relationship\nmatrix. Then, we convert vector shape prediction, regularization, and topology\nreconstruction into a unique primitive graph learning problem. Specifically,\nGraphMapper is a generic primitive graph learning network based on global shape\ncontext modelling through multi-head-attention. An embedding space sorting\nmethod is developed for accurate primitive relationship modelling. We\nempirically demonstrate the effectiveness of GraphMapper on two challenging\nmapping tasks, building footprint regularization and road network topology\nreconstruction. Our model outperforms state-of-the-art methods by 8-10% in both\ntasks on public benchmarks. All code will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1\">Min Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jingwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingwei Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Gait Representation from Massive Unlabelled Walking Videos: A Benchmark. (arXiv:2206.13964v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13964","description":"<p>Gait depicts individuals' unique and distinguishing walking patterns and has\nbecome one of the most promising biometric features for human identification.\nAs a fine-grained recognition task, gait recognition is easily affected by many\nfactors and usually requires a large amount of completely annotated data that\nis costly and insatiable. This paper proposes a large-scale self-supervised\nbenchmark for gait recognition with contrastive learning, aiming to learn the\ngeneral gait representation from massive unlabelled walking videos for\npractical applications via offering informative walking priors and diverse\nreal-world variations. Specifically, we collect a large-scale unlabelled gait\ndataset GaitLU-1M consisting of 1.02M walking sequences and propose a\nconceptually simple yet empirically powerful baseline model GaitSSB.\nExperimentally, we evaluate the pre-trained model on four widely-used gait\nbenchmarks, CASIA-B, OU-MVLP, GREW and Gait3D with or without transfer\nlearning. The unsupervised results are comparable to or even better than the\nearly model-based and GEI-based methods. After transfer learning, our method\noutperforms existing methods by a large margin in most cases. Theoretically, we\ndiscuss the critical issues for gait-specific contrastive framework and present\nsome insights for further study. As far as we know, GaitLU-1M is the first\nlarge-scale unlabelled gait dataset, and GaitSSB is the first method that\nachieves remarkable unsupervised results on the aforementioned benchmarks. The\nsource code of GaitSSB will be integrated into OpenGait which is available at\nhttps://github.com/ShiqiYu/OpenGait.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Saihui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shiqi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Entropy Initialized Concrete Autoencoder for Optimal Sensor Placement and Reconstruction of Geophysical Fields. (arXiv:2206.13968v1 [cs.LG])","link":"http://arxiv.org/abs/2206.13968","description":"<p>We propose a new approach to the optimal placement of sensors for the problem\nof reconstructing geophysical fields from sparse measurements. Our method\nconsists of two stages. In the first stage, we estimate the variability of the\nphysical field as a function of spatial coordinates by approximating its\ninformation entropy through the Conditional PixelCNN network. To calculate the\nentropy, a new ordering of a two-dimensional data array (spiral ordering) is\nproposed, which makes it possible to obtain the entropy of a physical field\nsimultaneously for several spatial scales. In the second stage, the entropy of\nthe physical field is used to initialize the distribution of optimal sensor\nlocations. This distribution is further optimized with the Concrete Autoencoder\narchitecture with the straight-through gradient estimator and adversarial loss\nto simultaneously minimize the number of sensors and maximize reconstruction\naccuracy. Our method scales linearly with data size, unlike commonly used\nPrincipal Component Analysis. We demonstrate our method on the two examples:\n(a) temperature and (b) salinity fields around the Barents Sea and the Svalbard\ngroup of islands. For these examples, we compute the reconstruction error of\nour method and a few baselines. We test our approach against two baselines (1)\nPCA with QR factorization and (2) climatology. We find out that the obtained\noptimal sensor locations have clear physical interpretation and correspond to\nthe boundaries between sea currents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turko_N/0/1/0/all/0/1\">Nikita Turko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobashev_A/0/1/0/all/0/1\">Alexander Lobashev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ushakov_K/0/1/0/all/0/1\">Konstantin Ushakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaurkin_M/0/1/0/all/0/1\">Maxim Kaurkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrayev_R/0/1/0/all/0/1\">Rashit Ibrayev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Increasing Confidence in Adversarial Robustness Evaluations. (arXiv:2206.13991v1 [cs.LG])","link":"http://arxiv.org/abs/2206.13991","description":"<p>Hundreds of defenses have been proposed to make deep neural networks robust\nagainst minimal (adversarial) input perturbations. However, only a handful of\nthese defenses held up their claims because correctly evaluating robustness is\nextremely challenging: Weak attacks often fail to find adversarial examples\neven if they unknowingly exist, thereby making a vulnerable network look\nrobust. In this paper, we propose a test to identify weak attacks, and thus\nweak defense evaluations. Our test slightly modifies a neural network to\nguarantee the existence of an adversarial example for every sample.\nConsequentially, any correct attack must succeed in breaking this modified\nnetwork. For eleven out of thirteen previously-published defenses, the original\nevaluation of the defense fails our test, while stronger attacks that break\nthese defenses pass it. We hope that attack unit tests - such as ours - will be\na major component in future robustness evaluations and increase confidence in\nan empirical field that is currently riddled with skepticism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roland S. Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting tiny objects in aerial images: A normalized Wasserstein distance and a new benchmark. (arXiv:2206.13996v1 [cs.CV])","link":"http://arxiv.org/abs/2206.13996","description":"<p>Tiny object detection (TOD) in aerial images is challenging since a tiny\nobject only contains a few pixels. State-of-the-art object detectors do not\nprovide satisfactory results on tiny objects due to the lack of supervision\nfrom discriminative features. Our key observation is that the Intersection over\nUnion (IoU) metric and its extensions are very sensitive to the location\ndeviation of the tiny objects, which drastically deteriorates the quality of\nlabel assignment when used in anchor-based detectors. To tackle this problem,\nwe propose a new evaluation metric dubbed Normalized Wasserstein Distance (NWD)\nand a new RanKing-based Assigning (RKA) strategy for tiny object detection. The\nproposed NWD-RKA strategy can be easily embedded into all kinds of anchor-based\ndetectors to replace the standard IoU threshold-based one, significantly\nimproving label assignment and providing sufficient supervision information for\nnetwork training. Tested on four datasets, NWD-RKA can consistently improve\ntiny object detection performance by a large margin. Besides, observing\nprominent noisy labels in the Tiny Object Detection in Aerial Images (AI-TOD)\ndataset, we are motivated to meticulously relabel it and release AI-TOD-v2 and\nits corresponding benchmark. In AI-TOD-v2, the missing annotation and location\nerror problems are considerably mitigated, facilitating more reliable training\nand validation processes. Embedding NWD-RKA into DetectoRS, the detection\nperformance achieves 4.3 AP points improvement over state-of-the-art\ncompetitors on AI-TOD-v2. Datasets, codes, and more visualizations are\navailable at: https://chasel-tsui.github.io/AI-TOD-v2/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Show Me Your Face, And I'll Tell You How You Speak. (arXiv:2206.14009v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14009","description":"<p>When we speak, the prosody and content of the speech can be inferred from the\nmovement of our lips. In this work, we explore the task of lip to speech\nsynthesis, i.e., learning to generate speech given only the lip movements of a\nspeaker where we focus on learning accurate lip to speech mappings for multiple\nspeakers in unconstrained, large vocabulary settings. We capture the speaker's\nvoice identity through their facial characteristics, i.e., age, gender,\nethnicity and condition them along with the lip movements to generate speaker\nidentity aware speech. To this end, we present a novel method \"Lip2Speech\",\nwith key design choices to achieve accurate lip to speech synthesis in\nunconstrained scenarios. We also perform various experiments and extensive\nevaluation using quantitative, qualitative metrics and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Millerdurai_C/0/1/0/all/0/1\">Christen Millerdurai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaliq_L/0/1/0/all/0/1\">Lotfy Abdel Khaliq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulrich_T/0/1/0/all/0/1\">Timon Ulrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taxonomy and evolution predicting using deep learning in images. (arXiv:2206.14011v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14011","description":"<p>Molecular and morphological characters, as important parts of biological\ntaxonomy, are contradictory but need to be integrated. Organism's image\nrecognition and bioinformatics are emerging and hot problems nowadays but with\na gap between them. In this work, a multi-branching recognition framework\nmediated by genetic information bridges this barrier, which establishes the\nlink between macro-morphology and micro-molecular information of mushrooms. The\nnovel multi-perspective structure is proposed to fuse the feature images from\nthree branching models, which significantly improves the accuracy of\nrecognition by about 10% and up to more than 90%. Further, genetic information\nis implemented to the mushroom image recognition task by using genetic distance\nembeddings as the representation space for predicting image distance and\nspecies identification. Semantic overfitting of traditional classification\ntasks and the granularity of fine-grained image recognition are also discussed\nin depth for the first time. The generalizability of the model was investigated\nin fine-grained scenarios using zero-shot learning tasks, which could predict\nthe taxonomic and evolutionary information of unseen samples. We presented the\nfirst method to map images to DNA, namely used an encoder mapping image to\ngenetic distances, and then decoded DNA through a pre-trained decoder, where\nthe total test accuracy on 37 species for DNA prediction is 87.45%. This study\ncreates a novel recognition framework by systematically studying the mushroom\nimage recognition problem, bridging the gap between macroscopic biological\ninformation and microscopic molecular information, which will provide a new\nreference for intelligent biometrics in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jiewen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wenbin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yihua Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Adversarial Examples for Location Privacy Protection. (arXiv:2206.14020v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14020","description":"<p>We have investigated a new application of adversarial examples, namely\nlocation privacy protection against landmark recognition systems. We introduce\nmask-guided multimodal projected gradient descent (MM-PGD), in which\nadversarial examples are trained on different deep models. Image contents are\nprotected by analyzing the properties of regions to identify the ones most\nsuitable for blending in adversarial examples. We investigated two region\nidentification strategies: class activation map-based MM-PGD, in which the\ninternal behaviors of trained deep models are targeted; and human-vision-based\nMM-PGD, in which regions that attract less human attention are targeted.\nExperiments on the Places365 dataset demonstrated that these strategies are\npotentially effective in defending against black-box landmark recognition\nsystems without the need for much image manipulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Nghia Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1\">Ta Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Networks pruning via the Structured Perspective Regularization. (arXiv:2206.14056v1 [cs.LG])","link":"http://arxiv.org/abs/2206.14056","description":"<p>In Machine Learning, Artificial Neural Networks (ANNs) are a very powerful\ntool, broadly used in many applications. Often, the selected (deep)\narchitectures include many layers, and therefore a large amount of parameters,\nwhich makes training, storage and inference expensive. This motivated a stream\nof research about compressing the original networks into smaller ones without\nexcessively sacrificing performances. Among the many proposed compression\napproaches, one of the most popular is \\emph{pruning}, whereby entire elements\nof the ANN (links, nodes, channels, \\ldots) and the corresponding weights are\ndeleted. Since the nature of the problem is inherently combinatorial (what\nelements to prune and what not), we propose a new pruning method based on\nOperational Research tools. We start from a natural Mixed-Integer-Programming\nmodel for the problem, and we use the Perspective Reformulation technique to\nstrengthen its continuous relaxation. Projecting away the indicator variables\nfrom this reformulation yields a new regularization term, which we call the\nStructured Perspective Regularization, that leads to structured pruning of the\ninitial architecture. We test our method on some ResNet architectures applied\nto CIFAR-10, CIFAR-100 and ImageNet datasets, obtaining competitive\nperformances w.r.t.~the state of the art for structured pruning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cacciola_M/0/1/0/all/0/1\">Matteo Cacciola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangioni_A/0/1/0/all/0/1\">Antonio Frangioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lodi_A/0/1/0/all/0/1\">Andrea Lodi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning with Transformers for Image Classification. (arXiv:2206.14085v1 [cs.LG])","link":"http://arxiv.org/abs/2206.14085","description":"<p>In many real-world scenarios, data to train machine learning models become\navailable over time. However, neural network models struggle to continually\nlearn new concepts without forgetting what has been learnt in the past. This\nphenomenon is known as catastrophic forgetting and it is often difficult to\nprevent due to practical constraints, such as the amount of data that can be\nstored or the limited computation sources that can be used. Moreover, training\nlarge neural networks, such as Transformers, from scratch is very costly and\nrequires a vast amount of training data, which might not be available in the\napplication domain of interest. A recent trend indicates that dynamic\narchitectures based on an expansion of the parameters can reduce catastrophic\nforgetting efficiently in continual learning, but this needs complex tuning to\nbalance the growing number of parameters and barely share any information\nacross tasks. As a result, they struggle to scale to a large number of tasks\nwithout significant overhead. In this paper, we validate in the computer vision\ndomain a recent solution called Adaptive Distillation of Adapters (ADA), which\nis developed to perform continual learning using pre-trained Transformers and\nAdapters on text classification tasks. We empirically demonstrate on different\nclassification tasks that this method maintains a good predictive performance\nwithout retraining the model or increasing the number of model parameters over\nthe time. Besides it is significantly faster at inference time compared to the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ermis_B/0/1/0/all/0/1\">Beyza Ermis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zappella_G/0/1/0/all/0/1\">Giovanni Zappella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wistuba_M/0/1/0/all/0/1\">Martin Wistuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawal_A/0/1/0/all/0/1\">Aditya Rawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1\">Cedric Archambeau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network. (arXiv:2206.14098v1 [cs.LG])","link":"http://arxiv.org/abs/2206.14098","description":"<p>This work introduces the RevSilo, the first reversible module for\nbidirectional multi-scale feature fusion. Like other reversible methods,\nRevSilo eliminates the need to store hidden activations by recomputing them.\nExisting reversible methods, however, do not apply to multi-scale feature\nfusion and are therefore not applicable to a large class of networks.\nBidirectional multi-scale feature fusion promotes local and global coherence\nand has become a de facto design principle for networks targeting spatially\nsensitive tasks e.g. HRNet and EfficientDet. When paired with high-resolution\ninputs, these networks achieve state-of-the-art results across various computer\nvision tasks, but training them requires substantial accelerator memory for\nsaving large, multi-resolution activations. These memory requirements cap\nnetwork size and limit progress. Using reversible recomputation, the RevSilo\nalleviates memory issues while still operating across resolution scales.\nStacking RevSilos, we create RevBiFPN, a fully reversible bidirectional feature\npyramid network. For classification, RevBiFPN is competitive with networks such\nas EfficientNet while using up to 19.8x lesser training memory. When fine-tuned\non COCO, RevBiFPN provides up to a 2.5% boost in AP over HRNet using fewer MACs\nand a 2.4x reduction in training-time memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiley_V/0/1/0/all/0/1\">Vitaliy Chiley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thangarasa_V/0/1/0/all/0/1\">Vithursan Thangarasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samar_A/0/1/0/all/0/1\">Anshul Samar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hestness_J/0/1/0/all/0/1\">Joel Hestness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeCoste_D/0/1/0/all/0/1\">Dennis DeCoste</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSL-Lanes: Self-Supervised Learning for Motion Forecasting in Autonomous Driving. (arXiv:2206.14116v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14116","description":"<p>Self-supervised learning (SSL) is an emerging technique that has been\nsuccessfully employed to train convolutional neural networks (CNNs) and graph\nneural networks (GNNs) for more transferable, generalizable, and robust\nrepresentation learning. However its potential in motion forecasting for\nautonomous driving has rarely been explored. In this study, we report the first\nsystematic exploration and assessment of incorporating self-supervision into\nmotion forecasting. We first propose to investigate four novel self-supervised\nlearning tasks for motion forecasting with theoretical rationale and\nquantitative and qualitative comparisons on the challenging large-scale\nArgoverse dataset. Secondly, we point out that our auxiliary SSL-based learning\nsetup not only outperforms forecasting methods which use transformers,\ncomplicated fusion mechanisms and sophisticated online dense goal candidate\noptimization algorithms in terms of performance accuracy, but also has low\ninference time and architectural complexity. Lastly, we conduct several\nexperiments to understand why SSL improves motion forecasting. Code is\nopen-sourced at \\url{https://github.com/AutoVision-cloud/SSL-Lanes}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Prarthana Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czarnecki_K/0/1/0/all/0/1\">Krzysztof Czarnecki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"aSTDP: A More Biologically Plausible Learning. (arXiv:2206.14137v1 [cs.NE])","link":"http://arxiv.org/abs/2206.14137","description":"<p>Spike-timing dependent plasticity in biological neural networks has been\nproven to be important during biological learning process. On the other hand,\nartificial neural networks use a different way to learn, such as\nBack-Propagation or Contrastive Hebbian Learning. In this work we introduce\napproximate STDP, a new neural networks learning framework more similar to the\nbiological learning process. It uses only STDP rules for supervised and\nunsupervised learning, every neuron distributed learn patterns and don' t need\na global loss or other supervised information. We also use a numerical way to\napproximate the derivatives of each neuron in order to better use SDTP learning\nand use the derivatives to set a target for neurons to accelerate training and\ntesting process. The framework can make predictions or generate patterns in one\nmodel without additional configuration. Finally, we verified our framework on\nMNIST dataset for classification and generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiyuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualizing and Alleviating the Effect of Radial Distortion on Camera Calibration Using Principal Lines. (arXiv:2206.14164v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14164","description":"<p>Preparing appropriate images for camera calibration is crucial to obtain\naccurate results. In this paper, new suggestions for preparing such data to\nalleviate the adverse effect of radial distortion for a calibration procedure\nusing principal lines are developed through the investigations of: (i)\nidentifying directions of checkerboard movements in an image which will result\nin maximum (and minimum) influence on the calibration results, and (ii)\ninspecting symmetry and monotonicity of such effect in (i) using the above\nprincipal lines. Accordingly, it is suggested that the estimation of principal\npoint should based on linearly independent pairs of nearly parallel principal\nlines, with a member in each pair corresponds to a near 180-degree rotation (in\nthe image plane) of the other. Experimental results show that more robust and\nconsistent calibration results for the foregoing estimation can actually be\nobtained, compared with the renowned algebraic methods which estimate\ndistortion parameters explicitly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_J/0/1/0/all/0/1\">Jen-Hui Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsin-Yi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled Conditions. (arXiv:2206.14180v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14180","description":"<p>Image-based virtual try-on aims to synthesize an image of a person wearing a\ngiven clothing item. To solve the task, the existing methods warp the clothing\nitem to fit the person's body and generate the segmentation map of the person\nwearing the item, before fusing the item with the person. However, when the\nwarping and the segmentation generation stages operate individually without\ninformation exchange, the misalignment between the warped clothes and the\nsegmentation map occurs, which leads to the artifacts in the final image. The\ninformation disconnection also causes excessive warping near the clothing\nregions occluded by the body parts, so called pixel-squeezing artifacts. To\nsettle the issues, we propose a novel try-on condition generator as a unified\nmodule of the two stages (i.e., warping and segmentation generation stages). A\nnewly proposed feature fusion block in the condition generator implements the\ninformation exchange, and the condition generator does not create any\nmisalignment or pixel-squeezing artifacts. We also introduce discriminator\nrejection that filters out the incorrect segmentation map predictions and\nassures the performance of virtual try-on frameworks. Experiments on a\nhigh-resolution dataset demonstrate that our model successfully handles the\nmisalignment and the occlusion, and significantly outperforms the baselines.\nCode is available at https://github.com/sangyun884/HR-VITON.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1\">Gyojung Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seunghwan Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pedestrian 3D Bounding Box Prediction. (arXiv:2206.14195v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14195","description":"<p>Safety is still the main issue of autonomous driving, and in order to be\nglobally deployed, they need to predict pedestrians' motions sufficiently in\nadvance. While there is a lot of research on coarse-grained (human center\nprediction) and fine-grained predictions (human body keypoints prediction), we\nfocus on 3D bounding boxes, which are reasonable estimates of humans without\nmodeling complex motion details for autonomous vehicles. This gives the\nflexibility to predict in longer horizons in real-world settings. We suggest\nthis new problem and present a simple yet effective model for pedestrians' 3D\nbounding box prediction. This method follows an encoder-decoder architecture\nbased on recurrent neural networks, and our experiments show its effectiveness\nin both the synthetic (JTA) and real-world (NuScenes) datasets. The learned\nrepresentation has useful information to enhance the performance of other\ntasks, such as action anticipation. Our code is available online:\nhttps://github.com/vita-epfl/bounding-box-prediction\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saadatnejad_S/0/1/0/all/0/1\">Saeed Saadatnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yi Zhou Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Impacts from Datasets to Monocular Depth Estimation (MDE) Models with MineNavi. (arXiv:2008.08454v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.08454","description":"<p>Current computer vision tasks based on deep learning require a huge amount of\ndata with annotations for model training or testing, especially in some dense\nestimation tasks, such as optical flow segmentation and depth estimation. In\npractice, manual labeling for dense estimation tasks is very difficult or even\nimpossible, and the scenes of the dataset are often restricted to a small\nrange, which dramatically limits the development of the community. To overcome\nthis deficiency, we propose a synthetic dataset generation method to obtain the\nexpandable dataset without burdensome manual workforce. By this method, we\nconstruct a dataset called MineNavi containing video footages from\nfirst-perspective-view of the aircraft matched with accurate ground truth for\ndepth estimation in aircraft navigation application. We also provide\nquantitative experiments to prove that pre-training via our MineNavi dataset\ncan improve the performance of depth estimation model and speed up the\nconvergence of the model on real scene data. Since the synthetic dataset has a\nsimilar effect to the real-world dataset in the training process of deep model,\nwe also provide additional experiments with monocular depth estimation method\nto demonstrate the impact of various factors in our dataset such as lighting\nconditions and motion mode.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiangtong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1\">Binbin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Menglong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Variational Network Toward Blind Image Restoration. (arXiv:2008.10796v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2008.10796","description":"<p>Blind image restoration (IR) is a common yet challenging problem in computer\nvision. Classical model-based methods and recent deep learning (DL)-based\nmethods represent two different methodologies for this problem, each with their\nown merits and drawbacks. In this paper, we propose a novel blind image\nrestoration method, aiming to integrate both the advantages of them.\nSpecifically, we construct a general Bayesian generative model for the blind\nIR, which explicitly depicts the degradation process. In this proposed model, a\npixel-wise non-i.i.d. Gaussian distribution is employed to fit the image noise.\nIt is with more flexibility than the simple i.i.d. Gaussian or Laplacian\ndistributions as adopted in most of conventional methods, so as to handle more\ncomplicated noise types contained in the image degradation. To solve the model,\nwe design a variational inference algorithm where all the expected posteriori\ndistributions are parameterized as deep neural networks to increase their model\ncapability. Notably, such an inference algorithm induces a unified framework to\njointly deal with the tasks of degradation estimation and image restoration.\nFurther, the degradation information estimated in the former task is utilized\nto guide the latter IR process. Experiments on two typical blind IR tasks,\nnamely image denoising and super-resolution, demonstrate that the proposed\nmethod achieves superior performance over current state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yue_Z/0/1/0/all/0/1\">Zongsheng Yue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yong_H/0/1/0/all/0/1\">Hongwei Yong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_K/0/1/0/all/0/1\">Kwan-Yen K. Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Image Destruction: Vulnerability of Deep Image-to-Image Models against Adversarial Attacks. (arXiv:2104.15022v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.15022","description":"<p>Recently, the vulnerability of deep image classification models to\nadversarial attacks has been investigated. However, such an issue has not been\nthoroughly studied for image-to-image tasks that take an input image and\ngenerate an output image (e.g., colorization, denoising, deblurring, etc.) This\npaper presents comprehensive investigations into the vulnerability of deep\nimage-to-image models to adversarial attacks. For five popular image-to-image\ntasks, 16 deep models are analyzed from various standpoints such as output\nquality degradation due to attacks, transferability of adversarial examples\nacross different tasks, and characteristics of perturbations. We show that\nunlike image classification tasks, the performance degradation on\nimage-to-image tasks largely differs depending on various factors, e.g., attack\nmethods and task objectives. In addition, we analyze the effectiveness of\nconventional defense methods used for classification models in improving the\nrobustness of the image-to-image models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jun-Ho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jun-Hyuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Seok Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated triaging of head MRI examinations using convolutional neural networks. (arXiv:2106.08176v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.08176","description":"<p>The growing demand for head magnetic resonance imaging (MRI) examinations,\nalong with a global shortage of radiologists, has led to an increase in the\ntime taken to report head MRI scans around the world. For many neurological\nconditions, this delay can result in increased morbidity and mortality. An\nautomated triaging tool could reduce reporting times for abnormal examinations\nby identifying abnormalities at the time of imaging and prioritizing the\nreporting of these scans. In this work, we present a convolutional neural\nnetwork for detecting clinically-relevant abnormalities in\n$\\text{T}_2$-weighted head MRI scans. Using a validated neuroradiology report\nclassifier, we generated a labelled dataset of 43,754 scans from two large UK\nhospitals for model training, and demonstrate accurate classification (area\nunder the receiver operating curve (AUC) = 0.943) on a test set of 800 scans\nlabelled by a team of neuroradiologists. Importantly, when trained on scans\nfrom only a single hospital the model generalized to scans from the other\nhospital ($\\Delta$AUC $\\leq$ 0.02). A simulation study demonstrated that our\nmodel would reduce the mean reporting time for abnormal examinations from 28\ndays to 14 days and from 9 days to 5 days at the two hospitals, demonstrating\nfeasibility for use in a clinical triage environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wood_D/0/1/0/all/0/1\">David A. Wood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kafiabadi_S/0/1/0/all/0/1\">Sina Kafiabadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Busaidi_A/0/1/0/all/0/1\">Ayisha Al Busaidi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guilhem_E/0/1/0/all/0/1\">Emily Guilhem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Montvila_A/0/1/0/all/0/1\">Antanas Montvila</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1\">Siddharth Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lynch_J/0/1/0/all/0/1\">Jeremy Lynch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Townend_M/0/1/0/all/0/1\">Matthew Townend</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barker_G/0/1/0/all/0/1\">Gareth Barker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cole_J/0/1/0/all/0/1\">James H. Cole</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Booth_T/0/1/0/all/0/1\">Thomas C. Booth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows. (arXiv:2108.05015v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05015","description":"<p>Different from visible cameras which record intensity images frame by frame,\nthe biologically inspired event camera produces a stream of asynchronous and\nsparse events with much lower latency. In practice, the visible cameras can\nbetter perceive texture details and slow motion, while event cameras can be\nfree from motion blurs and have a larger dynamic range which enables them to\nwork well under fast motion and low illumination. Therefore, the two sensors\ncan cooperate with each other to achieve more reliable object tracking. In this\nwork, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to\nthe lack of a realistic and scaled dataset for this task. Our dataset consists\nof 820 video pairs captured under low illumination, high speed, and background\nclutter scenarios, and it is divided into a training and a testing subset, each\nof which contains 500 and 320 videos, respectively. Based on VisEvent, we\ntransform the event flows into event images and construct more than 30 baseline\nmethods by extending current single-modality trackers into dual-modality\nversions. More importantly, we further build a simple but effective tracking\nalgorithm by proposing a cross-modality transformer, to achieve more effective\nfeature fusion between visible and event data. Extensive experiments on the\nproposed VisEvent dataset, FE108, and two simulated datasets (i.e., OTB-DVS and\nVOT-DVS), validated the effectiveness of our model. The dataset and source code\nhave been released at our project page:\n\\url{https://sites.google.com/view/viseventtrack/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Action Segmentation with High-level Complex Activity Labels. (arXiv:2108.06706v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06706","description":"<p>The temporal action segmentation task segments videos temporally and predicts\naction labels for all frames. Fully supervising such a segmentation model\nrequires dense frame-wise action annotations, which are expensive and tedious\nto collect.\n</p>\n<p>This work is the first to propose a Constituent Action Discovery (CAD)\nframework that only requires the video-wise high-level complex activity label\nas supervision for temporal action segmentation. The proposed approach\nautomatically discovers constituent video actions using an activity\nclassification task. Specifically, we define a finite number of latent action\nprototypes to construct video-level dual representations with which these\nprototypes are learned collectively through the activity classification\ntraining. This setting endows our approach with the capability to discover\npotentially shared actions across multiple complex activities.\n</p>\n<p>Due to the lack of action-level supervision, we adopt the Hungarian matching\nalgorithm to relate latent action prototypes to ground truth semantic classes\nfor evaluation. We show that with the high-level supervision, the Hungarian\nmatching can be extended from the existing video and activity levels to the\nglobal level. The global-level matching allows for action sharing across\nactivities, which has never been considered in the literature before. Extensive\nexperiments demonstrate that our discovered actions can help perform temporal\naction segmentation and activity recognition tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guodong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization for Medical Image Segmentation via Hierarchical Consistency Regularization. (arXiv:2109.05742v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05742","description":"<p>Modern deep neural networks struggle to transfer knowledge and generalize\nacross diverse domains when deployed to real-world applications. Currently,\ndomain generalization (DG) is introduced to learn a universal representation\nfrom multiple domains to improve the network generalization ability on unseen\ndomains. However, previous DG methods only focus on the data-level consistency\nscheme without considering the synergistic regularization among different\nconsistency schemes. In this paper, we present a novel Hierarchical Consistency\nframework for Domain Generalization (HCDG) by integrating Extrinsic Consistency\nand Intrinsic Consistency synergistically. Particularly, for the Extrinsic\nConsistency, we leverage the knowledge across multiple source domains to\nenforce data-level consistency. To better enhance such consistency, we design a\nnovel Amplitude Gaussian-mixing strategy into Fourier-based data augmentation\ncalled DomainUp. For the Intrinsic Consistency, we perform task-level\nconsistency for the same instance under the dual-task scenario. We evaluate the\nproposed HCDG framework on two medical image segmentation tasks, i.e., optic\ncup/disc segmentation on fundus images and prostate MRI segmentation. Extensive\nexperimental results manifest the effectiveness and versatility of our HCDG\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yijun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shujun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepStroke: An Efficient Stroke Screening Framework for Emergency Rooms with Multimodal Adversarial Deep Learning. (arXiv:2109.12065v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12065","description":"<p>In an emergency room (ER) setting, stroke triage or screening is a common\nchallenge. A quick CT is usually done instead of MRI due to MRI's slow\nthroughput and high cost. Clinical tests are commonly referred to during the\nprocess, but the misdiagnosis rate remains high. We propose a novel multimodal\ndeep learning framework, DeepStroke, to achieve computer-aided stroke presence\nassessment by recognizing patterns of minor facial muscles incoordination and\nspeech inability for patients with suspicion of stroke in an acute setting. Our\nproposed DeepStroke takes one-minute facial video data and audio data readily\navailable during stroke triage for local facial paralysis detection and global\nspeech disorder analysis. Transfer learning was adopted to reduce\nface-attribute biases and improve generalizability. We leverage a multi-modal\nlateral fusion to combine the low- and high-level features and provide mutual\nregularization for joint training. Novel adversarial training is introduced to\nobtain identity-free and stroke-discriminative features. Experiments on our\nvideo-audio dataset with actual ER patients show that DeepStroke outperforms\nstate-of-the-art models and achieves better performance than both a triage team\nand ER doctors, attaining a 10.94% higher sensitivity and maintaining 7.37%\nhigher accuracy than traditional stroke triage when specificity is aligned.\nMeanwhile, each assessment can be completed in less than six minutes,\ndemonstrating the framework's great potential for clinical translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tongan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_H/0/1/0/all/0/1\">Haomiao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingli Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kelvin Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volpi_J/0/1/0/all/0/1\">John Volpi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">James Z. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1\">Stephen T.C. Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fire Together Wire Together: A Dynamic Pruning Approach with Self-Supervised Mask Prediction. (arXiv:2110.08232v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08232","description":"<p>Dynamic model pruning is a recent direction that allows for the inference of\na different sub-network for each input sample during deployment. However,\ncurrent dynamic methods rely on learning a continuous channel gating through\nregularization by inducing sparsity loss. This formulation introduces\ncomplexity in balancing different losses (e.g task loss, regularization loss).\nIn addition, regularization based methods lack transparent tradeoff\nhyperparameter selection to realize computational budget. Our contribution is\ntwo-fold: 1) decoupled task and pruning training. 2) Simple hyperparameter\nselection that enables FLOPs reduction estimation before training. Inspired by\nthe Hebbian theory in Neuroscience: \"neurons that fire together wire together\",\nwe propose to predict a mask to process k filters in a layer based on the\nactivation of its previous layer. We pose the problem as a self-supervised\nbinary classification problem. Each mask predictor module is trained to predict\nif the log-likelihood for each filter in the current layer belongs to the top-k\nactivated filters. The value k is dynamically estimated for each input based on\na novel criterion using the mass of heatmaps. We show experiments on several\nneural architectures, such as VGG, ResNet and MobileNet on CIFAR and ImageNet\ndatasets. On CIFAR, we reach similar accuracy to SOTA methods with 15% and 24%\nhigher FLOPs reduction. Similarly in ImageNet, we achieve lower drop in\naccuracy with up to 13% improvement in FLOPs reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elkerdawy_S/0/1/0/all/0/1\">Sara Elkerdawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoushi_M/0/1/0/all/0/1\">Mostafa Elhoushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1\">Nilanjan Ray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking Blobs in the Turbulent Edge Plasma of a Tokamak Fusion Device. (arXiv:2111.08570v2 [physics.plasm-ph] UPDATED)","link":"http://arxiv.org/abs/2111.08570","description":"<p>The analysis of turbulence in plasmas is fundamental in fusion research.\nDespite extensive progress in theoretical modeling in the past 15 years, we\nstill lack a complete and consistent understanding of turbulence in magnetic\nconfinement devices, such as tokamaks. Experimental studies are challenging due\nto the diverse processes that drive the high-speed dynamics of turbulent\nphenomena. This work presents a novel application of motion tracking to\nidentify and track turbulent filaments in fusion plasmas, called blobs, in a\nhigh-frequency video obtained from Gas Puff Imaging diagnostics. We compare\nfour baseline methods (RAFT, GMA, Flow Walk, and Mask R-CNN) trained on\nsynthetic data and then test on synthetic and real-world data obtained from\nplasmas in the Tokamak `a Configuration Variable (TCV). The blob regime\nidentified from an analysis of blob trajectories agrees with state-of-the-art\nconditional averaging methods for each of the baseline methods employed, giving\nconfidence in the accuracy of these techniques. High entry barriers\ntraditionally limit tokamak plasma research to a small community of researchers\nin the field. By making a dataset and benchmark publicly available, we hope to\nopen the field to a broad community in science and engineering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Han_W/0/1/0/all/0/1\">Woonghee Han</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pietersen_R/0/1/0/all/0/1\">Randall A. Pietersen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Villamor_Lora_R/0/1/0/all/0/1\">Rafael Villamor-Lora</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Beveridge_M/0/1/0/all/0/1\">Matthew Beveridge</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Offeddu_N/0/1/0/all/0/1\">Nicola Offeddu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Golfinopoulos_T/0/1/0/all/0/1\">Theodore Golfinopoulos</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Theiler_C/0/1/0/all/0/1\">Christian Theiler</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Terry_J/0/1/0/all/0/1\">James L. Terry</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Marmar_E/0/1/0/all/0/1\">Earl S. Marmar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation. (arXiv:2111.12707v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12707","description":"<p>Estimating 3D human poses from monocular videos is a challenging task due to\ndepth ambiguity and self-occlusion. Most existing works attempt to solve both\nissues by exploiting spatial and temporal relationships. However, those works\nignore the fact that it is an inverse problem where multiple feasible solutions\n(i.e., hypotheses) exist. To relieve this limitation, we propose a\nMulti-Hypothesis Transformer (MHFormer) that learns spatio-temporal\nrepresentations of multiple plausible pose hypotheses. In order to effectively\nmodel multi-hypothesis dependencies and build strong relationships across\nhypothesis features, the task is decomposed into three stages: (i) Generate\nmultiple initial hypothesis representations; (ii) Model self-hypothesis\ncommunication, merge multiple hypotheses into a single converged representation\nand then partition it into several diverged hypotheses; (iii) Learn\ncross-hypothesis communication and aggregate the multi-hypothesis features to\nsynthesize the final 3D pose. Through the above processes, the final\nrepresentation is enhanced and the synthesized pose is much more accurate.\nExtensive experiments show that MHFormer achieves state-of-the-art results on\ntwo challenging datasets: Human3.6M and MPI-INF-3DHP. Without bells and\nwhistles, its performance surpasses the previous best result by a large margin\nof 3% on Human3.6M. Code and models are available at\n\\url{https://github.com/Vegetebird/MHFormer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anisotropic mesh adaptation for region-based segmentation accounting for image spatial information. (arXiv:2112.10138v2 [math.NA] UPDATED)","link":"http://arxiv.org/abs/2112.10138","description":"<p>A finite element-based image segmentation strategy enhanced by an anisotropic\nmesh adaptation procedure is presented. The methodology relies on a split\nBregman algorithm for the minimisation of a region-based energy functional and\non an anisotropic recovery-based error estimate to drive mesh adaptation. More\nprecisely, a Bayesian energy functional is considered to account for image\nspatial information, ensuring that the methodology is able to identify\ninhomogeneous spatial patterns in complex images. In addition, the anisotropic\nmesh adaptation guarantees a sharp detection of the interface between\nbackground and foreground of the image, with a reduced number of degrees of\nfreedom. The resulting split-adapt Bregman algorithm is tested on a set of real\nimages showing the accuracy and robustness of the method, even in the presence\nof Gaussian, salt and pepper and speckle noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Giacomini_M/0/1/0/all/0/1\">Matteo Giacomini</a>, <a href=\"http://arxiv.org/find/math/1/au:+Perotto_S/0/1/0/all/0/1\">Simona Perotto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralTailor: Reconstructing Sewing Pattern Structures from 3D Point Clouds of Garments. (arXiv:2201.13063v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.13063","description":"<p>The fields of SocialVR, performance capture, and virtual try-on are often\nfaced with a need to faithfully reproduce real garments in the virtual world.\nOne critical task is the disentanglement of the intrinsic garment shape from\ndeformations due to fabric properties, physical forces, and contact with the\nbody. We propose to use a garment sewing pattern, a realistic and compact\ngarment descriptor, to facilitate the intrinsic garment shape estimation.\nAnother major challenge is a high diversity of shapes and designs in the\ndomain. The most common approach for Deep Learning on 3D garments is to build\nspecialized models for individual garments or garment types. We argue that\nbuilding a unified model for various garment designs has the benefit of\ngeneralization to novel garment types, hence covering a larger design domain\nthan individual models would. We introduce NeuralTailor, a novel architecture\nbased on point-level attention for set regression with variable cardinality,\nand apply it to the task of reconstructing 2D garment sewing patterns from the\n3D point could garment models. Our experiments show that NeuralTailor\nsuccessfully reconstructs sewing patterns and generalizes to garment types with\npattern topologies unseen during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korosteleva_M/0/1/0/all/0/1\">Maria Korosteleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sung-Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Graph Convolutional Networks for Weakly Supervised Anomaly Detection in Videos. (arXiv:2202.06503v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06503","description":"<p>For weakly supervised anomaly detection, most existing work is limited to the\nproblem of inadequate video representation due to the inability of modeling\nlong-term contextual information. To solve this, we propose a novel weakly\nsupervised adaptive graph convolutional network (WAGCN) to model the complex\ncontextual relationship among video segments. By which, we fully consider the\ninfluence of other video segments on the current one when generating the\nanomaly probability score for each segment. Firstly, we combine the temporal\nconsistency as well as feature similarity of video segments to construct a\nglobal graph, which makes full use of the association information among\nspatial-temporal features of anomalous events in videos. Secondly, we propose a\ngraph learning layer in order to break the limitation of setting topology\nmanually, which can extract graph adjacency matrix based on data adaptively and\neffectively. Extensive experiments on two public datasets (i.e., UCF-Crime\ndataset and ShanghaiTech dataset) demonstrate the effectiveness of our approach\nwhich achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Congqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shizhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceived Overlap: A Prerequisite for VAE Disentanglement. (arXiv:2202.13341v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.13341","description":"<p>Learning disentangled representations with variational autoencoders (VAEs) is\noften attributed to the regularisation component of the loss. In this work, we\nhighlight the interaction between data and the reconstruction term of the loss\nas the main contributor to disentanglement in VAEs. We note that standardised\nbenchmark datasets are constructed in ways that are conducive to learning what\nappear to be disentangled representations. We design an intuitive adversarial\ndataset that exploits this mechanism to break existing state-of-the-art\ndisentanglement frameworks. Finally, we supply a solution that enables\ndisentanglement by modifying the reconstruction loss, affecting how VAEs\nperceive distances between data points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michlo_N/0/1/0/all/0/1\">Nathan Michlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Steven James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_R/0/1/0/all/0/1\">Richard Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation. (arXiv:2203.01324v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.01324","description":"<p>Semi-supervised segmentation remains challenging in medical imaging since the\namount of annotated medical data is often scarce and there are many blurred\npixels near the adhesive edges or in the low-contrast regions. To address the\nissues, we advocate to firstly constrain the consistency of pixels with and\nwithout strong perturbations to apply a sufficient smoothness constraint and\nfurther encourage the class-level separation to exploit the low-entropy\nregularization for the model training. Particularly, in this paper, we propose\nthe SS-Net for semi-supervised medical image segmentation tasks, via exploring\nthe pixel-level smoothness and inter-class separation at the same time. The\npixel-level smoothness forces the model to generate invariant results under\nadversarial perturbations. Meanwhile, the inter-class separation encourages\nindividual class features should approach their corresponding high-quality\nprototypes, in order to make each class distribution compact and separate\ndifferent classes. We evaluated our SS-Net against five recent methods on the\npublic LA and ACDC datasets. Extensive experimental results under two\nsemi-supervised settings demonstrate the superiority of our proposed SS-Net\nmodel, achieving new state-of-the-art (SOTA) performance on both datasets. The\ncode is available at https://github.com/ycwu1997/SS-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yicheng Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1\">Qianyi Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region Proposal Rectification Towards Robust Instance Segmentation of Biological Images. (arXiv:2203.02846v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02846","description":"<p>Top-down instance segmentation framework has shown its superiority in object\ndetection compared to the bottom-up framework. While it is efficient in\naddressing over-segmentation, top-down instance segmentation suffers from\nover-crop problem. However, a complete segmentation mask is crucial for\nbiological image analysis as it delivers important morphological properties\nsuch as shapes and volumes. In this paper, we propose a region proposal\nrectification (RPR) module to address this challenging incomplete segmentation\nproblem. In particular, we offer a progressive ROIAlign module to introduce\nneighbor information into a series of ROIs gradually. The ROI features are fed\ninto an attentive feed-forward network (FFN) for proposal box regression. With\nadditional neighbor information, the proposed RPR module shows significant\nimprovement in correction of region proposal locations and thereby exhibits\nfavorable instance segmentation performances on three biological image datasets\ncompared to state-of-the-art baseline methods. Experimental results demonstrate\nthat the proposed RPR module is effective in both anchor-based and anchor-free\ntop-down instance segmentation approaches, suggesting the proposed method can\nbe applied to general top-down instance segmentation of biological images. Code\nis available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhangli_Q/0/1/0/all/0/1\">Qilong Zhangli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jingru Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Di Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zhaoyang Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1\">Qi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Ligong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Song Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haiming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cartoon-texture evolution for two-region image segmentation. (arXiv:2203.03513v2 [math.NA] UPDATED)","link":"http://arxiv.org/abs/2203.03513","description":"<p>Two-region image segmentation is the process of dividing an image into two\nregions of interest, i.e., the foreground and the background. To this aim, Chan\net al. [Chan, Esedo\\=glu, Nikolova, SIAM Journal on Applied Mathematics 66(5),\n1632-1648, 2006] designed a model well suited for smooth images. One drawback\nof this model is that it may produce a bad segmentation when the image contains\noscillatory components. Based on a cartoon-texture decomposition of the image\nto be segmented, we propose a new model that is able to produce an accurate\nsegmentation of images also containing noise or oscillatory information like\ntexture. The novel model leads to a non-smooth constrained optimization problem\nwhich we solve by means of the ADMM method. The convergence of the numerical\nscheme is also proved. Several experiments on smooth, noisy, and textural\nimages show the effectiveness of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Antonelli_L/0/1/0/all/0/1\">Laura Antonelli</a>, <a href=\"http://arxiv.org/find/math/1/au:+Simone_V/0/1/0/all/0/1\">Valentina De Simone</a>, <a href=\"http://arxiv.org/find/math/1/au:+Viola_M/0/1/0/all/0/1\">Marco Viola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stepwise Feature Fusion: Local Guides Global. (arXiv:2203.03635v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.03635","description":"<p>Colonoscopy, currently the most efficient and recognized colon polyp\ndetection technology, is necessary for early screening and prevention of\ncolorectal cancer. However, due to the varying size and complex morphological\nfeatures of colonic polyps as well as the indistinct boundary between polyps\nand mucosa, accurate segmentation of polyps is still challenging. Deep learning\nhas become popular for accurate polyp segmentation tasks with excellent\nresults. However, due to the structure of polyps image and the varying shapes\nof polyps, it easy for existing deep learning models to overfitting the current\ndataset. As a result, the model may not process unseen colonoscopy data. To\naddress this, we propose a new State-Of-The-Art model for medical image\nsegmentation, the SSFormer, which uses a pyramid Transformer encoder to improve\nthe generalization ability of models. Specifically, our proposed Progressive\nLocality Decoder can be adapted to the pyramid Transformer backbone to\nemphasize local features and restrict attention dispersion. The SSFormer\nachieves statet-of-the-art performance in both learning and generalization\nassessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinfeng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1\">Qiming Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_F/0/1/0/all/0/1\">Feilong Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_J/0/1/0/all/0/1\">Jia Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_J/0/1/0/all/0/1\">Jionglong Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1\">Sifan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic-to-Real Domain Adaptation using Contrastive Unpaired Translation. (arXiv:2203.09454v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09454","description":"<p>The usefulness of deep learning models in robotics is largely dependent on\nthe availability of training data. Manual annotation of training data is often\ninfeasible. Synthetic data is a viable alternative, but suffers from domain\ngap. We propose a multi-step method to obtain training data without manual\nannotation effort: From 3D object meshes, we generate images using a modern\nsynthesis pipeline. We utilize a state-of-the-art image-to-image translation\nmethod to adapt the synthetic images to the real domain, minimizing the domain\ngap in a learned manner. The translation network is trained from unpaired\nimages, i.e. just requires an un-annotated collection of real images. The\ngenerated and refined images can then be used to train deep learning models for\na particular task. We also propose and evaluate extensions to the translation\nmethod that further increase performance, such as patch-based training, which\nshortens training time and increases global consistency. We evaluate our method\nand demonstrate its effectiveness on two robotic datasets. We finally give\ninsight into the learned refinement operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imbusch_B/0/1/0/all/0/1\">Benedikt T. Imbusch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_M/0/1/0/all/0/1\">Max Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaRTS: Causality-driven Robot Tool Segmentation from Vision and Kinematics Data. (arXiv:2203.09475v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2203.09475","description":"<p>Vision-based segmentation of the robotic tool during robot-assisted surgery\nenables downstream applications, such as augmented reality feedback, while\nallowing for inaccuracies in robot kinematics. With the introduction of deep\nlearning, many methods were presented to solve instrument segmentation directly\nand solely from images. While these approaches made remarkable progress on\nbenchmark datasets, fundamental challenges pertaining to their robustness\nremain. We present CaRTS, a causality-driven robot tool segmentation algorithm,\nthat is designed based on a complementary causal model of the robot tool\nsegmentation task. Rather than directly inferring segmentation masks from\nobserved images, CaRTS iteratively aligns tool models with image observations\nby updating the initially incorrect robot kinematic parameters through forward\nkinematics and differentiable rendering to optimize image feature similarity\nend-to-end. We benchmark CaRTS with competing techniques on both synthetic as\nwell as real data from the dVRK, generated in precisely controlled scenarios to\nallow for counterfactual synthesis. On training-domain test data, CaRTS\nachieves a Dice score of 93.4 that is preserved well (Dice score of 91.8) when\ntested on counterfactually altered test data, exhibiting low brightness, smoke,\nblood, and altered background patterns. This compares favorably to Dice scores\nof 95.0 and 86.7, respectively, of the SOTA image-based method. Future work\nwill involve accelerating CaRTS to achieve video framerate and estimating the\nimpact occlusion has in practice. Despite these limitations, our results are\npromising: In addition to achieving high segmentation accuracy, CaRTS provides\nestimates of the true robot kinematics, which may benefit applications such as\nforce estimation. Code is available at: https://github.com/hding2455/CaRTS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jintan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazanzides_P/0/1/0/all/0/1\">Peter Kazanzides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Ying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Neck Feature Representation for Object Detection in Aerial Images. (arXiv:2204.02033v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02033","description":"<p>Object detection in aerial images is a fundamental research topic in the\ngeoscience and remote sensing domain. However, the advanced approaches on this\ntopic mainly focus on designing the elaborate backbones or head networks but\nignore neck networks. In this letter, we first underline the importance of the\nneck network in object detection from the perspective of information\nbottleneck. Then, to alleviate the information deficiency problem in the\ncurrent approaches, we propose a global semantic network (GSNet), which acts as\na bridge from the backbone network to the head network in a bidirectional\nglobal pattern. Compared to the existing approaches, our model can capture the\nrich and enhanced image features with less computational costs. Besides, we\nfurther propose a feature fusion refinement module (FRM) for different levels\nof features, which are suffering from the problem of semantic gap in feature\nfusion. To demonstrate the effectiveness and efficiency of our approach,\nexperiments are carried out on two challenging and representative aerial image\ndatasets (i.e., DOTA and HRSC2016). Experimental results in terms of accuracy\nand complexity validate the superiority of our method. The code has been\nopen-sourced at GSNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuchen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhihao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xuesong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qiaolin Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Generalizable Dexterous Manipulation from Human Grasp Affordance. (arXiv:2204.02320v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2204.02320","description":"<p>Dexterous manipulation with a multi-finger hand is one of the most\nchallenging problems in robotics. While recent progress in imitation learning\nhas largely improved the sample efficiency compared to Reinforcement Learning,\nthe learned policy can hardly generalize to manipulate novel objects, given\nlimited expert demonstrations. In this paper, we propose to learn dexterous\nmanipulation using large-scale demonstrations with diverse 3D objects in a\ncategory, which are generated from a human grasp affordance model. This\ngeneralizes the policy to novel object instances within the same category. To\ntrain the policy, we propose a novel imitation learning objective jointly with\na geometric representation learning objective using our demonstrations. By\nexperimenting with relocating diverse objects in simulation, we show that our\napproach outperforms baselines with a large margin when manipulating novel\nobjects. We also ablate the importance on 3D object representation learning for\nmanipulation. We include videos, code, and additional information on the\nproject website - https://kristery.github.io/ILAD/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yueh-Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiashun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Dual Emotion with Fusion of Visual Sentiment for Rumor Detection. (arXiv:2204.11515v3 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2204.11515","description":"<p>In recent years, rumors have had a devastating impact on society, making\nrumor detection a significant challenge. However, the studies on rumor\ndetection ignore the intense emotions of images in the rumor content. This\npaper verifies that the image emotion improves the rumor detection efficiency.\nA Multimodal Dual Emotion feature in rumor detection, which consists of visual\nand textual emotions, is proposed. To the best of our knowledge, this is the\nfirst study which uses visual emotion in rumor detection. The experiments on\nreal datasets verify that the proposed features outperform the state-of-the-art\nsentiment features, and can be extended in rumor detectors while improving\ntheir performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Li Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Z/0/1/0/all/0/1\">Ziliang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">He Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-level Consistency Learning for Semi-supervised Domain Adaptation. (arXiv:2205.04066v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04066","description":"<p>Semi-supervised domain adaptation (SSDA) aims to apply knowledge learned from\na fully labeled source domain to a scarcely labeled target domain. In this\npaper, we propose a Multi-level Consistency Learning (MCL) framework for SSDA.\nSpecifically, our MCL regularizes the consistency of different views of target\ndomain samples at three levels: (i) at inter-domain level, we robustly and\naccurately align the source and target domains using a prototype-based optimal\ntransport method that utilizes the pros and cons of different views of target\nsamples; (ii) at intra-domain level, we facilitate the learning of both\ndiscriminative and compact target feature representations by proposing a novel\nclass-wise contrastive clustering loss; (iii) at sample level, we follow\nstandard practice and improve the prediction accuracy by conducting a\nconsistency-based self-training. Empirically, we verified the effectiveness of\nour MCL framework on three popular SSDA benchmarks, i.e., VisDA2017, DomainNet,\nand Office-Home datasets, and the experimental results demonstrate that our MCL\nframework achieves the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zizheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yushuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yipeng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Updates of a Pre-trained Model for Few-shot Learning. (arXiv:2205.07874v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.07874","description":"<p>Most of the recent few-shot learning algorithms are based on transfer\nlearning, where a model is pre-trained using a large amount of source data, and\nthe pre-trained model is updated using a small amount of target data afterward.\nIn transfer-based few-shot learning, sophisticated pre-training methods have\nbeen widely studied for universal and improved representation. However, there\nis little study on updating pre-trained models for few-shot learning. In this\npaper, we compare the two popular updating methods, fine-tuning (i.e., updating\nthe entire network) and linear probing (i.e., updating only the linear\nclassifier), considering the distribution shift between the source and target\ndata. We find that fine-tuning is better than linear probing as the number of\nsamples increases, regardless of distribution shift. Next, we investigate the\neffectiveness and ineffectiveness of data augmentation when pre-trained models\nare fine-tuned. Our fundamental analyses demonstrate that careful\nconsiderations of the details about updating pre-trained models are required\nfor better few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yujin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jaehoon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungnyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subcellular Protein Localisation in the Human Protein Atlas using Ensembles of Diverse Deep Architectures. (arXiv:2205.09841v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09841","description":"<p>Automated visual localisation of subcellular proteins can accelerate our\nunderstanding of cell function in health and disease. Despite recent advances\nin machine learning (ML), humans still attain superior accuracy by using\ndiverse visual cues. We show how this gap can be narrowed by addressing three\nkey aspects: (i) automated improvement of cell annotation quality, (ii) new\nDeep Neural Network (DNN) architectures supporting unbalanced and noisy data,\nand (iii) informed selection and fusion of multiple &amp; diverse machine learning\nmodels. We introduce a new ``AI-trains-AI'' method for improving the quality of\nweak labels and propose novel DNN architectures exploiting wavelet filters and\nWeibull activations. We also explore key factors in the multi-DNN ensembling\nprocess by analysing correlations between image-level and cell-level\npredictions. Finally, in the context of the Human Protein Atlas, we demonstrate\nthat our system achieves state-of-the-art performance in the multi-label\nsingle-cell classification of protein localisation patterns, while\nstrengthening generalisation ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Husain_S/0/1/0/all/0/1\">Syed Sameed Husain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1\">Eng-Jon Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minskiy_D/0/1/0/all/0/1\">Dmitry Minskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bober_Irizar_M/0/1/0/all/0/1\">Mikel Bober-Irizar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irizar_A/0/1/0/all/0/1\">Amaia Irizar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bober_M/0/1/0/all/0/1\">Miroslaw Bober</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiview Textured Mesh Recovery by Differentiable Rendering. (arXiv:2205.12468v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12468","description":"<p>Although having achieved the promising results on shape and color recovery\nthrough self-supervision, the multi-layer perceptrons-based methods usually\nsuffer from heavy computational cost on learning the deep implicit surface\nrepresentation. Since rendering each pixel requires a forward network\ninference, it is very computational intensive to synthesize a whole image. To\ntackle these challenges, we propose an effective coarse-to-fine approach to\nrecover the textured mesh from multi-views in this paper. Specifically, a\ndifferentiable Poisson Solver is employed to represent the object's shape,\nwhich is able to produce topology-agnostic and watertight surfaces. To account\nfor depth information, we optimize the shape geometry by minimizing the\ndifferences between the rendered mesh and the predicted depth from multi-view\nstereo. In contrast to the implicit neural representation on shape and color,\nwe introduce a physically based inverse rendering scheme to jointly estimate\nthe environment lighting and object's reflectance, which is able to render the\nhigh resolution image at real-time. The texture of the reconstructed mesh is\ninterpolated from a learnable dense texture grid. We have conducted the\nextensive experiments on several multi-view stereo datasets, whose promising\nresults demonstrate the efficacy of our proposed approach. The code is\navailable at https://github.com/l1346792580123/diff.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lixiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yisu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIT: A Generative Image-to-text Transformer for Vision and Language. (arXiv:2205.14100v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14100","description":"<p>In this paper, we design and train a Generative Image-to-text Transformer,\nGIT, to unify vision-language tasks such as image/video captioning and question\nanswering. While generative models provide a consistent network architecture\nbetween pre-training and fine-tuning, existing work typically contains complex\nstructures (uni/multi-modal encoder/decoder) and depends on external modules\nsuch as object detectors/taggers and optical character recognition (OCR). In\nGIT, we simplify the architecture as one image encoder and one text decoder\nunder a single language modeling task. We also scale up the pre-training data\nand the model size to boost the model performance. Without bells and whistles,\nour GIT establishes new state of the arts on 12 challenging benchmarks with a\nlarge margin. For instance, our model surpasses the human performance for the\nfirst time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a\nnew scheme of generation-based image classification and scene text recognition,\nachieving decent performance on standard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.01612","description":"<p>We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python\nlibrary of eXplainable AI (XAI), which offers omni-way explainable AI\ncapabilities and various interpretable machine learning techniques to address\nthe pain points of understanding and interpreting the decisions made by machine\nlearning (ML) in practice. OmniXAI aims to be a one-stop comprehensive library\nthat makes explainable AI easy for data scientists, ML researchers and\npractitioners who need explanation for various types of data, models and\nexplanation methods at different stages of ML process (data exploration,\nfeature engineering, model development, evaluation, and decision-making, etc).\nIn particular, our library includes a rich family of explanation methods\nintegrated in a unified interface, which supports multiple data types (tabular\ndata, images, texts, time-series), multiple types of ML models (traditional ML\nin Scikit-learn and deep learning models in PyTorch/TensorFlow), and a range of\ndiverse explanation methods including \"model-specific\" and \"model-agnostic\"\nones (such as feature-attribution explanation, counterfactual explanation,\ngradient-based explanation, etc). For practitioners, the library provides an\neasy-to-use unified interface to generate the explanations for their\napplications by only writing a few lines of codes, and also a GUI dashboard for\nvisualization of different explanations for more insights about decisions. In\nthis technical report, we present OmniXAI's design principles, system\narchitectures, and major functionalities, and also demonstrate several example\nuse cases across different types of data, tasks, and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenzhuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Study of Quality Image Assessment for Synthesis of Fetal Head Ultrasound Imaging with DCGANs. (arXiv:2206.01731v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.01731","description":"<p>In this work, we present an empirical study of DCGANs, including\nhyperparameter heuristics and image quality assessment, as a way to address the\nscarcity of datasets to investigate fetal head ultrasound. We present\nexperiments to show the impact of different image resolutions, epochs, dataset\nsize input, and learning rates for quality image assessment on four metrics:\nmutual information (MI), Fr\\'echet inception distance (FID),\npeak-signal-to-noise ratio (PSNR), and local binary pattern vector (LBPv). The\nresults show that FID and LBPv have stronger relationship with clinical image\nquality scores. The resources to reproduce this work are available at\n\\url{https://github.com/budai4medtech/miua2022}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bautista_T/0/1/0/all/0/1\">Thea Bautista</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matthew_J/0/1/0/all/0/1\">Jacqueline Matthew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kerdegari_H/0/1/0/all/0/1\">Hamideh Kerdegari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_L/0/1/0/all/0/1\">Laura Peralta Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xochicale_M/0/1/0/all/0/1\">Miguel Xochicale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralODF: Learning Omnidirectional Distance Fields for 3D Shape Representation. (arXiv:2206.05837v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.05837","description":"<p>In visual computing, 3D geometry is represented in many different forms\nincluding meshes, point clouds, voxel grids, level sets, and depth images. Each\nrepresentation is suited for different tasks thus making the transformation of\none representation into another (forward map) an important and common problem.\nWe propose Omnidirectional Distance Fields (ODFs), a new 3D shape\nrepresentation that encodes geometry by storing the depth to the object's\nsurface from any 3D position in any viewing direction. Since rays are the\nfundamental unit of an ODF, it can be used to easily transform to and from\ncommon 3D representations like meshes or point clouds. Different from level set\nmethods that are limited to representing closed surfaces, ODFs are unsigned and\ncan thus model open surfaces (e.g., garments). We demonstrate that ODFs can be\neffectively learned with a neural network (NeuralODF) despite the inherent\ndiscontinuities at occlusion boundaries. We also introduce efficient forward\nmapping algorithms for transforming ODFs to and from common 3D representations.\nSpecifically, we introduce an efficient Jumping Cubes algorithm for generating\nmeshes from ODFs. Experiments demonstrate that NeuralODF can learn to capture\nhigh-quality shape by overfitting to a single object, and also learn to\ngeneralize on common shape categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Houchens_T/0/1/0/all/0/1\">Trevor Houchens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng-You Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duggal_S/0/1/0/all/0/1\">Shivam Duggal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Rao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Srinath Sridhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmented Imagefication: A Data-driven Fault Detection Method for Aircraft Air Data Sensors. (arXiv:2206.09055v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09055","description":"<p>In this paper, a novel data-driven approach named Augmented Imagefication for\nFault detection (FD) of aircraft air data sensors (ADS) is proposed.\nExemplifying the FD problem of aircraft air data sensors, an online FD scheme\non edge device based on deep neural network (DNN) is developed. First, the\naircraft inertial reference unit measurements is adopted as equivalent inputs,\nwhich is scalable to different aircraft/flight cases. Data associated with 6\ndifferent aircraft/flight conditions are collected to provide diversity\n(scalability) in the training/testing database. Then Augmented Imagefication is\nproposed for the DNN-based prediction of flying conditions. The raw data are\nreshaped as a grayscale image for convolutional operation, and the necessity of\naugmentation is analyzed and pointed out. Different kinds of augmented method,\ni.e. Flip, Repeat, Tile and their combinations are discussed, the result shows\nthat the All Repeat operation in both axes of image matrix leads to the best\nperformance of DNN. The interpretability of DNN is studied based on Grad-CAM,\nwhich provide a better understanding and further solidifies the robustness of\nDNN. Next the DNN model, VGG-16 with augmented imagefication data is optimized\nfor mobile hardware deployment. After pruning of DNN, a lightweight model\n(98.79% smaller than original VGG-16) with high accuracy (slightly up by 0.27%)\nand fast speed (time delay is reduced by 87.54%) is obtained. And the\nhyperparameters optimization of DNN based on TPE is implemented and the best\ncombination of hyperparameters is determined (learning rate 0.001, iterative\nepochs 600, and batch size 100 yields the highest accuracy at 0.987). Finally,\na online FD deployment based on edge device, Jetson Nano, is developed and the\nreal time monitoring of aircraft is achieved. We believe that this method is\ninstructive for addressing the FD problems in other similar fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jinyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yiqun Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_J/0/1/0/all/0/1\">Jianliang Ai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11723","description":"<p>Deep convolutional autoencoders provide an effective tool for learning\nnon-linear dimensionality reduction in an unsupervised way. Recently, they have\nbeen used for the task of anomaly detection in the visual domain. By optimising\nfor the reconstruction error using anomaly-free examples, the common belief is\nthat a trained network will have difficulties to reconstruct anomalous parts\nduring the test phase. This is usually done by controlling the capacity of the\nnetwork by either reducing the size of the bottleneck layer or enforcing\nsparsity constraints on its activations. However, neither of these techniques\ndoes explicitly penalise reconstruction of anomalous signals often resulting in\na poor detection. We tackle this problem by adapting a self-supervised learning\nregime which allows to use discriminative information during training while\nregularising the model to focus on the data manifold by means of a modified\nreconstruction error resulting in an accurate detection. Unlike related\napproaches, the inference of the proposed method during training and prediction\nis very efficient processing the whole input image in one single step. Our\nexperiments on the MVTec Anomaly Detection dataset demonstrate high recognition\nand localisation performance of the proposed method. On the texture-subset, in\nparticular, our approach consistently outperforms a bunch of recent anomaly\ndetection methods by a big margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bauer_A/0/1/0/all/0/1\">Alexander Bauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DDPM-CD: Remote Sensing Change Detection using Denoising Diffusion Probabilistic Models. (arXiv:2206.11892v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11892","description":"<p>Human civilization has an increasingly powerful influence on the earth\nsystem, and earth observations are an invaluable tool for assessing and\nmitigating the negative impacts. To this end, observing precisely defined\nchanges on Earth's surface is essential, and we propose an effective way to\nachieve this goal. Notably, our change detection (CD)/ segmentation method\nproposes a novel way to incorporate the millions of off-the-shelf, unlabeled,\nremote sensing images available through different earth observation programs\ninto the training process through denoising diffusion probabilistic models. We\nfirst leverage the information from these off-the-shelf, uncurated, and\nunlabeled remote sensing images by using a pre-trained denoising diffusion\nprobabilistic model and then employ the multi-scale feature representations\nfrom the diffusion model decoder to train a lightweight CD classifier to detect\nprecise changes. The experiments performed on four publically available CD\ndatasets show that the proposed approach achieves remarkably better results\nthan the state-of-the-art methods in F1, IoU, and overall accuracy. Code and\npre-trained models are available at: https://github.com/wgcban/ddpm-cd\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_N/0/1/0/all/0/1\">Nithin Gopalakrishnan Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Your Sparse Neural Network Better with Any Mask. (arXiv:2206.12755v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.12755","description":"<p>Pruning large neural networks to create high-quality, independently trainable\nsparse masks, which can maintain similar performance to their dense\ncounterparts, is very desirable due to the reduced space and time complexity.\nAs research effort is focused on increasingly sophisticated pruning methods\nthat leads to sparse subnetworks trainable from the scratch, we argue for an\northogonal, under-explored theme: improving training techniques for pruned\nsub-networks, i.e. sparse training. Apart from the popular belief that only the\nquality of sparse masks matters for sparse training, in this paper we\ndemonstrate an alternative opportunity: one can carefully customize the sparse\ntraining techniques to deviate from the default dense network training\nprotocols, consisting of introducing ``ghost\" neurons and skip connections at\nthe early stage of training, and strategically modifying the initialization as\nwell as labels. Our new sparse training recipe is generally applicable to\nimproving training from scratch with various sparse masks. By adopting our\nnewly curated techniques, we demonstrate significant performance gains across\nvarious popular datasets (CIFAR-10, CIFAR-100, TinyImageNet), architectures\n(ResNet-18/32/104, Vgg16, MobileNet), and sparse mask options (lottery ticket,\nSNIP/GRASP, SynFlow, or even randomly pruning), compared to the default\ntraining protocols, especially at high sparsity levels. Code is at\nhttps://github.com/VITA-Group/ToST\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Ajay Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Aesthetics Assessment Using Graph Attention Network. (arXiv:2206.12869v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.12869","description":"<p>Aspect ratio and spatial layout are two of the principal factors determining\nthe aesthetic value of a photograph. But, incorporating these into the\ntraditional convolution-based frameworks for the task of image aesthetics\nassessment is problematic. The aspect ratio of the photographs gets distorted\nwhile they are resized/cropped to a fixed dimension to facilitate training\nbatch sampling. On the other hand, the convolutional filters process\ninformation locally and are limited in their ability to model the global\nspatial layout of a photograph. In this work, we present a two-stage framework\nbased on graph neural networks and address both these problems jointly. First,\nwe propose a feature-graph representation in which the input image is modelled\nas a graph, maintaining its original aspect ratio and resolution. Second, we\npropose a graph neural network architecture that takes this feature-graph and\ncaptures the semantic relationship between the different regions of the input\nimage using visual attention. Our experiments show that the proposed framework\nadvances the state-of-the-art results in aesthetic score regression on the\nAesthetic Visual Analysis (AVA) benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_K/0/1/0/all/0/1\">Koustav Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smolic_A/0/1/0/all/0/1\">Aljosa Smolic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Strategy Optimized Pix2pix Approach for SAR-to-Optical Image Translation Task. (arXiv:2206.13042v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13042","description":"<p>This technical report summarizes the analysis and approach on the\nimage-to-image translation task in the Multimodal Learning for Earth and\nEnvironment Challenge (MultiEarth 2022). In terms of strategy optimization,\ncloud classification is utilized to filter optical images with dense cloud\ncoverage to aid the supervised learning alike approach. The commonly used\npix2pix framework with a few optimizations is applied to build the model. A\nweighted combination of mean squared error and mean absolute error is\nincorporated in the loss function. As for evaluation, peak to signal ratio and\nstructural similarity were both considered in our preliminary analysis. Lastly,\nour method achieved the second place with a final error score of 0.0412. The\nresults indicate great potential towards SAR-to-optical translation in remote\nsensing tasks, specifically for the support of long-term environmental\nmonitoring and protection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fujian Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yashu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chunlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kezhao Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Key-frame Guided Network for Thyroid Nodule Recognition using Ultrasound Videos. (arXiv:2206.13318v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13318","description":"<p>Ultrasound examination is widely used in the clinical diagnosis of thyroid\nnodules (benign/malignant). However, the accuracy relies heavily on radiologist\nexperience. Although deep learning techniques have been investigated for\nthyroid nodules recognition. Current solutions are mainly based on static\nultrasound images, with limited temporal information used and inconsistent with\nclinical diagnosis. This paper proposes a novel method for the automated\nrecognition of thyroid nodules through an exhaustive exploration of ultrasound\nvideos and key-frames. We first propose a detection-localization framework to\nautomatically identify the clinical key-frames with typical nodules in each\nultrasound video. Based on the localized key-frames, we develop a key-frame\nguided video classification model for thyroid nodule recognition. Besides, we\nintroduce motion attention module to help network focus on significant frames\nin an ultrasound video, which is consistent with clinical diagnosis. The\nproposed thyroid nodule recognition framework is validated on clinically\ncollected ultrasound videos, demonstrating superior performance compared with\nother state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiangxiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Meng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shi Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Structured Prediction for Facial Landmark Detection. (arXiv:2010.09035v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2010.09035","description":"<p>Existing deep learning based facial landmark detection methods have achieved\nexcellent performance. These methods, however, do not explicitly embed the\nstructural dependencies among landmark points. They hence cannot preserve the\ngeometric relationships between landmark points or generalize well to\nchallenging conditions or unseen data. This paper proposes a method for deep\nstructured facial landmark detection based on combining a deep Convolutional\nNetwork with a Conditional Random Field. We demonstrate its superior\nperformance to existing state-of-the-art techniques in facial landmark\ndetection, especially a better generalization ability on challenging datasets\nthat include large pose and occlusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lisha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hui Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1\">Qiang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn Fast, Segment Well: Fast Object Segmentation Learning on the iCub Robot. (arXiv:2206.13462v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2206.13462","description":"<p>The visual system of a robot has different requirements depending on the\napplication: it may require high accuracy or reliability, be constrained by\nlimited resources or need fast adaptation to dynamically changing environments.\nIn this work, we focus on the instance segmentation task and provide a\ncomprehensive study of different techniques that allow adapting an object\nsegmentation model in presence of novel objects or different domains. We\npropose a pipeline for fast instance segmentation learning designed for robotic\napplications where data come in stream. It is based on an hybrid method\nleveraging on a pre-trained CNN for feature extraction and fast-to-train\nKernel-based classifiers. We also propose a training protocol that allows to\nshorten the training time by performing feature extraction during the data\nacquisition. We benchmark the proposed pipeline on two robotics datasets and we\ndeploy it on a real robot, i.e. the iCub humanoid. To this aim, we adapt our\nmethod to an incremental setting in which novel objects are learned on-line by\nthe robot. The code to reproduce the experiments is publicly available on\nGitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ceola_F/0/1/0/all/0/1\">Federico Ceola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiettini_E/0/1/0/all/0/1\">Elisa Maiettini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasquale_G/0/1/0/all/0/1\">Giulia Pasquale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meanti_G/0/1/0/all/0/1\">Giacomo Meanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosasco_L/0/1/0/all/0/1\">Lorenzo Rosasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natale_L/0/1/0/all/0/1\">Lorenzo Natale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}