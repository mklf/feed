{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Using Natural Sentences for Understanding Biases in Language Models. (arXiv:2205.06303v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06303","description":"<p>Evaluation of biases in language models is often limited to synthetically\ngenerated datasets. This dependence traces back to the need for a prompt-style\ndataset to trigger specific behaviors of language models. In this paper, we\naddress this gap by creating a prompt dataset with respect to occupations\ncollected from real-world natural sentences present in Wikipedia. We aim to\nunderstand the differences between using template-based prompts and natural\nsentence prompts when studying gender-occupation biases in language models. We\nfind bias evaluations are very sensitive to the design choices of template\nprompts, and we propose using natural sentence prompts for systematic\nevaluations to step away from design choices that could introduce bias in the\nobservations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alnegheimish_S/0/1/0/all/0/1\">Sarah Alnegheimish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_A/0/1/0/all/0/1\">Alicia Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noun2Verb: Probabilistic frame semantics for word class conversion. (arXiv:2205.06321v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06321","description":"<p>Humans can flexibly extend word usages across different grammatical classes,\na phenomenon known as word class conversion. Noun-to-verb conversion, or\ndenominal verb (e.g., to Google a cheap flight), is one of the most prevalent\nforms of word class conversion. However, existing natural language processing\nsystems are impoverished in interpreting and generating novel denominal verb\nusages. Previous work has suggested that novel denominal verb usages are\ncomprehensible if the listener can compute the intended meaning based on shared\nknowledge with the speaker. Here we explore a computational formalism for this\nproposal couched in frame semantics. We present a formal framework, Noun2Verb,\nthat simulates the production and comprehension of novel denominal verb usages\nby modeling shared knowledge of speaker and listener in semantic frames. We\nevaluate an incremental set of probabilistic models that learn to interpret and\ngenerate novel denominal verb usages via paraphrasing. We show that a model\nwhere the speaker and listener cooperatively learn the joint distribution over\nsemantic frame elements better explains the empirical denominal verb usages\nthan state-of-the-art language models, evaluated against data from 1)\ncontemporary English in both adult and child speech, 2) contemporary Mandarin\nChinese, and 3) the historical development of English. Our work grounds word\nclass conversion in probabilistic frame semantics and bridges the gap between\nnatural language processing systems and humans in lexical creativity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Economics of Multilingual Few-shot Learning: Modeling the Cost-Performance Trade-offs of Machine Translated and Manual Data. (arXiv:2205.06350v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06350","description":"<p>Borrowing ideas from {\\em Production functions} in micro-economics, in this\npaper we introduce a framework to systematically evaluate the performance and\ncost trade-offs between machine-translated and manually-created labelled data\nfor task-specific fine-tuning of massively multilingual language models. We\nillustrate the effectiveness of our framework through a case-study on the\nTyDIQA-GoldP dataset. One of the interesting conclusions of the study is that\nif the cost of machine translation is greater than zero, the optimal\nperformance at least cost is always achieved with at least some or only\nmanually-created data. To our knowledge, this is the first attempt towards\nextending the concept of production functions to study data collection\nstrategies for training multilingual models, and can serve as a valuable tool\nfor other similar cost vs data trade-offs in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kabir Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Static Models and Test Sets: Benchmarking the Potential of Pre-trained Models Across Tasks and Languages. (arXiv:2205.06356v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06356","description":"<p>Although recent Massively Multilingual Language Models (MMLMs) like mBERT and\nXLMR support around 100 languages, most existing multilingual NLP benchmarks\nprovide evaluation data in only a handful of these languages with little\nlinguistic diversity. We argue that this makes the existing practices in\nmultilingual evaluation unreliable and does not provide a full picture of the\nperformance of MMLMs across the linguistic landscape. We propose that the\nrecent work done in Performance Prediction for NLP tasks can serve as a\npotential solution in fixing benchmarking in Multilingual NLP by utilizing\nfeatures related to data and language typology to estimate the performance of\nan MMLM on different languages. We compare performance prediction with\ntranslating test data with a case study on four different multilingual\ndatasets, and observe that these methods can provide reliable estimates of the\nperformance that are often on-par with the translation based approaches,\nwithout the need for any additional translation as well as evaluation costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kabir Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design and Implementation of a Quantum Kernel for Natural Language Processing. (arXiv:2205.06409v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06409","description":"<p>Natural language processing (NLP) is the field that attempts to make human\nlanguage accessible to computers, and it relies on applying a mathematical\nmodel to express the meaning of symbolic language. One such model, DisCoCat,\ndefines how to express both the meaning of individual words as well as their\ncompositional nature. This model can be naturally implemented on quantum\ncomputers, leading to the field quantum NLP (QNLP). Recent experimental work\nused quantum machine learning techniques to map from text to class label using\nthe expectation value of the quantum encoded sentence. Theoretical work has\nbeen done on computing the similarity of sentences but relies on an unrealized\nquantum memory store. The main goal of this thesis is to leverage the DisCoCat\nmodel to design a quantum-based kernel function that can be used by a support\nvector machine (SVM) for NLP tasks. Two similarity measures were studied: (i)\nthe transition amplitude approach and (ii) the SWAP test. A simple NLP meaning\nclassification task from previous work was used to train the word embeddings\nand evaluate the performance of both models. The Python module lambeq and its\nrelated software stack was used for implementation. The explicit model from\nprevious work was used to train word embeddings and achieved a testing accuracy\nof $93.09 \\pm 0.01$%. It was shown that both the SVM variants achieved a higher\ntesting accuracy of $95.72 \\pm 0.01$% for approach (i) and $97.14 \\pm 0.01$%\nfor (ii). The SWAP test was then simulated under a noise model defined by the\nreal quantum device, ibmq_guadalupe. The explicit model achieved an accuracy of\n$91.94 \\pm 0.01$% while the SWAP test SVM achieved 96.7% on the testing\ndataset, suggesting that the kernelized classifiers are resilient to noise.\nThese are encouraging results and motivate further investigations of our\nproposed kernelized QNLP paradigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wright_M/0/1/0/all/0/1\">Matt Wright</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages. (arXiv:2205.06435v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06435","description":"<p>Recently, the structural reading comprehension (SRC) task on web pages has\nattracted increasing research interests. Although previous SRC work has\nleveraged extra information such as HTML tags or XPaths, the informative\ntopology of web pages is not effectively exploited. In this work, we propose a\nTopological Information Enhanced model (TIE), which transforms the token-level\ntask into a tag-level task by introducing a two-stage process (i.e. node\nlocating and answer refining). Based on that, TIE integrates Graph Attention\nNetwork (GAT) and Pre-trained Language Model (PLM) to leverage the topological\ninformation of both logical structures and spatial structures. Experimental\nresults demonstrate that our model outperforms strong baselines and achieves\nstate-of-the-art performances on the web-based SRC benchmark WebSRC at the time\nof writing. The code of TIE will be publicly available at\nhttps://github.com/X-LANCE/TIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Ruisheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongshen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Low-Cost, Controllable and Interpretable Task-Oriented Chatbot: With Real-World After-Sale Services as Example. (arXiv:2205.06436v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06436","description":"<p>Though widely used in industry, traditional task-oriented dialogue systems\nsuffer from three bottlenecks: (i) difficult ontology construction (e.g.,\nintents and slots); (ii) poor controllability and interpretability; (iii)\nannotation-hungry. In this paper, we propose to represent utterance with a\nsimpler concept named Dialogue Action, upon which we construct a\ntree-structured TaskFlow and further build task-oriented chatbot with TaskFlow\nas core component. A framework is presented to automatically construct TaskFlow\nfrom large-scale dialogues and deploy online. Our experiments on real-world\nafter-sale customer services show TaskFlow can satisfy the major needs, as well\nas reduce the developer burden effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xi_X/0/1/0/all/0/1\">Xiangyu Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chenxu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yuncheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chaobo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaipeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1\">Guanglu Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AEON: A Method for Automatic Evaluation of NLP Test Cases. (arXiv:2205.06439v1 [cs.SE])","link":"http://arxiv.org/abs/2205.06439","description":"<p>Due to the labor-intensive nature of manual test oracle construction, various\nautomated testing techniques have been proposed to enhance the reliability of\nNatural Language Processing (NLP) software. In theory, these techniques mutate\nan existing test case (e.g., a sentence with its label) and assume the\ngenerated one preserves an equivalent or similar semantic meaning and thus, the\nsame label. However, in practice, many of the generated test cases fail to\npreserve similar semantic meaning and are unnatural (e.g., grammar errors),\nwhich leads to a high false alarm rate and unnatural test cases. Our evaluation\nstudy finds that 44% of the test cases generated by the state-of-the-art (SOTA)\napproaches are false alarms. These test cases require extensive manual checking\neffort, and instead of improving NLP software, they can even degrade NLP\nsoftware when utilized in model training. To address this problem, we propose\nAEON for Automatic Evaluation Of NLP test cases. For each generated test case,\nit outputs scores based on semantic similarity and language naturalness. We\nemploy AEON to evaluate test cases generated by four popular testing techniques\non five datasets across three typical NLP tasks. The results show that AEON\naligns the best with human judgment. In particular, AEON achieves the best\naverage precision in detecting semantic inconsistent test cases, outperforming\nthe best baseline metric by 10%. In addition, AEON also has the highest average\nprecision of finding unnatural test cases, surpassing the baselines by more\nthan 15%. Moreover, model training with test cases prioritized by AEON leads to\nmodels that are more accurate and robust, demonstrating AEON's potential in\nimproving NLP software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pinjia He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuxin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple and Effective Relation-based Embedding Propagation for Knowledge Representation Learning. (arXiv:2205.06456v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06456","description":"<p>Relational graph neural networks have garnered particular attention to encode\ngraph context in knowledge graphs (KGs). Although they achieved competitive\nperformance on small KGs, how to efficiently and effectively utilize graph\ncontext for large KGs remains an open problem. To this end, we propose the\nRelation-based Embedding Propagation (REP) method. It is a post-processing\ntechnique to adapt pre-trained KG embeddings with graph context. As relations\nin KGs are directional, we model the incoming head context and the outgoing\ntail context separately. Accordingly, we design relational context functions\nwith no external parameters. Besides, we use averaging to aggregate context\ninformation, making REP more computation-efficient. We theoretically prove that\nsuch designs can avoid information distortion during propagation. Extensive\nexperiments also demonstrate that REP has significant scalability while\nimproving or maintaining prediction quality. Notably, it averagely brings about\n10% relative improvement to triplet-based embedding methods on OGBL-WikiKG2 and\ntakes 5%-83% time to achieve comparable results as the state-of-the-art GC-OTE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Siming Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weiyue Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Hui Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zeyang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shikun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dianhai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation. (arXiv:2205.06457v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06457","description":"<p>We present ViT5, a pretrained Transformer-based encoder-decoder model for the\nVietnamese language. With T5-style self-supervised pretraining, ViT5 is trained\non a large corpus of high-quality and diverse Vietnamese texts. We benchmark\nViT5 on two downstream text generation tasks, Abstractive Text Summarization\nand Named Entity Recognition. Although Abstractive Text Summarization has been\nwidely studied for the English language thanks to its rich and large source of\ndata, there has been minimal research into the same task in Vietnamese, a much\nlower resource language. In this work, we perform exhaustive experiments on\nboth Vietnamese Abstractive Summarization and Named Entity Recognition,\nvalidating the performance of ViT5 against many other pretrained\nTransformer-based encoder-decoder models. Our experiments show that ViT5\nsignificantly outperforms existing models and achieves state-of-the-art results\non Vietnamese Text Summarization. On the task of Named Entity Recognition, ViT5\nis competitive against previous best results from pretrained encoder-based\nTransformer models. Further analysis shows the importance of context length\nduring the self-supervised pretraining on downstream performance across\ndifferent settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trinh_T/0/1/0/all/0/1\">Trieu H. Trinh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Generation of Captions and Subtitles with Dual Decoding. (arXiv:2205.06522v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06522","description":"<p>As the amount of audio-visual content increases, the need to develop\nautomatic captioning and subtitling solutions to match the expectations of a\ngrowing international audience appears as the only viable way to boost\nthroughput and lower the related post-production costs. Automatic captioning\nand subtitling often need to be tightly intertwined to achieve an appropriate\nlevel of consistency and synchronization with each other and with the video\nsignal. In this work, we assess a dual decoding scheme to achieve a strong\ncoupling between these two tasks and show how adequacy and consistency are\nincreased, with virtually no additional cost in terms of model size and\ntraining complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jitao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buet_F/0/1/0/all/0/1\">Fran&#xe7;ois Buet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crego_J/0/1/0/all/0/1\">Josep Crego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertin_Lemee_E/0/1/0/all/0/1\">Elise Bertin-Lem&#xe9;e</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Productivity Assessment of Neural Code Completion. (arXiv:2205.06537v1 [cs.SE])","link":"http://arxiv.org/abs/2205.06537","description":"<p>Neural code synthesis has reached a point where snippet generation is\naccurate enough to be considered for integration into human software\ndevelopment workflows. Commercial products aim to increase programmers'\nproductivity, without being able to measure it directly. In this case study, we\nasked users of GitHub Copilot about its impact on their productivity, and\nsought to find a reflection of their perception in directly measurable user\ndata. We find that the rate with which shown suggestions are accepted, rather\nthan more specific metrics regarding the persistence of completions in the code\nover time, drives developers' perception of productivity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_A/0/1/0/all/0/1\">Albert Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalliamvakou_E/0/1/0/all/0/1\">Eirini Kalliamvakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simister_S/0/1/0/all/0/1\">Shawn Simister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sittampalam_G/0/1/0/all/0/1\">Ganesh Sittampalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alice Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rice_A/0/1/0/all/0/1\">Andrew Rice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rifkin_D/0/1/0/all/0/1\">Devon Rifkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aftandilian_E/0/1/0/all/0/1\">Edward Aftandilian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Question Answering Datasets and Their Generalizability: Are They Enough for Future Research?. (arXiv:2205.06573v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06573","description":"<p>Existing approaches on Question Answering over Knowledge Graphs (KGQA) have\nweak generalizability. That is often due to the standard i.i.d. assumption on\nthe underlying dataset. Recently, three levels of generalization for KGQA were\ndefined, namely i.i.d., compositional, zero-shot. We analyze 25 well-known KGQA\ndatasets for 5 different Knowledge Graphs (KGs). We show that according to this\ndefinition many existing and online available KGQA datasets are either not\nsuited to train a generalizable KGQA system or that the datasets are based on\ndiscontinued and out-dated KGs. Generating new datasets is a costly process\nand, thus, is not an alternative to smaller research groups and companies. In\nthis work, we propose a mitigation method for re-splitting available KGQA\ndatasets to enable their applicability to evaluate generalization, without any\ncost and manual effort. We test our hypothesis on three KGQA datasets, i.e.,\nLC-QuAD, LC-QuAD 2.0 and QALD-9). Experiments on re-splitted KGQA datasets\ndemonstrate its effectiveness towards generalizability. The code and a unified\nway to access 18 available datasets is online at\nhttps://github.com/semantic-systems/KGQA-datasets as well as\nhttps://github.com/semantic-systems/KGQA-datasets-generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Longquan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Contextual Representation with Gloss Regularized Pre-training. (arXiv:2205.06603v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06603","description":"<p>Though achieving impressive results on many NLP tasks, the BERT-like masked\nlanguage models (MLM) encounter the discrepancy between pre-training and\ninference. In light of this gap, we investigate the contextual representation\nof pre-training and inference from the perspective of word probability\ndistribution. We discover that BERT risks neglecting the contextual word\nsimilarity in pre-training. To tackle this issue, we propose an auxiliary gloss\nregularizer module to BERT pre-training (GR-BERT), to enhance word semantic\nsimilarity. By predicting masked words and aligning contextual embeddings to\ncorresponding glosses simultaneously, the word similarity can be explicitly\nmodeled. We design two architectures for GR-BERT and evaluate our model in\ndownstream tasks. Experimental results show that the gloss regularizer benefits\nBERT in word-level and sentence-level semantic representation. The GR-BERT\nachieves new state-of-the-art in lexical substitution task and greatly promotes\nBERT sentence representation in both unsupervised and supervised STS tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhecheng An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Text Classification using Supervision Signals from a Language Model. (arXiv:2205.06604v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06604","description":"<p>Solving text classification in a weakly supervised manner is important for\nreal-world applications where human annotations are scarce. In this paper, we\npropose to query a masked language model with cloze style prompts to obtain\nsupervision signals. We design a prompt which combines the document itself and\n\"this article is talking about [MASK].\" A masked language model can generate\nwords for the [MASK] token. The generated words which summarize the content of\na document can be utilized as supervision signals. We propose a latent variable\nmodel to learn a word distribution learner which associates generated words to\npre-defined categories and a document classifier simultaneously without using\nany annotated data. Evaluation on three datasets, AGNews, 20Newsgroups, and\nUCINews, shows that our method can outperform baselines by 2%, 4%, and 3%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziqian Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_W/0/1/0/all/0/1\">Weimin Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinran Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Devil is in the Details: On the Pitfalls of Vocabulary Selection in Neural Machine Translation. (arXiv:2205.06618v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06618","description":"<p>Vocabulary selection, or lexical shortlisting, is a well-known technique to\nimprove latency of Neural Machine Translation models by constraining the set of\nallowed output words during inference. The chosen set is typically determined\nby separately trained alignment model parameters, independent of the\nsource-sentence context at inference time. While vocabulary selection appears\ncompetitive with respect to automatic quality metrics in prior work, we show\nthat it can fail to select the right set of output words, particularly for\nsemantically non-compositional linguistic phenomena such as idiomatic\nexpressions, leading to reduced translation quality as perceived by humans.\nTrading off latency for quality by increasing the size of the allowed set is\noften not an option in real-world scenarios. We propose a model of vocabulary\nselection, integrated into the neural translation model, that predicts the set\nof allowed output words from contextualized encoder representations. This\nrestores translation quality of an unconstrained system, as measured by human\nevaluations on WMT newstest2020 and idiomatic expressions, at an inference\nlatency competitive with alignment-based selection using aggressive thresholds,\nthereby removing the dependency on separately trained alignment models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Domhan_T/0/1/0/all/0/1\">Tobias Domhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasler_E/0/1/0/all/0/1\">Eva Hasler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Ke Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trenous_S/0/1/0/all/0/1\">Sony Trenous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">Bill Byrne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hieber_F/0/1/0/all/0/1\">Felix Hieber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Hate Speech Data along Racial, Gender and Intersectional Axes. (arXiv:2205.06621v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06621","description":"<p>To tackle the rising phenomenon of hate speech, efforts have been made\ntowards data curation and analysis. When it comes to analysis of bias, previous\nwork has focused predominantly on race. In our work, we further investigate\nbias in hate speech datasets along racial, gender and intersectional axes. We\nidentify strong bias against African American English (AAE), masculine and\nAAE+Masculine tweets, which are annotated as disproportionately more hateful\nand offensive than from other demographics. We provide evidence that BERT-based\nmodels propagate this bias and show that balancing the training data for these\nprotected attributes can lead to fairer models with regards to gender, but not\nrace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maronikolakis_A/0/1/0/all/0/1\">Antonis Maronikolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baader_P/0/1/0/all/0/1\">Philip Baader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling Translation Formality Using Pre-trained Multilingual Language Models. (arXiv:2205.06644v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06644","description":"<p>This paper describes the University of Maryland's submission to the Special\nTask on Formality Control for Spoken Language Translation at \\iwslt, which\nevaluates translation from English into 6 languages with diverse grammatical\nformality markers. We investigate to what extent this problem can be addressed\nwith a \\textit{single multilingual model}, simultaneously controlling its\noutput for target language and formality. Results show that this strategy can\napproach the translation quality and formality control achieved by dedicated\ntranslation models. However, the nature of the underlying pre-trained language\nmodel and of the finetuning samples greatly impact results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rippeth_E/0/1/0/all/0/1\">Elijah Rippeth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sweta Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1\">Marine Carpuat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Modeling of Multi-Domain Multi-Device ASR Systems. (arXiv:2205.06655v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06655","description":"<p>Modern Automatic Speech Recognition (ASR) systems often use a portfolio of\ndomain-specific models in order to get high accuracy for distinct user\nutterance types across different devices. In this paper, we propose an\ninnovative approach that integrates the different per-domain per-device models\ninto a unified model, using a combination of domain embedding, domain experts,\nmixture of experts and adversarial training. We run careful ablation studies to\nshow the benefit of each of these innovations in contributing to the accuracy\nof the overall unified model. Experiments show that our proposed unified\nmodeling approach actually outperforms the carefully tuned per-domain models,\ngiving relative gains of up to 10% over a baseline model with negligible\nincrease in the number of parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1\">Soumyajit Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1\">Swayambhu Nath Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padi_B/0/1/0/all/0/1\">Bharat Padi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1\">Arunasish Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilgi_R/0/1/0/all/0/1\">Raghavendra Bilgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arsikere_H/0/1/0/all/0/1\">Harish Arsikere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shalini Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasamurthy_A/0/1/0/all/0/1\">Ajay Srinivasamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_S/0/1/0/all/0/1\">Sri Garimella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Case for a Legal Compliance API for the Enforcement of the EU's Digital Services Act on Social Media Platforms. (arXiv:2205.06666v1 [cs.CY])","link":"http://arxiv.org/abs/2205.06666","description":"<p>In the course of under a year, the European Commission has launched some of\nthe most important regulatory proposals to date on platform governance. The\nCommission's goals behind cross-sectoral regulation of this sort include the\nprotection of markets and democracies alike. While all these acts propose\nsophisticated rules for setting up new enforcement institutions and procedures,\none aspect remains highly unclear: how digital enforcement will actually take\nplace in practice. Focusing on the Digital Services Act (DSA), this discussion\npaper critically addresses issues around social media data access for the\npurpose of digital enforcement and proposes the use of a legal compliance\napplication programming interface (API) as a means to facilitate compliance\nwith the DSA and complementary European and national regulation. To\ncontextualize this discussion, the paper pursues two scenarios that exemplify\nthe harms arising out of content monetization affecting a particularly\nvulnerable category of social media users: children. The two scenarios are used\nto further reflect upon essential issues surrounding data access and legal\ncompliance with the DSA and further applicable legal standards in the field of\nlabour and consumer law.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goanta_C/0/1/0/all/0/1\">Catalina Goanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertaglia_T/0/1/0/all/0/1\">Thales Bertaglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iamnitchi_A/0/1/0/all/0/1\">Adriana Iamnitchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LSCDiscovery: A shared task on semantic change discovery and detection in Spanish. (arXiv:2205.06691v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06691","description":"<p>We present the first shared task on semantic change discovery and detection\nin Spanish and create the first dataset of Spanish words manually annotated for\nsemantic change using the DURel framework (Schlechtweg et al., 2018). The task\nis divided in two phases: 1) Graded Change Discovery, and 2) Binary Change\nDetection. In addition to introducing a new language the main novelty with\nrespect to the previous tasks consists in predicting and evaluating changes for\nall vocabulary words in the corpus. Six teams participated in phase 1 and seven\nteams in phase 2 of the shared task, and the best system obtained a Spearman\nrank correlation of 0.735 for phase 1 and an F1 score of 0.716 for phase 2. We\ndescribe the systems developed by the competing teams, highlighting the\ntechniques that were particularly useful and discuss the limits of these\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zamora_Reina_F/0/1/0/all/0/1\">Frank D. Zamora-Reina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bravo_Marquez_F/0/1/0/all/0/1\">Felipe Bravo-Marquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlechtweg_D/0/1/0/all/0/1\">Dominik Schlechtweg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuCPAD: A Multi-Domain Chinese Predicate-Argument Dataset. (arXiv:2205.06703v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06703","description":"<p>During the past decade, neural network models have made tremendous progress\non in-domain semantic role labeling (SRL). However, performance drops\ndramatically under the out-of-domain setting. In order to facilitate research\non cross-domain SRL, this paper presents MuCPAD, a multi-domain Chinese\npredicate-argument dataset, which consists of 30,897 sentences and 92,051\npredicates from six different domains. MuCPAD exhibits three important\nfeatures. 1) Based on a frame-free annotation methodology, we avoid writing\ncomplex frames for new predicates. 2) We explicitly annotate omitted core\narguments to recover more complete semantic structure, considering that\nomission of content words is ubiquitous in multi-domain Chinese texts. 3) We\ncompile 53 pages of annotation guidelines and adopt strict double annotation\nfor improving data quality. This paper describes in detail the annotation\nmethodology and annotation process of MuCPAD, and presents in-depth data\nanalysis. We also give benchmark results on cross-domain SRL based on MuCPAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qingrong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Numerical Reasoning Skills of Pretrained Language Models. (arXiv:2205.06733v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06733","description":"<p>State-of-the-art pretrained language models tend to perform below their\ncapabilities when applied out-of-the-box on tasks that require reasoning over\nnumbers. Recent work sees two main reasons for this: (1) popular tokenisation\nalgorithms are optimized for common words, and therefore have limited\nexpressiveness for numbers, and (2) common pretraining objectives do not target\nnumerical reasoning or understanding numbers at all. Recent approaches usually\naddress them separately and mostly by proposing architectural changes or\npretraining models from scratch. In this paper, we propose a new extended\npretraining approach called reasoning-aware pretraining to jointly address both\nshortcomings without requiring architectural changes or pretraining from\nscratch. Using contrastive learning, our approach incorporates an alternative\nnumber representation into an already pretrained model, while improving its\nnumerical reasoning skills by training on a novel pretraining objective called\ninferable number prediction task. We evaluate our approach on three different\ntasks that require numerical reasoning, including (a) reading comprehension in\nthe DROP dataset, (b) inference-on-tables in the InfoTabs dataset, and (c)\ntable-to-text generation in WikiBio and SciGen datasets. Our results on DROP\nand InfoTabs show that our approach improves the accuracy by 9.6 and 33.9\npoints on these datasets, respectively. Our human evaluation on SciGen and\nWikiBio shows that our approach improves the factual correctness on all\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrak_D/0/1/0/all/0/1\">Dominic Petrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_N/0/1/0/all/0/1\">Nafise Sadat Moosavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An empirical study of CTC based models for OCR of Indian languages. (arXiv:2205.06740v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06740","description":"<p>Recognition of text on word or line images, without the need for sub-word\nsegmentation has become the mainstream of research and development of text\nrecognition for Indian languages. Modelling unsegmented sequences using\nConnectionist Temporal Classification (CTC) is the most commonly used approach\nfor segmentation-free OCR. In this work we present a comprehensive empirical\nstudy of various neural network models that uses CTC for transcribing step-wise\npredictions in the neural network output to a Unicode sequence. The study is\nconducted for 13 Indian languages, using an internal dataset that has around\n1000 pages per language. We study the choice of line vs word as the recognition\nunit, and use of synthetic data to train the models. We compare our models with\npopular publicly available OCR tools for end-to-end document image recognition.\nOur end-to-end pipeline that employ our recognition models and existing text\nsegmentation tools outperform these public OCR tools for 8 out of the 13\nlanguages. We also introduce a new public dataset called Mozhi for word and\nline recognition in Indian language. The dataset contains more than 1.2 million\nannotated word images (120 thousand text lines) across 13 Indian languages. Our\ncode, trained models and the Mozhi dataset will be made available at\n<a href=\"http://cvit.iiit.ac.in/research/projects/cvit-projects/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1\">Minesh Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">CV Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who Are We Talking About? Handling Person Names in Speech Translation. (arXiv:2205.06755v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06755","description":"<p>Recent work has shown that systems for speech translation (ST) -- similarly\nto automatic speech recognition (ASR) -- poorly handle person names. This\nshortcoming does not only lead to errors that can seriously distort the meaning\nof the input, but also hinders the adoption of such systems in application\nscenarios (like computer-assisted interpreting) where the translation of named\nentities, like person names, is crucial. In this paper, we first analyse the\noutputs of ASR/ST systems to identify the reasons of failures in person name\ntranscription/translation. Besides the frequency in the training data, we\npinpoint the nationality of the referred person as a key factor. We then\nmitigate the problem by creating multilingual models, and further improve our\nST systems by forcing them to jointly generate transcripts and translations,\nprioritising the former over the latter. Overall, our solutions result in a\nrelative improvement in token-level person name accuracy by 47.8% on average\nfor three language pairs (en-&gt;es,fr,it).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interlock-Free Multi-Aspect Rationalization for Text Classification. (arXiv:2205.06756v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06756","description":"<p>Explanation is important for text classification tasks. One prevalent type of\nexplanation is rationales, which are text snippets of input text that suffice\nto yield the prediction and are meaningful to humans. A lot of research on\nrationalization has been based on the selective rationalization framework,\nwhich has recently been shown to be problematic due to the interlocking\ndynamics. In this paper, we show that we address the interlocking problem in\nthe multi-aspect setting, where we aim to generate multiple rationales for\nmultiple outputs. More specifically, we propose a multi-stage training method\nincorporating an additional self-supervised contrastive loss that helps to\ngenerate more semantically diverse rationales. Empirical results on the beer\nreview dataset show that our method improves significantly the rationalization\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuangqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter-Based Gender Recognition Using Transformers. (arXiv:2205.06801v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06801","description":"<p>Social media contains useful information about people and the society that\ncould help advance research in many different areas (e.g. by applying opinion\nmining, emotion/sentiment analysis, and statistical analysis) such as business\nand finance, health, socio-economic inequality and gender vulnerability. User\ndemographics provide rich information that could help study the subject\nfurther. However, user demographics such as gender are considered private and\nare not freely available. In this study, we propose a model based on\ntransformers to predict the user's gender from their images and tweets. We\nfine-tune a model based on Vision Transformers (ViT) to stratify female and\nmale images. Next, we fine-tune another model based on Bidirectional Encoders\nRepresentations from Transformers (BERT) to recognize the user's gender by\ntheir tweets. This is highly beneficial, because not all users provide an image\nthat indicates their gender. The gender of such users could be detected form\ntheir tweets. The combination model improves the accuracy of image and text\nclassification models by 6.98% and 4.43%, respectively. This shows that the\nimage and text classification models are capable of complementing each other by\nproviding additional information to one another. We apply our method to the\nPAN-2018 dataset, and obtain an accuracy of 85.52%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nia_Z/0/1/0/all/0/1\">Zahra Movahedi Nia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_A/0/1/0/all/0/1\">Ali Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellado_B/0/1/0/all/0/1\">Bruce Mellado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianhong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orbinski_J/0/1/0/all/0/1\">James Orbinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agary_A/0/1/0/all/0/1\">Ali Agary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_J/0/1/0/all/0/1\">Jude Dzevela Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Embeddings and Validity Indexes in Fuzzy Clustering. (arXiv:2205.06802v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06802","description":"<p>In the new era of internet systems and applications, a concept of detecting\ndistinguished topics from huge amounts of text has gained a lot of attention.\nThese methods use representation of text in a numerical format -- called\nembeddings -- to imitate human-based semantic similarity between words. In this\nstudy, we perform a fuzzy-based analysis of various vector representations of\nwords, i.e., word embeddings. Also we introduce new methods of fuzzy clustering\nbased on hybrid implementation of fuzzy clustering methods with an evolutionary\nalgorithm named Forest Optimization. We use two popular fuzzy clustering\nalgorithms on count-based word embeddings, with different methods and\ndimensionality. Words about covid from Kaggle dataset gathered and calculated\ninto vectors and clustered. The results indicate that fuzzy clustering\nalgorithms are very sensitive to high-dimensional data, and parameter tuning\ncan dramatically change their performance. We evaluate results of experiments\nwith various clustering validity indexes to compare different algorithm\nvariation with different embeddings accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toufani_Movaghar_D/0/1/0/all/0/1\">Danial Toufani-Movaghar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PaCo: Preconditions Attributed to Commonsense Knowledge. (arXiv:2104.08712v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08712","description":"<p>Humans can seamlessly reason with circumstantial preconditions of commonsense\nknowledge. We understand that a glass is used for drinking water, unless the\nglass is broken or the water is toxic. Despite state-of-the-art (SOTA) language\nmodels' (LMs) impressive performance on inferring commonsense knowledge, it is\nunclear whether they understand the circumstantial preconditions. To address\nthis gap, we propose a novel challenge of reasoning with circumstantial\npreconditions. We collect a dataset, called PaCo, consisting of 12.4 thousand\npreconditions of commonsense statements expressed in natural language. Based on\nthis dataset, we create three canonical evaluation tasks and use them to\nexamine the capability of existing LMs to understand situational preconditions.\nOur results reveal a 10-30% gap between machine and human performance on our\ntasks, which shows that reasoning with preconditions is an open challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qasemi_E/0/1/0/all/0/1\">Ehsan Qasemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szekely_P/0/1/0/all/0/1\">Pedro Szekely</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-train or Annotate? Domain Adaptation with a Constrained Budget. (arXiv:2109.04711v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04711","description":"<p>Recent work has demonstrated that pre-training in-domain language models can\nboost performance when adapting to a new domain. However, the costs associated\nwith pre-training raise an important question: given a fixed budget, what steps\nshould an NLP practitioner take to maximize performance? In this paper, we view\ndomain adaptation with a constrained budget as a consumer choice problem, where\nthe goal is to select an optimal combination of data annotation and\npre-training. We measure annotation costs of three procedural text datasets,\nalong with the pre-training costs of several in-domain language models. The\nutility of different combinations of pre-training and data annotation are\nevaluated under varying budget constraints to assess which combination strategy\nworks best. We find that for small budgets, spending all funds on annotation\nleads to the best performance; once the budget becomes large enough, however, a\ncombination of data annotation and in-domain pre-training yields better\nperformance. Our experiments suggest task-specific data annotation should be\npart of an economical strategy when adapting an NLP model to a new domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1\">Fan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoKE: A Prompt-based Knowledge Eliciting Approach for Event Argument Extraction. (arXiv:2109.05190v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05190","description":"<p>Eliciting knowledge from pre-trained language models via prompt-based\nlearning has shown great potential in many natural language processing tasks.\nWhereas, the applications for more complex tasks such as event extraction are\nless studied since the design of prompt is not straightforward for the\nstructured event containing various triggers and arguments. % Meanwhile,\ncurrent conditional generation methods employ large encoder-decoder models,\nwhich are costly to train and serve. In this paper, we present a novel\nprompt-based approach, which elicits both the independent and joint knowledge\nabout different events for event argument extraction. The experimental results\non the benchmark ACE2005 dataset show the great advantages of our proposed\napproach. In particular, our approach is superior to the recent advanced\nmethods in both fully-supervised and low-resource scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiaju Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora. (arXiv:2110.08534v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08534","description":"<p>Pretrained language models (PTLMs) are typically learned over a large, static\ncorpus and further fine-tuned for various downstream tasks. However, when\ndeployed in the real world, a PTLM-based model must deal with data\ndistributions that deviate from what the PTLM was initially trained on. In this\npaper, we study a lifelong language model pretraining challenge where a PTLM is\ncontinually updated so as to adapt to emerging data. Over a domain-incremental\nresearch paper stream and a chronologically-ordered tweet stream, we\nincrementally pretrain a PTLM with different continual learning algorithms, and\nkeep track of the downstream task performance (after fine-tuning). We evaluate\nPTLM's ability to adapt to new corpora while retaining learned knowledge in\nearlier corpora. Our experiments show distillation-based approaches to be most\neffective in retaining downstream performance in earlier domains. The\nalgorithms also improve knowledge transfer, allowing models to achieve better\ndownstream performance over the latest data, and improve temporal\ngeneralization when distribution gaps exist between training and evaluation\nbecause of time. We believe our problem formulation, methods, and analysis will\ninspire future studies towards continual pretraining of language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaokai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforced Abstractive Summarization with Adaptive Length Controlling. (arXiv:2112.07534v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07534","description":"<p>Document summarization, as a fundamental task in natural language generation,\naims to generate a short and coherent summary for a given document.\nControllable summarization, especially of the length, is an important issue for\nsome practical applications, especially how to trade-off the length constraint\nand information integrity. In this paper, we propose an \\textbf{A}daptive\n\\textbf{L}ength \\textbf{C}ontrolling \\textbf{O}ptimization (\\textbf{ALCO})\nmethod to leverage two-stage abstractive summarization model via reinforcement\nlearning. ALCO incorporates length constraint into the stage of sentence\nextraction to penalize the overlength extracted sentences. Meanwhile, a\nsaliency estimation mechanism is designed to preserve the salient information\nin the generated sentences. A series of experiments have been conducted on a\nwildly-used benchmark dataset \\textit{CNN/Daily Mail}. The results have shown\nthat ALCO performs better than the popular baselines in terms of length\ncontrollability and content preservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound. (arXiv:2201.02639v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02639","description":"<p>As humans, we navigate a multimodal world, building a holistic understanding\nfrom all our senses. We introduce MERLOT Reserve, a model that represents\nvideos jointly over time -- through a new training objective that learns from\naudio, subtitles, and video frames. Given a video, we replace snippets of text\nand audio with a MASK token; the model learns by choosing the correct\nmasked-out snippet. Our objective learns faster than alternatives, and performs\nwell at scale: we pretrain on 20 million YouTube videos.\n</p>\n<p>Empirical results show that MERLOT Reserve learns strong multimodal\nrepresentations. When finetuned, it sets state-of-the-art on Visual Commonsense\nReasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%,\nand 1.5% respectively. Ablations show that these tasks benefit from audio\npretraining -- even VCR, a QA task centered around images (without sound).\nMoreover, our objective enables out-of-the-box prediction, revealing strong\nmultimodal commonsense understanding. In a fully zero-shot setting, our model\nobtains competitive results on four video tasks, even outperforming supervised\napproaches on the recently proposed Situated Reasoning (STAR) benchmark.\n</p>\n<p>We analyze why audio enables better vision-language representations,\nsuggesting significant opportunities for future research. We conclude by\ndiscussing ethical and societal implications of multimodal pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Intensity and its Control for Emotional Voice Conversion. (arXiv:2201.03967v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2201.03967","description":"<p>Emotional voice conversion (EVC) seeks to convert the emotional state of an\nutterance while preserving the linguistic content and speaker identity. In EVC,\nemotions are usually treated as discrete categories overlooking the fact that\nspeech also conveys emotions with various intensity levels that the listener\ncan perceive. In this paper, we aim to explicitly characterize and control the\nintensity of emotion. We propose to disentangle the speaker style from\nlinguistic content and encode the speaker style into a style embedding in a\ncontinuous space that forms the prototype of emotion embedding. We further\nlearn the actual emotion encoder from an emotion-labelled database and study\nthe use of relative attributes to represent fine-grained emotion intensity. To\nensure emotional intelligibility, we incorporate emotion classification loss\nand emotion embedding similarity loss into the training of the EVC network. As\ndesired, the proposed network controls the fine-grained emotion intensity in\nthe output speech. Through both objective and subjective evaluations, we\nvalidate the effectiveness of the proposed network for emotional expressiveness\nand emotion intensity control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sisman_B/0/1/0/all/0/1\">Berrak Sisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_R/0/1/0/all/0/1\">Rajib Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should we tweet this? Generative response modeling for predicting reception of public health messaging on Twitter. (arXiv:2204.04353v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04353","description":"<p>The way people respond to messaging from public health organizations on\nsocial media can provide insight into public perceptions on critical health\nissues, especially during a global crisis such as COVID-19. It could be\nvaluable for high-impact organizations such as the US Centers for Disease\nControl and Prevention (CDC) or the World Health Organization (WHO) to\nunderstand how these perceptions impact reception of messaging on health policy\nrecommendations. We collect two datasets of public health messages and their\nresponses from Twitter relating to COVID-19 and Vaccines, and introduce a\npredictive method which can be used to explore the potential reception of such\nmessages. Specifically, we harness a generative model (GPT-2) to directly\npredict probable future responses and demonstrate how it can be used to\noptimize expected reception of important health guidance. Finally, we introduce\na novel evaluation scheme with extensive statistical testing which allows us to\nconclude that our models capture the semantics and sentiment found in actual\npublic health responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanders_A/0/1/0/all/0/1\">Abraham Sanders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_Majumder_D/0/1/0/all/0/1\">Debjani Ray-Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erickson_J/0/1/0/all/0/1\">John S. Erickson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_K/0/1/0/all/0/1\">Kristin P. Bennett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Retrieve Videos by Asking Questions. (arXiv:2205.05739v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05739","description":"<p>The majority of traditional text-to-video retrieval systems operate in static\nenvironments, i.e., there is no interaction between the user and the agent\nbeyond the initial textual query provided by the user. This can be suboptimal\nif the initial query has ambiguities, which would lead to many falsely\nretrieved videos. To overcome this limitation, we propose a novel framework for\nVideo Retrieval using Dialog (ViReD), which enables the user to interact with\nan AI agent via multiple rounds of dialog. The key contribution of our\nframework is a novel multimodal question generator that learns to ask questions\nthat maximize the subsequent video retrieval performance. Our multimodal\nquestion generator uses (i) the video candidates retrieved during the last\nround of interaction with the user and (ii) the text-based dialog history\ndocumenting all previous interactions, to generate questions that incorporate\nboth visual and linguistic cues relevant to video retrieval. Furthermore, to\ngenerate maximally informative questions, we propose an Information-Guided\nSupervision (IGS), which guides the question generator to ask questions that\nwould boost subsequent video retrieval accuracy. We validate the effectiveness\nof our interactive ViReD framework on the AVSD dataset, showing that our\ninteractive method performs significantly better than traditional\nnon-interactive video retrieval systems. Furthermore, we also demonstrate that\nour proposed approach also generalizes to the real-world settings that involve\ninteractions with real humans, thus, demonstrating the robustness and\ngenerality of our framework\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1\">Avinash Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1\">Junier Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-thinking Knowledge Graph Completion Evaluation from an Information Retrieval Perspective. (arXiv:2205.04105v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2205.04105","description":"<p>Knowledge graph completion (KGC) aims to infer missing knowledge triples\nbased on known facts in a knowledge graph. Current KGC research mostly follows\nan entity ranking protocol, wherein the effectiveness is measured by the\npredicted rank of a masked entity in a test triple. The overall performance is\nthen given by a micro(-average) metric over all individual answer entities. Due\nto the incomplete nature of the large-scale knowledge bases, such an entity\nranking setting is likely affected by unlabelled top-ranked positive examples,\nraising questions on whether the current evaluation protocol is sufficient to\nguarantee a fair comparison of KGC systems. To this end, this paper presents a\nsystematic study on whether and how the label sparsity affects the current KGC\nevaluation with the popular micro metrics. Specifically, inspired by the TREC\nparadigm for large-scale information retrieval (IR) experimentation, we create\na relatively \"complete\" judgment set based on a sample from the popular\nFB15k-237 dataset following the TREC pooling method. According to our analysis,\nit comes as a surprise that switching from the original labels to our\n\"complete\" labels results in a drastic change of system ranking of a variety of\n13 popular KGC models in terms of micro metrics. Further investigation\nindicates that the IR-like macro(-average) metrics are more stable and\ndiscriminative under different settings, meanwhile, less affected by label\nsparsity. Thus, for KGC evaluation, we recommend conducting TREC-style pooling\nto balance between human efforts and label completeness, and reporting also the\nIR-like macro metrics to reflect the ranking nature of the KGC task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Ying Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuanang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Ben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Overparameterization Improves StyleGAN Inversion. (arXiv:2205.06304v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06304","description":"<p>Deep generative models like StyleGAN hold the promise of semantic image\nediting: modifying images by their content, rather than their pixel values.\nUnfortunately, working with arbitrary images requires inverting the StyleGAN\ngenerator, which has remained challenging so far. Existing inversion approaches\nobtain promising yet imperfect results, having to trade-off between\nreconstruction quality and downstream editability. To improve quality, these\napproaches must resort to various techniques that extend the model latent space\nafter training. Taking a step back, we observe that these methods essentially\nall propose, in one way or another, to increase the number of free parameters.\nThis suggests that inversion might be difficult because it is underconstrained.\nIn this work, we address this directly and dramatically overparameterize the\nlatent space, before training, with simple changes to the original StyleGAN\narchitecture. Our overparameterization increases the available degrees of\nfreedom, which in turn facilitates inversion. We show that this allows us to\nobtain near-perfect image reconstruction without the need for encoders nor for\naltering the latent space after training. Our approach also retains\neditability, which we demonstrate by realistically interpolating between\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poirier_Ginter_Y/0/1/0/all/0/1\">Yohan Poirier-Ginter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lessard_A/0/1/0/all/0/1\">Alexandre Lessard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_R/0/1/0/all/0/1\">Ryan Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Lalonde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Virtual-Try-On from a Single Example Image through Deep Inverse Graphics and Learned Differentiable Renderers. (arXiv:2205.06305v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06305","description":"<p>Augmented reality applications have rapidly spread across online platforms,\nallowing consumers to virtually try-on a variety of products, such as makeup,\nhair dying, or shoes. However, parametrizing a renderer to synthesize realistic\nimages of a given product remains a challenging task that requires expert\nknowledge. While recent work has introduced neural rendering methods for\nvirtual try-on from example images, current approaches are based on large\ngenerative models that cannot be used in real-time on mobile devices. This\ncalls for a hybrid method that combines the advantages of computer graphics and\nneural rendering approaches. In this paper we propose a novel framework based\non deep learning to build a real-time inverse graphics encoder that learns to\nmap a single example image into the parameter space of a given augmented\nreality rendering engine. Our method leverages self-supervised learning and\ndoes not require labeled training data which makes it extendable to many\nvirtual try-on applications. Furthermore, most augmented reality renderers are\nnot differentiable in practice due to algorithmic choices or implementation\nconstraints to reach real-time on portable devices. To relax the need for a\ngraphics-based differentiable renderer in inverse graphics problems, we\nintroduce a trainable imitator module. Our imitator is a generative network\nthat learns to accurately reproduce the behavior of a given non-differentiable\nrenderer. We propose a novel rendering sensitivity loss to train the imitator,\nwhich ensures that the network learns an accurate and continuous representation\nfor each rendering parameter. Our framework enables novel applications where\nconsumers can virtually try-on a novel unknown product from an inspirational\nreference image on social media. It can also be used by graphics artists to\nautomatically create realistic rendering from a reference product image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kips_R/0/1/0/all/0/1\">Robin Kips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Ruowei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ba_S/0/1/0/all/0/1\">Sileye Ba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duke_B/0/1/0/all/0/1\">Brendan Duke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perrot_M/0/1/0/all/0/1\">Matthieu Perrot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloch_I/0/1/0/all/0/1\">Isabelle Bloch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visuomotor Control in Multi-Object Scenes Using Object-Aware Representations. (arXiv:2205.06333v1 [cs.RO])","link":"http://arxiv.org/abs/2205.06333","description":"<p>Perceptual understanding of the scene and the relationship between its\ndifferent components is important for successful completion of robotic tasks.\nRepresentation learning has been shown to be a powerful technique for this, but\nmost of the current methodologies learn task specific representations that do\nnot necessarily transfer well to other tasks. Furthermore, representations\nlearned by supervised methods require large labeled datasets for each task that\nare expensive to collect in the real world. Using self-supervised learning to\nobtain representations from unlabeled data can mitigate this problem. However,\ncurrent self-supervised representation learning methods are mostly object\nagnostic, and we demonstrate that the resulting representations are\ninsufficient for general purpose robotics tasks as they fail to capture the\ncomplexity of scenes with many components. In this paper, we explore the\neffectiveness of using object-aware representation learning techniques for\nrobotic tasks. Our self-supervised representations are learned by observing the\nagent freely interacting with different parts of the environment and is queried\nin two different settings: (i) policy learning and (ii) object location\nprediction. We show that our model learns control policies in a\nsample-efficient manner and outperforms state-of-the-art object agnostic\ntechniques as well as methods trained on raw RGB images. Our results show a 20\npercent increase in performance in low data regimes (1000 trajectories) in\npolicy training using implicit behavioral cloning (IBC). Furthermore, our\nmethod outperforms the baselines for the task of object localization in\nmulti-object scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heravi_N/0/1/0/all/0/1\">Negin Heravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahid_A/0/1/0/all/0/1\">Ayzaan Wahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynch_C/0/1/0/all/0/1\">Corey Lynch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armstrong_T/0/1/0/all/0/1\">Travis Armstrong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1\">Jonathan Tompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1\">Pierre Sermanet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1\">Jeannette Bohg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwibedi_D/0/1/0/all/0/1\">Debidatta Dwibedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LANTERN-RD: Enabling Deep Learning for Mitigation of the Invasive Spotted Lanternfly. (arXiv:2205.06397v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06397","description":"<p>The Spotted Lanternfly (SLF) is an invasive planthopper that threatens the\nlocal biodiversity and agricultural economy of regions such as the Northeastern\nUnited States and Japan. As researchers scramble to study the insect, there is\na great potential for computer vision tasks such as detection, pose estimation,\nand accurate identification to have important downstream implications in\ncontaining the SLF. However, there is currently no publicly available dataset\nfor training such AI models. To enable computer vision applications and\nmotivate advancements to challenge the invasive SLF problem, we propose\nLANTERN-RD, the first curated image dataset of the spotted lanternfly and its\nlook-alikes, featuring images with varied lighting conditions, diverse\nbackgrounds, and subjects in assorted poses. A VGG16-based baseline CNN\nvalidates the potential of this dataset for stimulating fresh computer vision\napplications to accelerate invasive SLF research. Additionally, we implement\nthe trained model in a simple mobile classification application in order to\ndirectly empower responsible public mitigation efforts. The overarching mission\nof this work is to introduce a novel SLF image dataset and release a\nclassification framework that enables computer vision applications, boosting\nstudies surrounding the invasive SLF and assisting in minimizing its\nagricultural and economic damage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kundurthy_S/0/1/0/all/0/1\">Srivatsa Kundurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning. (arXiv:2205.06401v1 [cs.CR])","link":"http://arxiv.org/abs/2205.06401","description":"<p>Contrastive learning pre-trains an image encoder using a large amount of\nunlabeled data such that the image encoder can be used as a general-purpose\nfeature extractor for various downstream tasks. In this work, we propose\nPoisonedEncoder, a data poisoning attack to contrastive learning. In\nparticular, an attacker injects carefully crafted poisoning inputs into the\nunlabeled pre-training data, such that the downstream classifiers built based\non the poisoned encoder for multiple target downstream tasks simultaneously\nclassify attacker-chosen, arbitrary clean inputs as attacker-chosen, arbitrary\nclasses. We formulate our data poisoning attack as a bilevel optimization\nproblem, whose solution is the set of poisoning inputs; and we propose a\ncontrastive-learning-tailored method to approximately solve it. Our evaluation\non multiple datasets shows that PoisonedEncoder achieves high attack success\nrates while maintaining the testing accuracy of the downstream classifiers\nbuilt upon the poisoned encoder for non-attacker-chosen inputs. We also\nevaluate five defenses against PoisonedEncoder, including one pre-processing,\nthree in-processing, and one post-processing defenses. Our results show that\nthese defenses can decrease the attack success rate of PoisonedEncoder, but\nthey also sacrifice the utility of the encoder or require a large clean\npre-training dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinyuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Decompositions for Hyperspectral Data Processing in Remote Sensing: A Comprehensive Review. (arXiv:2205.06407v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06407","description":"<p>Owing to the rapid development of sensor technology, hyperspectral (HS)\nremote sensing (RS) imaging has provided a significant amount of spatial and\nspectral information for the observation and analysis of the Earth's surface at\na distance of data acquisition devices, such as aircraft, spacecraft, and\nsatellite. The recent advancement and even revolution of the HS RS technique\noffer opportunities to realize the full potential of various applications,\nwhile confronting new challenges for efficiently processing and analyzing the\nenormous HS acquisition data. Due to the maintenance of the 3-D HS inherent\nstructure, tensor decomposition has aroused widespread concern and research in\nHS data processing tasks over the past decades. In this article, we aim at\npresenting a comprehensive overview of tensor decomposition, specifically\ncontextualizing the five broad topics in HS data processing, and they are HS\nrestoration, compressed sensing, anomaly detection, super-resolution, and\nspectral unmixing. For each topic, we elaborate on the remarkable achievements\nof tensor decomposition models for HS RS with a pivotal description of the\nexisting methodologies and a representative exhibition on the experimental\nresults. As a result, the remaining challenges of the follow-up research\ndirections are outlined and discussed from the perspective of the real HS RS\npractices and tensor decomposition merged with advanced priors and even with\ndeep neural networks. This article summarizes different tensor\ndecomposition-based HS data processing methods and categorizes them into\ndifferent classes from simple adoptions to complex combinations with other\npriors for the algorithm beginners. We also expect this survey can provide new\ninvestigations and development trends for the experienced researchers who\nunderstand tensor decomposition and HS RS to some extent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minghua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Danfeng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianru Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-based assessment of intraoperative surgical skill. (arXiv:2205.06416v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06416","description":"<p>Purpose: The objective of this investigation is to provide a comprehensive\nanalysis of state-of-the-art methods for video-based assessment of surgical\nskill in the operating room. Methods: Using a data set of 99 videos of\ncapsulorhexis, a critical step in cataract surgery, we evaluate feature based\nmethods previously developed for surgical skill assessment mostly under\nbenchtop settings. In addition, we present and validate two deep learning\nmethods that directly assess skill using RGB videos. In the first method, we\npredict instrument tips as keypoints, and learn surgical skill using temporal\nconvolutional neural networks. In the second method, we propose a novel\narchitecture for surgical skill assessment that includes a frame-wise encoder\n(2D convolutional neural network) followed by a temporal model (recurrent\nneural network), both of which are augmented by visual attention mechanisms. We\nreport the area under the receiver operating characteristic curve, sensitivity,\nspecificity, and predictive values with each method through 5-fold\ncross-validation. Results: For the task of binary skill classification (expert\nvs. novice), deep neural network based methods exhibit higher AUC than the\nclassical spatiotemporal interest point based methods. The neural network\napproach using attention mechanisms also showed high sensitivity and\nspecificity. Conclusion: Deep learning methods are necessary for video-based\nassessment of surgical skill in the operating room. Our findings of internal\nvalidity of a network using attention mechanisms to assess skill directly using\nRGB videos should be evaluated for external validity in other data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hira_S/0/1/0/all/0/1\">Sanchit Hira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1\">Digvijay Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae Soo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shobhit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Gregory Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikder_S/0/1/0/all/0/1\">Shameema Sikder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1\">S. Swaroop Vedula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talking Face Generation with Multilingual TTS. (arXiv:2205.06421v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06421","description":"<p>In this work, we propose a joint system combining a talking face generation\nsystem with a text-to-speech system that can generate multilingual talking face\nvideos from only the text input. Our system can synthesize natural multilingual\nspeeches while maintaining the vocal identity of the speaker, as well as lip\nmovements synchronized to the synthesized speech. We demonstrate the\ngeneralization capabilities of our system by selecting four languages (Korean,\nEnglish, Japanese, and Chinese) each from a different language family. We also\ncompare the outputs of our talking face generation model to outputs of a prior\nwork that claims multilingual support. For our demo, we add a translation API\nto the preprocessing stage and present it in the form of a neural dubber so\nthat users can utilize the multilingual property of our system more easily.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hyoung-Kyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sang Hoon Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junhyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seungmin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunjae Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youseong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dongho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kang-wook Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-time Fourier Style Calibration for Domain Generalization. (arXiv:2205.06427v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06427","description":"<p>The topic of generalizing machine learning models learned on a collection of\nsource domains to unknown target domains is challenging. While many domain\ngeneralization (DG) methods have achieved promising results, they primarily\nrely on the source domains at train-time without manipulating the target\ndomains at test-time. Thus, it is still possible that those methods can overfit\nto source domains and perform poorly on target domains. Driven by the\nobservation that domains are strongly related to styles, we argue that reducing\nthe gap between source and target styles can boost models' generalizability. To\nsolve the dilemma of having no access to the target domain during training, we\nintroduce Test-time Fourier Style Calibration (TF-Cal) for calibrating the\ntarget domain style on the fly during testing. To access styles, we utilize\nFourier transformation to decompose features into amplitude (style) features\nand phase (semantic) features. Furthermore, we present an effective technique\nto Augment Amplitude Features (AAF) to complement TF-Cal. Extensive experiments\non several popular DG benchmarks and a segmentation dataset for medical images\ndemonstrate that our method outperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1\">Anthony Sicilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seong Jae Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FRIH: Fine-grained Region-aware Image Harmonization. (arXiv:2205.06448v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06448","description":"<p>Image harmonization aims to generate a more realistic appearance of\nforeground and background for a composite image. Existing methods perform the\nsame harmonization process for the whole foreground. However, the implanted\nforeground always contains different appearance patterns. All the existing\nsolutions ignore the difference of each color block and losing some specific\ndetails. Therefore, we propose a novel global-local two stages framework for\nFine-grained Region-aware Image Harmonization (FRIH), which is trained\nend-to-end. In the first stage, the whole input foreground mask is used to make\na global coarse-grained harmonization. In the second stage, we adaptively\ncluster the input foreground mask into several submasks by the corresponding\npixel RGB values in the composite image. Each submask and the coarsely adjusted\nimage are concatenated respectively and fed into a lightweight cascaded module,\nadjusting the global harmonization performance according to the region-aware\nlocal feature. Moreover, we further designed a fusion prediction module by\nfusing features from all the cascaded decoder layers together to generate the\nfinal result, which could utilize the different degrees of harmonization\nresults comprehensively. Without bells and whistles, our FRIH algorithm\nachieves the best performance on iHarmony4 dataset (PSNR is 38.19 dB) with a\nlightweight model. The parameters for our model are only 11.98 M, far below the\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jinlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zekun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boshen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A microstructure estimation Transformer inspired by sparse representation for diffusion MRI. (arXiv:2205.06450v1 [eess.SP])","link":"http://arxiv.org/abs/2205.06450","description":"<p>Diffusion magnetic resonance imaging (dMRI) is an important tool in\ncharacterizing tissue microstructure based on biophysical models, which are\ncomplex and highly non-linear. Resolving microstructures with optimization\ntechniques is prone to estimation errors and requires dense sampling in the\nq-space. Deep learning based approaches have been proposed to overcome these\nlimitations. Motivated by the superior performance of the Transformer, in this\nwork, we present a learning-based framework based on Transformer, namely, a\nMicrostructure Estimation Transformer with Sparse Coding (METSC) for dMRI-based\nmicrostructure estimation with downsampled q-space data. To take advantage of\nthe Transformer while addressing its limitation in large training data\nrequirements, we explicitly introduce an inductive bias - model bias into the\nTransformer using a sparse coding technique to facilitate the training process.\nThus, the METSC is composed with three stages, an embedding stage, a sparse\nrepresentation stage, and a mapping stage. The embedding stage is a\nTransformer-based structure that encodes the signal to ensure the voxel is\nrepresented effectively. In the sparse representation stage, a dictionary is\nconstructed by solving a sparse reconstruction problem that unfolds the\nIterative Hard Thresholding (IHT) process. The mapping stage is essentially a\ndecoder that computes the microstructural parameters from the output of the\nsecond stage, based on the weighted sum of normalized dictionary coefficients\nwhere the weights are also learned. We tested our framework on two dMRI models\nwith downsampled q-space data, including the intravoxel incoherent motion\n(IVIM) model and the neurite orientation dispersion and density imaging (NODDI)\nmodel. The proposed method achieved up to 11.25 folds of acceleration in scan\ntime and outperformed the other state-of-the-art learning-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_T/0/1/0/all/0/1\">Tianshu Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_C/0/1/0/all/0/1\">Cong Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_W/0/1/0/all/0/1\">Weihao Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_W/0/1/0/all/0/1\">Wen Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haotian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guangbin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_C/0/1/0/all/0/1\">Chuyang Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_D/0/1/0/all/0/1\">Dan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Human Digitization via Implicit Re-projection Networks. (arXiv:2205.06468v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06468","description":"<p>We present an approach to generating 3D human models from images. The key to\nour framework is that we predict double-sided orthographic depth maps and color\nimages from a single perspective projected image. Our framework consists of\nthree networks. The first network predicts normal maps to recover geometric\ndetails such as wrinkles in the clothes and facial regions. The second network\npredicts shade-removed images for the front and back views by utilizing the\npredicted normal maps. The last multi-headed network takes both normal maps and\nshade-free images and predicts depth maps while selectively fusing photometric\nand geometric information through multi-headed attention gates. Experimental\nresults demonstrate that our method shows visually plausible results and\ncompetitive performance in terms of various evaluation metrics over\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Min-Gyu Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Ju-Mi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Je Woo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Ju Hong Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Left Atrial Appendage Segmentation and Analysis in 3D and 4D Medical Images. (arXiv:2205.06486v1 [eess.IV])","link":"http://arxiv.org/abs/2205.06486","description":"<p>Atrial fibrillation (AF) is a cardiovascular disease identified as one of the\nmain risk factors for stroke. The majority of strokes due to AF are caused by\nclots originating in the left atrial appendage (LAA). LAA occlusion is an\neffective procedure for reducing stroke risk. Planning the procedure using\npre-procedural imaging and analysis has shown benefits. The analysis is\ncommonly done by manually segmenting the appendage on 2D slices. Automatic LAA\nsegmentation methods could save an expert's time and provide insightful 3D\nvisualizations and accurate automatic measurements to aid in medical\nprocedures. Several semi- and fully-automatic methods for segmenting the\nappendage have been proposed. This paper provides a review of automatic LAA\nsegmentation methods on 3D and 4D medical images, including CT, MRI, and\nechocardiogram images. We classify methods into heuristic and model-based\nmethods, as well as into semi- and fully-automatic methods. We summarize and\ncompare the proposed methods, evaluate their effectiveness, and present current\nchallenges in the field and approaches to overcome them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Leventic_H/0/1/0/all/0/1\">Hrvoje Leventi&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bencevic_M/0/1/0/all/0/1\">Marin Ben&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Babin_D/0/1/0/all/0/1\">Danilo Babin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Habijan_M/0/1/0/all/0/1\">Marija Habijan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galic_I/0/1/0/all/0/1\">Irena Gali&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RTMaps-based Local Dynamic Map for multi-ADAS data fusion. (arXiv:2205.06497v1 [cs.DB])","link":"http://arxiv.org/abs/2205.06497","description":"<p>Work on Local Dynamic Maps (LDM) implementation is still in its early stages,\nas the LDM standards only define how information shall be structured in\ndatabases, while the mechanism to fuse or link information across different\nlayers is left undefined. A working LDM component, as a real-time database\ninside the vehicle is an attractive solution to multi-ADAS systems, which may\nfeed a real-time LDM database that serves as a central point of information\ninside the vehicle, exposing fused and structured information to other\ncomponents (e.g., decision-making systems). In this paper we describe our\napproach implementing a real-time LDM component using the RTMaps middleware, as\na database deployed in a vehicle, but also at road-side units (RSU), making use\nof the three pillars that guide a successful fusion strategy: utilisation of\nstandards (with conversions between domains), middlewares to unify multiple\nADAS sources, and linkage of data via semantic concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nieto_M/0/1/0/all/0/1\">Marcos Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_M/0/1/0/all/0/1\">Mikel Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urbieta_I/0/1/0/all/0/1\">Itziar Urbieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otaegui_O/0/1/0/all/0/1\">Oihana Otaegui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FontNet: Closing the gap to font designer performance in font synthesis. (arXiv:2205.06512v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06512","description":"<p>Font synthesis has been a very active topic in recent years because manual\nfont design requires domain expertise and is a labor-intensive and\ntime-consuming job. While remarkably successful, existing methods for font\nsynthesis have major shortcomings; they require finetuning for unobserved font\nstyle with large reference images, the recent few-shot font synthesis methods\nare either designed for specific language systems or they operate on\nlow-resolution images which limits their use. In this paper, we tackle this\nfont synthesis problem by learning the font style in the embedding space. To\nthis end, we propose a model, called FontNet, that simultaneously learns to\nseparate font styles in the embedding space where distances directly correspond\nto a measure of font similarity, and translates input images into the given\nobserved or unobserved font style. Additionally, we design the network\narchitecture and training procedure that can be adopted for any language system\nand can produce high-resolution font images. Thanks to this approach, our\nproposed method outperforms the existing state-of-the-art font generation\nmethods on both qualitative and quantitative experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_A/0/1/0/all/0/1\">Ammar Ul Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaeyoung Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Semantic Composition with Syntactic Hypergraph for Video Question Answering. (arXiv:2205.06530v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06530","description":"<p>A key challenge in video question answering is how to realize the cross-modal\nsemantic alignment between textual concepts and corresponding visual objects.\nExisting methods mostly seek to align the word representations with the video\nregions. However, word representations are often not able to convey a complete\ndescription of textual concepts, which are in general described by the\ncompositions of certain words. To address this issue, we propose to first build\na syntactic dependency tree for each question with an off-the-shelf tool and\nuse it to guide the extraction of meaningful word compositions. Based on the\nextracted compositions, a hypergraph is further built by viewing the words as\nnodes and the compositions as hyperedges. Hypergraph convolutional networks\n(HCN) are then employed to learn the initial representations of word\ncompositions. Afterwards, an optimal transport based method is proposed to\nperform cross-modal semantic alignment for the textual and visual semantic\nspace. To reflect the cross-modal influences, the cross-modal information is\nincorporated into the initial representations, leading to a model named\ncross-modality-aware syntactic HCN. Experimental results on three benchmarks\nshow that our method outperforms all strong baselines. Further analyses\ndemonstrate the effectiveness of each component, and show that our model is\ngood at modeling different levels of semantic compositions and filtering out\nirrelevant information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qinliang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zijing Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuwei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of attention models and post-hoc explanation methods for embryo stage identification: a case study. (arXiv:2205.06546v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06546","description":"<p>An important limitation to the development of AI-based solutions for In Vitro\nFertilization (IVF) is the black-box nature of most state-of-the-art models,\ndue to the complexity of deep learning architectures, which raises potential\nbias and fairness issues. The need for interpretable AI has risen not only in\nthe IVF field but also in the deep learning community in general. This has\nstarted a trend in literature where authors focus on designing objective\nmetrics to evaluate generic explanation methods. In this paper, we study the\nbehavior of recently proposed objective faithfulness metrics applied to the\nproblem of embryo stage identification. We benchmark attention models and\npost-hoc methods using metrics and further show empirically that (1) the\nmetrics produce low overall agreement on the model ranking and (2) depending on\nthe metric approach, either post-hoc methods or attention models are favored.\nWe conclude with general remarks about the difficulty of defining faithfulness\nand the necessity of understanding its relationship with the type of approach\nthat is favored.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1\">Tristan Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freour_T/0/1/0/all/0/1\">Thomas Fr&#xe9;our</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchere_H/0/1/0/all/0/1\">Harold Mouch&#xe8;re</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Balanced Network for Fair Face Recognition. (arXiv:2205.06548v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06548","description":"<p>Although deep face recognition has achieved impressive progress in recent\nyears, controversy has arisen regarding discrimination based on skin tone,\nquestioning their deployment into real-world scenarios. In this paper, we aim\nto systematically and scientifically study this bias from both data and\nalgorithm aspects. First, using the dermatologist approved Fitzpatrick Skin\nType classification system and Individual Typology Angle, we contribute a\nbenchmark called Identity Shades (IDS) database, which effectively quantifies\nthe degree of the bias with respect to skin tone in existing face recognition\nalgorithms and commercial APIs. Further, we provide two skin-tone aware\ntraining datasets, called BUPT-Globalface dataset and BUPT-Balancedface\ndataset, to remove bias in training data. Finally, to mitigate the algorithmic\nbias, we propose a novel meta-learning algorithm, called Meta Balanced Network\n(MBN), which learns adaptive margins in large margin loss such that the model\noptimized by this loss can perform fairly across people with different skin\ntones. To determine the margins, our method optimizes a meta skewness loss on a\nclean and unbiased meta set and utilizes backward-on-backward automatic\ndifferentiation to perform a second order gradient descent step on the current\nmargins. Extensive experiments show that MBN successfully mitigates bias and\nlearns more balanced performance for people with different skin tones in face\nrecognition. The proposed datasets are available at\n<a href=\"http://www.whdeng.cn/RFW/index.html.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaobin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Structure-Texture Separation Network for Oracle Character Recognition. (arXiv:2205.06549v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06549","description":"<p>Oracle bone script is the earliest-known Chinese writing system of the Shang\ndynasty and is precious to archeology and philology. However, real-world\nscanned oracle data are rare and few experts are available for annotation which\nmake the automatic recognition of scanned oracle characters become a\nchallenging task. Therefore, we aim to explore unsupervised domain adaptation\nto transfer knowledge from handprinted oracle data, which are easy to acquire,\nto scanned domain. We propose a structure-texture separation network (STSN),\nwhich is an end-to-end learning framework for joint disentanglement,\ntransformation, adaptation and recognition. First, STSN disentangles features\ninto structure (glyph) and texture (noise) components by generative models, and\nthen aligns handprinted and scanned data in structure feature space such that\nthe negative influence caused by serious noises can be avoided when adapting.\nSecond, transformation is achieved via swapping the learned textures across\ndomains and a classifier for final classification is trained to predict the\nlabels of the transformed scanned characters. This not only guarantees the\nabsolute separation, but also enhances the discriminative ability of the\nlearned features. Extensive experiments on Oracle-241 dataset show that STSN\noutperforms other adaptation methods and successfully improves recognition\nperformance on scanned data even when they are contaminated by long burial and\ncareless excavation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng-Lin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Domain Disentanglement for Generalizable Medical Image Segmentation. (arXiv:2205.06551v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06551","description":"<p>Efficiently utilizing discriminative features is crucial for convolutional\nneural networks to achieve remarkable performance in medical image segmentation\nand is also important for model generalization across multiple domains, where\nletting model recognize domain-specific and domain-invariant information among\nmulti-site datasets is a reasonable strategy for domain generalization.\nUnfortunately, most of the recent disentangle networks are not directly\nadaptable to unseen-domain datasets because of the limitations of offered data\ndistribution. To tackle this deficiency, we propose Contrastive Domain\nDisentangle (CDD) network for generalizable medical image segmentation. We\nfirst introduce a disentangle network to decompose medical images into an\nanatomical representation factor and a modality representation factor. Then, a\nstyle contrastive loss is proposed to encourage the modality representations\nfrom the same domain to distribute as close as possible while different domains\nare estranged from each other. Finally, we propose a domain augmentation\nstrategy that can randomly generate new domains for model generalization\ntraining. Experimental results on multi-site fundus image datasets for optic\ncup and disc segmentation show that the CDD has good model generalization. Our\nproposed CDD outperforms several state-of-the-art methods in domain\ngeneralizable segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Ran Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangshan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenhui Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual passengers for real car solutions: synthetic datasets. (arXiv:2205.06556v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06556","description":"<p>Strategies that include the generation of synthetic data are beginning to be\nviable as obtaining real data can be logistically complicated, very expensive\nor slow. Not only the capture of the data can lead to complications, but also\nits annotation. To achieve high-fidelity data for training intelligent systems,\nwe have built a 3D scenario and set-up to resemble reality as closely as\npossible. With our approach, it is possible to configure and vary parameters to\nadd randomness to the scene and, in this way, allow variation in data, which is\nso important in the construction of a dataset. Besides, the annotation task is\nalready included in the data generation exercise, rather than being a\npost-capture task, which can save a lot of resources. We present the process\nand concept of synthetic data generation in an automotive context, specifically\nfor driver and passenger monitoring purposes, as an alternative to real data\ncapturing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Canas_P/0/1/0/all/0/1\">Paola Natalia Canas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1\">Juan Diego Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_M/0/1/0/all/0/1\">Marcos Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otaegui_O/0/1/0/all/0/1\">Oihana Otaegui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Masking for Unsupervised Anomaly Detection and Localization. (arXiv:2205.06568v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06568","description":"<p>Recently, anomaly detection and localization in multimedia data have received\nsignificant attention among the machine learning community. In real-world\napplications such as medical diagnosis and industrial defect detection,\nanomalies only present in a fraction of the images. To extend the\nreconstruction-based anomaly detection architecture to the localized anomalies,\nwe propose a self-supervised learning approach through random masking and then\nrestoring, named Self-Supervised Masking (SSM) for unsupervised anomaly\ndetection and localization. SSM not only enhances the training of the\ninpainting network but also leads to great improvement in the efficiency of\nmask prediction at inference. Through random masking, each image is augmented\ninto a diverse set of training triplets, thus enabling the autoencoder to learn\nto reconstruct with masks of various sizes and shapes during training. To\nimprove the efficiency and effectiveness of anomaly detection and localization\nat inference, we propose a novel progressive mask refinement approach that\nprogressively uncovers the normal regions and finally locates the anomalous\nregions. The proposed SSM method outperforms several state-of-the-arts for both\nanomaly detection and anomaly localization, achieving 98.3% AUC on Retinal-OCT\nand 93.9% AUC on MVTec AD, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chaoqin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qinwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind Image Inpainting with Sparse Directional Filter Dictionaries for Lightweight CNNs. (arXiv:2205.06597v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06597","description":"<p>Blind inpainting algorithms based on deep learning architectures have shown a\nremarkable performance in recent years, typically outperforming model-based\nmethods both in terms of image quality and run time. However, neural network\nstrategies typically lack a theoretical explanation, which contrasts with the\nwell-understood theory underlying model-based methods. In this work, we\nleverage the advantages of both approaches by integrating theoretically founded\nconcepts from transform domain methods and sparse approximations into a\nCNN-based approach for blind image inpainting. To this end, we present a novel\nstrategy to learn convolutional kernels that applies a specifically designed\nfilter dictionary whose elements are linearly combined with trainable weights.\nNumerical experiments demonstrate the competitiveness of this approach. Our\nresults show not only an improved inpainting quality compared to conventional\nCNNs but also significantly faster network convergence within a lightweight\nnetwork design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmalfuss_J/0/1/0/all/0/1\">Jenny Schmalfuss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheurer_E/0/1/0/all/0/1\">Erik Scheurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Heng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karantzas_N/0/1/0/all/0/1\">Nikolaos Karantzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruhn_A/0/1/0/all/0/1\">Andr&#xe9;s Bruhn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labate_D/0/1/0/all/0/1\">Demetrio Labate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyLandGAN: A StyleGAN based Landscape Image Synthesis using Depth-map. (arXiv:2205.06611v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06611","description":"<p>Despite recent success in conditional image synthesis, prevalent input\nconditions such as semantics and edges are not clear enough to express `Linear\n(Ridges)' and `Planar (Scale)' representations. To address this problem, we\npropose a novel framework StyLandGAN, which synthesizes desired landscape\nimages using a depth map which has higher expressive power. Our StyleLandGAN is\nextended from the unconditional generation model to accept input conditions. We\nalso propose a '2-phase inference' pipeline which generates diverse depth maps\nand shifts local parts so that it can easily reflect user's intend. As a\ncomparison, we modified the existing semantic image synthesis models to accept\na depth map as well. Experimental results show that our method is superior to\nexisting methods in quality, diversity, and depth-accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gunhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yim_J/0/1/0/all/0/1\">Jonghwa Yim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chanran Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minjae Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Attention Graph-based Transformer for Multi-target Genetic Alteration Prediction. (arXiv:2205.06672v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06672","description":"<p>Classical multiple instance learning (MIL) methods are often based on the\nidentical and independent distributed assumption between instances, hence\nneglecting the potentially rich contextual information beyond individual\nentities. On the other hand, Transformers with global self-attention modules\nhave been proposed to model the interdependencies among all instances. However,\nin this paper we question: Is global relation modeling using self-attention\nnecessary, or can we appropriately restrict self-attention calculations to\nlocal regimes in large-scale whole slide images (WSIs)? We propose a\ngeneral-purpose local attention graph-based Transformer for MIL (LA-MIL),\nintroducing an inductive bias by explicitly contextualizing instances in\nadaptive local regimes of arbitrary size. Additionally, an efficiently adapted\nloss function enables our approach to learn expressive WSI embeddings for the\njoint analysis of multiple biomarkers. We demonstrate that LA-MIL achieves\nstate-of-the-art results in mutation prediction for gastrointestinal cancer,\noutperforming existing models on important biomarkers such as microsatellite\ninstability for colorectal cancer. This suggests that local self-attention\nsufficiently models dependencies on par with global modules. Our implementation\nwill be published.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reisenbuchler_D/0/1/0/all/0/1\">Daniel Reisenb&#xfc;chler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1\">Sophia J. Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boxberg_M/0/1/0/all/0/1\">Melanie Boxberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1\">Tingying Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VesNet-RL: Simulation-based Reinforcement Learning for Real-World US Probe Navigation. (arXiv:2205.06676v1 [eess.IV])","link":"http://arxiv.org/abs/2205.06676","description":"<p>Ultrasound (US) is one of the most common medical imaging modalities since it\nis radiation-free, low-cost, and real-time. In freehand US examinations,\nsonographers often navigate a US probe to visualize standard examination planes\nwith rich diagnostic information. However, reproducibility and stability of the\nresulting images often suffer from intra- and inter-operator variation.\nReinforcement learning (RL), as an interaction-based learning method, has\ndemonstrated its effectiveness in visual navigating tasks; however, RL is\nlimited in terms of generalization. To address this challenge, we propose a\nsimulation-based RL framework for real-world navigation of US probes towards\nthe standard longitudinal views of vessels. A UNet is used to provide binary\nmasks from US images; thereby, the RL agent trained on simulated binary vessel\nimages can be applied in real scenarios without further training. To accurately\ncharacterize actual states, a multi-modality state representation structure is\nintroduced to facilitate the understanding of environments. Moreover,\nconsidering the characteristics of vessels, a novel standard view recognition\napproach based on the minimum bounding rectangle is proposed to terminate the\nsearching process. To evaluate the effectiveness of the proposed method, the\ntrained policy is validated virtually on 3D volumes of a volunteer's in-vivo\ncarotid artery, and physically on custom-designed gel phantoms using robotic\nUS. The results demonstrate that proposed approach can effectively and\naccurately navigate the probe towards the longitudinal view of vessels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bi_Y/0/1/0/all/0/1\">Yuan Bi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhongliang Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wendler_T/0/1/0/all/0/1\">Thomas Wendler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karlas_A/0/1/0/all/0/1\">Angelos Karlas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Eye: An Open Platform to Study Human Performance on Identifying AI-Synthesized Faces. (arXiv:2205.06680v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06680","description":"<p>AI-synthesized faces are visually challenging to discern from real ones. They\nhave been used as profile images for fake social media accounts, which leads to\nhigh negative social impacts. Although progress has been made in developing\nautomatic methods to detect AI-synthesized faces, there is no open platform to\nstudy the human performance of AI-synthesized faces detection. In this work, we\ndevelop an online platform called Open-eye to study the human performance of\nAI-synthesized face detection. We describe the design and workflow of the\nOpen-eye in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effectiveness of Temporal Dependency in Deepfake Video Detection. (arXiv:2205.06684v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06684","description":"<p>Deepfakes are a form of synthetic image generation used to generate fake\nvideos of individuals for malicious purposes. The resulting videos may be used\nto spread misinformation, reduce trust in media, or as a form of blackmail.\nThese threats necessitate automated methods of deepfake video detection. This\npaper investigates whether temporal information can improve the deepfake\ndetection performance of deep learning models.\n</p>\n<p>To investigate this, we propose a framework that classifies new and existing\napproaches by their defining characteristics. These are the types of feature\nextraction: automatic or manual, and the temporal relationship between frames:\ndependent or independent. We apply this framework to investigate the effect of\ntemporal dependency on a model's deepfake detection performance.\n</p>\n<p>We find that temporal dependency produces a statistically significant (p &lt;\n0.05) increase in performance in classifying real images for the model using\nautomatic feature selection, demonstrating that spatio-temporal information can\nincrease the performance of deepfake video detection models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rowan_W/0/1/0/all/0/1\">Will Rowan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pears_N/0/1/0/all/0/1\">Nick Pears</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Framework for Implicit Sinkhorn Differentiation. (arXiv:2205.06688v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06688","description":"<p>The Sinkhorn operator has recently experienced a surge of popularity in\ncomputer vision and related fields. One major reason is its ease of integration\ninto deep learning frameworks. To allow for an efficient training of respective\nneural networks, we propose an algorithm that obtains analytical gradients of a\nSinkhorn layer via implicit differentiation. In comparison to prior work, our\nframework is based on the most general formulation of the Sinkhorn operator. It\nallows for any type of loss function, while both the target capacities and cost\nmatrices are differentiated jointly. We further construct error bounds of the\nresulting algorithm for approximate inputs. Finally, we demonstrate that for a\nnumber of applications, simply replacing automatic differentiation with our\nalgorithm directly improves the stability and accuracy of the obtained\ngradients. Moreover, we show that it is computationally more efficient,\nparticularly when resources like GPU memory are scarce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eisenberger_M/0/1/0/all/0/1\">Marvin Eisenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toker_A/0/1/0/all/0/1\">Aysim Toker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation Meets Open-Set Semi-Supervised Learning. (arXiv:2205.06701v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06701","description":"<p>Existing knowledge distillation methods mostly focus on distillation of\nteacher's prediction and intermediate activation. However, the structured\nrepresentation, which arguably is one of the most critical ingredients of deep\nmodels, is largely overlooked. In this work, we propose a novel {\\em\n\\modelname{}} ({\\bf\\em \\shortname{})} method dedicated for distilling\nrepresentational knowledge semantically from a pretrained teacher to a target\nstudent. The key idea is that we leverage the teacher's classifier as a\nsemantic critic for evaluating the representations of both teacher and student\nand distilling the semantic knowledge with high-order structured information\nover all feature dimensions. This is accomplished by introducing a notion of\ncross-network logit computed through passing student's representation into\nteacher's classifier. Further, considering the set of seen classes as a basis\nfor the semantic space in a combinatorial perspective, we scale \\shortname{} to\nunseen classes for enabling effective exploitation of largely available,\narbitrary unlabeled training data. At the problem level, this establishes an\ninteresting connection between knowledge distillation with open-set\nsemi-supervised learning (SSL). Extensive experiments show that our\n\\shortname{} outperforms significantly previous state-of-the-art knowledge\ndistillation methods on both coarse object classification and fine face\nrecognition tasks, as well as less studied yet practically crucial binary\nnetwork distillation. Under more realistic open-set SSL settings we introduce,\nwe reveal that knowledge distillation is generally more effective than existing\nOut-Of-Distribution (OOD) sample detection, and our proposed \\shortname{} is\nsuperior over both previous distillation and SSL competitors. The source code\nis available at \\url{https://github.com/jingyang2017/SRD\\_ossl}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulat_A/0/1/0/all/0/1\">Adrian Bulat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1\">Brais Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1\">Georgios Tzimiropoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-encoder Network for Parameter Reduction of a Kernel-based Interpolation Architecture. (arXiv:2205.06723v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06723","description":"<p>Video frame interpolation involves the synthesis of new frames from existing\nones. Convolutional neural networks (CNNs) have been at the forefront of the\nrecent advances in this field. One popular CNN-based approach involves the\napplication of generated kernels to the input frames to obtain an interpolated\nframe. Despite all the benefits interpolation methods offer, many of these\nnetworks require a lot of parameters, with more parameters meaning a heavier\ncomputational burden. Reducing the size of the model typically impacts\nperformance negatively. This paper presents a method for parameter reduction\nfor a popular flow-less kernel-based network (Adaptive Collaboration of Flows).\nThrough our technique of removing the layers that require the most parameters\nand replacing them with smaller encoders, we reduce the number of parameters of\nthe network and even achieve better performance compared to the original\nmethod. This is achieved by deploying rotation to force each individual encoder\nto learn different features from the input images. Ablations are conducted to\njustify design choices and an evaluation on how our method performs on\nfull-length videos is presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalifeh_I/0/1/0/all/0/1\">Issa Khalifeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanch_M/0/1/0/all/0/1\">Marc Gorriz Blanch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izquierdo_E/0/1/0/all/0/1\">Ebroul Izquierdo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An empirical study of CTC based models for OCR of Indian languages. (arXiv:2205.06740v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06740","description":"<p>Recognition of text on word or line images, without the need for sub-word\nsegmentation has become the mainstream of research and development of text\nrecognition for Indian languages. Modelling unsegmented sequences using\nConnectionist Temporal Classification (CTC) is the most commonly used approach\nfor segmentation-free OCR. In this work we present a comprehensive empirical\nstudy of various neural network models that uses CTC for transcribing step-wise\npredictions in the neural network output to a Unicode sequence. The study is\nconducted for 13 Indian languages, using an internal dataset that has around\n1000 pages per language. We study the choice of line vs word as the recognition\nunit, and use of synthetic data to train the models. We compare our models with\npopular publicly available OCR tools for end-to-end document image recognition.\nOur end-to-end pipeline that employ our recognition models and existing text\nsegmentation tools outperform these public OCR tools for 8 out of the 13\nlanguages. We also introduce a new public dataset called Mozhi for word and\nline recognition in Indian language. The dataset contains more than 1.2 million\nannotated word images (120 thousand text lines) across 13 Indian languages. Our\ncode, trained models and the Mozhi dataset will be made available at\n<a href=\"http://cvit.iiit.ac.in/research/projects/cvit-projects/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1\">Minesh Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">CV Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slimmable Video Codec. (arXiv:2205.06754v1 [eess.IV])","link":"http://arxiv.org/abs/2205.06754","description":"<p>Neural video compression has emerged as a novel paradigm combining trainable\nmultilayer neural networks and machine learning, achieving competitive\nrate-distortion (RD) performances, but still remaining impractical due to heavy\nneural architectures, with large memory and computational demands. In addition,\nmodels are usually optimized for a single RD tradeoff. Recent slimmable image\ncodecs can dynamically adjust their model capacity to gracefully reduce the\nmemory and computation requirements, without harming RD performance. In this\npaper we propose a slimmable video codec (SlimVC), by integrating a slimmable\ntemporal entropy model in a slimmable autoencoder. Despite a significantly more\ncomplex architecture, we show that slimming remains a powerful mechanism to\ncontrol rate, memory footprint, computational cost and latency, all being\nimportant requirements for practical video compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaocheng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Saiping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_S/0/1/0/all/0/1\">Shuai Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blanch_M/0/1/0/all/0/1\">Marc G&#xf3;rriz Blanch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scribble2D5: Weakly-Supervised Volumetric Image Segmentation via Scribble Annotations. (arXiv:2205.06779v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06779","description":"<p>Recently, weakly-supervised image segmentation using weak annotations like\nscribbles has gained great attention, since such annotations are much easier to\nobtain compared to time-consuming and label-intensive labeling at the\npixel/voxel level. However, because scribbles lack structure information of\nregion of interest (ROI), existing scribble-based methods suffer from poor\nboundary localization. Furthermore, most current methods are designed for 2D\nimage segmentation, which do not fully leverage the volumetric information if\ndirectly applied to image slices. In this paper, we propose a scribble-based\nvolumetric image segmentation, Scribble2D5, which tackles 3D anisotropic image\nsegmentation and improves boundary prediction. To achieve this, we augment a\n2.5D attention UNet with a proposed label propagation module to extend semantic\ninformation from scribbles and a combination of static and active boundary\nprediction to learn ROI's boundary and regularize its shape. Extensive\nexperiments on three public datasets demonstrate Scribble2D5 significantly\noutperforms current scribble-based methods and approaches the performance of\nfully-supervised ones. Our code is available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiuhui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yi Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KG-SP: Knowledge Guided Simple Primitives for Open World Compositional Zero-Shot Learning. (arXiv:2205.06784v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06784","description":"<p>The goal of open-world compositional zero-shot learning (OW-CZSL) is to\nrecognize compositions of state and objects in images, given only a subset of\nthem during training and no prior on the unseen compositions. In this setting,\nmodels operate on a huge output space, containing all possible state-object\ncompositions. While previous works tackle the problem by learning embeddings\nfor the compositions jointly, here we revisit a simple CZSL baseline and\npredict the primitives, i.e. states and objects, independently. To ensure that\nthe model develops primitive-specific features, we equip the state and object\nclassifiers with separate, non-linear feature extractors. Moreover, we estimate\nthe feasibility of each composition through external knowledge, using this\nprior to remove unfeasible compositions from the output space. Finally, we\npropose a new setting, i.e. CZSL under partial supervision (pCZSL), where\neither only objects or state labels are available during training, and we can\nuse our prior to estimate the missing labels. Our model, Knowledge-Guided\nSimple Primitives (KG-SP), achieves state of the art in both OW-CZSL and pCZSL,\nsurpassing most recent competitors even when coupled with semi-supervised\nlearning techniques. Code available at: https://github.com/ExplainableML/KG-SP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karthik_S/0/1/0/all/0/1\">Shyamgopal Karthik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Massimiliano Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VQFR: Blind Face Restoration with Vector-Quantized Dictionary and Parallel Decoder. (arXiv:2205.06803v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06803","description":"<p>Although generative facial prior and geometric prior have recently\ndemonstrated high-quality results for blind face restoration, producing\nfine-grained facial details faithful to inputs remains a challenging problem.\nMotivated by the classical dictionary-based methods and the recent vector\nquantization (VQ) technique, we propose a VQ-based face restoration method --\nVQFR. VQFR takes advantage of high-quality low-level feature banks extracted\nfrom high-quality faces and can thus help recover realistic facial details.\nHowever, the simple application of the VQ codebook cannot achieve good results\nwith faithful details and identity preservation. Therefore, we further\nintroduce two special network designs. 1). We first investigate the compression\npatch size in the VQ codebook and find that the VQ codebook designed with a\nproper compression patch size is crucial to balance the quality and fidelity.\n2). To further fuse low-level features from inputs while not \"contaminating\"\nthe realistic details generated from the VQ codebook, we proposed a parallel\ndecoder consisting of a texture decoder and a main decoder. Those two decoders\nthen interact with a texture warping module with deformable convolution.\nEquipped with the VQ codebook as a facial detail dictionary and the parallel\ndecoder design, the proposed VQFR can largely enhance the restored quality of\nfacial details while keeping the fidelity to previous methods. Codes will be\navailable at https://github.com/TencentARC/VQFR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuchao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Liangbin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"secml: A Python Library for Secure and Explainable Machine Learning. (arXiv:1912.10013v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1912.10013","description":"<p>We present \\texttt{secml}, an open-source Python library for secure and\nexplainable machine learning. It implements the most popular attacks against\nmachine learning, including test-time evasion attacks to generate adversarial\nexamples against deep neural networks and training-time poisoning attacks\nagainst support vector machines and many other algorithms. These attacks enable\nevaluating the security of learning algorithms and the corresponding defenses\nunder both white-box and black-box threat models. To this end, \\texttt{secml}\nprovides built-in functions to compute security evaluation curves, showing how\nquickly classification performance decreases against increasing adversarial\nperturbations of the input data. \\texttt{secml} also includes explainability\nmethods to help understand why adversarial attacks succeed against a given\nmodel, by visualizing the most influential features and training prototypes\ncontributing to each decision. It is distributed under the Apache License 2.0\nand hosted at \\url{https://github.com/pralab/secml}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1\">Maura Pintor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demetrio_L/0/1/0/all/0/1\">Luca Demetrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotgiu_A/0/1/0/all/0/1\">Angelo Sotgiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melis_M/0/1/0/all/0/1\">Marco Melis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1\">Ambra Demontis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning for Domain Adaptation on Point-Clouds. (arXiv:2003.12641v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.12641","description":"<p>Self-supervised learning (SSL) is a technique for learning useful\nrepresentations from unlabeled data. It has been applied effectively to domain\nadaptation (DA) on images and videos. It is still unknown if and how it can be\nleveraged for domain adaptation in 3D perception problems. Here we describe the\nfirst study of SSL for DA on point clouds. We introduce a new family of pretext\ntasks, Deformation Reconstruction, inspired by the deformations encountered in\nsim-to-real transformations. In addition, we propose a novel training procedure\nfor labeled point cloud data motivated by the MixUp method called Point cloud\nMixup (PCM). Evaluations on domain adaptations datasets for classification and\nsegmentation, demonstrate a large improvement over existing and baseline\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1\">Idan Achituve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1\">Haggai Maron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distance Guided Channel Weighting for Semantic Segmentation. (arXiv:2004.12679v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.12679","description":"<p>Recent works have achieved great success in improving the performance of\nmultiple computer vision tasks by capturing features with a high channel number\nutilizing deep neural networks. However, many channels of extracted features\nare not discriminative and contain a lot of redundant information. In this\npaper, we address above issue by introducing the Distance Guided Channel\nWeighting (DGCW) Module. The DGCW module is constructed in a pixel-wise context\nextraction manner, which enhances the discriminativeness of features by\nweighting different channels of each pixel's feature vector when modeling its\nrelationship with other pixels. It can make full use of the high-discriminative\ninformation while ignore the low-discriminative information containing in\nfeature maps, as well as capture the long-range dependencies. Furthermore, by\nincorporating the DGCW module with a baseline segmentation network, we propose\nthe Distance Guided Channel Weighting Network (DGCWNet). We conduct extensive\nexperiments to demonstrate the effectiveness of DGCWNet. In particular, it\nachieves 81.6% mIoU on Cityscapes with only fine annotated data for training,\nand also gains satisfactory performance on another two semantic segmentation\ndatasets, i.e. Pascal Context and ADE20K. Code will be available soon at\nhttps://github.com/LanyunZhu/DGCWNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuanyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lanyun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shiping Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Li Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super-Resolution Domain Adaptation Networks for Semantic Segmentation via Pixel and Output Level Aligning. (arXiv:2005.06382v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.06382","description":"<p>Recently, Unsupervised Domain Adaptation (UDA) has attracted increasing\nattention to address the domain shift problem in the semantic segmentation\ntask. Although previous UDA methods have achieved promising performance, they\nstill suffer from the distribution gaps between source and target domains,\nespecially the resolution discrepany in the remote sensing images. To address\nthis problem, this paper designs a novel end-to-end semantic segmentation\nnetwork, namely Super-Resolution Domain Adaptation Network (SRDA-Net). SRDA-Net\ncan simultaneously achieve the super-resolution task and the domain adaptation\ntask, thus satisfying the requirement of semantic segmentation for remote\nsensing images which usually involve various resolution images. The proposed\nSRDA-Net includes three parts: a Super-Resolution and Segmentation (SRS) model\nwhich focuses on recovering high-resolution image and predicting segmentation\nmap, a Pixel-level Domain Classifier (PDC) for determining which domain the\npixel belongs to, and an Output-space Domain Classifier (ODC) for\ndistinguishing which domain the pixel contribution is from. By jointly\noptimizing SRS with two classifiers, the proposed method can not only\neliminates the resolution difference between source and target domains, but\nalso improve the performance of the semantic segmentation task. Experimental\nresults on two remote sensing datasets with different resolutions demonstrate\nthat SRDA-Net performs favorably against some state-of-the-art methods in terms\nof accuracy and visual quality. Code and models are available at\nhttps://github.com/tangzhenjie/SRDA-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junfeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenjie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Congan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Enhai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Long Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wenjun Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Sampling for Neural Point Cloud Consolidation. (arXiv:2008.06471v3 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2008.06471","description":"<p>We introduce a novel technique for neural point cloud consolidation which\nlearns from only the input point cloud. Unlike other point upsampling methods\nwhich analyze shapes via local patches, in this work, we learn from global\nsubsets. We repeatedly self-sample the input point cloud with global subsets\nthat are used to train a deep neural network. Specifically, we define source\nand target subsets according to the desired consolidation criteria (e.g.,\ngenerating sharp points or points in sparse regions). The network learns a\nmapping from source to target subsets, and implicitly learns to consolidate the\npoint cloud. During inference, the network is fed with random subsets of points\nfrom the input, which it displaces to synthesize a consolidated point set. We\nleverage the inductive bias of neural networks to eliminate noise and outliers,\na notoriously difficult problem in point cloud consolidation. The shared\nweights of the network are optimized over the entire shape, learning non-local\nstatistics and exploiting the recurrence of local-scale geometries.\nSpecifically, the network encodes the distribution of the underlying shape\nsurface within a fixed set of local kernels, which results in the best\nexplanation of the underlying shape surface. We demonstrate the ability to\nconsolidate point sets from a variety of shapes, while eliminating outliers and\nnoise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Metzer_G/0/1/0/all/0/1\">Gal Metzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1\">Rana Hanocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Local Robust Quaternion Matrix Completion for Color Images and Videos Inpainting. (arXiv:2011.08675v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.08675","description":"<p>The image nonlocal self-similarity (NSS) prior refers to the fact that a\nlocal patch often has many nonlocal similar patches to it across the image and\nhas been widely applied in many recently proposed machining learning algorithms\nfor image processing. However, there is no theoretical analysis on its working\nprinciple in the literature. In this paper, we discover a potential causality\nbetween NSS and low-rank property of color images, which is also available to\ngrey images. A new patch group based NSS prior scheme is proposed to learn\nexplicit NSS models of natural color images. The numerical low-rank property of\npatched matrices is also rigorously proved. The NSS-based QMC algorithm\ncomputes an optimal low-rank approximation to the high-rank color image,\nresulting in high PSNR and SSIM measures and particularly the better visual\nquality. A new tensor NSS-based QMC method is also presented to solve the color\nvideo inpainting problem based on quaternion tensor representation. The\nnumerical experiments on color images and videos indicate the advantages of\nNSS-based QMC over the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhigang Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiyu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael K. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xile Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Image Generation via Sparse Modeling. (arXiv:2104.00464v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00464","description":"<p>The interest of the deep learning community in image synthesis has grown\nmassively in recent years. Nowadays, deep generative methods, and especially\nGenerative Adversarial Networks (GANs), are leading to state-of-the-art\nperformance, capable of synthesizing images that appear realistic. While the\nefforts for improving the quality of the generated images are extensive, most\nattempts still consider the generator part as an uncorroborated \"black-box\". In\nthis paper, we aim to provide a better understanding and design of the image\ngeneration process. We interpret existing generators as implicitly relying on\nsparsity-inspired models. More specifically, we show that generators can be\nviewed as manifestations of the Convolutional Sparse Coding (CSC) and its\nMulti-Layered version (ML-CSC) synthesis processes. We leverage this\nobservation by explicitly enforcing a sparsifying regularization on\nappropriately chosen activation layers in the generator, and demonstrate that\nthis leads to improved image synthesis. Furthermore, we show that the same\nrationale and benefits apply to generators serving inverse problems,\ndemonstrated on the Deep Image Prior (DIP) method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1\">Roy Ganz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval. (arXiv:2104.00650v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00650","description":"<p>Our objective in this work is video-text retrieval - in particular a joint\nembedding that enables efficient text-to-video retrieval. The challenges in\nthis area include the design of the visual architecture and the nature of the\ntraining data, in that the available large scale video-text training datasets,\nsuch as HowTo100M, are noisy and hence competitive performance is achieved only\nat scale through large amounts of compute. We address both these challenges in\nthis paper. We propose an end-to-end trainable model that is designed to take\nadvantage of both large-scale image and video captioning datasets. Our model is\nan adaptation and extension of the recent ViT and Timesformer architectures,\nand consists of attention in both space and time. The model is flexible and can\nbe trained on both image and video text datasets, either independently or in\nconjunction. It is trained with a curriculum learning schedule that begins by\ntreating images as 'frozen' snapshots of video, and then gradually learns to\nattend to increasing temporal context when trained on video datasets. We also\nprovide a new video-text pretraining dataset WebVid-2M, comprised of over two\nmillion videos with weak captions scraped from the internet. Despite training\non datasets that are an order of magnitude smaller, we show that this approach\nyields state-of-the-art results on standard downstream video-retrieval\nbenchmarks including MSR-VTT, MSVD, DiDeMo and LSMDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bain_M/0/1/0/all/0/1\">Max Bain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Hyper-Network for Blind Super-Resolution with Multiple Degradations. (arXiv:2104.03926v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03926","description":"<p>Although single-image super-resolution (SISR) methods have achieved great\nsuccess on single degradation, they still suffer performance drop with multiple\ndegrading effects in real scenarios. Recently, some blind and non-blind models\nfor multiple degradations have been explored. However, those methods usually\ndegrade significantly for distribution shifts between the training and test\ndata. Towards this end, we propose a conditional meta-network framework (named\nCMDSR) for the first time, which helps SR framework learn how to adapt to\nchanges in input distribution. We extract degradation prior at task-level with\nthe proposed ConditionNet, which will be used to adapt the parameters of the\nbasic SR network (BaseNet). Specifically, the ConditionNet of our framework\nfirst learns the degradation prior from a support set, which is composed of a\nseries of degraded image patches from the same task. Then the adaptive BaseNet\nrapidly shifts its parameters according to the conditional features. Moreover,\nin order to better extract degradation prior, we propose a task contrastive\nloss to decrease the inner-task distance and increase the cross-task distance\nbetween task-level features. Without predefining degradation maps, our blind\nframework can conduct one single parameter update to yield considerable SR\nresults. Extensive experiments demonstrate the effectiveness of CMDSR over\nvarious blind, even non-blind methods. The flexible BaseNet structure also\nreveals that CMDSR can be a general framework for large series of SISR models.\nOur code is available at \\url{https://github.com/guanghaoyin/CMDSR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1\">Guanghao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dongdong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shouqian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning general and distinctive 3D local deep descriptors for point cloud registration. (arXiv:2105.10382v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10382","description":"<p>An effective 3D descriptor should be invariant to different geometric\ntransformations, such as scale and rotation, robust to occlusions and clutter,\nand capable of generalising to different application domains. We present a\nsimple yet effective method to learn general and distinctive 3D local\ndescriptors that can be used to register point clouds that are captured in\ndifferent domains. Point cloud patches are extracted, canonicalised with\nrespect to their local reference frame, and encoded into scale and\nrotation-invariant compact descriptors by a deep neural network that is\ninvariant to permutations of the input points. This design is what enables our\ndescriptors to generalise across domains. We evaluate and compare our\ndescriptors with alternative handcrafted and deep learning-based descriptors on\nseveral indoor and outdoor datasets that are reconstructed by using both RGBD\nsensors and laser scanners. Our descriptors outperform most recent descriptors\nby a large margin in terms of generalisation, and also become the state of the\nart in benchmarks where training and testing are performed in the same domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1\">Fabio Poiesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boscaini_D/0/1/0/all/0/1\">Davide Boscaini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Coupling of Depth and Egomotion Networks for Self-Supervised Structure from Motion. (arXiv:2106.04007v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04007","description":"<p>Structure from motion (SfM) has recently been formulated as a self-supervised\nlearning problem, where neural network models of depth and egomotion are\nlearned jointly through view synthesis. Herein, we address the open problem of\nhow to best couple, or link, the depth and egomotion network components, so\nthat information such as a common scale factor can be shared between the\nnetworks. Towards this end, we introduce several notions of coupling,\ncategorize existing approaches, and present a novel tightly-coupled approach\nthat leverages the interdependence of depth and egomotion at training time and\nat test time. Our approach uses iterative view synthesis to recursively update\nthe egomotion network input, permitting contextual information to be passed\nbetween the components. We demonstrate through substantial experiments that our\napproach promotes consistency between the depth and egomotion predictions at\ntest time, improves generalization, and leads to state-of-the-art accuracy on\nindoor and outdoor depth and egomotion evaluation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wagstaff_B/0/1/0/all/0/1\">Brandon Wagstaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peretroukhin_V/0/1/0/all/0/1\">Valentin Peretroukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal and frequency-weighted tensor nuclear norm for hyperspectral image denoising. (arXiv:2106.12489v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.12489","description":"<p>Low-rankness is important in the hyperspectral image (HSI) denoising tasks.\nThe tensor nuclear norm (TNN), defined based on the tensor singular value\ndecomposition, is a state-of-the-art method to describe the low-rankness of\nHSI. However, TNN ignores some physical meanings of HSI in tackling denoising\ntasks, leading to suboptimal denoising performance. In this paper, we propose\nthe multi-modal and frequency-weighted tensor nuclear norm (MFWTNN) and the\nnon-convex MFWTNN for HSI denoising tasks. Firstly, we investigate the physical\nmeaning of frequency components and reconsider their weights to improve the\nlow-rank representation ability of TNN. Secondly, we consider the correlation\namong two spatial dimensions and the spectral dimension of HSI and combine the\nabove improvements to TNN to propose MFWTNN. Thirdly, we use non-convex\nfunctions to approximate the rank function of the frequency tensor and propose\nthe NonMFWTNN to relax the MFWTNN better. Besides, we adaptively choose bigger\nweights for slices mainly containing noise information and smaller weights for\nslices containing profile information. Finally, we develop the efficient\nalternating direction method of multiplier (ADMM) based algorithm to solve the\nproposed models, and the effectiveness of our models are substantiated in\nsimulated and real HSI datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_X/0/1/0/all/0/1\">Xiaozhen Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_W/0/1/0/all/0/1\">Wenfeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaDIV-Syn: Fast Depth-Independent View Synthesis using Soft Masks and Implicit Blending. (arXiv:2106.13139v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13139","description":"<p>Novel view synthesis is required in many robotic applications, such as VR\nteleoperation and scene reconstruction. Existing methods are often too slow for\nthese contexts, cannot handle dynamic scenes, and are limited by their explicit\ndepth estimation stage, where incorrect depth predictions can lead to large\nprojection errors. Our proposed method runs in real time on live streaming data\nand avoids explicit depth estimation by efficiently warping input images into\nthe target frame for a range of assumed depth planes. The resulting plane sweep\nvolume (PSV) is directly fed into our network, which first estimates soft PSV\nmasks in a self-supervised manner, and then directly produces the novel output\nview. This improves efficiency and performance on transparent, reflective,\nthin, and feature-less scene parts. FaDIV-Syn can perform both interpolation\nand extrapolation tasks at 540p in real-time and outperforms state-of-the-art\nextrapolation methods on the large-scale RealEstate10k dataset. We thoroughly\nevaluate ablations, such as removing the Soft-Masking network, training from\nfewer examples as well as generalization to higher resolutions and stronger\ndepth discretization. Our implementation is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rochow_A/0/1/0/all/0/1\">Andre Rochow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_M/0/1/0/all/0/1\">Max Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1\">Michael Weinmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertify: Attacks Against Neural Network Certification. (arXiv:2108.11299v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.11299","description":"<p>A key concept towards reliable, robust, and safe AI systems is the idea to\nimplement fallback strategies when predictions of the AI cannot be trusted.\nCertifiers for neural networks have made great progress towards provable\nrobustness guarantees against evasion attacks using adversarial examples. These\nmethods guarantee for some predictions that a certain class of manipulations or\nattacks could not have changed the outcome. For the remaining predictions\nwithout guarantees, the method abstains from making a prediction and a fallback\nstrategy needs to be invoked, which is typically more costly, less accurate, or\neven involves a human operator. While this is a key concept towards safe and\nsecure AI, we show for the first time that this strategy comes with its own\nsecurity risks, as such fallback strategies can be deliberately triggered by an\nadversary. In particular, we conduct the first systematic analysis of\ntraining-time attacks against certifiers in practical application pipelines,\nidentifying new threat vectors that can be exploited to degrade the overall\nsystem. Using these insights, we design two backdoor attacks against network\ncertifiers, which can drastically reduce certified robustness. For example,\nadding 1% poisoned data during training is sufficient to reduce certified\nrobustness by up to 95 percentage points, effectively rendering the certifier\nuseless. We analyze how such novel attacks can compromise the overall system's\nintegrity or availability. Our extensive experiments across multiple datasets,\nmodel architectures, and certifiers demonstrate the wide applicability of these\nattacks. A first investigation into potential defenses shows that current\napproaches are insufficient to mitigate the issue, highlighting the need for\nnew, more specific solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorenz_T/0/1/0/all/0/1\">Tobias Lorenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1\">Marta Kwiatkowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Authentication Attacks on Projection-based Cancelable Biometric Schemes (long version). (arXiv:2110.15163v5 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2110.15163","description":"<p>Cancelable biometric schemes aim at generating secure biometric templates by\ncombining user specific tokens, such as password, stored secret or salt, along\nwith biometric data. This type of transformation is constructed as a\ncomposition of a biometric transformation with a feature extraction algorithm.\nThe security requirements of cancelable biometric schemes concern the\nirreversibility, unlinkability and revocability of templates, without losing in\naccuracy of comparison. While several schemes were recently attacked regarding\nthese requirements, full reversibility of such a composition in order to\nproduce colliding biometric characteristics, and specifically presentation\nattacks, were never demonstrated to the best of our knowledge. In this paper,\nwe formalize these attacks for a traditional cancelable scheme with the help of\ninteger linear programming (ILP) and quadratically constrained quadratic\nprogramming (QCQP). Solving these optimization problems allows an adversary to\nslightly alter its fingerprint image in order to impersonate any individual.\nMoreover, in an even more severe scenario, it is possible to simultaneously\nimpersonate several individuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durbet_A/0/1/0/all/0/1\">Axel Durbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lafourcade_P/0/1/0/all/0/1\">Pascal Lafourcade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migdal_D/0/1/0/all/0/1\">Denis Migdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiry_Atighehchi_K/0/1/0/all/0/1\">Kevin Thiry-Atighehchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grollemund_P/0/1/0/all/0/1\">Paul-Marie Grollemund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeltaConv: Anisotropic Operators for Geometric Deep Learning on Point Clouds. (arXiv:2111.08799v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08799","description":"<p>Learning from 3D point-cloud data has rapidly gained momentum, motivated by\nthe success of deep learning on images and the increased availability of\n3D~data. In this paper, we aim to construct anisotropic convolution layers that\nwork directly on the surface derived from a point cloud. This is challenging\nbecause of the lack of a global coordinate system for tangential directions on\nsurfaces. We introduce DeltaConv, a convolution layer that combines geometric\noperators from vector calculus to enable the construction of anisotropic\nfilters on point clouds. Because these operators are defined on scalar- and\nvector-fields, we separate the network into a scalar- and a vector-stream,\nwhich are connected by the operators. The vector stream enables the network to\nexplicitly represent, evaluate, and process directional information. Our\nconvolutions are robust and simple to implement and match or improve on\nstate-of-the-art approaches on several benchmarks, while also speeding up\ntraining and inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiersma_R/0/1/0/all/0/1\">Ruben Wiersma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasikun_A/0/1/0/all/0/1\">Ahmad Nasikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisemann_E/0/1/0/all/0/1\">Elmar Eisemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hildebrandt_K/0/1/0/all/0/1\">Klaus Hildebrandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeCGAN: Parallel Conditional Generative Adversarial Networks for Face Editing via Semantic Consistency. (arXiv:2111.09298v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09298","description":"<p>Semantically guided conditional Generative Adversarial Networks (cGANs) have\nbecome a popular approach for face editing in recent years. However, most\nexisting methods introduce semantic masks as direct conditional inputs to the\ngenerator and often require the target masks to perform the corresponding\ntranslation in the RGB space. We propose SeCGAN, a novel label-guided cGAN for\nediting face images utilising semantic information without the need to specify\ntarget semantic masks. During training, SeCGAN has two branches of generators\nand discriminators operating in parallel, with one trained to translate RGB\nimages and the other for semantic masks. To bridge the two branches in a\nmutually beneficial manner, we introduce a semantic consistency loss which\nconstrains both branches to have consistent semantic outputs. Whilst both\nbranches are required during training, the RGB branch is our primary network\nand the semantic branch is not needed for inference. Our results on CelebA and\nCelebA-HQ demonstrate that our approach is able to generate facial images with\nmore accurate attributes, outperforming competitive baselines in terms of\nTarget Attribute Recognition Rate whilst maintaining quality metrics such as\nself-supervised Fr\\'{e}chet Inception Distance and Inception Score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiaze Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1\">Binod Bhattarai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae-Kyun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Anomaly Detection Using Self-Supervised Multi-Cue Tasks. (arXiv:2111.12379v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12379","description":"<p>Anomaly detection is important in many real-life applications. Recently,\nself-supervised learning has greatly helped deep anomaly detection by\nrecognizing several geometric transformations. However these methods lack finer\nfeatures, usually highly depend on the anomaly type, and do not perform well on\nfine-grained problems. To address these issues, we first introduce in this work\nthree novel and efficient discriminative and generative tasks which have\ncomplementary strength: (i) a piece-wise jigsaw puzzle task focuses on\nstructure cues; (ii) a tint rotation recognition is used within each piece,\ntaking into account the colorimetry information; (iii) and a partial\nre-colorization task considers the image texture. In order to make the\nre-colorization task more object-oriented than background-oriented, we propose\nto include the contextual color information of the image border via an\nattention mechanism. We then present a new out-of-distribution detection\nfunction and highlight its better stability compared to existing methods. Along\nwith it, we also experiment different score fusion functions. Finally, we\nevaluate our method on an extensive protocol composed of various anomaly types,\nfrom object anomalies, style anomalies with fine-grained classification to\nlocal anomalies with face anti-spoofing datasets. Our model significantly\noutperforms state-of-the-art with up to 36% relative error improvement on\nobject anomalies and 40% on face anti-spoofing problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jezequel_L/0/1/0/all/0/1\">Loic Jezequel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc-Son Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaudet_J/0/1/0/all/0/1\">Jean Beaudet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Histace_A/0/1/0/all/0/1\">Aymeric Histace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound. (arXiv:2201.02639v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02639","description":"<p>As humans, we navigate a multimodal world, building a holistic understanding\nfrom all our senses. We introduce MERLOT Reserve, a model that represents\nvideos jointly over time -- through a new training objective that learns from\naudio, subtitles, and video frames. Given a video, we replace snippets of text\nand audio with a MASK token; the model learns by choosing the correct\nmasked-out snippet. Our objective learns faster than alternatives, and performs\nwell at scale: we pretrain on 20 million YouTube videos.\n</p>\n<p>Empirical results show that MERLOT Reserve learns strong multimodal\nrepresentations. When finetuned, it sets state-of-the-art on Visual Commonsense\nReasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%,\nand 1.5% respectively. Ablations show that these tasks benefit from audio\npretraining -- even VCR, a QA task centered around images (without sound).\nMoreover, our objective enables out-of-the-box prediction, revealing strong\nmultimodal commonsense understanding. In a fully zero-shot setting, our model\nobtains competitive results on four video tasks, even outperforming supervised\napproaches on the recently proposed Situated Reasoning (STAR) benchmark.\n</p>\n<p>We analyze why audio enables better vision-language representations,\nsuggesting significant opportunities for future research. We conclude by\ndiscussing ethical and societal implications of multimodal pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M\\\"obius Convolutions for Spherical CNNs. (arXiv:2201.12212v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12212","description":"<p>M\\\"obius transformations play an important role in both geometry and\nspherical image processing - they are the group of conformal automorphisms of\n2D surfaces and the spherical equivalent of homographies. Here we present a\nnovel, M\\\"obius-equivariant spherical convolution operator which we call\nM\\\"obius convolution, and with it, develop the foundations for\nM\\\"obius-equivariant spherical CNNs. Our approach is based on a simple\nobservation: to achieve equivariance, we only need to consider the\nlower-dimensional subgroup which transforms the positions of points as seen in\nthe frames of their neighbors. To efficiently compute M\\\"obius convolutions at\nscale we derive an approximation of the action of the transformations on\nspherical filters, allowing us to compute our convolutions in the spectral\ndomain with the fast Spherical Harmonic Transform. The resulting framework is\nboth flexible and descriptive, and we demonstrate its utility by achieving\npromising results in both shape classification and image segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitchel_T/0/1/0/all/0/1\">Thomas W. Mitchel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1\">Noam Aigerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vladimir G. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazhdan_M/0/1/0/all/0/1\">Michael Kazhdan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computing Multiple Image Reconstructions with a Single Hypernetwork. (arXiv:2202.11009v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11009","description":"<p>Deep learning based techniques achieve state-of-the-art results in a wide\nrange of image reconstruction tasks like compressed sensing. These methods\nalmost always have hyperparameters, such as the weight coefficients that\nbalance the different terms in the optimized loss function. The typical\napproach is to train the model for a hyperparameter setting determined with\nsome empirical or theoretical justification. Thus, at inference time, the model\ncan only compute reconstructions corresponding to the pre-determined\nhyperparameter values. In this work, we present a hypernetwork-based approach,\ncalled HyperRecon, to train reconstruction models that are agnostic to\nhyperparameter settings. At inference time, HyperRecon can efficiently produce\ndiverse reconstructions, which would each correspond to different\nhyperparameter values. In this framework, the user is empowered to select the\nmost useful output(s) based on their own judgement. We demonstrate our method\nin compressed sensing, super-resolution and denoising tasks, using two\nlarge-scale and publicly-available MRI datasets. Our code is available at\nhttps://github.com/alanqrwang/hyperrecon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alan Q. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabuncu_M/0/1/0/all/0/1\">Mert R. Sabuncu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards deep learning-powered IVF: A large public benchmark for morphokinetic parameter prediction. (arXiv:2203.00531v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.00531","description":"<p>An important limitation to the development of Artificial Intelligence\n(AI)-based solutions for In Vitro Fertilization (IVF) is the absence of a\npublic reference benchmark to train and evaluate deep learning (DL) models. In\nthis work, we describe a fully annotated dataset of 704 videos of developing\nembryos, for a total of 337k images. We applied ResNet, LSTM, and ResNet-3D\narchitectures to our dataset and demonstrate that they overperform algorithmic\napproaches to automatically annotate stage development phases. Altogether, we\npropose the first public benchmark that will allow the community to evaluate\nmorphokinetic models. This is the first step towards deep learning-powered IVF.\nOf note, we propose highly detailed annotations with 16 different development\nphases, including early cell division phases, but also late cell divisions,\nphases after morulation, and very early phases, which have never been used\nbefore. We postulate that this original approach will help improve the overall\nperformance of deep learning approaches on time-lapse videos of embryo\ndevelopment, ultimately benefiting infertile patients with improved clinical\nsuccess rates (Code and data are available at\nhttps://gitlab.univ-nantes.fr/E144069X/bench_mk_pred.git).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gomez_T/0/1/0/all/0/1\">Tristan Gomez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feyeux_M/0/1/0/all/0/1\">Magalie Feyeux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Normand_N/0/1/0/all/0/1\">Nicolas Normand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+David_L/0/1/0/all/0/1\">Laurent David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paul_Gilloteaux_P/0/1/0/all/0/1\">Perrine Paul-Gilloteaux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Freour_T/0/1/0/all/0/1\">Thomas Fr&#xe9;our</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mouchere_H/0/1/0/all/0/1\">Harold Mouch&#xe8;re</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Vision Transformers for Joint SAR-optical Representation Learning. (arXiv:2204.05381v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05381","description":"<p>Self-supervised learning (SSL) has attracted much interest in remote sensing\nand earth observation due to its ability to learn task-agnostic representations\nwithout human annotation. While most of the existing SSL works in remote\nsensing utilize ConvNet backbones and focus on a single modality, we explore\nthe potential of vision transformers (ViTs) for joint SAR-optical\nrepresentation learning. Based on DINO, a state-of-the-art SSL algorithm that\ndistills knowledge from two augmented views of an input image, we combine SAR\nand optical imagery by concatenating all channels to a unified input.\nSubsequently, we randomly mask out channels of one modality as a data\naugmentation strategy. While training, the model gets fed optical-only,\nSAR-only, and SAR-optical image pairs learning both inner- and intra-modality\nrepresentations. Experimental results employing the BigEarthNet-MM dataset\ndemonstrate the benefits of both, the ViT backbones and the proposed multimodal\nSSL algorithm DINO-MM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_C/0/1/0/all/0/1\">Conrad M Albrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A9-Dataset: Multi-Sensor Infrastructure-Based Dataset for Mobility Research. (arXiv:2204.06527v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06527","description":"<p>Data-intensive machine learning based techniques increasingly play a\nprominent role in the development of future mobility solutions - from driver\nassistance and automation functions in vehicles, to real-time traffic\nmanagement systems realized through dedicated infrastructure. The availability\nof high quality real-world data is often an important prerequisite for the\ndevelopment and reliable deployment of such systems in large scale. Towards\nthis endeavour, we present the A9-Dataset based on roadside sensor\ninfrastructure from the 3 km long Providentia++ test field near Munich in\nGermany. The dataset includes anonymized and precision-timestamped multi-modal\nsensor and object data in high resolution, covering a variety of traffic\nsituations. As part of the first set of data, which we describe in this paper,\nwe provide camera and LiDAR frames from two overhead gantry bridges on the A9\nautobahn with the corresponding objects labeled with 3D bounding boxes. The\nfirst set includes in total more than 1000 sensor frames and 14000 traffic\nobjects. The dataset is available for download at https://a9-dataset.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cress_C/0/1/0/all/0/1\">Christian Cre&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_W/0/1/0/all/0/1\">Walter Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strand_L/0/1/0/all/0/1\">Leah Strand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarasimhan_V/0/1/0/all/0/1\">Venkatnarayanan Lakshminarasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortkord_M/0/1/0/all/0/1\">Maximilian Fortkord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Siyi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Recent Work in Media Forensics: Methods and Threats. (arXiv:2204.12067v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12067","description":"<p>In this paper, we review recent work in media forensics for digital images,\nvideo, audio (specifically speech), and documents. For each data modality, we\ndiscuss synthesis and manipulation techniques that can be used to create and\nmodify digital media. We then review technological advancements for detecting\nand quantifying such manipulations. Finally, we consider open issues and\nsuggest directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhagtani_K/0/1/0/all/0/1\">Kratika Bhagtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_A/0/1/0/all/0/1\">Amit Kumar Singh Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartusiak_E/0/1/0/all/0/1\">Emily R. Bartusiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Ziyue Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Ruiting Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baireddy_S/0/1/0/all/0/1\">Sriram Baireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart City Intersections: Intelligence Nodes for Future Metropolises. (arXiv:2205.01686v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01686","description":"<p>Traffic intersections are the most suitable locations for the deployment of\ncomputing, communications, and intelligence services for smart cities of the\nfuture. The abundance of data to be collected and processed, in combination\nwith privacy and security concerns, motivates the use of the edge-computing\nparadigm which aligns well with physical intersections in metropolises. This\npaper focuses on high-bandwidth, low-latency applications, and in that context\nit describes: (i) system design considerations for smart city intersection\nintelligence nodes; (ii) key technological components including sensors,\nnetworking, edge computing, low latency design, and AI-based intelligence; and\n(iii) applications such as privacy preservation, cloud-connected vehicles, a\nreal-time \"radar-screen\", traffic management, and monitoring of pedestrian\nbehavior during pandemics. The results of the experimental studies performed on\nthe COSMOS testbed located in New York City are illustrated. Future challenges\nin designing human-centered smart city intersections are summarized.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1\">Zoran Kosti&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angus_A/0/1/0/all/0/1\">Alex Angus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhuoxu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seskar_I/0/1/0/all/0/1\">Ivan Seskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zussman_G/0/1/0/all/0/1\">Gil Zussman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raychaudhuri_D/0/1/0/all/0/1\">Dipankar Raychaudhuri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Clustering Based Pseudo-labeling Strategy for Multi-modal Aerial View Object Classification. (arXiv:2205.01920v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01920","description":"<p>Multi-modal aerial view object classification (MAVOC) in Automatic target\nrecognition (ATR), although an important and challenging problem, has been\nunder studied. This paper firstly finds that fine-grained data, class imbalance\nand various shooting conditions preclude the representational ability of\ngeneral image classification. Moreover, the MAVOC dataset has scene aggregation\ncharacteristics. By exploiting these properties, we propose Scene Clustering\nBased Pseudo-labeling Strategy (SCP-Label), a simple yet effective method to\nemploy in post-processing. The SCP-Label brings greater accuracy by assigning\nthe same label to objects within the same scene while also mitigating bias and\nconfusion with model ensembles. Its performance surpasses the official baseline\nby a large margin of +20.57% Accuracy on Track 1 (SAR), and +31.86% Accuracy on\nTrack 2 (SAR+EO), demonstrating the potential of SCP-Label as post-processing.\nFinally, we win the championship both on Track1 and Track2 in the CVPR 2022\nPerception Beyond the Visible Spectrum (PBVS) Workshop MAVOC Challenge. Our\ncode is available at https://github.com/HowieChangchn/SCP-Label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keda Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Shenshen Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arrhythmia Classifier using Binarized Convolutional Neural Network for Resource-Constrained Devices. (arXiv:2205.03661v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03661","description":"<p>Monitoring electrocardiogram signals is of great significance for the\ndiagnosis of arrhythmias. In recent years, deep learning and convolutional\nneural networks have been widely used in the classification of cardiac\narrhythmias. However, the existing neural network applied to ECG signal\ndetection usually requires a lot of computing resources, which is not friendlyF\nto resource-constrained equipment, and it is difficult to realize real-time\nmonitoring. In this paper, a binarized convolutional neural network suitable\nfor ECG monitoring is proposed, which is hardware-friendly and more suitable\nfor use in resource-constrained wearable devices. Targeting the MIT-BIH\narrhythmia database, the classifier based on this network reached an accuracy\nof 95.67% in the five-class test. Compared with the proposed baseline\nfull-precision network with an accuracy of 96.45%, it is only 0.78% lower.\nImportantly, it achieves 12.65 times the computing speedup, 24.8 times the\nstorage compression ratio, and only requires a quarter of the memory overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenxing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hanshi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_N/0/1/0/all/0/1\">Ninghao Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Retrieve Videos by Asking Questions. (arXiv:2205.05739v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05739","description":"<p>The majority of traditional text-to-video retrieval systems operate in static\nenvironments, i.e., there is no interaction between the user and the agent\nbeyond the initial textual query provided by the user. This can be suboptimal\nif the initial query has ambiguities, which would lead to many falsely\nretrieved videos. To overcome this limitation, we propose a novel framework for\nVideo Retrieval using Dialog (ViReD), which enables the user to interact with\nan AI agent via multiple rounds of dialog. The key contribution of our\nframework is a novel multimodal question generator that learns to ask questions\nthat maximize the subsequent video retrieval performance. Our multimodal\nquestion generator uses (i) the video candidates retrieved during the last\nround of interaction with the user and (ii) the text-based dialog history\ndocumenting all previous interactions, to generate questions that incorporate\nboth visual and linguistic cues relevant to video retrieval. Furthermore, to\ngenerate maximally informative questions, we propose an Information-Guided\nSupervision (IGS), which guides the question generator to ask questions that\nwould boost subsequent video retrieval accuracy. We validate the effectiveness\nof our interactive ViReD framework on the AVSD dataset, showing that our\ninteractive method performs significantly better than traditional\nnon-interactive video retrieval systems. Furthermore, we also demonstrate that\nour proposed approach also generalizes to the real-world settings that involve\ninteractions with real humans, thus, demonstrating the robustness and\ngenerality of our framework\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1\">Avinash Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1\">Junier Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Representation for Point Clouds. (arXiv:2205.05740v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05740","description":"<p>Most prior work represents the shapes of point clouds by coordinates.\nHowever, it is insufficient to describe the local geometry directly. In this\npaper, we present \\textbf{RepSurf} (representative surfaces), a novel\nrepresentation of point clouds to \\textbf{explicitly} depict the very local\nstructure. We explore two variants of RepSurf, Triangular RepSurf and Umbrella\nRepSurf inspired by triangle meshes and umbrella curvature in computer\ngraphics. We compute the representations of RepSurf by predefined geometric\npriors after surface reconstruction. RepSurf can be a plug-and-play module for\nmost point cloud models thanks to its free collaboration with irregular points.\nBased on a simple baseline of PointNet++ (SSG version), Umbrella RepSurf\nsurpasses the previous state-of-the-art by a large margin for classification,\nsegmentation and detection on various benchmarks in terms of performance and\nefficiency. With an increase of around \\textbf{0.008M} number of parameters,\n\\textbf{0.04G} FLOPs, and \\textbf{1.12ms} inference time, our method achieves\n\\textbf{94.7\\%} (+0.5\\%) on ModelNet40, and \\textbf{84.6\\%} (+1.8\\%) on\nScanObjectNN for classification, while \\textbf{74.3\\%} (+0.8\\%) mIoU on S3DIS\n6-fold, and \\textbf{70.0\\%} (+1.6\\%) mIoU on ScanNet for segmentation. For\ndetection, previous state-of-the-art detector with our RepSurf obtains\n\\textbf{71.2\\%} (+2.1\\%) mAP$\\mathit{_{25}}$, \\textbf{54.8\\%} (+2.0\\%)\nmAP$\\mathit{_{50}}$ on ScanNetV2, and \\textbf{64.9\\%} (+1.9\\%)\nmAP$\\mathit{_{25}}$, \\textbf{47.7\\%} (+2.5\\%) mAP$\\mathit{_{50}}$ on SUN RGB-D.\nOur lightweight Triangular RepSurf performs its excellence on these benchmarks\nas well. The code is publicly available at\n\\url{https://github.com/hancyran/RepSurf}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ran_H/0/1/0/all/0/1\">Haoxi Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEWS: Real-time Social Media Manipulation Detection and Analysis. (arXiv:2205.05783v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05783","description":"<p>This article presents a beta-version of MEWS (Misinformation Early Warning\nSystem). It describes the various aspects of the ingestion, manipulation\ndetection, and graphing algorithms employed to determine--in near\nreal-time--the relationships between social media images as they emerge and\nspread on social media platforms. By combining these various technologies into\na single processing pipeline, MEWS can identify manipulated media items as they\narise and identify when these particular items begin trending on individual\nsocial media platforms or even across multiple platforms. The emergence of a\nnovel manipulation followed by rapid diffusion of the manipulated content\nsuggests a disinformation campaign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ford_T/0/1/0/all/0/1\">Trenton W. Ford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theisen_W/0/1/0/all/0/1\">William Theisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yankoski_M/0/1/0/all/0/1\">Michael Yankoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Tom Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashman_F/0/1/0/all/0/1\">Farah Khashman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dearstyne_K/0/1/0/all/0/1\">Katherine R. Dearstyne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weninger_T/0/1/0/all/0/1\">Tim Weninger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"F3A-GAN: Facial Flow for Face Animation with Generative Adversarial Networks. (arXiv:2205.06204v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.06204","description":"<p>Formulated as a conditional generation problem, face animation aims at\nsynthesizing continuous face images from a single source image driven by a set\nof conditional face motion. Previous works mainly model the face motion as\nconditions with 1D or 2D representation (e.g., action units, emotion codes,\nlandmark), which often leads to low-quality results in some complicated\nscenarios such as continuous generation and largepose transformation. To tackle\nthis problem, the conditions are supposed to meet two requirements, i.e.,\nmotion information preserving and geometric continuity. To this end, we propose\na novel representation based on a 3D geometric flow, termed facial flow, to\nrepresent the natural motion of the human face at any pose. Compared with other\nprevious conditions, the proposed facial flow well controls the continuous\nchanges to the face. After that, in order to utilize the facial flow for face\nediting, we build a synthesis framework generating continuous images with\nconditional facial flows. To fully take advantage of the motion information of\nfacial flows, a hierarchical conditional framework is designed to combine the\nextracted multi-scale appearance features from images and motion features from\nflows in a hierarchical manner. The framework then decodes multiple fused\nfeatures back to images progressively. Experimental results demonstrate the\neffectiveness of our method compared to other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qihang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yiming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Songyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lingyun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}