{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"SRU++: Pioneering Fast Recurrence with Attention for Speech Recognition. (arXiv:2110.05571v1 [eess.AS])","link":"http://arxiv.org/abs/2110.05571","description":"<p>The Transformer architecture has been well adopted as a dominant architecture\nin most sequence transduction tasks including automatic speech recognition\n(ASR), since its attention mechanism excels in capturing long-range\ndependencies. While models built solely upon attention can be better\nparallelized than regular RNN, a novel network architecture, SRU++, was\nrecently proposed. By combining the fast recurrence and attention mechanism,\nSRU++ exhibits strong capability in sequence modeling and achieves\nnear-state-of-the-art results in various language modeling and machine\ntranslation tasks with improved compute efficiency. In this work, we present\nthe advantages of applying SRU++ in ASR tasks by comparing with Conformer\nacross multiple ASR benchmarks and study how the benefits can be generalized to\nlong-form speech inputs. On the popular LibriSpeech benchmark, our SRU++ model\nachieves 2.0% / 4.7% WER on test-clean / test-other, showing competitive\nperformances compared with the state-of-the-art Conformer encoder under the\nsame set-up. Specifically, SRU++ can surpass Conformer on long-form speech\ninput with a large margin, based on our analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1\">Jing Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kwangyoun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_K/0/1/0/all/0/1\">Kyu Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Data Mining of Public Transport Incidents reported in Social Media. (arXiv:2110.05573v1 [cs.SI])","link":"http://arxiv.org/abs/2110.05573","description":"<p>Public transport agencies use social media as an essential tool for\ncommunicating mobility incidents to passengers. However, while the short term,\nday-to-day information about transport phenomena is usually posted in social\nmedia with low latency, its availability is short term as the content is rarely\nmade an aggregated form. Social media communication of transport phenomena\nusually lacks GIS annotations as most social media platforms do not allow\nattaching non-POI GPS coordinates to posts. As a result, the analysis of\ntransport phenomena information is minimal. We collected three years of social\nmedia posts of a polish public transport company with user comments. Through\nexploration, we infer a six-class transport information typology. We\nsuccessfully build an information type classifier for social media posts,\ndetect stop names in posts, and relate them to GPS coordinates, obtaining a\nspatial understanding of long-term aggregated phenomena. We show that our\napproach enables citizen science and use it to analyze the impact of three\nyears of infrastructure incidents on passenger mobility, and the sentiment and\nreaction scale towards each of the events. All these results are achieved for\nPolish, an under-resourced language when it comes to spatial language\nunderstanding, especially in social media contexts. To improve the situation,\nwe released two of our annotated data sets: social media posts with incident\ntype labels and matched stop names and social media comments with the annotated\nsentiment. We also opensource the experimental codebase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raczycki_K/0/1/0/all/0/1\">Kamil Raczycki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szymanski_M/0/1/0/all/0/1\">Marcin Szyma&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeliseyenka_Y/0/1/0/all/0/1\">Yahor Yeliseyenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szymanski_P/0/1/0/all/0/1\">Piotr Szyma&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajdanowicz_T/0/1/0/all/0/1\">Tomasz Kajdanowicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing to New Domains by Mapping Natural Language to Lifted LTL. (arXiv:2110.05603v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05603","description":"<p>Recent work on using natural language to specify commands to robots has\ngrounded that language to LTL. However, mapping natural language task\nspecifications to LTL task specifications using language models require\nprobability distributions over finite vocabulary. Existing state-of-the-art\nmethods have extended this finite vocabulary to include unseen terms from the\ninput sequence to improve output generalization. However, novel\nout-of-vocabulary atomic propositions cannot be generated using these methods.\nTo overcome this, we introduce an intermediate contextual query representation\nwhich can be learned from single positive task specification examples,\nassociating a contextual query with an LTL template. We demonstrate that this\nintermediate representation allows for generalization over unseen object\nreferences, assuming accurate groundings are available. We compare our method\nof mapping natural language task specifications to intermediate contextual\nqueries against state-of-the-art CopyNet models capable of translating natural\nlanguage to LTL, by evaluating whether correct LTL for manipulation and\nnavigation task specifications can be output, and show that our method\noutperforms the CopyNet model on unseen object references. We demonstrate that\nthe grounded LTL our method outputs can be used for planning in a simulated\nOO-MDP environment. Finally, we discuss some common failure modes encountered\nwhen translating natural language task specifications to grounded LTL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsiung_E/0/1/0/all/0/1\">Eric Hsiung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Hiloni Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_J/0/1/0/all/0/1\">Junchi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1\">Roma Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1\">Stefanie Tellex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1\">George Konidaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TCube: Domain-Agnostic Neural Time-series Narration. (arXiv:2110.05633v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05633","description":"<p>The task of generating rich and fluent narratives that aptly describe the\ncharacteristics, trends, and anomalies of time-series data is invaluable to the\nsciences (geology, meteorology, epidemiology) or finance (trades, stocks, or\nsales and inventory). The efforts for time-series narration hitherto are\ndomain-specific and use predefined templates that offer consistency but lead to\nmechanical narratives. We present TCube (Time-series-to-text), a\ndomain-agnostic neural framework for time-series narration, that couples the\nrepresentation of essential time-series elements in the form of a dense\nknowledge graph and the translation of said knowledge graph into rich and\nfluent narratives through the transfer-learning capabilities of PLMs\n(Pre-trained Language Models). TCube's design primarily addresses the challenge\nthat lies in building a neural framework in the complete paucity of annotated\ntraining data for time-series. The design incorporates knowledge graphs as an\nintermediary for the representation of essential time-series elements which can\nbe linearized for textual translation. To the best of our knowledge, TCube is\nthe first investigation of the use of neural strategies for time-series\nnarration. Through extensive evaluations, we show that TCube can improve the\nlexical diversity of the generated narratives by up to 65.38% while still\nmaintaining grammatical integrity. The practicality and deployability of TCube\nis further validated through an expert review (n=21) where 76.2% of\nparticipating experts wary of auto-generated narratives favored TCube as a\ndeployable system for time-series narration due to its richer narratives. Our\ncode-base, models, and datasets, with detailed instructions for reproducibility\nis publicly hosted at https://github.com/Mandar-Sharma/TCube.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mandar Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brownstein_J/0/1/0/all/0/1\">John S. Brownstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_N/0/1/0/all/0/1\">Naren Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Construction Grammars Converge Across Registers Given Increased Exposure. (arXiv:2110.05663v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05663","description":"<p>This paper measures the impact of increased exposure on whether learned\nconstruction grammars converge onto shared representations when trained on data\nfrom different registers. Register influences the frequency of constructions,\nwith some structures common in formal but not informal usage. We expect that a\ngrammar induction algorithm exposed to different registers will acquire\ndifferent constructions. To what degree does increased exposure lead to the\nconvergence of register-specific grammars? The experiments in this paper\nsimulate language learning in 12 languages (half Germanic and half Romance)\nwith corpora representing three registers (Twitter, Wikipedia, Web). These\nsimulations are repeated with increasing amounts of exposure, from 100k to 2\nmillion words, to measure the impact of exposure on the convergence of\ngrammars. The results show that increased exposure does lead to converging\ngrammars across all languages. In addition, a shared core of register-universal\nconstructions remains constant across increasing amounts of exposure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dunn_J/0/1/0/all/0/1\">Jonathan Dunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are you doing what I say? On modalities alignment in ALFRED. (arXiv:2110.05665v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05665","description":"<p>ALFRED is a recently proposed benchmark that requires a model to complete\ntasks in simulated house environments specified by instructions in natural\nlanguage. We hypothesize that key to success is accurately aligning the text\nmodality with visual inputs. Motivated by this, we inspect how well existing\nmodels can align these modalities using our proposed intrinsic metric, boundary\nadherence score (BAS). The results show the previous models are indeed failing\nto perform proper alignment. To address this issue, we introduce approaches\naimed at improving model alignment and demonstrate how improved alignment,\nimproves end task performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_T/0/1/0/all/0/1\">Ting-Rui Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1\">Ta-Chung Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yau-Shian Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v1 [cs.LG])","link":"http://arxiv.org/abs/2110.05679","description":"<p>Differentially Private (DP) learning has seen limited success for building\nlarge deep learning models of text, and attempts at straightforwardly applying\nDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have\nresulted in large performance drops and high computational overhead. We show\nthat this performance drop can be mitigated with (1) the use of large\npretrained models; (2) hyperparameters that suit DP optimization; and (3)\nfine-tuning objectives aligned with the pretraining procedure. With these\nfactors set right, we obtain private NLP models that outperform\nstate-of-the-art private training approaches and strong non-private baselines\n-- by directly fine-tuning pretrained models with DP optimization on\nmoderately-sized corpora. To address the computational challenge of running\nDP-SGD with large Transformers, we propose a memory saving technique that\nallows clipping in DP-SGD to run without instantiating per-example gradients\nfor any layer in the model. The technique enables privately training\nTransformers with almost the same memory cost as non-private training at a\nmodest run-time overhead. Contrary to conventional wisdom that DP optimization\nfails at learning high-dimensional models (due to noise that scales with\ndimension) empirical results reveal that private learning with pretrained\nmodels tends to not suffer from dimension-dependent performance degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doubly-Trained Adversarial Data Augmentation for Neural Machine Translation. (arXiv:2110.05691v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05691","description":"<p>Neural Machine Translation (NMT) models are known to suffer from noisy\ninputs. To make models robust, we generate adversarial augmentation samples\nthat attack the model and preserve the source-side semantic meaning at the same\ntime. To generate such samples, we propose a doubly-trained architecture that\npairs two NMT models of opposite translation directions with a joint loss\nfunction, which combines the target-side attack and the source-side semantic\nsimilarity constraint. The results from our experiments across three different\nlanguage pairs and two evaluation metrics show that these adversarial samples\nimprove the model robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weiting Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuoyang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khayrallah_H/0/1/0/all/0/1\">Huda Khayrallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Releasing Annotator-Level Labels and Information in Datasets. (arXiv:2110.05699v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05699","description":"<p>A common practice in building NLP datasets, especially using crowd-sourced\nannotations, involves obtaining multiple annotator judgements on the same data\ninstances, which are then flattened to produce a single \"ground truth\" label or\nscore, through majority voting, averaging, or adjudication. While these\napproaches may be appropriate in certain annotation tasks, such aggregations\noverlook the socially constructed nature of human perceptions that annotations\nfor relatively more subjective tasks are meant to capture. In particular,\nsystematic disagreements between annotators owing to their socio-cultural\nbackgrounds and/or lived experiences are often obfuscated through such\naggregations. In this paper, we empirically demonstrate that label aggregation\nmay introduce representational biases of individual and group perspectives.\nBased on this finding, we propose a set of recommendations for increased\nutility and transparency of datasets for downstream use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davani_A/0/1/0/all/0/1\">Aida Mostafazadeh Davani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Mark D&#xed;az</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-aware Video Reading Comprehension for Temporal Language Grounding. (arXiv:2110.05717v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05717","description":"<p>Temporal language grounding in videos aims to localize the temporal span\nrelevant to the given query sentence. Previous methods treat it either as a\nboundary regression task or a span extraction task. This paper will formulate\ntemporal language grounding into video reading comprehension and propose a\nRelation-aware Network (RaNet) to address it. This framework aims to select a\nvideo moment choice from the predefined answer set with the aid of\ncoarse-and-fine choice-query interaction and choice-choice relation\nconstruction. A choice-query interactor is proposed to match the visual and\ntextual information simultaneously in sentence-moment and token-moment levels,\nleading to a coarse-and-fine cross-modal interaction. Moreover, a novel\nmulti-choice relation constructor is introduced by leveraging graph convolution\nto capture the dependencies among video moment choices for the best choice\nselection. Extensive experiments on ActivityNet-Captions, TACoS, and\nCharades-STA demonstrate the effectiveness of our solution. Codes will be\nreleased soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jialin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations. (arXiv:2110.05719v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05719","description":"<p>Majority voting and averaging are common approaches employed to resolve\nannotator disagreements and derive single ground truth labels from multiple\nannotations. However, annotators may systematically disagree with one another,\noften reflecting their individual biases and values, especially in the case of\nsubjective tasks such as detecting affect, aggression, and hate speech.\nAnnotator disagreements may capture important nuances in such tasks that are\noften ignored while aggregating annotations to a single ground truth. In order\nto address this, we investigate the efficacy of multi-annotator models. In\nparticular, our multi-task based approach treats predicting each annotators'\njudgements as separate subtasks, while sharing a common learned representation\nof the task. We show that this approach yields same or better performance than\naggregating labels in the data prior to training across seven different binary\nclassification tasks. Our approach also provides a way to estimate uncertainty\nin predictions, which we demonstrate better correlate with annotation\ndisagreements than traditional methods. Being able to model uncertainty is\nespecially useful in deployment scenarios where knowing when not to make a\nprediction is important.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davani_A/0/1/0/all/0/1\">Aida Mostafazadeh Davani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Mark D&#xed;az</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightSeq: Accelerated Training for Transformer-based Models on GPUs. (arXiv:2110.05722v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05722","description":"<p>Transformer-based models have proven to be powerful in many natural language,\ncomputer vision, and speech recognition applications. It is expensive to train\nthese types of models due to unfixed input length, complex computation, and\nlarge numbers of parameters. Existing systems either only focus on efficient\ninference or optimize only BERT-like encoder models. In this paper, we present\nLightSeq, a system for efficient training of Transformer-based models on GPUs.\nWe propose a series of GPU optimization techniques tailored to computation flow\nand memory access patterns of neural layers in Transformers. LightSeq supports\na variety of network architectures, including BERT (encoder-only), GPT\n(decoder-only), and Transformer (encoder-decoder). Our experiments on GPUs with\nvarying models and datasets show that LightSeq is 1.4-3.5x faster than previous\nsystems. In particular, it gains 308% training speedup compared with existing\nsystems on a large public machine translation benchmark (WMT14 English-German).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Ying Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xian Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of Political Leanings of Chinese Speaking Twitter Users. (arXiv:2110.05723v1 [cs.CY])","link":"http://arxiv.org/abs/2110.05723","description":"<p>This work presents a supervised method for generating a classifier model of\nthe stances held by Chinese-speaking politicians and other Twitter users. Many\nprevious works of political tweets prediction exist on English tweets, but to\nthe best of our knowledge, this is the first work that builds prediction model\non Chinese political tweets. It firstly collects data by scraping tweets of\nfamous political figure and their related users. It secondly defines the\npolitical spectrum in two groups: the group that shows approvals to the Chinese\nCommunist Party and the group that does not. Since there are not space between\nwords in Chinese to identify the independent words, it then completes\nsegmentation and vectorization by Jieba, a Chinese segmentation tool. Finally,\nit trains the data collected from political tweets and produce a classification\nmodel with high accuracy for understanding users' political stances from their\ntweets on Twitter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_F/0/1/0/all/0/1\">Fenglei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Duoji Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anatomy of OntoGUM--Adapting GUM to the OntoNotes Scheme to Evaluate Robustness of SOTA Coreference Algorithms. (arXiv:2110.05727v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05727","description":"<p>SOTA coreference resolution produces increasingly impressive scores on the\nOntoNotes benchmark. However lack of comparable data following the same scheme\nfor more genres makes it difficult to evaluate generalizability to open domain\ndata. Zhu et al. (2021) introduced the creation of the OntoGUM corpus for\nevaluating geralizability of the latest neural LM-based end-to-end systems.\nThis paper covers details of the mapping process which is a set of\ndeterministic rules applied to the rich syntactic and discourse annotations\nmanually annotated in the GUM corpus. Out-of-domain evaluation across 12 genres\nshows nearly 15-20% degradation for both deterministic and deep learning\nsystems, indicating a lack of generalizability or covert overfitting in\nexisting coreference resolution models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yilun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_S/0/1/0/all/0/1\">Sameer Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VarArray: Array-Geometry-Agnostic Continuous Speech Separation. (arXiv:2110.05745v1 [eess.AS])","link":"http://arxiv.org/abs/2110.05745","description":"<p>Continuous speech separation using a microphone array was shown to be\npromising in dealing with the speech overlap problem in natural conversation\ntranscription. This paper proposes VarArray, an array-geometry-agnostic speech\nseparation neural network model. The proposed model is applicable to any number\nof microphones without retraining while leveraging the nonlinear correlation\nbetween the input channels. The proposed method adapts different elements that\nwere proposed before separately, including transform-average-concatenate,\nconformer speech separation, and inter-channel phase differences, and combines\nthem in an efficient and cohesive way. Large-scale evaluation was performed\nwith two real meeting transcription tasks by using a fully developed\ntranscription system requiring no prior knowledge such as reference\nsegmentations, which allowed us to measure the impact that the continuous\nspeech separation system could have in realistic settings. The proposed model\noutperformed a previous approach to array-geometry-agnostic modeling for all of\nthe geometry configurations considered, achieving asclite-based\nspeaker-agnostic word error rates of 17.5% and 20.4% for the AMI development\nand evaluation sets, respectively, in the end-to-end setting using no\nground-truth segmentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Dongmei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_M/0/1/0/all/0/1\">Min Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1\">Zirun Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEPP: Similarity Estimation of Predicted Probabilities for Defending and Detecting Adversarial Text. (arXiv:2110.05748v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05748","description":"<p>There are two cases describing how a classifier processes input text, namely,\nmisclassification and correct classification. In terms of misclassified texts,\na classifier handles the texts with both incorrect predictions and adversarial\ntexts, which are generated to fool the classifier, which is called a victim.\nBoth types are misunderstood by the victim, but they can still be recognized by\nother classifiers. This induces large gaps in predicted probabilities between\nthe victim and the other classifiers. In contrast, text correctly classified by\nthe victim is often successfully predicted by the others and induces small\ngaps. In this paper, we propose an ensemble model based on similarity\nestimation of predicted probabilities (SEPP) to exploit the large gaps in the\nmisclassified predictions in contrast to small gaps in the correct\nclassification. SEPP then corrects the incorrect predictions of the\nmisclassified texts. We demonstrate the resilience of SEPP in defending and\ndetecting adversarial texts through different types of victim classifiers,\nclassification tasks, and adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Son_H/0/1/0/all/0/1\">Hoang-Quoc Nguyen-Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidano_S/0/1/0/all/0/1\">Seira Hidano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukushima_K/0/1/0/all/0/1\">Kazuhide Fukushima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyomoto_S/0/1/0/all/0/1\">Shinsaku Kiyomoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SportsSum2.0: Generating High-Quality Sports News from Live Text Commentary. (arXiv:2110.05750v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05750","description":"<p>Sports game summarization aims to generate news articles from live text\ncommentaries. A recent state-of-the-art work, SportsSum, not only constructs a\nlarge benchmark dataset, but also proposes a two-step framework. Despite its\ngreat contributions, the work has three main drawbacks: 1) the noise existed in\nSportsSum dataset degrades the summarization performance; 2) the neglect of\nlexical overlap between news and commentaries results in low-quality\npseudo-labeling algorithm; 3) the usage of directly concatenating rewritten\nsentences to form news limits its practicability. In this paper, we publish a\nnew benchmark dataset SportsSum2.0, together with a modified summarization\nframework. In particular, to obtain a clean dataset, we employ crowd workers to\nmanually clean the original dataset. Moreover, the degree of lexical overlap is\nincorporated into the generation of pseudo labels. Further, we introduce a\nreranker-enhanced summarizer to take into account the fluency and\nexpressiveness of the summarized news. Extensive experiments show that our\nmodel outperforms the state-of-the-art baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianfeng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingsheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guoping Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniSpeech-SAT: Universal Speech Representation Learning with Speaker Aware Pre-Training. (arXiv:2110.05752v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05752","description":"<p>Self-supervised learning (SSL) is a long-standing goal for speech processing,\nsince it utilizes large-scale unlabeled data and avoids extensive human\nlabeling. Recent years witness great successes in applying self-supervised\nlearning in speech recognition, while limited exploration was attempted in\napplying SSL for modeling speaker characteristics. In this paper, we aim to\nimprove the existing SSL framework for speaker representation learning. Two\nmethods are introduced for enhancing the unsupervised speaker information\nextraction. First, we apply the multi-task learning to the current SSL\nframework, where we integrate the utterance-wise contrastive loss with the SSL\nobjective function. Second, for better speaker discrimination, we propose an\nutterance mixing strategy for data augmentation, where additional overlapped\nutterances are created unsupervisely and incorporate during training. We\nintegrate the proposed methods into the HuBERT framework. Experiment results on\nSUPERB benchmark show that the proposed system achieves state-of-the-art\nperformance in universal representation learning, especially for speaker\nidentification oriented tasks. An ablation study is performed verifying the\nefficacy of each proposed method. Finally, we scale up training dataset to 94\nthousand hours public audio data and achieve further performance improvement in\nall SUPERB tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiangzhan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Cognitive Factors in Lexical Decline. (arXiv:2110.05775v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05775","description":"<p>We adopt an evolutionary view on language change in which cognitive factors\n(in addition to social ones) affect the fitness of words and their success in\nthe linguistic ecosystem. Specifically, we propose a variety of\npsycholinguistic factors -- semantic, distributional, and phonological -- that\nwe hypothesize are predictive of lexical decline, in which words greatly\ndecrease in frequency over time. Using historical data across three languages\n(English, French, and German), we find that most of our proposed factors show a\nsignificant difference in the expected direction between each curated set of\ndeclining words and their matched stable words. Moreover, logistic regression\nanalyses show that semantic and distributional factors are significant in\npredicting declining words. Further diachronic analysis reveals that declining\nwords tend to decrease in the diversity of their lexical contexts over time,\ngradually narrowing their 'ecological niches'.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_D/0/1/0/all/0/1\">David Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabinovich_E/0/1/0/all/0/1\">Ella Rabinovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samir_F/0/1/0/all/0/1\">Farhan Samir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David Mortensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_S/0/1/0/all/0/1\">Suzanne Stevenson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"We've had this conversation before: A Novel Approach to Measuring Dialog Similarity. (arXiv:2110.05780v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05780","description":"<p>Dialog is a core building block of human natural language interactions. It\ncontains multi-party utterances used to convey information from one party to\nanother in a dynamic and evolving manner. The ability to compare dialogs is\nbeneficial in many real world use cases, such as conversation analytics for\ncontact center calls and virtual agent design.\n</p>\n<p>We propose a novel adaptation of the edit distance metric to the scenario of\ndialog similarity. Our approach takes into account various conversation aspects\nsuch as utterance semantics, conversation flow, and the participants. We\nevaluate this new approach and compare it to existing document similarity\nmeasures on two publicly available datasets. The results demonstrate that our\nmethod outperforms the other approaches in capturing dialog flow, and is better\naligned with the human perception of conversation similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lavi_O/0/1/0/all/0/1\">Ofer Lavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabinovich_E/0/1/0/all/0/1\">Ella Rabinovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlomov_S/0/1/0/all/0/1\">Segev Shlomov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boaz_D/0/1/0/all/0/1\">David Boaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronen_I/0/1/0/all/0/1\">Inbal Ronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anaby_Tavor_A/0/1/0/all/0/1\">Ateret Anaby-Tavor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTraffic: A Robust BERT-Based Approach for Speaker Change Detection and Role Identification of Air-Traffic Communications. (arXiv:2110.05781v1 [eess.AS])","link":"http://arxiv.org/abs/2110.05781","description":"<p>Automatic Speech Recognition (ASR) is gaining special interest in Air Traffic\nControl (ATC). ASR allows transcribing the communications between air traffic\ncontrollers (ATCOs) and pilots. These transcriptions are used to extract ATC\ncommand types and named entities such as aircraft callsigns. One common problem\nis when the Speech Activity Detection (SAD) or diarization system fails and\nthen two or more single speaker segments are in the same recording,\njeopardizing the overall system's performance. We developed a system that\ncombines the segmentation of a SAD module with a BERT-based model that performs\nSpeaker Change Detection (SCD) and Speaker Role Identification (SRI) based on\nASR transcripts (i.e., diarization + SRI). This research demonstrates on a\nreal-life ATC test set that performing diarization directly on textual data\nsurpass acoustic level diarization. The proposed model reaches up to\n~0.90/~0.95 F1-score on ATCO/pilot for SRI on several test sets. The text-based\ndiarization system brings a 27% relative improvement on Diarization Error Rate\n(DER) compared to standard acoustic-based diarization. These results were on\nASR transcripts of a challenging ATC test set with an estimated ~13% word error\nrate, validating the approach's robustness even on noisy ASR transcripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Seyyed Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ohneiser_O/0/1/0/all/0/1\">Oliver Ohneiser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Helmke_H/0/1/0/all/0/1\">Hartmut Helmke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting TTS models For New Speakers using Transfer Learning. (arXiv:2110.05798v1 [cs.SD])","link":"http://arxiv.org/abs/2110.05798","description":"<p>Training neural text-to-speech (TTS) models for a new speaker typically\nrequires several hours of high quality speech data. Prior works on voice\ncloning attempt to address this challenge by adapting pre-trained multi-speaker\nTTS models for a new voice, using a few minutes of speech data of the new\nspeaker. However, publicly available large multi-speaker datasets are often\nnoisy, thereby resulting in TTS models that are not suitable for use in\nproducts. We address this challenge by proposing transfer-learning guidelines\nfor adapting high quality single-speaker TTS models for a new speaker, using\nonly a few minutes of speech data. We conduct an extensive study using\ndifferent amounts of data for a new speaker and evaluate the synthesized speech\nin terms of naturalness and voice/style similarity to the target speaker. We\nfind that fine-tuning a single-speaker TTS model on just 30 minutes of data,\ncan yield comparable performance to a model trained from scratch on more than\n27 hours of data for both male and female target speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neekhara_P/0/1/0/all/0/1\">Paarth Neekhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jason Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Average and Worst-case Accuracy in Multitask Learning. (arXiv:2110.05838v1 [cs.LG])","link":"http://arxiv.org/abs/2110.05838","description":"<p>When training and evaluating machine learning models on a large number of\ntasks, it is important to not only look at average task accuracy -- which may\nbe biased by easy or redundant tasks -- but also worst-case accuracy (i.e. the\nperformance on the task with the lowest accuracy). In this work, we show how to\nuse techniques from the distributionally robust optimization (DRO) literature\nto improve worst-case performance in multitask learning. We highlight several\nfailure cases of DRO when applied off-the-shelf and present an improved method,\nLookahead-DRO (L-DRO), which mitigates these issues. The core idea of L-DRO is\nto anticipate the interaction between tasks during training in order to choose\na dynamic re-weighting of the various task losses, which will (i) lead to\nminimal worst-case loss and (ii) train on as many tasks as possible. After\ndemonstrating the efficacy of L-DRO on a small controlled synthetic setting, we\nevaluate it on two realistic benchmarks: a multitask version of the CIFAR-100\nimage classification dataset and a large-scale multilingual language modeling\nexperiment. Our empirical results show that L-DRO achieves a better trade-off\nbetween average and worst-case accuracy with little computational overhead\ncompared to several strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michel_P/0/1/0/all/0/1\">Paul Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Abstractive Summarisation Models with Machine Translation in Deliberative Processes. (arXiv:2110.05847v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05847","description":"<p>We present work on summarising deliberative processes for non-English\nlanguages. Unlike commonly studied datasets, such as news articles, this\ndeliberation dataset reflects difficulties of combining multiple narratives,\nmostly of poor grammatical quality, in a single text. We report an extensive\nevaluation of a wide range of abstractive summarisation models in combination\nwith an off-the-shelf machine translation model. Texts are translated into\nEnglish, summarised, and translated back to the original language. We obtain\npromising results regarding the fluency, consistency and relevance of the\nsummaries produced. Our approach is easy to implement for many languages for\nproduction purposes by simply changing the translation model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arana_Catania_M/0/1/0/all/0/1\">M. Arana-Catania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1\">Maria Liakata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"text2sdg: An open-source solution to monitoring sustainable development goals from text. (arXiv:2110.05856v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05856","description":"<p>Monitoring progress on the United Nations Sustainable Development Goals\n(SDGs) is important for both academic and non-academic organizations. Existing\napproaches to monitoring SDGs have focused on specific data types, namely,\npublications listed in proprietary research databases. We present the text2sdg\nR package, a user-friendly, open-source package that detects SDGs in any kind\nof text data using several different query systems from any text source. The\ntext2sdg package thereby facilitates the monitoring of SDGs for a wide array of\ntext sources and provides a much-needed basis for validating and improving\nextant methods to detect SDGs from text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wulff_D/0/1/0/all/0/1\">Dirk U. Wulff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mata_R/0/1/0/all/0/1\">Rui Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_D/0/1/0/all/0/1\">Dominik S. Meier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetricGAN-U: Unsupervised speech enhancement/ dereverberation based only on noisy/ reverberated speech. (arXiv:2110.05866v1 [cs.SD])","link":"http://arxiv.org/abs/2110.05866","description":"<p>Most of the deep learning-based speech enhancement models are learned in a\nsupervised manner, which implies that pairs of noisy and clean speech are\nrequired during training. Consequently, several noisy speeches recorded in\ndaily life cannot be used to train the model. Although certain unsupervised\nlearning frameworks have also been proposed to solve the pair constraint, they\nstill require clean speech or noise for training. Therefore, in this paper, we\npropose MetricGAN-U, which stands for MetricGAN-unsupervised, to further\nrelease the constraint from conventional unsupervised learning. In MetricGAN-U,\nonly noisy speech is required to train the model by optimizing non-intrusive\nspeech quality metrics. The experimental results verified that MetricGAN-U\noutperforms baselines in both objective and subjective metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Szu-Wei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_K/0/1/0/all/0/1\">Kuo-Hsuan Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravanelli_M/0/1/0/all/0/1\">Mirco Ravanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenHands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages. (arXiv:2110.05877v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05877","description":"<p>AI technologies for Natural Languages have made tremendous progress recently.\nHowever, commensurate progress has not been made on Sign Languages, in\nparticular, in recognizing signs as individual words or as complete sentences.\nWe introduce OpenHands, a library where we take four key ideas from the NLP\ncommunity for low-resource languages and apply them to sign languages for\nword-level recognition. First, we propose using pose extracted through\npretrained models as the standard modality of data to reduce training time and\nenable efficient inference, and we release standardized pose datasets for 6\ndifferent sign languages - American, Argentinian, Chinese, Greek, Indian, and\nTurkish. Second, we train and release checkpoints of 4 pose-based isolated sign\nlanguage recognition models across all 6 languages, providing baselines and\nready checkpoints for deployment. Third, to address the lack of labelled data,\nwe propose self-supervised pretraining on unlabelled data. We curate and\nrelease the largest pose-based pretraining dataset on Indian Sign Language\n(Indian-SL). Fourth, we compare different pretraining strategies and for the\nfirst time establish that pretraining is effective for sign language\nrecognition by demonstrating (a) improved fine-tuning performance especially in\nlow-resource settings, and (b) high crosslingual transfer from Indian-SL to few\nother sign languages. We open-source all models and datasets in OpenHands with\na hope that it makes research in sign languages more accessible, available here\nat https://github.com/AI4Bharat/OpenHands .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Selvaraj_P/0/1/0/all/0/1\">Prem Selvaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+NC_G/0/1/0/all/0/1\">Gokul NC</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigation on Data Adaptation Techniques for Neural Named Entity Recognition. (arXiv:2110.05892v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05892","description":"<p>Data processing is an important step in various natural language processing\ntasks. As the commonly used datasets in named entity recognition contain only a\nlimited number of samples, it is important to obtain additional labeled data in\nan efficient and reliable manner. A common practice is to utilize large\nmonolingual unlabeled corpora. Another popular technique is to create synthetic\ndata from the original labeled data (data augmentation). In this work, we\ninvestigate the impact of these two methods on the performance of three\ndifferent named entity recognition tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tokarchuk_E/0/1/0/all/0/1\">Evgeniia Tokarchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thulke_D/0/1/0/all/0/1\">David Thulke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dugast_C/0/1/0/all/0/1\">Christian Dugast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaoPLM: Pre-trained Language Models for Lao. (arXiv:2110.05896v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05896","description":"<p>Trained on the large corpus, pre-trained language models (PLMs) can capture\ndifferent levels of concepts in context and hence generate universal language\nrepresentations. They can benefit multiple downstream natural language\nprocessing (NLP) tasks. Although PTMs have been widely used in most NLP\napplications, especially for high-resource languages such as English, it is\nunder-represented in Lao NLP research. Previous work on Lao has been hampered\nby the lack of annotated datasets and the sparsity of language resources. In\nthis work, we construct a text classification dataset to alleviate the\nresource-scare situation of the Lao language. We additionally present the first\ntransformer-based PTMs for Lao with four versions: BERT-small, BERT-base,\nELECTRA-small and ELECTRA-base, and evaluate it over two downstream tasks:\npart-of-speech tagging and text classification. Experiments demonstrate the\neffectiveness of our Lao models. We will release our models and datasets to the\ncommunity, hoping to facilitate the future development of Lao NLP applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Order Does Not Matter For Speech Recognition. (arXiv:2110.05994v1 [eess.AS])","link":"http://arxiv.org/abs/2110.05994","description":"<p>In this paper, we study training of automatic speech recognition system in a\nweakly supervised setting where the order of words in transcript labels of the\naudio training data is not known. We train a word-level acoustic model which\naggregates the distribution of all output frames using LogSumExp operation and\nuses a cross-entropy loss to match with the ground-truth words distribution.\nUsing the pseudo-labels generated from this model on the training set, we then\ntrain a letter-based acoustic model using Connectionist Temporal Classification\nloss. Our system achieves 2.4%/5.3% on test-clean/test-other subsets of\nLibriSpeech, which is competitive with the supervised baseline's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pratap_V/0/1/0/all/0/1\">Vineel Pratap</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer. (arXiv:2110.05999v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05999","description":"<p>Despite the recent advances in applying pre-trained language models to\ngenerate high-quality texts, generating long passages that maintain long-range\ncoherence is yet challenging for these models. In this paper, we propose\nDiscoDVT, a discourse-aware discrete variational Transformer to tackle the\nincoherence issue. DiscoDVT learns a discrete variable sequence that summarizes\nthe global structure of the text and then applies it to guide the generation\nprocess at each decoding step. To further embed discourse-aware information\ninto the discrete latent representations, we introduce an auxiliary objective\nto model the discourse relations within the text. We conduct extensive\nexperiments on two open story generation datasets and demonstrate that the\nlatent codes learn meaningful correspondence to the discourse structures that\nguide the model to generate long texts with better long-range coherence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Haozhe Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Model Supervised by Understanding Map. (arXiv:2110.06043v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06043","description":"<p>Inspired by the notion of Center of Mass in physics, an extension called\nSemantic Center of Mass (SCOM) is proposed, and used to discover the abstract\n\"topic\" of a document. The notion is under a framework model called\nUnderstanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM\nis to let both the document content and a semantic network -- specifically,\nUnderstanding Map -- play a role, in interpreting the meaning of a document.\nBased on different justifications, three possible methods are devised to\ndiscover the SCOM of a document. Some experiments on artificial documents and\nUnderstanding Maps are conducted to test their outcomes. In addition, its\nability of vectorization of documents and capturing sequential information are\ntested. We also compared UM-S-TM with probabilistic topic models like Latent\nDirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gangli Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects. (arXiv:2110.06078v1 [q-bio.NC])","link":"http://arxiv.org/abs/2110.06078","description":"<p>A popular approach to decompose the neural bases of language consists in\ncorrelating, across individuals, the brain responses to different stimuli (e.g.\nregular speech versus scrambled words, sentences, or paragraphs). Although\nsuccessful, this `model-free' approach necessitates the acquisition of a large\nand costly set of neuroimaging data. Here, we show that a model-based approach\ncan reach equivalent results within subjects exposed to natural stimuli. We\ncapitalize on the recently-discovered similarities between deep language models\nand the human brain to compute the mapping between i) the brain responses to\nregular speech and ii) the activations of deep language models elicited by\nmodified stimuli (e.g. scrambled words, sentences, or paragraphs). Our\nmodel-based approach successfully replicates the seminal study of Lerner et al.\n(2011), which revealed the hierarchy of language areas by comparing the\nfunctional-magnetic resonance imaging (fMRI) of seven subjects listening to\n7min of both regular and scrambled narratives. We further extend and precise\nthese results to the brain signals of 305 individuals listening to 4.1 hours of\nnarrated stories. Overall, this study paves the way for efficient and flexible\nanalyses of the brain bases of language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Caucheteux_C/0/1/0/all/0/1\">Charlotte Caucheteux</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gramfort_A/0/1/0/all/0/1\">Alexandre Gramfort</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+King_J/0/1/0/all/0/1\">Jean-R&#xe9;mi King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A large scale lexical and semantic analysis of Spanish language variations in Twitter. (arXiv:2110.06128v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06128","description":"<p>Dialectometry is a discipline devoted to studying the variations of a\nlanguage around a geographical region. One of their goals is the creation of\nlinguistic atlases capturing the similarities and differences of the language\nunder study around the area in question. For instance, Spanish is one of the\nmost spoken languages across the world, but not necessarily Spanish is written\nand spoken in the same way in different countries. This manuscript presents a\nbroad analysis describing lexical and semantic relationships among 26\nSpanish-speaking countries around the globe. For this study, we analyze\nfour-year of the Twitter geotagged public stream to provide an extensive survey\nof the Spanish language vocabularies of different countries, its distributions,\nsemantic usage of terms, and emojis. We also offer open regional word-embedding\nresources for Spanish Twitter to help other researchers and practitioners take\nadvantage of regionalized models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tellez_E/0/1/0/all/0/1\">Eric S. Tellez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moctezuma_D/0/1/0/all/0/1\">Daniela Moctezuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miranda_S/0/1/0/all/0/1\">Sabino Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graff_M/0/1/0/all/0/1\">Mario Graff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Feelings of People Regarding COVID-19 by Social Network Mining. (arXiv:2110.06151v1 [cs.SI])","link":"http://arxiv.org/abs/2110.06151","description":"<p>In 2020, COVID-19 became the chief concern of the world and is still\nreflected widely in all social networks. Each day, users post millions of\ntweets and comments on this subject, which contain significant implicit\ninformation about the public opinion. In this regard, a dataset of\nCOVID-related tweets in English language is collected, which consists of more\nthan two million tweets from March 23 to June 23 of 2020 to extract the\nfeelings of the people in various countries in the early stages of this\noutbreak. To this end, first, we use a lexicon-based approach in conjunction\nwith the GeoNames geographic database to label the tweets with their locations.\nNext, a method based on the recently introduced and widely cited RoBERTa model\nis proposed to analyze their sentimental content. After that, the trend graphs\nof the frequency of tweets as well as sentiments are produced for the world and\nthe nations that were more engaged with COVID-19. Graph analysis shows that the\nfrequency graphs of the tweets for the majority of nations are significantly\ncorrelated with the official statistics of the daily afflicted in them.\nMoreover, several implicit knowledge is extracted and discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_Nejad_H/0/1/0/all/0/1\">Hamed Vahdat-Nejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salmani_F/0/1/0/all/0/1\">Fatemeh Salmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiabadi_M/0/1/0/all/0/1\">Mahdi Hajiabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizi_F/0/1/0/all/0/1\">Faezeh Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_S/0/1/0/all/0/1\">Sajedeh Abbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamalian_M/0/1/0/all/0/1\">Mohadese Jamalian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosafer_R/0/1/0/all/0/1\">Reyhane Mosafer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiabadi_H/0/1/0/all/0/1\">Hamideh Hajiabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mention Memory: incorporating textual knowledge into Transformers through entity mention attention. (arXiv:2110.06176v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06176","description":"<p>Natural language understanding tasks such as open-domain question answering\noften require retrieving and assimilating factual information from multiple\nsources. We propose to address this problem by integrating a semi-parametric\nrepresentation of a large text corpus into a Transformer model as a source of\nfactual knowledge. Specifically, our method represents knowledge with `mention\nmemory', a table of dense vector representations of every entity mention in a\ncorpus. The proposed model - TOME - is a Transformer that accesses the\ninformation through internal memory layers in which each entity mention in the\ninput passage attends to the mention memory. This approach enables synthesis of\nand reasoning over many disparate sources of information within a single\nTransformer model. In experiments using a memory of 150 million Wikipedia\nmentions, TOME achieves strong performance on several open-domain\nknowledge-intensive tasks, including the claim verification benchmarks HoVer\nand FEVER and several entity-based QA benchmarks. We also show that the model\nlearns to attend to informative mentions without any direct supervision.\nFinally we demonstrate that the model can generalize to new unseen entities by\nupdating the memory without retraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1\">Michiel de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemlyanskiy_Y/0/1/0/all/0/1\">Yury Zemlyanskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+FitzGerald_N/0/1/0/all/0/1\">Nicholas FitzGerald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Objectives of Extractive Question Answering. (arXiv:2008.12804v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.12804","description":"<p>This work demonstrates that using the objective with independence assumption\nfor modelling the span probability $P(a_s,a_e) = P(a_s)P(a_e)$ of span starting\nat position $a_s$ and ending at position $a_e$ has adverse effects. Therefore\nwe propose multiple approaches to modelling joint probability $P(a_s,a_e)$\ndirectly. Among those, we propose a compound objective, composed from the joint\nprobability while still keeping the objective with independence assumption as\nan auxiliary objective. We find that the compound objective is consistently\nsuperior or equal to other assumptions in exact match. Additionally, we\nidentified common errors caused by the assumption of independence and manually\nchecked the counterpart predictions, demonstrating the impact of the compound\nobjective on the real examples. Our findings are supported via experiments with\nthree extractive QA models (BIDAF, BERT, ALBERT) over six datasets and our\ncode, individual results and manual analysis are available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1\">Josef Jon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimal Supervision for Morphological Inflection. (arXiv:2104.08512v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08512","description":"<p>Neural models for the various flavours of morphological inflection tasks have\nproven to be extremely accurate given ample labeled data -- data that may be\nslow and costly to obtain. In this work we aim to overcome this annotation\nbottleneck by bootstrapping labeled data from a seed as little as {\\em five}\nlabeled paradigms, accompanied by a large bulk of unlabeled text. Our approach\nexploits different kinds of regularities in morphological systems in a\ntwo-phased setup, where word tagging based on {\\em analogies} is followed by\nword pairing based on {\\em distances}. We experiment with the Paradigm Cell\nFilling Problem over eight typologically different languages, and find that, in\nlanguages with relatively simple morphology, orthographic regularities on their\nown allow inflection models to achieve respectable accuracy. Combined\northographic and semantic regularities alleviate difficulties with particularly\ncomplex morpho-phonological systems. Our results suggest that hand-crafting\nmany tagged examples might be an unnecessary effort. However, more work is\nneeded in order to address rarely used forms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding. (arXiv:2104.12763v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12763","description":"<p>Multi-modal reasoning systems rely on a pre-trained object detector to\nextract regions of interest from the image. However, this crucial module is\ntypically used as a black box, trained independently of the downstream task and\non a fixed vocabulary of objects and attributes. This makes it challenging for\nsuch systems to capture the long tail of visual concepts expressed in free form\ntext. In this paper we propose MDETR, an end-to-end modulated detector that\ndetects objects in an image conditioned on a raw text query, like a caption or\na question. We use a transformer-based architecture to reason jointly over text\nand image by fusing the two modalities at an early stage of the model. We\npre-train the network on 1.3M text-image pairs, mined from pre-existing\nmulti-modal datasets having explicit alignment between phrases in text and\nobjects in the image. We then fine-tune on several downstream tasks such as\nphrase grounding, referring expression comprehension and segmentation,\nachieving state-of-the-art results on popular benchmarks. We also investigate\nthe utility of our model as an object detector on a given label set when\nfine-tuned in a few-shot setting. We show that our pre-training approach\nprovides a way to handle the long tail of object categories which have very few\nlabelled instances. Our approach can be easily extended for visual question\nanswering, achieving competitive performance on GQA and CLEVR. The code and\nmodels are available at https://github.com/ashkamath/mdetr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Aishwarya Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mannat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carion_N/0/1/0/all/0/1\">Nicolas Carion</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prosodic segmentation for parsing spoken dialogue. (arXiv:2105.12667v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.12667","description":"<p>Parsing spoken dialogue poses unique difficulties, including disfluencies and\nunmarked boundaries between sentence-like units. Previous work has shown that\nprosody can help with parsing disfluent speech (Tran et al. 2018), but has\nassumed that the input to the parser is already segmented into sentence-like\nunits (SUs), which isn't true in existing speech applications. We investigate\nhow prosody affects a parser that receives an entire dialogue turn as input (a\nturn-based model), instead of gold standard pre-segmented SUs (an SU-based\nmodel). In experiments on the English Switchboard corpus, we find that when\nusing transcripts alone, the turn-based model has trouble segmenting SUs,\nleading to worse parse performance than the SU-based model. However, prosody\ncan effectively replace gold standard SU boundaries: with prosody, the\nturn-based model performs as well as the SU-based model (90.79 vs. 90.65 F1\nscore, respectively), despite performing two tasks (SU segmentation and\nparsing) rather than one (parsing alone). Analysis shows that pitch and\nintensity features are the most important for this corpus, since they allow the\nmodel to correctly distinguish an SU boundary from a speech disfluency -- a\ndistinction that the model otherwise struggles to make.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_E/0/1/0/all/0/1\">Elizabeth Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1\">Sharon Goldwater</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information. (arXiv:2106.05707v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.05707","description":"<p>Fact verification has attracted a lot of attention in the machine learning\nand natural language processing communities, as it is one of the key methods\nfor detecting misinformation. Existing large-scale benchmarks for this task\nhave focused mostly on textual sources, i.e. unstructured information, and thus\nignored the wealth of information available in structured formats, such as\ntables. In this paper we introduce a novel dataset and benchmark, Fact\nExtraction and VERification Over Unstructured and Structured information\n(FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated\nwith evidence in the form of sentences and/or cells from tables in Wikipedia,\nas well as a label indicating whether this evidence supports, refutes, or does\nnot provide enough information to reach a verdict. Furthermore, we detail our\nefforts to track and minimize the biases present in the dataset and could be\nexploited by models, e.g. being able to predict the label without using\nevidence. Finally, we develop a baseline for verifying claims against text and\ntables which predicts both the correct evidence and verdict for 18% of the\nclaims.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aly_R/0/1/0/all/0/1\">Rami Aly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christodoulopoulos_C/0/1/0/all/0/1\">Christos Christodoulopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cocarascu_O/0/1/0/all/0/1\">Oana Cocarascu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Arpit Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding. (arXiv:2106.07250v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07250","description":"<p>In knowledge graph embedding, the theoretical relationship between the\nsoftmax cross-entropy and negative sampling loss functions has not been\ninvestigated. This makes it difficult to fairly compare the results of the two\ndifferent loss functions. We attempted to solve this problem by using the\nBregman divergence to provide a unified interpretation of the softmax\ncross-entropy and negative sampling loss functions. Under this interpretation,\nwe can derive theoretical findings for fair comparison. Experimental results on\nthe FB15k-237 and WN18RR datasets show that the theoretical findings are valid\nin practical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1\">Hidetaka Kamigaito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1\">Katsuhiko Hayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07306","description":"<p>A major challenge in structured prediction is to represent the\ninterdependencies within output structures. When outputs are structured as\nsequences, linear-chain conditional random fields (CRFs) are a widely used\nmodel class which can learn \\textit{local} dependencies in the output. However,\nthe CRF's Markov assumption makes it impossible for CRFs to represent\ndistributions with \\textit{nonlocal} dependencies, and standard CRFs are unable\nto respect nonlocal constraints of the data (such as global arity constraints\non output labels). We present a generalization of CRFs that can enforce a broad\nclass of constraints, including nonlocal ones, by specifying the space of\npossible output structures as a regular language $\\mathcal{L}$. The resulting\nregular-constrained CRF (RegCCRF) has the same formal properties as a standard\nCRF, but assigns zero probability to all label sequences not in $\\mathcal{L}$.\nNotably, RegCCRFs can incorporate their constraints during training, while\nrelated models only enforce constraints during decoding. We prove that\nconstrained training is never worse than constrained decoding, and show\nempirically that it can be substantially better in practice. Additionally, we\ndemonstrate a practical benefit on downstream tasks by incorporating a RegCCRF\ninto a deep neural model for semantic role labeling, exceeding state-of-the-art\nresults on a standard dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papay_S/0/1/0/all/0/1\">Sean Papay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01951","description":"<p>The task of learning from only a few examples (called a few-shot setting) is\nof key importance and relevance to a real-world setting. For question answering\n(QA), the current state-of-the-art pre-trained models typically need\nfine-tuning on tens of thousands of examples to obtain good results. Their\nperformance degrades significantly in a few-shot setting (&lt; 100 examples). To\naddress this, we propose a simple fine-tuning framework that leverages\npre-trained text-to-text models and is directly aligned with their pre-training\nframework. Specifically, we construct the input as a concatenation of the\nquestion, a mask token representing the answer span and a context. Given this\ninput, the model is fine-tuned using the same objective as that of its\npre-training objective. Through experimental studies on various few-shot\nconfigurations, we show that this formulation leads to significant gains on\nmultiple QA benchmarks (an absolute gain of 34.2 F1 points on average when\nthere are only 16 training examples). The gains extend further when used with\nlarger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)\nand translate well to a multilingual setting . On the multilingual TydiQA\nbenchmark, our model outperforms the XLM-Roberta-large by an absolute margin of\nupto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;= 64\ntraining examples). We conduct detailed ablation studies to analyze factors\ncontributing to these gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Voice Activated Framework using Self-supervised Learning. (arXiv:2110.01077v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.01077","description":"<p>Self-supervised learning methods such as wav2vec 2.0 have shown promising\nresults in learning speech representations from unlabelled and untranscribed\nspeech data that are useful for speech recognition. Since these representations\nare learned without any task-specific supervision, they can also be useful for\nother voice-activated tasks like speaker verification, keyword spotting,\nemotion classification etc. In our work, we propose a general purpose framework\nfor adapting a pre-trained wav2vec 2.0 model for different voice-activated\ntasks. We develop downstream network architectures that operate on the\ncontextualized speech representations of wav2vec 2.0 to adapt the\nrepresentations for solving a given task. Finally, we extend our framework to\nperform multi-task learning by jointly optimizing the network parameters on\nmultiple voice activated tasks using a shared transformer backbone. Both of our\nsingle and multi-task frameworks achieve state-of-the-art results in speaker\nverification and keyword spotting benchmarks. Our best performing models\nachieve 1.98% and 3.15% EER on VoxCeleb1 test set when trained on VoxCeleb2 and\nVoxCeleb1 respectively, and 98.23% accuracy on Google Speech Commands v1.0\nkeyword spotting dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hussain_S/0/1/0/all/0/1\">Shehzeen Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shuhua Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Visser_E/0/1/0/all/0/1\">Erik Visser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Lexical Normalization with Multilingual Transformers. (arXiv:2110.02869v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02869","description":"<p>Current benchmark tasks for natural language processing contain text that is\nqualitatively different from the text used in informal day to day digital\ncommunication. This discrepancy has led to severe performance degradation of\nstate-of-the-art NLP models when fine-tuned on real-world data. One way to\nresolve this issue is through lexical normalization, which is the process of\ntransforming non-standard text, usually from social media, into a more\nstandardized form. In this work, we propose a sentence-level\nsequence-to-sequence model based on mBART, which frames the problem as a\nmachine translation problem. As the noisy text is a pervasive problem across\nlanguages, not just English, we leverage the multi-lingual pre-training of\nmBART to fine-tune it to our data. While current approaches mainly operate at\nthe word or subword level, we argue that this approach is straightforward from\na technical standpoint and builds upon existing pre-trained transformer\nnetworks. Our results show that while word-level, intrinsic, performance\nevaluation is behind other methods, our model improves performance on\nextrinsic, downstream tasks through normalization compared to models operating\non raw, unprocessed, social media text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition. (arXiv:2110.03370v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.03370","description":"<p>In this paper, we present WenetSpeech, a multi-domain Mandarin corpus\nconsisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly\nlabeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in\ntotal. We collect the data from YouTube and Podcast, which covers a variety of\nspeaking styles, scenarios, domains, topics, and noisy conditions. An optical\ncharacter recognition (OCR) based method is introduced to generate the\naudio/text segmentation candidates for the YouTube data on its corresponding\nvideo captions, while a high-quality ASR transcription system is used to\ngenerate audio/text pair candidates for the Podcast data. Then we propose a\nnovel end-to-end label error detection approach to further validate and filter\nthe candidates. We also provide three manually labelled high-quality test sets\nalong with WenetSpeech for evaluation -- Dev for cross-validation purpose in\ntraining, Test_Net, collected from Internet for matched test, and\nTest\\_Meeting, recorded from real meetings for more challenging mismatched\ntest. Baseline systems trained with WenetSpeech are provided for three popular\nspeech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition\nresults on the three test sets are also provided as benchmarks. To the best of\nour knowledge, WenetSpeech is the current largest open-sourced Mandarin speech\ncorpus with transcriptions, which benefits research on production-level speech\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Hang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1\">Qijie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_H/0/1/0/all/0/1\">Hui Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chenchen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taming Sparsely Activated Transformer with Stochastic Experts. (arXiv:2110.04260v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04260","description":"<p>Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can\neasily scale to have outrageously large amounts of parameters without\nsignificant increase in computational cost. However, SAMs are reported to be\nparameter inefficient such that larger models do not always lead to better\nperformance. While most on-going research focuses on improving SAMs models by\nexploring methods of routing inputs to experts, our analysis reveals that such\nresearch might not lead to the solution we expect, i.e., the commonly-used\nrouting methods based on gating mechanisms do not work better than randomly\nrouting inputs to experts. In this paper, we propose a new expert-based model,\nTHOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models,\nsuch as the Switch Transformer, experts in THOR are randomly activated for each\ninput during training and inference. THOR models are trained using a\nconsistency regularized loss, where experts learn not only from training data\nbut also from other experts as teachers, such that all the experts make\nconsistent predictions. We validate the effectiveness of THOR on machine\ntranslation tasks. Results show that THOR models are more parameter efficient\nin that they significantly outperform the Transformer and MoE models across\nvarious settings. For example, in multilingual translation, THOR outperforms\nthe Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as\nthat of a state-of-the-art MoE model that is 18 times larger. Our code is\npublicly available at:\nhttps://github.com/microsoft/Stochastic-Mixture-of-Experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_H/0/1/0/all/0/1\">Hany Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning. (arXiv:2110.04725v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04725","description":"<p>Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot\nand Few-Shot learning on many natural language processing (NLP) tasks by\nscaling up model size, dataset size and the amount of computation. However,\ntraining a model like GPT-3 requires huge amount of computational resources\nwhich makes it challengeable to researchers. In this work, we propose a method\nthat incorporates large-scale distributed training performance into model\narchitecture design. With this method, Yuan 1.0, the current largest singleton\nlanguage model with 245B parameters, achieves excellent performance on\nthousands GPUs during training, and the state-of-the-art results on NLP tasks.\nA data processing method is designed to efficiently filter massive amount of\nraw data. The current largest high-quality Chinese corpus with 5TB high quality\ntexts is built based on this method. In addition, a calibration and label\nexpansion method is proposed to improve the Zero-Shot and Few-Shot performance,\nand steady improvement is observed on the accuracy of various tasks. Yuan 1.0\npresents strong capacity of natural language generation, and the generated\narticles are difficult to distinguish from the human-written ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shaohua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xudong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiangang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanwei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advances in Multi-turn Dialogue Comprehension: A Survey. (arXiv:2110.04984v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04984","description":"<p>Training machines to understand natural language and interact with humans is\nan elusive and essential task of artificial intelligence. A diversity of\ndialogue systems has been designed with the rapid development of deep learning\ntechniques, especially the recent pre-trained language models (PrLMs). Among\nthese studies, the fundamental yet challenging type of task is dialogue\ncomprehension whose role is to teach the machines to read and comprehend the\ndialogue context before responding. In this paper, we review the previous\nmethods from the technical perspective of dialogue modeling for the dialogue\ncomprehension task. We summarize the characteristics and challenges of dialogue\ncomprehension in contrast to plain-text reading comprehension. Then, we discuss\nthree typical patterns of dialogue modeling. In addition, we categorize\ndialogue-related pre-training techniques which are employed to enhance PrLMs in\ndialogue scenarios. Finally, we highlight the technical advances in recent\nyears and point out the lessons from the empirical analysis and the prospects\ntowards a new frontier of researches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-trained Language Models in Biomedical Domain: A Systematic Survey. (arXiv:2110.05006v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05006","description":"<p>Pre-trained language models (PLMs) have been the de facto paradigm for most\nnatural language processing (NLP) tasks. This also benefits biomedical domain:\nresearchers from informatics, medicine, and computer science (CS) communities\npropose various PLMs trained on biomedical datasets, e.g., biomedical text,\nelectronic health records, protein, and DNA sequences for various biomedical\ntasks. However, the cross-discipline characteristics of biomedical PLMs hinder\ntheir spreading among communities; some existing works are isolated from each\nother without comprehensive comparison and discussions. It expects a survey\nthat not only systematically reviews recent advances of biomedical PLMs and\ntheir applications but also standardizes terminology and benchmarks. In this\npaper, we summarize the recent progress of pre-trained language models in the\nbiomedical domain and their applications in biomedical downstream tasks.\nParticularly, we discuss the motivations and propose a taxonomy of existing\nbiomedical PLMs. Their applications in biomedical downstream tasks are\nexhaustively discussed. At last, we illustrate various limitations and future\ntrends, which we hope can provide inspiration for the future research of the\nresearch community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiahuan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1\">Prayag Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+fu_J/0/1/0/all/0/1\">Jie fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViSeRet: A simple yet effective approach to moment retrieval via fine-grained video segmentation. (arXiv:2110.05146v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05146","description":"<p>Video-text retrieval has many real-world applications such as media\nanalytics, surveillance, and robotics. This paper presents the 1st place\nsolution to the video retrieval track of the ICCV VALUE Challenge 2021. We\npresent a simple yet effective approach to jointly tackle two video-text\nretrieval tasks (video retrieval and video corpus moment retrieval) by\nleveraging the model trained only on the video retrieval task. In addition, we\ncreate an ensemble model that achieves the new state-of-the-art performance on\nall four datasets (TVr, How2r, YouCook2r, and VATEXr) presented in the VALUE\nChallenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Aiden Seungjoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Hanseok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"UnfairGAN: An Enhanced Generative Adversarial Network for Raindrop Removal from A Single Image. (arXiv:2110.05523v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05523","description":"<p>Image deraining is a new challenging problem in real-world applications, such\nas autonomous vehicles. In a bad weather condition of heavy rainfall,\nraindrops, mainly hitting glasses or windshields, can significantly reduce\nobservation ability. Moreover, raindrops spreading over the glass can yield\nrefraction's physical effect, which seriously impedes the sightline or\nundermine machine learning systems. In this paper, we propose an enhanced\ngenerative adversarial network to deal with the challenging problems of\nraindrops. UnfairGAN is an enhanced generative adversarial network that can\nutilize prior high-level information, such as edges and rain estimation, to\nboost deraining performance. To demonstrate UnfairGAN, we introduce a large\ndataset for training deep learning models of rain removal. The experimental\nresults show that our proposed method is superior to other state-of-the-art\napproaches of deraining raindrops regarding quantitative metrics and visual\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc Manh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development and testing of an image transformer for explainable autonomous driving systems. (arXiv:2110.05559v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05559","description":"<p>In the last decade, deep learning (DL) approaches have been used successfully\nin computer vision (CV) applications. However, DL-based CV models are generally\nconsidered to be black boxes due to their lack of interpretability. This black\nbox behavior has exacerbated user distrust and therefore has prevented\nwidespread deployment DLCV models in autonomous driving tasks even though some\nof these models exhibit superiority over human performance. For this reason, it\nis essential to develop explainable DL models for autonomous driving task.\nExplainable DL models can not only boost user trust in autonomy but also serve\nas a diagnostic approach to identify anydefects and weaknesses of the model\nduring the system development phase. In this paper, we propose an explainable\nend-to-end autonomous driving system based on \"Transformer\", a state-of-the-art\n(SOTA) self-attention based model, to map visual features from images collected\nby onboard cameras to guide potential driving actions with corresponding\nexplanations. The model achieves a soft attention over the global features of\nthe image. The results demonstrate the efficacy of our proposed model as it\nexhibits superior performance (in terms of correct prediction of actions and\nexplanations) compared to the benchmark model by a significant margin with\nlower computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sikai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1\">Shuya Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tiantian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miralinaghi_M/0/1/0/all/0/1\">Mohammad Miralinaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labi_S/0/1/0/all/0/1\">Samuel Labi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UrbanNet: Leveraging Urban Maps for Long Range 3D Object Detection. (arXiv:2110.05561v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05561","description":"<p>Relying on monocular image data for precise 3D object detection remains an\nopen problem, whose solution has broad implications for cost-sensitive\napplications such as traffic monitoring. We present UrbanNet, a modular\narchitecture for long range monocular 3D object detection with static cameras.\nOur proposed system combines commonly available urban maps along with a mature\n2D object detector and an efficient 3D object descriptor to accomplish accurate\ndetection at long range even when objects are rotated along any of their three\naxes. We evaluate UrbanNet on a novel challenging synthetic dataset and\nhighlight the advantages of its design for traffic detection in roads with\nchanging slope, where the flat ground approximation does not hold. Data and\ncode are available at https://github.com/TRAILab/UrbanNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carrillo_J/0/1/0/all/0/1\">Juan Carrillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1\">Steven Waslander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EchoVPR: Echo State Networks for Visual Place Recognition. (arXiv:2110.05572v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05572","description":"<p>Recognising previously visited locations is an important, but unsolved, task\nin autonomous navigation. Current visual place recognition (VPR) benchmarks\ntypically challenge models to recover the position of a query image (or images)\nfrom sequential datasets that include both spatial and temporal components.\nRecently, Echo State Network (ESN) varieties have proven particularly powerful\nat solving machine learning tasks that require spatio-temporal modelling. These\nnetworks are simple, yet powerful neural architectures that -- exhibiting\nmemory over multiple time-scales and non-linear high-dimensional\nrepresentations -- can discover temporal relations in the data while still\nmaintaining linearity in the learning. In this paper, we present a series of\nESNs and analyse their applicability to the VPR problem. We report that the\naddition of ESNs to pre-processed convolutional neural networks led to a\ndramatic boost in performance in comparison to non-recurrent networks in four\nstandard benchmarks (GardensPoint, SPEDTest, ESSEX3IN1, Nordland) demonstrating\nthat ESNs are able to capture the temporal structure inherent in VPR problems.\nMoreover, we show that ESNs can outperform class-leading VPR models which also\nexploit the sequential dynamics of the data. Finally, our results demonstrate\nthat ESNs also improve generalisation abilities, robustness, and accuracy\nfurther supporting their suitability to VPR applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozdemir_A/0/1/0/all/0/1\">Anil Ozdemir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_A/0/1/0/all/0/1\">Andrew B. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philippides_A/0/1/0/all/0/1\">Andrew Philippides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangan_M/0/1/0/all/0/1\">Michael Mangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilaki_E/0/1/0/all/0/1\">Eleni Vasilaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manneschi_L/0/1/0/all/0/1\">Luca Manneschi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Radiance Fields Approach to Deep Multi-View Photometric Stereo. (arXiv:2110.05594v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05594","description":"<p>We present a modern solution to the multi-view photometric stereo problem\n(MVPS). Our work suitably exploits the image formation model in a MVPS\nexperimental setup to recover the dense 3D reconstruction of an object from\nimages. We procure the surface orientation using a photometric stereo (PS)\nimage formation model and blend it with a multi-view neural radiance field\nrepresentation to recover the object's surface geometry. Contrary to the\nprevious multi-staged framework to MVPS, where the position, iso-depth\ncontours, or orientation measurements are estimated independently and then\nfused later, our method is simple to implement and realize. Our method performs\nneural rendering of multi-view images while utilizing surface normals estimated\nby a deep photometric stereo network. We render the MVPS images by considering\nthe object's surface normals for each 3D sample point along the viewing\ndirection rather than explicitly using the density gradient in the volume space\nvia 3D occupancy information. We optimize the proposed neural radiance field\nrepresentation for the MVPS setup efficiently using a fully connected deep\nnetwork to recover the 3D geometry of an object. Extensive evaluation on the\nDiLiGenT-MV benchmark dataset shows that our method performs better than the\napproaches that perform only PS or only multi-view stereo (MVS) and provides\ncomparable results against the state-of-the-art multi-stage fusion methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaya_B/0/1/0/all/0/1\">Berk Kaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarno_F/0/1/0/all/0/1\">Francesco Sarno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Architecture Search for Efficient Uncalibrated Deep Photometric Stereo. (arXiv:2110.05621v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05621","description":"<p>We present an automated machine learning approach for uncalibrated\nphotometric stereo (PS). Our work aims at discovering lightweight and\ncomputationally efficient PS neural networks with excellent surface normal\naccuracy. Unlike previous uncalibrated deep PS networks, which are handcrafted\nand carefully tuned, we leverage differentiable neural architecture search\n(NAS) strategy to find uncalibrated PS architecture automatically. We begin by\ndefining a discrete search space for a light calibration network and a normal\nestimation network, respectively. We then perform a continuous relaxation of\nthis search space and present a gradient-based optimization strategy to find an\nefficient light calibration and normal estimation network. Directly applying\nthe NAS methodology to uncalibrated PS is not straightforward as certain\ntask-specific constraints must be satisfied, which we impose explicitly.\nMoreover, we search for and train the two networks separately to account for\nthe Generalized Bas-Relief (GBR) ambiguity. Extensive experiments on the\nDiLiGenT dataset show that the automatically searched neural architectures\nperformance compares favorably with the state-of-the-art uncalibrated PS\nmethods while having a lower memory footprint.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarno_F/0/1/0/all/0/1\">Francesco Sarno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaya_B/0/1/0/all/0/1\">Berk Kaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameterizing Activation Functions for Adversarial Robustness. (arXiv:2110.05626v1 [cs.LG])","link":"http://arxiv.org/abs/2110.05626","description":"<p>Deep neural networks are known to be vulnerable to adversarially perturbed\ninputs. A commonly used defense is adversarial training, whose performance is\ninfluenced by model capacity. While previous works have studied the impact of\nvarying model width and depth on robustness, the impact of increasing capacity\nby using learnable parametric activation functions (PAFs) has not been studied.\nWe study how using learnable PAFs can improve robustness in conjunction with\nadversarial training. We first ask the question: how should we incorporate\nparameters into activation functions to improve robustness? To address this, we\nanalyze the direct impact of activation shape on robustness through PAFs and\nobserve that activation shapes with positive outputs on negative inputs and\nwith high finite curvature can increase robustness. We combine these properties\nto create a new PAF, which we call Parametric Shifted Sigmoidal Linear Unit\n(PSSiLU). We then combine PAFs (including PReLU, PSoftplus and PSSiLU) with\nadversarial training and analyze robust performance. We find that PAFs optimize\ntowards activation shape properties found to directly affect robustness.\nAdditionally, we find that while introducing only 1-2 learnable parameters into\nthe network, smooth PAFs can significantly increase robustness over ReLU. For\ninstance, when trained on CIFAR-10 with additional synthetic data, PSSiLU\nimproves robust accuracy by 4.54% over ReLU on ResNet-18 and 2.69% over ReLU on\nWRN-28-10 in the $\\ell_{\\infty}$ threat model while adding only 2 additional\nparameters into the network architecture. The PSSiLU WRN-28-10 model achieves\n61.96% AutoAttack accuracy, improving over the state-of-the-art robust accuracy\non RobustBench (Croce et al., 2020).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Sihui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1\">Saeed Mahloujifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Prateek Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Robust PCA: A Scalable Deep Unfolding Approach for High-Dimensional Outlier Detection. (arXiv:2110.05649v1 [cs.LG])","link":"http://arxiv.org/abs/2110.05649","description":"<p>Robust principal component analysis (RPCA) is a critical tool in modern\nmachine learning, which detects outliers in the task of low-rank matrix\nreconstruction. In this paper, we propose a scalable and learnable non-convex\napproach for high-dimensional RPCA problems, which we call Learned Robust PCA\n(LRPCA). LRPCA is highly efficient, and its free parameters can be effectively\nlearned to optimize via deep unfolding. Moreover, we extend deep unfolding from\nfinite iterations to infinite iterations via a novel\nfeedforward-recurrent-mixed neural network model. We establish the recovery\nguarantee of LRPCA under mild assumptions for RPCA. Numerical experiments show\nthat LRPCA outperforms the state-of-the-art RPCA algorithms, such as ScaledGD\nand AltProj, on both synthetic datasets and real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">HanQin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wotao Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defocus Map Estimation and Deblurring from a Single Dual-Pixel Image. (arXiv:2110.05655v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05655","description":"<p>We present a method that takes as input a single dual-pixel image, and\nsimultaneously estimates the image's defocus map -- the amount of defocus blur\nat each pixel -- and recovers an all-in-focus image. Our method is inspired\nfrom recent works that leverage the dual-pixel sensors available in many\nconsumer cameras to assist with autofocus, and use them for recovery of defocus\nmaps or all-in-focus images. These prior works have solved the two recovery\nproblems independently of each other, and often require large labeled datasets\nfor supervised training. By contrast, we show that it is beneficial to treat\nthese two closely-connected problems simultaneously. To this end, we set up an\noptimization problem that, by carefully modeling the optics of dual-pixel\nimages, jointly solves both problems. We use data captured with a consumer\nsmartphone camera to demonstrate that, after a one-time calibration step, our\napproach improves upon prior works for both defocus map estimation and blur\nremoval, despite being entirely unsupervised.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xin_S/0/1/0/all/0/1\">Shumian Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadhwa_N/0/1/0/all/0/1\">Neal Wadhwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Tianfan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Pratul P. Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkioulekas_I/0/1/0/all/0/1\">Ioannis Gkioulekas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1\">Rahul Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate and Generalizable Quantitative Scoring of Liver Steatosis from Ultrasound Images via Scalable Deep Learning. (arXiv:2110.05664v1 [eess.IV])","link":"http://arxiv.org/abs/2110.05664","description":"<p>Background &amp; Aims: Hepatic steatosis is a major cause of chronic liver\ndisease. 2D ultrasound is the most widely used non-invasive tool for screening\nand monitoring, but associated diagnoses are highly subjective. We developed a\nscalable deep learning (DL) algorithm for quantitative scoring of liver\nsteatosis from 2D ultrasound images.\n</p>\n<p>Approach &amp; Results: Using retrospectively collected multi-view ultrasound\ndata from 3,310 patients, 19,513 studies, and 228,075 images, we trained a DL\nalgorithm to diagnose steatosis stages (healthy, mild, moderate, or severe)\nfrom ultrasound diagnoses. Performance was validated on two multi-scanner\nunblinded and blinded (initially to DL developer) histology-proven cohorts (147\nand 112 patients) with histopathology fatty cell percentage diagnoses, and a\nsubset with FibroScan diagnoses. We also quantified reliability across scanners\nand viewpoints. Results were evaluated using Bland-Altman and receiver\noperating characteristic (ROC) analysis. The DL algorithm demonstrates\nrepeatable measurements with a moderate number of images (3 for each viewpoint)\nand high agreement across 3 premium ultrasound scanners. High diagnostic\nperformance was observed across all viewpoints: area under the curves of the\nROC to classify &gt;=mild, &gt;=moderate, =severe steatosis grades were 0.85, 0.90,\nand 0.93, respectively. The DL algorithm outperformed or performed at least\ncomparably to FibroScan with statistically significant improvements for all\nlevels on the unblinded histology-proven cohort, and for =severe steatosis on\nthe blinded histology-proven cohort.\n</p>\n<p>Conclusions: The DL algorithm provides a reliable quantitative steatosis\nassessment across view and scanners on two multi-scanner cohorts. Diagnostic\nperformance was high with comparable or better performance than FibroScan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tai_D/0/1/0/all/0/1\">Dar-In Tai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Cheng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_S/0/1/0/all/0/1\">Shiu-Feng Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_T/0/1/0/all/0/1\">Tse-Hwa Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_W/0/1/0/all/0/1\">Wan-Ting Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harrison_A/0/1/0/all/0/1\">Adam P. Harrison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAS-Bench-360: Benchmarking Diverse Tasks for Neural Architecture Search. (arXiv:2110.05668v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05668","description":"<p>Most existing neural architecture search (NAS) benchmarks and algorithms\nprioritize performance on well-studied tasks, e.g., image classification on\nCIFAR and ImageNet. This makes the applicability of NAS approaches in more\ndiverse areas inadequately understood. In this paper, we present NAS-Bench-360,\na benchmark suite for evaluating state-of-the-art NAS methods for convolutional\nneural networks (CNNs). To construct it, we curate a collection of ten tasks\nspanning a diverse array of application domains, dataset sizes, problem\ndimensionalities, and learning objectives. By carefully selecting tasks that\ncan both interoperate with modern CNN-based search methods but that are also\nfar-afield from their original development domain, we can use NAS-Bench-360 to\ninvestigate the following central question: do existing state-of-the-art NAS\nmethods perform well on diverse tasks? Our experiments show that a modern NAS\nprocedure designed for image classification can indeed find good architectures\nfor tasks with other dimensionalities and learning objectives; however, the\nsame method struggles against more task-specific methods and performs\ncatastrophically poorly on classification in non-vision domains. The case for\nNAS robustness becomes even more dire in a resource-constrained setting, where\na recent NAS method provides little-to-no benefit over much simpler baselines.\nThese results demonstrate the need for a benchmark such as NAS-Bench-360 to\nhelp develop NAS approaches that work well on a variety of tasks, a crucial\ncomponent of a truly robust and automated pipeline. We conclude with a\ndemonstration of the kind of future research our suite of tasks will enable.\nAll data and code is made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Renbo Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodak_M/0/1/0/all/0/1\">Mikhail Khodak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1\">Nicholas Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Heatmap-based Landmark Detection. (arXiv:2110.05676v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05676","description":"<p>Mitral valve repair is a very difficult operation, often requiring\nexperienced surgeons. The doctor will insert a prosthetic ring to aid in the\nrestoration of heart function. The location of the prosthesis' sutures is\ncritical. Obtaining and studying them during the procedure is a valuable\nlearning experience for new surgeons. This paper proposes a landmark detection\nnetwork for detecting sutures in endoscopic pictures, which solves the problem\nof a variable number of suture points in the images. Because there are two\ndatasets, one from the simulated domain and the other from real intraoperative\ndata, this work uses cycleGAN to interconvert the images from the two domains\nto obtain a larger dataset and a better score on real intraoperative data. This\npaper performed the tests using a simulated dataset of 2708 photos and a real\ndataset of 2376 images. The mean sensitivity on the simulated dataset is about\n75.64% and the precision is about 73.62%. The mean sensitivity on the real\ndataset is about 50.23% and the precision is about 62.76%. The data is from the\nAdaptOR MICCAI Challenge 2021, which can be found at\nhttps://zenodo.org/record/4646979\\#.YO1zLUxCQ2x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huifeng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yatao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No way to crop: On robust image crop localization. (arXiv:2110.05687v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05687","description":"<p>Previous image forensics schemes for crop detection are only limited on\npredicting whether an image has been cropped. This paper presents a novel\nscheme for image crop localization using robust watermarking. We further extend\nour scheme to detect tampering attack on the attacked image. We demonstrate\nthat our scheme is the first to provide high-accuracy and robust image crop\nlocalization. Besides, the accuracy of tamper detection is comparable to many\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1\">Qichao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoxiao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhengxin You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inclusive Design: Accessibility Settings for People with Cognitive Disabilities. (arXiv:2110.05688v1 [cs.HC])","link":"http://arxiv.org/abs/2110.05688","description":"<p>The advancement of technology has progressed faster than any other field in\nthe world and with the development of these new technologies, it is important\nto make sure that these tools can be used by everyone, including people with\ndisabilities. Accessibility options in computing devices help ensure that\neveryone has the same access to advanced technologies. Unfortunately, for those\nwho require more unique and sometimes challenging accommodations, such as\npeople with Amyotrophic lateral sclerosis ( ALS), the most commonly used\naccessibility features are simply not enough. While assistive technology for\nthose with ALS does exist, it requires multiple peripheral devices that can\nbecome quite expensive collectively. The purpose of this paper is to suggest a\nmore affordable and readily available option for ALS assistive technology that\ncan be implemented on a smartphone or tablet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Waggoner_T/0/1/0/all/0/1\">Trae Waggoner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1\">Julia Ann Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Ashwin Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manikandan_S/0/1/0/all/0/1\">Sudarsan Manikandan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hiding Images into Images with Real-world Robustness. (arXiv:2110.05689v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05689","description":"<p>The existing image embedding networks are basically vulnerable to malicious\nattacks such as JPEG compression and noise adding, not applicable for\nreal-world copyright protection tasks. To solve this problem, we introduce a\ngenerative deep network based method for hiding images into images while\nassuring high-quality extraction from the destructive synthesized images. An\nembedding network is sequentially concatenated with an attack layer, a\ndecoupling network and an image extraction network. The addition of decoupling\nnetwork learns to extract the embedded watermark from the attacked image. We\nalso pinpoint the weaknesses of the adversarial training for robustness in\nprevious works and build our improved real-world attack simulator. Experimental\nresults demonstrate the superiority of the proposed method against typical\ndigital attacks by a large margin, as well as the performance boost of the\nrecovered images with the aid of progressive recovery strategy. Besides, we are\nthe first to robustly hide three secret images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1\">Qichao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianhan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haisheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Modeling for Task Recognition and Action Segmentation in Weakly-Labeled Instructional Videos. (arXiv:2110.05697v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05697","description":"<p>This paper focuses on task recognition and action segmentation in\nweakly-labeled instructional videos, where only the ordered sequence of\nvideo-level actions is available during training. We propose a two-stream\nframework, which exploits semantic and temporal hierarchies to recognize\ntop-level tasks in instructional videos. Further, we present a novel top-down\nweakly-supervised action segmentation approach, where the predicted task is\nused to constrain the inference of fine-grained action sequences. Experimental\nresults on the popular Breakfast and Cooking 2 datasets show that our\ntwo-stream hierarchical task modeling significantly outperforms existing\nmethods in top-level task recognition for all datasets and metrics.\nAdditionally, using our task recognition framework in the proposed top-down\naction segmentation approach consistently improves the state of the art, while\nalso reducing segmentation inference time by 80-90 percent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghoddoosian_R/0/1/0/all/0/1\">Reza Ghoddoosian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayed_S/0/1/0/all/0/1\">Saif Sayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athitsos_V/0/1/0/all/0/1\">Vassilis Athitsos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Exploring and Improving Robustness of Scene Text Detection Models. (arXiv:2110.05700v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05700","description":"<p>It is crucial to understand the robustness of text detection models with\nregard to extensive corruptions, since scene text detection techniques have\nmany practical applications. For systematically exploring this problem, we\npropose two datasets from which to evaluate scene text detection models:\nICDAR2015-C (IC15-C) and CTW1500-C (CTW-C). Our study extends the investigation\nof the performance and robustness of the proposed region proposal, regression\nand segmentation-based scene text detection frameworks. Furthermore, we perform\na robustness analysis of six key components: pre-training data, backbone,\nfeature fusion module, multi-scale predictions, representation of text\ninstances and loss function. Finally, we present a simple yet effective\ndata-based method to destroy the smoothness of text regions by merging\nbackground and foreground, which can significantly increase the robustness of\ndifferent text detection networks. We hope that this study will provide valid\ndata points as well as experience for future research. Benchmark, code and data\nwill be made available at\n\\url{https://github.com/wushilian/robust-scene-text-detection-benchmark}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shilian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongrui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kewei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zengfu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Fusion Prior for Multi-Focus Image Super Resolution Fusion. (arXiv:2110.05706v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05706","description":"<p>This paper unifies the multi-focus images fusion (MFIF) and blind super\nresolution (SR) problems as the multi-focus image super resolution fusion\n(MFISRF) task, and proposes a novel unified dataset-free unsupervised framework\nnamed deep fusion prior (DFP) to address such MFISRF task. DFP consists of\nSKIPnet network, DoubleReblur focus measurement tactic, decision embedding\nmodule and loss functions. In particular, DFP can obtain MFISRF only from two\nlow-resolution inputs without any extent dataset; SKIPnet implementing\nunsupervised learning via deep image prior is an end-to-end generated network\nacting as the engine of DFP; DoubleReblur is used to determine the primary\ndecision map without learning but based on estimated PSF and Gaussian kernels\nconvolution; decision embedding module optimizes the decision map via learning;\nand DFP losses composed of content loss, joint gradient loss and gradient limit\nloss can obtain high-quality MFISRF results robustly. Experiments have proved\nthat our proposed DFP approaches and even outperforms those state-of-art MFIF\nand SR method combinations. Additionally, DFP is a general framework, thus its\nnetworks and focus measurement tactics can be continuously updated to further\nimprove the MFISRF performance. DFP codes are open source and will be available\nsoon at <a href=\"http://github.com/GuYuanjie/DeepFusionPrior.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuanjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhibo Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hailun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shouyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-aware Video Reading Comprehension for Temporal Language Grounding. (arXiv:2110.05717v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05717","description":"<p>Temporal language grounding in videos aims to localize the temporal span\nrelevant to the given query sentence. Previous methods treat it either as a\nboundary regression task or a span extraction task. This paper will formulate\ntemporal language grounding into video reading comprehension and propose a\nRelation-aware Network (RaNet) to address it. This framework aims to select a\nvideo moment choice from the predefined answer set with the aid of\ncoarse-and-fine choice-query interaction and choice-choice relation\nconstruction. A choice-query interactor is proposed to match the visual and\ntextual information simultaneously in sentence-moment and token-moment levels,\nleading to a coarse-and-fine cross-modal interaction. Moreover, a novel\nmulti-choice relation constructor is introduced by leveraging graph convolution\nto capture the dependencies among video moment choices for the best choice\nselection. Extensive experiments on ActivityNet-Captions, TACoS, and\nCharades-STA demonstrate the effectiveness of our solution. Codes will be\nreleased soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jialin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Spatial Route Prior in Vision-and-Language Navigation. (arXiv:2110.05728v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05728","description":"<p>Vision-and-language navigation (VLN) is a trending topic which aims to\nnavigate an intelligent agent to an expected position through natural language\ninstructions. This work addresses the task of VLN from a previously-ignored\naspect, namely the spatial route prior of the navigation scenes. A critically\nenabling innovation of this work is explicitly considering the spatial route\nprior under several different VLN settings. In a most information-rich case of\nknowing environment maps and admitting shortest-path prior, we observe that\ngiven an origin-destination node pair, the internal route can be uniquely\ndetermined. Thus, VLN can be effectively formulated as an ordinary\nclassification problem over all possible destination nodes in the scenes.\nFurthermore, we relax it to other more general VLN settings, proposing a\nsequential-decision variant (by abandoning the shortest-path route prior) and\nan explore-and-exploit scheme (for addressing the case of not knowing the\nenvironment maps) that curates a compact and informative sub-graph to exploit.\nAs reported by [34], the performance of VLN methods has been stuck at a plateau\nin past two years. Even with increased model complexity, the state-of-the-art\nsuccess rate on R2R validation-unseen set has stayed around 62% for single-run\nand 73% for beam-search with model-ensemble. We have conducted comprehensive\nevaluations on both R2R and R4R, and surprisingly found that utilizing the\nspatial route priors may be the key of breaking above-mentioned performance\nceiling. For example, on R2R validation-unseen set, when the number of discrete\nnodes explored is about 40, our single-model success rate reaches 73%, and\nincreases to 78% if a Speaker model is ensembled, which significantly outstrips\nprevious state-of-the-art VLN-BERT with 3 models ensembled.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinzhe Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yadong Mu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Scene Graph Generation by Attention Distillation from Caption. (arXiv:2110.05731v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05731","description":"<p>If an image tells a story, the image caption is the briefest narrator.\nGenerally, a scene graph prefers to be an omniscient generalist, while the\nimage caption is more willing to be a specialist, which outlines the gist. Lots\nof previous studies have found that a scene graph is not as practical as\nexpected unless it can reduce the trivial contents and noises. In this respect,\nthe image caption is a good tutor. To this end, we let the scene graph borrow\nthe ability from the image caption so that it can be a specialist on the basis\nof remaining all-around, resulting in the so-called Topic Scene Graph. What an\nimage caption pays attention to is distilled and passed to the scene graph for\nestimating the importance of partial objects, relationships, and events.\nSpecifically, during the caption generation, the attention about individual\nobjects in each time step is collected, pooled, and assembled to obtain the\nattention about relationships, which serves as weak supervision for\nregularizing the estimated importance scores of relationships. In addition, as\nthis attention distillation process provides an opportunity for combining the\ngeneration of image caption and scene graph together, we further transform the\nscene graph into linguistic form with rich and free-form expressions by sharing\na single generation model with image caption. Experiments show that attention\ndistillation brings significant improvements in mining important relationships\nwithout strong supervision, and the topic scene graph shows great potential in\nsubsequent applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">W. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">R. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">X. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Efficient Multi-Agent Cooperative Visual Exploration. (arXiv:2110.05734v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05734","description":"<p>We consider the task of visual indoor exploration with multiple agents, where\nthe agents need to cooperatively explore the entire indoor region using as few\nsteps as possible. Classical planning-based methods often suffer from\nparticularly expensive computation at each inference step and a limited\nexpressiveness of cooperation strategy. By contrast, reinforcement learning\n(RL) has become a trending paradigm for tackling this challenge due to its\nmodeling capability of arbitrarily complex strategies and minimal inference\noverhead. We extend the state-of-the-art single-agent RL solution, Active\nNeural SLAM (ANS), to the multi-agent setting by introducing a novel RL-based\nglobal-goal planner, Spatial Coordination Planner (SCP), which leverages\nspatial information from each individual agent in an end-to-end manner and\neffectively guides the agents to navigate towards different spatial goals with\nhigh exploration efficiency. SCP consists of a transformer-based relation\nencoder to capture intra-agent interactions and a spatial action decoder to\nproduce accurate goals. In addition, we also implement a few multi-agent\nenhancements to process local information from each agent for an aligned\nspatial representation and more precise planning. Our final solution,\nMulti-Agent Active Neural SLAM (MAANS), combines all these techniques and\nsubstantially outperforms 4 different planning-based methods and various RL\nbaselines in the photo-realistic physical testbed, Habitat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiaxuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huazhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Refinement of Low-level Feature Based Activation Map for Weakly Supervised Object Localization. (arXiv:2110.05741v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05741","description":"<p>We present a two-stage learning framework for weakly supervised object\nlocalization (WSOL). While most previous efforts rely on high-level feature\nbased CAMs (Class Activation Maps), this paper proposes to localize objects\nusing the low-level feature based activation maps. In the first stage, an\nactivation map generator produces activation maps based on the low-level\nfeature maps in the classifier, such that rich contextual object information is\nincluded in an online manner. In the second stage, we employ an evaluator to\nevaluate the activation maps predicted by the activation map generator. Based\non this, we further propose a weighted entropy loss, an attentive erasing, and\nan area loss to drive the activation map generator to substantially reduce the\nuncertainty of activations between object and background, and explore less\ndiscriminative regions. Based on the low-level object information preserved in\nthe first stage, the second stage model gradually generates a well-separated,\ncomplete, and compact activation map of object in the image, which can be\neasily thresholded for accurate localization. Extensive experiments on\nCUB-200-2011 and ImageNet-1K datasets show that our framework surpasses\nprevious methods by a large margin, which sets a new state-of-the-art for WSOL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Cheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangping Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Ziqi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weizeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seamless Copy Move Manipulation in Digital Images. (arXiv:2110.05747v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05747","description":"<p>The importance and relevance of digital image forensics has attracted\nresearchers to establish different techniques for creating as well as detecting\nforgeries. The core category in passive image forgery is copy-move image\nforgery that affects the originality of image by applying a different\ntransformation. In this paper frequency domain image manipulation method is\nbeing presented.The method exploits the localized nature of discrete wavelet\ntransform (DWT) to get hold of the region of the host image to be manipulated.\nBoth the patch and host image are subjected to DWT at the same level $l$ to get\n$3l + 1$ sub-bands and each sub-band of the patch is pasted to the identified\nregion in the corresponding sub-band of the host image. The resultant\nmanipulated host sub-bands are then subjected to inverse DWT to get the final\nmanipulated host image. The proposed method shows good resistance against\ndetection by two frequency domain forgery detection methods from the\nliterature. The purpose of this research work is to create the forgery and\nhighlight the need to produce forgery detection methods that are robust against\nthe malicious copy-move forgery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qazi_T/0/1/0/all/0/1\">Tanzila Qazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mushtaq Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_K/0/1/0/all/0/1\">Khizar Hayat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Damage Building Using Real-time Crowdsourced Images and Transfer Learning. (arXiv:2110.05762v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05762","description":"<p>After significant earthquakes, we can see images posted on social media\nplatforms by individuals and media agencies owing to the mass usage of\nsmartphones these days. These images can be utilized to provide information\nabout the shaking damage in the earthquake region both to the public and\nresearch community, and potentially to guide rescue work. This paper presents\nan automated way to extract the damaged building images after earthquakes from\nsocial media platforms such as Twitter and thus identify the particular user\nposts containing such images. Using transfer learning and ~6500 manually\nlabelled images, we trained a deep learning model to recognize images with\ndamaged buildings in the scene. The trained model achieved good performance\nwhen tested on newly acquired images of earthquakes at different locations and\nran in near real-time on Twitter feed after the 2020 M7.0 earthquake in Turkey.\nFurthermore, to better understand how the model makes decisions, we also\nimplemented the Grad-CAM method to visualize the important locations on the\nimages that facilitate the decision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chachra_G/0/1/0/all/0/1\">Gaurav Chachra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1\">Qingkai Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jim Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korlakunta_S/0/1/0/all/0/1\">Srujay Korlakunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grannen_J/0/1/0/all/0/1\">Jennifer Grannen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robson_A/0/1/0/all/0/1\">Alexander Robson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_R/0/1/0/all/0/1\">Richard Allen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents. (arXiv:2110.05769v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05769","description":"<p>Communication between embodied AI agents has received increasing attention in\nrecent years. Despite its use, it is still unclear whether the learned\ncommunication is interpretable and grounded in perception. To study the\ngrounding of emergent forms of communication, we first introduce the\ncollaborative multi-object navigation task CoMON. In this task, an oracle agent\nhas detailed environment information in the form of a map. It communicates with\na navigator agent that perceives the environment visually and is tasked to find\na sequence of goals. To succeed at the task, effective communication is\nessential. CoMON hence serves as a basis to study different communication\nmechanisms between heterogeneous agents, that is, agents with different\ncapabilities and roles. We study two common communication mechanisms and\nanalyze their communication patterns through an egocentric and spatial lens. We\nshow that the emergent communication can be grounded to the agent observations\nand the spatial structure of the 3D environment. Video summary:\nhttps://youtu.be/kLv2rxO9t0g\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shivansh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wani_S/0/1/0/all/0/1\">Saim Wani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_U/0/1/0/all/0/1\">Unnat Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazebnik_S/0/1/0/all/0/1\">Svetlana Lazebnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1\">Manolis Savva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Angel X. Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperCube: Implicit Field Representations of Voxelized 3D Models. (arXiv:2110.05770v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05770","description":"<p>Recently introduced implicit field representations offer an effective way of\ngenerating 3D object shapes. They leverage implicit decoder trained to take a\n3D point coordinate concatenated with a shape encoding and to output a value\nwhich indicates whether the point is outside the shape or not. Although this\napproach enables efficient rendering of visually plausible objects, it has two\nsignificant limitations. First, it is based on a single neural network\ndedicated for all objects from a training set which results in a cumbersome\ntraining procedure and its application in real life. More importantly, the\nimplicit decoder takes only points sampled within voxels (and not the entire\nvoxels) which yields problems at the classification boundaries and results in\nempty spaces within the rendered mesh.\n</p>\n<p>To solve the above limitations, we introduce a new HyperCube architecture\nbased on interval arithmetic network, that enables direct processing of 3D\nvoxels, trained using a hypernetwork paradigm to enforce model convergence.\nInstead of processing individual 3D samples from within a voxel, our approach\nallows to input the entire voxel (3D cube) represented with its convex hull\ncoordinates, while the target network constructed by a hypernet assigns it to\nan inside or outside category. As a result our HyperCube model outperforms the\ncompeting approaches both in terms of training and inference efficiency, as\nwell as the final mesh quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Proszewska_M/0/1/0/all/0/1\">Magdalena Proszewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazur_M/0/1/0/all/0/1\">Marcin Mazur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1\">Przemys&#x142;aw Spurek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDWNet: A Straight Dilated Network with Wavelet Transformation for Image Deblurring. (arXiv:2110.05803v1 [eess.IV])","link":"http://arxiv.org/abs/2110.05803","description":"<p>Image deblurring is a classical computer vision problem that aims to recover\na sharp image from a blurred image. To solve this problem, existing methods\napply the Encode-Decode architecture to design the complex networks to make a\ngood performance. However, most of these methods use repeated up-sampling and\ndown-sampling structures to expand the receptive field, which results in\ntexture information loss during the sampling process and some of them design\nthe multiple stages that lead to difficulties with convergence. Therefore, our\nmodel uses dilated convolution to enable the obtainment of the large receptive\nfield with high spatial resolution. Through making full use of the different\nreceptive fields, our method can achieve better performance. On this basis, we\nreduce the number of up-sampling and down-sampling and design a simple network\nstructure. Besides, we propose a novel module using the wavelet transform,\nwhich effectively helps the network to recover clear high-frequency texture\ndetails. Qualitative and quantitative evaluations of real and synthetic\ndatasets show that our deblurring method is comparable to existing algorithms\nin terms of performance with much lower training requirements. The source code\nand pre-trained models are available at https://github.com/FlyEgle/SDWNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zou_W/0/1/0/all/0/1\">Wenbin Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_M/0/1/0/all/0/1\">Mingchao Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunchen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Satellite Image Semantic Segmentation. (arXiv:2110.05812v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05812","description":"<p>In this paper, we propose a method for the automatic semantic segmentation of\nsatellite images into six classes (sparse forest, dense forest, moor,\nherbaceous formation, building, and road). We rely on Swin Transformer\narchitecture and build the dataset from IGN open data. We report quantitative\nand qualitative segmentation results on this dataset and discuss strengths and\nlimitations. The dataset and the trained model are made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerin_E/0/1/0/all/0/1\">Eric Gu&#xe9;rin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oechslin_K/0/1/0/all/0/1\">Killian Oechslin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1\">Beno&#xee;t Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-Based high-speed low-latency fiducial marker tracking. (arXiv:2110.05819v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05819","description":"<p>Motion and dynamic environments, especially under challenging lighting\nconditions, are still an open issue for robust robotic applications. In this\npaper, we propose an end-to-end pipeline for real-time, low latency, 6\ndegrees-of-freedom pose estimation of fiducial markers. Instead of achieving a\npose estimation through a conventional frame-based approach, we employ the\nhigh-speed abilities of event-based sensors to directly refine the spatial\ntransformation, using consecutive events. Furthermore, we introduce a novel\ntwo-way verification process for detecting tracking errors by backtracking the\nestimated pose, allowing us to evaluate the quality of our tracking. This\napproach allows us to achieve pose estimation at a rate up to 156~kHz, while\nonly relying on CPU resources. The average end-to-end latency of our method is\n3~ms. Experimental results demonstrate outstanding potential for robotic tasks,\nsuch as visual servoing in fast action-perception loops.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loch_A/0/1/0/all/0/1\">Adam Loch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haessig_G/0/1/0/all/0/1\">Germain Haessig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincze_M/0/1/0/all/0/1\">Markus Vincze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVoE: A Synthetic 3D Dataset on Understanding Violation of Expectation for Artificial Cognition. (arXiv:2110.05836v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05836","description":"<p>Recent work in cognitive reasoning and computer vision has engendered an\nincreasing popularity for the Violation-of-Expectation (VoE) paradigm in\nsynthetic datasets. Inspired by work in infant psychology, researchers have\nstarted evaluating a model's ability to discriminate between expected and\nsurprising scenes as a sign of its reasoning ability. Existing VoE-based 3D\ndatasets in physical reasoning only provide vision data. However, current\ncognitive models of physical reasoning by psychologists reveal infants create\nhigh-level abstract representations of objects and interactions. Capitalizing\non this knowledge, we propose AVoE: a synthetic 3D VoE-based dataset that\npresents stimuli from multiple novel sub-categories for five event categories\nof physical reasoning. Compared to existing work, AVoE is armed with\nground-truth labels of abstract features and rules augmented to vision data,\npaving the way for high-level symbolic predictions in physical reasoning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1\">Arijit Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiafei Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H. Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheston Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLNet: Plane and Line Priors for Unsupervised Indoor Depth Estimation. (arXiv:2110.05839v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05839","description":"<p>Unsupervised learning of depth from indoor monocular videos is challenging as\nthe artificial environment contains many textureless regions. Fortunately, the\nindoor scenes are full of specific structures, such as planes and lines, which\nshould help guide unsupervised depth learning. This paper proposes PLNet that\nleverages the plane and line priors to enhance the depth estimation. We first\nrepresent the scene geometry using local planar coefficients and impose the\nsmoothness constraint on the representation. Moreover, we enforce the planar\nand linear consistency by randomly selecting some sets of points that are\nprobably coplanar or collinear to construct simple and effective consistency\nlosses. To verify the proposed method's effectiveness, we further propose to\nevaluate the flatness and straightness of the predicted point cloud on the\nreliable planar and linear regions. The regularity of these regions indicates\nquality indoor reconstruction. Experiments on NYU Depth V2 and ScanNet show\nthat PLNet outperforms existing methods. The code is available at\n\\url{https://github.com/HalleyJiang/PLNet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hualie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Laiyan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Adversarial Semi-supervised Learning. (arXiv:2110.05848v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05848","description":"<p>In this paper we exploit Semi-Supervised Learning (SSL) to increase the\namount of training data to improve the performance of Fine-Grained Visual\nCategorization (FGVC). This problem has not been investigated in the past in\nspite of prohibitive annotation costs that FGVC requires. Our approach\nleverages unlabeled data with an adversarial optimization strategy in which the\ninternal features representation is obtained with a second-order pooling model.\nThis combination allows to back-propagate the information of the parts,\nrepresented by second-order pooling, onto unlabeled data in an adversarial\ntraining setting. We demonstrate the effectiveness of the combined use by\nconducting experiments on six state-of-the-art fine-grained datasets, which\ninclude Aircrafts, Stanford Cars, CUB-200-2011, Oxford Flowers, Stanford Dogs,\nand the recent Semi-Supervised iNaturalist-Aves. Experimental results clearly\nshow that our proposed method has better performance than the only previous\napproach that examined this problem; it also obtained higher classification\naccuracy with respect to the supervised learning methods with which we\ncompared.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mugnai_D/0/1/0/all/0/1\">Daniele Mugnai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pernici_F/0/1/0/all/0/1\">Federico Pernici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchini_F/0/1/0/all/0/1\">Francesco Turchini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1\">Alberto Del Bimbo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Binary Neural Networks through Fully Utilizing Latent Weights. (arXiv:2110.05850v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05850","description":"<p>Binary Neural Networks (BNNs) rely on a real-valued auxiliary variable W to\nhelp binary training. However, pioneering binary works only use W to accumulate\ngradient updates during backward propagation, which can not fully exploit its\npower and may hinder novel advances in BNNs. In this work, we explore the role\nof W in training besides acting as a latent variable. Notably, we propose to\nadd W into the computation graph, making it perform as a real-valued feature\nextractor to aid the binary training. We make different attempts on how to\nutilize the real-valued weights and propose a specialized supervision.\nVisualization experiments qualitatively verify the effectiveness of our\napproach in making it easier to distinguish between different categories.\nQuantitative experiments show that our approach outperforms current\nstate-of-the-arts, further closing the performance gap between floating-point\nnetworks and BNNs. Evaluation on ImageNet with ResNet-18 (Top-1 63.4%),\nResNet-34 (Top-1 67.0%) achieves new state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weixiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peisong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Learning On The Hierarchy Representation for Fine-Grained Human Action Recognition. (arXiv:2110.05853v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05853","description":"<p>Fine-grained human action recognition is a core research topic in computer\nvision. Inspired by the recently proposed hierarchy representation of\nfine-grained actions in FineGym and SlowFast network for action recognition, we\npropose a novel multi-task network which exploits the FineGym hierarchy\nrepresentation to achieve effective joint learning and prediction for\nfine-grained human action recognition. The multi-task network consists of three\npathways of SlowOnly networks with gradually increased frame rates for events,\nsets and elements of fine-grained actions, followed by our proposed integration\nlayers for joint learning and prediction. It is a two-stage approach, where it\nfirst learns deep feature representation at each hierarchical level, and is\nfollowed by feature encoding and fusion for multi-task learning. Our empirical\nresults on the FineGym dataset achieve a new state-of-the-art performance, with\n91.80% Top-1 accuracy and 88.46% mean accuracy for element actions, which are\n3.40% and 7.26% higher than the previous best results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leong_M/0/1/0/all/0/1\">Mei Chee Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hui Li Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haosong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Feng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Joo Hwee Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Networks Are Not Invariant to Translation, but They Can Learn to Be. (arXiv:2110.05861v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05861","description":"<p>When seeing a new object, humans can immediately recognize it across\ndifferent retinal locations: the internal object representation is invariant to\ntranslation. It is commonly believed that Convolutional Neural Networks (CNNs)\nare architecturally invariant to translation thanks to the convolution and/or\npooling operations they are endowed with. In fact, several studies have found\nthat these networks systematically fail to recognise new objects on untrained\nlocations. In this work, we test a wide variety of CNNs architectures showing\nhow, apart from DenseNet-121, none of the models tested was architecturally\ninvariant to translation. Nevertheless, all of them could learn to be invariant\nto translation. We show how this can be achieved by pretraining on ImageNet,\nand it is sometimes possible with much simpler data sets when all the items are\nfully translated across the input canvas. At the same time, this invariance can\nbe disrupted by further training due to catastrophic forgetting/interference.\nThese experiments show how pretraining a network on an environment with the\nright `latent' characteristics (a more naturalistic environment) can result in\nthe network learning deep perceptual rules which would dramatically improve\nsubsequent generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biscione_V/0/1/0/all/0/1\">Valerio Biscione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowers_J/0/1/0/all/0/1\">Jeffrey S. Bowers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenHands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages. (arXiv:2110.05877v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05877","description":"<p>AI technologies for Natural Languages have made tremendous progress recently.\nHowever, commensurate progress has not been made on Sign Languages, in\nparticular, in recognizing signs as individual words or as complete sentences.\nWe introduce OpenHands, a library where we take four key ideas from the NLP\ncommunity for low-resource languages and apply them to sign languages for\nword-level recognition. First, we propose using pose extracted through\npretrained models as the standard modality of data to reduce training time and\nenable efficient inference, and we release standardized pose datasets for 6\ndifferent sign languages - American, Argentinian, Chinese, Greek, Indian, and\nTurkish. Second, we train and release checkpoints of 4 pose-based isolated sign\nlanguage recognition models across all 6 languages, providing baselines and\nready checkpoints for deployment. Third, to address the lack of labelled data,\nwe propose self-supervised pretraining on unlabelled data. We curate and\nrelease the largest pose-based pretraining dataset on Indian Sign Language\n(Indian-SL). Fourth, we compare different pretraining strategies and for the\nfirst time establish that pretraining is effective for sign language\nrecognition by demonstrating (a) improved fine-tuning performance especially in\nlow-resource settings, and (b) high crosslingual transfer from Indian-SL to few\nother sign languages. We open-source all models and datasets in OpenHands with\na hope that it makes research in sign languages more accessible, available here\nat https://github.com/AI4Bharat/OpenHands .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Selvaraj_P/0/1/0/all/0/1\">Prem Selvaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+NC_G/0/1/0/all/0/1\">Gokul NC</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fourier-based Video Prediction through Relational Object Motion. (arXiv:2110.05881v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05881","description":"<p>The ability to predict future outcomes conditioned on observed video frames\nis crucial for intelligent decision-making in autonomous systems. Recently,\ndeep recurrent architectures have been applied to the task of video prediction.\nHowever, this often results in blurry predictions and requires tedious training\non large datasets. Here, we explore a different approach by (1) using\nfrequency-domain approaches for video prediction and (2) explicitly inferring\nobject-motion relationships in the observed scene. The resulting predictions\nare consistent with the observed dynamics in a scene and do not suffer from\nblur.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1\">Malte Mosbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Depth Estimation with Sharp Boundary. (arXiv:2110.05885v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05885","description":"<p>Monocular depth estimation is the base task in computer vision. It has a\ntremendous development in the decade with the development of deep learning. But\nthe boundary blur of the depth map is still a serious problem. Research finds\nthe boundary blur problem is mainly caused by two factors, first, the low-level\nfeatures containing boundary and structure information may loss in deeper\nnetworks during the convolution process., second, the model ignores the errors\nintroduced by the boundary area due to the few portions of the boundary in the\nwhole areas during the backpropagation. In order to mitigate the boundary blur\nproblem, we focus on the above two impact factors. Firstly, we design a scene\nunderstanding module to learn the global information with low- and high-level\nfeatures, and then to transform the global information to different scales with\nour proposed scale transform module according to the different phases in the\ndecoder. Secondly, we propose a boundary-aware depth loss function to pay\nattention to the effects of the boundary's depth value. The extensive\nexperiments show that our method can predict the depth maps with clearer\nboundaries, and the performance of the depth accuracy base on NYU-depth v2 and\nSUN RGB-D is competitive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1\">Qingling Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yan Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGH: Metadata Guided Hypergraph Modeling for Unsupervised Person Re-identification. (arXiv:2110.05886v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05886","description":"<p>As a challenging task, unsupervised person ReID aims to match the same\nidentity with query images which does not require any labeled information. In\ngeneral, most existing approaches focus on the visual cues only, leaving\npotentially valuable auxiliary metadata information (e.g., spatio-temporal\ncontext) unexplored. In the real world, such metadata is normally available\nalongside captured images, and thus plays an important role in separating\nseveral hard ReID matches. With this motivation in mind, we\npropose~\\textbf{MGH}, a novel unsupervised person ReID approach that uses meta\ninformation to construct a hypergraph for feature learning and label\nrefinement. In principle, the hypergraph is composed of camera-topology-aware\nhyperedges, which can model the heterogeneous data correlations across cameras.\nTaking advantage of label propagation on the hypergraph, the proposed approach\nis able to effectively refine the ReID results, such as correcting the wrong\nlabels or smoothing the noisy labels. Given the refined results, We further\npresent a memory-based listwise loss to directly optimize the average precision\nin an approximate manner. Extensive experiments on three benchmarks demonstrate\nthe effectiveness of the proposed approach against the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yiming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jian Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Is Graph: Structured Graph Module for Video Action Recognition. (arXiv:2110.05904v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05904","description":"<p>In the field of action recognition, video clips are always treated as ordered\nframes for subsequent processing. To achieve spatio-temporal perception,\nexisting approaches propose to embed adjacent temporal interaction in the\nconvolutional layer. The global semantic information can therefore be obtained\nby stacking multiple local layers hierarchically. However, such global temporal\naccumulation can only reflect the high-level semantics in deep layers,\nneglecting the potential low-level holistic clues in shallow layers. In this\npaper, we first propose to transform a video sequence into a graph to obtain\ndirect long-term dependencies among temporal frames. To preserve sequential\ninformation during transformation, we devise a structured graph module (SGM),\nachieving fine-grained temporal interactions throughout the entire network. In\nparticular, SGM divides the neighbors of each node into several temporal\nregions so as to extract global structural information with diverse sequential\nflows. Extensive experiments are performed on standard benchmark datasets,\ni.e., Something-Something V1 &amp; V2, Diving48, Kinetics-400, UCF101, and HMDB51.\nThe reported performance and analysis demonstrate that SGM can achieve\noutstanding precision with less computational complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rong-Chang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rescoring Sequence-to-Sequence Models for Text Line Recognition with CTC-Prefixes. (arXiv:2110.05909v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05909","description":"<p>In contrast to Connectionist Temporal Classification (CTC) approaches,\nSequence-To-Sequence (S2S) models for Handwritten Text Recognition (HTR) suffer\nfrom errors such as skipped or repeated words which often occur at the end of a\nsequence. In this paper, to combine the best of both approaches, we propose to\nuse the CTC-Prefix-Score during S2S decoding. Hereby, during beam search, paths\nthat are invalid according to the CTC confidence matrix are penalised. Our\nnetwork architecture is composed of a Convolutional Neural Network (CNN) as\nvisual backbone, bidirectional Long-Short-Term-Memory-Cells (LSTMs) as encoder,\nand a decoder which is a Transformer with inserted mutual attention layers. The\nCTC confidences are computed on the encoder while the Transformer is only used\nfor character-wise S2S decoding. We evaluate this setup on three HTR data sets:\nIAM, Rimes, and StAZH. On IAM, we achieve a competitive Character Error Rate\n(CER) of 2.95% when pretraining our model on synthetic data and including a\ncharacter-based language model for contemporary English. Compared to other\nstate-of-the-art approaches, our model requires about 10-20 times less\nparameters. Access our shared implementations via this link to GitHub:\nhttps://github.com/Planet-AI-GmbH/tfaip-hybrid-ctc-s2s.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wick_C/0/1/0/all/0/1\">Christoph Wick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">Jochen Z&#xf6;llner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruning_T/0/1/0/all/0/1\">Tobias Gr&#xfc;ning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trivial or impossible -- dichotomous data difficulty masks model differences (on ImageNet and beyond). (arXiv:2110.05922v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05922","description":"<p>\"The power of a generalization system follows directly from its biases\"\n(Mitchell 1980). Today, CNNs are incredibly powerful generalisation systems --\nbut to what degree have we understood how their inductive bias influences model\ndecisions? We here attempt to disentangle the various aspects that determine\nhow a model decides. In particular, we ask: what makes one model decide\ndifferently from another? In a meticulously controlled setting, we find that\n(1.) irrespective of the network architecture or objective (e.g.\nself-supervised, semi-supervised, vision transformers, recurrent models) all\nmodels end up with a similar decision boundary. (2.) To understand these\nfindings, we analysed model decisions on the ImageNet validation set from epoch\nto epoch and image by image. We find that the ImageNet validation set, among\nothers, suffers from dichotomous data difficulty (DDD): For the range of\ninvestigated models and their accuracies, it is dominated by 46.0% \"trivial\"\nand 11.5% \"impossible\" images (beyond label errors). Only 42.5% of the images\ncould possibly be responsible for the differences between two models' decision\nboundaries. (3.) Only removing the \"impossible\" and \"trivial\" images allows us\nto see pronounced differences between models. (4.) Humans are highly accurate\nat predicting which images are \"trivial\" and \"impossible\" for CNNs (81.4%).\nThis implies that in future comparisons of brains, machines and behaviour, much\nmay be gained from investigating the decisive role of images and the\ndistribution of their difficulties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meding_K/0/1/0/all/0/1\">Kristof Meding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buschoff_L/0/1/0/all/0/1\">Luca M. Schulze Buschoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1\">Robert Geirhos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wichmann_F/0/1/0/all/0/1\">Felix A. Wichmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Semantic Segmentation by Learning Label Uncertainty. (arXiv:2110.05926v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05926","description":"<p>Since the rise of deep learning, many computer vision tasks have seen\nsignificant advancements. However, the downside of deep learning is that it is\nvery data-hungry. Especially for segmentation problems, training a deep neural\nnet requires dense supervision in the form of pixel-perfect image labels, which\nare very costly. In this paper, we present a new loss function to train a\nsegmentation network with only a small subset of pixel-perfect labels, but take\nthe advantage of weakly-annotated training samples in the form of cheap\nbounding-box labels. Unlike recent works which make use of box-to-mask proposal\ngenerators, our loss trains the network to learn a label uncertainty within the\nbounding-box, which can be leveraged to perform online bootstrapping (i.e.\ntransforming the boxes to segmentation masks), while training the network. We\nevaluated our method on binary segmentation tasks, as well as a multi-class\nsegmentation task (CityScapes vehicles and persons). We trained each task on a\ndataset comprised of only 18% pixel-perfect and 82% bounding-box labels, and\ncompared the results to a baseline model trained on a completely pixel-perfect\ndataset. For the binary segmentation tasks, our method achieves an IoU score\nwhich is ~98.33% as good as our baseline model, while for the multi-class task,\nour method is 97.12% as good as our baseline model (77.5 vs. 79.8 mIoU).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neven_R/0/1/0/all/0/1\">Robby Neven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neven_D/0/1/0/all/0/1\">Davy Neven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brabandere_B/0/1/0/all/0/1\">Bert De Brabandere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proesmans_M/0/1/0/all/0/1\">Marc Proesmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goedeme_T/0/1/0/all/0/1\">Toon Goedem&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising Diffusion Gamma Models. (arXiv:2110.05948v1 [eess.SP])","link":"http://arxiv.org/abs/2110.05948","description":"<p>Generative diffusion processes are an emerging and effective tool for image\nand speech generation. In the existing methods, the underlying noise\ndistribution of the diffusion process is Gaussian noise. However, fitting\ndistributions with more degrees of freedom could improve the performance of\nsuch generative models. In this work, we investigate other types of noise\ndistribution for the diffusion process. Specifically, we introduce the\nDenoising Diffusion Gamma Model (DDGM) and show that noise from Gamma\ndistribution provides improved results for image and speech generation. Our\napproach preserves the ability to efficiently sample state in the training\ndiffusion process while using Gamma noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nachmani_E/0/1/0/all/0/1\">Eliya Nachmani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roman_R/0/1/0/all/0/1\">Robin San Roman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imitating Deep Learning Dynamics via Locally Elastic Stochastic Differential Equations. (arXiv:2110.05960v1 [cs.LG])","link":"http://arxiv.org/abs/2110.05960","description":"<p>Understanding the training dynamics of deep learning models is perhaps a\nnecessary step toward demystifying the effectiveness of these models. In\nparticular, how do data from different classes gradually become separable in\ntheir feature spaces when training neural networks using stochastic gradient\ndescent? In this study, we model the evolution of features during deep learning\ntraining using a set of stochastic differential equations (SDEs) that each\ncorresponds to a training sample. As a crucial ingredient in our modeling\nstrategy, each SDE contains a drift term that reflects the impact of\nbackpropagation at an input on the features of all samples. Our main finding\nuncovers a sharp phase transition phenomenon regarding the {intra-class impact:\nif the SDEs are locally elastic in the sense that the impact is more\nsignificant on samples from the same class as the input, the features of the\ntraining data become linearly separable, meaning vanishing training loss;\notherwise, the features are not separable, regardless of how long the training\ntime is. Moreover, in the presence of local elasticity, an analysis of our SDEs\nshows that the emergence of a simple geometric structure called the neural\ncollapse of the features. Taken together, our results shed light on the\ndecisive role of local elasticity in the training dynamics of neural networks.\nWe corroborate our theoretical analysis with experiments on a synthesized\ndataset of geometric shapes and CIFAR-10.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can machines learn to see without visual databases?. (arXiv:2110.05973v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05973","description":"<p>This paper sustains the position that the time has come for thinking of\nlearning machines that conquer visual skills in a truly human-like context,\nwhere a few human-like object supervisions are given by vocal interactions and\npointing aids only. This likely requires new foundations on computational\nprocesses of vision with the final purpose of involving machines in tasks of\nvisual description by living in their own visual environment under simple\nman-machine linguistic interactions. The challenge consists of developing\nmachines that learn to see without needing to handle visual databases. This\nmight open the doors to a truly orthogonal competitive track concerning deep\nlearning technologies for vision which does not rely on the accumulation of\nhuge visual databases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Betti_A/0/1/0/all/0/1\">Alessandro Betti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1\">Marco Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1\">Stefano Melacci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelillo_M/0/1/0/all/0/1\">Marcello Pelillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1\">Fabio Roli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early Melanoma Diagnosis with Sequential Dermoscopic Images. (arXiv:2110.05976v1 [eess.IV])","link":"http://arxiv.org/abs/2110.05976","description":"<p>Dermatologists often diagnose or rule out early melanoma by evaluating the\nfollow-up dermoscopic images of skin lesions. However, existing algorithms for\nearly melanoma diagnosis are developed using single time-point images of\nlesions. Ignoring the temporal, morphological changes of lesions can lead to\nmisdiagnosis in borderline cases. In this study, we propose a framework for\nautomated early melanoma diagnosis using sequential dermoscopic images. To this\nend, we construct our method in three steps. First, we align sequential\ndermoscopic images of skin lesions using estimated Euclidean transformations,\nextract the lesion growth region by computing image differences among the\nconsecutive images, and then propose a spatio-temporal network to capture the\ndermoscopic changes from aligned lesion images and the corresponding difference\nimages. Finally, we develop an early diagnosis module to compute probability\nscores of malignancy for lesion images over time. We collected 179 serial\ndermoscopic imaging data from 122 patients to verify our method. Extensive\nexperiments show that the proposed model outperforms other commonly used\nsequence models. We also compared the diagnostic results of our model with\nthose of seven experienced dermatologists and five registrars. Our model\nachieved higher diagnostic accuracy than clinicians (63.69% vs. 54.33%,\nrespectively) and provided an earlier diagnosis of melanoma (60.7% vs. 32.7% of\nmelanoma correctly diagnosed on the first follow-up images). These results\ndemonstrate that our model can be used to identify melanocytic lesions that are\nat high-risk of malignant transformation earlier in the disease process and\nthereby redefine what is possible in the early detection of melanoma.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1\">Zhen Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_J/0/1/0/all/0/1\">Jennifer Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1\">Toan D Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kelly_J/0/1/0/all/0/1\">John Kelly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mclean_C/0/1/0/all/0/1\">Catriona Mclean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonnington_P/0/1/0/all/0/1\">Paul Bonnington</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mar_V/0/1/0/all/0/1\">Victoria Mar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking supervised pre-training for better downstream transferring. (arXiv:2110.06014v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06014","description":"<p>The pretrain-finetune paradigm has shown outstanding performance on many\napplications of deep learning, where a model is pre-trained on a upstream large\ndataset (e.g. ImageNet), and is then fine-tuned to different downstream tasks.\nThough for most cases, the pre-training stage is conducted based on supervised\nmethods, recent works on self-supervised pre-training have shown powerful\ntransferability and even outperform supervised pre-training on multiple\ndownstream tasks. It thus remains an open question how to better generalize\nsupervised pre-training model to downstream tasks. In this paper, we argue that\nthe worse transferability of existing supervised pre-training methods arise\nfrom the negligence of valuable intra-class semantic difference. This is\nbecause these methods tend to push images from the same class close to each\nother despite of the large diversity in their visual contents, a problem to\nwhich referred as \"overfit of upstream tasks\". To alleviate this problem, we\npropose a new supervised pre-training method based on Leave-One-Out\nK-Nearest-Neighbor, or LOOK for short. It relieves the problem of overfitting\nupstream tasks by only requiring each image to share its class label with most\nof its k nearest neighbors, thus allowing each class to exhibit a multi-mode\ndistribution and consequentially preserving part of intra-class difference for\nbetter transferring to downstream tasks. We developed efficient implementation\nof the proposed method that scales well to large datasets. Experimental studies\non multiple downstream tasks show that LOOK outperforms other state-of-the-art\nmethods for supervised and self-supervised pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Security Risks of AutoML. (arXiv:2110.06018v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06018","description":"<p>Neural Architecture Search (NAS) represents an emerging machine learning (ML)\nparadigm that automatically searches for models tailored to given tasks, which\ngreatly simplifies the development of ML systems and propels the trend of ML\ndemocratization. Yet, little is known about the potential security risks\nincurred by NAS, which is concerning given the increasing use of NAS-generated\nmodels in critical domains.\n</p>\n<p>This work represents a solid initial step towards bridging the gap. Through\nan extensive empirical study of 10 popular NAS methods, we show that compared\nwith their manually designed counterparts, NAS-generated models tend to suffer\ngreater vulnerability to various malicious attacks (e.g., adversarial evasion,\nmodel poisoning, and functionality stealing). Further, with both empirical and\nanalytical evidence, we provide possible explanations for such phenomena: given\nthe prohibitive search space and training cost, most NAS methods favor models\nthat converge fast at early training stages; this preference results in\narchitectural properties associated with attack vulnerability (e.g., high loss\nsmoothness and low gradient variance). Our findings not only reveal the\nrelationships between model characteristics and attack vulnerability but also\nsuggest the inherent connections underlying different attacks. Finally, we\ndiscuss potential remedies to mitigate such drawbacks, including increasing\ncell depth and suppressing skip connects, which lead to several promising\nresearch directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ren Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zhaohan Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiapu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoftNeuro: Fast Deep Inference using Multi-platform Optimization. (arXiv:2110.06037v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06037","description":"<p>Faster inference of deep learning models is highly demanded on edge devices\nand even servers, for both financial and environmental reasons. To address this\nissue, we propose SoftNeuro, a novel, high-performance inference framework with\nefficient performance tuning. The key idea is to separate algorithmic routines\nfrom network layers. Our framework maximizes the inference performance by\nprofiling various routines for each layer and selecting the fastest path. To\nefficiently find the best path, we propose a routine-selection algorithm based\non dynamic programming. Experiments show that the proposed framework achieves\nboth fast inference and efficient tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hilaga_M/0/1/0/all/0/1\">Masaki Hilaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuroda_Y/0/1/0/all/0/1\">Yasuhiro Kuroda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_H/0/1/0/all/0/1\">Hitoshi Matsuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_T/0/1/0/all/0/1\">Tatsuya Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogawa_G/0/1/0/all/0/1\">Gabriel Ogawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyake_H/0/1/0/all/0/1\">Hiroshi Miyake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozawa_Y/0/1/0/all/0/1\">Yusuke Kozawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SlideGraph+: Whole Slide Image Level Graphs to Predict HER2Status in Breast Cancer. (arXiv:2110.06042v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06042","description":"<p>Human epidermal growth factor receptor 2 (HER2) is an important prognostic\nand predictive factor which is overexpressed in 15-20% of breast cancer (BCa).\nThe determination of its status is a key clinical decision making step for\nselection of treatment regimen and prognostication. HER2 status is evaluated\nusing transcroptomics or immunohistochemistry (IHC) through situ hybridisation\n(ISH) which require additional costs and tissue burden in addition to\nanalytical variabilities in terms of manual observational biases in scoring. In\nthis study, we propose a novel graph neural network (GNN) based model (termed\nSlideGraph+) to predict HER2 status directly from whole-slide images of routine\nHaematoxylin and Eosin (H&amp;E) slides. The network was trained and tested on\nslides from The Cancer Genome Atlas (TCGA) in addition to two independent test\ndatasets. We demonstrate that the proposed model outperforms the\nstate-of-the-art methods with area under the ROC curve (AUC) values &gt; 0.75 on\nTCGA and 0.8 on independent test sets. Our experiments show that the proposed\napproach can be utilised for case triaging as well as pre-ordering diagnostic\ntests in a diagnostic setting. It can also be used for other weakly supervised\nprediction problems in computational pathology. The SlideGraph+ code is\navailable at https://github.com/wenqi006/SlideGraph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wenqi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toss_M/0/1/0/all/0/1\">Michael Toss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakha_E/0/1/0/all/0/1\">Emad Rakha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz Minhas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Pillar with Fine-grained Feature for 3D Object Detection. (arXiv:2110.06049v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06049","description":"<p>3D object detection with LiDAR point clouds plays an important role in\nautonomous driving perception module that requires high speed, stability and\naccuracy. However, the existing point-based methods are challenging to reach\nthe speed requirements because of too many raw points, and the voxel-based\nmethods are unable to ensure stable speed because of the 3D sparse convolution.\nIn contrast, the 2D grid-based methods, such as PointPillar, can easily achieve\na stable and efficient speed based on simple 2D convolution, but it is hard to\nget the competitive accuracy limited by the coarse-grained point clouds\nrepresentation. So we propose an improved pillar with fine-grained feature\nbased on PointPillar that can significantly improve detection accuracy. It\nconsists of two modules, including height-aware sub-pillar and sparsity-based\ntiny-pillar, which get fine-grained representation respectively in the vertical\nand horizontal direction of 3D space. For height-aware sub-pillar, we introduce\na height position encoding to keep height information of each sub-pillar during\nprojecting to a 2D pseudo image. For sparsity-based tiny-pillar, we introduce\nsparsity-based CNN backbone stacked by dense feature and sparse attention\nmodule to extract feature with larger receptive field efficiently. Experimental\nresults show that our proposed method significantly outperforms previous\nstate-of-the-art 3D detection methods on the Waymo Open Dataset. The related\ncode will be released to facilitate the academic and industrial study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jiahui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1\">Guanghui Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Interaction Graph Convolutional Network for Temporal Language Localization in Videos. (arXiv:2110.06058v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06058","description":"<p>This paper focuses on tackling the problem of temporal language localization\nin videos, which aims to identify the start and end points of a moment\ndescribed by a natural language sentence in an untrimmed video. However, it is\nnon-trivial since it requires not only the comprehensive understanding of the\nvideo and sentence query, but also the accurate semantic correspondence capture\nbetween them. Existing efforts are mainly centered on exploring the sequential\nrelation among video clips and query words to reason the video and sentence\nquery, neglecting the other intra-modal relations (e.g., semantic similarity\namong video clips and syntactic dependency among the query words). Towards this\nend, in this work, we propose a Multi-modal Interaction Graph Convolutional\nNetwork (MIGCN), which jointly explores the complex intra-modal relations and\ninter-modal interactions residing in the video and sentence query to facilitate\nthe understanding and semantic correspondence capture of the video and sentence\nquery. In addition, we devise an adaptive context-aware localization method,\nwhere the context information is taken into the candidate moments and the\nmulti-scale fully connected layers are designed to rank and adjust the boundary\nof the generated coarse candidate moments with different lengths. Extensive\nexperiments on Charades-STA and ActivityNet datasets demonstrate the promising\nperformance and superior efficiency of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zongmeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianjing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuemeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEDUSA: Multi-scale Encoder-Decoder Self-Attention Deep Neural Network Architecture for Medical Image Analysis. (arXiv:2110.06063v1 [eess.IV])","link":"http://arxiv.org/abs/2110.06063","description":"<p>Medical image analysis continues to hold interesting challenges given the\nsubtle characteristics of certain diseases and the significant overlap in\nappearance between diseases. In this work, we explore the concept of\nself-attention for tackling such subtleties in and between diseases. To this\nend, we introduce MEDUSA, a multi-scale encoder-decoder self-attention\nmechanism tailored for medical image analysis. While self-attention deep\nconvolutional neural network architectures in existing literature center around\nthe notion of multiple isolated lightweight attention mechanisms with limited\nindividual capacities being incorporated at different points in the network\narchitecture, MEDUSA takes a significant departure from this notion by\npossessing a single, unified self-attention mechanism with significantly higher\ncapacity with multiple attention heads feeding into different scales in the\nnetwork architecture. To the best of the authors' knowledge, this is the first\n\"single body, multi-scale heads\" realization of self-attention and enables\nexplicit global context amongst selective attention at different levels of\nrepresentational abstractions while still enabling differing local attention\ncontext at individual levels of abstractions. With MEDUSA, we obtain\nstate-of-the-art performance on multiple challenging medical image analysis\nbenchmarks including COVIDx, RSNA RICORD, and RSNA Pneumonia Challenge when\ncompared to previous work. Our MEDUSA model is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Aboutalebi_H/0/1/0/all/0/1\">Hossein Aboutalebi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pavlova_M/0/1/0/all/0/1\">Maya Pavlova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1\">Hayden Gunraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shafiee_M/0/1/0/all/0/1\">Mohammad Javad Shafiee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sabri_A/0/1/0/all/0/1\">Ali Sabri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alaref_A/0/1/0/all/0/1\">Amer Alaref</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral analysis of re-parameterized light fields. (arXiv:2110.06064v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06064","description":"<p>In this paper, we study the spectral properties of re-parameterized light\nfield. Following previous studies of the light field spectrum, which notably\nprovided sampling guidelines, we focus on the two plane parameterization of the\nlight field. However, we introduce additional flexibility by allowing the image\nplane to be tilted and not only parallel. A formal theoretical analysis is\nfirst presented, which shows that more flexible sampling guidelines (i.e. wider\ncamera baselines) can be used to sample the light field when adapting the image\nplane orientation to the scene geometry. We then present our simulations and\nresults to support these theoretical findings. While the work introduced in\nthis paper is mostly theoretical, we believe these new findings open exciting\navenues for more practical application of light fields, such as view synthesis\nor compact representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alain_M/0/1/0/all/0/1\">Martin Alain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smolic_A/0/1/0/all/0/1\">Aljosa Smolic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expressivity and Trainability of Quadratic Networks. (arXiv:2110.06081v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06081","description":"<p>Inspired by diversity of biological neurons, quadratic artificial neurons can\nplay an important role in deep learning models. The type of quadratic neurons\nof our interest replaces the inner-product operation in the conventional neuron\nwith a quadratic function. Despite promising results so far achieved by\nnetworks of quadratic neurons, there are important issues not well addressed.\nTheoretically, the superior expressivity of a quadratic network over either a\nconventional network or a conventional network via quadratic activation is not\nfully elucidated, which makes the use of quadratic networks not well grounded.\nPractically, although a quadratic network can be trained via generic\nbackpropagation, it can be subject to a higher risk of collapse than the\nconventional counterpart. To address these issues, we first apply the spline\ntheory and a measure from algebraic geometry to give two theorems that\ndemonstrate better model expressivity of a quadratic network than the\nconventional counterpart with or without quadratic activation. Then, we propose\nan effective and efficient training strategy referred to as ReLinear to\nstabilize the training process of a quadratic network, thereby unleashing the\nfull potential in its associated machine learning tasks. Comprehensive\nexperiments on popular datasets are performed to support our findings and\nevaluate the performance of quadratic deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1\">Feng-Lei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengzhou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1\">Rongjie Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Conditional Random Field Convolution for Point Cloud Segmentation. (arXiv:2110.06085v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06085","description":"<p>Point cloud segmentation is the foundation of 3D environmental perception for\nmodern intelligent systems. To solve this problem and image segmentation,\nconditional random fields (CRFs) are usually formulated as discrete models in\nlabel space to encourage label consistency, which is actually a kind of\npostprocessing. In this paper, we reconsider the CRF in feature space for point\ncloud segmentation because it can capture the structure of features well to\nimprove the representation ability of features rather than simply smoothing.\nTherefore, we first model the point cloud features with a continuous quadratic\nenergy model and formulate its solution process as a message-passing graph\nconvolution, by which it can be easily integrated into a deep network. We\ntheoretically demonstrate that the message passing in the graph convolution is\nequivalent to the mean-field approximation of a continuous CRF model.\nFurthermore, we build an encoder-decoder network based on the proposed\ncontinuous CRF graph convolution (CRFConv), in which the CRFConv embedded in\nthe decoding layers can restore the details of high-level features that were\nlost in the encoding stage to enhance the location ability of the network,\nthereby benefiting segmentation. Analogous to the CRFConv, we show that the\nclassical discrete CRF can also work collaboratively with the proposed network\nvia another graph convolution to further improve the segmentation results.\nExperiments on various point cloud benchmarks demonstrate the effectiveness and\nrobustness of the proposed method. Compared with the state-of-the-art methods,\nthe proposed method can also achieve competitive segmentation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davoine_F/0/1/0/all/0/1\">Franck Davoine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sign Language Recognition via Skeleton-Aware Multi-Model Ensemble. (arXiv:2110.06161v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06161","description":"<p>Sign language is commonly used by deaf or mute people to communicate but\nrequires extensive effort to master. It is usually performed with the fast yet\ndelicate movement of hand gestures, body posture, and even facial expressions.\nCurrent Sign Language Recognition (SLR) methods usually extract features via\ndeep neural networks and suffer overfitting due to limited and noisy data.\nRecently, skeleton-based action recognition has attracted increasing attention\ndue to its subject-invariant and background-invariant nature, whereas\nskeleton-based SLR is still under exploration due to the lack of hand\nannotations. Some researchers have tried to use off-line hand pose trackers to\nobtain hand keypoints and aid in recognizing sign language via recurrent neural\nnetworks. Nevertheless, none of them outperforms RGB-based approaches yet. To\nthis end, we propose a novel Skeleton Aware Multi-modal Framework with a Global\nEnsemble Model (GEM) for isolated SLR (SAM-SLR-v2) to learn and fuse\nmulti-modal feature representations towards a higher recognition rate.\nSpecifically, we propose a Sign Language Graph Convolution Network (SL-GCN) to\nmodel the embedded dynamics of skeleton keypoints and a Separable\nSpatial-Temporal Convolution Network (SSTCN) to exploit skeleton features. The\nskeleton-based predictions are fused with other RGB and depth based modalities\nby the proposed late-fusion GEM to provide global information and make a\nfaithful SLR prediction. Experiments on three isolated SLR datasets demonstrate\nthat our proposed SAM-SLR-v2 framework is exceedingly effective and achieves\nstate-of-the-art performance with significant margins. Our code will be\navailable at https://github.com/jackyjsy/SAM-SLR-v2\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Songyao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lichen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M2GAN: A Multi-Stage Self-Attention Network for Image Rain Removal on Autonomous Vehicles. (arXiv:2110.06164v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06164","description":"<p>Image deraining is a new challenging problem in applications of autonomous\nvehicles. In a bad weather condition of heavy rainfall, raindrops, mainly\nhitting the vehicle's windshield, can significantly reduce observation ability\neven though the windshield wipers might be able to remove part of it. Moreover,\nrain flows spreading over the windshield can yield the physical effect of\nrefraction, which seriously impede the sightline or undermine the machine\nlearning system equipped in the vehicle. In this paper, we propose a new\nmulti-stage multi-task recurrent generative adversarial network (M2GAN) to deal\nwith challenging problems of raindrops hitting the car's windshield. This\nmethod is also applicable for removing raindrops appearing on a glass window or\nlens. M2GAN is a multi-stage multi-task generative adversarial network that can\nutilize prior high-level information, such as semantic segmentation, to boost\nderaining performance. To demonstrate M2GAN, we introduce the first real-world\ndataset for rain removal on autonomous vehicles. The experimental results show\nthat our proposed method is superior to other state-of-the-art approaches of\nderaining raindrops in respect of quantitative metrics and visual quality.\nM2GAN is considered the first method to deal with challenging problems of\nreal-world rains under unconstrained environments such as autonomous vehicles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc Manh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAda! Temporally-Adaptive Convolutions for Video Understanding. (arXiv:2110.06178v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06178","description":"<p>Spatial convolutions are widely used in numerous deep video models. It\nfundamentally assumes spatio-temporal invariance, i.e., using shared weights\nfor every location in different frames. This work presents Temporally-Adaptive\nConvolutions (TAdaConv) for video understanding, which shows that adaptive\nweight calibration along the temporal dimension is an efficient way to\nfacilitate modelling complex temporal dynamics in videos. Specifically,\nTAdaConv empowers the spatial convolutions with temporal modelling abilities by\ncalibrating the convolution weights for each frame according to its local and\nglobal temporal context. Compared to previous temporal modelling operations,\nTAdaConv is more efficient as it operates over the convolution kernels instead\nof the features, whose dimension is an order of magnitude smaller than the\nspatial resolutions. Further, the kernel calibration also brings an increased\nmodel capacity. We construct TAda2D networks by replacing the spatial\nconvolutions in ResNet with TAdaConv, which leads to on par or better\nperformance compared to state-of-the-art approaches on multiple video action\nrecognition and localization benchmarks. We also demonstrate that as a readily\nplug-in operation with negligible computation overhead, TAdaConv can\neffectively improve many existing video models with a convincing margin. Codes\nand models will be made available at\nhttps://github.com/alibaba-mmai-research/pytorch-video-understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H. Ang Jr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABO: Dataset and Benchmarks for Real-World 3D Object Understanding. (arXiv:2110.06199v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06199","description":"<p>We introduce Amazon-Berkeley Objects (ABO), a new large-scale dataset of\nproduct images and 3D models corresponding to real household objects. We use\nthis realistic, object-centric 3D dataset to measure the domain gap for\nsingle-view 3D reconstruction networks trained on synthetic objects. We also\nuse multi-view images from ABO to measure the robustness of state-of-the-art\nmetric learning approaches to different camera viewpoints. Finally, leveraging\nthe physically-based rendering materials in ABO, we perform single- and\nmulti-view material estimation for a variety of complex, real-world geometries.\nThe full dataset is available for download at\nhttps://amazon-berkeley-objects.s3.amazonaws.com/index.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1\">Jasmine Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Shubham Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luthra_A/0/1/0/all/0/1\">Achleshwar Luthra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Leon Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kenan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicente_T/0/1/0/all/0/1\">Tomas F. Yago Vicente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_H/0/1/0/all/0/1\">Himanshu Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dideriksen_T/0/1/0/all/0/1\">Thomas Dideriksen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillaumin_M/0/1/0/all/0/1\">Matthieu Guillaumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Set Recognition: A Good Closed-Set Classifier is All You Need. (arXiv:2110.06207v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06207","description":"<p>The ability to identify whether or not a test sample belongs to one of the\nsemantic classes in a classifier's training set is critical to practical\ndeployment of the model. This task is termed open-set recognition (OSR) and has\nreceived significant attention in recent years. In this paper, we first\ndemonstrate that the ability of a classifier to make the 'none-of-above'\ndecision is highly correlated with its accuracy on the closed-set classes. We\nfind that this relationship holds across loss objectives and architectures, and\nfurther demonstrate the trend both on the standard OSR benchmarks as well as on\na large-scale ImageNet evaluation. Second, we use this correlation to boost the\nperformance of the cross-entropy OSR 'baseline' by improving its closed-set\naccuracy, and with this strong baseline achieve a new state-of-the-art on the\nmost challenging OSR benchmark. Similarly, we boost the performance of the\nexisting state-of-the-art method by improving its closed-set accuracy, but this\ndoes not surpass the strong baseline on the most challenging dataset. Our third\ncontribution is to reappraise the datasets used for OSR evaluation, and\nconstruct new benchmarks which better respect the task of detecting semantic\nnovelty, as opposed to low-level distributional shifts as tackled by\nneighbouring machine learning fields. In this new setting, we again demonstrate\nthat there is negligible difference between the strong baseline and the\nexisting state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaze_S/0/1/0/all/0/1\">Sagar Vaze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PX-NET: Simple and Efficient Pixel-Wise Training of Photometric Stereo Networks. (arXiv:2008.04933v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.04933","description":"<p>Retrieving accurate 3D reconstructions of objects from the way they reflect\nlight is a very challenging task in computer vision. Despite more than four\ndecades since the definition of the Photometric Stereo problem, most of the\nliterature has had limited success when global illumination effects such as\ncast shadows, self-reflections and ambient light come into play, especially for\nspecular surfaces.\n</p>\n<p>Recent approaches have leveraged the power of deep learning in conjunction\nwith computer graphics in order to cope with the need of a vast number of\ntraining data in order to invert the image irradiance equation and retrieve the\ngeometry of the object. However, rendering global illumination effects is a\nslow process which can limit the amount of training data that can be generated.\n</p>\n<p>In this work we propose a novel pixel-wise training procedure for normal\nprediction by replacing the training data (observation maps) of globally\nrendered images with independent per-pixel generated data. We show that global\nphysical effects can be approximated on the observation map domain and this\nsimplifies and speeds up the data creation procedure.\n</p>\n<p>Our network, PX-NET, achieves the state-of-the-art performance compared to\nother pixelwise methods on synthetic datasets, as well as the Diligent real\ndataset on both dense and sparse light settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Logothetis_F/0/1/0/all/0/1\">Fotios Logothetis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budvytis_I/0/1/0/all/0/1\">Ignas Budvytis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mecca_R/0/1/0/all/0/1\">Roberto Mecca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1\">Roberto Cipolla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallax Attention for Unsupervised Stereo Correspondence Learning. (arXiv:2009.08250v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.08250","description":"<p>Stereo image pairs encode 3D scene cues into stereo correspondences between\nthe left and right images. To exploit 3D cues within stereo images, recent CNN\nbased methods commonly use cost volume techniques to capture stereo\ncorrespondence over large disparities. However, since disparities can vary\nsignificantly for stereo cameras with different baselines, focal lengths and\nresolutions, the fixed maximum disparity used in cost volume techniques hinders\nthem to handle different stereo image pairs with large disparity variations. In\nthis paper, we propose a generic parallax-attention mechanism (PAM) to capture\nstereo correspondence regardless of disparity variations. Our PAM integrates\nepipolar constraints with attention mechanism to calculate feature similarities\nalong the epipolar line to capture stereo correspondence. Based on our PAM, we\npropose a parallax-attention stereo matching network (PASMnet) and a\nparallax-attention stereo image super-resolution network (PASSRnet) for stereo\nmatching and stereo image super-resolution tasks. Moreover, we introduce a new\nand large-scale dataset named Flickr1024 for stereo image super-resolution.\nExperimental results show that our PAM is generic and can effectively learn\nstereo correspondence under large disparity variations in an unsupervised\nmanner. Comparative results show that our PASMnet and PASSRnet achieve the\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhengfa Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zaiping Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jungang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1\">Wei An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Transformer-based Set Prediction for Object Detection. (arXiv:2011.10881v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.10881","description":"<p>DETR is a recently proposed Transformer-based method which views object\ndetection as a set prediction problem and achieves state-of-the-art performance\nbut demands extra-long training time to converge. In this paper, we investigate\nthe causes of the optimization difficulty in the training of DETR. Our\nexaminations reveal several factors contributing to the slow convergence of\nDETR, primarily the issues with the Hungarian loss and the Transformer\ncross-attention mechanism. To overcome these issues we propose two solutions,\nnamely, TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN\n(Transformer-based Set Prediction with RCNN). Experimental results show that\nthe proposed methods not only converge much faster than the original DETR, but\nalso significantly outperform DETR and other baselines in terms of detection\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhiqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shengcao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Aggregation in Test-Time Augmentation. (arXiv:2011.11156v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11156","description":"<p>Test-time augmentation -- the aggregation of predictions across transformed\nversions of a test input -- is a common practice in image classification.\nTraditionally, predictions are combined using a simple average. In this paper,\nwe present 1) experimental analyses that shed light on cases in which the\nsimple average is suboptimal and 2) a method to address these shortcomings. A\nkey finding is that even when test-time augmentation produces a net improvement\nin accuracy, it can change many correct predictions into incorrect predictions.\nWe delve into when and why test-time augmentation changes a prediction from\nbeing correct to incorrect and vice versa. Building on these insights, we\npresent a learning-based method for aggregating test-time augmentations.\nExperiments across a diverse set of models, datasets, and augmentations show\nthat our method delivers consistent improvements over existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_D/0/1/0/all/0/1\">Divya Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blalock_D/0/1/0/all/0/1\">Davis Blalock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_G/0/1/0/all/0/1\">Guha Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1\">John Guttag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Regularization Prediction in Diffeomorphic Image Registration. (arXiv:2011.14229v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2011.14229","description":"<p>This paper presents a predictive model for estimating regularization\nparameters of diffeomorphic image registration. We introduce a novel framework\nthat automatically determines the parameters controlling the smoothness of\ndiffeomorphic transformations. Our method significantly reduces the effort of\nparameter tuning, which is time and labor-consuming. To achieve the goal, we\ndevelop a predictive model based on deep convolutional neural networks (CNN)\nthat learns the mapping between pairwise images and the regularization\nparameter of image registration. In contrast to previous methods that estimate\nsuch parameters in a high-dimensional image space, our model is built in an\nefficient bandlimited space with much lower dimensions. We demonstrate the\neffectiveness of our model on both 2D synthetic data and 3D real brain images.\nExperimental results show that our model not only predicts appropriate\nregularization parameters for image registration, but also improving the\nnetwork training in terms of time and memory efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Miaomiao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Attribute Learning. (arXiv:2012.05895v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.05895","description":"<p>Semantic concepts are frequently defined by combinations of underlying\nattributes. As mappings from attributes to classes are often simple,\nattribute-based representations facilitate novel concept learning with zero or\nfew examples. A significant limitation of existing attribute-based learning\nparadigms, such as zero-shot learning, is that the attributes are assumed to be\nknown and fixed. In this work we study the rapid learning of attributes that\nwere not previously labeled. Compared to standard few-shot learning of semantic\nclasses, in which novel classes may be defined by attributes that were relevant\nat training time, learning new attributes imposes a stiffer challenge. We found\nthat supervised learning with training attributes does not generalize well to\nnew test attributes, whereas self-supervised pre-training brings significant\nimprovement. We further experimented with random splits of the attribute space\nand found that predictability of test attributes provides an informative\nestimate of a model's generalization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triantafillou_E/0/1/0/all/0/1\">Eleni Triantafillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuan-Chieh Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_J/0/1/0/all/0/1\">James Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snell_J/0/1/0/all/0/1\">Jake Snell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitkow_X/0/1/0/all/0/1\">Xaq Pitkow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1\">Andreas S. Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Representation Learning from Flow Equivariance. (arXiv:2101.06553v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06553","description":"<p>Self-supervised representation learning is able to learn semantically\nmeaningful features; however, much of its recent success relies on multiple\ncrops of an image with very few objects. Instead of learning view-invariant\nrepresentation from simple images, humans learn representations in a complex\nworld with changing scenes by observing object movement, deformation, pose\nvariation, and ego motion. Motivated by this ability, we present a new\nself-supervised learning representation framework that can be directly deployed\non a video stream of complex scenes with many moving objects. Our framework\nfeatures a simple flow equivariance objective that encourages the network to\npredict the features of another frame by applying a flow transformation to the\nfeatures of the current frame. Our representations, learned from\nhigh-resolution raw video, can be readily used for downstream tasks on static\nimages. Readout experiments on challenging semantic segmentation, instance\nsegmentation, and object detection benchmarks show that we are able to\noutperform representations obtained from previous state-of-the-art methods\nincluding SimCLR and BYOL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuwen Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenyuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attacks On Multi-Agent Communication. (arXiv:2101.06560v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.06560","description":"<p>Growing at a fast pace, modern autonomous systems will soon be deployed at\nscale, opening up the possibility for cooperative multi-agent systems. Sharing\ninformation and distributing workloads allow autonomous agents to better\nperform tasks and increase computation efficiency. However, shared information\ncan be modified to execute adversarial attacks on deep learning models that are\nwidely employed in modern systems. Thus, we aim to study the robustness of such\nsystems and focus on exploring adversarial attacks in a novel multi-agent\nsetting where communication is done through sharing learned intermediate\nrepresentations of neural networks. We observe that an indistinguishable\nadversarial message can severely degrade performance, but becomes weaker as the\nnumber of benign agents increases. Furthermore, we show that black-box transfer\nattacks are more difficult in this setting when compared to directly perturbing\nthe inputs, as it is necessary to align the distribution of learned\nrepresentations with domain adaptation. Our work studies robustness at the\nneural network level to contribute an additional layer of fault tolerance to\nmodern security protocols for more secure multi-agent systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1\">James Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tsunhsuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingkang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manivasagam_S/0/1/0/all/0/1\">Sivabalan Manivasagam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Monocular Hand Pose Estimation on Embedded Systems. (arXiv:2102.07067v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2102.07067","description":"<p>Hand pose estimation is a fundamental task in many human-robot\ninteraction-related applications. However, previous approaches suffer from\nunsatisfying hand landmark predictions in real-world scenes and high\ncomputation burden. This paper proposes a fast and accurate framework for hand\npose estimation, dubbed as \"FastHand\". Using a lightweight encoder-decoder\nnetwork architecture, FastHand fulfills the requirements of practical\napplications running on embedded devices. The encoder consists of deep layers\nwith a small number of parameters, while the decoder makes use of spatial\nlocation information to obtain more accurate results. The evaluation took place\non two publicly available datasets demonstrating the improved performance of\nthe proposed pipeline compared to other state-of-the-art approaches. FastHand\noffers high accuracy scores while reaching a speed of 25 frames per second on\nan NVIDIA Jetson TX2 graphics processing unit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiajie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haogang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsintotas_K/0/1/0/all/0/1\">Konstantinos A. Tsintotas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Low-Rank Simplicity Bias in Deep Networks. (arXiv:2103.10427v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.10427","description":"<p>Modern deep neural networks are highly over-parameterized compared to the\ndata on which they are trained, yet they often generalize remarkably well. A\nflurry of recent work has asked: why do deep networks not overfit to their\ntraining data? In this work, we make a series of empirical observations that\ninvestigate the hypothesis that deeper networks are inductively biased to find\nsolutions with lower rank embeddings. We conjecture that this bias exists\nbecause the volume of functions that maps to low-rank embedding increases with\ndepth. We show empirically that our claim holds true on finite width linear and\nnon-linear models and show that these are the solutions that generalize well.\nWe then show that the low-rank simplicity bias exists even after training,\nusing a wide variety of commonly used optimizers. We found this phenomenon to\nbe resilient to initialization, hyper-parameters, and learning methods. We\nfurther demonstrate how linear over-parameterization of deep non-linear models\ncan be used to induce low-rank bias, improving generalization performance\nwithout changing the effective model capacity. Practically, we demonstrate that\nsimply linearly over-parameterizing standard models at training time can\nimprove performance on image classification tasks, including ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huh_M/0/1/0/all/0/1\">Minyoung Huh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobahi_H/0/1/0/all/0/1\">Hossein Mobahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richard Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1\">Brian Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Pulkit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Depth Estimation through Virtual-world Supervision and Real-world SfM Self-Supervision. (arXiv:2103.12209v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12209","description":"<p>Depth information is essential for on-board perception in autonomous driving\nand driver assistance. Monocular depth estimation (MDE) is very appealing since\nit allows for appearance and depth being on direct pixelwise correspondence\nwithout further calibration. Best MDE models are based on Convolutional Neural\nNetworks (CNNs) trained in a supervised manner, i.e., assuming pixelwise ground\ntruth (GT). Usually, this GT is acquired at training time through a calibrated\nmulti-modal suite of sensors. However, also using only a monocular system at\ntraining time is cheaper and more scalable. This is possible by relying on\nstructure-from-motion (SfM) principles to generate self-supervision.\nNevertheless, problems of camouflaged objects, visibility changes,\nstatic-camera intervals, textureless areas, and scale ambiguity, diminish the\nusefulness of such self-supervision. In this paper, we perform monocular depth\nestimation by virtual-world supervision (MonoDEVS) and real-world SfM\nself-supervision. We compensate the SfM self-supervision limitations by\nleveraging virtual-world images with accurate semantic and depth supervision\nand addressing the virtual-to-real domain gap. Our MonoDEVSNet outperforms\nprevious MDE CNNs trained on monocular and even stereo sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gurram_A/0/1/0/all/0/1\">Akhil Gurram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuna_A/0/1/0/all/0/1\">Ahmet Faruk Tuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Fengyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urfalioglu_O/0/1/0/all/0/1\">Onay Urfalioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Antonio M. L&#xf3;pez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling the Power of Mixup for Stronger Classifiers. (arXiv:2103.13027v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13027","description":"<p>Mixup-based data augmentations have achieved great success as regularizers\nfor deep neural networks. However, existing methods rely on deliberately\nhandcrafted mixup policies, which ignore or oversell the semantic matching\nbetween mixed samples and labels. Driven by their prior assumptions, early\nmethods attempt to smooth decision boundaries by random linear interpolation\nwhile others focus on maximizing class-related information via offline saliency\noptimization. As a result, the issue of label mismatch has not been well\naddressed. Additionally, the optimization stability of mixup training is\nconstantly troubled by the label mismatch. To address these challenges, we\nfirst reformulate mixup for supervised classification as two sub-tasks, mixup\nsample generation and classification, then propose Automatic Mixup (AutoMix), a\nrevolutionary mixup framework. Specifically, a learnable lightweight Mix Block\n(MB) with a cross-attention mechanism is proposed to generate a mixed sample by\nmodeling a fair relationship between the pair of samples under direct\nsupervision of the corresponding mixed label. Moreover, the proposed Momentum\nPipeline (MP) enhances training stability and accelerates convergence on top of\nmaking the Mix Block fully trained end-to-end. Extensive experiments on five\npopular classification benchmarks show that the proposed approach consistently\noutperforms leading methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianzhu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Graphs: A Survey of Generations and Applications. (arXiv:2104.01111v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01111","description":"<p>Scene graph is a structured representation of a scene that can clearly\nexpress the objects, attributes, and relationships between objects in the\nscene. As computer vision technology continues to develop, people are no longer\nsatisfied with simply detecting and recognizing objects in images; instead,\npeople look forward to a higher level of understanding and reasoning about\nvisual scenes. For example, given an image, we want to not only detect and\nrecognize objects in the image, but also know the relationship between objects\n(visual relationship detection), and generate a text description (image\ncaptioning) based on the image content. Alternatively, we might want the\nmachine to tell us what the little girl in the image is doing (Visual Question\nAnswering (VQA)), or even remove the dog from the image and find similar images\n(image editing and retrieval), etc. These tasks require a higher level of\nunderstanding and reasoning for image vision tasks. The scene graph is just\nsuch a powerful tool for scene understanding. Therefore, scene graphs have\nattracted the attention of a large number of researchers, and related research\nis often cross-modal, complex, and rapidly developing. However, no relatively\nsystematic survey of scene graphs exists at present. To this end, this survey\nconducts a comprehensive investigation of the current scene graph research.\nMore specifically, we first summarized the general definition of the scene\ngraph, then conducted a comprehensive and systematic discussion on the\ngeneration method of the scene graph (SGG) and the SGG with the aid of prior\nknowledge. We then investigated the main applications of scene graphs and\nsummarized the most commonly used datasets. Finally, we provide some insights\ninto the future development of scene graphs. We believe this will be a very\nhelpful foundation for future research on scene graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengzhen Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alex Hauptmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct Differentiable Augmentation Search. (arXiv:2104.04282v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04282","description":"<p>Data augmentation has been an indispensable tool to improve the performance\nof deep neural networks, however the augmentation can hardly transfer among\ndifferent tasks and datasets. Consequently, a recent trend is to adopt AutoML\ntechnique to learn proper augmentation policy without extensive hand-crafted\ntuning. In this paper, we propose an efficient differentiable search algorithm\ncalled Direct Differentiable Augmentation Search (DDAS). It exploits\nmeta-learning with one-step gradient update and continuous relaxation to the\nexpected training loss for efficient search. Our DDAS can achieve efficient\naugmentation search without relying on approximations such as Gumbel Softmax or\nsecond order gradient approximation. To further reduce the adverse effect of\nimproper augmentations, we organize the search space into a two level\nhierarchy, in which we first decide whether to apply augmentation, and then\ndetermine the specific augmentation policy. On standard image classification\nbenchmarks, our DDAS achieves state-of-the-art performance and efficiency\ntradeoff while reducing the search cost dramatically, e.g. 0.15 GPU hours for\nCIFAR-10. In addition, we also use DDAS to search augmentation for object\ndetection task and achieve comparable performance with AutoAugment, while being\n1000x faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aoming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zehao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naiyan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Subjective Ratings Using Auto-Decoded Deep Latent Embeddings. (arXiv:2104.05570v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05570","description":"<p>Depending on the application, radiological diagnoses can be associated with\nhigh inter- and intra-rater variabilities. Most computer-aided diagnosis (CAD)\nsolutions treat such data as incontrovertible, exposing learning algorithms to\nconsiderable and possibly contradictory label noise and biases. Thus, managing\nsubjectivity in labels is a fundamental problem in medical imaging analysis. To\naddress this challenge, we introduce auto-decoded deep latent embeddings\n(ADDLE), which explicitly models the tendencies of each rater using an\nauto-decoder framework. After a simple linear transformation, the latent\nvariables can be injected into any backbone at any and multiple points,\nallowing the model to account for rater-specific effects on the diagnosis.\nImportantly, ADDLE does not expect multiple raters per image in training,\nmeaning it can readily learn from data mined from hospital archives. Moreover,\nthe complexity of training ADDLE does not increase as more raters are added.\nDuring inference each rater can be simulated and a 'mean' or 'greedy' virtual\nrating can be produced. We test ADDLE on the problem of liver steatosis\ndiagnosis from 2D ultrasound (US) by collecting 46 084 studies along with\nclinical US diagnoses originating from 65 different raters. We evaluated\ndiagnostic performance using a separate dataset with gold-standard biopsy\ndiagnoses. ADDLE can improve the partial areas under the curve (AUCs) for\ndiagnosing severe steatosis by 10.5% over standard classifiers while\noutperforming other annotator-noise approaches, including those requiring 65\ntimes the parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xinping Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lingyun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_D/0/1/0/all/0/1\">Dar-In Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_A/0/1/0/all/0/1\">Adam P. Harrison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PARE: Part Attention Regressor for 3D Human Body Estimation. (arXiv:2104.08527v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08527","description":"<p>Despite significant progress, we show that state of the art 3D human pose and\nshape estimation methods remain sensitive to partial occlusion and can produce\ndramatically wrong predictions although much of the body is observable. To\naddress this, we introduce a soft attention mechanism, called the Part\nAttention REgressor (PARE), that learns to predict body-part-guided attention\nmasks. We observe that state-of-the-art methods rely on global feature\nrepresentations, making them sensitive to even small occlusions. In contrast,\nPARE's part-guided attention mechanism overcomes these issues by exploiting\ninformation about the visibility of individual body parts while leveraging\ninformation from neighboring body-parts to predict occluded parts. We show\nqualitatively that PARE learns sensible attention masks, and quantitative\nevaluation confirms that PARE achieves more accurate and robust reconstruction\nresults than existing approaches on both occlusion-specific and standard\nbenchmarks. The code and data are available for research purposes at {\\small\n\\url{https://pare.is.tue.mpg.de/}}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1\">Muhammed Kocabas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chun-Hao P. Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaLaLoc: Latent Layout Localisation in Dynamic, Unvisited Environments. (arXiv:2104.09169v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09169","description":"<p>We present LaLaLoc to localise in environments without the need for prior\nvisitation, and in a manner that is robust to large changes in scene\nappearance, such as a full rearrangement of furniture. Specifically, LaLaLoc\nperforms localisation through latent representations of room layout. LaLaLoc\nlearns a rich embedding space shared between RGB panoramas and layouts inferred\nfrom a known floor plan that encodes the structural similarity between\nlocations. Further, LaLaLoc introduces direct, cross-modal pose optimisation in\nits latent space. Thus, LaLaLoc enables fine-grained pose estimation in a scene\nwithout the need for prior visitation, as well as being robust to dynamics,\nsuch as a change in furniture configuration. We show that in a domestic\nenvironment LaLaLoc is able to accurately localise a single RGB panorama image\nto within 8.3cm, given only a floor plan as a prior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Howard_Jenkins_H/0/1/0/all/0/1\">Henry Howard-Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Sarmiento_J/0/1/0/all/0/1\">Jose-Raul Ruiz-Sarmiento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Adrian Prisacariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding. (arXiv:2104.12763v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12763","description":"<p>Multi-modal reasoning systems rely on a pre-trained object detector to\nextract regions of interest from the image. However, this crucial module is\ntypically used as a black box, trained independently of the downstream task and\non a fixed vocabulary of objects and attributes. This makes it challenging for\nsuch systems to capture the long tail of visual concepts expressed in free form\ntext. In this paper we propose MDETR, an end-to-end modulated detector that\ndetects objects in an image conditioned on a raw text query, like a caption or\na question. We use a transformer-based architecture to reason jointly over text\nand image by fusing the two modalities at an early stage of the model. We\npre-train the network on 1.3M text-image pairs, mined from pre-existing\nmulti-modal datasets having explicit alignment between phrases in text and\nobjects in the image. We then fine-tune on several downstream tasks such as\nphrase grounding, referring expression comprehension and segmentation,\nachieving state-of-the-art results on popular benchmarks. We also investigate\nthe utility of our model as an object detector on a given label set when\nfine-tuned in a few-shot setting. We show that our pre-training approach\nprovides a way to handle the long tail of object categories which have very few\nlabelled instances. Our approach can be easily extended for visual question\nanswering, achieving competitive performance on GQA and CLEVR. The code and\nmodels are available at https://github.com/ashkamath/mdetr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Aishwarya Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mannat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carion_N/0/1/0/all/0/1\">Nicolas Carion</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LUCES: A Dataset for Near-Field Point Light Source Photometric Stereo. (arXiv:2104.13135v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13135","description":"<p>Three-dimensional reconstruction of objects from shading information is a\nchallenging task in computer vision. As most of the approaches facing the\nPhotometric Stereo problem use simplified far-field assumptions, real-world\nscenarios have essentially more complex physical effects that need to be\nhandled for accurately reconstructing the 3D shape. An increasing number of\nmethods have been proposed to address the problem when point light sources are\nassumed to be nearby the target object. The proximity of the light sources\ncomplicates the modeling of the image formation as the light behaviour requires\nnon-linear parameterisation to describe its propagation and attenuation.\n</p>\n<p>To understand the capability of the approaches dealing with this near-field\nscenario, the literature till now has used synthetically rendered photometric\nimages or minimal and very customised real-world data. In order to fill the gap\nin evaluating near-field photometric stereo methods, we introduce LUCES the\nfirst real-world 'dataset for near-fieLd point light soUrCe photomEtric Stereo'\nof 14 objects of a varying of materials. A device counting 52 LEDs has been\ndesigned to lit each object positioned 10 to 30 centimeters away from the\ncamera. Together with the raw images, in order to evaluate the 3D\nreconstructions, the dataset includes both normal and depth maps for comparing\ndifferent features of the retrieved 3D geometry. Furthermore, we evaluate the\nperformance of the latest near-field Photometric Stereo algorithms on the\nproposed dataset to assess the SOTA method with respect to actual close range\neffects and object materials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mecca_R/0/1/0/all/0/1\">Roberto Mecca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logothetis_F/0/1/0/all/0/1\">Fotios Logothetis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budvytis_I/0/1/0/all/0/1\">Ignas Budvytis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1\">Roberto Cipolla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COMISR: Compression-Informed Video Super-Resolution. (arXiv:2105.01237v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01237","description":"<p>Most video super-resolution methods focus on restoring high-resolution video\nframes from low-resolution videos without taking into account compression.\nHowever, most videos on the web or mobile devices are compressed, and the\ncompression can be severe when the bandwidth is limited. In this paper, we\npropose a new compression-informed video super-resolution model to restore\nhigh-resolution content without introducing artifacts caused by compression.\nThe proposed model consists of three modules for video super-resolution:\nbi-directional recurrent warping, detail-preserving flow estimation, and\nLaplacian enhancement. All these three modules are used to deal with\ncompression properties such as the location of the intra-frames in the input\nand smoothness in the output frames. For thorough performance evaluation, we\nconducted extensive experiments on standard datasets with a wide range of\ncompression rates, covering many real video use cases. We showed that our\nmethod not only recovers high-resolution content on uncompressed frames from\nthe widely-used benchmark datasets, but also achieves state-of-the-art\nperformance in super-resolving compressed videos based on numerous quantitative\nmetrics. We also evaluated the proposed method by simulating streaming from\nYouTube to demonstrate its effectiveness and robustness. The source codes and\ntrained models are available at\nhttps://github.com/google-research/google-research/tree/master/comisr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Pengchong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations. (arXiv:2106.01548v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01548","description":"<p>Vision Transformers (ViTs) and MLPs signal further efforts on replacing\nhand-wired features or inductive biases with general-purpose neural\narchitectures. Existing works empower the models by massive data, such as\nlarge-scale pre-training and/or repeated strong data augmentations, and still\nreport optimization-related problems (e.g., sensitivity to initialization and\nlearning rates). Hence, this paper investigates ViTs and MLP-Mixers from the\nlens of loss geometry, intending to improve the models' data efficiency at\ntraining and generalization at inference. Visualization and Hessian reveal\nextremely sharp local minima of converged models. By promoting smoothness with\na recently proposed sharpness-aware optimizer, we substantially improve the\naccuracy and robustness of ViTs and MLP-Mixers on various tasks spanning\nsupervised, adversarial, contrastive, and transfer learning (e.g., +5.3\\% and\n+11.0\\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively,\nwith the simple Inception-style preprocessing). We show that the improved\nsmoothness attributes to sparser active neurons in the first few layers. The\nresultant ViTs outperform ResNets of similar size and throughput when trained\nfrom scratch on ImageNet without large-scale pre-training or strong data\naugmentations. They also possess more perceptive attention maps. Our model\ncheckpoints are released at\n\\url{https://github.com/google-research/vision_transformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12423","description":"<p>We observe that despite their hierarchical convolutional nature, the\nsynthesis process of typical generative adversarial networks depends on\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\nof depicted objects. We trace the root cause to careless signal processing that\ncauses aliasing in the generator network. Interpreting all signals in the\nnetwork as continuous, we derive generally applicable, small architectural\nchanges that guarantee that unwanted information cannot leak into the\nhierarchical synthesis process. The resulting networks match the FID of\nStyleGAN2 but differ dramatically in their internal representations, and they\nare fully equivariant to translation and rotation even at subpixel scales. Our\nresults pave the way for generative models better suited for video and\nanimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1\">Samuli Laine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1\">Erik H&#xe4;rk&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Align Yourself: Self-supervised Pre-training for Fine-grained Recognition via Saliency Alignment. (arXiv:2106.15788v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15788","description":"<p>Self-supervised contrastive learning has demonstrated great potential in\nlearning visual representations. Despite their success on various downstream\ntasks such as image classification and object detection, self-supervised\npre-training for fine-grained scenarios is not fully explored. In this paper,\nwe first point out that current contrastive methods are prone to memorizing\nbackground/foreground texture and therefore have a limitation in localizing the\nforeground object. Analysis suggests that learning to extract discriminative\ntexture information and localization are equally crucial for self-supervised\npre-training in fine-grained scenarios. Based on our findings, we introduce\ncross-view saliency alignment (CVSA), a contrastive learning framework that\nfirst crops and swaps saliency regions of images as a novel view generation and\nthen guides the model to localize on the foreground object via a cross-view\nalignment loss. Extensive experiments on four popular fine-grained\nclassification benchmarks show that CVSA significantly improves the learned\nrepresentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1\">Zelin Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ResViT: Residual vision transformers for multi-modal medical image synthesis. (arXiv:2106.16031v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.16031","description":"<p>Multi-modal imaging is a key healthcare technology that is often\nunderutilized due to costs associated with multiple separate scans. This\nlimitation yields the need for synthesis of unacquired modalities from the\nsubset of available modalities. In recent years, generative adversarial network\n(GAN) models with superior depiction of structural details have been\nestablished as state-of-the-art in numerous medical image synthesis tasks. GANs\nare characteristically based on convolutional neural network (CNN) backbones\nthat perform local processing with compact filters. This inductive bias in turn\ncompromises learning of contextual features. Here, we propose a novel\ngenerative adversarial approach for medical image synthesis, ResViT, to combine\nlocal precision of convolution operators with contextual sensitivity of vision\ntransformers. ResViT employs a central bottleneck comprising novel aggregated\nresidual transformer (ART) blocks that synergistically combine convolutional\nand transformer modules. Comprehensive demonstrations are performed for\nsynthesizing missing sequences in multi-contrast MRI, and CT images from MRI.\nOur results indicate superiority of ResViT against competing methods in terms\nof qualitative observations and quantitative metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dalmaz_O/0/1/0/all/0/1\">Onat Dalmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yurt_M/0/1/0/all/0/1\">Mahmut Yurt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Positional Encoding. (arXiv:2107.02561v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.02561","description":"<p>It is well noted that coordinate based MLPs benefit -- in terms of preserving\nhigh-frequency information -- through the encoding of coordinate positions as\nan array of Fourier features. Hitherto, the rationale for the effectiveness of\nthese positional encodings has been solely studied through a Fourier lens. In\nthis paper, we strive to broaden this understanding by showing that alternative\nnon-Fourier embedding functions can indeed be used for positional encoding.\nMoreover, we show that their performance is entirely determined by a trade-off\nbetween the stable rank of the embedded matrix and the distance preservation\nbetween embedded coordinates. We further establish that the now ubiquitous\nFourier feature mapping of position is a special case that fulfills these\nconditions. Consequently, we present a more general theory to analyze\npositional encoding in terms of shifted basis functions. To this end, we\ndevelop the necessary theoretical formulae and empirically verify that our\ntheoretical claims hold in practice. Codes available at\nhttps://github.com/osiriszjq/Rethinking-positional-encoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianqiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1\">Sameera Ramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Brain Reconstruction by Hierarchical Shape-Perception Network from a Single Incomplete Image. (arXiv:2107.11010v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.11010","description":"<p>3D shape reconstruction is essential in the navigation of minimally-invasive\nand auto robot-guided surgeries whose operating environments are indirect and\nnarrow, and there have been some works that focused on reconstructing the 3D\nshape of the surgical organ through limited 2D information available. However,\nthe lack and incompleteness of such information caused by intraoperative\nemergencies (such as bleeding) and risk control conditions have not been\nconsidered. In this paper, a novel hierarchical shape-perception network (HSPN)\nis proposed to reconstruct the 3D point clouds (PCs) of specific brains from\none single incomplete image with low latency. A branching predictor and several\nhierarchical attention pipelines are constructed to generate point clouds that\naccurately describe the incomplete images and then complete these point clouds\nwith high quality. Meanwhile, attention gate blocks (AGBs) are designed to\nefficiently aggregate geometric local features of incomplete PCs transmitted by\nhierarchical attention pipelines and internal features of reconstructing point\nclouds. With the proposed HSPN, 3D shape perception and completion can be\nachieved spontaneously. Comprehensive results measured by Chamfer distance and\nPC-to-PC error demonstrate that the performance of the proposed HSPN\noutperforms other competitive methods in terms of qualitative displays,\nquantitative experiment, and classification evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_B/0/1/0/all/0/1\">Bowen Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuqiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1\">Bingchuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_M/0/1/0/all/0/1\">Min Gan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1\">Yanyan Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoVideo: An Automated Video Action Recognition System. (arXiv:2108.04212v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04212","description":"<p>Action recognition is a crucial task for video understanding. In this paper,\nwe present AutoVideo, a Python system for automated video action recognition.\nIt currently supports seven action recognition algorithms and various\npre-processing modules. Unlike the existing libraries that only provide model\nzoos, AutoVideo is built with the standard pipeline language. The basic\nbuilding block is primitive, which wraps a pre-processing module or an\nalgorithm with some hyperparameters. AutoVideo is highly modular and\nextendable. It can be easily combined with AutoML searchers. The pipeline\nlanguage is quite general so that we can easily enrich AutoVideo with\nalgorithms for various other video-related tasks in the future. AutoVideo is\nreleased under MIT license at https://github.com/datamllab/autovideo\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1\">Zaid Pervaiz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sirui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaben Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1\">Kwei-Herng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anmoll Kumar Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1\">Mohammad Qazim Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1\">Na Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation. (arXiv:2108.07482v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07482","description":"<p>In this paper, we investigate the knowledge distillation (KD) strategy for\nobject detection and propose an effective framework applicable to both\nhomogeneous and heterogeneous student-teacher pairs. The conventional feature\nimitation paradigm introduces imitation masks to focus on informative\nforeground areas while excluding the background noises. However, we find that\nthose methods fail to fully utilize the semantic information in all feature\npyramid levels, which leads to inefficiency for knowledge distillation between\nFPN-based detectors. To this end, we propose a novel semantic-guided feature\nimitation technique, which automatically performs soft matching between feature\npairs across all pyramid levels to provide the optimal guidance to the student.\nTo push the envelop even further, we introduce contrastive distillation to\neffectively capture the information encoded in the relationship between\ndifferent feature regions. Finally, we propose a generalized detection KD\npipeline, which is capable of distilling both homogeneous and heterogeneous\ndetector pairs. Our method consistently outperforms the existing detection KD\ntechniques, and works when (1) components in the framework are used separately\nand in conjunction; (2) for both homogeneous and heterogenous student-teacher\npairs and (3) on multiple detection benchmarks. With a powerful\nX101-FasterRCNN-Instaboost detector as the teacher, R50-FasterRCNN reaches\n44.0% AP, R50-RetinaNet reaches 43.3% AP and R50-FCOS reaches 43.1% AP on COCO\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1\">Renjie Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full-Cycle Energy Consumption Benchmark for Low-Carbon Computer Vision. (arXiv:2108.13465v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.13465","description":"<p>The energy consumption of deep learning models is increasing at a\nbreathtaking rate, which raises concerns due to potential negative effects on\ncarbon neutrality in the context of global warming and climate change. With the\nprogress of efficient deep learning techniques, e.g., model compression,\nresearchers can obtain efficient models with fewer parameters and smaller\nlatency. However, most of the existing efficient deep learning methods do not\nexplicitly consider energy consumption as a key performance indicator.\nFurthermore, existing methods mostly focus on the inference costs of the\nresulting efficient models, but neglect the notable energy consumption\nthroughout the entire life cycle of the algorithm. In this paper, we present\nthe first large-scale energy consumption benchmark for efficient computer\nvision models, where a new metric is proposed to explicitly evaluate the\nfull-cycle energy consumption under different model usage intensity. The\nbenchmark can provide insights for low carbon emission when selecting efficient\ndeep learning algorithms in different model usage scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_D/0/1/0/all/0/1\">Donglin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Ningxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xuanyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System. (arXiv:2109.03144v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03144","description":"<p>Optical Character Recognition (OCR) systems have been widely used in various\nof application scenarios. Designing an OCR system is still a challenging task.\nIn previous work, we proposed a practical ultra lightweight OCR system (PP-OCR)\nto balance the accuracy against the efficiency. In order to improve the\naccuracy of PP-OCR and keep high efficiency, in this paper, we propose a more\nrobust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better\ntext detector and a better text recognizer, which include Collaborative Mutual\nLearning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual\nLearning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the\nprecision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost.\nIt is also comparable to the server models of the PP-OCR which uses ResNet\nseries as backbones. All of the above mentioned models are open-sourced and the\ncode is available in the GitHub repository PaddleOCR which is powered by\nPaddlePaddle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruoyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Cheng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoguang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dianhai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanjun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Unsupervised Learning of Visual Representations and Categories. (arXiv:2109.05675v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05675","description":"<p>Real world learning scenarios involve a nonstationary distribution of classes\nwith sequential dependencies among the samples, in contrast to the standard\nmachine learning formulation of drawing samples independently from a fixed,\ntypically uniform distribution. Furthermore, real world interactions demand\nlearning on-the-fly from few or no class labels. In this work, we propose an\nunsupervised model that simultaneously performs online visual representation\nlearning and few-shot learning of new categories without relying on any class\nlabels. Our model is a prototype-based memory network with a control component\nthat determines when to form a new class prototype. We formulate it as an\nonline Gaussian mixture model, where components are created online with only a\nsingle new example, and assignments do not have to be balanced, which permits\nan approximation to natural imbalanced distributions from uncurated raw data.\nLearning includes a contrastive loss that encourages different views of the\nsame image to be assigned to the same prototype. The result is a mechanism that\nforms categorical representations of objects in nonstationary environments.\nExperiments show that our method can learn from an online stream of visual\ninput data and is significantly better at category recognition compared to\nstate-of-the-art self-supervised learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_T/0/1/0/all/0/1\">Tyler R. Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iuzzolino_M/0/1/0/all/0/1\">Michael L. Iuzzolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael C. Mozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Discrete Wavelet Pooling (LDW-Pooling) For Convolutional Networks. (arXiv:2109.06638v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06638","description":"<p>Pooling is a simple but essential layer in modern deep CNN architectures for\nfeature aggregation and extraction. Typical CNN design focuses on the conv\nlayers and activation functions, while leaving the pooling layers with fewer\noptions. We introduce the Learning Discrete Wavelet Pooling (LDW-Pooling) that\ncan be applied universally to replace standard pooling operations to better\nextract features with improved accuracy and efficiency. Motivated from the\nwavelet theory, we adopt the low-pass (L) and high-pass (H) filters\nhorizontally and vertically for pooling on a 2D feature map. Feature signals\nare decomposed into four (LL, LH, HL, HH) subbands to retain features better\nand avoid information dropping. The wavelet transform ensures features after\npooling can be fully preserved and recovered. We next adopt an energy-based\nattention learning to fine-select crucial and representative features.\nLDW-Pooling is effective and efficient when compared with other\nstate-of-the-art pooling techniques such as WaveletPooling and LiftPooling.\nExtensive experimental validation shows that LDW-Pooling can be applied to a\nwide range of standard CNN architectures and consistently outperform standard\n(max, mean, mixed, and stochastic) pooling operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bor-Shiun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1\">Jun-Wei Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping-Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Lipeng Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An optimised deep spiking neural network architecture without gradients. (arXiv:2109.12813v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2109.12813","description":"<p>We present an end-to-end trainable modular event-driven neural architecture\nthat uses local synaptic and threshold adaptation rules to perform\ntransformations between arbitrary spatio-temporal spike patterns. The\narchitecture represents a highly abstracted model of existing Spiking Neural\nNetwork (SNN) architectures. The proposed Optimized Deep Event-driven Spiking\nneural network Architecture (ODESA) can simultaneously learn hierarchical\nspatio-temporal features at multiple arbitrary time scales. ODESA performs\nonline learning without the use of error back-propagation or the calculation of\ngradients. Through the use of simple local adaptive selection thresholds at\neach node, the network rapidly learns to appropriately allocate its neuronal\nresources at each layer for any given problem without using a real-valued error\nmeasure. These adaptive selection thresholds are the central feature of ODESA,\nensuring network stability and remarkable robustness to noise as well as to the\nselection of initial system parameters. Network activations are inherently\nsparse due to a hard Winner-Take-All (WTA) constraint at each layer. We\nevaluate the architecture on existing spatio-temporal datasets, including the\nspike-encoded IRIS and TIDIGITS datasets, as well as a novel set of tasks based\non International Morse Code that we created. These tests demonstrate the\nhierarchical spatio-temporal learning capabilities of ODESA. Through these\ntests, we demonstrate ODESA can optimally solve practical and highly\nchallenging hierarchical spatio-temporal learning tasks with the minimum\npossible number of computing nodes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bethi_Y/0/1/0/all/0/1\">Yeshwanth Bethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_G/0/1/0/all/0/1\">Gregory Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaik_A/0/1/0/all/0/1\">Andre van Schaik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afshar_S/0/1/0/all/0/1\">Saeed Afshar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Landmark Detection Based Spatiotemporal Motion Estimation for 4D Dynamic Medical Images. (arXiv:2109.14805v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.14805","description":"<p>Motion estimation is a fundamental step in dynamic medical image processing\nfor the assessment of target organ anatomy and function. However, existing\nimage-based motion estimation methods, which optimize the motion field by\nevaluating the local image similarity, are prone to produce implausible\nestimation, especially in the presence of large motion. In this study, we\nprovide a novel motion estimation framework of Dense-Sparse-Dense (DSD), which\ncomprises two stages. In the first stage, we process the raw dense image to\nextract sparse landmarks to represent the target organ anatomical topology and\ndiscard the redundant information that is unnecessary for motion estimation.\nFor this purpose, we introduce an unsupervised 3D landmark detection network to\nextract spatially sparse but representative landmarks for the target organ\nmotion estimation. In the second stage, we derive the sparse motion\ndisplacement from the extracted sparse landmarks of two images of different\ntime points. Then, we present a motion reconstruction network to construct the\nmotion field by projecting the sparse landmarks displacement back into the\ndense image domain. Furthermore, we employ the estimated motion field from our\ntwo-stage DSD framework as initialization and boost the motion estimation\nquality in light-weight yet effective iterative optimization. We evaluate our\nmethod on two dynamic medical imaging tasks to model cardiac motion and lung\nrespiratory motion, respectively. Our method has produced superior motion\nestimation accuracy compared to existing comparative methods. Besides, the\nextensive experimental results demonstrate that our solution can extract well\nrepresentative anatomical landmarks without any requirement of manual\nannotation. Our code is publicly available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yuyu Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bi_L/0/1/0/all/0/1\">Lei Bi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_D/0/1/0/all/0/1\">Dongming Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Liyun Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhengbin Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_D/0/1/0/all/0/1\">Dagan Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiyan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PubTables-1M: Towards a universal dataset and metrics for training and evaluating table extraction models. (arXiv:2110.00061v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.00061","description":"<p>Recently, interest has grown in applying machine learning to the problem of\ntable structure inference and extraction from unstructured documents. However,\nprogress in this area has been challenging both to make and to measure, due to\nseveral issues that arise in training and evaluating models from labeled data.\nThis includes challenges as fundamental as the lack of a single definitive\nground truth output for each input sample and the lack of an ideal metric for\nmeasuring partial correctness for this task. To address these issues we propose\na new dataset, PubMed Tables One Million (PubTables-1M), and a new class of\nmetric, grid table similarity (GriTS). PubTables-1M is nearly twice as large as\nthe previous largest comparable dataset, contains highly-detailed structure\nannotations, and can be used for models across multiple architectures and\nmodalities. Further, it addresses issues such as ambiguity and lack of\nconsistency in the annotations via a novel canonicalization and quality control\nprocedure. We apply DETR to table extraction for the first time and show that\nobject detection models trained on PubTables-1M produce excellent results\nout-of-the-box for all three tasks of detection, structure recognition, and\nfunctional analysis. It is our hope that PubTables-1M and GriTS can further\nprogress in this area by creating data and metrics suitable for training and\nevaluating a wide variety of models for table extraction. Data and code will be\nreleased at https://github.com/microsoft/table-transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smock_B/0/1/0/all/0/1\">Brandon Smock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pesala_R/0/1/0/all/0/1\">Rohith Pesala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_R/0/1/0/all/0/1\">Robin Abraham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From SLAM to Situational Awareness: Challenges and Survey. (arXiv:2110.00273v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2110.00273","description":"<p>The knowledge that an intelligent and autonomous mobile robot has and is able\nto acquire of itself and the environment, namely the situation, limits its\nreasoning, decision-making, and execution skills to efficiently and safely\nperform complex missions. Situational awareness is a basic capability of humans\nthat has been deeply studied in fields like Psychology, Military, Aerospace,\nEducation, etc., but it has barely been considered in robotics, which has\nfocused on ideas such as sensing, perception, sensor fusion, state estimation,\nlocalization and mapping, spatial AI, etc. In our research, we connected the\nbroad multidisciplinary existing knowledge on situational awareness with its\ncounterpart in mobile robotics. In this paper, we survey the state-of-the-art\nrobotics algorithms, we analyze the situational awareness aspects that have\nbeen covered by them, and we discuss their missing points. We found out that\nthe existing robotics algorithms are still missing manifold important aspects\nof situational awareness. As a consequence, we conclude that these missing\nfeatures are limiting the performance of robotic situational awareness, and\nfurther research is needed to overcome this challenge. We see this as an\nopportunity, and provide our vision for future research on robotic situational\nawareness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bavle_H/0/1/0/all/0/1\">Hriday Bavle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Lopez_J/0/1/0/all/0/1\">Jose Luis Sanchez-Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_E/0/1/0/all/0/1\">Eduardo F. Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voos_H/0/1/0/all/0/1\">Holger Voos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MovingFashion: a Benchmark for the Video-to-Shop Challenge. (arXiv:2110.02627v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02627","description":"<p>Retrieving clothes which are worn in social media videos (Instagram, TikTok)\nis the latest frontier of e-fashion, referred to as \"video-to-shop\" in the\ncomputer vision literature. In this paper we present MovingFashion, the first\npublicly available dataset to cope with this challenge. MovingFashion is\ncomposed of 14855 social videos, each one of them associated to e-commerce\n\"shop\" images where the corresponding clothing items are clearly portrayed. In\naddition, we present a network for retrieving the shop images in this scenario,\ndubbed SEAM Match-RCNN. The model is trained by image-to-video domain\nadaptation, allowing to use video sequences where only their association with a\nshop image is given, eliminating the need of millions of annotated bounding\nboxes. SEAM Match-RCNN builds an embedding, where an attention-based weighted\nsum of few frames (10) of a social video is enough to individuate the correct\nproduct within the first 5 retrieved items in a 14K+ shop element gallery with\nan accuracy of 80%. This provides the best performance on MovingFashion,\ncomparing exhaustively against the related state-of-the-art approaches and\nalternative baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godi_M/0/1/0/all/0/1\">Marco Godi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joppi_C/0/1/0/all/0/1\">Christian Joppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1\">Geri Skenderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Thing to Fool them All: Generating Interpretable, Universal, and Physically-Realizable Adversarial Features. (arXiv:2110.03605v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03605","description":"<p>It is well understood that modern deep networks are vulnerable to adversarial\nattacks. However, conventional methods fail to produce adversarial\nperturbations that are intelligible to humans, and they pose limited threats in\nthe physical world. To study feature-class associations in networks and better\nunderstand the real-world threats they face, we develop feature-level\nadversarial perturbations using deep image generators and a novel optimization\nobjective. We term these feature-fool attacks. We show that they are versatile\nand use them to generate targeted feature-level attacks at the ImageNet scale\nthat are simultaneously interpretable, universal to any source image, and\nphysically-realizable. These attacks can also reveal spurious,\nsemantically-describable feature/class associations, and we use them to guide\nthe design of \"copy/paste\" adversaries in which one natural image is pasted\ninto another to cause a targeted misclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeau_M/0/1/0/all/0/1\">Max Nadeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1\">Gabriel Kreiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVG-Net: An SVG-based Trajectory Prediction Model. (arXiv:2110.03706v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03706","description":"<p>Anticipating motions of vehicles in a scene is an essential problem for safe\nautonomous driving systems. To this end, the comprehension of the scene's\ninfrastructure is often the main clue for predicting future trajectories. Most\nof the proposed approaches represent the scene with a rasterized format and\nsome of the more recent approaches leverage custom vectorized formats. In\ncontrast, we propose representing the scene's information by employing Scalable\nVector Graphics (SVG). SVG is a well-established format that matches the\nproblem of trajectory prediction better than rasterized formats while being\nmore general than arbitrary vectorized formats. SVG has the potential to\nprovide the convenience and generality of raster-based solutions if coupled\nwith a powerful tool such as CNNs, for which we introduce SVG-Net. SVG-Net is a\nTransformer-based Neural Network that can effectively capture the scene's\ninformation from SVG inputs. Thanks to the self-attention mechanism in its\nTransformers, SVG-Net can also adequately apprehend relations amongst the scene\nand the agents. We demonstrate SVG-Net's effectiveness by evaluating its\nperformance on the publicly available Argoverse forecasting dataset. Finally,\nwe illustrate how, by using SVG, one can benefit from datasets and advancements\nin other research fronts that also utilize the same input format. Our code is\navailable at https://vita-epfl.github.io/SVGNet/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahari_M/0/1/0/all/0/1\">Mohammadhossein Bahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zehtab_V/0/1/0/all/0/1\">Vahid Zehtab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorasani_S/0/1/0/all/0/1\">Sadegh Khorasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayromlou_S/0/1/0/all/0/1\">Sana Ayromlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadatnejad_S/0/1/0/all/0/1\">Saeed Saadatnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Identity Preserving Landmark Synthesis for Face Reenactment. (arXiv:2110.04708v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04708","description":"<p>Recent face reenactment works are limited by the coarse reference landmarks,\nleading to unsatisfactory identity preserving performance due to the\ndistribution gap between the manipulated landmarks and those sampled from a\nreal person. To address this issue, we propose a fine-grained\nidentity-preserving landmark-guided face reenactment approach. The proposed\nmethod has two novelties. First, a landmark synthesis network which is designed\nto generate fine-grained landmark faces with more details. The network refines\nthe manipulated landmarks and generates a smooth and gradually changing face\nlandmark sequence with good identity preserving ability. Second, several novel\nloss functions including synthesized face identity preserving loss,\nforeground/background mask loss as well as boundary loss are designed, which\naims at synthesizing clear and sharp high-quality faces. Experiments are\nconducted on our self-collected BeautySelfie and the public VoxCeleb1 datasets.\nThe presented qualitative and quantitative results show that our method can\nreenact fine-grained higher quality faces with good ID-preserved appearance\ndetails, fewer artifacts and clearer boundaries than state-of-the-art works.\nCode will be released for reproduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haichao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Y/0/1/0/all/0/1\">Youcheng Ben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Gang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Bin Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Dual Relation Graph for Multi-label Image Recognition. (arXiv:2110.04722v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04722","description":"<p>The simultaneous recognition of multiple objects in one image remains a\nchallenging task, spanning multiple events in the recognition field such as\nvarious object scales, inconsistent appearances, and confused inter-class\nrelationships. Recent research efforts mainly resort to the statistic label\nco-occurrences and linguistic word embedding to enhance the unclear semantics.\nDifferent from these researches, in this paper, we propose a novel\nTransformer-based Dual Relation learning framework, constructing complementary\nrelationships by exploring two aspects of correlation, i.e., structural\nrelation graph and semantic relation graph. The structural relation graph aims\nto capture long-range correlations from object context, by developing a\ncross-scale transformer-based architecture. The semantic graph dynamically\nmodels the semantic meanings of image objects with explicit semantic-aware\nconstraints. In addition, we also incorporate the learnt structural\nrelationship into the semantic graph, constructing a joint relation graph for\nrobust representations. With the collaborative learning of these two effective\nrelation graphs, our approach achieves new state-of-the-art on two popular\nmulti-label recognition benchmarks, i.e., MS-COCO and VOC 2007 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiawei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaowei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEV-Net: Assessing Social Distancing Compliance by Joint People Localization and Geometric Reasoning. (arXiv:2110.04931v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04931","description":"<p>Social distancing, an essential public health measure to limit the spread of\ncontagious diseases, has gained significant attention since the outbreak of the\nCOVID-19 pandemic. In this work, the problem of visual social distancing\ncompliance assessment in busy public areas, with wide field-of-view cameras, is\nconsidered. A dataset of crowd scenes with people annotations under a bird's\neye view (BEV) and ground truth for metric distances is introduced, and several\nmeasures for the evaluation of social distance detection systems are proposed.\nA multi-branch network, BEV-Net, is proposed to localize individuals in world\ncoordinates and identify high-risk regions where social distancing is violated.\nBEV-Net combines detection of head and feet locations, camera pose estimation,\na differentiable homography module to map image into BEV coordinates, and\ngeometric reasoning to produce a BEV map of the people locations in the scene.\nExperiments on complex crowded scenes demonstrate the power of the approach and\nshow superior performance over baselines derived from methods in the\nliterature. Applications of interest for public health decision makers are\nfinally discussed. Datasets, code and pretrained models are publicly available\nat GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhirui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuepeng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1\">Nuno Vasconcelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DANIEL: A Fast and Robust Consensus Maximization Method for Point Cloud Registration with High Outlier Ratios. (arXiv:2110.05075v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05075","description":"<p>Correspondence-based point cloud registration is a cornerstone in geometric\ncomputer vision, robotics perception, photogrammetry and remote sensing, which\nseeks to estimate the best rigid transformation between two point clouds from\nthe correspondences established over 3D keypoints. However, due to limited\nrobustness and accuracy, current 3D keypoint matching techniques are very prone\nto yield outliers, probably even in very large numbers, making robust\nestimation for point cloud registration of great importance. Unfortunately,\nexisting robust methods may suffer from high computational cost or insufficient\nrobustness when encountering high (or even extreme) outlier ratios, hardly\nideal enough for practical use. In this paper, we present a novel\ntime-efficient RANSAC-type consensus maximization solver, named DANIEL\n(Double-layered sAmpliNg with consensus maximization based on stratIfied\nElement-wise compatibiLity), for robust registration. DANIEL is designed with\ntwo layers of random sampling, in order to find inlier subsets with the lowest\ncomputational cost possible. Specifically, we: (i) apply the rigidity\nconstraint to prune raw outliers in the first layer of one-point sampling, (ii)\nintroduce a series of stratified element-wise compatibility tests to conduct\nrapid compatibility checking between minimal models so as to realize more\nefficient consensus maximization in the second layer of two-point sampling, and\n(iii) probabilistic termination conditions are employed to ensure the timely\nreturn of the final inlier set. Based on a variety of experiments over multiple\nreal datasets, we show that DANIEL is robust against over 99% outliers and also\nsignificantly faster than existing state-of-the-art robust solvers (e.g.\nRANSAC, FGR, GORE).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Prototype Classifier for Few-shot Image Classification. (arXiv:2110.05076v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05076","description":"<p>The prototypical network is a prototype classifier based on meta-learning and\nis widely used for few-shot learning because it classifies unseen examples by\nconstructing class-specific prototypes without adjusting hyper-parameters\nduring meta-testing. Interestingly, recent research has attracted a lot of\nattention, showing that a linear classifier with fine-tuning, which does not\nuse a meta-learning algorithm, performs comparably with the prototypical\nnetwork. However, fine-tuning requires additional hyper-parameters when\nadapting a model to a new environment. In addition, although the purpose of\nfew-shot learning is to enable the model to quickly adapt to a new environment,\nfine-tuning needs to be applied every time a new class appears, making fast\nadaptation difficult. In this paper, we analyze how a prototype classifier\nworks equally well without fine-tuning and meta-learning. We experimentally\nfound that directly using the feature vector extracted using standard\npre-trained models to construct a prototype classifier in meta-testing does not\nperform as well as the prototypical network and linear classifiers with\nfine-tuning and feature vectors of pre-trained models. Thus, we derive a novel\ngeneralization bound for the prototypical network and show that focusing on the\nvariance of the norm of a feature vector can improve performance. We\nexperimentally investigated several normalization methods for minimizing the\nvariance of the norm and found that the same performance can be obtained by\nusing the L2 normalization and embedding space transformation without\nfine-tuning or meta-learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_M/0/1/0/all/0/1\">Mingcheng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Issei Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViSeRet: A simple yet effective approach to moment retrieval via fine-grained video segmentation. (arXiv:2110.05146v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05146","description":"<p>Video-text retrieval has many real-world applications such as media\nanalytics, surveillance, and robotics. This paper presents the 1st place\nsolution to the video retrieval track of the ICCV VALUE Challenge 2021. We\npresent a simple yet effective approach to jointly tackle two video-text\nretrieval tasks (video retrieval and video corpus moment retrieval) by\nleveraging the model trained only on the video retrieval task. In addition, we\ncreate an ensemble model that achieves the new state-of-the-art performance on\nall four datasets (TVr, How2r, YouCook2r, and VATEXr) presented in the VALUE\nChallenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Aiden Seungjoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Hanseok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}