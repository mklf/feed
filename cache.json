{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Hausa Visual Genome: A Dataset for Multi-Modal English to Hausa Machine Translation. (arXiv:2205.01133v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01133","description":"<p>Multi-modal Machine Translation (MMT) enables the use of visual information\nto enhance the quality of translations. The visual information can serve as a\nvaluable piece of context information to decrease the ambiguity of input\nsentences. Despite the increasing popularity of such a technique, good and\nsizeable datasets are scarce, limiting the full extent of their potential.\nHausa, a Chadic language, is a member of the Afro-Asiatic language family. It\nis estimated that about 100 to 150 million people speak the language, with more\nthan 80 million indigenous speakers. This is more than any of the other Chadic\nlanguages. Despite a large number of speakers, the Hausa language is considered\nlow-resource in natural language processing (NLP). This is due to the absence\nof sufficient resources to implement most NLP tasks. While some datasets exist,\nthey are either scarce, machine-generated, or in the religious domain.\nTherefore, there is a need to create training and evaluation data for\nimplementing machine learning tasks and bridging the research gap in the\nlanguage. This work presents the Hausa Visual Genome (HaVG), a dataset that\ncontains the description of an image or a section within the image in Hausa and\nits equivalent in English. To prepare the dataset, we started by translating\nthe English description of the images in the Hindi Visual Genome (HVG) into\nHausa automatically. Afterward, the synthetic Hausa data was carefully\npost-edited considering the respective images. The dataset comprises 32,923\nimages and their descriptions that are divided into training, development,\ntest, and challenge test set. The Hausa Visual Genome is the first dataset of\nits kind and can be used for Hausa-English machine translation, multi-modal\nresearch, and image description, among various other natural language\nprocessing and generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Satya Ranjan Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawud_M/0/1/0/all/0/1\">Musa Abdullahi Dawud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parida_S/0/1/0/all/0/1\">Shantipriya Parida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Ibrahim Sa&#x27;id Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_S/0/1/0/all/0/1\">Subhadarshi Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galadanci_B/0/1/0/all/0/1\">Bashir Shehu Galadanci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_B/0/1/0/all/0/1\">Bello Shehu Bello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Text Classification using Graph Convolutional Networks for Large-Scale Low Resource Language. (arXiv:2205.01204v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01204","description":"<p>Graph Convolutional Networks (GCN) have achieved state-of-art results on\nsingle text classification tasks like sentiment analysis, emotion detection,\netc. However, the performance is achieved by testing and reporting on\nresource-rich languages like English. Applying GCN for multi-task text\nclassification is an unexplored area. Moreover, training a GCN or adopting an\nEnglish GCN for Indian languages is often limited by data availability, rich\nmorphological variation, syntax, and semantic differences. In this paper, we\nstudy the use of GCN for the Telugu language in single and multi-task settings\nfor four natural language processing (NLP) tasks, viz. sentiment analysis (SA),\nemotion identification (EI), hate-speech (HS), and sarcasm detection (SAR). In\norder to evaluate the performance of GCN with one of the Indian languages,\nTelugu, we analyze the GCN based models with extensive experiments on four\ndownstream tasks. In addition, we created an annotated Telugu dataset, TEL-NLP,\nfor the four NLP tasks. Further, we propose a supervised graph reconstruction\nmethod, Multi-Task Text GCN (MT-Text GCN) on the Telugu that leverages to\nsimultaneously (i) learn the low-dimensional word and sentence graph embeddings\nfrom word-sentence graph reconstruction using graph autoencoder (GAE) and (ii)\nperform multi-task text classification using these latent sentence graph\nembeddings. We argue that our proposed MT-Text GCN achieves significant\nimprovements on TEL-NLP over existing Telugu pretrained word embeddings, and\nmultilingual pretrained Transformer models: mBERT, and XLM-R. On TEL-NLP, we\nachieve a high F1-score for four NLP tasks: SA (0.84), EI (0.55), HS (0.83) and\nSAR (0.66). Finally, we show our model's quantitative and qualitative analysis\non the four NLP tasks in Telugu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marreddy_M/0/1/0/all/0/1\">Mounika Marreddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oota_S/0/1/0/all/0/1\">Subba Reddy Oota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakada_L/0/1/0/all/0/1\">Lakshmi Sireesha Vakada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinni_V/0/1/0/all/0/1\">Venkata Charan Chinni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paragraph-based Transformer Pre-training for Multi-Sentence Inference. (arXiv:2205.01228v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01228","description":"<p>Inference tasks such as answer sentence selection (AS2) or fact verification\nare typically solved by fine-tuning transformer-based models as individual\nsentence-pair classifiers. Recent studies show that these tasks benefit from\nmodeling dependencies across multiple candidate sentences jointly. In this\npaper, we first show that popular pre-trained transformers perform poorly when\nused for fine-tuning on multi-candidate inference tasks. We then propose a new\npre-training objective that models the paragraph-level semantics across\nmultiple input sentences. Our evaluation on three AS2 and one fact verification\ndatasets demonstrates the superiority of our pre-training technique over the\ntraditional ones for transformers used as joint models for multi-candidate\ninference tasks, as well as when used as cross-encoders for sentence-pair\nformulations of these tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liello_L/0/1/0/all/0/1\">Luca Di Liello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddhant Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Enhanced Machine Learning. (arXiv:2205.01230v1 [cs.LG])","link":"http://arxiv.org/abs/2205.01230","description":"<p>Although information access systems have long supported people in\naccomplishing a wide range of tasks, we propose broadening the scope of users\nof information access systems to include task-driven machines, such as machine\nlearning models. In this way, the core principles of indexing, representation,\nretrieval, and ranking can be applied and extended to substantially improve\nmodel generalization, scalability, robustness, and interpretability. We\ndescribe a generic retrieval-enhanced machine learning (REML) framework, which\nincludes a number of existing models as special cases. REML challenges\ninformation retrieval conventions, presenting opportunities for novel advances\nin core areas, including optimization. The REML research agenda lays a\nfoundation for a new style of information access research and paves a path\ntowards advancing machine learning and artificial intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemAttack: Natural Textual Attacks via Different Semantic Spaces. (arXiv:2205.01287v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01287","description":"<p>Recent studies show that pre-trained language models (LMs) are vulnerable to\ntextual adversarial attacks. However, existing attack methods either suffer\nfrom low attack success rates or fail to search efficiently in the\nexponentially large perturbation space. We propose an efficient and effective\nframework SemAttack to generate natural adversarial text by constructing\ndifferent semantic perturbation functions. In particular, SemAttack optimizes\nthe generated perturbations constrained on generic semantic spaces, including\ntypo space, knowledge space (e.g., WordNet), contextualized semantic space\n(e.g., the embedding space of BERT clusterings), or the combination of these\nspaces. Thus, the generated adversarial texts are more semantically close to\nthe original inputs. Extensive experiments reveal that state-of-the-art (SOTA)\nlarge-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) are\nstill vulnerable to SemAttack. We further demonstrate that SemAttack is general\nand able to generate natural adversarial texts for different languages (e.g.,\nEnglish and Chinese) with high attack success rates. Human evaluations also\nconfirm that our generated adversarial texts are natural and barely affect\nhuman performance. Our code is publicly available at\nhttps://github.com/AI-secure/SemAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chejian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding Hallucination for Few-Shot Language Fine-tuning. (arXiv:2205.01307v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01307","description":"<p>Few-shot language learners adapt knowledge from a pre-trained model to\nrecognize novel classes from a few-labeled sentences. In such settings,\nfine-tuning a pre-trained language model can cause severe over-fitting. In this\npaper, we propose an Embedding Hallucination (EmbedHalluc) method, which\ngenerates auxiliary embedding-label pairs to expand the fine-tuning dataset.\nThe hallucinator is trained by playing an adversarial game with the\ndiscriminator, such that the hallucinated embedding is indiscriminative to the\nreal ones in the fine-tuning dataset. By training with the extended dataset,\nthe language learner effectively learns from the diverse hallucinated\nembeddings to overcome the over-fitting issue. Experiments demonstrate that our\nproposed method is effective in a wide range of language tasks, outperforming\ncurrent fine-tuning methods. Further, we show that EmbedHalluc outperforms\nother methods that address this over-fitting problem, such as common data\naugmentation, semi-supervised pseudo-labeling, and regularization. The code\nwill be made available at: https://github.com/yiren-jian/EmbedHalluc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jian_Y/0/1/0/all/0/1\">Yiren Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chongyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning for Prompt-Based Few-Shot Language Learners. (arXiv:2205.01308v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01308","description":"<p>The impressive performance of GPT-3 using natural language prompts and\nin-context learning has inspired work on better fine-tuning of moderately-sized\nmodels under this paradigm. Following this line of work, we present a\ncontrastive learning framework that clusters inputs from the same class for\nbetter generality of models trained with only limited examples. Specifically,\nwe propose a supervised contrastive framework that clusters inputs from the\nsame class under different augmented \"views\" and repel the ones from different\nclasses. We create different \"views\" of an example by appending it with\ndifferent language prompts and contextual demonstrations. Combining a\ncontrastive loss with the standard masked language modeling (MLM) loss in\nprompt-based few-shot learners, the experimental results show that our method\ncan improve over the state-of-the-art methods in a diverse set of 15 language\ntasks. Our framework makes minimal assumptions on the task or the base model,\nand can be applied to many recent methods with little modification. The code\nwill be made available at: https://github.com/yiren-jian/LM-SupCon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jian_Y/0/1/0/all/0/1\">Yiren Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chongyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open vs Closed-ended questions in attitudinal surveys -- comparing, combining, and interpreting using natural language processing. (arXiv:2205.01317v1 [econ.GN])","link":"http://arxiv.org/abs/2205.01317","description":"<p>To improve the traveling experience, researchers have been analyzing the role\nof attitudes in travel behavior modeling. Although most researchers use\nclosed-ended surveys, the appropriate method to measure attitudes is debatable.\nTopic Modeling could significantly reduce the time to extract information from\nopen-ended responses and eliminate subjective bias, thereby alleviating analyst\nconcerns. Our research uses Topic Modeling to extract information from\nopen-ended questions and compare its performance with closed-ended responses.\nFurthermore, some respondents might prefer answering questions using their\npreferred questionnaire type. So, we propose a modeling framework that allows\nrespondents to use their preferred questionnaire type to answer the survey and\nenable analysts to use the modeling frameworks of their choice to predict\nbehavior. We demonstrate this using a dataset collected from the USA that\nmeasures the intention to use Autonomous Vehicles for commute trips.\nRespondents were presented with alternative questionnaire versions (open- and\nclosed- ended). Since our objective was also to compare the performance of\nalternative questionnaire versions, the survey was designed to eliminate\ninfluences resulting from statements, behavioral framework, and the choice\nexperiment. Results indicate the suitability of using Topic Modeling to extract\ninformation from open-ended responses; however, the models estimated using the\nclosed-ended questions perform better compared to them. Besides, the proposed\nmodel performs better compared to the models used currently. Furthermore, our\nproposed framework will allow respondents to choose the questionnaire type to\nanswer, which could be particularly beneficial to them when using voice-based\nsurveys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/econ/1/au:+Baburajan_V/0/1/0/all/0/1\">Vishnu Baburajan</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Silva_J/0/1/0/all/0/1\">Jo&#xe3;o de Abreu e Silva</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Pereira_F/0/1/0/all/0/1\">Francisco Camara Pereira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Issue Types with seBERT. (arXiv:2205.01335v1 [cs.SE])","link":"http://arxiv.org/abs/2205.01335","description":"<p>Pre-trained transformer models are the current state-of-the-art for natural\nlanguage models processing. seBERT is such a model, that was developed based on\nthe BERT architecture, but trained from scratch with software engineering data.\nWe fine-tuned this model for the NLBSE challenge for the task of issue type\nprediction. Our model dominates the baseline fastText for all three issue types\nin both recall and precisio} to achieve an overall F1-score of 85.7%, which is\nan increase of 4.1% over the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trautsch_A/0/1/0/all/0/1\">Alexander Trautsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbold_S/0/1/0/all/0/1\">Steffen Herbold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding patterns in Knowledge Attribution for Transformers. (arXiv:2205.01366v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01366","description":"<p>We analyze the Knowledge Neurons framework for the attribution of factual and\nrelational knowledge to particular neurons in the transformer network. We use a\n12-layer multi-lingual BERT model for our experiments. Our study reveals\nvarious interesting phenomena. We observe that mostly factual knowledge can be\nattributed to middle and higher layers of the network($\\ge 6$). Further\nanalysis reveals that the middle layers($6-9$) are mostly responsible for\nrelational information, which is further refined into actual factual knowledge\nor the \"correct answer\" in the last few layers($10-12$). Our experiments also\nshow that the model handles prompts in different languages, but representing\nthe same fact, similarly, providing further evidence for effectiveness of\nmulti-lingual pre-training. Applying the attribution scheme for grammatical\nknowledge, we find that grammatical knowledge is far more dispersed among the\nneurons than factual knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Juneja_J/0/1/0/all/0/1\">Jeevesh Juneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Ritu Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hidden behind the obvious: misleading keywords and implicitly abusive language on social media. (arXiv:2205.01374v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01374","description":"<p>While social media offers freedom of self-expression, abusive language carry\nsignificant negative social impact. Driven by the importance of the issue,\nresearch in the automated detection of abusive language has witnessed growth\nand improvement. However, these detection models display a reliance on strongly\nindicative keywords, such as slurs and profanity. This means that they can\nfalsely (1a) miss abuse without such keywords or (1b) flag non-abuse with such\nkeywords, and that (2) they perform poorly on unseen data. Despite the\nrecognition of these problems, gaps and inconsistencies remain in the\nliterature. In this study, we analyse the impact of keywords from dataset\nconstruction to model behaviour in detail, with a focus on how models make\nmistakes on (1a) and (1b), and how (1a) and (1b) interact with (2). Through the\nanalysis, we provide suggestions for future research to address all three\nproblems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenjie Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Entailment for Event Argument Extraction: Zero- and Few-Shot with Multi-Source Learning. (arXiv:2205.01376v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01376","description":"<p>Recent work has shown that NLP tasks such as Relation Extraction (RE) can be\nrecasted as Textual Entailment tasks using verbalizations, with strong\nperformance in zero-shot and few-shot settings thanks to pre-trained entailment\nmodels. The fact that relations in current RE datasets are easily verbalized\ncasts doubts on whether entailment would be effective in more complex tasks. In\nthis work we show that entailment is also effective in Event Argument\nExtraction (EAE), reducing the need of manual annotation to 50% and 20% in ACE\nand WikiEvents respectively, while achieving the same performance as with full\ntraining. More importantly, we show that recasting EAE as entailment alleviates\nthe dependency on schemas, which has been a road-block for transferring\nannotations between domains. Thanks to the entailment, the multi-source\ntransfer between ACE and WikiEvents further reduces annotation down to 10% and\n5% (respectively) of the full training without transfer. Our analysis shows\nthat the key to good results is the use of several entailment datasets to\npre-train the entailment model. Similar to previous approaches, our method\nrequires a small amount of effort for manual verbalization: only less than 15\nminutes per event argument type is needed, and comparable results can be\nachieved with users with different level of expertise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sainz_O/0/1/0/all/0/1\">Oscar Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Dios_I/0/1/0/all/0/1\">Itziar Gonzalez-Dios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1\">Oier Lopez de Lacalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1\">Bonan Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kompetencer: Fine-grained Skill Classification in Danish Job Postings via Distant Supervision and Transfer Learning. (arXiv:2205.01381v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01381","description":"<p>Skill Classification (SC) is the task of classifying job competences from job\npostings. This work is the first in SC applied to Danish job vacancy data. We\nrelease the first Danish job posting dataset: Kompetencer (en: competences),\nannotated for nested spans of competences. To improve upon coarse-grained\nannotations, we make use of The European Skills, Competences, Qualifications\nand Occupations (ESCO; le Vrang et al., 2014) taxonomy API to obtain\nfine-grained labels via distant supervision. We study two setups: The zero-shot\nand few-shot classification setting. We fine-tune English-based models and\nRemBERT (Chung et al., 2020) and compare them to in-language Danish models. Our\nresults show RemBERT significantly outperforms all other models in both the\nzero-shot and the few-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jensen_K/0/1/0/all/0/1\">Kristian N&#xf8;rgaard Jensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP). (arXiv:2205.01397v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01397","description":"<p>Contrastively trained image-text models such as CLIP, ALIGN, and BASIC have\ndemonstrated unprecedented robustness to multiple challenging natural\ndistribution shifts. Since these image-text models differ from previous\ntraining approaches in several ways, an important question is what causes the\nlarge robustness gains. We answer this question via a systematic experimental\ninvestigation. Concretely, we study five different possible causes for the\nrobustness gains: (i) the training set size, (ii) the training distribution,\n(iii) language supervision at training time, (iv) language supervision at test\ntime, and (v) the contrastive loss function. Our experiments show that the more\ndiverse training distribution is the main cause for the robustness gains, with\nthe other factors contributing little to no robustness. Beyond our experimental\nresults, we also introduce ImageNet-Captions, a version of ImageNet with\noriginal text annotations from Flickr, to enable further controlled experiments\nof language-image training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1\">Alex Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yuhao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1\">Vaishaal Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1\">Achal Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Language Taskonomy: Which NLP Tasks are the most Predictive of fMRI Brain Activity?. (arXiv:2205.01404v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01404","description":"<p>Several popular Transformer based language models have been found to be\nsuccessful for text-driven brain encoding. However, existing literature\nleverages only pretrained text Transformer models and has not explored the\nefficacy of task-specific learned Transformer representations. In this work, we\nexplore transfer learning from representations learned for ten popular natural\nlanguage processing tasks (two syntactic and eight semantic) for predicting\nbrain responses from two diverse datasets: Pereira (subjects reading sentences\nfrom paragraphs) and Narratives (subjects listening to the spoken stories).\nEncoding models based on task features are used to predict activity in\ndifferent regions across the whole brain. Features from coreference resolution,\nNER, and shallow syntax parsing explain greater variance for the reading\nactivity. On the other hand, for the listening activity, tasks such as\nparaphrase generation, summarization, and natural language inference show\nbetter encoding performance. Experiments across all 10 task representations\nprovide the following cognitive insights: (i) language left hemisphere has\nhigher predictive brain activity versus language right hemisphere, (ii)\nposterior medial cortex, temporo-parieto-occipital junction, dorsal frontal\nlobe have higher correlation versus early auditory and auditory association\ncortex, (iii) syntactic and semantic tasks display a good predictive\nperformance across brain regions for reading and listening stimuli resp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oota_S/0/1/0/all/0/1\">Subba Reddy Oota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_J/0/1/0/all/0/1\">Jashn Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1\">Veeral Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marreddy_M/0/1/0/all/0/1\">Mounika Marreddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surampudi_B/0/1/0/all/0/1\">Bapi Raju Surampudi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exact Paired-Permutation Testing for Structured Test Statistics. (arXiv:2205.01416v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01416","description":"<p>Significance testing -- especially the paired-permutation test -- has played\na vital role in developing NLP systems to provide confidence that the\ndifference in performance between two systems (i.e., the test statistic) is not\ndue to luck. However, practitioners rely on Monte Carlo approximation to\nperform this test due to a lack of a suitable exact algorithm. In this paper,\nwe provide an efficient exact algorithm for the paired-permutation test for a\nfamily of structured test statistics. Our algorithm runs in $\\mathcal{O}(GN\n(\\log GN )(\\log N ))$ time where $N$ is the dataset size and $G$ is the range\nof the test statistic. We found that our exact algorithm was $10$x faster than\nthe Monte Carlo approximation with $20000$ samples on a common dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zmigrod_R/0/1/0/all/0/1\">Ran Zmigrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inducing and Using Alignments for Transition-based AMR Parsing. (arXiv:2205.01464v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01464","description":"<p>Transition-based parsers for Abstract Meaning Representation (AMR) rely on\nnode-to-word alignments. These alignments are learned separately from parser\ntraining and require a complex pipeline of rule-based components,\npre-processing, and post-processing to satisfy domain-specific constraints.\nParsers also train on a point-estimate of the alignment pipeline, neglecting\nthe uncertainty due to the inherent ambiguity of alignment. In this work we\nexplore two avenues for overcoming these limitations. First, we propose a\nneural aligner for AMR that learns node-to-word alignments without relying on\ncomplex pipelines. We subsequently explore a tighter integration of aligner and\nparser training by considering a distribution over oracle action sequences\narising from aligner uncertainty. Empirical results show this approach leads to\nmore accurate alignments and generalization better from the AMR2.0 to AMR3.0\ncorpora. We attain a new state-of-the art for gold-only trained models,\nmatching silver-trained performance without the need for beam search on AMR3.0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Drozdov_A/0/1/0/all/0/1\">Andrew Drozdov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiawei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1\">Tahira Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Ramon Fernandez Astudillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Diversity in Dialogue with Natural Language Inference. (arXiv:2205.01497v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01497","description":"<p>Generating diverse, interesting responses to chitchat conversations is a\nproblem for neural conversational agents. This paper makes two substantial\ncontributions to improving diversity in dialogue generation. First, we propose\na novel metric which uses Natural Language Inference (NLI) to measure the\nsemantic diversity of a set of model responses for a conversation. We evaluate\nthis metric using an established framework (Tevet and Berant, 2021) and find\nstrong evidence indicating NLI Diversity is correlated with semantic diversity.\nSpecifically, we show that the contradiction relation is more useful than the\nneutral relation for measuring this diversity and that incorporating the NLI\nmodel's confidence achieves state-of-the-art results. Second, we demonstrate\nhow to iteratively improve the semantic diversity of a sampled set of responses\nvia a new generation procedure called Diversity Threshold Generation, which\nresults in an average 137% increase in NLI Diversity compared to standard\ngeneration procedures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stasaski_K/0/1/0/all/0/1\">Katherine Stasaski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearst_M/0/1/0/all/0/1\">Marti A. Hearst</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Learning for Natural Language Processing: A Survey. (arXiv:2205.01500v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01500","description":"<p>Deep learning has been the mainstream technique in natural language\nprocessing (NLP) area. However, the techniques require many labeled data and\nare less generalizable across domains. Meta-learning is an arising field in\nmachine learning studying approaches to learn better learning algorithms.\nApproaches aim at improving algorithms in various aspects, including data\nefficiency and generalizability. Efficacy of approaches has been shown in many\nNLP tasks, but there is no systematic survey of these approaches in NLP, which\nhinders more researchers from joining the field. Our goal with this survey\npaper is to offer researchers pointers to relevant meta-learning works in NLP\nand attract more attention from the NLP community to drive future innovation.\nThis paper first introduces the general concepts of meta-learning and the\ncommon approaches. Then we summarize task construction settings and application\nof meta-learning for various NLP problems and review the development of\nmeta-learning in NLP community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BasqueParl: A Bilingual Corpus of Basque Parliamentary Transcriptions. (arXiv:2205.01506v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01506","description":"<p>Parliamentary transcripts provide a valuable resource to understand the\nreality and know about the most important facts that occur over time in our\nsocieties. Furthermore, the political debates captured in these transcripts\nfacilitate research on political discourse from a computational social science\nperspective. In this paper we release the first version of a newly compiled\ncorpus from Basque parliamentary transcripts. The corpus is characterized by\nheavy Basque-Spanish code-switching, and represents an interesting resource to\nstudy political discourse in contrasting languages such as Basque and Spanish.\nWe enrich the corpus with metadata related to relevant attributes of the\nspeakers and speeches (language, gender, party...) and process the text to\nobtain named entities and lemmas. The obtained metadata is then used to perform\na detailed corpus analysis which provides interesting insights about the\nlanguage use of the Basque political representatives across time, parties and\ngender.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Escribano_N/0/1/0/all/0/1\">Nayla Escribano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Jon Ander Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orbegozo_Terradillos_J/0/1/0/all/0/1\">Julen Orbegozo-Terradillos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larrondo_Ureta_A/0/1/0/all/0/1\">Ainara Larrondo-Ureta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pena_Fernandez_S/0/1/0/all/0/1\">Sim&#xf3;n Pe&#xf1;a-Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_de_Vinaspre_O/0/1/0/all/0/1\">Olatz Perez-de-Vi&#xf1;aspre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models. (arXiv:2205.01523v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01523","description":"<p>Nowadays, pretrained language models (PLMs) have dominated the majority of\nNLP tasks. While, little research has been conducted on systematically\nevaluating the language abilities of PLMs. In this paper, we present a\nlarge-scale empirical study on general language ability evaluation of PLMs\n(ElitePLM). In our study, we design four evaluation dimensions, i.e. memory,\ncomprehension, reasoning, and composition, to measure ten widely-used PLMs\nwithin five categories. Our empirical results demonstrate that: (1) PLMs with\nvarying training objectives and strategies are good at different ability tests;\n(2) fine-tuning PLMs in downstream tasks is usually sensitive to the data size\nand distribution; (3) PLMs have excellent transferability between similar\ntasks. Moreover, the prediction results of PLMs in our experiments are released\nas an open resource for more deep and detailed analysis on the language\nabilities of PLMs. This paper can guide the future work to select, apply, and\ndesign PLMs for specific tasks. We have made all the details of experiments\npublicly available at https://github.com/RUCAIBox/ElitePLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhuohao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUBS: Subtree Substitution for Compositional Semantic Parsing. (arXiv:2205.01538v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01538","description":"<p>Although sequence-to-sequence models often achieve good performance in\nsemantic parsing for i.i.d. data, their performance is still inferior in\ncompositional generalization. Several data augmentation methods have been\nproposed to alleviate this problem. However, prior work only leveraged\nsuperficial grammar or rules for data augmentation, which resulted in limited\nimprovement. We propose to use subtree substitution for compositional data\naugmentation, where we consider subtrees with similar semantic functions as\nexchangeable. Our experiments showed that such augmented data led to\nsignificantly better performance on SCAN and GeoQuery, and reached new SOTA on\ncompositional split of GeoQuery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Fine-Tuning of BERT Models on the Edge. (arXiv:2205.01541v1 [cs.LG])","link":"http://arxiv.org/abs/2205.01541","description":"<p>Resource-constrained devices are increasingly the deployment targets of\nmachine learning applications. Static models, however, do not always suffice\nfor dynamic environments. On-device training of models allows for quick\nadaptability to new scenarios. With the increasing size of deep neural\nnetworks, as noted with the likes of BERT and other natural language processing\nmodels, comes increased resource requirements, namely memory, computation,\nenergy, and time. Furthermore, training is far more resource intensive than\ninference. Resource-constrained on-device learning is thus doubly difficult,\nespecially with large BERT-like models. By reducing the memory usage of\nfine-tuning, pre-trained BERT models can become efficient enough to fine-tune\non resource-constrained devices. We propose Freeze And Reconfigure (FAR), a\nmemory-efficient training regime for BERT-like models that reduces the memory\nusage of activation maps during fine-tuning by avoiding unnecessary parameter\nupdates. FAR reduces fine-tuning time on the DistilBERT model and CoLA dataset\nby 30%, and time spent on memory operations by 47%. More broadly, reductions in\nmetric performance on the GLUE and SQuAD datasets are around 1% on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vucetic_D/0/1/0/all/0/1\">Danilo Vucetic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tayaranian_M/0/1/0/all/0/1\">Mohammadreza Tayaranian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziaeefard_M/0/1/0/all/0/1\">Maryam Ziaeefard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">James J. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_B/0/1/0/all/0/1\">Brett H. Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gross_W/0/1/0/all/0/1\">Warren J. Gross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Transfer Prompts for Text Generation. (arXiv:2205.01543v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01543","description":"<p>Pretrained language models (PLMs) have made remarkable progress in text\ngeneration tasks via fine-tuning. While, it is challenging to fine-tune PLMs in\na data-scarce situation. Therefore, it is non-trivial to develop a general and\nlightweight model that can adapt to various text generation tasks based on\nPLMs. To fulfill this purpose, the recent prompt-based learning offers a\npotential solution. In this paper, we improve this technique and propose a\nnovel prompt-based method (PTG) for text generation in a transferable setting.\nFirst, PTG learns a set of source prompts for various source generation tasks\nand then transfers these prompts as target prompts to perform target generation\ntasks. To consider both task- and instance-level information, we design an\nadaptive attention mechanism to derive the target prompts. For each data\ninstance, PTG learns a specific target prompt by attending to highly relevant\nsource prompts. In extensive experiments, PTG yields competitive or better\nresults than fine-tuning methods. We release our source prompts as an open\nresource, where users can add or reuse them to improve new text generation\ntasks for future research. Code and data can be available at\nhttps://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptable Adapters. (arXiv:2205.01549v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01549","description":"<p>State-of-the-art pretrained NLP models contain a hundred million to trillion\nparameters. Adapters provide a parameter-efficient alternative for the full\nfinetuning in which we can only finetune lightweight neural network layers on\ntop of pretrained weights. Adapter layers are initialized randomly. However,\nexisting work uses the same adapter architecture -- i.e., the same adapter\nlayer on top of each layer of the pretrained model -- for every dataset,\nregardless of the properties of the dataset or the amount of available training\ndata. In this work, we introduce adaptable adapters that contain (1) learning\ndifferent activation functions for different layers and different input data,\nand (2) a learnable switch to select and only use the beneficial adapter\nlayers. We show that adaptable adapters achieve on-par performances with the\nstandard adapter architecture while using a considerably smaller number of\nadapter layers. In addition, we show that the selected adapter architecture by\nadaptable adapters transfers well across different data settings and similar\ntasks. We propose to use adaptable adapters for designing efficient and\neffective adapter architectures. The resulting adapters (a) contain about 50%\nof the learning parameters of the standard adapter and are therefore more\nefficient at training and inference, and require less storage space, and (b)\nachieve considerably higher performances in low-data settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_N/0/1/0/all/0/1\">Nafise Sadat Moosavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delfosse_Q/0/1/0/all/0/1\">Quentin Delfosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Mixed-Domain Translation Models via Federated Learning. (arXiv:2205.01557v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01557","description":"<p>Training mixed-domain translation models is a complex task that demands\ntailored architectures and costly data preparation techniques. In this work, we\nleverage federated learning (FL) in order to tackle the problem. Our\ninvestigation demonstrates that with slight modifications in the training\nprocess, neural machine translation (NMT) engines can be easily adapted when an\nFL-based aggregation is applied to fuse different domains. Experimental results\nalso show that engines built via FL are able to perform on par with\nstate-of-the-art baselines that rely on centralized training techniques. We\nevaluate our hypothesis in the presence of five datasets with different sizes,\nfrom different domains, to translate from German into English and discuss how\nFL and NMT can mutually benefit from each other. In addition to providing\nbenchmarking results on the union of FL and NMT, we also propose a novel\ntechnique to dynamically control the communication bandwidth by selecting\nimpactful parameters during FL updates. This is a significant achievement\nconsidering the large size of NMT engines that need to be exchanged between FL\nparties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passban_P/0/1/0/all/0/1\">Peyman Passban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roosta_T/0/1/0/all/0/1\">Tanya Roosta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Ankit Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_C/0/1/0/all/0/1\">Clement Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SparCAssist: A Model Risk Assessment Assistant Based on Sparse Generated Counterfactuals. (arXiv:2205.01588v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01588","description":"<p>We introduce SparcAssist, a general-purpose risk assessment tool for the\nmachine learning models trained for language tasks. It evaluates models' risk\nby inspecting their behavior on counterfactuals, namely out-of-distribution\ninstances generated based on the given data instance. The counterfactuals are\ngenerated by replacing tokens in rational subsequences identified by ExPred,\nwhile the replacements are retrieved using HotFlip or\nMasked-Language-Model-based algorithms. The main purpose of our system is to\nhelp the human annotators to assess the model's risk on deployment. The\ncounterfactual instances generated during the assessment are the by-product and\ncan be used to train more robust NLP models in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Setty_V/0/1/0/all/0/1\">Vinay Setty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparison of Approaches for Imbalanced Classification Problems in the Context of Retrieving Relevant Documents for an Analysis. (arXiv:2205.01600v1 [cs.IR])","link":"http://arxiv.org/abs/2205.01600","description":"<p>One of the first steps in many text-based social science studies is to\nretrieve documents that are relevant for the analysis from large corpora of\notherwise irrelevant documents. The conventional approach in social science to\naddress this retrieval task is to apply a set of keywords and to consider those\ndocuments to be relevant that contain at least one of the keywords. But the\napplication of incomplete keyword lists risks drawing biased inferences. More\ncomplex and costly methods such as query expansion techniques, topic\nmodel-based classification rules, and active as well as passive supervised\nlearning could have the potential to more accurately separate relevant from\nirrelevant documents and thereby reduce the potential size of bias. Yet,\nwhether applying these more expensive approaches increases retrieval\nperformance compared to keyword lists at all, and if so, by how much, is\nunclear as a comparison of these approaches is lacking. This study closes this\ngap by comparing these methods across three retrieval tasks associated with a\ndata set of German tweets (Linder, 2017), the Social Bias Inference Corpus\n(SBIC) (Sap et al., 2020), and the Reuters-21578 corpus (Lewis, 1997). Results\nshow that query expansion techniques and topic model-based classification rules\nin most studied settings tend to decrease rather than increase retrieval\nperformance. Active supervised learning, however, if applied on a not too small\nset of labeled training instances (e.g. 1,000 documents), reaches a\nsubstantially higher retrieval performance than keyword lists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wankmuller_S/0/1/0/all/0/1\">Sandra Wankm&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTM -- A Model for Large-Scale Multi-View Tweet Topic Classification. (arXiv:2205.01603v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01603","description":"<p>Automatically associating social media posts with topics is an important\nprerequisite for effective search and recommendation on many social media\nplatforms. However, topic classification of such posts is quite challenging\nbecause of (a) a large topic space (b) short text with weak topical cues, and\n(c) multiple topic associations per post. In contrast to most prior work which\nonly focuses on post classification into a small number of topics ($10$-$20$),\nwe consider the task of large-scale topic classification in the context of\nTwitter where the topic space is $10$ times larger with potentially multiple\ntopic associations per Tweet. We address the challenges above by proposing a\nnovel neural model, CTM that (a) supports a large topic space of $300$ topics\nand (b) takes a holistic approach to tweet content modeling -- leveraging\nmulti-modal content, author context, and deeper semantic cues in the Tweet. Our\nmethod offers an effective way to classify Tweets into topics at scale by\nyielding superior performance to other approaches (a relative lift of\n$\\mathbf{20}\\%$ in median average precision score) and has been successfully\ndeployed in production at Twitter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_V/0/1/0/all/0/1\">Vivek Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_K/0/1/0/all/0/1\">Kenny Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghighi_A/0/1/0/all/0/1\">Aria Haghighi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniKnight: Multilingual Neural Machine Translation with Language-Specific Self-Distillation. (arXiv:2205.01620v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01620","description":"<p>Although all-in-one-model multilingual neural machine translation (MNMT) has\nachieved remarkable progress in recent years, its selected best overall\ncheckpoint fails to achieve the best performance simultaneously in all language\npairs. It is because that the best checkpoints for each individual language\npair (i.e., language-specific best checkpoints) scatter in different epochs. In\nthis paper, we present a novel training strategy dubbed Language-Specific\nSelf-Distillation (LSSD) for bridging the gap between language-specific best\ncheckpoints and the overall best checkpoint. In detail, we regard each\nlanguage-specific best checkpoint as a teacher to distill the overall best\ncheckpoint. Moreover, we systematically explore three variants of our LSSD,\nwhich perform distillation statically, selectively, and adaptively.\nExperimental results on two widely-used benchmarks show that LSSD obtains\nconsistent improvements towards all language pairs and achieves the\nstate-of-the-art\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yichong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xinwei Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Training for High-Stakes Reliability. (arXiv:2205.01663v1 [cs.LG])","link":"http://arxiv.org/abs/2205.01663","description":"<p>In the future, powerful AI systems may be deployed in high-stakes settings,\nwhere a single failure could be catastrophic. One technique for improving AI\nsafety in high-stakes settings is adversarial training, which uses an adversary\nto generate examples to train on in order to achieve better worst-case\nperformance.\n</p>\n<p>In this work, we used a language generation task as a testbed for achieving\nhigh reliability through adversarial training. We created a series of\nadversarial training techniques -- including a tool that assists human\nadversaries -- to find and eliminate failures in a classifier that filters text\ncompletions suggested by a generator. In our simple \"avoid injuries\" task, we\ndetermined that we can set very conservative classifier thresholds without\nsignificantly impacting the quality of the filtered outputs. With our chosen\nthresholds, filtering with our baseline classifier decreases the rate of unsafe\ncompletions from about 2.4% to 0.003% on in-distribution data, which is near\nthe limit of our ability to measure. We found that adversarial training\nsignificantly increased robustness to the adversarial attacks that we trained\non, without affecting in-distribution performance. We hope to see further work\nin the high-stakes reliability setting, including more powerful tools for\nenhancing human adversaries and better ways to measure high levels of\nreliability, until we can confidently rule out the possibility of catastrophic\ndeployment-time failures of powerful models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1\">Daniel M. Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nix_S/0/1/0/all/0/1\">Seraphina Nix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_L/0/1/0/all/0/1\">Lawrence Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauman_T/0/1/0/all/0/1\">Tim Bauman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Nielsen_P/0/1/0/all/0/1\">Peter Schmidt-Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherlis_A/0/1/0/all/0/1\">Adam Scherlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabeshima_N/0/1/0/all/0/1\">Noa Nabeshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinstein_Raun_B/0/1/0/all/0/1\">Ben Weinstein-Raun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haas_D/0/1/0/all/0/1\">Daniel de Haas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1\">Buck Shlegeris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_N/0/1/0/all/0/1\">Nate Thomas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Modeling Hierarchical and Horizontal Features for Relational Triple Extraction. (arXiv:1908.08672v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1908.08672","description":"<p>Recent works on relational triple extraction have shown the superiority of\njointly extracting entities and relations over the pipelined extraction manner.\nHowever, most existing joint models fail to balance the modeling of entity\nfeatures and the joint decoding strategy, and thus the interactions between the\nentity level and triple level are not fully investigated. In this work, we\nfirst introduce the hierarchical dependency and horizontal commonality between\nthe two levels, and then propose an entity-enhanced dual tagging framework that\nenables the triple extraction (TE) task to utilize such interactions with\nself-learned entity features through an auxiliary entity extraction (EE) task,\nwithout breaking the joint decoding of relational triples. Specifically, we\nalign the EE and TE tasks in a position-wise manner by formulating them as two\nsequence labeling problems with identical encoder-decoder structure. Moreover,\nthe two tasks are organized in a carefully designed parameter sharing setting\nso that the learned entity features could be naturally shared via multi-task\nlearning. Empirical experiments on the NYT benchmark demonstrate the\neffectiveness of the proposed framework compared to the state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhepei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yantao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mohammad Javad Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimum projective linearizations of trees in linear time. (arXiv:2102.03277v5 [cs.DS] UPDATED)","link":"http://arxiv.org/abs/2102.03277","description":"<p>The Minimum Linear Arrangement problem (MLA) consists of finding a mapping\n$\\pi$ from vertices of a graph to distinct integers that minimizes\n$\\sum_{\\{u,v\\}\\in E}|\\pi(u) - \\pi(v)|$. In that setting, vertices are often\nassumed to lie on a horizontal line and edges are drawn as semicircles above\nsaid line. For trees, various algorithms are available to solve the problem in\npolynomial time in $n=|V|$. There exist variants of the MLA in which the\narrangements are constrained. Iordanskii, and later Hochberg and Stallmann\n(HS), put forward $O(n)$-time algorithms that solve the problem when\narrangements are constrained to be planar (also known as one-page book\nembeddings). We also consider linear arrangements of rooted trees that are\nconstrained to be projective (planar embeddings where the root is not covered\nby any edge). Gildea and Temperley (GT) sketched an algorithm for projective\narrangements which they claimed runs in $O(n)$ but did not provide any\njustification of its cost. In contrast, Park and Levy claimed that GT's\nalgorithm runs in $O(n \\log d_{max})$ where $d_{max}$ is the maximum degree but\ndid not provide sufficient detail. Here we correct an error in HS's algorithm\nfor the planar case, show its relationship with the projective case, and derive\nsimple algorithms for the projective and planar cases that run without a doubt\nin $O(n)$ time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Juan Luis Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extended Parallel Corpus for Amharic-English Machine Translation. (arXiv:2104.03543v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.03543","description":"<p>This paper describes the acquisition, preprocessing, segmentation, and\nalignment of an Amharic-English parallel corpus. It will be helpful for machine\ntranslation of a low-resource language, Amharic. We freely released the corpus\nfor research purposes. Furthermore, we developed baseline statistical and\nneural machine translation systems; we trained statistical and neural machine\ntranslation models using the corpus. In the experiments, we also used a large\nmonolingual corpus for the language model of statistical machine translation\nand back-translation of neural machine translation. In the automatic\nevaluation, neural machine translation models outperform statistical machine\ntranslation models by approximately six to seven Bilingual Evaluation\nUnderstudy (BLEU) points. Besides, among the neural machine translation models,\nthe subword models outperform the word-based models by three to four BLEU\npoints. Moreover, two other relevant automatic evaluation metrics, Translation\nEdit Rate on Character Level and Better Evaluation as Ranking, reflect\ncorresponding differences among the trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gezmu_A/0/1/0/all/0/1\">Andargachew Mekonnen Gezmu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bati_T/0/1/0/all/0/1\">Tesfaye Bayu Bati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Restoring Hebrew Diacritics Without a Dictionary. (arXiv:2105.05209v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.05209","description":"<p>We demonstrate that it is feasible to diacritize Hebrew script without any\nhuman-curated resources other than plain diacritized text. We present NAKDIMON,\na two-layer character level LSTM, that performs on par with much more\ncomplicated curation-dependent systems, across a diverse array of modern Hebrew\nsources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gershuni_E/0/1/0/all/0/1\">Elazar Gershuni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should We Trust This Summary? Bayesian Abstractive Summarization to The Rescue. (arXiv:2105.10155v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.10155","description":"<p>We explore the notion of uncertainty in the context of modern abstractive\nsummarization models, using the tools of Bayesian Deep Learning. Our approach\napproximates Bayesian inference by first extending state-of-the-art\nsummarization models with Monte Carlo dropout and then using them to perform\nmultiple stochastic forward passes. Based on Bayesian inference we are able to\neffectively quantify uncertainty at prediction time. Having a reliable\nuncertainty measure, we can improve the experience of the end user by filtering\nout generated summaries of high uncertainty. Furthermore, uncertainty\nestimation could be used as a criterion for selecting samples for annotation,\nand can be paired nicely with active learning and human-in-the-loop approaches.\nFinally, Bayesian inference enables us to find a Bayesian summary which\nperforms better than a deterministic one and is more robust to uncertainty. In\npractice, we show that our Variational Bayesian equivalents of BART and PEGASUS\ncan outperform their deterministic counterparts on multiple benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gidiotis_A/0/1/0/all/0/1\">Alexios Gidiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Speech Recognition. (arXiv:2105.11084v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.11084","description":"<p>Despite rapid progress in the recent past, current speech recognition systems\nstill require labeled training data which limits this technology to a small\nfraction of the languages spoken around the globe. This paper describes\nwav2vec-U, short for wav2vec Unsupervised, a method to train speech recognition\nmodels without any labeled data. We leverage self-supervised speech\nrepresentations to segment unlabeled audio and learn a mapping from these\nrepresentations to phonemes via adversarial training. The right representations\nare key to the success of our method. Compared to the best previous\nunsupervised work, wav2vec-U reduces the phoneme error rate on the TIMIT\nbenchmark from 26.1 to 11.3. On the larger English Librispeech benchmark,\nwav2vec-U achieves a word error rate of 5.9 on test-other, rivaling some of the\nbest published systems trained on 960 hours of labeled data from only two years\nago. We also experiment on nine other languages, including low-resource\nlanguages such as Kyrgyz, Swahili and Tatar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Identification of Dementia from Transcripts using Transformer Networks. (arXiv:2109.06980v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06980","description":"<p>Alzheimer's disease (AD) is the main cause of dementia which is accompanied\nby loss of memory and may lead to severe consequences in peoples' everyday life\nif not diagnosed on time. Very few works have exploited transformer-based\nnetworks and despite the high accuracy achieved, little work has been done in\nterms of model interpretability. In addition, although Mini-Mental State Exam\n(MMSE) scores are inextricably linked with the identification of dementia,\nresearch works face the task of dementia identification and the task of the\nprediction of MMSE scores as two separate tasks. In order to address these\nlimitations, we employ several transformer-based models, with BERT achieving\nthe highest accuracy accounting for 87.50%. Concurrently, we propose an\ninterpretable method to detect AD patients based on siamese networks reaching\naccuracy up to 83.75%. Next, we introduce two multi-task learning models, where\nthe main task refers to the identification of dementia (binary classification),\nwhile the auxiliary one corresponds to the identification of the severity of\ndementia (multiclass classification). Our model obtains accuracy equal to\n86.25% on the detection of AD patients in the multi-task learning setting.\nFinally, we present some new methods to identify the linguistic patterns used\nby AD patients and non-AD ones, including text statistics, vocabulary\nuniqueness, word usage, correlations via a detailed linguistic analysis, and\nexplainability techniques (LIME). Findings indicate significant differences in\nlanguage between AD and non-AD patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Training with Differentiable Teacher. (arXiv:2109.07049v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07049","description":"<p>Self-training achieves enormous success in various semi-supervised and\nweakly-supervised learning tasks. The method can be interpreted as a\nteacher-student framework, where the teacher generates pseudo-labels, and the\nstudent makes predictions. The two models are updated alternatingly. However,\nsuch a straightforward alternating update rule leads to training instability.\nThis is because a small change in the teacher may result in a significant\nchange in the student. To address this issue, we propose DRIFT, short for\ndifferentiable self-training, that treats teacher-student as a Stackelberg\ngame. In this game, a leader is always in a more advantageous position than a\nfollower. In self-training, the student contributes to the prediction\nperformance, and the teacher controls the training process by generating\npseudo-labels. Therefore, we treat the student as the leader and the teacher as\nthe follower. The leader procures its advantage by acknowledging the follower's\nstrategy, which involves differentiable pseudo-labels and differentiable sample\nweights. Consequently, the leader-follower interaction can be effectively\ncaptured via Stackelberg gradient, obtained by differentiating the follower's\nstrategy. Experimental results on semi- and weakly-supervised classification\nand named entity recognition tasks show that our model outperforms existing\napproaches by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Er_S/0/1/0/all/0/1\">Siawpeng Er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1\">Hongyuan Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaICL: Learning to Learn In Context. (arXiv:2110.15943v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15943","description":"<p>We introduce MetaICL (Meta-training for In-Context Learning), a new\nmeta-training framework for few-shot learning where a pretrained language model\nis tuned to do in-context learning on a large set of training tasks. This\nmeta-training enables the model to more effectively learn a new task in context\nat test time, by simply conditioning on a few training examples with no\nparameter updates or task-specific templates. We experiment on a large, diverse\ncollection of tasks consisting of 142 NLP datasets including classification,\nquestion answering, natural language inference, paraphrase detection and more,\nacross seven different meta-training/target splits. MetaICL outperforms a range\nof baselines including in-context learning without meta-training and multi-task\nlearning followed by zero-shot transfer. We find that the gains are\nparticularly significant for target tasks that have domain shifts from the\nmeta-training tasks, and that using a diverse set of the meta-training tasks is\nkey to improvements. We also show that MetaICL approaches (and sometimes beats)\nthe performance of models fully finetuned on the target task, and outperforms\nmuch bigger models with nearly 8x parameters. Finally, we show that MetaICL is\ncomplementary to human-written instructions, and the best performance can be\nachieved by combining both approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieve-then-extract Based Knowledge Graph Querying Using Graph Neural Networks. (arXiv:2111.10541v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10541","description":"<p>The abstract of Retrieve-then-extract Based Knowledge Graph Querying Using\nGraph Neural Networks will be updated here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hanning Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Po Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Conversational Data using Discourse Mutual Information Maximization. (arXiv:2112.05787v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.05787","description":"<p>Although many pretrained models exist for text or images, there have been\nrelatively fewer attempts to train representations specifically for dialog\nunderstanding. Prior works usually relied on finetuned representations based on\ngeneric text representation models like BERT or GPT-2. But such language\nmodeling pretraining objectives do not take the structural information of\nconversational text into consideration. Although generative dialog models can\nlearn structural features too, we argue that the structure-unaware word-by-word\ngeneration is not suitable for effective conversation modeling. We empirically\ndemonstrate that such representations do not perform consistently across\nvarious dialog understanding tasks. Hence, we propose a structure-aware Mutual\nInformation based loss-function DMI (Discourse Mutual Information) for training\ndialog-representation models, that additionally captures the inherent\nuncertainty in response prediction. Extensive evaluation on nine diverse dialog\nmodeling tasks shows that our proposed DMI-based models outperform strong\nbaselines by significant margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santra_B/0/1/0/all/0/1\">Bishal Santra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sumegh Roychowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_A/0/1/0/all/0/1\">Aishik Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurram_V/0/1/0/all/0/1\">Vasu Gurram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Atharva Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASCEND: A Spontaneous Chinese-English Dataset for Code-switching in Multi-turn Conversation. (arXiv:2112.06223v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06223","description":"<p>Code-switching is a speech phenomenon occurring when a speaker switches\nlanguage during a conversation. Despite the spontaneous nature of\ncode-switching in conversational spoken language, most existing works collect\ncode-switching data from read speech instead of spontaneous speech. ASCEND (A\nSpontaneous Chinese-English Dataset) is a high-quality Mandarin Chinese-English\ncode-switching corpus built on spontaneous multi-turn conversational dialogue\nsources collected in Hong Kong. We report ASCEND's design and procedure for\ncollecting the speech data, including annotations. ASCEND consists of 10.62\nhours of clean speech, collected from 23 bilingual speakers of Chinese and\nEnglish. Furthermore, we conduct baseline experiments using pre-trained wav2vec\n2.0 models, achieving a best performance of 22.69\\% character error rate and\n27.05% mixed error rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1\">Elham J. Barezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bertram E. Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Massive-scale Decoding for Text Generation using Lattices. (arXiv:2112.07660v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07660","description":"<p>Conditional neural text generation models generate high-quality outputs, but\noften concentrate around a mode when what we really want is a diverse set of\noptions. We present a search algorithm to construct lattices encoding a massive\nnumber of generation options. First, we restructure decoding as a best-first\nsearch, which explores the space differently than beam search and improves\nefficiency by avoiding pruning paths. Second, we revisit the idea of hypothesis\nrecombination: we can identify pairs of similar generation candidates during\nsearch and merge them as an approximation. On both summarization and machine\ntranslation, we show that our algorithm encodes thousands of diverse options\nthat remain grammatical and high-quality into one lattice. This algorithm\nprovides a foundation for building downstream generation applications on top of\nmassive-scale diverse outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonnalagadda_S/0/1/0/all/0/1\">Siddhartha Reddy Jonnalagadda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maximum Bayes Smatch Ensemble Distillation for AMR Parsing. (arXiv:2112.07790v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07790","description":"<p>AMR parsing has experienced an unprecendented increase in performance in the\nlast three years, due to a mixture of effects including architecture\nimprovements and transfer learning. Self-learning techniques have also played a\nrole in pushing performance forward. However, for most recent high performant\nparsers, the effect of self-learning and silver data augmentation seems to be\nfading. In this paper we propose to overcome this diminishing returns of silver\ndata by combining Smatch-based ensembling techniques with ensemble\ndistillation. In an extensive experimental setup, we push single model English\nparser performance to a new state-of-the-art, 85.9 (AMR2.0) and 84.3 (AMR3.0),\nand return to substantial gains from silver data augmentation. We also attain a\nnew state-of-the-art for cross-lingual AMR parsing for Chinese, German, Italian\nand Spanish. Finally we explore the impact of the proposed technique on domain\nadaptation, and show that it can produce gains rivaling those of human\nannotated data for QALD-9 and achieve a new state-of-the-art for BioAMR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Young-Suk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Ramon Fernandez Astudillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Thanh Lam Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1\">Tahira Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1\">Salim Roukos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LongT5: Efficient Text-To-Text Transformer for Long Sequences. (arXiv:2112.07916v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07916","description":"<p>Recent work has shown that either (1) increasing the input length or (2)\nincreasing model size can improve the performance of Transformer-based neural\nmodels. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same\ntime. Specifically, we integrated attention ideas from long-input transformers\n(ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention\nmechanism we call {\\em Transient Global} (TGlobal), which mimics ETC's\nlocal/global attention mechanism, but without requiring additional side-inputs.\nWe are able to achieve state-of-the-art results on several summarization tasks\nand outperform the original T5 models on question answering tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1\">David Uthus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Ontanon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yun-Hsuan Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter-COMMs: Detecting Climate, COVID, and Military Multimodal Misinformation. (arXiv:2112.08594v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08594","description":"<p>Detecting out-of-context media, such as \"mis-captioned\" images on Twitter, is\na relevant problem, especially in domains of high public significance. In this\nwork we aim to develop defenses against such misinformation for the topics of\nClimate Change, COVID-19, and Military Vehicles. We first present a large-scale\nmultimodal dataset with over 884k tweets relevant to these topics. Next, we\npropose a detection method, based on the state-of-the-art CLIP model, that\nleverages automatically generated hard image-text mismatches. While this\napproach works well on our automatically constructed out-of-context tweets, we\naim to validate its usefulness on data representative of the real world. Thus,\nwe test it on a set of human-generated fakes created by mimicking in-the-wild\nmisinformation. We achieve an 11% detection improvement in a high precision\nregime over a strong baseline. Finally, we share insights about our best model\ndesign and analyze the challenges of this emerging threat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biamby_G/0/1/0/all/0/1\">Giscard Biamby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Grace Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Hierarchical Domain Adaptation for Pretrained Language Models. (arXiv:2112.08786v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08786","description":"<p>The remarkable success of large language models has been driven by dense\nmodels trained on massive unlabeled, unstructured corpora. These corpora\ntypically contain text from diverse, heterogeneous sources, but information\nabout the source of the text is rarely used during training. Transferring their\nknowledge to a target domain is typically done by continuing training\nin-domain. In this paper, we introduce a method to permit domain adaptation to\nmany diverse domains using a computationally efficient adapter approach. Our\nmethod is based on the observation that textual domains are partially\noverlapping, and we represent domains as a hierarchical tree structure where\neach node in the tree is associated with a set of adapter weights. When\ncombined with a frozen pretrained language model, this approach enables\nparameter sharing among related domains, while avoiding negative interference\nbetween unrelated ones. Experimental results with GPT-2 and a large fraction of\nthe 100 most represented websites in C4 show across-the-board improvements\nin-domain. We additionally provide an inference time algorithm for a held-out\ndomain and show that averaging over multiple paths through the tree enables\nfurther gains in generalization, while adding only a marginal cost to\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chronopoulou_A/0/1/0/all/0/1\">Alexandra Chronopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AcTune: Uncertainty-aware Active Self-Training for Semi-Supervised Active Learning with Pretrained Language Models. (arXiv:2112.08787v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08787","description":"<p>While pre-trained language model (PLM) fine-tuning has achieved strong\nperformance in many NLP tasks, the fine-tuning stage can be still demanding in\nlabeled data. Recent works have resorted to active fine-tuning to improve the\nlabel efficiency of PLM fine-tuning, but none of them investigate the potential\nof unlabeled data. We propose {\\ours}, a new framework that leverages unlabeled\ndata to improve the label efficiency of active PLM fine-tuning. AcTune switches\nbetween data annotation and model self-training based on uncertainty: it\nselects high-uncertainty unlabeled samples for active annotation and\nlow-uncertainty ones for model self-training. Under this framework, we design\n(1) a region-aware sampling strategy that reduces redundancy when actively\nquerying for annotations and (2) a momentum-based memory bank that dynamically\naggregates the model's pseudo labels to suppress label noise in self-training.\nExperiments on 6 text classification datasets show that AcTune outperforms the\nstrongest active learning and self-training baselines and improves the label\nefficiency of PLM fine-tuning by 56.2\\% on average. Our implementation will be\navailable at \\url{https://github.com/yueyu1030/actune}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingkai Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer. (arXiv:2112.08995v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2112.08995","description":"<p>Machines that can represent and describe environmental soundscapes have\npractical potential, e.g., for audio tagging and captioning systems. Prevailing\nlearning paradigms have been relying on parallel audio-text data, which is,\nhowever, scarcely available on the web. We propose VIP-ANT that induces\n\\textbf{A}udio-\\textbf{T}ext alignment without using any parallel audio-text\ndata. Our key idea is to share the image modality between bi-modal image-text\nrepresentations and bi-modal image-audio representations; the image modality\nfunctions as a pivot and connects audio and text in a tri-modal embedding space\nimplicitly.\n</p>\n<p>In a difficult zero-shot setting with no paired audio-text data, our model\ndemonstrates state-of-the-art zero-shot performance on the ESC50 and US8K audio\nclassification tasks, and even surpasses the supervised state of the art for\nClotho caption retrieval (with audio queries) by 2.2\\% R@1. We further\ninvestigate cases of minimal audio-text supervision, finding that, e.g., just a\nfew hundred supervised audio-text pairs increase the zero-shot audio\nclassification accuracy by 8\\% on US8K. However, to match human parity on some\nzero-shot tasks, our empirical scaling experiments suggest that we would need\nabout $2^{21} \\approx 2M$ supervised audio-caption pairs. Our work opens up new\navenues for learning audio-text connections with little to no parallel\naudio-text data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DigNet: Digging Clues from Local-Global Interactive Graph for Aspect-level Sentiment Classification. (arXiv:2201.00989v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.00989","description":"<p>In aspect-level sentiment classification (ASC), state-of-the-art models\nencode either syntax graph or relation graph to capture the local syntactic\ninformation or global relational information. Despite the advantages of syntax\nand relation graphs, they have respective shortages which are neglected,\nlimiting the representation power in the graph modeling process. To resolve\ntheir limitations, we design a novel local-global interactive graph, which\nmarries their advantages by stitching the two graphs via interactive edges. To\nmodel this local-global interactive graph, we propose a novel neural network\ntermed DigNet, whose core module is the stacked local-global interactive (LGI)\nlayers performing two processes: intra-graph message passing and cross-graph\nmessage passing. In this way, the local syntactic and global relational\ninformation can be reconciled as a whole in understanding the aspect-level\nsentiment. Concretely, we design two variants of local-global interactive\ngraphs with different kinds of interactive edges and three variants of LGI\nlayers. We conduct experiments on several public benchmark datasets and the\nresults show that we outperform previous best scores by 3\\%, 2.32\\%, and 6.33\\%\nin terms of Macro-F1 on Lap14, Res14, and Res15 datasets, respectively,\nconfirming the effectiveness and superiority of the proposed local-global\ninteractive graph and DigNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1\">Bowen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor Tsang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LARD: Large-scale Artificial Disfluency Generation. (arXiv:2201.05041v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05041","description":"<p>Disfluency detection is a critical task in real-time dialogue systems.\nHowever, despite its importance, it remains a relatively unexplored field,\nmainly due to the lack of appropriate datasets. At the same time, existing\ndatasets suffer from various issues, including class imbalance issues, which\ncan significantly affect the performance of the model on rare classes, as it is\ndemonstrated in this paper. To this end, we propose LARD, a method for\ngenerating complex and realistic artificial disfluencies with little effort.\nThe proposed method can handle three of the most common types of disfluencies:\nrepetitions, replacements and restarts. In addition, we release a new\nlarge-scale dataset with disfluencies that can be used on four different tasks:\ndisfluency detection, classification, extraction and correction. Experimental\nresults on the LARD dataset demonstrate that the data produced by the proposed\nmethod can be effectively used for detecting and removing disfluencies, while\nalso addressing limitations of existing datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passali_T/0/1/0/all/0/1\">T. Passali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavropoulos_T/0/1/0/all/0/1\">T. Mavropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">G. Tsoumakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meditskos_G/0/1/0/all/0/1\">G. Meditskos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vrochidis_S/0/1/0/all/0/1\">S. Vrochidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text characterization based on recurrence networks. (arXiv:2201.06665v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06665","description":"<p>Several complex systems are characterized by presenting intricate\ncharacteristics taking place at several scales of time and space. These\nmultiscale characterizations are used in various applications, including better\nunderstanding diseases, characterizing transportation systems, and comparison\nbetween cities, among others. In particular, texts are also characterized by a\nhierarchical structure that can be approached by using multi-scale concepts and\nmethods. The multiscale properties of texts constitute a subject worth further\ninvestigation. In addition, more effective approaches to text characterization\nand analysis can be obtained by emphasizing words with potentially more\ninformational content. The present work aims at developing these possibilities\nwhile focusing on mesoscopic representations of networks. More specifically, we\nadopt an extension to the mesoscopic approach to represent text narratives, in\nwhich only the recurrent relationships among tagged parts of speech (subject,\nverb and direct object) are considered to establish connections among\nsequential pieces of text (e.g., paragraphs). The characterization of the texts\nwas then achieved by considering scale-dependent complementary methods:\naccessibility, symmetry and recurrence signatures. In order to evaluate the\npotential of these concepts and methods, we approached the problem of\ndistinguishing between literary genres (fiction and non-fiction). A set of 300\nbooks organized into the two genres was considered and were compared by using\nthe aforementioned approaches. All the methods were capable of differentiating\nto some extent between the two genres. The accessibility and symmetry reflected\nthe narrative asymmetries, while the recurrence signature provided a more\ndirect indication about the non-sequential semantic connections taking place\nalong the narrative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souza_B/0/1/0/all/0/1\">B&#xe1;rbara C. e Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_F/0/1/0/all/0/1\">Filipi N. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arruda_H/0/1/0/all/0/1\">Henrique F. de Arruda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_G/0/1/0/all/0/1\">Giovana D. da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1\">Luciano da F. Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amancio_D/0/1/0/all/0/1\">Diego R. Amancio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TableFormer: Robust Transformer Modeling for Table-Text Encoding. (arXiv:2203.00274v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00274","description":"<p>Understanding tables is an important aspect of natural language\nunderstanding. Existing models for table understanding require linearization of\nthe table structure, where row or column order is encoded as an unwanted bias.\nSuch spurious biases make the model vulnerable to row and column order\nperturbations. Additionally, prior work has not thoroughly modeled the table\nstructures or table-text alignments, hindering the table-text understanding\nability. In this work, we propose a robust and structurally aware table-text\nencoding architecture TableFormer, where tabular structural biases are\nincorporated completely through learnable attention biases. TableFormer is (1)\nstrictly invariant to row and column orders, and, (2) could understand tables\nbetter due to its tabular inductive biases. Our evaluations showed that\nTableFormer outperforms strong baselines in all settings on SQA, WTQ and\nTabFact table reasoning datasets, and achieves state-of-the-art performance on\nSQA, especially when facing answer-invariant row and column order perturbations\n(6% improvement over the best baseline), because previous SOTA models'\nperformance drops by 4% - 6% when facing such perturbations while TableFormer\nis not affected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aditya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Shyam Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Luheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Shachi Paul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01322","description":"<p>Understanding visual question answering is going to be crucial for numerous\nhuman activities. However, it presents major challenges at the heart of the\nartificial intelligence endeavor. This paper presents an update on the rapid\nadvancements in visual question answering using images that have occurred in\nthe last couple of years. Tremendous growth in research on improving visual\nquestion answering system architecture has been published recently, showing the\nimportance of multimodal architectures. Several points on the benefits of\nvisual question answering are mentioned in the review paper by Manmadhan et al.\n(2020), on which the present article builds, including subsequent updates in\nthe field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kodali_V/0/1/0/all/0/1\">Venkat Kodali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Free Attentive Scoring for Speaker Verification. (arXiv:2203.05642v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.05642","description":"<p>This paper presents a novel study of parameter-free attentive scoring for\nspeaker verification. Parameter-free scoring provides the flexibility of\ncomparing speaker representations without the need of an accompanying\nparametric scoring model. Inspired by the attention component in Transformer\nneural networks, we propose a variant of the scaled dot product attention\nmechanism to compare enrollment and test segment representations. In addition,\nthis work explores the effect on performance of (i) different types of\nnormalization, (ii) independent versus tied query/key estimation, (iii) varying\nthe number of key-value pairs and (iv) pooling multiple enrollment utterance\nstatistics. Experimental results for a 4 task average show that a simple\nparameter-free attentive scoring mechanism can improve the average EER by 10%\nover the best cosine similarity baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelecanos_J/0/1/0/all/0/1\">Jason Pelecanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yiling Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_I/0/1/0/all/0/1\">Ignacio Lopez Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection. (arXiv:2203.09509v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09509","description":"<p>Toxic language detection systems often falsely flag text that contains\nminority group mentions as toxic, as those groups are often the targets of\nonline hate. Such over-reliance on spurious correlations also causes systems to\nstruggle with detecting implicitly toxic language. To help mitigate these\nissues, we create ToxiGen, a new large-scale and machine-generated dataset of\n274k toxic and benign statements about 13 minority groups. We develop a\ndemonstration-based prompting framework and an adversarial\nclassifier-in-the-loop decoding method to generate subtly toxic and benign text\nwith a massive pretrained language model. Controlling machine generation in\nthis way allows ToxiGen to cover implicitly toxic text at a larger scale, and\nabout more demographic groups, than previous resources of human-written text.\nWe conduct a human evaluation on a challenging subset of ToxiGen and find that\nannotators struggle to distinguish machine-generated text from human-written\nlanguage. We also find that 94.5% of toxic examples are labeled as hate speech\nby human annotators. Using three publicly-available datasets, we show that\nfinetuning a toxicity classifier on our data improves its performance on\nhuman-written data substantially. We also demonstrate that ToxiGen can be used\nto fight machine-generated toxicity as finetuning improves the classifier\nsignificantly on our evaluation subset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartvigsen_T/0/1/0/all/0/1\">Thomas Hartvigsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1\">Saadia Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_D/0/1/0/all/0/1\">Dipankar Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZS4IE: A toolkit for Zero-Shot Information Extraction with simple Verbalizations. (arXiv:2203.13602v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13602","description":"<p>The current workflow for Information Extraction (IE) analysts involves the\ndefinition of the entities/relations of interest and a training corpus with\nannotated examples. In this demonstration we introduce a new workflow where the\nanalyst directly verbalizes the entities/relations, which are then used by a\nTextual Entailment model to perform zero-shot IE. We present the design and\nimplementation of a toolkit with a user interface, as well as experiments on\nfour IE tasks that show that the system achieves very good performance at\nzero-shot learning using only 5--15 minutes per type of a user's effort. Our\ndemonstration system is open-sourced at https://github.com/BBN-E/ZS4IE . A\ndemonstration video is available at https://vimeo.<a href=\"/abs/com/6761383\">com/6761383</a>40 .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sainz_O/0/1/0/all/0/1\">Oscar Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Haoling Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1\">Oier Lopez de Lacalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1\">Bonan Min</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Pruning Learns Compact and Accurate Models. (arXiv:2204.00408v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00408","description":"<p>The growing size of neural language models has led to increased attention in\nmodel compression. The two predominant approaches are pruning, which gradually\nremoves weights from a pre-trained model, and distillation, which trains a\nsmaller compact model to match a larger one. Pruning methods can significantly\nreduce the model size but hardly achieve large speedups as distillation.\nHowever, distillation methods require large amounts of unlabeled data and are\nexpensive to train. In this work, we propose a task-specific structured pruning\nmethod CoFi (Coarse- and Fine-grained Pruning), which delivers highly\nparallelizable subnetworks and matches the distillation methods in both\naccuracy and latency, without resorting to any unlabeled data. Our key insight\nis to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads\nand hidden units) modules, which controls the pruning decision of each\nparameter with masks of different granularity. We also devise a layerwise\ndistillation strategy to transfer knowledge from unpruned to pruned models\nduring optimization. Our experiments on GLUE and SQuAD datasets show that CoFi\nyields models with over 10x speedups with a small accuracy drop, showing its\neffectiveness and efficiency compared to previous pruning and distillation\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Mengzhou Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MINER: Improving Out-of-Vocabulary Named Entity Recognition from an Information Theoretic Perspective. (arXiv:2204.04391v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04391","description":"<p>NER model has achieved promising performance on standard NER benchmarks.\nHowever, recent studies show that previous approaches may over-rely on entity\nmention information, resulting in poor performance on out-of-vocabulary (OOV)\nentity recognition. In this work, we propose MINER, a novel NER learning\nframework, to remedy this issue from an information-theoretic perspective. The\nproposed approach contains two mutual information-based training objectives: i)\ngeneralizing information maximization, which enhances representation via deep\nunderstanding of context and entity surface forms; ii) superfluous information\nminimization, which discourages representation from rote memorizing entity\nnames or exploiting biased cues in data. Experiments on various settings and\ndatasets demonstrate that it achieves better performance in predicting OOV\nentities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shihan Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Limao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Liang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuS: Neutral Multi-News Summarization for Mitigating Framing Bias. (arXiv:2204.04902v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04902","description":"<p>Media news framing bias can increase political polarization and undermine\ncivil society. The need for automatic mitigation methods is therefore growing.\nWe propose a new task, a neutral summary generation from multiple news articles\nof the varying political leanings to facilitate balanced and unbiased news\nreading. In this paper, we first collect a new dataset, illustrate insights\nabout framing bias through a case study, and propose a new effective metric and\nmodel (NeuS-TITLE) for the task. Based on our discovery that title provides a\ngood signal for framing bias, we present NeuS-TITLE that learns to neutralize\nnews content in hierarchical order from title to article. Our hierarchical\nmulti-task learning is achieved by formatting our hierarchical data pair\n(title, article) sequentially with identifier-tokens (\"TITLE=&gt;\", \"ARTICLE=&gt;\")\nand fine-tuning the auto-regressive decoder with the standard negative\nlog-likelihood objective. We then analyze and point out the remaining\nchallenges and future directions. One of the most interesting observations is\nthat neural NLG models can hallucinate not only factually inaccurate or\nunverifiable content but also politically biased content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRUE: Re-evaluating Factual Consistency Evaluation. (arXiv:2204.04991v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04991","description":"<p>Grounded text generation systems often generate text that contains factual\ninconsistencies, hindering their real-world applicability. Automatic factual\nconsistency evaluation may help alleviate this limitation by accelerating\nevaluation cycles, filtering inconsistent outputs and augmenting training data.\nWhile attracting increasing attention, such evaluation metrics are usually\ndeveloped and evaluated in silo for a single task or dataset, slowing their\nadoption. Moreover, previous meta-evaluation protocols focused on system-level\ncorrelations with human annotations, which leave the example-level accuracy of\nsuch metrics unclear. In this work, we introduce TRUE: a comprehensive survey\nand assessment of factual consistency metrics on a standardized collection of\nexisting texts from diverse tasks, manually annotated for factual consistency.\nOur standardization enables an example-level meta-evaluation protocol that is\nmore actionable and interpretable than previously reported correlations,\nyielding clearer quality measures. Across diverse state-of-the-art metrics and\n11 datasets we find that large-scale NLI and question\ngeneration-and-answering-based approaches achieve strong and complementary\nresults. We recommend those methods as a starting point for model and metric\ndevelopers, and hope TRUE will foster progress towards even better evaluation\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Honovich_O/0/1/0/all/0/1\">Or Honovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taitelbaum_H/0/1/0/all/0/1\">Hagai Taitelbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukliansy_D/0/1/0/all/0/1\">Doron Kukliansy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_V/0/1/0/all/0/1\">Vered Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1\">Idan Szpektor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassidim_A/0/1/0/all/0/1\">Avinatan Hassidim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1\">Yossi Matias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. (arXiv:2204.05991v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05991","description":"<p>Training a referring expression comprehension (ReC) model for a new visual\ndomain requires collecting referring expressions, and potentially corresponding\nbounding boxes, for images in the domain. While large-scale pre-trained models\nare useful for image classification across domains, it remains unclear if they\ncan be applied in a zero-shot manner to more complex tasks like ReC. We present\nReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a\nstate-of-the-art large-scale model, for ReC. Motivated by the close connection\nbetween ReC and CLIP's contrastive pre-training objective, the first component\nof ReCLIP is a region-scoring method that isolates object proposals via\ncropping and blurring, and passes them to CLIP. However, through controlled\nexperiments on a synthetic dataset, we find that CLIP is largely incapable of\nperforming spatial reasoning off-the-shelf. Thus, the second component of\nReCLIP is a spatial relation resolver that handles several types of spatial\nrelations. We reduce the gap between zero-shot baselines from prior work and\nsupervised models by as much as 29% on RefCOCOg, and on RefGTA (video game\nimagery), ReCLIP's relative improvement over supervised ReC models trained on\nreal images is 8%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Sanjay Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Cluster-Based k-Nearest-Neighbor Machine Translation. (arXiv:2204.06175v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06175","description":"<p>k-Nearest-Neighbor Machine Translation (kNN-MT) has been recently proposed as\na non-parametric solution for domain adaptation in neural machine translation\n(NMT). It aims to alleviate the performance degradation of advanced MT systems\nin translating out-of-domain sentences by coordinating with an additional\ntoken-level feature-based retrieval module constructed from in-domain data.\nPrevious studies have already demonstrated that non-parametric NMT is even\nsuperior to models fine-tuned on out-of-domain data. In spite of this success,\nkNN retrieval is at the expense of high latency, in particular for large\ndatastores. To make it practical, in this paper, we explore a more efficient\nkNN-MT and propose to use clustering to improve the retrieval efficiency.\nConcretely, we first propose a cluster-based Compact Network for feature\nreduction in a contrastive learning manner to compress context features into\n90+% lower dimensional vectors. We then suggest a cluster-based pruning\nsolution to filter out 10%-40% redundant nodes in large datastores while\nretaining translation quality. Our proposed methods achieve better or\ncomparable performance while reducing up to 57% inference latency against the\nadvanced non-parametric MT model on several machine translation benchmarks.\nExperimental results indicate that the proposed methods maintain the most\nuseful information of the original datastore and the Compact Network shows good\ngeneralization on unseen domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1\">Kai Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding. (arXiv:2204.07316v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07316","description":"<p>Transformer-based models are widely used in natural language understanding\n(NLU) tasks, and multimodal transformers have been effective in visual-language\ntasks. This study explores distilling visual information from pretrained\nmultimodal transformers to pretrained language encoders. Our framework is\ninspired by cross-modal encoders' success in visual-language tasks while we\nalter the learning objective to cater to the language-heavy characteristics of\nNLU. After training with a small number of extra adapting steps and finetuned,\nthe proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in\ngeneral language understanding evaluation (GLUE), situations with adversarial\ngenerations (SWAG) benchmarks, and readability benchmarks. We analyze the\nperformance of XDBERT on GLUE to show that the improvement is likely visually\ngrounded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chan-Jan Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imagination-Augmented Natural Language Understanding. (arXiv:2204.08535v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08535","description":"<p>Human brains integrate linguistic and perceptual information simultaneously\nto understand natural language, and hold the critical ability to render\nimaginations. Such abilities enable us to construct new abstract concepts or\nconcrete objects, and are essential in involving practical knowledge to solve\nproblems in low-resource scenarios. However, most existing methods for Natural\nLanguage Understanding (NLU) are mainly focused on textual signals. They do not\nsimulate human visual imagination ability, which hinders models from inferring\nand learning efficiently from limited data samples. Therefore, we introduce an\nImagination-Augmented Cross-modal Encoder (iACE) to solve natural language\nunderstanding tasks from a novel learning perspective -- imagination-augmented\ncross-modal understanding. iACE enables visual imagination with external\nknowledge transferred from the powerful generative and pre-trained\nvision-and-language models. Extensive experiments on GLUE and SWAG show that\niACE achieves consistent improvement over visually-supervised pre-trained\nmodels. More importantly, results in extreme and normal few-shot settings\nvalidate the effectiveness of iACE in low-resource natural language\nunderstanding circumstances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Conservative are Language Models? Adapting to the Introduction of Gender-Neutral Pronouns. (arXiv:2204.10281v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10281","description":"<p>Gender-neutral pronouns have recently been introduced in many languages to a)\ninclude non-binary people and b) as a generic singular. Recent results from\npsycholinguistics suggest that gender-neutral pronouns (in Swedish) are not\nassociated with human processing difficulties. This, we show, is in sharp\ncontrast with automated processing. We show that gender-neutral pronouns in\nDanish, English, and Swedish are associated with higher perplexity, more\ndispersed attention patterns, and worse downstream performance. We argue that\nsuch conservativity in language models may limit widespread adoption of\ngender-neutral pronouns and must therefore be resolved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1\">Stephanie Brandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ruixiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opponent Modeling in Negotiation Dialogues by Related Data Adaptation. (arXiv:2205.00344v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00344","description":"<p>Opponent modeling is the task of inferring another party's mental state\nwithin the context of social interactions. In a multi-issue negotiation, it\ninvolves inferring the relative importance that the opponent assigns to each\nissue under discussion, which is crucial for finding high-value deals. A\npractical model for this task needs to infer these priorities of the opponent\non the fly based on partial dialogues as input, without needing additional\nannotations for training. In this work, we propose a ranker for identifying\nthese priorities from negotiation dialogues. The model takes in a partial\ndialogue as input and predicts the priority order of the opponent. We further\ndevise ways to adapt related data sources for this task to provide more\nexplicit supervision for incorporating the opponent's preferences and offers,\nas a proxy to relying on granular utterance-level annotations. We show the\nutility of our proposed approach through extensive experiments based on two\ndialogue datasets. We find that the proposed data adaptations lead to strong\nperformance in zero-shot and few-shot scenarios. Moreover, they allow the model\nto perform better than baselines while accessing fewer utterances from the\nopponent. We release our code to support future work in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chawla_K/0/1/0/all/0/1\">Kushal Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_G/0/1/0/all/0/1\">Gale M. Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1\">Jonathan Gratch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Cross-lingual Conversation Summarization Challenge. (arXiv:2205.00379v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00379","description":"<p>We propose the shared task of cross-lingual conversation summarization,\n\\emph{ConvSumX Challenge}, opening new avenues for researchers to investigate\nsolutions that integrate conversation summarization and machine translation.\nThis task can be particularly useful due to the emergence of online meetings\nand conferences. We construct a new benchmark, covering 2 real-world scenarios\nand 3 language directions, including a low-resource language. We hope that\n\\emph{ConvSumX} can motivate researches to go beyond English and break the\nbarrier for non-English speakers to benefit from recent advances of\nconversation summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuefeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Naihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xianchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"None Class Ranking Loss for Document-Level Relation Extraction. (arXiv:2205.00476v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00476","description":"<p>Document-level relation extraction (RE) aims at extracting relations among\nentities expressed across multiple sentences, which can be viewed as a\nmulti-label classification problem. In a typical document, most entity pairs do\nnot express any pre-defined relation and are labeled as \"none\" or \"no\nrelation\". For good document-level RE performance, it is crucial to distinguish\nsuch none class instances (entity pairs) from those of pre-defined classes\n(relations). However, most existing methods only estimate the probability of\npre-defined relations independently without considering the probability of \"no\nrelation\". This ignores the context of entity pairs and the label correlations\nbetween the none class and pre-defined classes, leading to sub-optimal\npredictions. To address this problem, we propose a new multi-label loss that\nencourages large margins of label confidence scores between each pre-defined\nclass and the none class, which enables captured label correlations and\ncontext-dependent thresholding for label prediction. To gain further robustness\nagainst positive-negative imbalance and mislabeled data that could appear in\nreal-world RE datasets, we propose a margin regularization and a margin\nshifting technique. Experimental results demonstrate that our method\nsignificantly outperforms existing multi-label losses for document-level RE and\nworks well in other multi-label tasks such as emotion classification when none\nclass instances are available for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wee Sun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Bias in Masked Language Models for Multiple Languages. (arXiv:2205.00551v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00551","description":"<p>Masked Language Models (MLMs) pre-trained by predicting masked tokens on\nlarge corpora have been used successfully in natural language processing tasks\nfor a variety of languages. Unfortunately, it was reported that MLMs also learn\ndiscriminative biases regarding attributes such as gender and race. Because\nmost studies have focused on MLMs in English, the bias of MLMs in other\nlanguages has rarely been investigated. Manual annotation of evaluation data\nfor languages other than English has been challenging due to the cost and\ndifficulty in recruiting annotators. Moreover, the existing bias evaluation\nmethods require the stereotypical sentence pairs consisting of the same context\nwith attribute words (e.g. He/She is a nurse). We propose Multilingual Bias\nEvaluation (MBE) score, to evaluate bias in various languages using only\nEnglish attribute word lists and parallel corpora between the target language\nand English without requiring manually annotated data. We evaluated MLMs in\neight languages using the MBE and confirmed that gender-related biases are\nencoded in MLMs for all those languages. We manually created datasets for\ngender bias in Japanese and Russian to evaluate the validity of the MBE. The\nresults show that the bias scores reported by the MBE significantly correlates\nwith that computed from the above manually created datasets and the existing\nEnglish datasets for gender bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1\">Masahiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imankulova_A/0/1/0/all/0/1\">Aizhan Imankulova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Factors Should Paper-Reviewer Assignments Rely On? Community Perspectives on Issues and Ideals in Conference Peer-Review. (arXiv:2205.01005v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01005","description":"<p>Both scientific progress and individual researcher careers depend on the\nquality of peer review, which in turn depends on paper-reviewer matching.\nSurprisingly, this problem has been mostly approached as an automated\nrecommendation problem rather than as a matter where different stakeholders\n(area chairs, reviewers, authors) have accumulated experience worth taking into\naccount. We present the results of the first survey of the NLP community,\nidentifying common issues and perspectives on what factors should be considered\nby paper-reviewer matching systems. This study contributes actionable\nrecommendations for improving future NLP conferences, and desiderata for\ninterpretable peer review assignments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jakobsen_T/0/1/0/all/0/1\">Terne Sasha Thorn Jakobsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPT: Open Pre-trained Transformer Language Models. (arXiv:2205.01068v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01068","description":"<p>Large language models, which are often trained for hundreds of thousands of\ncompute days, have shown remarkable capabilities for zero- and few-shot\nlearning. Given their computational cost, these models are difficult to\nreplicate without significant capital. For the few that are available through\nAPIs, no access is granted to the full model weights, making them difficult to\nstudy. We present Open Pre-trained Transformers (OPT), a suite of decoder-only\npre-trained transformers ranging from 125M to 175B parameters, which we aim to\nfully and responsibly share with interested researchers. We show that OPT-175B\nis comparable to GPT-3, while requiring only 1/7th the carbon footprint to\ndevelop. We are also releasing our logbook detailing the infrastructure\nchallenges we faced, along with code for experimenting with all of the released\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Susan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Moya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dewan_C/0/1/0/all/0/1\">Christopher Dewan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shleifer_S/0/1/0/all/0/1\">Sam Shleifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simig_D/0/1/0/all/0/1\">Daniel Simig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koura_P/0/1/0/all/0/1\">Punit Singh Koura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1\">Anjali Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoBERTuito: a pre-trained language model for social media text in Spanish. (arXiv:2111.09453v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2111.09453","description":"<p>Since BERT appeared, Transformer language models and transfer learning have\nbecome state-of-the-art for Natural Language Understanding tasks. Recently,\nsome works geared towards pre-training specially-crafted models for particular\ndomains, such as scientific papers, medical documents, user-generated texts,\namong others. These domain-specific models have been shown to improve\nperformance significantly in most tasks. However, for languages other than\nEnglish such models are not widely available.\n</p>\n<p>In this work, we present RoBERTuito, a pre-trained language model for\nuser-generated text in Spanish, trained on over 500 million tweets. Experiments\non a benchmark of tasks involving user-generated text showed that RoBERTuito\noutperformed other pre-trained language models in Spanish. In addition to this,\nour model achieves top results for some English-Spanish tasks of the Linguistic\nCode-Switching Evaluation benchmark (LinCE) and has also competitive\nperformance against monolingual models in English tasks. To facilitate further\nresearch, we make RoBERTuito publicly available at the HuggingFace model hub\ntogether with the dataset used to pre-train it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan Manuel P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furman_D/0/1/0/all/0/1\">Dami&#xe1;n A. Furman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alemany_L/0/1/0/all/0/1\">Laura Alonso Alemany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luque_F/0/1/0/all/0/1\">Franco Luque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Hausa Visual Genome: A Dataset for Multi-Modal English to Hausa Machine Translation. (arXiv:2205.01133v1 [cs.CL])","link":"http://arxiv.org/abs/2205.01133","description":"<p>Multi-modal Machine Translation (MMT) enables the use of visual information\nto enhance the quality of translations. The visual information can serve as a\nvaluable piece of context information to decrease the ambiguity of input\nsentences. Despite the increasing popularity of such a technique, good and\nsizeable datasets are scarce, limiting the full extent of their potential.\nHausa, a Chadic language, is a member of the Afro-Asiatic language family. It\nis estimated that about 100 to 150 million people speak the language, with more\nthan 80 million indigenous speakers. This is more than any of the other Chadic\nlanguages. Despite a large number of speakers, the Hausa language is considered\nlow-resource in natural language processing (NLP). This is due to the absence\nof sufficient resources to implement most NLP tasks. While some datasets exist,\nthey are either scarce, machine-generated, or in the religious domain.\nTherefore, there is a need to create training and evaluation data for\nimplementing machine learning tasks and bridging the research gap in the\nlanguage. This work presents the Hausa Visual Genome (HaVG), a dataset that\ncontains the description of an image or a section within the image in Hausa and\nits equivalent in English. To prepare the dataset, we started by translating\nthe English description of the images in the Hindi Visual Genome (HVG) into\nHausa automatically. Afterward, the synthetic Hausa data was carefully\npost-edited considering the respective images. The dataset comprises 32,923\nimages and their descriptions that are divided into training, development,\ntest, and challenge test set. The Hausa Visual Genome is the first dataset of\nits kind and can be used for Hausa-English machine translation, multi-modal\nresearch, and image description, among various other natural language\nprocessing and generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Satya Ranjan Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawud_M/0/1/0/all/0/1\">Musa Abdullahi Dawud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parida_S/0/1/0/all/0/1\">Shantipriya Parida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Ibrahim Sa&#x27;id Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_S/0/1/0/all/0/1\">Subhadarshi Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galadanci_B/0/1/0/all/0/1\">Bashir Shehu Galadanci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_B/0/1/0/all/0/1\">Bello Shehu Bello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D-DPCC: Deep Dynamic Point Cloud Compression via 3D Motion Prediction. (arXiv:2205.01135v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01135","description":"<p>The non-uniformly distributed nature of the 3D dynamic point cloud (DPC)\nbrings significant challenges to its high-efficient inter-frame compression.\nThis paper proposes a novel 3D sparse convolution-based Deep Dynamic Point\nCloud Compression (D-DPCC) network to compensate and compress the DPC geometry\nwith 3D motion estimation and motion compensation in the feature space. In the\nproposed D-DPCC network, we design a {\\it Multi-scale Motion Fusion} (MMF)\nmodule to accurately estimate the 3D optical flow between the feature\nrepresentations of adjacent point cloud frames. Specifically, we utilize a 3D\nsparse convolution-based encoder to obtain the latent representation for motion\nestimation in the feature space and introduce the proposed MMF module for fused\n3D motion embedding. Besides, for motion compensation, we propose a 3D {\\it\nAdaptively Weighted Interpolation} (3DAWI) algorithm with a penalty coefficient\nto adaptively decrease the impact of distant neighbors. We compress the motion\nembedding and the residual with a lossy autoencoder-based network. To our\nknowledge, this paper is the first work proposing an end-to-end deep dynamic\npoint cloud compression framework. The experimental result shows that the\nproposed D-DPCC framework achieves an average 76\\% BD-Rate (Bjontegaard Delta\nRate) gains against state-of-the-art Video-based Point Cloud Compression\n(V-PCC) v13 in inter mode.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1\">Tingyu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Linyao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiling Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cost-Aware Comparison of LiDAR-based 3D Object Detectors. (arXiv:2205.01142v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01142","description":"<p>Considerable research efforts have been devoted to LiDAR-based 3D object\ndetection and its empirical performance has been significantly improved. While\nthe progress has been encouraging, we observe an overlooked issue: it is not\nyet common practice to compare different 3D detectors under the same cost,\ne.g., inference latency. This makes it difficult to quantify the true\nperformance gain brought by recently proposed architecture designs. The goal of\nthis work is to conduct a fair comparison of LiDAR-based 3D object detectors.\nSpecifically, we focus on SECOND, a simple grid-based one-stage detector, and\nanalyze its performance under different costs by scaling its original\narchitecture. Then we compare the family of scaled SECOND with recent 3D\ndetection methods, such as Voxel R-CNN and PV-RCNN++. The results are\nsurprising. We find that, if allowed to use the same latency, SECOND can match\nthe performance of PV-RCNN++, the current state-of-the-art method on the Waymo\nOpen Dataset. Scaled SECOND also easily outperforms many recent 3D detection\nmethods published during the past year. We recommend future research control\nthe inference cost in their empirical comparison and include the family of\nscaled SECOND as a strong baseline when presenting novel 3D detection methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion-Controllable Generalized Talking Face Generation. (arXiv:2205.01155v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01155","description":"<p>Despite the significant progress in recent years, very few of the AI-based\ntalking face generation methods attempt to render natural emotions. Moreover,\nthe scope of the methods is majorly limited to the characteristics of the\ntraining dataset, hence they fail to generalize to arbitrary unseen faces. In\nthis paper, we propose a one-shot facial geometry-aware emotional talking face\ngeneration method that can generalize to arbitrary faces. We propose a graph\nconvolutional neural network that uses speech content feature, along with an\nindependent emotion input to generate emotion and speech-induced motion on\nfacial geometry-aware landmark representation. This representation is further\nused in our optical flow-guided texture generation network for producing the\ntexture. We propose a two-branch texture generation network, with motion and\ntexture branches designed to consider the motion and texture content\nindependently. Compared to the previous emotion talking face methods, our\nmethod can adapt to arbitrary faces captured in-the-wild by fine-tuning with\nonly a single image of the target identity in neutral emotion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Sanjana Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sandika Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_R/0/1/0/all/0/1\">Ravindra Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhowmick_B/0/1/0/all/0/1\">Brojeshwar Bhowmick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SELC: Self-Ensemble Label Correction Improves Learning with Noisy Labels. (arXiv:2205.01156v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01156","description":"<p>Deep neural networks are prone to overfitting noisy labels, resulting in poor\ngeneralization performance. To overcome this problem, we present a simple and\neffective method self-ensemble label correction (SELC) to progressively correct\nnoisy labels and refine the model. We look deeper into the memorization\nbehavior in training with noisy labels and observe that the network outputs are\nreliable in the early stage. To retain this reliable knowledge, SELC uses\nensemble predictions formed by an exponential moving average of network outputs\nto update the original noisy labels. We show that training with SELC refines\nthe model by gradually reducing supervision from noisy labels and increasing\nsupervision from ensemble predictions. Despite its simplicity, compared with\nmany state-of-the-art methods, SELC obtains more promising and stable results\nin the presence of class-conditional, instance-dependent, and real-world label\nnoise. The code is available at https://github.com/MacLLL/SELC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yangdi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wenbo He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency map using features derived from spiking neural networks of primate visual cortex. (arXiv:2205.01159v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01159","description":"<p>We propose a framework inspired by biological vision systems to produce\nsaliency maps of digital images. Well-known computational models for receptive\nfields of areas in the visual cortex that are specialized for color and\norientation perception are used. To model the connectivity between these areas\nwe use the CARLsim library which is a spiking neural network(SNN) simulator.\nThe spikes generated by CARLsim, then serve as extracted features and input to\nour saliency detection algorithm. This new method of saliency detection is\ndescribed and applied to benchmark images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeedy_R/0/1/0/all/0/1\">Reza Hojjaty Saeedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messner_R/0/1/0/all/0/1\">Richard A. Messner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Convolutional Neural Networks for Dendrite Segmentation Using Fine-Tuning and Hyperparameter Optimization. (arXiv:2205.01167v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01167","description":"<p>Dendritic microstructures are ubiquitous in nature and are the primary\nsolidification morphologies in metallic materials. Techniques such as x-ray\ncomputed tomography (XCT) have provided new insights into dendritic phase\ntransformation phenomena. However, manual identification of dendritic\nmorphologies in microscopy data can be both labor intensive and potentially\nambiguous. The analysis of 3D datasets is particularly challenging due to their\nlarge sizes (terabytes) and the presence of artifacts scattered within the\nimaged volumes. In this study, we trained 3D convolutional neural networks\n(CNNs) to segment 3D datasets. Three CNN architectures were investigated,\nincluding a new 3D version of FCDense. We show that using hyperparameter\noptimization (HPO) and fine-tuning techniques, both 2D and 3D CNN architectures\ncan be trained to outperform the previous state of the art. The 3D U-Net\narchitecture trained in this study produced the best segmentations according to\nquantitative metrics (pixel-wise accuracy of 99.84% and a boundary displacement\nerror of 0.58 pixels), while 3D FCDense produced the smoothest boundaries and\nbest segmentations according to visual inspection. The trained 3D CNNs are able\nto segment entire 852 x 852 x 250 voxel 3D volumes in only ~60 seconds, thus\nhastening the progress towards a deeper understanding of phase transformation\nphenomena such as dendritic solidification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+James_J/0/1/0/all/0/1\">Jim James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruyne_N/0/1/0/all/0/1\">Nathan Pruyne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stan_T/0/1/0/all/0/1\">Tiberiu Stan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarting_M/0/1/0/all/0/1\">Marcus Schwarting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeom_J/0/1/0/all/0/1\">Jiwon Yeom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Seungbum Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voorhees_P/0/1/0/all/0/1\">Peter Voorhees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blaiszik_B/0/1/0/all/0/1\">Ben Blaiszik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1\">Ian Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Video Object Segmentation based on Scale Inconsistency. (arXiv:2205.01197v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01197","description":"<p>We present a refinement framework to boost the performance of pre-trained\nsemi-supervised video object segmentation (VOS) models. Our work is based on\nscale inconsistency, which is motivated by the observation that existing VOS\nmodels generate inconsistent predictions from input frames with different\nsizes. We use the scale inconsistency as a clue to devise a pixel-level\nattention module that aggregates the advantages of the predictions from\ndifferent-size inputs. The scale inconsistency is also used to regularize the\ntraining based on a pixel-level variance measured by an uncertainty estimation.\nWe further present a self-supervised online adaptation, tailored for test-time\noptimization, that bootstraps the predictions without ground-truth masks based\non the scale inconsistency. Experiments on DAVIS 16 and DAVIS 17 datasets show\nthat our framework can be generically applied to various VOS models and improve\ntheir performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1\">Changjae Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NHA12D: A New Pavement Crack Dataset and a Comparison Study Of Crack Detection Algorithms. (arXiv:2205.01198v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01198","description":"<p>Crack detection plays a key role in automated pavement inspection. Although a\nlarge number of algorithms have been developed in recent years to further boost\nperformance, there are still remaining challenges in practice, due to the\ncomplexity of pavement images. To further accelerate the development and\nidentify the remaining challenges, this paper conducts a comparison study to\nevaluate the performance of the state of the art crack detection algorithms\nquantitatively and objectively. A more comprehensive annotated pavement crack\ndataset (NHA12D) that contains images with different viewpoints and pavements\ntypes is proposed. In the comparison study, crack detection algorithms were\ntrained equally on the largest public crack dataset collected and evaluated on\nthe proposed dataset (NHA12D). Overall, the U-Net model with VGG-16 as backbone\nhas the best all-around performance, but models generally fail to distinguish\ncracks from concrete joints, leading to a high false-positive rate. It also\nfound that detecting cracks from concrete pavement images still has huge room\nfor improvement. Dataset for concrete pavement images is also missing in the\nliterature. Future directions in this area include filling the gap for concrete\npavement images and using domain adaptation techniques to enhance the detection\nresults on unseen datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhening Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Tabbaa_A/0/1/0/all/0/1\">Abir Al-Tabbaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brilakis_I/0/1/0/all/0/1\">Ioannis Brilakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Defense Method against Adversarial Attacks on Traffic Sign Classifiers in Autonomous Vehicles. (arXiv:2205.01225v1 [cs.CR])","link":"http://arxiv.org/abs/2205.01225","description":"<p>Adversarial attacks can make deep neural network (DNN) models predict\nincorrect output labels, such as misclassified traffic signs, for autonomous\nvehicle (AV) perception modules. Resilience against adversarial attacks can\nhelp AVs navigate safely on the road by avoiding misclassication of signs or\nobjects. This DNN-based study develops a resilient traffic sign classifier for\nAVs that uses a hybrid defense method. We use transfer learning to retrain the\nInception-V3 and Resnet-152 models as traffic sign classifiers. This method\nalso utilizes a combination of three different strategies: random filtering,\nensembling, and local feature mapping. We use the random cropping and resizing\ntechnique for random filtering, plurality voting as ensembling strategy and an\noptical character recognition model as a local feature mapper. This DNN-based\nhybrid defense method has been tested for the no attack scenario and against\nwell-known untargeted adversarial attacks (e.g., Projected Gradient Descent or\nPGD, Fast Gradient Sign Method or FGSM, Momentum Iterative Method or MIM\nattack, and Carlini and Wagner or C&amp;W). We find that our hybrid defense method\nachieves 99% average traffic sign classification accuracy for the no attack\nscenario and 88% average traffic sign classification accuracy for all attack\nscenarios. Moreover, the hybrid defense method, presented in this study,\nimproves the accuracy for traffic sign classification compared to the\ntraditional defense methods (i.e., JPEG filtering, feature squeezing, binary\nfiltering, and random filtering) up to 6%, 50%, and 55% for FGSM, MIM, and PGD\nattacks, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zadid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Mashrur Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Sakib Mahmud Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial attacks on an optical neural network. (arXiv:2205.01226v1 [cs.CR])","link":"http://arxiv.org/abs/2205.01226","description":"<p>Adversarial attacks have been extensively investigated for machine learning\nsystems including deep learning in the digital domain. However, the adversarial\nattacks on optical neural networks (ONN) have been seldom considered\npreviously. In this work, we first construct an accurate image classifier with\nan ONN using a mesh of interconnected Mach-Zehnder interferometers (MZI). Then\na corresponding adversarial attack scheme is proposed for the first time. The\nattacked images are visually very similar to the original ones but the ONN\nsystem becomes malfunctioned and generates wrong classification results in most\ntime. The results indicate that adversarial attack is also a significant issue\nfor optical machine learning systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_S/0/1/0/all/0/1\">Shuming Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Ziwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Shuiying Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Weird Trick to Improve Your Semi-Weakly Supervised Semantic Segmentation Model. (arXiv:2205.01233v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01233","description":"<p>Semi-weakly supervised semantic segmentation (SWSSS) aims to train a model to\nidentify objects in images based on a small number of images with pixel-level\nlabels, and many more images with only image-level labels. Most existing SWSSS\nalgorithms extract pixel-level pseudo-labels from an image classifier - a very\ndifficult task to do well, hence requiring complicated architectures and\nextensive hyperparameter tuning on fully-supervised validation sets. We propose\na method called prediction filtering, which instead of extracting\npseudo-labels, just uses the classifier as a classifier: it ignores any\nsegmentation predictions from classes which the classifier is confident are not\npresent. Adding this simple post-processing method to baselines gives results\ncompetitive with or better than prior SWSSS algorithms. Moreover, it is\ncompatible with pseudo-label methods: adding prediction filtering to existing\nSWSSS algorithms further improves segmentation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bae_W/0/1/0/all/0/1\">Wonho Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1\">Junhyug Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadabadi_M/0/1/0/all/0/1\">Milad Jalali Asadabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutherland_D/0/1/0/all/0/1\">Danica J. Sutherland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Performance-Consistent and Computation-Efficient CNN System for High-Quality Automated Brain Tumor Segmentation. (arXiv:2205.01239v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01239","description":"<p>The research on developing CNN-based fully-automated Brain-Tumor-Segmentation\nsystems has been progressed rapidly. For the systems to be applicable in\npractice, a good The research on developing CNN-based fully-automated\nBrain-Tumor-Segmentation systems has been progressed rapidly. For the systems\nto be applicable in practice, a good processing quality and reliability are the\nmust. Moreover, for wide applications of such systems, a minimization of\ncomputation complexity is desirable, which can also result in a minimization of\nrandomness in computation and, consequently, a better performance consistency.\nTo this end, the CNN in the proposed system has a unique structure with 2\ndistinguished characters. Firstly, the three paths of its feature extraction\nblock are designed to extract, from the multi-modality input, comprehensive\nfeature information of mono-modality, paired-modality and cross-modality data,\nrespectively. Also, it has a particular three-branch classification block to\nidentify the pixels of 4 classes. Each branch is trained separately so that the\nparameters are updated specifically with the corresponding ground truth data of\na target tumor areas. The convolution layers of the system are custom-designed\nwith specific purposes, resulting in a very simple config of 61,843 parameters\nin total. The proposed system is tested extensively with BraTS2018 and\nBraTS2019 datasets. The mean Dice scores, obtained from the ten experiments on\nBraTS2018 validation samples, are 0.787+0.003, 0.886+0.002, 0.801+0.007, for\nenhancing tumor, whole tumor and tumor core, respectively, and 0.751+0.007,\n0.885+0.002, 0.776+0.004 on BraTS2019. The test results demonstrate that the\nproposed system is able to perform high-quality segmentation in a consistent\nmanner. Furthermore, its extremely low computation complexity will facilitate\nits implementation/application in various environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tong_J/0/1/0/all/0/1\">Juncheng Tong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chunyan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation. (arXiv:2205.01271v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01271","description":"<p>Pose estimation plays a critical role in human-centered vision applications.\nHowever, it is difficult to deploy state-of-the-art HRNet-based pose estimation\nmodels on resource-constrained edge devices due to the high computational cost\n(more than 150 GMACs per frame). In this paper, we study efficient architecture\ndesign for real-time multi-person pose estimation on edge. We reveal that\nHRNet's high-resolution branches are redundant for models at the\nlow-computation region via our gradual shrinking experiments. Removing them\nimproves both efficiency and performance. Inspired by this finding, we design\nLitePose, an efficient single-branch architecture for pose estimation, and\nintroduce two simple approaches to enhance the capacity of LitePose, including\nFusion Deconv Head and Large Kernel Convs. Fusion Deconv Head removes the\nredundancy in high-resolution branches, allowing scale-aware feature fusion\nwith low overhead. Large Kernel Convs significantly improve the model's\ncapacity and receptive field while maintaining a low computational cost. With\nonly 25% computation increment, 7x7 kernels achieve +14.0 mAP better than 3x3\nkernels on the CrowdPose dataset. On mobile platforms, LitePose reduces the\nlatency by up to 5.0x without sacrificing performance, compared with prior\nstate-of-the-art efficient pose estimation models, pushing the frontier of\nreal-time multi-person pose estimation on edge. Our code and pre-trained models\nare released at https://github.com/mit-han-lab/litepose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Muyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Han Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Ming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Domain Object Detection by Target-Perceived Dual Branch Distillation. (arXiv:2205.01291v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01291","description":"<p>Cross domain object detection is a realistic and challenging task in the\nwild. It suffers from performance degradation due to large shift of data\ndistributions and lack of instance-level annotations in the target domain.\nExisting approaches mainly focus on either of these two difficulties, even\nthough they are closely coupled in cross domain object detection. To solve this\nproblem, we propose a novel Target-perceived Dual-branch Distillation (TDD)\nframework. By integrating detection branches of both source and target domains\nin a unified teacher-student learning scheme, it can reduce domain shift and\ngenerate reliable supervision effectively. In particular, we first introduce a\ndistinct Target Proposal Perceiver between two domains. It can adaptively\nenhance source detector to perceive objects in a target image, by leveraging\ntarget proposal contexts from iterative cross-attention. Afterwards, we design\na concise Dual Branch Self Distillation strategy for model training, which can\nprogressively integrate complementary object knowledge from different domains\nvia self-distillation in two branches. Finally, we conduct extensive\nexperiments on a number of widely-used scenarios in cross domain object\ndetection. The results show that our TDD significantly outperforms the\nstate-of-the-art methods on all the benchmarks. Our code and model will be\navailable at https://github.com/Feobi1999/TDD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mengzhe He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Weihao Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RU-Net: Regularized Unrolling Network for Scene Graph Generation. (arXiv:2205.01297v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01297","description":"<p>Scene graph generation (SGG) aims to detect objects and predict the\nrelationships between each pair of objects. Existing SGG methods usually suffer\nfrom several issues, including 1) ambiguous object representations, as graph\nneural network-based message passing (GMP) modules are typically sensitive to\nspurious inter-node correlations, and 2) low diversity in relationship\npredictions due to severe class imbalance and a large number of missing\nannotations. To address both problems, in this paper, we propose a regularized\nunrolling network (RU-Net). We first study the relation between GMP and graph\nLaplacian denoising (GLD) from the perspective of the unrolling technique,\ndetermining that GMP can be formulated as a solver for GLD. Based on this\nobservation, we propose an unrolled message passing module and introduce an\n$\\ell_p$-based graph regularization to suppress spurious connections between\nnodes. Second, we propose a group diversity enhancement module that promotes\nthe prediction diversity of relationships via rank maximization. Systematic\nexperiments demonstrate that RU-Net is effective under a variety of settings\nand metrics. Furthermore, RU-Net achieves new state-of-the-arts on three\npopular databases: VG, VRD, and OI. Code is available at\nhttps://github.com/siml3/RU-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Governing Laws and Source Input for Dynamical Systems from Videos. (arXiv:2205.01314v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01314","description":"<p>Distilling interpretable physical laws from videos has led to expanded\ninterest in the computer vision community recently thanks to the advances in\ndeep learning, but still remains a great challenge. This paper introduces an\nend-to-end unsupervised deep learning framework to uncover the explicit\ngoverning equations of dynamics presented by moving object(s), based on\nrecorded videos. Instead in the pixel (spatial) coordinate system of image\nspace, the physical law is modeled in a regressed underlying physical\ncoordinate system where the physical states follow potential explicit governing\nequations. A numerical integrator-based sparse regression module is designed\nand serves as a physical constraint to the autoencoder and coordinate system\nregression, and, in the meanwhile, uncover the parsimonious closed-form\ngoverning equations from the learned physical states. Experiments on simulated\ndynamical scenes show that the proposed method is able to distill closed-form\ngoverning equations and simultaneously identify unknown excitation input for\nseveral dynamical systems recorded by videos, which fills in the gap in\nliterature where no existing methods are available and applicable for solving\nthis type of problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luan_L/0/1/0/all/0/1\">Lele Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HL-Net: Heterophily Learning Network for Scene Graph Generatio. (arXiv:2205.01316v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01316","description":"<p>Scene graph generation (SGG) aims to detect objects and predict their\npairwise relationships within an image. Current SGG methods typically utilize\ngraph neural networks (GNNs) to acquire context information between\nobjects/relationships. Despite their effectiveness, however, current SGG\nmethods only assume scene graph homophily while ignoring heterophily.\nAccordingly, in this paper, we propose a novel Heterophily Learning Network\n(HL-Net) to comprehensively explore the homophily and heterophily between\nobjects/relationships in scene graphs. More specifically, HL-Net comprises the\nfollowing 1) an adaptive reweighting transformer module, which adaptively\nintegrates the information from different layers to exploit both the\nheterophily and homophily in objects; 2) a relationship feature propagation\nmodule that efficiently explores the connections between relationships by\nconsidering heterophily in order to refine the relationship representation; 3)\na heterophily-aware message-passing scheme to further distinguish the\nheterophily and homophily between objects/relationships, thereby facilitating\nimproved message passing in graphs. We conducted extensive experiments on two\npublic datasets: Visual Genome (VG) and Open Images (OI). The experimental\nresults demonstrate the superiority of our proposed HL-Net over existing\nstate-of-the-art approaches. In more detail, HL-Net outperforms the second-best\ncompetitors by 2.1$\\%$ on the VG dataset for scene graph classification and\n1.2$\\%$ on the IO dataset for the final score. Code is available at\nhttps://github.com/siml3/HL-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zijian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioTouchPass: Handwritten Passwords for Touchscreen Biometrics. (arXiv:2205.01353v1 [cs.CR])","link":"http://arxiv.org/abs/2205.01353","description":"<p>This work enhances traditional authentication systems based on Personal\nIdentification Numbers (PIN) and One-Time Passwords (OTP) through the\nincorporation of biometric information as a second level of user\nauthentication. In our proposed approach, users draw each digit of the password\non the touchscreen of the device instead of typing them as usual. A complete\nanalysis of our proposed biometric system is carried out regarding the\ndiscriminative power of each handwritten digit and the robustness when\nincreasing the length of the password and the number of enrolment samples. The\nnew e-BioDigit database, which comprises on-line handwritten digits from 0 to\n9, has been acquired using the finger as input on a mobile device. This\ndatabase is used in the experiments reported in this work and it is available\ntogether with benchmark results in GitHub. Finally, we discuss specific details\nfor the deployment of our proposed approach on current PIN and OTP systems,\nachieving results with Equal Error Rates (EERs) ca. 4.0% when the attacker\nknows the password. These results encourage the deployment of our proposed\napproach in comparison to traditional PIN and OTP systems where the attack\nwould have 100% success rate under the same impostor scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Loose-Fitting Garment Deformations Using Bone-Driven Motion Networks. (arXiv:2205.01355v1 [cs.GR])","link":"http://arxiv.org/abs/2205.01355","description":"<p>We present a learning algorithm that uses bone-driven motion networks to\npredict the deformation of loose-fitting garment meshes at interactive rates.\nGiven a garment, we generate a simulation database and extract virtual bones\nfrom simulated mesh sequences using skin decomposition. At runtime, we\nseparately compute low- and high-frequency deformations in a sequential manner.\nThe low-frequency deformations are predicted by transferring body motions to\nvirtual bones' motions, and the high-frequency deformations are estimated\nleveraging the global information of virtual bones' motions and local\ninformation extracted from low-frequency meshes. In addition, our method can\nestimate garment deformations caused by variations of the simulation parameters\n(e.g., fabric's bending stiffness) using an RBF kernel ensembling trained\nnetworks for different sets of simulation parameters. Through extensive\ncomparisons, we show that our method outperforms state-of-the-art methods in\nterms of prediction accuracy of mesh deformations by about 20% in RMSE and 10%\nin Hausdorff distance and STED. The code and data are available at\nhttps://github.com/non-void/VirtualBones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiaoyu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_J/0/1/0/all/0/1\">Jiaming Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Dongxue Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1\">Tianjia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaogang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A hybrid multi-object segmentation framework with model-based B-splines for microbial single cell analysis. (arXiv:2205.01367v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01367","description":"<p>In this paper, we propose a hybrid approach for multi-object microbial cell\nsegmentation. The approach combines an ML-based detection with a geometry-aware\nvariational-based segmentation using B-splines that are parametrized based on a\ngeometric model of the cell shape. The detection is done first using YOLOv5. In\na second step, each detected cell is segmented individually. Thus, the\nsegmentation only needs to be done on a per-cell basis, which makes it amenable\nto a variational approach that incorporates prior knowledge on the geometry.\nHere, the contour of the segmentation is modelled as closed uniform cubic\nB-spline, whose control points are parametrized using the known cell geometry.\nCompared to purely ML-based segmentation approaches, which need accurate\nsegmentation maps as training data that are very laborious to produce, our\nmethod just needs bounding boxes as training data. Still, the proposed method\nperforms on par with ML-based segmentation approaches usually used in this\ncontext. We study the performance of the proposed method on time-lapse\nmicroscopy data of Corynebacterium glutamicum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruzaeva_K/0/1/0/all/0/1\">Karina Ruzaeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_K/0/1/0/all/0/1\">Katharina N&#xf6;h</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkels_B/0/1/0/all/0/1\">Benjamin Berkels</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Copy Motion From One to Another: Fake Motion Video Generation. (arXiv:2205.01373v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01373","description":"<p>One compelling application of artificial intelligence is to generate a video\nof a target person performing arbitrary desired motion (from a source person).\nWhile the state-of-the-art methods are able to synthesize a video demonstrating\nsimilar broad stroke motion details, they are generally lacking in texture\ndetails. A pertinent manifestation appears as distorted face, feet, and hands,\nand such flaws are very sensitively perceived by human observers. Furthermore,\ncurrent methods typically employ GANs with a L2 loss to assess the authenticity\nof the generated videos, inherently requiring a large amount of training\nsamples to learn the texture details for adequate video generation. In this\nwork, we tackle these challenges from three aspects: 1) We disentangle each\nvideo frame into foreground (the person) and background, focusing on generating\nthe foreground to reduce the underlying dimension of the network output. 2) We\npropose a theoretically motivated Gromov-Wasserstein loss that facilitates\nlearning the mapping from a pose to a foreground image. 3) To enhance texture\ndetails, we encode facial features with geometric guidance and employ local\nGANs to refine the face, feet, and hands. Extensive experiments show that our\nmethod is able to generate realistic target person videos, faithfully copying\ncomplex motions from a source person. Our code and datasets are released at\nhttps://github.com/Sifann/FakeMotion\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sifan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chejian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning in Multimodal Remote Sensing Data Fusion: A Comprehensive Review. (arXiv:2205.01380v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01380","description":"<p>With the extremely rapid advances in remote sensing (RS) technology, a great\nquantity of Earth observation (EO) data featuring considerable and complicated\nheterogeneity is readily available nowadays, which renders researchers an\nopportunity to tackle current geoscience applications in a fresh way. With the\njoint utilization of EO data, much research on multimodal RS data fusion has\nmade tremendous progress in recent years, yet these developed traditional\nalgorithms inevitably meet the performance bottleneck due to the lack of the\nability to comprehensively analyse and interpret these strongly heterogeneous\ndata. Hence, this non-negligible limitation further arouses an intense demand\nfor an alternative tool with powerful processing competence. Deep learning\n(DL), as a cutting-edge technology, has witnessed remarkable breakthroughs in\nnumerous computer vision tasks owing to its impressive ability in data\nrepresentation and reconstruction. Naturally, it has been successfully applied\nto the field of multimodal RS data fusion, yielding great improvement compared\nwith traditional methods. This survey aims to present a systematic overview in\nDL-based multimodal RS data fusion. More specifically, some essential knowledge\nabout this topic is first given. Subsequently, a literature survey is conducted\nto analyse the trends of this field. Some prevalent sub-fields in the\nmultimodal RS data fusion are then reviewed in terms of the to-be-fused data\nmodalities, i.e., spatiospectral, spatiotemporal, light detection and\nranging-optical, synthetic aperture radar-optical, and RS-Geospatial Big Data\nfusion. Furthermore, We collect and summarize some valuable resources for the\nsake of the development in multimodal RS data fusion. Finally, the remaining\nchallenges and potential future directions are highlighted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Danfeng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianru Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Ke Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sampling-free obstacle gradients and reactive planning in Neural Radiance Fields (NeRF). (arXiv:2205.01389v1 [cs.RO])","link":"http://arxiv.org/abs/2205.01389","description":"<p>This work investigates the use of Neural implicit representations,\nspecifically Neural Radiance Fields (NeRF), for geometrical queries and motion\nplanning. We show that by adding the capacity to infer occupancy in a radius to\na pre-trained NeRF, we are effectively learning an approximation to a Euclidean\nSigned Distance Field (ESDF). Using backward differentiation of the augmented\nnetwork, we obtain an obstacle gradient that is integrated into an obstacle\navoidance policy based on the Riemannian Motion Policies (RMP) framework. Thus,\nour findings allow for very fast sampling-free obstacle avoidance planning in\nthe implicit representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Michael Pantic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_C/0/1/0/all/0/1\">Cesar Cadena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1\">Roland Siegwart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_L/0/1/0/all/0/1\">Lionel Ott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP). (arXiv:2205.01397v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01397","description":"<p>Contrastively trained image-text models such as CLIP, ALIGN, and BASIC have\ndemonstrated unprecedented robustness to multiple challenging natural\ndistribution shifts. Since these image-text models differ from previous\ntraining approaches in several ways, an important question is what causes the\nlarge robustness gains. We answer this question via a systematic experimental\ninvestigation. Concretely, we study five different possible causes for the\nrobustness gains: (i) the training set size, (ii) the training distribution,\n(iii) language supervision at training time, (iv) language supervision at test\ntime, and (v) the contrastive loss function. Our experiments show that the more\ndiverse training distribution is the main cause for the robustness gains, with\nthe other factors contributing little to no robustness. Beyond our experimental\nresults, we also introduce ImageNet-Captions, a version of ImageNet with\noriginal text annotations from Flickr, to enable further controlled experiments\nof language-image training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1\">Alex Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yuhao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1\">Vaishaal Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1\">Achal Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Outdoor Monocular Depth Estimation: A Research Review. (arXiv:2205.01399v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01399","description":"<p>Depth estimation is an important task, applied in various methods and\napplications of computer vision. While the traditional methods of estimating\ndepth are based on depth cues and require specific equipment such as stereo\ncameras and configuring input according to the approach being used, the focus\nat the current time is on a single source, or monocular, depth estimation. The\nrecent developments in Convolution Neural Networks along with the integration\nof classical methods in these deep learning approaches have led to a lot of\nadvancements in the depth estimation problem. The problem of outdoor depth\nestimation, or depth estimation in wild, is a very scarcely researched field of\nstudy. In this paper, we give an overview of the available datasets, depth\nestimation methods, research work, trends, challenges, and opportunities that\nexist for open research. To our knowledge, no openly available survey work\nprovides a comprehensive collection of outdoor depth estimation techniques and\nresearch scope, making our work an essential contribution for people looking to\nenter this field of study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vyas_P/0/1/0/all/0/1\">Pulkit Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chirag Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badapanda_A/0/1/0/all/0/1\">Anwesh Badapanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_A/0/1/0/all/0/1\">Anurag Goswami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Detection of Unknown Objects on Roads for Autonomous Driving. (arXiv:2205.01414v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01414","description":"<p>Tremendous progress in deep learning over the last years has led towards a\nfuture with autonomous vehicles on our roads. Nevertheless, the performance of\ntheir perception systems is strongly dependent on the quality of the utilized\ntraining data. As these usually only cover a fraction of all object classes an\nautonomous driving system will face, such systems struggle with handling the\nunexpected. In order to safely operate on public roads, the identification of\nobjects from unknown classes remains a crucial task. In this paper, we propose\na novel pipeline to detect unknown objects. Instead of focusing on a single\nsensor modality, we make use of lidar and camera data by combining state-of-the\nart detection models in a sequential manner. We evaluate our approach on the\nWaymo Open Perception Dataset and point out current research gaps in anomaly\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogdoll_D/0/1/0/all/0/1\">Daniel Bogdoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisen_E/0/1/0/all/0/1\">Enrico Eisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitsche_M/0/1/0/all/0/1\">Maximilian Nitsche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheib_C/0/1/0/all/0/1\">Christin Scheib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">J. Marius Z&#xf6;llner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Analysis of the Use of Real-Time Reachability for the Safety Assurance of Autonomous Vehicles. (arXiv:2205.01419v1 [cs.RO])","link":"http://arxiv.org/abs/2205.01419","description":"<p>Recent advances in machine learning technologies and sensing have paved the\nway for the belief that safe, accessible, and convenient autonomous vehicles\nmay be realized in the near future. Despite tremendous advances within this\ncontext, fundamental challenges around safety and reliability are limiting\ntheir arrival and comprehensive adoption. Autonomous vehicles are often tasked\nwith operating in dynamic and uncertain environments. As a result, they often\nmake use of highly complex components, such as machine learning approaches, to\nhandle the nuances of sensing, actuation, and control. While these methods are\nhighly effective, they are notoriously difficult to assure. Moreover, within\nuncertain and dynamic environments, design time assurance analyses may not be\nsufficient to guarantee safety. Thus, it is critical to monitor the correctness\nof these systems at runtime. One approach for providing runtime assurance of\nsystems with components that may not be amenable to formal analysis is the\nsimplex architecture, where an unverified component is wrapped with a safety\ncontroller and a switching logic designed to prevent dangerous behavior. In\nthis paper, we propose using a real-time reachability algorithm for the\nimplementation of the simplex architecture to assure the safety of a 1/10 scale\nopen source autonomous vehicle platform known as F1/10. The reachability\nalgorithm that we leverage (a) provides provable guarantees of safety, and (b)\nis used to detect potentially unsafe scenarios. In our approach, the need to\nanalyze an underlying controller is abstracted away, instead focusing on the\neffects of the controller's decisions on the system's future states. We\ndemonstrate the efficacy of our architecture through a vast set of experiments\nconducted both in simulation and on an embedded hardware platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Musau_P/0/1/0/all/0/1\">Patrick Musau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_N/0/1/0/all/0/1\">Nathaniel Hamilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_D/0/1/0/all/0/1\">Diego Manzanas Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinette_P/0/1/0/all/0/1\">Preston Robinette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_T/0/1/0/all/0/1\">Taylor T. Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency-Selective Geometry Upsampling of Point Clouds. (arXiv:2205.01458v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01458","description":"<p>The demand for high-resolution point clouds has increased throughout the last\nyears. However, capturing high-resolution point clouds is expensive and thus,\nfrequently replaced by upsampling of low-resolution data. Most state-of-the-art\nmethods are either restricted to a rastered grid, incorporate normal vectors,\nor are trained for a single use case. We propose to use the frequency\nselectivity principle, where a frequency model is estimated locally that\napproximates the surface of the point cloud. Then, additional points are\ninserted into the approximated surface. Our novel frequency-selective geometry\nupsampling shows superior results in terms of subjective as well as objective\nquality compared to state-of-the-art methods for scaling factors of 2 and 4. On\naverage, our proposed method shows a 4.4 times smaller point-to-point error\nthan the second best state-of-the-art PU-Net for a scale factor of 4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Heimann_V/0/1/0/all/0/1\">Viktoria Heimann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Spruck_A/0/1/0/all/0/1\">Andreas Spruck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaup_A/0/1/0/all/0/1\">Andr&#xe9; Kaup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Semantic Scene Perception using Distributed Smart Edge Sensors. (arXiv:2205.01460v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01460","description":"<p>We present a system for 3D semantic scene perception consisting of a network\nof distributed smart edge sensors. The sensor nodes are based on an embedded\nCNN inference accelerator and RGB-D and thermal cameras. Efficient vision CNN\nmodels for object detection, semantic segmentation, and human pose estimation\nrun on-device in real time. 2D human keypoint estimations, augmented with the\nRGB-D depth estimate, as well as semantically annotated point clouds are\nstreamed from the sensors to a central backend, where multiple viewpoints are\nfused into an allocentric 3D semantic scene model. As the image interpretation\nis computed locally, only semantic information is sent over the network. The\nraw images remain on the sensor boards, significantly reducing the required\nbandwidth, and mitigating privacy risks for the observed persons. We evaluate\nthe proposed system in challenging real-world multi-person scenes in our lab.\nThe proposed perception system provides a complete scene view containing\nsemantically annotated 3D geometry and estimates 3D poses of multiple persons\nin real time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bultmann_S/0/1/0/all/0/1\">Simon Bultmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subspace Diffusion Generative Models. (arXiv:2205.01490v1 [cs.LG])","link":"http://arxiv.org/abs/2205.01490","description":"<p>Score-based models generate samples by mapping noise to data (and vice versa)\nvia a high-dimensional diffusion process. We question whether it is necessary\nto run this entire process at high dimensionality and incur all the\ninconveniences thereof. Instead, we restrict the diffusion via projections onto\nsubspaces as the data distribution evolves toward noise. When applied to\nstate-of-the-art models, our framework simultaneously improves sample quality\n-- reaching an FID of 2.17 on unconditional CIFAR-10 -- and reduces the\ncomputational cost of inference for the same number of denoising steps. Our\nframework is fully compatible with continuous-time diffusion and retains its\nflexible capabilities, including exact log-likelihoods and controllable\ngeneration. Code is available at\nhttps://github.com/bjing2016/subspace-diffusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Bowen Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corso_G/0/1/0/all/0/1\">Gabriele Corso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berlinghieri_R/0/1/0/all/0/1\">Renato Berlinghieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi Jaakkola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey of Image Augmentation Techniques for Deep Learning. (arXiv:2205.01491v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01491","description":"<p>Deep learning has been achieving decent performance in computer vision\nrequiring a large volume of images, however, collecting images is expensive and\ndifficult in many scenarios. To alleviate this issue, many image augmentation\nalgorithms have been proposed as effective and efficient strategies.\nUnderstanding current algorithms is essential to find suitable methods or\ndevelop novel techniques for given tasks. In this paper, we perform a\ncomprehensive survey on image augmentation for deep learning with a novel\ninformative taxonomy. To get the basic idea why we need image augmentation, we\nintroduce the challenges in computer vision tasks and vicinity distribution.\nThen, the algorithms are split into three categories; model-free, model-based,\nand optimizing policy-based. The model-free category employs image processing\nmethods while the model-based method leverages trainable image generation\nmodels. In contrast, the optimizing policy-based approach aims to find the\noptimal operations or their combinations. Furthermore, we discuss the current\ntrend of common applications with two more active topics, leveraging different\nways to understand image augmentation, such as group and kernel theory, and\ndeploying image augmentation for unsupervised learning. Based on the analysis,\nwe believe that our survey gives a better understanding helpful to choose\nsuitable methods or design novel algorithms for practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingle Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sook Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuentes_A/0/1/0/all/0/1\">Alvaro Fuentes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dong Sun Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compact Neural Networks via Stacking Designed Basic Units. (arXiv:2205.01508v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01508","description":"<p>Unstructured pruning has the limitation of dealing with the sparse and\nirregular weights. By contrast, structured pruning can help eliminate this\ndrawback but it requires complex criterion to determine which components to be\npruned. To this end, this paper presents a new method termed TissueNet, which\ndirectly constructs compact neural networks with fewer weight parameters by\nindependently stacking designed basic units, without requiring additional\njudgement criteria anymore. Given the basic units of various architectures,\nthey are combined and stacked in a certain form to build up compact neural\nnetworks. We formulate TissueNet in diverse popular backbones for comparison\nwith the state-of-the-art pruning methods on different benchmark datasets.\nMoreover, two new metrics are proposed to evaluate compression performance.\nExperiment results show that TissueNet can achieve comparable classification\naccuracy while saving up to around 80% FLOPs and 89.7% parameters. That is,\nstacking basic units provides a new promising way for network compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1\">Weichao Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_Y/0/1/0/all/0/1\">Yiu-ming Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Juyong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MS Lesion Segmentation: Revisiting Weighting Mechanisms for Federated Learning. (arXiv:2205.01509v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01509","description":"<p>Federated learning (FL) has been widely employed for medical image analysis\nto facilitate multi-client collaborative learning without sharing raw data.\nDespite great success, FL's performance is limited for multiple sclerosis (MS)\nlesion segmentation tasks, due to variance in lesion characteristics imparted\nby different scanners and acquisition parameters. In this work, we propose the\nfirst FL MS lesion segmentation framework via two effective re-weighting\nmechanisms. Specifically, a learnable weight is assigned to each local node\nduring the aggregation process, based on its segmentation performance. In\naddition, the segmentation loss function in each client is also re-weighted\naccording to the lesion volume for the data during training. Comparison\nexperiments on two FL MS segmentation scenarios using public and clinical\ndatasets have demonstrated the effectiveness of the proposed method by\noutperforming other FL methods significantly. Furthermore, the segmentation\nperformance of FL incorporating our proposed aggregation mechanism can exceed\ncentralised training with all the raw data. The extensive evaluation also\nindicated the superiority of our method when estimating brain volume\ndifferences estimation after lesion inpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Dongnan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cabezas_M/0/1/0/all/0/1\">Mariano Cabezas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Dongang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1\">Zihao Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhan_G/0/1/0/all/0/1\">Geng Zhan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_Y/0/1/0/all/0/1\">Yuling Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kyle_K/0/1/0/all/0/1\">Kain Kyle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ly_L/0/1/0/all/0/1\">Linda Ly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">James Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shieh_C/0/1/0/all/0/1\">Chun-Chien Shieh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_A/0/1/0/all/0/1\">Aria Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karuppiah_E/0/1/0/all/0/1\">Ettikan Kandasamy Karuppiah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sullivan_R/0/1/0/all/0/1\">Ryan Sullivan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Calamante_F/0/1/0/all/0/1\">Fernando Calamante</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barnett_M/0/1/0/all/0/1\">Michael Barnett</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chenyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask Network for Joint Object Detection, Semantic Segmentation and Human Pose Estimation in Vehicle Occupancy Monitoring. (arXiv:2205.01515v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01515","description":"<p>In order to ensure safe autonomous driving, precise information about the\nconditions in and around the vehicle must be available. Accordingly, the\nmonitoring of occupants and objects inside the vehicle is crucial. In the\nstate-of-the-art, single or multiple deep neural networks are used for either\nobject recognition, semantic segmentation, or human pose estimation. In\ncontrast, we propose our Multitask Detection, Segmentation and Pose Estimation\nNetwork (MDSP) -- the first multitask network solving all these three tasks\njointly in the area of occupancy monitoring. Due to the shared architecture,\nmemory and computing costs can be saved while achieving higher accuracy.\nFurthermore, our architecture allows a flexible combination of the three\nmentioned tasks during a simple end-to-end training. We perform comprehensive\nevaluations on the public datasets SVIRO and TiCaM in order to demonstrate the\nsuperior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebert_N/0/1/0/all/0/1\">Nikolas Ebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangat_P/0/1/0/all/0/1\">Patrick Mangat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1\">Oliver Wasenm&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Generative Distillation. (arXiv:2205.01529v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01529","description":"<p>Knowledge distillation has been applied to various tasks successfully. The\ncurrent distillation algorithm usually improves students' performance by\nimitating the output of the teacher. This paper shows that teachers can also\nimprove students' representation power by guiding students' feature recovery.\nFrom this point of view, we propose Masked Generative Distillation (MGD), which\nis simple: we mask random pixels of the student's feature and force it to\ngenerate the teacher's full feature through a simple block. MGD is a truly\ngeneral feature-based distillation method, which can be utilized on various\ntasks, including image classification, object detection, semantic segmentation\nand instance segmentation. We experiment on different models with extensive\ndatasets and the results show that all the students achieve excellent\nimprovements. Notably, we boost ResNet-18 from 69.90% to 71.69% ImageNet top-1\naccuracy, RetinaNet with ResNet-50 backbone from 37.4 to 41.0 Boundingbox mAP,\nSOLO based on ResNet-50 from 33.1 to 36.2 Mask mAP and DeepLabV3 based on\nResNet-18 from 73.20 to 76.02 mIoU. Our codes are available at\nhttps://github.com/yzd-v/MGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhendong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Mingqi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dachuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiOcularGAN: Bimodal Synthesis and Annotation of Ocular Images. (arXiv:2205.01536v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01536","description":"<p>Current state-of-the-art segmentation techniques for ocular images are\ncritically dependent on large-scale annotated datasets, which are\nlabor-intensive to gather and often raise privacy concerns. In this paper, we\npresent a novel framework, called BiOcularGAN, capable of generating synthetic\nlarge-scale datasets of photorealistic (visible light and near infrared) ocular\nimages, together with corresponding segmentation labels to address these\nissues. At its core, the framework relies on a novel Dual-Branch StyleGAN2\n(DB-StyleGAN2) model that facilitates bimodal image generation, and a Semantic\nMask Generator (SMG) that produces semantic annotations by exploiting\nDB-StyleGAN2's feature space. We evaluate BiOcularGAN through extensive\nexperiments across five diverse ocular datasets and analyze the effects of\nbimodal data generation on image quality and the produced annotations. Our\nexperimental results show that BiOcularGAN is able to produce high-quality\nmatching bimodal images and annotations (with minimal manual intervention) that\ncan be used to train highly competitive (deep) segmentation models that perform\nwell across multiple real-world datasets. The source code will be made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomasevic_D/0/1/0/all/0/1\">Darian Toma&#x161;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peer_P/0/1/0/all/0/1\">Peter Peer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struc_V/0/1/0/all/0/1\">Vitomir &#x160;truc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi Scale Sparse Convolution Point Cloud Semantic Segmentation Neural Network. (arXiv:2205.01550v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01550","description":"<p>Point clouds have the characteristics of disorder, unstructured and\nsparseness.Aiming at the problem of the non-structural nature of point clouds,\nthanks to the excellent performance of convolutional neural networks in image\nprocessing, one of the solutions is to extract features from point clouds based\non two-dimensional convolutional neural networks. The three-dimensional\ninformation carried in the point cloud can be converted to two-dimensional, and\nthen processed by a two-dimensional convolutional neural network, and finally\nback-projected to three-dimensional.In the process of projecting 3D information\nto 2D and back-projection, certain information loss will inevitably be caused\nto the point cloud and category inconsistency will be introduced in the\nback-projection stage;Another solution is the voxel-based point cloud\nsegmentation method, which divides the point cloud into small grids one by\none.However, the point cloud is sparse, and the direct use of 3D convolutional\nneural network inevitably wastes computing resources. In this paper, we propose\na feature extraction module based on multi-scale ultra-sparse convolution and a\nfeature selection module based on channel attention, and build a point cloud\nsegmentation network framework based on this.By introducing multi-scale sparse\nconvolution, network could capture richer feature information based on\nconvolution kernels of different sizes, improving the segmentation result of\npoint cloud segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yunzheng Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-View Cross-Scene Multi-View Crowd Counting. (arXiv:2205.01551v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01551","description":"<p>Multi-view crowd counting has been previously proposed to utilize\nmulti-cameras to extend the field-of-view of a single camera, capturing more\npeople in the scene, and improve counting performance for occluded people or\nthose in low resolution. However, the current multi-view paradigm trains and\ntests on the same single scene and camera-views, which limits its practical\napplication. In this paper, we propose a cross-view cross-scene (CVCS)\nmulti-view crowd counting paradigm, where the training and testing occur on\ndifferent scenes with arbitrary camera layouts. To dynamically handle the\nchallenge of optimal view fusion under scene and camera layout change and\nnon-correspondence noise due to camera calibration errors or erroneous\nfeatures, we propose a CVCS model that attentively selects and fuses multiple\nviews together using camera layout geometry, and a noise view regularization\nmethod to train the model to handle non-correspondence errors. We also generate\na large synthetic multi-camera crowd counting dataset with a large number of\nscenes and camera views to capture many possible variations, which avoids the\ndifficulty of collecting and annotating such a large real dataset. We then test\nour trained CVCS model on real multi-view counting datasets, by using\nunsupervised domain transfer. The proposed CVCS model trained on synthetic data\noutperforms the same model trained only on real data, and achieves promising\nperformance compared to fully supervised methods that train and test on the\nsame single scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAFT-MSF: Self-Supervised Monocular Scene Flow using Recurrent Optimizer. (arXiv:2205.01568v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01568","description":"<p>Learning scene flow from a monocular camera still remains a challenging task\ndue to its ill-posedness as well as lack of annotated data. Self-supervised\nmethods demonstrate learning scene flow estimation from unlabeled data, yet\ntheir accuracy lags behind (semi-)supervised methods. In this paper, we\nintroduce a self-supervised monocular scene flow method that substantially\nimproves the accuracy over the previous approaches. Based on RAFT, a\nstate-of-the-art optical flow model, we design a new decoder to iteratively\nupdate 3D motion fields and disparity maps simultaneously. Furthermore, we\npropose an enhanced upsampling layer and a disparity initialization technique,\nwhich overall further improves accuracy up to 7.2%. Our method achieves\nstate-of-the-art accuracy among all self-supervised monocular scene flow\nmethods, improving accuracy by 34.2%. Our fine-tuned model outperforms the best\nprevious semi-supervised method with 228 times faster runtime. Code will be\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayramli_B/0/1/0/all/0/1\">Bayram Bayramli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hur_J/0/1/0/all/0/1\">Junhwa Hur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongtao Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RangeSeg: Range-Aware Real Time Segmentation of 3D LiDAR Point Clouds. (arXiv:2205.01570v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01570","description":"<p>Semantic outdoor scene understanding based on 3D LiDAR point clouds is a\nchallenging task for autonomous driving due to the sparse and irregular data\nstructure. This paper takes advantages of the uneven range distribution of\ndifferent LiDAR laser beams to propose a range aware instance segmentation\nnetwork, RangeSeg. RangeSeg uses a shared encoder backbone with two range\ndependent decoders. A heavy decoder only computes top of a range image where\nthe far and small objects locate to improve small object detection accuracy,\nand a light decoder computes whole range image for low computational cost. The\nresults are further clustered by the DBSCAN method with a resolution weighted\ndistance function to get instance-level segmentation results. Experiments on\nthe KITTI dataset show that RangeSeg outperforms the state-of-the-art semantic\nsegmentation methods with enormous speedup and improves the instance-level\nsegmentation performance on small and far objects. The whole RangeSeg pipeline\nmeets the real time requirement on NVIDIA\\textsuperscript{\\textregistered}\nJETSON AGX Xavier with 19 frames per second in average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tzu-Hsuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tian Sheuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Real Time 1280x720 Object Detection Chip With 585MB/s Memory Traffic. (arXiv:2205.01571v1 [cs.AR])","link":"http://arxiv.org/abs/2205.01571","description":"<p>Memory bandwidth has become the real-time bottleneck of current deep learning\naccelerators (DLA), particularly for high definition (HD) object detection.\nUnder resource constraints, this paper proposes a low memory traffic DLA chip\nwith joint hardware and software optimization. To maximize hardware utilization\nunder memory bandwidth, we morph and fuse the object detection model into a\ngroup fusion-ready model to reduce intermediate data access. This reduces the\nYOLOv2's feature memory traffic from 2.9 GB/s to 0.15 GB/s. To support group\nfusion, our previous DLA based hardware employes a unified buffer with\nwrite-masking for simple layer-by-layer processing in a fusion group. When\ncompared to our previous DLA with the same PE numbers, the chip implemented in\na TSMC 40nm process supports 1280x720@30FPS object detection and consumes 7.9X\nless external DRAM access energy, from 2607 mJ to 327.6 mJ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kuo-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_H/0/1/0/all/0/1\">Hsu-Tung Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tian-Sheuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1\">Shang-Hong Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chih-Chyau Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chun-Ming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better plain ViT baselines for ImageNet-1k. (arXiv:2205.01580v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01580","description":"<p>It is commonly accepted that the Vision Transformer model requires\nsophisticated regularization techniques to excel at ImageNet-1k scale data.\nSurprisingly, we find this is not the case and standard data augmentation is\nsufficient. This note presents a few minor modifications to the original Vision\nTransformer (ViT) vanilla training setting that dramatically improve the\nperformance of plain ViT models. Notably, 90 epochs of training surpass 76%\ntop-1 accuracy in under seven hours on a TPUv3-8, similar to the classic\nResNet50 baseline, and 300 epochs of training reach 80% in less than one day.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simpler is Better: off-the-shelf Continual Learning Through Pretrained Backbones. (arXiv:2205.01586v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01586","description":"<p>In this short paper, we propose a baseline (off-the-shelf) for Continual\nLearning of Computer Vision problems, by leveraging the power of pretrained\nmodels. By doing so, we devise a simple approach achieving strong performance\nfor most of the common benchmarks. Our approach is fast since requires no\nparameters updates and has minimal memory requirements (order of KBytes). In\nparticular, the \"training\" phase reorders data and exploit the power of\npretrained models to compute a class prototype and fill a memory bank. At\ninference time we match the closest prototype through a knn-like approach,\nproviding us the prediction. We will see how this naive solution can act as an\noff-the-shelf continual learning system. In order to better consolidate our\nresults, we compare the devised pipeline with common CNN models and show the\nsuperiority of Vision Transformers, suggesting that such architectures have the\nability to produce features of higher quality. Moreover, this simple pipeline,\nraises the same questions raised by previous works \\cite{gdumb} on the\neffective progresses made by the CL community especially in the dataset\nconsidered and the usage of pretrained models. Code is live at\nhttps://github.com/francesco-p/off-the-shelf-cl\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelosin_F/0/1/0/all/0/1\">Francesco Pelosin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bidirectional Conversion Network for Cross-Spectral Face Recognition. (arXiv:2205.01595v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01595","description":"<p>Face recognition in the infrared (IR) band has become an important supplement\nto visible light face recognition due to its advantages of independent\nbackground light, strong penetration, ability of imaging under harsh\nenvironments such as nighttime, rain and fog. However, cross-spectral face\nrecognition (i.e., VIS to IR) is very challenging due to the dramatic\ndifference between the visible light and IR imageries as well as the lack of\npaired training data. This paper proposes a framework of bidirectional\ncross-spectral conversion (BCSC-GAN) between the heterogeneous face images, and\ndesigns an adaptive weighted fusion mechanism based on information fusion\ntheory. The network reduces the cross-spectral recognition problem into an\nintra-spectral problem, and improves performance by fusing bidirectional\ninformation. Specifically, a face identity retaining module (IRM) is introduced\nwith the ability to preserve identity features, and a new composite loss\nfunction is designed to overcome the modal differences caused by different\nspectral characteristics. Two datasets of TINDERS and CASIA were tested, where\nperformance metrics of FID, recognition rate, equal error rate and normalized\ndistance were compared. Results show that our proposed network is superior than\nother state-of-the-art methods. Additionally, the proposed rule of Self\nAdaptive Weighted Fusion (SAWF) is better than the recognition results of the\nunfused case and other traditional fusion rules that are commonly used, which\nfurther justifies the effectiveness and superiority of the proposed\nbidirectional conversion approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhicheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liaojun Pang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Modeling Creative Processes for Algorithmic Painting. (arXiv:2205.01605v1 [cs.AI])","link":"http://arxiv.org/abs/2205.01605","description":"<p>This paper proposes a framework for computational modeling of artistic\npainting algorithms, inspired by human creative practices. Based on examples\nfrom expert artists and from the author's own experience, the paper argues that\ncreative processes often involve two important components: vague, high-level\ngoals (e.g., \"make a good painting\"), and exploratory processes for discovering\nnew ideas. This paper then sketches out possible computational mechanisms for\nimitating those elements of the painting process, including underspecified loss\nfunctions and iterative painting procedures with explicit task decompositions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hertzmann_A/0/1/0/all/0/1\">Aaron Hertzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Segmentation of Aircraft Dents in Point Clouds. (arXiv:2205.01614v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01614","description":"<p>Dents on the aircraft skin are frequent and may easily go undetected during\nairworthiness checks, as their inspection process is tedious and extremely\nsubject to human factors and environmental conditions. Nowadays, 3D scanning\ntechnologies are being proposed for more reliable, human-independent\nmeasurements, yet the process of inspection and reporting remains laborious and\ntime consuming because data acquisition and validation are still carried out by\nthe engineer. For full automation of dent inspection, the acquired point cloud\ndata must be analysed via a reliable segmentation algorithm, releasing humans\nfrom the search and evaluation of damage. This paper reports on two\ndevelopments towards automated dent inspection. The first is a method to\ngenerate a synthetic dataset of dented surfaces to train a fully convolutional\nneural network. The training of machine learning algorithms needs a substantial\nvolume of dent data, which is not readily available. Dents are thus simulated\nin random positions and shapes, within criteria and definitions of a Boeing 737\nstructural repair manual. The noise distribution from the scanning apparatus is\nthen added to reflect the complete process of 3D point acquisition on the\ntraining. The second proposition is a surface fitting strategy to convert 3D\npoint clouds to 2.5D. This allows higher resolution point clouds to be\nprocessed with a small amount of memory compared with state-of-the-art methods\ninvolving 3D sampling approaches. Simulations with available ground truth data\nshow that the proposed technique reaches an intersection-over-union of over\n80%. Experiments over dent samples prove an effective detection of dents with a\nspeed of over 500 000 points per second.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lafiosca_P/0/1/0/all/0/1\">Pasquale Lafiosca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_I/0/1/0/all/0/1\">Ip-Shing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avdelidis_N/0/1/0/all/0/1\">Nicolas P. Avdelidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynopSet: Multiscale Visual Abstraction Set for Explanatory Analysis of DNA Nanotechnology Simulations. (arXiv:2205.01628v1 [q-bio.QM])","link":"http://arxiv.org/abs/2205.01628","description":"<p>We propose a new abstraction set (SynopSet) that has a continuum of visual\nrepresentations for the explanatory analysis of molecular dynamics simulations\n(MDS) in the DNA nanotechnology domain. By re-purposing the commonly used\nprogress bar and designing novel visuals, as well as transforming the data from\nthe domain format to a format that better fits the newly designed visuals, we\ncompose this new set of representations. This set is also designed to be\ncapable of showing all spatial and temporal details, and all structural\ncomplexity, or abstracting these to various degrees, enabling both the slow\nplayback of the simulation for detailed examinations or very fast playback for\nan overview that helps to efficiently identify events of interest, as well as\nseveral intermediate levels between these two extremes. For any pair of\nsuccessive representations, we demonstrate smooth, continuous transitions,\nenabling users to keep track of relevant information from one representation to\nthe next. By providing multiple representations suited to different temporal\nresolutions and connected by smooth transitions, we enable time-efficient\nsimulation analysis, giving users the opportunity to examine and present\nimportant phases in great detail, or leverage abstract representations to go\nover uneventful phases much faster. Domain experts can thus gain actionable\ninsight about their simulations and communicate it in a much shorter time.\nFurther, the novel representations are more intuitive and also enable\nresearchers unfamiliar with MDS analysis graphs to better understand the\nsimulation results. We assessed the effectiveness of SynopSet on 12 DNA\nnanostructure simulations together with a domain expert. We have also shown\nthat our set of representations can be systematically located in a\nvisualization space, dubbed SynopSpace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Luo_D/0/1/0/all/0/1\">Deng Luo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kouyoumdjian_A/0/1/0/all/0/1\">Alexandre Kouyoumdjian</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Strnad_O/0/1/0/all/0/1\">Ond&#x159;ej Strnad</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Miao_H/0/1/0/all/0/1\">Haichao Miao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Barisic_I/0/1/0/all/0/1\">Ivan Bari&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Viola_I/0/1/0/all/0/1\">Ivan Viola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view Geometry: Correspondences Refinement Based on Algebraic Properties. (arXiv:2205.01634v1 [cs.CG])","link":"http://arxiv.org/abs/2205.01634","description":"<p>Correspondences estimation or feature matching is a key step in the\nimage-based 3D reconstruction problem. In this paper, we propose two algebraic\nproperties for correspondences. The first is a rank deficient matrix construct\nfrom the correspondences of at least nine key-points on two images (two-view\ncorrespondences) and the second is also another rank deficient matrix built\nfrom the other correspondences of six key-points on at least five images\n(multi-view correspondences). To our knowledge, there are no theoretical\nresults for multi-view correspondences prior to this paper. To obtain accurate\ncorrespondences, multi-view correspondences seem to be more useful than\ntwo-view correspondences. From these two algebraic properties, we propose an\nrefinement algorithm for correspondences. This algorithm is a combination of\ncorrespondences refinement, outliers recognition and missing key-points\nrecovery. Real experiments from the project of reconstructing Buddha statue\nshow that the proposed refinement algorithm can reduce the average error from\n77 pixels to 55 pixels on the correspondences estimation. This drop is\nsubstantial and it validates our results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Kien Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Object Detection with Mean-Teacher Transformer. (arXiv:2205.01643v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01643","description":"<p>Recently, DEtection TRansformer (DETR), an end-to-end object detection\npipeline, has achieved promising performance. However, it requires large-scale\nlabeled data and suffers from domain shift, especially when no labeled data is\navailable in the target domain. To solve this problem, we propose an end-to-end\ncross-domain detection transformer based on the mean teacher knowledge transfer\n(MTKT), which transfers knowledge between domains via pseudo labels. To improve\nthe quality of pseudo labels in the target domain, which is a crucial factor\nfor better domain adaptation, we design three levels of source-target feature\nalignment strategies based on the architecture of the Transformer, including\ndomain query-based feature alignment (DQFA), bi-level-graph-based prototype\nalignment (BGPA), and token-wise image feature alignment (TIFA). These three\nlevels of feature alignment match the global, local, and instance features\nbetween source and target, respectively. With these strategies, more accurate\npseudo labels can be obtained, and knowledge can be better transferred from\nsource to target, thus improving the cross-domain capability of the detection\ntransformer. Extensive experiments demonstrate that our proposed method\nachieves state-of-the-art performance on three domain adaptation scenarios,\nespecially the result of Sim10k to Cityscapes scenario is remarkably improved\nfrom 52.6 mAP to 57.9 mAP. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jinze Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaobao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Haoyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakata_Y/0/1/0/all/0/1\">Yohei Nakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gudovskiy_D/0/1/0/all/0/1\">Denis Gudovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okuno_T/0/1/0/all/0/1\">Tomoyuki Okuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Enriched Features for Fast Image Restoration and Enhancement. (arXiv:2205.01649v1 [eess.IV])","link":"http://arxiv.org/abs/2205.01649","description":"<p>Given a degraded input image, image restoration aims to recover the missing\nhigh-quality image content. Numerous applications demand effective image\nrestoration, e.g., computational photography, surveillance, autonomous\nvehicles, and remote sensing. Significant advances in image restoration have\nbeen made in recent years, dominated by convolutional neural networks (CNNs).\nThe widely-used CNN-based methods typically operate either on full-resolution\nor on progressively low-resolution representations. In the former case, spatial\ndetails are preserved but the contextual information cannot be precisely\nencoded. In the latter case, generated outputs are semantically reliable but\nspatially less accurate. This paper presents a new architecture with a holistic\ngoal of maintaining spatially-precise high-resolution representations through\nthe entire network, and receiving complementary contextual information from the\nlow-resolution representations. The core of our approach is a multi-scale\nresidual block containing the following key elements: (a) parallel\nmulti-resolution convolution streams for extracting multi-scale features, (b)\ninformation exchange across the multi-resolution streams, (c) non-local\nattention mechanism for capturing contextual information, and (d) attention\nbased multi-scale feature aggregation. Our approach learns an enriched set of\nfeatures that combines contextual information from multiple scales, while\nsimultaneously preserving the high-resolution spatial details. Extensive\nexperiments on six real image benchmark datasets demonstrate that our method,\nnamed as MIRNet-v2 , achieves state-of-the-art results for a variety of image\nprocessing tasks, including defocus deblurring, image denoising,\nsuper-resolution, and image enhancement. The source code and pre-trained models\nare available at https://github.com/swz30/MIRNetv2\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zamir_S/0/1/0/all/0/1\">Syed Waqas Zamir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arora_A/0/1/0/all/0/1\">Aditya Arora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Episodic Memory Question Answering. (arXiv:2205.01652v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01652","description":"<p>Egocentric augmented reality devices such as wearable glasses passively\ncapture visual data as a human wearer tours a home environment. We envision a\nscenario wherein the human communicates with an AI agent powering such a device\nby asking questions (e.g., where did you last see my keys?). In order to\nsucceed at this task, the egocentric AI assistant must (1) construct\nsemantically rich and efficient scene memories that encode spatio-temporal\ninformation about objects seen during the tour and (2) possess the ability to\nunderstand the question and ground its answer into the semantic memory\nrepresentation. Towards that end, we introduce (1) a new task - Episodic Memory\nQuestion Answering (EMQA) wherein an egocentric AI assistant is provided with a\nvideo sequence (the tour) and a question as an input and is asked to localize\nits answer to the question within the tour, (2) a dataset of grounded questions\ndesigned to probe the agent's spatio-temporal understanding of the tour, and\n(3) a model for the task that encodes the scene as an allocentric, top-down\nsemantic feature map and grounds the question into the map to localize the\nanswer. We show that our choice of episodic scene memory outperforms naive,\noff-the-shelf solutions for the task as well as a host of very competitive\nbaselines and is robust to noise in depth, pose as well as camera jitter. The\nproject page can be found at: https://samyak-268.github.io/emqa .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1\">Samyak Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharur_S/0/1/0/all/0/1\">Sameer Dharur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cartillier_V/0/1/0/all/0/1\">Vincent Cartillier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_R/0/1/0/all/0/1\">Ruta Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_M/0/1/0/all/0/1\">Mukul Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeoRefine: Self-Supervised Online Depth Refinement for Accurate Dense Mapping. (arXiv:2205.01656v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01656","description":"<p>We present a robust and accurate depth refinement system, named GeoRefine,\nfor geometrically-consistent dense mapping from monocular sequences. GeoRefine\nconsists of three modules: a hybrid SLAM module using learning-based priors, an\nonline depth refinement module leveraging self-supervision, and a global\nmapping module via TSDF fusion. The proposed system is online by design and\nachieves great robustness and accuracy via: (i) a robustified hybrid SLAM that\nincorporates learning-based optical flow and/or depth; (ii) self-supervised\nlosses that leverage SLAM outputs and enforce long-term geometric consistency;\n(iii) careful system design that avoids degenerate cases in online depth\nrefinement. We extensively evaluate GeoRefine on multiple public datasets and\nreach as low as $5\\%$ absolute relative depth errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qingan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Representation Learning for Zero-shot Action Recognition. (arXiv:2205.01657v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01657","description":"<p>We present a cross-modal Transformer-based framework, which jointly encodes\nvideo data and text labels for zero-shot action recognition (ZSAR). Our model\nemploys a conceptually new pipeline by which visual representations are learned\nin conjunction with visual-semantic associations in an end-to-end manner. The\nmodel design provides a natural mechanism for visual and semantic\nrepresentations to be learned in a shared knowledge space, whereby it\nencourages the learned visual embedding to be discriminative and more\nsemantically consistent. In zero-shot inference, we devise a simple semantic\ntransfer scheme that embeds semantic relatedness information between seen and\nunseen classes to composite unseen visual prototypes. Accordingly, the\ndiscriminative features in the visual structure could be preserved and\nexploited to alleviate the typical zero-shot issues of information loss,\nsemantic gap, and the hubness problem. Under a rigorous zero-shot setting of\nnot pre-training on additional datasets, the experiment results show our model\nconsiderably improves upon the state of the arts in ZSAR, reaching encouraging\ntop-1 accuracy on UCF101, HMDB51, and ActivityNet benchmark datasets. Code will\nbe made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chung-Ching Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DANBO: Disentangled Articulated Neural Body Representations via Graph Neural Networks. (arXiv:2205.01666v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01666","description":"<p>Deep learning greatly improved the realism of animatable human models by\nlearning geometry and appearance from collections of 3D scans, template meshes,\nand multi-view imagery. High-resolution models enable photo-realistic avatars\nbut at the cost of requiring studio settings not available to end users. Our\ngoal is to create avatars directly from raw images without relying on expensive\nstudio setups and surface tracking. While a few such approaches exist, those\nhave limited generalization capabilities and are prone to learning spurious\n(chance) correlations between irrelevant body parts, resulting in implausible\ndeformations and missing body parts on unseen poses. We introduce a three-stage\nmethod that induces two inductive biases to better disentangled pose-dependent\ndeformation. First, we model correlations of body parts explicitly with a graph\nneural network. Second, to further reduce the effect of chance correlations, we\nintroduce localized per-bone features that use a factorized volumetric\nrepresentation and a new aggregation function. We demonstrate that our model\nproduces realistic body shapes under challenging unseen poses and shows\nhigh-quality image synthesis. Our proposed representation strikes a better\ntrade-off between model capacity, expressiveness, and robustness than competing\nmethods. Project website: https://lemonatsu.github.io/danbo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shih-Yang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Visual Editing with a Generatively Pre-Trained Artist. (arXiv:2205.01668v1 [cs.CV])","link":"http://arxiv.org/abs/2205.01668","description":"<p>We consider the targeted image editing problem: blending a region in a source\nimage with a driver image that specifies the desired change. Differently from\nprior works, we solve this problem by learning a conditional probability\ndistribution of the edits, end-to-end. Training such a model requires\naddressing a fundamental technical challenge: the lack of example edits for\ntraining. To this end, we propose a self-supervised approach that simulates\nedits by augmenting off-the-shelf images in a target domain. The benefits are\nremarkable: implemented as a state-of-the-art auto-regressive transformer, our\napproach is simple, sidesteps difficulties with previous methods based on\nGAN-like priors, obtains significantly better edits, and is efficient.\nFurthermore, we show that different blending effects can be learned by an\nintuitive control of the augmentation process, with no other changes required\nto the model architecture. We demonstrate the superiority of this approach\nacross several datasets in extensive quantitative and qualitative experiments,\nincluding human studies, significantly outperforming prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1\">Andrew Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Cheng-Yang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parkhi_O/0/1/0/all/0/1\">Omkar Parkhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation and Image Classification via Deep Conditional Adaptation Network. (arXiv:2006.07776v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.07776","description":"<p>Unsupervised domain adaptation aims to generalize the supervised model\ntrained on a source domain to an unlabeled target domain. Marginal distribution\nalignment of feature spaces is widely used to reduce the domain discrepancy\nbetween the source and target domains. However, it assumes that the source and\ntarget domains share the same label distribution, which limits their\napplication scope. In this paper, we consider a more general application\nscenario where the label distributions of the source and target domains are not\nthe same. In this scenario, marginal distribution alignment-based methods will\nbe vulnerable to negative transfer. To address this issue, we propose a novel\nunsupervised domain adaptation method, Deep Conditional Adaptation Network\n(DCAN), based on conditional distribution alignment of feature spaces. To be\nspecific, we reduce the domain discrepancy by minimizing the Conditional\nMaximum Mean Discrepancy between the conditional distributions of deep features\non the source and target domains, and extract the discriminant information from\ntarget domain by maximizing the mutual information between samples and the\nprediction labels. In addition, DCAN can be used to address a special scenario,\nPartial unsupervised domain adaptation, where the target domain category is a\nsubset of the source domain category. Experiments on both unsupervised domain\nadaptation and Partial unsupervised domain adaptation show that DCAN achieves\nsuperior classification performance over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_P/0/1/0/all/0/1\">Pengfei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chuan-Xian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dao-Qing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Learning of Multi-Object 3D Scene Decompositions Using Deep Shape Priors. (arXiv:2010.04030v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.04030","description":"<p>Representing scenes at the granularity of objects is a prerequisite for scene\nunderstanding and decision making. We propose PriSMONet, a novel approach based\non Prior Shape knowledge for learning Multi-Object 3D scene decomposition and\nrepresentations from single images. Our approach learns to decompose images of\nsynthetic scenes with multiple objects on a planar surface into its constituent\nscene objects and to infer their 3D properties from a single view. A recurrent\nencoder regresses a latent representation of 3D shape, pose and texture of each\nobject from an input RGB image. By differentiable rendering, we train our model\nto decompose scenes from RGB-D images in a self-supervised way. The 3D shapes\nare represented continuously in function-space as signed distance functions\nwhich we pre-train from example shapes in a supervised way. These shape priors\nprovide weak supervision signals to better condition the challenging overall\nlearning task. We evaluate the accuracy of our model in inferring 3D scene\nlayout, demonstrate its generative capabilities, assess its generalization to\nreal images, and point out benefits of the learned representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elich_C/0/1/0/all/0/1\">Cathrin Elich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stueckler_J/0/1/0/all/0/1\">Joerg Stueckler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepCloth: Neural Garment Representation for Shape and Style Editing. (arXiv:2011.14619v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14619","description":"<p>Garment representation, editing and animation are challenging topics in the\narea of computer vision and graphics. It remains difficult for existing garment\nrepresentations to achieve smooth and plausible transitions between different\nshapes and topologies. In this work, we introduce, DeepCloth, a unified\nframework for garment representation, reconstruction, animation and editing.\nOur unified framework contains 3 components: First, we represent the garment\ngeometry with a \"topology-aware UV-position map\", which allows for the unified\ndescription of various garments with different shapes and topologies by\nintroducing an additional topology-aware UV-mask for the UV-position map.\nSecond, to further enable garment reconstruction and editing, we contribute a\nmethod to embed the UV-based representations into a continuous feature space,\nwhich enables garment shape reconstruction and editing by optimization and\ncontrol in the latent space, respectively. Finally, we propose a garment\nanimation method by unifying our neural garment representation with body shape\nand pose, which achieves plausible garment animation results leveraging the\ndynamic information encoded by our shape and style representation, even under\ndrastic garment editing operations. To conclude, with DeepCloth, we move a step\nforward in establishing a more flexible and general 3D garment digitization\nframework. Experiments demonstrate that our method can achieve state-of-the-art\ngarment representation performance compared with previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhaoqi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yangang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalising via Meta-Examples for Continual Learning in the Wild. (arXiv:2101.12081v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.12081","description":"<p>Future deep learning systems call for techniques that can deal with the\nevolving nature of temporal data and scarcity of annotations when new problems\noccur. As a step towards this goal, we present FUSION (Few-shot UnSupervIsed\ncONtinual learning), a learning strategy that enables a neural network to learn\nquickly and continually on streams of unlabelled data and unbalanced tasks. The\nobjective is to maximise the knowledge extracted from the unlabelled data\nstream (unsupervised), favor the forward transfer of previously learnt tasks\nand features (continual) and exploit as much as possible the supervised\ninformation when available (few-shot). The core of FUSION is MEML -\nMeta-Example Meta-Learning - that consolidates a meta-representation through\nthe use of a self-attention mechanism during a single inner loop in the\nmeta-optimisation stage. To further enhance the capability of MEML to\ngeneralise from few data, we extend it by creating various augmented surrogate\ntasks and by optimising over the hardest. An extensive experimental evaluation\non public computer vision benchmarks shows that FUSION outperforms existing\nstate-of-the-art solutions both in the few-shot and continual learning\nexperimental settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertugli_A/0/1/0/all/0/1\">Alessia Bertugli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincenzi_S/0/1/0/all/0/1\">Stefano Vincenzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1\">Simone Calderara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1\">Andrea Passerini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural 3D Video Synthesis from Multi-view Video. (arXiv:2103.02597v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.02597","description":"<p>We propose a novel approach for 3D video synthesis that is able to represent\nmulti-view video recordings of a dynamic real-world scene in a compact, yet\nexpressive representation that enables high-quality view synthesis and motion\ninterpolation. Our approach takes the high quality and compactness of static\nneural radiance fields in a new direction: to a model-free, dynamic setting. At\nthe core of our approach is a novel time-conditioned neural radiance field that\nrepresents scene dynamics using a set of compact latent codes. We are able to\nsignificantly boost the training speed and perceptual quality of the generated\nimagery by a novel hierarchical training scheme in combination with ray\nimportance sampling. Our learned representation is highly compact and able to\nrepresent a 10 second 30 FPS multiview video recording by 18 cameras with a\nmodel size of only 28MB. We demonstrate that our method can render\nhigh-fidelity wide-angle novel views at over 1K resolution, even for complex\nand dynamic scenes. We perform an extensive qualitative and quantitative\nevaluation that shows that our approach outperforms the state of the art.\nProject website: https://neural-3d-video.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slavcheva_M/0/1/0/all/0/1\">Mira Slavcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_S/0/1/0/all/0/1\">Simon Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassner_C/0/1/0/all/0/1\">Christoph Lassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changil Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_T/0/1/0/all/0/1\">Tanner Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovegrove_S/0/1/0/all/0/1\">Steven Lovegrove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goesele_M/0/1/0/all/0/1\">Michael Goesele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newcombe_R/0/1/0/all/0/1\">Richard Newcombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhaoyang Lv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FetalNet: Multi-task Deep Learning Framework for Fetal Ultrasound Biometric Measurements. (arXiv:2107.06943v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.06943","description":"<p>In this paper, we propose an end-to-end multi-task neural network called\nFetalNet with an attention mechanism and stacked module for spatio-temporal\nfetal ultrasound scan video analysis. Fetal biometric measurement is a standard\nexamination during pregnancy used for the fetus growth monitoring and\nestimation of gestational age and fetal weight. The main goal in fetal\nultrasound scan video analysis is to find proper standard planes to measure the\nfetal head, abdomen and femur. Due to natural high speckle noise and shadows in\nultrasound data, medical expertise and sonographic experience are required to\nfind the appropriate acquisition plane and perform accurate measurements of the\nfetus. In addition, existing computer-aided methods for fetal US biometric\nmeasurement address only one single image frame without considering temporal\nfeatures. To address these shortcomings, we propose an end-to-end multi-task\nneural network for spatio-temporal ultrasound scan video analysis to\nsimultaneously localize, classify and measure the fetal body parts. We propose\na new encoder-decoder segmentation architecture that incorporates a\nclassification branch. Additionally, we employ an attention mechanism with a\nstacked module to learn salient maps to suppress irrelevant US regions and\nefficient scan plane localization. We trained on the fetal ultrasound video\ncomes from routine examinations of 700 different patients. Our method called\nFetalNet outperforms existing state-of-the-art methods in both classification\nand segmentation in fetal ultrasound video recordings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wlodarczyk_T/0/1/0/all/0/1\">Tomasz W&#x142;odarczyk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klasa_A/0/1/0/all/0/1\">Adam Klasa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lipa_M/0/1/0/all/0/1\">Micha&#x142; Lipa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair Conformal Predictors for Applications in Medical Imaging. (arXiv:2109.04392v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.04392","description":"<p>Deep learning has the potential to automate many clinically useful tasks in\nmedical imaging. However translation of deep learning into clinical practice\nhas been hindered by issues such as lack of the transparency and\ninterpretability in these \"black box\" algorithms compared to traditional\nstatistical methods. Specifically, many clinical deep learning models lack\nrigorous and robust techniques for conveying certainty (or lack thereof) in\ntheir predictions -- ultimately limiting their appeal for extensive use in\nmedical decision-making. Furthermore, numerous demonstrations of algorithmic\nbias have increased hesitancy towards deployment of deep learning for clinical\napplications. To this end, we explore how conformal predictions can complement\nexisting deep learning approaches by providing an intuitive way of expressing\nuncertainty while facilitating greater transparency to clinical users. In this\npaper, we conduct field interviews with radiologists to assess possible\nuse-cases for conformal predictors. Using insights gathered from these\ninterviews, we devise two clinical use-cases and empirically evaluate several\nmethods of conformal predictions on a dermatology photography dataset for skin\nlesion classification. We show how to modify conformal predictions to be more\nadaptive to subgroup differences in patient skin tones through equalized\ncoverage. Finally, we compare conformal prediction against measures of\nepistemic uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_C/0/1/0/all/0/1\">Charles Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemay_A/0/1/0/all/0/1\">Andreanne Lemay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Ken Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An optimised deep spiking neural network architecture without gradients. (arXiv:2109.12813v3 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2109.12813","description":"<p>We present an end-to-end trainable modular event-driven neural architecture\nthat uses local synaptic and threshold adaptation rules to perform\ntransformations between arbitrary spatio-temporal spike patterns. The\narchitecture represents a highly abstracted model of existing Spiking Neural\nNetwork (SNN) architectures. The proposed Optimized Deep Event-driven Spiking\nneural network Architecture (ODESA) can simultaneously learn hierarchical\nspatio-temporal features at multiple arbitrary time scales. ODESA performs\nonline learning without the use of error back-propagation or the calculation of\ngradients. Through the use of simple local adaptive selection thresholds at\neach node, the network rapidly learns to appropriately allocate its neuronal\nresources at each layer for any given problem without using a real-valued error\nmeasure. These adaptive selection thresholds are the central feature of ODESA,\nensuring network stability and remarkable robustness to noise as well as to the\nselection of initial system parameters. Network activations are inherently\nsparse due to a hard Winner-Take-All (WTA) constraint at each layer. We\nevaluate the architecture on existing spatio-temporal datasets, including the\nspike-encoded IRIS and TIDIGITS datasets, as well as a novel set of tasks based\non International Morse Code that we created. These tests demonstrate the\nhierarchical spatio-temporal learning capabilities of ODESA. Through these\ntests, we demonstrate ODESA can optimally solve practical and highly\nchallenging hierarchical spatio-temporal learning tasks with the minimum\npossible number of computing nodes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bethi_Y/0/1/0/all/0/1\">Yeshwanth Bethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_G/0/1/0/all/0/1\">Gregory Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaik_A/0/1/0/all/0/1\">Andre van Schaik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afshar_S/0/1/0/all/0/1\">Saeed Afshar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADOP: Approximate Differentiable One-Pixel Point Rendering. (arXiv:2110.06635v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06635","description":"<p>In this paper we present ADOP, a novel point-based, differentiable neural\nrendering pipeline. Like other neural renderers, our system takes as input\ncalibrated camera images and a proxy geometry of the scene, in our case a point\ncloud. To generate a novel view, the point cloud is rasterized with learned\nfeature vectors as colors and a deep neural network fills the remaining holes\nand shades each output pixel. The rasterizer renders points as one-pixel\nsplats, which makes it very fast and allows us to compute gradients with\nrespect to all relevant input parameters efficiently. Furthermore, our pipeline\ncontains a fully differentiable physically-based photometric camera model,\nincluding exposure, white balance, and a camera response function. Following\nthe idea of inverse rendering, we use our renderer to refine its input in order\nto reduce inconsistencies and optimize the quality of its output. In\nparticular, we can optimize structural parameters like the camera pose, lens\ndistortions, point positions and features, and a neural environment map, but\nalso photometric parameters like camera response function, vignetting, and\nper-image exposure and white balance. Because our pipeline includes photometric\nparameters, e.g.~exposure and camera response function, our system can smoothly\nhandle input images with varying exposure and white balance, and generates\nhigh-dynamic range output. We show that due to the improved input, we can\nachieve high render quality, also for difficult input, e.g. with imperfect\ncamera calibrations, inaccurate proxy geometry, or varying exposure. As a\nresult, a simpler and thus faster deep neural network is sufficient for\nreconstruction. In combination with the fast point rasterization, ADOP achieves\nreal-time rendering rates even for models with well over 100M points.\nhttps://github.com/darglein/ADOP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruckert_D/0/1/0/all/0/1\">Darius R&#xfc;ckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_L/0/1/0/all/0/1\">Linus Franke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamminger_M/0/1/0/all/0/1\">Marc Stamminger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CeyMo: See More on Roads -- A Novel Benchmark Dataset for Road Marking Detection. (arXiv:2110.11867v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11867","description":"<p>In this paper, we introduce a novel road marking benchmark dataset for road\nmarking detection, addressing the limitations in the existing publicly\navailable datasets such as lack of challenging scenarios, prominence given to\nlane markings, unavailability of an evaluation script, lack of annotation\nformats and lower resolutions. Our dataset consists of 2887 total images with\n4706 road marking instances belonging to 11 classes. The images have a high\nresolution of 1920 x 1080 and capture a wide range of traffic, lighting and\nweather conditions. We provide road marking annotations in polygons, bounding\nboxes and pixel-level segmentation masks to facilitate a diverse range of road\nmarking detection algorithms. The evaluation metrics and the evaluation script\nwe provide, will further promote direct comparison of novel approaches for road\nmarking detection with existing methods. Furthermore, we evaluate the\neffectiveness of using both instance segmentation and object detection based\napproaches for the road marking detection task. Speed and accuracy scores for\ntwo instance segmentation models and two object detector models are provided as\na performance baseline for our benchmark dataset. The dataset and the\nevaluation script is publicly available at https://github.com/oshadajay/CeyMo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayasinghe_O/0/1/0/all/0/1\">Oshada Jayasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemachandra_S/0/1/0/all/0/1\">Sahan Hemachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anhettigama_D/0/1/0/all/0/1\">Damith Anhettigama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kariyawasam_S/0/1/0/all/0/1\">Shenali Kariyawasam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasekara_P/0/1/0/all/0/1\">Peshala Jayasekara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMGP: Lifted Multicut Meets Geometry Projections for Multi-Camera Multi-Object Tracking. (arXiv:2111.11892v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11892","description":"<p>Multi-Camera Multi-Object Tracking is currently drawing attention in the\ncomputer vision field due to its superior performance in real-world\napplications such as video surveillance in crowded scenes or in wide spaces. In\nthis work, we propose a mathematically elegant multi-camera multiple object\ntracking approach based on a spatial-temporal lifted multicut formulation. Our\nmodel utilizes state-of-the-art tracklets produced by single-camera trackers as\nproposals. As these tracklets may contain ID-Switch errors, we refine them\nthrough a novel pre-clustering obtained from 3D geometry projections. As a\nresult, we derive a better tracking graph without ID switches and more precise\naffinity costs for the data association phase. Tracklets are then matched to\nmulti-camera trajectories by solving a global lifted multicut formulation that\nincorporates short and long-range temporal interactions on tracklets located in\nthe same camera as well as inter-camera ones. Experimental results on the\nWildTrack dataset yield near-perfect performance, outperforming\nstate-of-the-art trackers on Campus while being on par on the PETS-09 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy M. H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henschel_R/0/1/0/all/0/1\">Roberto Henschel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1\">Daniel Sonntag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-VField: Adversarial Augmentation of Point Clouds for Domain Generalization in 3D Object Detection. (arXiv:2112.04764v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04764","description":"<p>As 3D object detection on point clouds relies on the geometrical\nrelationships between the points, non-standard object shapes can hinder a\nmethod's detection capability. However, in safety-critical settings, robustness\nto out-of-domain and long-tail samples is fundamental to circumvent dangerous\nissues, such as the misdetection of damaged or rare cars. In this work, we\nsubstantially improve the generalization of 3D object detectors to\nout-of-domain data by deforming point clouds during training. We achieve this\nwith 3D-VField: a novel data augmentation method that plausibly deforms objects\nvia vector fields learned in an adversarial fashion. Our approach constrains 3D\npoints to slide along their sensor view rays while neither adding nor removing\nany of them. The obtained vectors are transferable, sample-independent and\npreserve shape and occlusions. Despite training only on a standard dataset,\nsuch as KITTI, augmenting with our vector fields significantly improves the\ngeneralization to differently shaped objects and scenes. Towards this end, we\npropose and share CrashD: a synthetic dataset of realistic damaged and rare\ncars, with a variety of crash scenarios. Extensive experiments on KITTI, Waymo,\nour CrashD and SUN RGB-D show the generalizability of our techniques to\nout-of-domain data, different models and sensors, namely LiDAR and ToF cameras,\nfor both indoor and outdoor scenes. Our CrashD dataset is available at\nhttps://crashd-cars.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lehner_A/0/1/0/all/0/1\">Alexander Lehner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1\">Stefano Gasperini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcos_Ramiro_A/0/1/0/all/0/1\">Alvaro Marcos-Ramiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1\">Michael Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahani_M/0/1/0/all/0/1\">Mohammad-Ali Nikouei Mahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter-COMMs: Detecting Climate, COVID, and Military Multimodal Misinformation. (arXiv:2112.08594v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08594","description":"<p>Detecting out-of-context media, such as \"mis-captioned\" images on Twitter, is\na relevant problem, especially in domains of high public significance. In this\nwork we aim to develop defenses against such misinformation for the topics of\nClimate Change, COVID-19, and Military Vehicles. We first present a large-scale\nmultimodal dataset with over 884k tweets relevant to these topics. Next, we\npropose a detection method, based on the state-of-the-art CLIP model, that\nleverages automatically generated hard image-text mismatches. While this\napproach works well on our automatically constructed out-of-context tweets, we\naim to validate its usefulness on data representative of the real world. Thus,\nwe test it on a set of human-generated fakes created by mimicking in-the-wild\nmisinformation. We achieve an 11% detection improvement in a high precision\nregime over a strong baseline. Finally, we share insights about our best model\ndesign and analyze the challenges of this emerging threat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biamby_G/0/1/0/all/0/1\">Giscard Biamby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Grace Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer. (arXiv:2112.08995v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2112.08995","description":"<p>Machines that can represent and describe environmental soundscapes have\npractical potential, e.g., for audio tagging and captioning systems. Prevailing\nlearning paradigms have been relying on parallel audio-text data, which is,\nhowever, scarcely available on the web. We propose VIP-ANT that induces\n\\textbf{A}udio-\\textbf{T}ext alignment without using any parallel audio-text\ndata. Our key idea is to share the image modality between bi-modal image-text\nrepresentations and bi-modal image-audio representations; the image modality\nfunctions as a pivot and connects audio and text in a tri-modal embedding space\nimplicitly.\n</p>\n<p>In a difficult zero-shot setting with no paired audio-text data, our model\ndemonstrates state-of-the-art zero-shot performance on the ESC50 and US8K audio\nclassification tasks, and even surpasses the supervised state of the art for\nClotho caption retrieval (with audio queries) by 2.2\\% R@1. We further\ninvestigate cases of minimal audio-text supervision, finding that, e.g., just a\nfew hundred supervised audio-text pairs increase the zero-shot audio\nclassification accuracy by 8\\% on US8K. However, to match human parity on some\nzero-shot tasks, our empirical scaling experiments suggest that we would need\nabout $2^{21} \\approx 2M$ supervised audio-caption pairs. Our work opens up new\navenues for learning audio-text connections with little to no parallel\naudio-text data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manifoldron: Direct Space Partition via Manifold Discovery. (arXiv:2201.05279v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.05279","description":"<p>A neural network with the widely-used ReLU activation has been shown to\npartition the sample space into many convex polytopes for prediction. However,\nthe parameterized way a neural network and other machine learning models use to\npartition the space has imperfections, \\textit{e}.\\textit{g}., the compromised\ninterpretability for complex models, the inflexibility in decision boundary\nconstruction due to the generic character of the model, and the risk of being\ntrapped into shortcut solutions. In contrast, although the non-parameterized\nmodels can adorably avoid or downplay these issues, they are usually\ninsufficiently powerful either due to over-simplification or the failure to\naccommodate the manifold structures of data. In this context, we first propose\na new type of machine learning models referred to as Manifoldron that directly\nderives decision boundaries from data and partitions the space via manifold\nstructure discovery. Then, we systematically analyze the key characteristics of\nthe Manifoldron such as manifold characterization capability and its link to\nneural networks. The experimental results on 4 synthetic examples, 20 public\nbenchmark datasets, and 1 real-world application demonstrate that the proposed\nManifoldron performs competitively compared to the mainstream machine learning\nmodels. We have shared our code in \\url{https://github.com/wdayang/Manifoldron}\nfor free download and evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dayang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1\">Feng-Lei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Bo-Jian Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Boce Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1\">Rongjie Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hengyong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decision boundaries and convex hulls in the feature space that deep learning functions learn from images. (arXiv:2202.04052v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04052","description":"<p>The success of deep neural networks in image classification and learning can\nbe partly attributed to the features they extract from images. It is often\nspeculated about the properties of a low-dimensional manifold that models\nextract and learn from images. However, there is not sufficient understanding\nabout this low-dimensional space based on theory or empirical evidence. For\nimage classification models, their last hidden layer is the one where images of\neach class is separated from other classes and it also has the least number of\nfeatures. Here, we develop methods and formulations to study that feature space\nfor any model. We study the partitioning of the domain in feature space,\nidentify regions guaranteed to have certain classifications, and investigate\nits implications for the pixel space. We observe that geometric arrangements of\ndecision boundaries in feature space is significantly different compared to\npixel space, providing insights about adversarial vulnerabilities, image\nmorphing, extrapolation, ambiguity in classification, and the mathematical\nunderstanding of image classification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yousefzadeh_R/0/1/0/all/0/1\">Roozbeh Yousefzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artemis: Articulated Neural Pets with Appearance and Motion synthesis. (arXiv:2202.05628v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2202.05628","description":"<p>We, humans, are entering into a virtual era and indeed want to bring animals\nto the virtual world as well for companion. Yet, computer-generated (CGI) furry\nanimals are limited by tedious off-line rendering, let alone interactive motion\ncontrol. In this paper, we present ARTEMIS, a novel neural modeling and\nrendering pipeline for generating ARTiculated neural pets with appEarance and\nMotion synthesIS. Our ARTEMIS enables interactive motion control, real-time\nanimation, and photo-realistic rendering of furry animals. The core of our\nARTEMIS is a neural-generated (NGI) animal engine, which adopts an efficient\noctree-based representation for animal animation and fur rendering. The\nanimation then becomes equivalent to voxel-level deformation based on explicit\nskeletal warping. We further use a fast octree indexing and efficient\nvolumetric rendering scheme to generate appearance and density features maps.\nFinally, we propose a novel shading network to generate high-fidelity details\nof appearance and opacity under novel poses from appearance and density feature\nmaps. For the motion control module in ARTEMIS, we combine state-of-the-art\nanimal motion capture approach with recent neural character control scheme. We\nintroduce an effective optimization scheme to reconstruct the skeletal motion\nof real animals captured by a multi-view RGB and Vicon camera array. We feed\nall the captured motion into a neural character control scheme to generate\nabstract control signals with motion styles. We further integrate ARTEMIS into\nexisting engines that support VR headsets, providing an unprecedented immersive\nexperience where a user can intimately interact with a variety of virtual\nanimals with vivid movements and photo-realistic appearance. We make available\nour ARTEMIS model and dynamic furry animal dataset at\nhttps://haiminluo.github.io/publication/artemis/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haimin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Teng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenglin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1\">Qiwei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual attention analysis of pathologists examining whole slide images of Prostate cancer. (arXiv:2202.08437v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.08437","description":"<p>We study the attention of pathologists as they examine whole-slide images\n(WSIs) of prostate cancer tissue using a digital microscope. To the best of our\nknowledge, our study is the first to report in detail how pathologists navigate\nWSIs of prostate cancer as they accumulate information for their diagnoses. We\ncollected slide navigation data (i.e., viewport location, magnification level,\nand time) from 13 pathologists in 2 groups (5 genitourinary (GU) specialists\nand 8 general pathologists) and generated visual attention heatmaps and\nscanpaths. Each pathologist examined five WSIs from the TCGA PRAD dataset,\nwhich were selected by a GU pathology specialist. We examined and analyzed the\ndistributions of visual attention for each group of pathologists after each WSI\nwas examined. To quantify the relationship between a pathologist's attention\nand evidence for cancer in the WSI, we obtained tumor annotations from a\ngenitourinary specialist. We used these annotations to compute the overlap\nbetween the distribution of visual attention and annotated tumor region to\nidentify strong correlations. Motivated by this analysis, we trained a deep\nlearning model to predict visual attention on unseen WSIs. We find that the\nattention heatmaps predicted by our model correlate quite well with the ground\ntruth attention heatmap and tumor annotations on a test set of 17 WSIs by using\nvarious spatial and temporal evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chakraborty_S/0/1/0/all/0/1\">Souradeep Chakraborty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Ke Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_R/0/1/0/all/0/1\">Rajarsi Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knudsen_B/0/1/0/all/0/1\">Beatrice Knudsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zelinsky_G/0/1/0/all/0/1\">Gregory J. Zelinsky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saltz_J/0/1/0/all/0/1\">Joel H. Saltz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active learning with binary models for real time data labelling. (arXiv:2203.00439v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00439","description":"<p>Machine learning (ML) and Deep Learning (DL) tasks primarily depend on data.\nMost of the ML and DL applications involve supervised learning which requires\nlabelled data. In the initial phases of ML realm lack of data used to be a\nproblem, now we are in a new era of big data. The supervised ML algorithms\nrequire data to be labelled and of good quality. Labelling task requires a\nlarge amount of money and time investment. Data labelling require a skilled\nperson who will charge high for this task, consider the case of the medical\nfield or the data is in bulk that requires a lot of people assigned to label\nit. The amount of data that is well enough for training needs to be known,\nmoney and time can not be wasted to label the whole data. This paper mainly\naims to propose a strategy that helps in labelling the data along with oracle\nin real-time. With balancing on model contribution for labelling is 89 and 81.1\nfor furniture type and intel scene image data sets respectively. Further with\nbalancing being kept off model contribution is found to be 83.47 and 78.71 for\nfurniture type and flower data sets respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1\">Ankush Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+C_B/0/1/0/all/0/1\">Bhargava B C</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhadhan_A/0/1/0/all/0/1\">A V Narasimhadhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01322","description":"<p>Understanding visual question answering is going to be crucial for numerous\nhuman activities. However, it presents major challenges at the heart of the\nartificial intelligence endeavor. This paper presents an update on the rapid\nadvancements in visual question answering using images that have occurred in\nthe last couple of years. Tremendous growth in research on improving visual\nquestion answering system architecture has been published recently, showing the\nimportance of multimodal architectures. Several points on the benefits of\nvisual question answering are mentioned in the review paper by Manmadhan et al.\n(2020), on which the present article builds, including subsequent updates in\nthe field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kodali_V/0/1/0/all/0/1\">Venkat Kodali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACID: Action-Conditional Implicit Visual Dynamics for Deformable Object Manipulation. (arXiv:2203.06856v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06856","description":"<p>Manipulating volumetric deformable objects in the real world, like plush toys\nand pizza dough, bring substantial challenges due to infinite shape variations,\nnon-rigid motions, and partial observability. We introduce ACID, an\naction-conditional visual dynamics model for volumetric deformable objects\nbased on structured implicit neural representations. ACID integrates two new\ntechniques: implicit representations for action-conditional dynamics and\ngeodesics-based contrastive learning. To represent deformable dynamics from\npartial RGB-D observations, we learn implicit representations of occupancy and\nflow-based forward dynamics. To accurately identify state change under large\nnon-rigid deformations, we learn a correspondence embedding field through a\nnovel geodesics-based contrastive loss. To evaluate our approach, we develop a\nsimulation framework for manipulating complex deformable shapes in realistic\nscenes and a benchmark containing over 17,000 action trajectories with six\ntypes of plush toys and 78 variants. Our model achieves the best performance in\ngeometry, correspondence, and dynamics predictions over existing approaches.\nThe ACID dynamics models are successfully employed to goal-conditioned\ndeformable manipulation tasks, resulting in a 30% increase in task success rate\nover the strongest baseline. For more results and information, please visit\nhttps://b0ku1.github.io/acid/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhenyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choy_C/0/1/0/all/0/1\">Christopher Choy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-set Recognition via Augmentation-based Similarity Learning. (arXiv:2203.13238v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13238","description":"<p>The primary assumption of conventional supervised learning or classification\nis that the test samples are drawn from the same distribution as the training\nsamples, which is called closed set learning or classification. In many\npractical scenarios, this is not the case because there are unknowns or unseen\nclass samples in the test data, which is called the open set scenario, and the\nunknowns need to be detected. This problem is referred to as the open set\nrecognition problem and is important in safety-critical applications. We\npropose to detect unknowns (or unseen class samples) through learning pairwise\nsimilarities. The proposed method works in two steps. It first learns a closed\nset classifier using the seen classes that have appeared in training and then\nlearns how to compare seen classes with pseudo-unseen (automatically generated\nunseen class samples). The pseudo-unseen generation is carried out by\nperforming distribution shifting augmentations on the seen or training samples.\nWe call our method OPG (Open set recognition based on Pseudo unseen data\nGeneration). The experimental evaluation shows that the learned\nsimilarity-based features can successfully distinguish seen from unseen in\nbenchmark datasets for open set recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilpour_S/0/1/0/all/0/1\">Sepideh Esmaeilpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOS! Self-supervised Learning Over Sets Of Handled Objects In Egocentric Action Recognition. (arXiv:2204.04796v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04796","description":"<p>Learning an egocentric action recognition model from video data is\nchallenging due to distractors (e.g., irrelevant objects) in the background.\nFurther integrating object information into an action model is hence\nbeneficial. Existing methods often leverage a generic object detector to\nidentify and represent the objects in the scene. However, several important\nissues remain. Object class annotations of good quality for the target domain\n(dataset) are still required for learning good object representation. Besides,\nprevious methods deeply couple the existing action models and need to retrain\nthem jointly with object representation, leading to costly and inflexible\nintegration. To overcome both limitations, we introduce Self-Supervised\nLearning Over Sets (SOS), an approach to pre-train a generic Objects In Contact\n(OIC) representation model from video object regions detected by an\noff-the-shelf hand-object contact detector. Instead of augmenting object\nregions individually as in conventional self-supervised learning, we view the\naction process as a means of natural data transformations with unique\nspatio-temporal continuity and exploit the inherent relationships among\nper-video object sets. Extensive experiments on two datasets, EPIC-KITCHENS-100\nand EGTEA, show that our OIC significantly boosts the performance of multiple\nstate-of-the-art video classification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Escorcia_V/0/1/0/all/0/1\">Victor Escorcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_R/0/1/0/all/0/1\">Ricardo Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1\">Brais Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. (arXiv:2204.05991v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05991","description":"<p>Training a referring expression comprehension (ReC) model for a new visual\ndomain requires collecting referring expressions, and potentially corresponding\nbounding boxes, for images in the domain. While large-scale pre-trained models\nare useful for image classification across domains, it remains unclear if they\ncan be applied in a zero-shot manner to more complex tasks like ReC. We present\nReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a\nstate-of-the-art large-scale model, for ReC. Motivated by the close connection\nbetween ReC and CLIP's contrastive pre-training objective, the first component\nof ReCLIP is a region-scoring method that isolates object proposals via\ncropping and blurring, and passes them to CLIP. However, through controlled\nexperiments on a synthetic dataset, we find that CLIP is largely incapable of\nperforming spatial reasoning off-the-shelf. Thus, the second component of\nReCLIP is a spatial relation resolver that handles several types of spatial\nrelations. We reduce the gap between zero-shot baselines from prior work and\nsupervised models by as much as 29% on RefCOCOg, and on RefGTA (video game\nimagery), ReCLIP's relative improvement over supervised ReC models trained on\nreal images is 8%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Sanjay Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06718","description":"<p>Convolutional neural network (CNN) has achieved impressive success in\ncomputer vision during the past few decades. The image convolution operation\nhelps CNNs to get good performance on image-related tasks. However, the image\nconvolution has high computation complexity and hard to be implemented. This\npaper proposes the CEMNet, which can be trained in the frequency domain. The\nmost important motivation of this research is that we can use the\nstraightforward element-wise multiplication operation to replace the image\nconvolution in the frequency domain based on the Cross-Correlation Theorem,\nwhich obviously reduces the computation complexity. We further introduce a\nWeight Fixation mechanism to alleviate the problem of over-fitting, and analyze\nthe working behavior of Batch Normalization, Leaky ReLU, and Dropout in the\nfrequency domain to design their counterparts for CEMNet. Also, to deal with\ncomplex inputs brought by Discrete Fourier Transform, we design a two-branches\nnetwork structure for CEMNet. Experimental results imply that CEMNet achieves\ngood performance on MNIST and CIFAR-10 databases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hengyue Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proto2Proto: Can you recognize the car, the way I do?. (arXiv:2204.11830v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11830","description":"<p>Prototypical methods have recently gained a lot of attention due to their\nintrinsic interpretable nature, which is obtained through the prototypes. With\ngrowing use cases of model reuse and distillation, there is a need to also\nstudy transfer of interpretability from one model to another. We present\nProto2Proto, a novel method to transfer interpretability of one prototypical\npart network to another via knowledge distillation. Our approach aims to add\ninterpretability to the \"dark\" knowledge transferred from the teacher to the\nshallower student model. We propose two novel losses: \"Global Explanation\" loss\nand \"Patch-Prototype Correspondence\" loss to facilitate such a transfer. Global\nExplanation loss forces the student prototypes to be close to teacher\nprototypes, and Patch-Prototype Correspondence loss enforces the local\nrepresentations of the student to be similar to that of the teacher. Further,\nwe propose three novel metrics to evaluate the student's proximity to the\nteacher as measures of interpretability transfer in our settings. We\nqualitatively and quantitatively demonstrate the effectiveness of our method on\nCUB-200-2011 and Stanford Cars datasets. Our experiments show that the proposed\nmethod indeed achieves interpretability transfer from teacher to student while\nsimultaneously exhibiting competitive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keswani_M/0/1/0/all/0/1\">Monish Keswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1\">Sriranjani Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_N/0/1/0/all/0/1\">Nishant Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Power Bundle Adjustment for Large-Scale 3D Reconstruction. (arXiv:2204.12834v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12834","description":"<p>We present the design and the implementation of a new expansion type\nalgorithm to solve large-scale bundle adjustment problems. Our approach --\ncalled Power Bundle Adjustment -- is based on the power series expansion of the\ninverse Schur complement. This initiates a new family of solvers that we call\ninverse expansion methods. We show with the real-world BAL dataset that the\nproposed solver challenges the traditional direct and iterative methods. The\nsolution of the normal equation is significantly accelerated, even for reaching\na very high accuracy. Last but not least, our solver can also complement a\nrecently presented distributed bundle adjustment framework. We demonstrate that\nemploying the proposed Power Bundle Adjustment as a sub-problem solver greatly\nimproves speed and accuracy of the distributed optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weber_S/0/1/0/all/0/1\">Simon Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demmel_N/0/1/0/all/0/1\">Nikolaus Demmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_T/0/1/0/all/0/1\">Tin Chon Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRIT: General Robust Image Task Benchmark. (arXiv:2204.13653v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13653","description":"<p>Computer vision models excel at making predictions when the test distribution\nclosely resembles the training distribution. Such models have yet to match the\nability of biological vision to learn from multiple sources and generalize to\nnew data sources and tasks. To facilitate the development and evaluation of\nmore general vision systems, we introduce the General Robust Image Task (GRIT)\nbenchmark. GRIT evaluates the performance, robustness, and calibration of a\nvision system across a variety of image prediction tasks, concepts, and data\nsources. The seven tasks in GRIT are selected to cover a range of visual\nskills: object categorization, object localization, referring expression\ngrounding, visual question answering, segmentation, human keypoint detection,\nand surface normal estimation. GRIT is carefully designed to enable the\nevaluation of robustness under image perturbations, image source distribution\nshift, and concept distribution shift. By providing a unified platform for\nthorough assessment of skills and concepts learned by a vision model, we hope\nGRIT catalyzes the development of performant and robust general purpose vision\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanmay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marten_R/0/1/0/all/0/1\">Ryan Marten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1\">Derek Hoiem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A very preliminary analysis of DALL-E 2. (arXiv:2204.13807v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13807","description":"<p>The DALL-E 2 system generates original synthetic images corresponding to an\ninput text as caption. We report here on the outcome of fourteen tests of this\nsystem designed to assess its common sense, reasoning and ability to understand\ncomplex texts. All of our prompts were intentionally much more challenging than\nthe typical ones that have been showcased in recent weeks. Nevertheless, for 5\nout of the 14 prompts, at least one of the ten images fully satisfied our\nrequests. On the other hand, on no prompt did all of the ten images satisfy our\nrequests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marcus_G/0/1/0/all/0/1\">Gary Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1\">Ernest Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aaronson_S/0/1/0/all/0/1\">Scott Aaronson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source Domain Subset Sampling for Semi-Supervised Domain Adaptation in Semantic Segmentation. (arXiv:2205.00312v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00312","description":"<p>In this paper, we introduce source domain subset sampling (SDSS) as a new\nperspective of semi-supervised domain adaptation. We propose domain adaptation\nby sampling and exploiting only a meaningful subset from source data for\ntraining. Our key assumption is that the entire source domain data may contain\nsamples that are unhelpful for the adaptation. Therefore, the domain adaptation\ncan benefit from a subset of source data composed solely of helpful and\nrelevant samples. The proposed method effectively subsamples full source data\nto generate a small-scale meaningful subset. Therefore, training time is\nreduced, and performance is improved with our subsampled source data. To\nfurther verify the scalability of our method, we construct a new dataset called\nOcean Ship, which comprises 500 real and 200K synthetic sample images with\nground-truth labels. The SDSS achieved a state-of-the-art performance when\napplied on GTA5 to Cityscapes and SYNTHIA to Cityscapes public benchmark\ndatasets and a 9.13 mIoU improvement on our Ocean Ship dataset over a baseline\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daehan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinsun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Geol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog. (arXiv:2205.00423v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00423","description":"<p>Visual Dialog aims to answer multi-round, interactive questions based on the\ndialog history and image content. Existing methods either consider answer\nranking and generating individually or only weakly capture the relation across\nthe two tasks implicitly by two separate models. The research on a universal\nframework that jointly learns to rank and generate answers in a single model is\nseldom explored. In this paper, we propose a contrastive learning-based\nframework UTC to unify and facilitate both discriminative and generative tasks\nin visual dialog with a single model. Specifically, considering the inherent\nlimitation of the previous learning paradigm, we devise two inter-task\ncontrastive losses i.e., context contrastive loss and answer contrastive loss\nto make the discriminative and generative tasks mutually reinforce each other.\nThese two complementary contrastive losses exploit dialog context and target\nanswer as anchor points to provide representation learning signals from\ndifferent perspectives. We evaluate our proposed UTC on the VisDial v1.0\ndataset, where our method outperforms the state-of-the-art on both\ndiscriminative and generative tasks and surpasses previous state-of-the-art\ngenerative methods by more than 2 absolute points on Recall@1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yudong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhenshan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qingrong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaodong Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational Lensing Data. (arXiv:2205.00701v2 [astro-ph.IM] UPDATED)","link":"http://arxiv.org/abs/2205.00701","description":"<p>Gravitational lensing is the relativistic effect generated by massive bodies,\nwhich bend the space-time surrounding them. It is a deeply investigated topic\nin astrophysics and allows validating theoretical relativistic results and\nstudying faint astrophysical objects that would not be visible otherwise. In\nrecent years Machine Learning methods have been applied to support the analysis\nof the gravitational lensing phenomena by detecting lensing effects in data\nsets consisting of images associated with brightness variation time series.\nHowever, the state-of-art approaches either consider only images and neglect\ntime-series data or achieve relatively low accuracy on the most difficult data\nsets. This paper introduces DeepGraviLens, a novel multi-modal network that\nclassifies spatio-temporal data belonging to one non-lensed system type and\nthree lensed system types. It surpasses the current state of the art accuracy\nresults by $\\approx$ 19% to $\\approx$ 43%, depending on the considered data\nset. Such an improvement will enable the acceleration of the analysis of lensed\nobjects in upcoming astrophysical surveys, which will exploit the petabytes of\ndata collected, e.g., from the Vera C. Rubin Observatory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Vago_N/0/1/0/all/0/1\">Nicol&#xf2; Oreste Pinciroli Vago</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Fraternali_P/0/1/0/all/0/1\">Piero Fraternali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCLF: A Contrastive-Curiosity-Driven Learning Framework for Sample-Efficient Reinforcement Learning. (arXiv:2205.00943v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.00943","description":"<p>In reinforcement learning (RL), it is challenging to learn directly from\nhigh-dimensional observations, where data augmentation has recently been shown\nto remedy this via encoding invariances from raw pixels. Nevertheless, we\nempirically find that not all samples are equally important and hence simply\ninjecting more augmented inputs may instead cause instability in Q-learning. In\nthis paper, we approach this problem systematically by developing a\nmodel-agnostic Contrastive-Curiosity-Driven Learning Framework (CCLF), which\ncan fully exploit sample importance and improve learning efficiency in a\nself-supervised manner. Facilitated by the proposed contrastive curiosity, CCLF\nis capable of prioritizing the experience replay, selecting the most\ninformative augmented inputs, and more importantly regularizing the Q-function\nas well as the encoder to concentrate more on under-learned data. Moreover, it\nencourages the agent to explore with a curiosity-based reward. As a result, the\nagent can focus on more informative samples and learn representation\ninvariances more efficiently, with significantly reduced augmented inputs. We\napply CCLF to several base RL algorithms and evaluate on the DeepMind Control\nSuite, Atari, and MiniGrid benchmarks, where our approach demonstrates superior\nsample efficiency and learning performances compared with other\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hangwei Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-stage deep architecture for summary generation of soccer videos. (arXiv:2205.00694v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2205.00694","description":"<p>Video content is present in an ever-increasing number of fields, both\nscientific and commercial. Sports, particularly soccer, is one of the\nindustries that has invested the most in the field of video analytics, due to\nthe massive popularity of the game and the emergence of new markets. Previous\nstate-of-the-art methods on soccer matches video summarization rely on\nhandcrafted heuristics to generate summaries which are poorly generalizable,\nbut these works have yet proven that multiple modalities help detect the best\nactions of the game. On the other hand, machine learning models with higher\ngeneralization potential have entered the field of summarization of\ngeneral-purpose videos, offering several deep learning approaches. However,\nmost of them exploit content specificities that are not appropriate for sport\nwhole-match videos. Although video content has been for many years the main\nsource for automatizing knowledge extraction in soccer, the data that records\nall the events happening on the field has become lately very important in\nsports analytics, since this event data provides richer context information and\nrequires less processing. We propose a method to generate the summary of a\nsoccer match exploiting both the audio and the event metadata. The results show\nthat our method can detect the actions of the match, identify which of these\nactions should belong to the summary and then propose multiple candidate\nsummaries which are similar enough but with relevant variability to provide\ndifferent options to the final editor. Furthermore, we show the generalization\ncapability of our work since it can transfer knowledge between datasets from\ndifferent broadcasting companies, different competitions, acquired in different\nconditions, and corresponding to summaries of different lengths\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanabria_M/0/1/0/all/0/1\">Melissa Sanabria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precioso_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Precioso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattei_P/0/1/0/all/0/1\">Pierre-Alexandre Mattei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menguy_T/0/1/0/all/0/1\">Thomas Menguy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}