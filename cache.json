{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-09T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"HyperMixer: An MLP-based Green AI Alternative to Transformers. (arXiv:2203.03691v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03691","description":"<p>Transformer-based architectures are the model of choice for natural language\nunderstanding, but they come at a significant cost, as they have quadratic\ncomplexity in the input length and can be difficult to tune. In the pursuit of\nGreen AI, we investigate simple MLP-based architectures. We find that existing\narchitectures such as MLPMixer, which achieves token mixing through a static\nMLP applied to each feature independently, are too detached from the inductive\nbiases required for natural language understanding. In this paper, we propose a\nsimple variant, HyperMixer, which forms the token mixing MLP dynamically using\nhypernetworks. Empirically, we demonstrate that our model performs better than\nalternative MLP-based models, and on par with Transformers. In contrast to\nTransformers, HyperMixer achieves these results at substantially lower costs in\nterms of processing time, training data, and hyperparameter tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_F/0/1/0/all/0/1\">Florian Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pannatier_A/0/1/0/all/0/1\">Arnaud Pannatier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fehr_F/0/1/0/all/0/1\">Fabio Fehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marelli_F/0/1/0/all/0/1\">Francois Marelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1\">Francois Fleuret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation. (arXiv:2203.03759v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03759","description":"<p>The T5 model and its unified text-to-text paradigm contributed in advancing\nthe state-of-the-art for many natural language processing tasks. While some\nmultilingual variants of the T5 model have recently been introduced, their\nperformances were found to provide suboptimal performances for languages other\nthan English if compared to monolingual variants. We are motivated by these\nfindings to introduce IT5, the first family of encoder-decoder transformer\nmodels pretrained specifically on Italian. We perform a thorough cleaning of a\nweb-crawled Italian corpus including more than 40 billion words and use it to\npretrain three IT5 models of different sizes. The performance of IT5 models and\ntheir multilingual counterparts is then evaluated on a broad range of natural\nlanguage understanding and generation benchmarks for Italian. We find the\nmonolingual IT5 models to provide the best scale-to-performance ratio across\ntested models, consistently outperforming their multilingual counterparts and\nsetting a new state-of-the-art for most Italian conditional language generation\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1\">Gabriele Sarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Preserving Linguistic Steganography by Pivot Translation and Semantic-Aware Bins Coding. (arXiv:2203.03795v1 [cs.CR])","link":"http://arxiv.org/abs/2203.03795","description":"<p>Linguistic steganography (LS) aims to embed secret information into a highly\nencoded text for covert communication. It can be roughly divided to two main\ncategories, i.e., modification based LS (MLS) and generation based LS (GLS).\nUnlike MLS that hides secret data by slightly modifying a given text without\nimpairing the meaning of the text, GLS uses a trained language model to\ndirectly generate a text carrying secret data. A common disadvantage for MLS\nmethods is that the embedding payload is very low, whose return is well\npreserving the semantic quality of the text. In contrast, GLS allows the data\nhider to embed a high payload, which has to pay the high price of\nuncontrollable semantics. In this paper, we propose a novel LS method to modify\na given text by pivoting it between two different languages and embed secret\ndata by applying a GLS-like information encoding strategy. Our purpose is to\nalter the expression of the given text, enabling a high payload to be embedded\nwhile keeping the semantic information unchanged. Experimental results have\nshown that the proposed work not only achieves a high embedding payload, but\nalso shows superior performance in maintaining the semantic consistency and\nresisting linguistic steganalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1\">Biao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1\">Guorui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Iterative Revision from Human-Written Text. (arXiv:2203.03802v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03802","description":"<p>Writing is, by nature, a strategic, adaptive, and more importantly, an\niterative process. A crucial part of writing is editing and revising the text.\nPrevious works on text revision have focused on defining edit intention\ntaxonomies within a single domain or developing computational models with a\nsingle level of edit granularity, such as sentence-level edits, which differ\nfrom human's revision cycles. This work describes IteraTeR: the first\nlarge-scale, multi-domain, edit-intention annotated corpus of iteratively\nrevised text. In particular, IteraTeR is collected based on a new framework to\ncomprehensively model the iterative text revisions that generalize to various\ndomains of formal writing, edit intentions, revision depths, and granularities.\nWhen we incorporate our annotated edit intentions, both generative and\nedit-based text revision models significantly improve automatic evaluations.\nThrough our work, we better understand the text revision process, making vital\nconnections between edit intentions and writing quality, enabling the creation\nof diverse corpora to support computational modeling of iterative text\nrevisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raheja_V/0/1/0/all/0/1\">Vipul Raheja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Dhruv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Z/0/1/0/all/0/1\">Zae Myung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_M/0/1/0/all/0/1\">Melissa Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Search with Text Feedback by Additive Attention Compositional Learning. (arXiv:2203.03809v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03809","description":"<p>Effective image retrieval with text feedback stands to impact a range of\nreal-world applications, such as e-commerce. Given a source image and text\nfeedback that describes the desired modifications to that image, the goal is to\nretrieve the target images that resemble the source yet satisfy the given\nmodifications by composing a multi-modal (image-text) query. We propose a novel\nsolution to this problem, Additive Attention Compositional Learning (AACL),\nthat uses a multi-modal transformer-based architecture and effectively models\nthe image-text contexts. Specifically, we propose a novel image-text\ncomposition module based on additive attention that can be seamlessly plugged\ninto deep neural networks. We also introduce a new challenging benchmark\nderived from the Shopping100k dataset. AACL is evaluated on three large-scale\ndatasets (FashionIQ, Fashion200k, and Shopping100k), each with strong\nbaselines. Extensive experiments show that AACL achieves new state-of-the-art\nresults on all three datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuxin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newsam_S/0/1/0/all/0/1\">Shawn Newsam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boakye_K/0/1/0/all/0/1\">Kofi Boakye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Variational Hierarchical Model for Neural Cross-Lingual Summarization. (arXiv:2203.03820v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03820","description":"<p>The goal of the cross-lingual summarization (CLS) is to convert a document in\none language (e.g., English) to a summary in another one (e.g., Chinese).\nEssentially, the CLS task is the combination of machine translation (MT) and\nmonolingual summarization (MS), and thus there exists the hierarchical\nrelationship between MT\\&amp;MS and CLS. Existing studies on CLS mainly focus on\nutilizing pipeline methods or jointly training an end-to-end model through an\nauxiliary MT or MS objective. However, it is very challenging for the model to\ndirectly conduct CLS as it requires both the abilities to translate and\nsummarize. To address this issue, we propose a hierarchical model for the CLS\ntask, based on the conditional variational auto-encoder. The hierarchical model\ncontains two kinds of latent variables at the local and global levels,\nrespectively. At the local level, there are two latent variables, one for\ntranslation and the other for summarization. As for the global level, there is\nanother latent variable for cross-lingual summarization conditioned on the two\nlocal-level variables. Experiments on two language directions (English-Chinese)\nverify the effectiveness and superiority of the proposed approach. In addition,\nwe show that our model is able to generate better cross-lingual summaries than\ncomparison models in the few-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chulun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Framework of Medical Information Annotation and Extraction for Chinese Clinical Text. (arXiv:2203.03823v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03823","description":"<p>Medical information extraction consists of a group of natural language\nprocessing (NLP) tasks, which collaboratively convert clinical text to\npre-defined structured formats. Current state-of-the-art (SOTA) NLP models are\nhighly integrated with deep learning techniques and thus require massive\nannotated linguistic data. This study presents an engineering framework of\nmedical entity recognition, relation extraction and attribute extraction, which\nare unified in annotation, modeling and evaluation. Specifically, the\nannotation scheme is comprehensive, and compatible between tasks, especially\nfor the medical relations. The resulted annotated corpus includes 1,200 full\nmedical records (or 18,039 broken-down documents), and achieves inter-annotator\nagreements (IAAs) of 94.53%, 73.73% and 91.98% F 1 scores for the three tasks.\nThree task-specific neural network models are developed within a shared\nstructure, and enhanced by SOTA NLP techniques, i.e., pre-trained language\nmodels. Experimental results show that the system can retrieve medical\nentities, relations and attributes with F 1 scores of 93.47%, 67.14% and\n90.89%, respectively. This study, in addition to our publicly released\nannotation scheme and code, provides solid and practical engineering experience\nof developing an integrated medical information extraction system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">Enwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qilin Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huanwan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Hierarchy into Text Encoder: a Contrastive Learning Approach for Hierarchical Text Classification. (arXiv:2203.03825v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03825","description":"<p>Hierarchical text classification is a challenging subtask of multi-label\nclassification due to its complex label hierarchy. Existing methods encode text\nand label hierarchy separately and mix their representations for\nclassification, where the hierarchy remains unchanged for all input text.\nInstead of modeling them separately, in this work, we propose Hierarchy-guided\nContrastive Learning (HGCLR) to directly embed the hierarchy into a text\nencoder. During training, HGCLR constructs positive samples for input text\nunder the guidance of the label hierarchy. By pulling together the input text\nand its positive sample, the text encoder can learn to generate the\nhierarchy-aware text representation independently. Therefore, after training,\nthe HGCLR enhanced text encoder can dispense with the redundant hierarchy.\nExtensive experiments on three benchmark datasets verify the effectiveness of\nHGCLR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lianzhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Building an Open-Domain Dialogue System Incorporated with Internet Memes. (arXiv:2203.03835v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03835","description":"<p>In recent years, Internet memes have been widely used in online chatting.\nCompared with text-based communication, conversations become more expressive\nand attractive when Internet memes are incorporated. This paper presents our\nsolutions for the Meme incorporated Open-domain Dialogue (MOD) Challenge of\nDSTC10, where three tasks are involved: text response modeling, meme retrieval,\nand meme emotion classification. Firstly, we leverage a large-scale pre-trained\ndialogue model for coherent and informative response generation. Secondly,\nbased on interaction-based text-matching, our approach can retrieve appropriate\nmemes with good generalization ability. Thirdly, we propose to model the\nemotion flow (EF) in conversations and introduce an auxiliary task of emotion\ndescription prediction (EDP) to boost the performance of meme emotion\nclassification. Experimental results on the MOD dataset demonstrate that our\nmethods can incorporate Internet memes into dialogue systems effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chanjuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Siqi Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation. (arXiv:2203.03850v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03850","description":"<p>Pre-trained models for programming languages have recently demonstrated great\nsuccess on code intelligence. To support both code-related understanding and\ngeneration tasks, recent works attempt to pre-train unified encoder-decoder\nmodels. However, such encoder-decoder framework is sub-optimal for\nauto-regressive tasks, especially code completion that requires a decoder-only\nmanner for efficient inference. In this paper, we present UniXcoder, a unified\ncross-modal pre-trained model for programming language. The model utilizes mask\nattention matrices with prefix adapters to control the behavior of the model\nand leverages cross-modal contents like AST and code comment to enhance code\nrepresentation. To encode AST that is represented as a tree in parallel, we\npropose a one-to-one mapping method to transform AST in a sequence structure\nthat retains all structural information from the tree. Furthermore, we propose\nto utilize multi-modal contents to learn representation of code fragment with\ncontrastive learning, and then align representations among programming\nlanguages using a cross-modal generation task. We evaluate UniXcoder on five\ncode-related tasks over nine datasets. To further evaluate the performance of\ncode fragment representation, we also construct a dataset for a new task,\ncalled zero-shot code-to-code search. Results show that our model achieves\nstate-of-the-art performance on most tasks and analysis reveals that comment\nand AST can both enhance UniXcoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where Does the Performance Improvement Come From? - A Reproducibility Concern about Image-Text Retrieval. (arXiv:2203.03853v1 [cs.IR])","link":"http://arxiv.org/abs/2203.03853","description":"<p>This paper seeks to provide the information retrieval community with some\nreflections on the current improvements of retrieval learning through the\nanalysis of the reproducibility aspects of image-text retrieval models. For the\nlatter part of the past decade, image-text retrieval has gradually become a\nmajor research direction in the field of information retrieval because of the\ngrowth of multi-modal data. Many researchers use benchmark datasets like\nMS-COCO and Flickr30k to train and assess the performance of image-text\nretrieval algorithms. Research in the past has mostly focused on performance,\nwith several state-of-the-art methods being proposed in various ways. According\nto their claims, these approaches achieve better modal interactions and thus\nbetter multimodal representations with greater precision. In contrast to those\nprevious works, we focus on the repeatability of the approaches and the overall\nexamination of the elements that lead to improved performance by pretrained and\nnonpretrained models in retrieving images and text. To be more specific, we\nfirst examine the related reproducibility concerns and why the focus is on\nimage-text retrieval tasks, and then we systematically summarize the current\nparadigm of image-text retrieval models and the stated contributions of those\napproaches. Second, we analyze various aspects of the reproduction of\npretrained and nonpretrained retrieval models. Based on this, we conducted\nablation experiments and obtained some influencing factors that affect\nretrieval recall more than the improvement claimed in the original paper.\nFinally, we also present some reflections and issues that should be considered\nby the retrieval community in the future. Our code is freely available at\nhttps://github.com/WangFei-2019/Image-text-Retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Shuhan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DARER: Dual-task Temporal Relational Recurrent Reasoning Network for Joint Dialog Sentiment Classification and Act Recognition. (arXiv:2203.03856v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03856","description":"<p>The task of joint dialog sentiment classification (DSC) and act recognition\n(DAR) aims to simultaneously predict the sentiment label and act label for each\nutterance in a dialog. In this paper, we put forward a new framework which\nmodels the explicit dependencies via integrating \\textit{prediction-level\ninteractions} other than semantics-level interactions, more consistent with\nhuman intuition. Besides, we propose a speaker-aware temporal graph (SATG) and\na dual-task relational temporal graph (DRTG) to introduce \\textit{temporal\nrelations} into dialog understanding and dual-task reasoning. To implement our\nframework, we propose a novel model dubbed DARER, which first generates the\ncontext-, speaker- and temporal-sensitive utterance representations via\nmodeling SATG, then conducts recurrent dual-task relational reasoning on DRTG,\nin which process the estimated label distributions act as key clues in\nprediction-level interactions. Experiment results show that DARER outperforms\nexisting models by large margins while requiring much less computation resource\nand costing less training time. Remarkably, on DSC task in Mastodon, DARER\ngains a relative improvement of about 25% over previous best model in terms of\nF1, with less than 50% parameters and about only 60% required GPU memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1\">Bowen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor W. Tsang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks. (arXiv:2203.03878v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03878","description":"<p>The workflow of pretraining and fine-tuning has emerged as a popular paradigm\nfor solving various NLP and V&amp;L (Vision-and-Language) downstream tasks. With\nthe capacity of pretrained models growing rapidly, how to perform\nparameter-efficient fine-tuning has become fairly important for quick transfer\nlearning and deployment. In this paper, we design a novel unified\nparameter-efficient transfer learning framework that works effectively on both\npure language and V&amp;L tasks. In particular, we use a shared hypernetwork that\ntakes trainable hyper-embeddings as input, and outputs weights for fine-tuning\ndifferent small modules in a pretrained language model, such as tuning the\nparameters inserted into multi-head attention blocks (i.e., prefix-tuning) and\nfeed-forward blocks (i.e., adapter-tuning). We define a set of embeddings\n(e.g., layer, block, task and visual embeddings) as the key components to\ncalculate hyper-embeddings, which thus can support both pure language and V&amp;L\ntasks. Our proposed framework adds fewer trainable parameters in multi-task\nlearning while achieving superior performances and transfer ability compared to\nstate-of-the-art methods. Empirical results on the GLUE benchmark and multiple\nV&amp;L tasks confirm the effectiveness of our framework on both textual and visual\nmodalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengkun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaojun Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yadao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenglu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Mixup for Robust Fine-tuning. (arXiv:2203.03897v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03897","description":"<p>Pre-trained large-scale models provide a transferable embedding, and they\nshow comparable performance on the diverse downstream task. However, the\ntransferability of multi-modal learning is restricted, and the analysis of\nlearned embedding has not been explored well. This paper provides a perspective\nto understand the multi-modal embedding in terms of uniformity and alignment.\nWe newly find that the representation learned by multi-modal learning models\nsuch as CLIP has a two separated representation space for each heterogeneous\ndataset with less alignment. Besides, there are unexplored large intermediate\nareas between two modalities with less uniformity. Less robust embedding might\nrestrict the transferability of the representation for the downstream task.\nThis paper provides a new end-to-end fine-tuning method for robust\nrepresentation that encourages better uniformity and alignment score. First, we\npropose a multi-modal Mixup, $m^{2}$-Mix that mixes the representation of image\nand text to generate the hard negative samples. Second, we fine-tune the\nmulti-modal model on a hard negative sample as well as normal negative and\npositive samples with contrastive learning. Our multi-modal Mixup provides a\nrobust representation, and we validate our methods on classification,\nretrieval, and structure-awareness task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1\">Junhyuk So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1\">Changdae Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Minchul Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kyungwoo Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructionNER: A Multi-Task Instruction-Based Generative Framework for Few-shot NER. (arXiv:2203.03903v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03903","description":"<p>Recently, prompt-based methods have achieved significant performance in\nfew-shot learning scenarios by bridging the gap between language model\npre-training and fine-tuning for downstream tasks. However, existing prompt\ntemplates are mostly designed for sentence-level tasks and are inappropriate\nfor sequence labeling objectives. To address the above issue, we propose a\nmulti-task instruction-based generative framework, named InstructionNER, for\nlow-resource named entity recognition. Specifically, we reformulate the NER\ntask as a generation problem, which enriches source sentences with\ntask-specific instructions and answer options, then inferences the entities and\ntypes in natural language. We further propose two auxiliary tasks, including\nentity extraction and entity typing, which enable the model to capture more\nboundary information of entities and deepen the understanding of entity type\nsemantics, respectively. Experimental results show that our method consistently\noutperforms other baselines on five datasets in few-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rumei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuanmeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation. (arXiv:2203.03910v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03910","description":"<p>Neural networks tend to gradually forget the previously learned knowledge\nwhen learning multiple tasks sequentially from dynamic data distributions. This\nproblem is called \\textit{catastrophic forgetting}, which is a fundamental\nchallenge in the continual learning of neural networks. In this work, we\nobserve that catastrophic forgetting not only occurs in continual learning but\nalso affects the traditional static training. Neural networks, especially\nneural machine translation models, suffer from catastrophic forgetting even if\nthey learn from a static training set. To be specific, the final model pays\nimbalanced attention to training samples, where recently exposed samples\nattract more attention than earlier samples. The underlying cause is that\ntraining samples do not get balanced training in each model update, so we name\nthis problem \\textit{imbalanced training}. To alleviate this problem, we\npropose Complementary Online Knowledge Distillation (COKD), which uses\ndynamically updated teacher models trained on specific data orders to\niteratively provide complementary knowledge to the student model. Experimental\nresults on multiple machine translation tasks show that our method successfully\nalleviates the problem of imbalanced training and achieves substantial\nimprovements over strong baseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1\">Chenze Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapt$\\mathcal{O}$r: Objective-Centric Adaptation Framework for Language Models. (arXiv:2203.03989v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03989","description":"<p>Progress in natural language processing research is catalyzed by the\npossibilities given by the widespread software frameworks. This paper\nintroduces Adaptor library that transposes the traditional model-centric\napproach composed of pre-training + fine-tuning steps to objective-centric\napproach, composing the training process by applications of selected\nobjectives. We survey research directions that can benefit from enhanced\nobjective-centric experimentation in multitask training, custom objectives\ndevelopment, dynamic training curricula, or domain adaptation. Adaptor aims to\nease reproducibility of these research directions in practice. Finally, we\ndemonstrate the practical applicability of Adaptor in selected unsupervised\ndomain adaptation scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groverova_N/0/1/0/all/0/1\">Nikola Groverov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration. (arXiv:2203.04006v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04006","description":"<p>Vision-language navigation (VLN) is a challenging task due to its large\nsearching space in the environment. To address this problem, previous works\nhave proposed some methods of fine-tuning a large model that pretrained on\nlarge-scale datasets. However, the conventional fine-tuning methods require\nextra human-labeled navigation data and lack self-exploration capabilities in\nenvironments, which hinders their generalization of unseen scenes. To improve\nthe ability of fast cross-domain adaptation, we propose Prompt-based\nEnvironmental Self-exploration (ProbES), which can self-explore the\nenvironments by sampling trajectories and automatically generates structured\ninstructions via a large-scale cross-modal pretrained model (CLIP). Our method\nfully utilizes the knowledge learned from CLIP to build an in-domain dataset by\nself-exploration without human labeling. Unlike the conventional approach of\nfine-tuning, we introduce prompt-based learning to achieve fast adaptation for\nlanguage embeddings, which substantially improves the learning efficiency by\nleveraging prior knowledge. By automatically synthesizing\ntrajectory-instruction pairs in any environment without human supervision and\nefficient prompt-based learning, our model can adapt to diverse vision-language\nnavigation tasks, including VLN and REVERIE. Both qualitative and quantitative\nresults show that our ProbES significantly improves the generalization ability\nof the navigation model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengda Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lingling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Generalized Models for Task-oriented Dialogue Modeling on Spoken Conversations. (arXiv:2203.04045v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04045","description":"<p>Building robust and general dialogue models for spoken conversations is\nchallenging due to the gap in distributions of spoken and written data. This\npaper presents our approach to build generalized models for the\nKnowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations\nChallenge of DSTC-10. In order to mitigate the discrepancies between spoken and\nwritten text, we mainly employ extensive data augmentation strategies on\nwritten data, including artificial error injection and round-trip text-speech\ntransformation. To train robust models for spoken conversations, we improve\npre-trained language models, and apply ensemble algorithms for each sub-task.\nTypically, for the detection task, we fine-tune \\roberta and ELECTRA, and run\nan error-fixing ensemble algorithm. For the selection task, we adopt a\ntwo-stage framework that consists of entity tracking and knowledge ranking, and\npropose a multi-task learning method to learn multi-level semantic information\nby domain classification and entity selection. For the generation task, we\nadopt a cross-validation data process to improve pre-trained generative\nlanguage models, followed by a consensus decoding algorithm, which can add\narbitrary features like relative \\rouge metric, and tune associated feature\nweights toward \\bleu directly. Our approach ranks third on the objective\nevaluation and second on the final official human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ruijie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shuang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shihui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuchi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiajun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liangrui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Distillation Guided Salient Object Detection. (arXiv:2203.04076v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04076","description":"<p>Most existing CNN-based salient object detection methods can identify local\nsegmentation details like hair and animal fur, but often misinterpret the real\nsaliency due to the lack of global contextual information caused by the\nsubjectiveness of the SOD task and the locality of convolution layers.\nMoreover, due to the unrealistically expensive labeling costs, the current\nexisting SOD datasets are insufficient to cover the real data distribution. The\nlimitation and bias of the training data add additional difficulty to fully\nexploring the semantic association between object-to-object and\nobject-to-environment in a given image. In this paper, we propose a semantic\ndistillation guided SOD (SDG-SOD) method that produces accurate results by\nfusing semantically distilled knowledge from generated image captioning into\nthe Vision-Transformer-based SOD framework. SDG-SOD can better uncover\ninter-objects and object-to-environment saliency and cover the gap between the\nsubjective nature of SOD and its expensive labeling. Comprehensive experiments\non five benchmark datasets demonstrate that the SDG-SOD outperforms the\nstate-of-the-art approaches on four evaluation metrics, and largely improves\nthe model performance on DUTS, ECSSD, DUT, HKU-IS, and PASCAL-S datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plumeria at SemEval-2022 Task 6: Robust Approaches for Sarcasm Detection for English and Arabic Using Transformers and Data Augmentation. (arXiv:2203.04111v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04111","description":"<p>This paper describes our submission to SemEval-2022 Task 6 on sarcasm\ndetection and its five subtasks for English and Arabic. Sarcasm conveys a\nmeaning which contradicts the literal meaning, and it is mainly found on social\nnetworks. It has a significant role in understanding the intention of the user.\nFor detecting sarcasm, we used deep learning techniques based on transformers\ndue to its success in the field of Natural Language Processing (NLP) without\nthe need for feature engineering. The datasets were taken from tweets. We\ncreated new datasets by augmenting with external data or by using word\nembeddings and repetition of instances. Experiments were done on the datasets\nwith different types of preprocessing because it is crucial in this task. The\nrank of our team was consistent across four subtasks (fourth rank in three\nsubtasks and sixth rank in one subtask); whereas other teams might be in the\ntop ranks for some subtasks but rank drastically less in other subtasks. This\nimplies the robustness and stability of the models and the techniques we used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nigam_S/0/1/0/all/0/1\">Shubham Kumar Nigam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaheen_M/0/1/0/all/0/1\">Mosab Shaheen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Mixing of Contextual Information in the Transformer. (arXiv:2203.04212v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04212","description":"<p>The Transformer architecture aggregates input information through the\nself-attention mechanism, but there is no clear understanding of how this\ninformation is mixed across the entire model. Additionally, recent works have\ndemonstrated that attention weights alone are not enough to describe the flow\nof information. In this paper, we consider the whole attention block\n--multi-head attention, residual connection, and layer normalization-- and\ndefine a metric to measure token-to-token interactions within each layer,\nconsidering the characteristics of the representation space. Then, we aggregate\nlayer-wise interpretations to provide input attribution scores for model\npredictions. Experimentally, we show that our method, ALTI (Aggregation of\nLayer-wise Token-to-token Interactions), provides faithful explanations and\noutperforms similar aggregation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrando_J/0/1/0/all/0/1\">Javier Ferrando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Bidirectional Translation between Descriptions and Actions with Small Paired Data. (arXiv:2203.04218v1 [cs.RO])","link":"http://arxiv.org/abs/2203.04218","description":"<p>This study achieved bidirectional translation between descriptions and\nactions using small paired data. The ability to mutually generate descriptions\nand actions is essential for robots to collaborate with humans in their daily\nlives. The robot is required to associate real-world objects with linguistic\nexpressions, and large-scale paired data are required for machine learning\napproaches. However, a paired dataset is expensive to construct and difficult\nto collect. This study proposes a two-stage training method for bidirectional\ntranslation. In the proposed method, we train recurrent autoencoders (RAEs) for\ndescriptions and actions with a large amount of non-paired data. Then, we\nfine-tune the entire model to bind their intermediate representations using\nsmall paired data. Because the data used for pre-training do not require\npairing, behavior-only data or a large language corpus can be used. We\nexperimentally evaluated our method using a paired dataset consisting of\nmotion-captured actions and descriptions. The results showed that our method\nperformed well, even when the amount of paired data to train was small. The\nvisualization of the intermediate representations of each RAE showed that\nsimilar actions were encoded in a clustered position and the corresponding\nfeature vectors well aligned.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toyoda_M/0/1/0/all/0/1\">Minori Toyoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1\">Kanata Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_Y/0/1/0/all/0/1\">Yoshihiko Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogata_T/0/1/0/all/0/1\">Tetsuya Ogata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-agnostic BERT Sentence Embedding. (arXiv:2007.01852v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.01852","description":"<p>While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning\n(Reimers and Gurevych, 2019), BERT based cross-lingual sentence embeddings have\nyet to be explored. We systematically investigate methods for learning\nmultilingual sentence embeddings by combining the best methods for learning\nmonolingual and cross-lingual representations including: masked language\nmodeling (MLM), translation language modeling (TLM) (Conneau and Lample, 2019),\ndual encoder translation ranking (Guo et al., 2018), and additive margin\nsoftmax (Yang et al., 2019a). We show that introducing a pre-trained\nmultilingual language model dramatically reduces the amount of parallel\ntraining data required to achieve good performance by 80%. Composing the best\nof these methods produces a model that achieves 83.7% bi-text retrieval\naccuracy over 112 languages on Tatoeba, well above the 65.5% achieved by\nArtetxe and Schwenk (2019b), while still performing competitively on\nmonolingual transfer learning benchmarks (Conneau and Kiela, 2018). Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fangxiaoyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arivazhagan_N/0/1/0/all/0/1\">Naveen Arivazhagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QuatRE: Relation-Aware Quaternions for Knowledge Graph Embeddings. (arXiv:2009.12517v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.12517","description":"<p>We propose a simple yet effective embedding model to learn quaternion\nembeddings for entities and relations in knowledge graphs. Our model aims to\nenhance correlations between head and tail entities given a relation within the\nQuaternion space with Hamilton product. The model achieves this goal by further\nassociating each relation with two relation-aware rotations, which are used to\nrotate quaternion embeddings of the head and tail entities, respectively.\nExperimental results show that our proposed model produces state-of-the-art\nperformances on well-known benchmark datasets for knowledge graph completion.\nOur code is available at: \\url{https://github.com/daiquocnguyen/QuatRE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thanh Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Dinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Label Annotation of Chest Abdomen Pelvis Computed Tomography Text Reports Using Deep Learning. (arXiv:2102.02959v5 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2102.02959","description":"<p>Purpose: To develop high throughput multi-label annotators for body (chest,\nabdomen, and pelvis) Computed Tomography (CT) reports that can be applied\nacross a variety of abnormalities, organs, and disease states.\n</p>\n<p>Approach: We used a dictionary approach to develop rule-based algorithms\n(RBA) for extraction of disease labels from radiology text reports. We targeted\nthree organ systems (lungs/pleura, liver/gallbladder, kidneys/ureters) with\nfour diseases per system based on their prevalence in our dataset. To expand\nthe algorithms beyond pre-defined keywords, attention-guided recurrent neural\nnetworks (RNN) were trained using the RBA-extracted labels to classify reports\nas being positive for one or more diseases or normal for each organ system.\nConfounding effects on model performance were evaluated using random\ninitialization or pre-trained embedding as well as different sizes of training\ndatasets. Performance was evaluated using the receiver operating characteristic\n(ROC) area under the curve (AUC) against 2,158 manually obtained labels.\n</p>\n<p>Results: Our models extracted disease labels from 261,229 radiology reports\nof 112,501 unique subjects. Pre-trained models outperformed random\ninitialization across all diseases. As the training dataset size was reduced,\nperformance was robust except for a few diseases with relatively small number\nof cases. Pre-trained classification AUCs achieved &gt; 0.95 for all five disease\noutcomes across all three organ systems.\n</p>\n<p>Conclusions: Our label-extracting pipeline was able to encompass a variety of\ncases and diseases by generalizing beyond strict rules with exceptional\naccuracy. This method can be easily adapted to enable automated labeling of\nhospital-scale medical data sets for training image-based disease classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DAnniballe_V/0/1/0/all/0/1\">Vincent M. D&#x27;Anniballe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tushar_F/0/1/0/all/0/1\">Fakrul Islam Tushar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faryna_K/0/1/0/all/0/1\">Khrystyna Faryna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Songyue Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1\">Maciej A. Mazurowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_G/0/1/0/all/0/1\">Geoffrey D. Rubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_J/0/1/0/all/0/1\">Joseph Y. Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ByT5: Towards a token-free future with pre-trained byte-to-byte models. (arXiv:2105.13626v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.13626","description":"<p>Most widely-used pre-trained language models operate on sequences of tokens\ncorresponding to word or subword units. By comparison, token-free models that\noperate directly on raw text (bytes or characters) have many benefits: they can\nprocess text in any language out of the box, they are more robust to noise, and\nthey minimize technical debt by removing complex and error-prone text\npreprocessing pipelines. Since byte or character sequences are longer than\ntoken sequences, past work on token-free models has often introduced new model\narchitectures designed to amortize the cost of operating directly on raw text.\nIn this paper, we show that a standard Transformer architecture can be used\nwith minimal modifications to process byte sequences. We characterize the\ntrade-offs in terms of parameter count, training FLOPs, and inference speed,\nand show that byte-level models are competitive with their token-level\ncounterparts. We also demonstrate that byte-level models are significantly more\nrobust to noise and perform better on tasks that are sensitive to spelling and\npronunciation. As part of our contribution, we release a new set of pre-trained\nbyte-level Transformer models based on the T5 architecture, as well as all code\nand data used in our experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Linting Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barua_A/0/1/0/all/0/1\">Aditya Barua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1\">Rami Al-Rfou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Integrated Gradients and Constituency Parse Trees to explain Linguistic Acceptability learnt by BERT. (arXiv:2106.07349v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07349","description":"<p>Linguistic Acceptability is the task of determining whether a sentence is\ngrammatical or ungrammatical. It has applications in several use cases like\nQuestion-Answering, Natural Language Generation, Neural Machine Translation,\nwhere grammatical correctness is crucial. In this paper we aim to understand\nthe decision-making process of BERT (Devlin et al., 2019) in distinguishing\nbetween Linguistically Acceptable sentences (LA) and Linguistically\nUnacceptable sentences (LUA). We leverage Layer Integrated Gradients\nAttribution Scores (LIG) to explain the Linguistic Acceptability criteria that\nare learnt by BERT on the Corpus of Linguistic Acceptability (CoLA) (Warstadt\net al., 2018) benchmark dataset. Our experiments on 5 categories of sentences\nlead to the following interesting findings: 1) LIG for LA are significantly\nsmaller in comparison to LUA, 2) There are specific subtrees of the\nConstituency Parse Tree (CPT) for LA and LUA which contribute larger LIG, 3)\nAcross the different categories of sentences we observed around 88% to 100% of\nthe Correctly classified sentences had positive LIG, indicating a strong\npositive relationship to the prediction confidence of the model, and 4) Around\n43% of the Misclassified sentences had negative LIG, which we believe can\nbecome correctly classified sentences if the LIG are parameterized in the loss\nfunction of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1\">Anmol Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timmapathini_H/0/1/0/all/0/1\">Hari Prasad Timmapathini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text. (arXiv:2107.01294v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.01294","description":"<p>Modern neural language models can produce remarkably fluent and grammatical\ntext. So much, in fact, that recent work by Clark et al. (2021) has reported\nthat conventional crowdsourcing can no longer reliably distinguish between\nmachine-authored (GPT-3) and human-authored writing. As errors in machine\ngenerations become ever subtler and harder to spot, it poses a new challenge to\nthe research community for robust machine text evaluation. We propose a new\nframework called Scarecrow for scrutinizing machine text via crowd annotation.\nTo support the broad range of real machine errors that can be identified by\nlaypeople, the ten error categories of Scarecrow -- such as redundancy,\ncommonsense errors, and incoherence -- are identified through several rounds of\ncrowd annotation experiments without a predefined ontology. We then use\nScarecrow to collect over 41k error spans in human-written and\nmachine-generated paragraphs of English language news text. We isolate factors\nfor detailed analysis, including parameter count, training data, and various\ndecoding-time configurations. Our approach successfully quantifies measurable\ngaps between human authored text and generations from models of several sizes,\nincluding fourteen configurations of GPT-3. In addition, our analysis unveils\nnew insights, with detailed rationales provided by laypeople, e.g., that the\ncommonsense capabilities have been improving with larger models while math\ncapabilities have not, and that the choices of simple decoding hyperparameters\ncan make remarkable differences on the perceived quality of machine text. We\nrelease our training material, annotation toolkit and dataset at\nhttps://yao-dou.github.io/scarecrow/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEAP-FAKED: Knowledge Graph based Approach for Fake News Detection. (arXiv:2107.10648v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.10648","description":"<p>Fake News on social media platforms has attracted a lot of attention in\nrecent times, primarily for events related to politics (2016 US Presidential\nelections), healthcare (infodemic during COVID-19), to name a few. Various\nmethods have been proposed for detecting Fake News. The approaches span from\nexploiting techniques related to network analysis, Natural Language Processing\n(NLP), and the usage of Graph Neural Networks (GNNs). In this work, we propose\nDEAP-FAKED, a knowleDgE grAPh FAKe nEws Detection framework for identifying\nFake News. Our approach is a combination of the NLP -- where we encode the news\ncontent, and the GNN technique -- where we encode the Knowledge Graph (KG). A\nvariety of these encodings provides a complementary advantage to our detector.\nWe evaluate our framework using two publicly available datasets containing\narticles from domains such as politics, business, technology, and healthcare.\nAs part of dataset pre-processing, we also remove the bias, such as the source\nof the articles, which could impact the performance of the models. DEAP-FAKED\nobtains an F1-score of 88% and 78% for the two datasets, which is an\nimprovement of 21%, and 3% respectively, which shows the effectiveness of the\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mayank_M/0/1/0/all/0/1\">Mohit Mayank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shakshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajesh Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSTL - From English Requirements to Signal Temporal Logic. (arXiv:2109.10294v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10294","description":"<p>Formal methods provide very powerful tools and techniques for the design and\nanalysis of complex systems. Their practical application remains however\nlimited, due to the widely accepted belief that formal methods require\nextensive expertise and a steep learning curve. Writing correct formal\nspecifications in form of logical formulas is still considered to be a\ndifficult and error prone task.\n</p>\n<p>In this paper we propose DeepSTL, a tool and technique for the translation of\ninformal requirements, given as free English sentences, into Signal Temporal\nLogic (STL), a formal specification language for cyber-physical systems, used\nboth by academia and advanced research labs in industry. A major challenge to\ndevise such a translator is the lack of publicly available informal\nrequirements and formal specifications. We propose a two-step workflow to\naddress this challenge. We first design a grammar-based generation technique of\nsynthetic data, where each output is a random STL formula and its associated\nset of possible English translations. In the second step, we use a\nstate-of-the-art transformer-based neural translation technique, to train an\naccurate attentional translator of English to STL. The experimental results\nshow high translation quality for patterns of English requirements that have\nbeen well trained, making this workflow promising to be extended for processing\nmore complex translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartocci_E/0/1/0/all/0/1\">Ezio Bartocci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickovic_D/0/1/0/all/0/1\">Dejan Ni&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isakovic_H/0/1/0/all/0/1\">Haris Isakovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1\">Radu Grosu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EntQA: Entity Linking as Question Answering. (arXiv:2110.02369v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02369","description":"<p>A conventional approach to entity linking is to first find mentions in a\ngiven document and then infer their underlying entities in the knowledge base.\nA well-known limitation of this approach is that it requires finding mentions\nwithout knowing their entities, which is unnatural and difficult. We present a\nnew model that does not suffer from this limitation called EntQA, which stands\nfor Entity linking as Question Answering. EntQA first proposes candidate\nentities with a fast retrieval module, and then scrutinizes the document to\nfind mentions of each candidate with a powerful reader module. Our approach\ncombines progress in entity linking with that in open-domain question answering\nand capitalizes on pretrained models for dense entity retrieval and reading\ncomprehension. Unlike in previous works, we do not rely on a mention-candidates\ndictionary or large-scale weak supervision. EntQA achieves strong results on\nthe GERBIL benchmarking platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stratos_K/0/1/0/all/0/1\">Karl Stratos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Combating Hype, Proceed with Caution. (arXiv:2110.08300v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08300","description":"<p>In an effort to avoid reinforcing widespread hype about the capabilities of\nstate-of-the-art language technology, researchers have developed practices in\nframing and citation that serve to deemphasize the field's successes. Though\nwell-meaning, these practices often yield misleading or even false claims about\nthe limits of our best technology. This is a problem, and it may be more\nserious than it looks: It limits our ability to mitigate short-term harms from\nNLP deployments and it limits our ability to prepare for the potentially\nenormous impacts of more distant future advances. This paper urges researchers\nto be careful about these claims and suggests some research directions and\ncommunication strategies that will make it easier to avoid or rebut them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Labeling for Massively Multilingual Speech Recognition. (arXiv:2111.00161v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00161","description":"<p>Semi-supervised learning through pseudo-labeling has become a staple of\nstate-of-the-art monolingual speech recognition systems. In this work, we\nextend pseudo-labeling to massively multilingual speech recognition with 60\nlanguages. We propose a simple pseudo-labeling recipe that works well even with\nlow-resource languages: train a supervised multilingual model, fine-tune it\nwith semi-supervised learning on a target language, generate pseudo-labels for\nthat language, and train a final model using pseudo-labels for all languages,\neither from scratch or by fine-tuning. Experiments on the labeled Common Voice\nand unlabeled VoxPopuli datasets show that our recipe can yield a model with\nbetter performance for many languages that also transfers well to LibriSpeech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lugosch_L/0/1/0/all/0/1\">Loren Lugosch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized Scene Imagination for Generative Commonsense Reasoning. (arXiv:2112.06318v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06318","description":"<p>Humans use natural language to compose common concepts from their environment\ninto plausible, day-to-day scene descriptions. However, such generative\ncommonsense reasoning (GCSR) skills are lacking in state-of-the-art text\ngeneration methods. Descriptive sentences about arbitrary concepts generated by\nneural text generation models (e.g., pre-trained text-to-text Transformers) are\noften grammatically fluent but may not correspond to human common sense,\nlargely due to their lack of mechanisms to capture concept relations, to\nidentify implicit concepts, and to perform generalizable reasoning about unseen\nconcept compositions. In this paper, we propose an Imagine-and-Verbalize (I&amp;V)\nmethod, which learns to imagine a relational scene knowledge graph (SKG) with\nrelations between the input concepts, and leverage the SKG as a constraint when\ngenerating a plausible scene description. We collect and harmonize a set of\nknowledge resources from different domains and modalities, providing a rich\nauxiliary supervision signal for I&amp;V. The experiments demonstrate the\neffectiveness of I&amp;V in improving language models on both concept-to-sentence\nand concept-to-story generation tasks, while enabling the model to learn well\nfrom fewer task examples and generate SKGs that make common sense to human\nannotators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">PeiFeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamora_J/0/1/0/all/0/1\">Jonathan Zamora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViNMT: Neural Machine Translation Toolkit. (arXiv:2112.15272v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.15272","description":"<p>We present an open-source toolkit for neural machine translation (NMT). The\nnew toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along\nwith many other improvements detailed below, in order to create a\nself-contained, simple to use, consistent and comprehensive framework for\nMachine Translation tasks of various domains. It is tooled to support both\nbilingual and multilingual translation tasks, starting from building the model\nfrom respective corpora, to inferring new predictions or packaging the model to\nserving-capable JIT format.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quan_N/0/1/0/all/0/1\">Nguyen Hoang Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dat_N/0/1/0/all/0/1\">Nguyen Thanh Dat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_N/0/1/0/all/0/1\">Nguyen Hoang Minh Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Nguyen Van Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Ngo Thi Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_N/0/1/0/all/0/1\">Nguyen Phuong Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viet_T/0/1/0/all/0/1\">Tran Hong Viet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. (arXiv:2201.07207v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.07207","description":"<p>Can world knowledge learned by large language models (LLMs) be used to act in\ninteractive environments? In this paper, we investigate the possibility of\ngrounding high-level tasks, expressed in natural language (e.g. \"make\nbreakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While\nprior work focused on learning from explicit step-by-step examples of how to\nact, we surprisingly find that if pre-trained LMs are large enough and prompted\nappropriately, they can effectively decompose high-level tasks into mid-level\nplans without any further training. However, the plans produced naively by LLMs\noften cannot map precisely to admissible actions. We propose a procedure that\nconditions on existing demonstrations and semantically translates the plans to\nadmissible actions. Our evaluation in the recent VirtualHome environment shows\nthat the resulting method substantially improves executability over the LLM\nbaseline. The conducted human evaluation reveals a trade-off between\nexecutability and correctness but shows a promising sign towards extracting\nactionable knowledge from language models. Website at\nhttps://huangwl18.github.io/language-planner\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenlong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuMiN: A Large-Scale Multilingual Multimodal Fact-Checked Misinformation Social Network Dataset. (arXiv:2202.11684v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.11684","description":"<p>Misinformation is becoming increasingly prevalent on social media and in news\narticles. It has become so widespread that we require algorithmic assistance\nutilising machine learning to detect such content. Training these machine\nlearning models require datasets of sufficient scale, diversity and quality.\nHowever, datasets in the field of automatic misinformation detection are\npredominantly monolingual, include a limited amount of modalities and are not\nof sufficient scale and quality. Addressing this, we develop a data collection\nand linking system (MuMiN-trawl), to build a public misinformation graph\ndataset (MuMiN), containing rich social media data (tweets, replies, users,\nimages, articles, hashtags) spanning 21 million tweets belonging to 26 thousand\nTwitter threads, each of which have been semantically linked to 13 thousand\nfact-checked claims across dozens of topics, events and domains, in 41\ndifferent languages, spanning more than a decade. The dataset is made available\nas a heterogeneous graph via a Python package (mumin). We provide baseline\nresults for two node classification tasks related to the veracity of a claim\ninvolving social media, and demonstrate that these are challenging tasks, with\nthe highest macro-average F1-score being 62.55% and 61.45% for the two tasks,\nrespectively. The MuMiN ecosystem is available at\nhttps://mumin-dataset.github.io/, including the data, documentation, tutorials\nand leaderboards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_D/0/1/0/all/0/1\">Dan Saattrup Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McConville_R/0/1/0/all/0/1\">Ryan McConville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion. (arXiv:2202.13785v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.13785","description":"<p>Knowledge graphs store a large number of factual triples while they are still\nincomplete, inevitably. The previous knowledge graph completion (KGC) models\npredict missing links between entities merely relying on fact-view data,\nignoring the valuable commonsense knowledge. The previous knowledge graph\nembedding (KGE) techniques suffer from invalid negative sampling and the\nuncertainty of fact-view link prediction, limiting KGC's performance. To\naddress the above challenges, we propose a novel and scalable Commonsense-Aware\nKnowledge Embedding (CAKE) framework to automatically extract commonsense from\nfactual triples with entity concepts. The generated commonsense augments\neffective self-supervision to facilitate both high-quality negative sampling\n(NS) and joint commonsense and fact-view link prediction. Experimental results\non the KGC task demonstrate that assembling our framework could enhance the\nperformance of the original KGE models, and the proposed commonsense-aware NS\nmodule is superior to other NS techniques. Besides, our proposed framework\ncould be easily adaptive to various KGE models and explain the predicted\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Guanglin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EPPAC: Entity Pre-typing Relation Classification with Prompt AnswerCentralizing. (arXiv:2203.00193v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00193","description":"<p>Relation classification (RC) aims to predict the relationship between a pair\nof subject and object in a given context. Recently, prompt tuning approaches\nhave achieved high performance in RC. However, existing prompt tuning\napproaches have the following issues: (1) numerous categories decrease RC\nperformance; (2) manually designed prompts require intensive labor. To address\nthese issues, a novel paradigm, Entity Pre-typing Relation Classification with\nPrompt Answer Centralizing(EPPAC) is proposed in this paper. The entity\npre-tying in EPPAC is presented to address the first issue using a double-level\nframework that pre-types entities before RC and prompt answer centralizing is\nproposed to address the second issue. Extensive experiments show that our\nproposed EPPAC outperformed state-of-the-art approaches on TACRED and TACREV by\n14.4% and 11.1%, respectively. The code is provided in the Supplementary\nMaterials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jiejun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">WeiWei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discontinuous Constituency and BERT: A Case Study of Dutch. (arXiv:2203.01063v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.01063","description":"<p>In this paper, we set out to quantify the syntactic capacity of BERT in the\nevaluation regime of non-context free patterns, as occurring in Dutch. We\ndevise a test suite based on a mildly context-sensitive formalism, from which\nwe derive grammars that capture the linguistic phenomena of control verb\nnesting and verb raising. The grammars, paired with a small lexicon, provide us\nwith a large collection of naturalistic utterances, annotated with verb-subject\npairings, that serve as the evaluation test bed for an attention-based span\nselection probe. Our results, backed by extensive analysis, suggest that the\nmodels investigated fail in the implicit acquisition of the dependencies\nexamined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kogkalidis_K/0/1/0/all/0/1\">Konstantinos Kogkalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijnholds_G/0/1/0/all/0/1\">Gijs Wijnholds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doctor Recommendation in Online Health Forums via Expertise Learning. (arXiv:2203.02932v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.02932","description":"<p>Huge volumes of patient queries are daily generated on online health forums,\nrendering manual doctor allocation a labor-intensive task. To better help\npatients, this paper studies a novel task of doctor recommendation to enable\nautomatic pairing of a patient to a doctor with relevant expertise. While most\nprior work in recommendation focuses on modeling target users from their past\nbehavior, we can only rely on the limited words in a query to infer a patient's\nneeds for privacy reasons. For doctor modeling, we study the joint effects of\ntheir profiles and previous dialogues with other patients and explore their\ninteractions via self-learning. The learned doctor embeddings are further\nemployed to estimate their capabilities of handling a patient query with a\nmulti-head attention mechanism. For experiments, a large-scale dataset is\ncollected from Chunyu Yisheng, a Chinese online health forum, where our model\nexhibits the state-of-the-art results, outperforming baselines only consider\nprofiles and past dialogues to characterize a doctor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoxin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yubo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1\">Shi Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Mammograms Classification: A Review. (arXiv:2203.03618v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03618","description":"<p>An advanced reliable low-cost form of screening method, Digital mammography\nhas been used as an effective imaging method for breast cancer detection. With\nan increased focus on technologies to aid healthcare, Mammogram images have\nbeen utilized in developing computer-aided diagnosis systems that will\npotentially help in clinical diagnosis. Researchers have proved that artificial\nintelligence with its emerging technologies can be used in the early detection\nof the disease and improve radiologists' performance in assessing breast\ncancer. In this paper, we review the methods developed for mammogram mass\nclassification in two categories. The first one is classifying manually\nprovided cropped region of interests (ROI) as either malignant or benign, and\nthe second one is the classification of automatically segmented ROIs as either\nmalignant or benign. We also provide an overview of datasets and evaluation\nmetrics used in the classification task. Finally, we compare and discuss the\ndeep learning approach to classical image processing and learning approach in\nthis domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Elbatel_M/0/1/0/all/0/1\">Marawan Elbatel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Cross-Layer Attention for Image Restoration. (arXiv:2203.03619v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03619","description":"<p>Non-local attention module has been proven to be crucial for image\nrestoration. Conventional non-local attention processes features of each layer\nseparately, so it risks missing correlation between features among different\nlayers. To address this problem, we propose Cross-Layer Attention (CLA) module\nin this paper. Instead of finding correlated key pixels within the same layer,\neach query pixel can attend to key pixels at previous layers of the network. In\norder to further enhance the learning capability and reduce the inference cost\nof CLA, we further propose Adaptive CLA, or ACLA, as an improved CLA. Two\nadaptive designs are proposed for ACLA: 1) adaptively selecting the keys for\nnon-local attention at each layer; 2) automatically searching for the insertion\nlocations for ACLA modules. By these two adaptive designs, ACLA dynamically\nselects the number of keys to be aggregated for non-local attention at layer.\nIn addition, ACLA searches for the optimal insert positions of ACLA modules by\na neural architecture search method to render a compact neural network with\ncompelling performance. Extensive experiments on image restoration tasks,\nincluding single image super-resolution, image denoising, image demosaicing,\nand image compression artifacts reduction, validate the effectiveness and\nefficiency of ACLA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yancheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yingzhen Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Triple Motion Estimation and Frame Interpolation based on Adaptive Threshold for Frame Rate Up-Conversion. (arXiv:2203.03621v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03621","description":"<p>In this paper, we propose a novel motion-compensated frame rate up-conversion\n(MC-FRUC) algorithm. The proposed algorithm creates interpolated frames by\nfirst estimating motion vectors using unilateral (jointing forward and\nbackward) and bilateral motion estimation. Then motion vectors are combined\nbased on adaptive threshold, in order to creates high-quality interpolated\nframes and reduce block artifacts. Since motion-compensated frame interpolation\nalong unilateral motion trajectories yields holes, a new algorithm is\nintroduced to resolve this problem. The experimental results show that the\nquality of the interpolated frames using the proposed algorithm is much higher\nthan the existing algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Naderi_H/0/1/0/all/0/1\">Hanieh Naderi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahmati_M/0/1/0/all/0/1\">Mohammad Rahmati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep-ASPECTS: A Segmentation-Assisted Model for Stroke Severity Measurement. (arXiv:2203.03622v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03622","description":"<p>A stroke occurs when an artery in the brain ruptures and bleeds or when the\nblood supply to the brain is cut off. Blood and oxygen cannot reach the brain's\ntissues due to the rupture or obstruction resulting in tissue death. The Middle\ncerebral artery (MCA) is the largest cerebral artery and the most commonly\ndamaged vessel in stroke. The quick onset of a focused neurological deficit\ncaused by interruption of blood flow in the territory supplied by the MCA is\nknown as an MCA stroke. Alberta stroke programme early CT score (ASPECTS) is\nused to estimate the extent of early ischemic changes in patients with MCA\nstroke. This study proposes a deep learning-based method to score the CT scan\nfor ASPECTS. Our work has three highlights. First, we propose a novel method\nfor medical image segmentation for stroke detection. Second, we show the\neffectiveness of AI solution for fully-automated ASPECT scoring with reduced\ndiagnosis time for a given non-contrast CT (NCCT) Scan. Our algorithms show a\ndice similarity coefficient of 0.64 for the MCA anatomy segmentation and 0.72\nfor the infarcts segmentation. Lastly, we show that our model's performance is\ninline with inter-reader variability between radiologists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1\">Ujjwal Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ranjan_M/0/1/0/all/0/1\">Mukul Ranjan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golla_S/0/1/0/all/0/1\">Satish Golla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanamala_S/0/1/0/all/0/1\">Swetha Tanamala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sreenivas_P/0/1/0/all/0/1\">Preetham Sreenivas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chilamkurthy_S/0/1/0/all/0/1\">Sasank Chilamkurthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pandian_J/0/1/0/all/0/1\">Jeyaraj Pandian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tarpley_J/0/1/0/all/0/1\">Jason Tarpley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction. (arXiv:2203.03623v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03623","description":"<p>We propose a novel and unified method, measurement-conditioned denoising\ndiffusion probabilistic model (MC-DDPM), for under-sampled medical image\nreconstruction based on DDPM. Different from previous works, MC-DDPM is defined\nin measurement domain (e.g. k-space in MRI reconstruction) and conditioned on\nunder-sampling mask. We apply this method to accelerate MRI reconstruction and\nthe experimental results show excellent performance, outperforming full\nsupervision baseline and the state-of-the-art score-based reconstruction\nmethod. Due to its generative nature, MC-DDPM can also quantify the uncertainty\nof reconstruction. Our code is available on github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xie_Y/0/1/0/all/0/1\">Yutong Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Quanzheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusion-Correction Network for Single-Exposure Correction and Multi-Exposure Fusion. (arXiv:2203.03624v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03624","description":"<p>The photographs captured by digital cameras usually suffer from over-exposure\nor under-exposure problems. The Single-Exposure Correction (SEC) and\nMulti-Exposure Fusion (MEF) are two widely studied image processing tasks for\nimage exposure enhancement. However, current SEC and MEF methods ignore the\ninternal correlation between SEC and MEF, and are proposed under distinct\nframeworks. What's more, most MEF methods usually fail at processing a sequence\ncontaining only under-exposed or over-exposed images. To alleviate these\nproblems, in this paper, we develop an integrated framework to simultaneously\ntackle the SEC and MEF tasks. Built upon the Laplacian Pyramid (LP)\ndecomposition, we propose a novel Fusion-Correction Network (FCNet) to fuse and\ncorrect an image sequence sequentially in a multi-level scheme. In each LP\nlevel, the image sequence is feed into a Fusion block and a Correction block\nfor consecutive image fusion and exposure correction. The corrected image is\nupsampled and re-composed with the high-frequency detail components in\nnext-level, producing the base sequence for the next-level blocks. Experiments\non the benchmark dataset demonstrate that our FCNet is effective on both the\nSEC and MEF tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1\">Jin Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_A/0/1/0/all/0/1\">Anran Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coordinate Translator for Learning Deformable Medical Image Registration. (arXiv:2203.03626v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03626","description":"<p>The majority of deep learning (DL) based deformable image registration\nmethods use convolutional neural networks (CNNs) to estimate displacement\nfields from pairs of moving and fixed images. This, however, requires the\nconvolutional kernels in the CNN to not only extract intensity features from\nthe inputs but also understand image coordinate systems. We argue that the\nlatter task is challenging for traditional CNNs, limiting their performance in\nregistration tasks. To tackle this problem, we first introduce Coordinate\nTranslator (CoTr), a differentiable module that identifies matched features\nbetween the fixed and moving image and outputs their coordinate correspondences\nwithout the need for training. It unloads the burden of understanding image\ncoordinate systems for CNNs, allowing them to focus on feature extraction. We\nthen propose a novel deformable registration network, im2grid, that uses\nmultiple CoTr's with the hierarchical features extracted from a CNN encoder and\noutputs a deformation field in a coarse-to-fine fashion. We compared im2grid\nwith the state-of-the-art DL and non-DL methods for unsupervised 3D magnetic\nresonance image registration. Our experiments show that im2grid outperforms\nthese methods both qualitatively and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuo_L/0/1/0/all/0/1\">Lianrui Zuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1\">Shuo Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prince_J/0/1/0/all/0/1\">Jerry L. Prince</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carass_A/0/1/0/all/0/1\">Aaron Carass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-channel deep convolutional neural networks for multi-classifying thyroid disease. (arXiv:2203.03627v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03627","description":"<p>Thyroid disease instances have been continuously increasing since the 1990s,\nand thyroid cancer has become the most rapidly rising disease among all the\nmalignancies in recent years. Most existing studies focused on applying deep\nconvolutional neural networks for detecting thyroid cancer. Despite their\nsatisfactory performance on binary classification tasks, limited studies have\nexplored multi-class classification of thyroid disease types; much less is\nknown of the diagnosis of co-existence situation for different types of thyroid\ndiseases. Therefore, this study proposed a novel multi-channel convolutional\nneural network (CNN) architecture to address the multi-class classification\ntask of thyroid disease. The multi-channel CNN merits from computed tomography\nto drive a comprehensive diagnostic decision for the overall thyroid gland,\nemphasizing the disease co-existence circumstance. Moreover, this study also\nexamined alternative strategies to enhance the diagnostic accuracy of CNN\nmodels through concatenation of different scales of feature maps. Benchmarking\nexperiments demonstrate the improved performance of the proposed multi-channel\nCNN architecture compared with the standard single-channel CNN architecture.\nMore specifically, the multi-channel CNN achieved an accuracy of 0.909,\nprecision of 0.944, recall of 0.896, specificity of 0.994, and F1 of 0.917, in\ncontrast to the single-channel CNN, which obtained 0.902, 0.892, 0.909, 0.993,\n0.898, respectively. In addition, the proposed model was evaluated in different\ngender groups; it reached a diagnostic accuracy of 0.908 for the female group\nand 0.901 for the male group. Collectively, the results highlight that the\nproposed multi-channel CNN has excellent generalization and has the potential\nto be deployed to provide computational decision support in clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_V/0/1/0/all/0/1\">Vincent CS. Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rong_J/0/1/0/all/0/1\">Jia Rong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">James C. Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Jiangning Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Student Become Decathlon Master in Retinal Vessel Segmentation via Dual-teacher Multi-target Domain Adaptation. (arXiv:2203.03631v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03631","description":"<p>Unsupervised domain adaptation has been proposed recently to tackle the\nso-called domain shift between training data and test data with different\ndistributions. However, most of them only focus on single-target domain\nadaptation and cannot be applied to the scenario with multiple target domains.\nIn this paper, we propose RVms, a novel unsupervised multi-target domain\nadaptation approach to segment retinal vessels (RVs) from multimodal and\nmulticenter retinal images. RVms mainly consists of a style augmentation and\ntransfer (SAT) module and a dual-teacher knowledge distillation (DTKD) module.\nSAT augments and clusters images into source-similar domains and\nsource-dissimilar domains via B\\'ezier and Fourier transformations. DTKD\nutilizes the augmented and transformed data to train two teachers, one for\nsource-similar domains and the other for source-dissimilar domains. Afterwards,\nknowledge distillation is performed to iteratively distill different domain\nknowledge from teachers to a generic student. The local relative intensity\ntransformation is employed to characterize RVs in a domain invariant manner and\npromote the generalizability of teachers and student models. Moreover, we\nconstruct a new multimodal and multicenter vascular segmentation dataset from\nexisting publicly-available datasets, which can be used to benchmark various\ndomain adaptation and domain generalization methods. Through extensive\nexperiments, RVms is found to be very close to the target-trained Oracle in\nterms of segmenting the RVs, largely outperforming other state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Linkai Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_H/0/1/0/all/0/1\">Huaqing He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InsightNet: non-contact blood pressure measuring network based on face video. (arXiv:2203.03634v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03634","description":"<p>Blood pressure indicates cardiac function and peripheral vascular resistance\nand is critical for disease diagnosis. Traditionally, blood pressure data are\nmainly acquired through contact sensors, which require high maintenance and may\nbe inconvenient and unfriendly to some people (e.g., burn patients). In this\npaper, an efficient non-contact blood pressure measurement network based on\nface videos is proposed for the first time. An innovative oversampling training\nstrategy is proposed to handle the unbalanced data distribution. The input\nvideo sequences are first normalized and converted to our proposed YUVT color\nspace. Then, the Spatio-temporal slicer encodes it into a multi-domain\nSpatio-temporal mapping. Finally, the neural network computation module, used\nfor high-dimensional feature extraction of the multi-domain spatial feature\nmapping, after which the extracted high-dimensional features are used to\nenhance the time-domain feature association using LSTM, is computed by the\nblood pressure classifier to obtain the blood pressure measurement intervals.\nCombining the output of feature extraction and the result after classification,\nthe blood pressure calculator, calculates the blood pressure measurement\nvalues. The solution uses a blood pressure classifier to calculate blood\npressure intervals, which can help the neural network distinguish between the\nhigh-dimensional features of different blood pressure intervals and alleviate\nthe overfitting phenomenon. It can also locate the blood pressure intervals,\ncorrect the final blood pressure values and improve the network performance.\nExperimental results on two datasets show that the network outperforms existing\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_J/0/1/0/all/0/1\">Jialiang Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_X/0/1/0/all/0/1\">Xiujuan Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stepwise Feature Fusion: Local Guides Global. (arXiv:2203.03635v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03635","description":"<p>Colonoscopy, currently the most efficient and recognized colon polyp\ndetection technology, is necessary for early screening and prevention of\ncolorectal cancer. However, due to the varying size and complex morphological\nfeatures of colonic polyps as well as the indistinct boundary between polyps\nand mucosa, accurate segmentation of polyps is still challenging. Deep learning\nhas become popular for accurate polyp segmentation tasks with excellent\nresults. However, due to the structure of polyps image and the varying shapes\nof polyps, it easy for existing deep learning models to overfitting the current\ndataset. As a result, the model may not process unseen colonoscopy data. To\naddress this, we propose a new State-Of-The-Art model for medical image\nsegmentation, the SSFormer, which uses a pyramid Transformer encoder to improve\nthe generalization ability of models. Specifically, our proposed Progressive\nLocality Decoder can be adapted to the pyramid Transformer backbone to\nemphasize local features and restrict attention dispersion. The SSFormer\nachieves statet-of-the-art performance in both learning and generalization\nassessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinfeng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1\">Qiming Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_F/0/1/0/all/0/1\">Feilong Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_J/0/1/0/all/0/1\">Jia Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_J/0/1/0/all/0/1\">Jionglong Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1\">Sifan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering and classification of low-dimensional data in explicit feature map domain: intraoperative pixel-wise diagnosis of adenocarcinoma of a colon in a liver. (arXiv:2203.03636v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03636","description":"<p>Application of artificial intelligence in medicine brings in highly accurate\npredictions achieved by complex models, the reasoning of which is hard to\ninterpret. Their generalization ability can be reduced because of the lack of\npixel wise annotated images that occurs in frozen section tissue analysis. To\npartially overcome this gap, this paper explores the approximate explicit\nfeature map (aEFM) transform of low-dimensional data into a low-dimensional\nsubspace in Hilbert space. There, with a modest increase in computational\ncomplexity, linear algorithms yield improved performance and keep\ninterpretability. They remain amenable to incremental learning that is not a\ntrivial issue for some nonlinear algorithms. We demonstrate proposed\nmethodology on a very large-scale problem related to intraoperative pixel-wise\nsemantic segmentation and clustering of adenocarcinoma of a colon in a liver.\nCompared to the results in the input space, logistic classifier achieved\nstatistically significant performance improvements in micro balanced accuracy\nand F1 score in the amounts of 12.04% and 12.58%, respectively. Support vector\nmachine classifier yielded the increase of 8.04% and 9.41%. For clustering,\nincreases of 0.79% and 0.85% are obtained with ultra large-scale spectral\nclustering algorithm. Results are supported by a discussion of interpretability\nusing Shapely additive explanation values for predictions of linear classifier\nin input space and aEFM induced space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sitnik_D/0/1/0/all/0/1\">Dario Sitnik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kopriva_I/0/1/0/all/0/1\">Ivica Kopriva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image Registration Towards Enhancing Performance and Explainability in Cardiac And Brain Image Analysis. (arXiv:2203.03638v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03638","description":"<p>Magnetic Resonance Imaging (MRI) typically recruits multiple sequences\n(defined here as \"modalities\"). As each modality is designed to offer different\nanatomical and functional clinical information, there are evident disparities\nin the imaging content across modalities. Inter- and intra-modality affine and\nnon-rigid image registration is an essential medical image analysis process in\nclinical imaging, as for example before imaging biomarkers need to be derived\nand clinically evaluated across different MRI modalities, time phases and\nslices. Although commonly needed in real clinical scenarios, affine and\nnon-rigid image registration is not extensively investigated using a single\nunsupervised model architecture. In our work, we present an un-supervised deep\nlearning registration methodology which can accurately model affine and\nnon-rigid trans-formations, simultaneously. Moreover, inverse-consistency is a\nfundamental inter-modality registration property that is not considered in deep\nlearning registration algorithms. To address inverse-consistency, our\nmethodology performs bi-directional cross-modality image synthesis to learn\nmodality-invariant latent rep-resentations, while involves two factorised\ntransformation networks and an inverse-consistency loss to learn\ntopology-preserving anatomical transformations. Overall, our model (named\n\"FIRE\") shows improved performances against the reference standard baseline\nmethod on multi-modality brain 2D and 3D MRI and intra-modality cardiac 4D MRI\ndata experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chengjia Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papanastasiou_G/0/1/0/all/0/1\">Giorgos Papanastasiou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conquering Data Variations in Resolution: A Slice-Aware Multi-Branch Decoder Network. (arXiv:2203.03640v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03640","description":"<p>Fully convolutional neural networks have made promising progress in joint\nliver and liver tumor segmentation. Instead of following the debates over 2D\nversus 3D networks (for example, pursuing the balance between large-scale 2D\npretraining and 3D context), in this paper, we novelly identify the wide\nvariation in the ratio between intra- and inter-slice resolutions as a crucial\nobstacle to the performance. To tackle the mismatch between the intra- and\ninter-slice information, we propose a slice-aware 2.5D network that emphasizes\nextracting discriminative features utilizing not only in-plane semantics but\nalso out-of-plane coherence for each separate slice. Specifically, we present a\nslice-wise multi-input multi-output architecture to instantiate such a design\nparadigm, which contains a Multi-Branch Decoder (MD) with a Slice-centric\nAttention Block (SAB) for learning slice-specific features and a Densely\nConnected Dice (DCD) loss to regularize the inter-slice predictions to be\ncoherent and continuous. Based on the aforementioned innovations, we achieve\nstate-of-the-art results on the MICCAI 2017 Liver Tumor Segmentation (LiTS)\ndataset. Besides, we also test our model on the ISBI 2019 Segmentation of\nTHoracic Organs at Risk (SegTHOR) dataset, and the result proves the robustness\nand generalizability of the proposed method in other segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuxin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_S/0/1/0/all/0/1\">Shilei Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chai_Z/0/1/0/all/0/1\">Zhizhong Chai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation with Contrastive Learning for OCT Segmentation. (arXiv:2203.03664v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03664","description":"<p>Accurate segmentation of retinal fluids in 3D Optical Coherence Tomography\nimages is key for diagnosis and personalized treatment of eye diseases. While\ndeep learning has been successful at this task, trained supervised models often\nfail for images that do not resemble labeled examples, e.g. for images acquired\nusing different devices. We hereby propose a novel semi-supervised learning\nframework for segmentation of volumetric images from new unlabeled domains. We\njointly use supervised and contrastive learning, also introducing a contrastive\npairing scheme that leverages similarity between nearby slices in 3D. In\naddition, we propose channel-wise aggregation as an alternative to conventional\nspatial-pooling aggregation for contrastive feature map projection. We evaluate\nour methods for domain adaptation from a (labeled) source domain to an\n(unlabeled) target domain, each containing images acquired with different\nacquisition devices. In the target domain, our method achieves a Dice\ncoefficient 13.8% higher than SimCLR (a state-of-the-art contrastive\nframework), and leads to results comparable to an upper bound with supervised\ntraining in that domain. In the source domain, our model also improves the\nresults by 5.4% Dice, by successfully leveraging information from many\nunlabeled images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomariz_A/0/1/0/all/0/1\">Alvaro Gomariz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huanxiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Yvonna Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_T/0/1/0/all/0/1\">Thomas Albrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maunz_A/0/1/0/all/0/1\">Andreas Maunz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benmansour_F/0/1/0/all/0/1\">Fethallah Benmansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valcarcel_A/0/1/0/all/0/1\">Alessandra M.Valcarcel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_J/0/1/0/all/0/1\">Jennifer Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_D/0/1/0/all/0/1\">Daniela Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1\">Orcun Goksel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-centric and memory-guided normality reconstruction for video anomaly detection. (arXiv:2203.03677v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03677","description":"<p>This paper addresses video anomaly detection problem for videosurveillance.\nDue to the inherent rarity and heterogeneity of abnormal events, the problem is\nviewed as a normality modeling strategy, in which our model learns\nobject-centric normal patterns without seeing anomalous samples during\ntraining. The main contributions consist in coupling pretrained object-level\naction features prototypes with a cosine distance-based anomaly estimation\nfunction, therefore extending previous methods by introducing additional\nconstraints to the mainstream reconstruction-based strategy. Our framework\nleverages both appearance and motion information to learn object-level behavior\nand captures prototypical patterns within a memory module. Experiments on\nseveral well-known datasets demonstrate the effectiveness of our method as it\noutperforms current state-of-the-art on most relevant spatio-temporal\nevaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bergaoui_K/0/1/0/all/0/1\">Khalil Bergaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naji_Y/0/1/0/all/0/1\">Yassine Naji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Setkov_A/0/1/0/all/0/1\">Aleksandr Setkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loesch_A/0/1/0/all/0/1\">Ang&#xe9;lique Loesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouiffes_M/0/1/0/all/0/1\">Mich&#xe8;le Gouiff&#xe8;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audigier_R/0/1/0/all/0/1\">Romaric Audigier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers. (arXiv:2203.03682v1 [cs.RO])","link":"http://arxiv.org/abs/2203.03682","description":"<p>In this work, we consider the problem of learning a perception model for\nmonocular robot navigation using few annotated images. Using a Vision\nTransformer (ViT) pretrained with a label-free self-supervised method, we\nsuccessfully train a coarse image segmentation model for the Duckietown\nenvironment using 70 training images. Our model performs coarse image\nsegmentation at the 8x8 patch level, and the inference resolution can be\nadjusted to balance prediction granularity and real-time perception\nconstraints. We study how best to adapt a ViT to our task and environment, and\nfind that some lightweight architectures can yield good single-image\nsegmentations at a usable frame rate, even on CPU. The resulting perception\nmodel is used as the backbone for a simple yet robust visual servoing agent,\nwhich we deploy on a differential drive mobile robot to perform two tasks: lane\nfollowing and obstacle avoidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saavedra_Ruiz_M/0/1/0/all/0/1\">Miguel Saavedra-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morin_S/0/1/0/all/0/1\">Sacha Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1\">Liam Paull</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WaveMix: Resource-efficient Token Mixing for Images. (arXiv:2203.03689v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03689","description":"<p>Although certain vision transformer (ViT) and CNN architectures generalize\nwell on vision tasks, it is often impractical to use them on green, edge, or\ndesktop computing due to their computational requirements for training and even\ntesting. We present WaveMix as an alternative neural architecture that uses a\nmulti-scale 2D discrete wavelet transform (DWT) for spatial token mixing.\nUnlike ViTs, WaveMix neither unrolls the image nor requires self-attention of\nquadratic complexity. Additionally, DWT introduces another inductive bias --\nbesides convolutional filtering -- to utilize the 2D structure of an image to\nimprove generalization. The multi-scale nature of the DWT also reduces the\nrequirement for a deeper architecture compared to the CNNs, as the latter\nrelies on pooling for partial spatial mixing. WaveMix models show\ngeneralization that is competitive with ViTs, CNNs, and token mixers on several\ndatasets while requiring lower GPU RAM (training and testing), number of\ncomputations, and storage. WaveMix have achieved State-of-the-art (SOTA)\nresults in EMNIST Byclass and EMNIST Balanced datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1\">Pranav Jeevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1\">Amit Sethi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biometric recognition: why not massively adopted yet?. (arXiv:2203.03719v1 [cs.CY])","link":"http://arxiv.org/abs/2203.03719","description":"<p>Although there has been a dramatically reduction on the prices of capturing\ndevices and an increase on computing power in the last decade, it seems that\nbiometric systems are still far from massive adoption for civilian\napplications. This paper deals with the causes of this phenomenon, as well as\nsome misconceptions regarding biometric identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Barlow constrained optimization for Visual Question Answering. (arXiv:2203.03727v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03727","description":"<p>Visual question answering is a vision-and-language multimodal task, that aims\nat predicting answers given samples from the question and image modalities.\nMost recent methods focus on learning a good joint embedding space of images\nand questions, either by improving the interaction between these two\nmodalities, or by making it a more discriminant space. However, how informative\nthis joint space is, has not been well explored. In this paper, we propose a\nnovel regularization for VQA models, Constrained Optimization using Barlow's\ntheory (COB), that improves the information content of the joint space by\nminimizing the redundancy. It reduces the correlation between the learned\nfeature components and thereby disentangles semantic concepts. Our model also\naligns the joint space with the answer embedding space, where we consider the\nanswer and image+question as two different `views' of what in essence is the\nsame semantic information. We propose a constrained optimization policy to\nbalance the categorical and redundancy minimization forces. When built on the\nstate-of-the-art GGE model, the resulting model improves VQA accuracy by 1.4%\nand 4% on the VQA-CP v2 and VQA v2 datasets respectively. The model also\nexhibits better interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Abhishek Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patro_B/0/1/0/all/0/1\">Badri N. Patro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrowdFormer: Weakly-supervised Crowd counting with Improved Generalizability. (arXiv:2203.03768v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03768","description":"<p>Convolutional neural networks (CNNs) have dominated the field of computer\nvision for nearly a decade due to their strong ability to learn local features.\nHowever, due to their limited receptive field, CNNs fail to model the global\ncontext. On the other hand, transformer, an attention-based architecture can\nmodel the global context easily. Despite this, there are limited studies that\ninvestigate the effectiveness of transformers in crowd counting. In addition,\nthe majority of the existing crowd counting methods are based on the regression\nof density maps which requires point-level annotation of each person present in\nthe scene. This annotation task is laborious and also error-prone. This has led\nto increased focus on weakly-supervised crowd counting methods which require\nonly the count-level annotations. In this paper, we propose a weakly-supervised\nmethod for crowd counting using a pyramid vision transformer. We have conducted\nextensive evaluations to validate the effectiveness of the proposed method. Our\nmethod is comparable to the state-of-the-art on the benchmark crowd datasets.\nMore importantly, it shows remarkable generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savner_S/0/1/0/all/0/1\">Siddharth Singh Savner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanhangad_V/0/1/0/all/0/1\">Vivek Kanhangad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAMI-AD: An Activity Detector Exploiting Part-attention and Motion Information in Surveillance Videos. (arXiv:2203.03796v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03796","description":"<p>Activity detection in surveillance videos is a challenging task caused by\nsmall objects, complex activity categories, its untrimmed nature, etc. In this\nwork, we propose an effective activity detection system for person-only and\nvehicle-only activities in untrimmed surveillance videos, named PAMI-AD. It\nconsists of four modules, i.e., multi-object tracking, background modeling,\nactivity classifier and post-processing. In particular, we propose a novel\npart-attention mechanism for person-only activities and a simple but strong\nmotion information encoding method for vehicle-only activities. Our proposed\nsystem achieves the best results on the VIRAT dataset. Furthermore, our team\nwon the 1st place in the TRECVID 2021 ActEV challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yunhao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zhihang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Junfeng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanyun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unknown-Aware Object Detection: Learning What You Don't Know from Videos in the Wild. (arXiv:2203.03800v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03800","description":"<p>Building reliable object detectors that can detect out-of-distribution (OOD)\nobjects is critical yet underexplored. One of the key challenges is that models\nlack supervision signals from unknown data, producing overconfident predictions\non OOD objects. We propose a new unknown-aware object detection framework\nthrough Spatial-Temporal Unknown Distillation (STUD), which distills unknown\nobjects from videos in the wild and meaningfully regularizes the model's\ndecision boundary. STUD first identifies the unknown candidate object proposals\nin the spatial dimension, and then aggregates the candidates across multiple\nvideo frames to form a diverse set of unknown objects near the decision\nboundary. Alongside, we employ an energy-based uncertainty regularization loss,\nwhich contrastively shapes the uncertainty space between the in-distribution\nand distilled unknown objects. STUD establishes the state-of-the-art\nperformance on OOD detection tasks for object detection, reducing the FPR95\nscore by over 10% compared to the previous best method. Code is available at\nhttps://github.com/deeplearning-wisc/stud.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gozum_G/0/1/0/all/0/1\">Gabriel Gozum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoramic Human Activity Recognition. (arXiv:2203.03806v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03806","description":"<p>To obtain a more comprehensive activity understanding for a crowded scene, in\nthis paper, we propose a new problem of panoramic human activity recognition\n(PAR), which aims to simultaneous achieve the individual action, social group\nactivity, and global activity recognition. This is a challenging yet practical\nproblem in real-world applications. For this problem, we develop a novel\nhierarchical graph neural network to progressively represent and model the\nmulti-granularity human activities and mutual social relations for a crowd of\npeople. We further build a benchmark to evaluate the proposed method and other\nexisting related methods. Experimental results verify the rationality of the\nproposed PAR problem, the effectiveness of our method and the usefulness of the\nbenchmark. We will release the source code and benchmark to the public for\npromoting the study on this problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Ruize Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Haomin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Songmiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Search with Text Feedback by Additive Attention Compositional Learning. (arXiv:2203.03809v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03809","description":"<p>Effective image retrieval with text feedback stands to impact a range of\nreal-world applications, such as e-commerce. Given a source image and text\nfeedback that describes the desired modifications to that image, the goal is to\nretrieve the target images that resemble the source yet satisfy the given\nmodifications by composing a multi-modal (image-text) query. We propose a novel\nsolution to this problem, Additive Attention Compositional Learning (AACL),\nthat uses a multi-modal transformer-based architecture and effectively models\nthe image-text contexts. Specifically, we propose a novel image-text\ncomposition module based on additive attention that can be seamlessly plugged\ninto deep neural networks. We also introduce a new challenging benchmark\nderived from the Shopping100k dataset. AACL is evaluated on three large-scale\ndatasets (FashionIQ, Fashion200k, and Shopping100k), each with strong\nbaselines. Extensive experiments show that AACL achieves new state-of-the-art\nresults on all three datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuxin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newsam_S/0/1/0/all/0/1\">Shawn Newsam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boakye_K/0/1/0/all/0/1\">Kofi Boakye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating 3D Bio-Printable Patches Using Wound Segmentation and Reconstruction to Treat Diabetic Foot Ulcers. (arXiv:2203.03814v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03814","description":"<p>We introduce AiD Regen, a novel system that generates 3D wound models\ncombining 2D semantic segmentation with 3D reconstruction so that they can be\nprinted via 3D bio-printers during the surgery to treat diabetic foot ulcers\n(DFUs). AiD Regen seamlessly binds the full pipeline, which includes RGB-D\nimage capturing, semantic segmentation, boundary-guided point-cloud processing,\n3D model reconstruction, and 3D printable G-code generation, into a single\nsystem that can be used out of the box. We developed a multi-stage data\npreprocessing method to handle small and unbalanced DFU image datasets. AiD\nRegen's human-in-the-loop machine learning interface enables clinicians to not\nonly create 3D regenerative patches with just a few touch interactions but also\ncustomize and confirm wound boundaries. As evidenced by our experiments, our\nmodel outperforms prior wound segmentation models and our reconstruction\nalgorithm is capable of generating 3D wound models with compelling accuracy. We\nfurther conducted a case study on a real DFU patient and demonstrated the\neffectiveness of AiD Regen in treating DFU wounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chae_H/0/1/0/all/0/1\">Han Joo Chae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Seunghwan Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Son_H/0/1/0/all/0/1\">Hyewon Son</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1\">Seungyeob Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lim_T/0/1/0/all/0/1\">Taebin Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon. (arXiv:2203.03818v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03818","description":"<p>Estimating the risk level of adversarial examples is essential for safely\ndeploying machine learning models in the real world. One popular approach for\nphysical-world attacks is to adopt the \"sticker-pasting\" strategy, which\nhowever suffers from some limitations, including difficulties in access to the\ntarget or printing by valid colors. A new type of non-invasive attacks emerged\nrecently, which attempt to cast perturbation onto the target by optics based\ntools, such as laser beam and projector. However, the added optical patterns\nare artificial but not natural. Thus, they are still conspicuous and\nattention-grabbed, and can be easily noticed by humans. In this paper, we study\na new type of optical adversarial examples, in which the perturbations are\ngenerated by a very common natural phenomenon, shadow, to achieve naturalistic\nand stealthy physical-world adversarial attack under the black-box setting. We\nextensively evaluate the effectiveness of this new attack on both simulated and\nreal-world environments. Experimental results on traffic sign recognition\ndemonstrate that our algorithm can generate adversarial examples effectively,\nreaching 98.23% and 90.47% success rates on LISA and GTSRB test sets\nrespectively, while continuously misleading a moving camera over 95% of the\ntime in real-world scenarios. We also offer discussions about the limitations\nand the defense mechanism of this attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1\">Deming Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Table Structure Recognition with Conditional Attention. (arXiv:2203.03819v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03819","description":"<p>Tabular data in digital documents is widely used to express compact and\nimportant information for readers. However, it is challenging to parse tables\nfrom unstructured digital documents, such as PDFs and images, into\nmachine-readable format because of the complexity of table structures and the\nmissing of meta-information. Table Structure Recognition (TSR) problem aims to\nrecognize the structure of a table and transform the unstructured tables into a\nstructured and machine-readable format so that the tabular data can be further\nanalysed by the down-stream tasks, such as semantic modeling and information\nretrieval. In this study, we hypothesize that a complicated table structure can\nbe represented by a graph whose vertices and edges represent the cells and\nassociation between cells, respectively. Then we define the table structure\nrecognition problem as a cell association classification problem and propose a\nconditional attention network (CATT-Net). The experimental results demonstrate\nthe superiority of our proposed method over the state-of-the-art methods on\nvarious datasets. Besides, we investigate whether the alignment of a cell\nbounding box or a text-focused approach has more impact on the model\nperformance. Due to the lack of public dataset annotations based on these two\napproaches, we further annotate the ICDAR2013 dataset providing both types of\nbounding boxes, which can be a new benchmark dataset for evaluating the methods\nin this field. Experimental results show that the alignment of a cell bounding\nbox can help improve the Micro-averaged F1 score from 0.915 to 0.963, and the\nMacro-average F1 score from 0.787 to 0.923.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simsek_M/0/1/0/all/0/1\">Murat Simsek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantarci_B/0/1/0/all/0/1\">Burak Kantarci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkheir_A/0/1/0/all/0/1\">Ala Abu Alkheir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Vision Transformer. (arXiv:2203.03821v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03821","description":"<p>Vision Transformers (ViT) have made many breakthroughs in computer vision\ntasks. However, considerable redundancy arises in the spatial dimension of an\ninput image, leading to massive computational costs. Therefore, We propose a\ncoarse-to-fine vision transformer (CF-ViT) to relieve computational burden\nwhile retaining performance in this paper. Our proposed CF-ViT is motivated by\ntwo important observations in modern ViT models: (1) The coarse-grained patch\nsplitting can locate informative regions of an input image. (2) Most images can\nbe well recognized by a ViT model in a small-length token sequence. Therefore,\nour CF-ViT implements network inference in a two-stage manner. At coarse\ninference stage, an input image is split into a small-length patch sequence for\na computationally economical classification. If not well recognized, the\ninformative patches are identified and further re-split in a fine-grained\ngranularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For\nexample, without any compromise on performance, CF-ViT reduces 53% FLOPs of\nLV-ViT, and also achieves 2.01x throughput.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengzhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Rectangling for Image Stitching: A Learning Baseline. (arXiv:2203.03831v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03831","description":"<p>Stitched images provide a wide field-of-view (FoV) but suffer from unpleasant\nirregular boundaries. To deal with this problem, existing image rectangling\nmethods devote to searching an initial mesh and optimizing a target mesh to\nform the mesh deformation in two stages. Then rectangular images can be\ngenerated by warping stitched images. However, these solutions only work for\nimages with rich linear structures, leading to noticeable distortions for\nportraits and landscapes with non-linear objects. In this paper, we address\nthese issues by proposing the first deep learning solution to image\nrectangling. Concretely, we predefine a rigid target mesh and only estimate an\ninitial mesh to form the mesh deformation, contributing to a compact one-stage\nsolution. The initial mesh is predicted using a fully convolutional network\nwith a residual progressive regression strategy. To obtain results with high\ncontent fidelity, a comprehensive objective function is proposed to\nsimultaneously encourage the boundary rectangular, mesh shape-preserving, and\ncontent perceptually natural. Besides, we build the first image stitching\nrectangling dataset with a large diversity in irregular boundaries and scenes.\nExperiments demonstrate our superiority over traditional methods both\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quasi-Balanced Self-Training on Noise-Aware Synthesis of Object Point Clouds for Closing Domain Gap. (arXiv:2203.03833v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03833","description":"<p>Semantic analyses of object point clouds are largely driven by releasing of\nbenchmarking datasets, including synthetic ones whose instances are sampled\nfrom object CAD models. However, learning from synthetic data may not\ngeneralize to practical scenarios, where point clouds are typically incomplete,\nnon-uniformly distributed, and noisy. Such a challenge of Simulation-to-Real\n(Sim2Real) domain gap could be mitigated via learning algorithms of domain\nadaptation; however, we argue that generation of synthetic point clouds via\nmore physically realistic rendering is a powerful alternative, as systematic\nnon-uniform noise patterns can be captured. To this end, we propose an\nintegrated scheme consisting of physically realistic synthesis of object point\nclouds via rendering stereo images via projection of speckle patterns onto CAD\nmodels and a novel quasi-balanced self-training designed for more balanced data\ndistribution by sparsity-driven selection of pseudo labeled samples for long\ntailed classes. Experiment results can verify the effectiveness of our method\nas well as both of its modules for unsupervised domain adaptation on point\ncloud classification, achieving the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Longkun Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scale Self-Contrastive Learning with Hard Negative Mining for Weakly-Supervised Query-based Video Grounding. (arXiv:2203.03838v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03838","description":"<p>Query-based video grounding is an important yet challenging task in video\nunderstanding, which aims to localize the target segment in an untrimmed video\naccording to a sentence query. Most previous works achieve significant progress\nby addressing this task in a fully-supervised manner with segment-level labels,\nwhich require high labeling cost. Although some recent efforts develop\nweakly-supervised methods that only need the video-level knowledge, they\ngenerally match multiple pre-defined segment proposals with query and select\nthe best one, which lacks fine-grained frame-level details for distinguishing\nframes with high repeatability and similarity within the entire video. To\nalleviate the above limitations, we propose a self-contrastive learning\nframework to address the query-based video grounding task under a\nweakly-supervised setting. Firstly, instead of utilizing redundant segment\nproposals, we propose a new grounding scheme that learns frame-wise matching\nscores referring to the query semantic to predict the possible foreground\nframes by only using the video-level annotations. Secondly, since some\npredicted frames (i.e., boundary frames) are relatively coarse and exhibit\nsimilar appearance to their adjacent frames, we propose a coarse-to-fine\ncontrastive learning paradigm to learn more discriminative frame-wise\nrepresentations for distinguishing the false positive frames. In particular, we\niteratively explore multi-scale hard negative samples that are close to\npositive samples in the representation space for distinguishing fine-grained\nframe-wise details, thus enforcing more accurate segment grounding. Extensive\nexperiments on two challenging benchmarks demonstrate the superiority of our\nproposed method compared with the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Social Relation Representation for Human Group Detection. (arXiv:2203.03843v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03843","description":"<p>Human group detection, which splits crowd of people into groups, is an\nimportant step for video-based human social activity analysis. The core of\nhuman group detection is the human social relation representation and\ndivision.In this paper, we propose a new two-stage multi-head framework for\nhuman group detection. In the first stage, we propose a human behavior\nsimulator head to learn the social relation feature embedding, which is\nself-supervisely trained by leveraging the socially grounded multi-person\nbehavior relationship. In the second stage, based on the social relation\nembedding, we develop a self-attention inspired network for human group\ndetection. Remarkable performance on two state-of-the-art large-scale\nbenchmarks, i.e., PANDA and JRDB-Group, verifies the effectiveness of the\nproposed framework. Benefiting from the self-supervised social relation\nembedding, our method can provide promising results with very few (labeled)\ntraining data. We will release the source code to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Ruize Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Haomin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zekun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks. (arXiv:2203.03844v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03844","description":"<p>Light-weight super-resolution (SR) models have received considerable\nattention for their serviceability in mobile devices. Many efforts employ\nnetwork quantization to compress SR models. However, these methods suffer from\nsevere performance degradation when quantizing the SR models to ultra-low\nprecision (e.g., 2-bit and 3-bit) with the low-cost layer-wise quantizer. In\nthis paper, we identify that the performance drop comes from the contradiction\nbetween the layer-wise symmetric quantizer and the highly asymmetric activation\ndistribution in SR models. This discrepancy leads to either a waste on the\nquantization levels or detail loss in reconstructed images. Therefore, we\npropose a novel activation quantizer, referred to as Dynamic Dual Trainable\nBounds (DDTB), to accommodate the asymmetry of the activations. Specifically,\nDDTB innovates in: 1) A layer-wise quantizer with trainable upper and lower\nbounds to tackle the highly asymmetric activations. 2) A dynamic gate\ncontroller to adaptively adjust the upper and lower bounds at runtime to\novercome the drastically varying activation ranges over different samples.To\nreduce the extra overhead, the dynamic gate controller is quantized to 2-bit\nand applied to only part of the SR networks according to the introduced dynamic\nintensity. Extensive experiments demonstrate that our DDTB exhibits significant\nperformance improvements in ultra-low precision. For example, our DDTB achieves\na 0.70dB PSNR increase on Urban100 benchmark when quantizing EDSR to 2-bit and\nscaling up output images to x4. Code is at\n\\url{https://github.com/zysxmu/DDTB}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhong_Y/0/1/0/all/0/1\">Yunshan Zhong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xunchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where Does the Performance Improvement Come From? - A Reproducibility Concern about Image-Text Retrieval. (arXiv:2203.03853v1 [cs.IR])","link":"http://arxiv.org/abs/2203.03853","description":"<p>This paper seeks to provide the information retrieval community with some\nreflections on the current improvements of retrieval learning through the\nanalysis of the reproducibility aspects of image-text retrieval models. For the\nlatter part of the past decade, image-text retrieval has gradually become a\nmajor research direction in the field of information retrieval because of the\ngrowth of multi-modal data. Many researchers use benchmark datasets like\nMS-COCO and Flickr30k to train and assess the performance of image-text\nretrieval algorithms. Research in the past has mostly focused on performance,\nwith several state-of-the-art methods being proposed in various ways. According\nto their claims, these approaches achieve better modal interactions and thus\nbetter multimodal representations with greater precision. In contrast to those\nprevious works, we focus on the repeatability of the approaches and the overall\nexamination of the elements that lead to improved performance by pretrained and\nnonpretrained models in retrieving images and text. To be more specific, we\nfirst examine the related reproducibility concerns and why the focus is on\nimage-text retrieval tasks, and then we systematically summarize the current\nparadigm of image-text retrieval models and the stated contributions of those\napproaches. Second, we analyze various aspects of the reproduction of\npretrained and nonpretrained retrieval models. Based on this, we conducted\nablation experiments and obtained some influencing factors that affect\nretrieval recall more than the improvement claimed in the original paper.\nFinally, we also present some reflections and issues that should be considered\nby the retrieval community in the future. Our code is freely available at\nhttps://github.com/WangFei-2019/Image-text-Retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Shuhan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New 27 Class Sign Language Dataset Collected from 173 Individuals. (arXiv:2203.03859v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03859","description":"<p>After the interviews, it has been comprehended that speech-impaired\nindividuals who use sign languages have difficulty communicating with other\npeople who do not know sign language. Due to the communication problems, the\nsense of independence of speech-impaired individuals could be damaged and lead\nthem to socialize less with society. To contribute to the development of\ntechnologies, that can reduce the communication problems of speech-impaired\npersons, a new dataset was presented with this paper. The dataset was created\nby processing American Sign Language-based photographs collected from 173\nvolunteers, published as 27 Class Sign Language Dataset on the Kaggle Datasets\nweb page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mavi_A/0/1/0/all/0/1\">Arda Mavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dikle_Z/0/1/0/all/0/1\">Zeynep Dikle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Semantic Segmentation using Out-of-Distribution Data. (arXiv:2203.03860v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03860","description":"<p>Weakly supervised semantic segmentation (WSSS) methods are often built on\npixel-level localization maps obtained from a classifier. However, training on\nclass labels only, classifiers suffer from the spurious correlation between\nforeground and background cues (e.g. train and rail), fundamentally bounding\nthe performance of WSSS. There have been previous endeavors to address this\nissue with additional supervision. We propose a novel source of information to\ndistinguish foreground from the background: Out-of-Distribution (OoD) data, or\nimages devoid of foreground object classes. In particular, we utilize the hard\nOoDs that the classifier is likely to make false-positive predictions. These\nsamples typically carry key visual features on the background (e.g. rail) that\nthe classifiers often confuse as foreground (e.g. train), so these cues let\nclassifiers correctly suppress spurious background cues. Acquiring such hard\nOoDs does not require an extensive amount of annotation efforts; it only incurs\na few additional image-level labeling costs on top of the original efforts to\ncollect class labels. We propose a method, W-OoD, for utilizing the hard OoDs.\nW-OoD achieves state-of-the-art performance on Pascal VOC 2012.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungbeom Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seong Joon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Junsuk Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eunji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminability-Transferability Trade-Off: An Information-Theoretic Perspective. (arXiv:2203.03871v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03871","description":"<p>This work simultaneously considers the discriminability and transferability\nproperties of deep representations in the typical supervised learning task,\ni.e., image classification. By a comprehensive temporal analysis, we observe a\ntrade-off between these two properties. The discriminability keeps increasing\nwith the training progressing while the transferability intensely diminishes in\nthe later training period.\n</p>\n<p>From the perspective of information-bottleneck theory, we reveal that the\nincompatibility between discriminability and transferability is attributed to\nthe over-compression of input information. More importantly, we investigate why\nand how the InfoNCE loss can alleviate the over-compression, and further\npresent a learning framework, named contrastive temporal coding~(CTC), to\ncounteract the over-compression and alleviate the incompatibility. Extensive\nexperiments validate that CTC successfully mitigates the incompatibility,\nyielding discriminative and transferable representations. Noticeable\nimprovements are achieved on the image classification task and challenging\ntransfer learning tasks. We hope that this work will raise the significance of\nthe transferability property in the conventional supervised learning setting.\nCode will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Quan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhao-Min Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Borui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Renjie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiajun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Boyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshie_O/0/1/0/all/0/1\">Osamu Yoshie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual anomaly detection in video by variational autoencoder. (arXiv:2203.03872v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03872","description":"<p>Video anomalies detection is the intersection of anomaly detection and visual\nintelligence. It has commercial applications in surveillance, security,\nself-driving cars and crop monitoring. Videos can capture a variety of\nanomalies. Due to efforts needed to label training data, unsupervised\napproaches to train anomaly detection models for videos is more practical An\nautoencoder is a neural network that is trained to recreate its input using\nlatent representation of input also called a bottleneck layer. Variational\nautoencoder uses distribution (mean and variance) as compared to latent vector\nas bottleneck layer and can have better regularization effect. In this paper we\nhave demonstrated comparison between performance of convolutional LSTM versus a\nvariation convolutional LSTM autoencoder\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Waseem_F/0/1/0/all/0/1\">Faraz Waseem</a> (yahoo), <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_R/0/1/0/all/0/1\">Rafael Perez Martinez</a> (Stanford University), <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chris Wu</a> (Stanford University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Semantic Segmentation Using Unreliable Pseudo-Labels. (arXiv:2203.03884v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03884","description":"<p>The crux of semi-supervised semantic segmentation is to assign adequate\npseudo-labels to the pixels of unlabeled images. A common practice is to select\nthe highly confident predictions as the pseudo ground-truth, but it leads to a\nproblem that most pixels may be left unused due to their unreliability. We\nargue that every pixel matters to the model training, even its prediction is\nambiguous. Intuitively, an unreliable prediction may get confused among the top\nclasses (i.e., those with the highest probabilities), however, it should be\nconfident about the pixel not belonging to the remaining classes. Hence, such a\npixel can be convincingly treated as a negative sample to those most unlikely\ncategories. Based on this insight, we develop an effective pipeline to make\nsufficient use of unlabeled data. Concretely, we separate reliable and\nunreliable pixels via the entropy of predictions, push each unreliable pixel to\na category-wise queue that consists of negative samples, and manage to train\nthe model with all candidate pixels. Considering the training evolution, where\nthe prediction becomes more and more accurate, we adaptively adjust the\nthreshold for the reliable-unreliable partition. Experimental results on\nvarious benchmarks and training settings demonstrate the superiority of our\napproach over the state-of-the-art alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1\">Jingjing Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1\">Guoqiang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_X/0/1/0/all/0/1\">Xinyi Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Mask R-CNN Performance for Long, Thin Forensic Traces with Pre-Segmentation and IoU Region Merging. (arXiv:2203.03886v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03886","description":"<p>Mask R-CNN has recently achieved great success in the field of instance\nsegmentation. However, weaknesses of the algorithm have been repeatedly pointed\nout as well, especially in the segmentation of long, sparse objects whose\norientation is not exclusively horizontal or vertical. We present here an\napproach that significantly improves the performance of the algorithm by first\npre-segmenting the images with a PSPNet algorithm. To further improve its\nprediction, we have developed our own cost functions and heuristics in the form\nof training strategies, which can prevent so-called (early) overfitting and\nachieve a more targeted convergence. Furthermore, due to the high variance of\nthe images, especially for PSPNet, we aimed to develop strategies for a high\nrobustness and generalization, which are also presented here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zink_M/0/1/0/all/0/1\">Moritz Zink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_M/0/1/0/all/0/1\">Martin Schiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_P/0/1/0/all/0/1\">Pengcheng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasterstadt_S/0/1/0/all/0/1\">Stephan Gasterst&#xe4;dt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ART-Point: Improving Rotation Robustness of Point Cloud Classifiers via Adversarial Rotation. (arXiv:2203.03888v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03888","description":"<p>Point cloud classifiers with rotation robustness have been widely discussed\nin the 3D deep learning community. Most proposed methods either use rotation\ninvariant descriptors as inputs or try to design rotation equivariant networks.\nHowever, robust models generated by these methods have limited performance\nunder clean aligned datasets due to modifications on the original classifiers\nor input space. In this study, for the first time, we show that the rotation\nrobustness of point cloud classifiers can also be acquired via adversarial\ntraining with better performance on both rotated and clean datasets.\nSpecifically, our proposed framework named ART-Point regards the rotation of\nthe point cloud as an attack and improves rotation robustness by training the\nclassifier on inputs with Adversarial RoTations. We contribute an axis-wise\nrotation attack that uses back-propagated gradients of the pre-trained model to\neffectively find the adversarial rotations. To avoid model over-fitting on\nadversarial inputs, we construct rotation pools that leverage the\ntransferability of adversarial rotations among samples to increase the\ndiversity of training data. Moreover, we propose a fast one-step optimization\nto efficiently reach the final robust model. Experiments show that our proposed\nrotation attack achieves a high success rate and ART-Point can be used on most\nexisting classifiers to improve the rotation robustness while showing better\nperformance on clean datasets than state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Robin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClearPose: Large-scale Transparent Object Dataset and Benchmark. (arXiv:2203.03890v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03890","description":"<p>Transparent objects are ubiquitous in household settings and pose distinct\nchallenges for visual sensing and perception systems. The optical properties of\ntransparent objects leave conventional 3D sensors alone unreliable for object\ndepth and pose estimation. These challenges are highlighted by the shortage of\nlarge-scale RGB-Depth datasets focusing on transparent objects in real-world\nsettings. In this work, we contribute a large-scale real-world RGB-Depth\ntransparent object dataset named ClearPose to serve as a benchmark dataset for\nsegmentation, scene-level depth completion and object-centric pose estimation\ntasks. The ClearPose dataset contains over 350K labeled real-world RGB-Depth\nframes and 4M instance annotations covering 63 household objects. The dataset\nincludes object categories commonly used in daily life under various lighting\nand occluding conditions as well as challenging test scenarios such as cases of\nocclusion by opaque or translucent objects, non-planar orientations, presence\nof liquids, etc. We benchmark several state-of-the-art depth completion and\nobject pose estimation deep neural networks on ClearPose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaotong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zeren Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opipari_A/0/1/0/all/0/1\">Anthony Opipari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_O/0/1/0/all/0/1\">Odest Chadwicke Jenkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Mixup for Robust Fine-tuning. (arXiv:2203.03897v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03897","description":"<p>Pre-trained large-scale models provide a transferable embedding, and they\nshow comparable performance on the diverse downstream task. However, the\ntransferability of multi-modal learning is restricted, and the analysis of\nlearned embedding has not been explored well. This paper provides a perspective\nto understand the multi-modal embedding in terms of uniformity and alignment.\nWe newly find that the representation learned by multi-modal learning models\nsuch as CLIP has a two separated representation space for each heterogeneous\ndataset with less alignment. Besides, there are unexplored large intermediate\nareas between two modalities with less uniformity. Less robust embedding might\nrestrict the transferability of the representation for the downstream task.\nThis paper provides a new end-to-end fine-tuning method for robust\nrepresentation that encourages better uniformity and alignment score. First, we\npropose a multi-modal Mixup, $m^{2}$-Mix that mixes the representation of image\nand text to generate the hard negative samples. Second, we fine-tune the\nmulti-modal model on a hard negative sample as well as normal negative and\npositive samples with contrastive learning. Our multi-modal Mixup provides a\nrobust representation, and we validate our methods on classification,\nretrieval, and structure-awareness task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1\">Junhyuk So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1\">Changdae Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Minchul Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kyungwoo Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end system for object detection from sub-sampled radar data. (arXiv:2203.03905v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03905","description":"<p>Robust and accurate sensing is of critical importance for advancing\nautonomous automotive systems. The need to acquire situational awareness in\ncomplex urban conditions using sensors such as radar has motivated research on\npower and latency-efficient signal acquisition methods. In this paper, we\npresent an end-to-end signal processing pipeline, capable of operating in\nextreme weather conditions, that relies on sub-sampled radar data to perform\nobject detection in vehicular settings. The results of the object detection are\nfurther utilized to sub-sample forthcoming radar data, which stands in contrast\nto prior work where the sub-sampling relies on image information. We show\nrobust detection based on radar data reconstructed using 20% of samples under\nextreme weather conditions such as snow or fog, and on low-illuminated nights.\nAdditionally, we generate 20% sampled radar data in a fine-tuning set and show\n1.1% gain in AP50 across scenes and 3% AP50 gain in motorway condition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakthi_M/0/1/0/all/0/1\">Madhumitha Sakthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed Tewfik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arvinte_M/0/1/0/all/0/1\">Marius Arvinte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vikalo_H/0/1/0/all/0/1\">Haris Vikalo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Matters: A Weakly Supervised Pre-training Approach for Scene Text Detection and Spotting. (arXiv:2203.03911v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03911","description":"<p>Recently, Vision-Language Pre-training (VLP) techniques have greatly\nbenefited various vision-language tasks by jointly learning visual and textual\nrepresentations, which intuitively helps in Optical Character Recognition (OCR)\ntasks due to the rich visual and textual information in scene text images.\nHowever, these methods cannot well cope with OCR tasks because of the\ndifficulty in both instance-level text encoding and image-text pair acquisition\n(i.e. images and captured texts in them). This paper presents a weakly\nsupervised pre-training method that can acquire effective scene text\nrepresentations by jointly learning and aligning visual and textual\ninformation. Our network consists of an image encoder and a character-aware\ntext encoder that extract visual and textual features, respectively, as well as\na visual-textual decoder that models the interaction among textual and visual\nfeatures for learning effective scene text representations. With the learning\nof textual features, the pre-trained model can attend texts in images well with\ncharacter awareness. Besides, these designs enable the learning from weakly\nannotated texts (i.e. partial texts in images without text bounding boxes)\nwhich mitigates the data annotation constraint greatly. Experiments over the\nweakly annotated images in ICDAR2019-LSVT show that our pre-trained model\nimproves F-score by +2.5% and +4.8% while transferring its weights to other\ntext detection and spotting networks, respectively. In addition, the proposed\nmethod outperforms existing pre-training techniques consistently across\nmultiple public datasets (e.g., +3.2% and +1.3% for Total-Text and CTW1500).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Chuhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yu Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Globally-Optimal Event Camera Motion Estimation. (arXiv:2203.03914v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03914","description":"<p>Event cameras are bio-inspired sensors that perform well in HDR conditions\nand have high temporal resolution. However, different from traditional\nframe-based cameras, event cameras measure asynchronous pixel-level brightness\nchanges and return them in a highly discretised format, hence new algorithms\nare needed. The present paper looks at fronto-parallel motion estimation of an\nevent camera. The flow of the events is modeled by a general homographic\nwarping in a space-time volume, and the objective is formulated as a\nmaximisation of contrast within the image of unwarped events. However, in stark\ncontrast to prior art, we derive a globally optimal solution to this generally\nnon-convex problem, and thus remove the dependency on a good initial guess. Our\nalgorithm relies on branch-and-bound optimisation for which we derive novel,\nrecursive upper and lower bounds for six different contrast estimation\nfunctions. The practical validity of our approach is supported by a highly\nsuccessful application to AGV motion estimation with a downward facing event\ncamera, a challenging scenario in which the sensor experiences fronto-parallel\nmotion in front of noisy, fast moving textures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Ling Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Part-Aware Self-Supervised Pre-Training for Person Re-Identification. (arXiv:2203.03931v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03931","description":"<p>In person re-identification (ReID), very recent researches have validated\npre-training the models on unlabelled person images is much better than on\nImageNet. However, these researches directly apply the existing self-supervised\nlearning (SSL) methods designed for image classification to ReID without any\nadaption in the framework. These SSL methods match the outputs of local views\n(e.g., red T-shirt, blue shorts) to those of the global views at the same time,\nlosing lots of details. In this paper, we propose a ReID-specific pre-training\nmethod, Part-Aware Self-Supervised pre-training (PASS), which can generate\npart-level features to offer fine-grained information and is more suitable for\nReID. PASS divides the images into several local areas, and the local views\nrandomly cropped from each area are assigned with a specific learnable [PART]\ntoken. On the other hand, the [PART]s of all local areas are also appended to\nthe global views. PASS learns to match the output of the local views and global\nviews on the same [PART]. That is, the learned [PART] of the local views from a\nlocal area is only matched with the corresponding [PART] learned from the\nglobal views. As a result, each [PART] can focus on a specific local area of\nthe image and extracts fine-grained information of this area. Experiments show\nPASS sets the new state-of-the-art performances on Market1501 and MSMT17 on\nvarious ReID tasks, e.g., vanilla ViT-S/16 pre-trained by PASS achieves\n92.2\\%/90.2\\%/88.5\\% mAP accuracy on Market1501 for supervised/UDA/USL ReID.\nOur codes are available at https://github.com/CASIA-IVA-Lab/PASS-reID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haiyun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tianyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yousong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Ming Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention. (arXiv:2203.03937v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03937","description":"<p>Recently, Transformers have shown promising performance in various vision\ntasks. To reduce the quadratic computation complexity caused by each query\nattending to all keys/values, various methods have constrained the range of\nattention within local regions, where each query only attends to keys/values\nwithin a hand-crafted window. However, these hand-crafted window partition\nmechanisms are data-agnostic and ignore their input content, so it is likely\nthat one query maybe attends to irrelevant keys/values. To address this issue,\nwe propose a Dynamic Group Attention (DG-Attention), which dynamically divides\nall queries into multiple groups and selects the most relevant keys/values for\neach group. Our DG-Attention can flexibly model more relevant dependencies\nwithout any spatial constraint that is used in hand-crafted window based\nattention. Built on the DG-Attention, we develop a general vision transformer\nbackbone named Dynamic Group Transformer (DGT). Extensive experiments show that\nour models can outperform the state-of-the-art methods on multiple common\nvision tasks, including image classification, semantic segmentation, object\ndetection, and instance segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Online Semantic Mapping System for Extending and Enhancing Visual SLAM. (arXiv:2203.03944v1 [cs.RO])","link":"http://arxiv.org/abs/2203.03944","description":"<p>We present a real-time semantic mapping approach for mobile vision systems\nwith a 2D to 3D object detection pipeline and rapid data association for\ngenerated landmarks. Besides the semantic map enrichment the associated\ndetections are further introduced as semantic constraints into a simultaneous\nlocalization and mapping (SLAM) system for pose correction purposes. This way,\nwe are able generate additional meaningful information that allows to achieve\nhigher-level tasks, while simultaneously leveraging the view-invariance of\nobject detections to improve the accuracy and the robustness of the odometry\nestimation. We propose tracklets of locally associated object observations to\nhandle ambiguous and false predictions and an uncertainty-based greedy\nassociation scheme for an accelerated processing time. Our system reaches\nreal-time capabilities with an average iteration duration of 65~ms and is able\nto improve the pose estimation of a state-of-the-art SLAM by up to 68% on a\npublic dataset. Additionally, we implemented our approach as a modular ROS\npackage that makes it straightforward for integration in arbitrary graph-based\nSLAM methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hempel_T/0/1/0/all/0/1\">Thorsten Hempel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Hamadi_A/0/1/0/all/0/1\">Ayoub Al-Hamadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering. (arXiv:2203.03949v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03949","description":"<p>Finding accurate correspondences among different views is the Achilles' heel\nof unsupervised Multi-View Stereo (MVS). Existing methods are built upon the\nassumption that corresponding pixels share similar photometric features.\nHowever, multi-view images in real scenarios observe non-Lambertian surfaces\nand experience occlusions. In this work, we propose a novel approach with\nneural rendering (RC-MVSNet) to solve such ambiguity issues of correspondences\namong views. Specifically, we impose a depth rendering consistency loss to\nconstrain the geometry features close to the object surface to alleviate\nocclusions. Concurrently, we introduce a reference view synthesis loss to\ngenerate consistent supervision, even for non-Lambertian surfaces. Extensive\nexperiments on DTU and Tanks\\&amp;Temples benchmarks demonstrate that our RC-MVSNet\napproach achieves state-of-the-art performance over unsupervised MVS frameworks\nand competitive performance to many supervised methods.The trained models and\ncode will be released at https://github.com/Boese0601/RC-MVSNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Di Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozic_A/0/1/0/all/0/1\">Alja&#x17e; Bo&#x17e;i&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qingsong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingcong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1\">Sabine S&#xfc;sstrunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient and Accurate Hyperspectral Pansharpening Using 3D VolumeNet and 2.5D Texture Transfer. (arXiv:2203.03951v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03951","description":"<p>Recently, convolutional neural networks (CNN) have obtained promising results\nin single-image SR for hyperspectral pansharpening. However, enhancing CNNs'\nrepresentation ability with fewer parameters and a shorter prediction time is a\nchallenging and critical task. In this paper, we propose a novel multi-spectral\nimage fusion method using a combination of the previously proposed 3D CNN model\nVolumeNet and 2.5D texture transfer method using other modality high resolution\n(HR) images. Since a multi-spectral (MS) image consists of several bands and\neach band is a 2D image slice, MS images can be seen as 3D data. Thus, we use\nthe previously proposed VolumeNet to fuse HR panchromatic (PAN) images and\nbicubic interpolated MS images. Because the proposed 3D VolumeNet can\neffectively improve the accuracy by expanding the receptive field of the model,\nand due to its lightweight structure, we can achieve better performance against\nthe existing method without purchasing a large number of remote sensing images\nfor training. In addition, VolumeNet can restore the high-frequency information\nlost in the HR MR image as much as possible, reducing the difficulty of feature\nextraction in the following step: 2.5D texture transfer. As one of the latest\ntechnologies, deep learning-based texture transfer has been demonstrated to\neffectively and efficiently improve the visual performance and quality\nevaluation indicators of image reconstruction. Different from the texture\ntransfer processing of RGB image, we use HR PAN images as the reference images\nand perform texture transfer for each frequency band of MS images, which is\nnamed 2.5D texture transfer. The experimental results show that the proposed\nmethod outperforms the existing methods in terms of objective accuracy\nassessment, method efficiency, and visual subjective evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamoto_Y/0/1/0/all/0/1\">Yutaro Iwamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_R/0/1/0/all/0/1\">Ryousuke Nakamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1\">Ruofeng Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Wei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdgeFormer: Improving Light-weight ConvNets by Learning from Vision Transformers. (arXiv:2203.03952v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03952","description":"<p>Recently, vision transformers started to show impressive results which\noutperform large convolution based models significantly. However, in the area\nof small models for mobile or resource constrained devices, ConvNet still has\nits own advantages in both performance and model complexity. We propose\nEdgeFormer, a pure ConvNet based backbone model that further strengthens these\nadvantages by fusing the merits of vision transformers into ConvNets.\nSpecifically, we propose global circular convolution (GCC) with position\nembeddings, a light-weight convolution op which boasts a global receptive field\nwhile producing location sensitive features as in local convolutions. We\ncombine the GCCs and squeeze-exictation ops to form a meta-former like model\nblock, which further has the attention mechanism like transformers. The\naforementioned block can be used in plug-and-play manner to replace relevant\nblocks in ConvNets or transformers. Experiment results show that the proposed\nEdgeFormer achieves better performance than popular light-weight ConvNets and\nvision transformer based models in common vision tasks and datasets, while\nhaving fewer parameters and faster inference speed. For classification on\nImageNet-1k, EdgeFormer achieves 78.6% top-1 accuracy with about 5.0 million\nparameters, saving 11% parameters and 13% computational cost but gaining 0.2%\nhigher accuracy and 23% faster inference speed (on ARM based Rockchip RK3288)\ncompared with MobileViT, and uses only 0.5 times parameters but gaining 2.7%\naccuracy compared with DeIT. On MS-COCO object detection and PASCAL VOC\nsegmentation tasks, EdgeFormer also shows better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenze Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Cooperative Learning for Unsupervised Video Anomaly Detection. (arXiv:2203.03962v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03962","description":"<p>Video anomaly detection is well investigated in weakly-supervised and\none-class classification (OCC) settings. However, unsupervised video anomaly\ndetection methods are quite sparse, likely because anomalies are less frequent\nin occurrence and usually not well-defined, which when coupled with the absence\nof ground truth supervision, could adversely affect the performance of the\nlearning algorithms. This problem is challenging yet rewarding as it can\ncompletely eradicate the costs of obtaining laborious annotations and enable\nsuch systems to be deployed without human intervention. To this end, we propose\na novel unsupervised Generative Cooperative Learning (GCL) approach for video\nanomaly detection that exploits the low frequency of anomalies towards building\na cross-supervision between a generator and a discriminator. In essence, both\nnetworks get trained in a cooperative fashion, thereby allowing unsupervised\nlearning. We conduct extensive experiments on two large-scale video anomaly\ndetection datasets, UCF crime, and ShanghaiTech. Consistent improvement over\nthe existing state-of-the-art unsupervised and OCC methods corroborate the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Muhammad Zaigham Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1\">Arif Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Haris Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segu_M/0/1/0/all/0/1\">Mattia Segu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung-Ik Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GaitStrip: Gait Recognition via Effective Strip-based Feature Representations and Multi-Level Framework. (arXiv:2203.03966v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03966","description":"<p>Many gait recognition methods first partition the human gait into N-parts and\nthen combine them to establish part-based feature representations. Their gait\nrecognition performance is often affected by partitioning strategies, which are\nempirically chosen in different datasets. However, we observe that strips as\nthe basic component of parts are agnostic against different partitioning\nstrategies. Motivated by this observation, we present a strip-based multi-level\ngait recognition network, named GaitStrip, to extract comprehensive gait\ninformation at different levels. To be specific, our high-level branch explores\nthe context of gait sequences and our low-level one focuses on detailed posture\nchanges. We introduce a novel StriP-Based feature extractor (SPB) to learn the\nstrip-based feature representations by directly taking each strip of the human\nbody as the basic unit. Moreover, we propose a novel multi-branch structure,\ncalled Enhanced Convolution Module (ECM), to extract different representations\nof gaits. ECM consists of the Spatial-Temporal feature extractor (ST), the\nFrame-Level feature extractor (FL) and SPB, and has two obvious advantages:\nFirst, each branch focuses on a specific representation, which can be used to\nimprove the robustness of the network. Specifically, ST aims to extract\nspatial-temporal features of gait sequences, while FL is used to generate the\nfeature representation of each frame. Second, the parameters of the ECM can be\nreduced in test by introducing a structural re-parameterization technique.\nExtensive experimental results demonstrate that our GaitStrip achieves\nstate-of-the-art performance in both normal walking and complex conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Ming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Beibei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xianda Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lincheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiande Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shunli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Generalizing Beyond Domains in Cross-Domain Continual Learning. (arXiv:2203.03970v1 [cs.LG])","link":"http://arxiv.org/abs/2203.03970","description":"<p>Humans have the ability to accumulate knowledge of new tasks in varying\nconditions, but deep neural networks often suffer from catastrophic forgetting\nof previously learned knowledge after learning a new task. Many recent methods\nfocus on preventing catastrophic forgetting under the assumption of train and\ntest data following similar distributions. In this work, we consider a more\nrealistic scenario of continual learning under domain shifts where the model\nmust generalize its inference to an unseen domain. To this end, we encourage\nlearning semantically meaningful features by equipping the classifier with\nclass similarity metrics as learning parameters which are obtained through\nMahalanobis similarity computations. Learning of the backbone representation\nalong with these extra parameters is done seamlessly in an end-to-end manner.\nIn addition, we propose an approach based on the exponential moving average of\nthe parameters for better knowledge distillation. We demonstrate that, to a\ngreat extent, existing continual learning algorithms fail to handle the\nforgetting issue under multiple distributions, while our proposed approach\nlearns new tasks under domain shift with accuracy boosts up to 10% on\nchallenging datasets such as DomainNet and OfficeHome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simon_C/0/1/0/all/0/1\">Christian Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faraki_M/0/1/0/all/0/1\">Masoud Faraki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Hsuan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulter_S/0/1/0/all/0/1\">Samuel Schulter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_Y/0/1/0/all/0/1\">Yumin Suh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Prototype Transport for Zero-Shot Action Recognition and Localization. (arXiv:2203.03971v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03971","description":"<p>This work addresses the problem of recognizing action categories in videos\nfor which no training examples are available. The current state-of-the-art\nenables such a zero-shot recognition by learning universal mappings from videos\nto a shared semantic space, either trained on large-scale seen actions or on\nobjects. While effective, we find that universal action and object mappings are\nbiased to their seen categories. Such biases are further amplified due to\nbiases between seen and unseen categories in the semantic space. The\ncompounding biases result in many unseen action categories simply never being\nselected during inference, hampering zero-shot progress. We seek to address\nthis limitation and introduce universal prototype transport for zero-shot\naction recognition. The main idea is to re-position the semantic prototypes of\nunseen actions through transduction, i.e. by using the distribution of the\nunlabelled test set. For universal action models, we first seek to find a\nhyperspherical optimal transport mapping from unseen action prototypes to the\nset of all projected test videos. We then define a target prototype for each\nunseen action as the weighted Fr\\'echet mean over the transport couplings.\nEquipped with a target prototype, we propose to re-position unseen action\nprototypes along the geodesic spanned by the original and target prototypes,\nacting as a form of semantic regularization. For universal object models, we\noutline a variant that defines target prototypes based on an optimal transport\nbetween unseen action prototypes and semantic object prototypes. Empirically,\nwe show that universal prototype transport diminishes the biased selection of\nunseen action prototypes and boosts both universal action and object models,\nresulting in state-of-the-art performance for zero-shot classification and\nspatio-temporal localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GaitEdge: Beyond Plain End-to-end Gait Recognition for Better Practicality. (arXiv:2203.03972v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03972","description":"<p>Gait is one of the most promising biometrics to identify individuals at a\nlong distance. Although most previous methods have focused on recognizing the\nsilhouettes, several end-to-end methods that extract gait features directly\nfrom RGB images perform better. However, we argue that these end-to-end methods\ninevitably suffer from the gait-unrelated noises, i.e., low-level texture and\ncolorful information. Experimentally, we design both the cross-domain\nevaluation and visualization to stand for this view. In this work, we propose a\nnovel end-to-end framework named GaitEdge which can effectively block\ngait-unrelated information and release end-to-end training potential.\nSpecifically, GaitEdge synthesizes the output of the pedestrian segmentation\nnetwork and then feeds it to the subsequent recognition network, where the\nsynthetic silhouettes consist of trainable edges of bodies and fixed interiors\nto limit the information that the recognition network receives. Besides,\nGaitAlign for aligning silhouettes is embedded into the GaitEdge without loss\nof differentiability. Experimental results on CASIA-B and our newly built\nTTG-200 indicate that GaitEdge significantly outperforms the previous methods\nand provides a more practical end-to-end paradigm for blocking RGB noises\neffectively. All the source code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Junhao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Saihui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chuanfu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shiqi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Multiple Instance Learning with Gradient Accumulation. (arXiv:2203.03981v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03981","description":"<p>Being able to learn on weakly labeled data, and provide interpretability, are\ntwo of the main reasons why attention-based deep multiple instance learning\n(ABMIL) methods have become particularly popular for classification of\nhistopathological images. Such image data usually come in the form of\ngigapixel-sized whole-slide-images (WSI) that are cropped into smaller patches\n(instances). However, the sheer size of the data makes training of ABMIL models\nchallenging. All the instances from one WSI cannot be processed at once by\nconventional GPUs. Existing solutions compromise training by relying on\npre-trained models, strategic sampling or selection of instances, or\nself-supervised learning. We propose a training strategy based on gradient\naccumulation that enables direct end-to-end training of ABMIL models without\nbeing limited by GPU memory. We conduct experiments on both QMNIST and\nImagenette to investigate the performance and training time, and compare with\nthe conventional memory-expensive baseline and a recent sampled-based approach.\nThis memory-efficient approach, although slower, reaches performance\nindistinguishable from the memory-expensive baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Andersson_A/0/1/0/all/0/1\">Axel Andersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koriakina_N/0/1/0/all/0/1\">Nadezhda Koriakina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sladoje_N/0/1/0/all/0/1\">Nata&#x161;a Sladoje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindblad_J/0/1/0/all/0/1\">Joakim Lindblad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild. (arXiv:2203.03984v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03984","description":"<p>Talking face generation with great practical significance has attracted more\nattention in recent audio-visual studies. How to achieve accurate lip\nsynchronization is a long-standing challenge to be further investigated.\nMotivated by xxx, in this paper, an AttnWav2Lip model is proposed by\nincorporating spatial attention module and channel attention module into\nlip-syncing strategy. Rather than focusing on the unimportant regions of the\nface image, the proposed AttnWav2Lip model is able to pay more attention on the\nlip region reconstruction. To our limited knowledge, this is the first attempt\nto introduce attention mechanism to the scheme of talking face generation. An\nextensive experiments have been conducted to evaluate the effectiveness of the\nproposed model. Compared to the baseline measured by LSE-D and LSE-C metrics, a\nsuperior performance has been demonstrated on the benchmark lip synthesis\ndatasets, including LRW, LRS2 and LRS3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ganglai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1\">Yufei Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimpleTrack: Rethinking and Improving the JDE Approach for Multi-Object Tracking. (arXiv:2203.03985v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03985","description":"<p>Joint detection and embedding (JDE) based methods usually estimate bounding\nboxes and embedding features of objects with a single network in Multi-Object\nTracking (MOT). In the tracking stage, JDE-based methods fuse the target motion\ninformation and appearance information by applying the same rule, which could\nfail when the target is briefly lost or blocked. To overcome this problem, we\npropose a new association matrix, the Embedding and Giou matrix, which combines\nembedding cosine distance and Giou distance of objects. To further improve the\nperformance of data association, we develop a simple, effective tracker named\nSimpleTrack, which designs a bottom-up fusion method for Re-identity and\nproposes a new tracking strategy based on our EG matrix. The experimental\nresults indicate that SimpleTrack has powerful data association capability,\ne.g., 61.6 HOTA and 76.3 IDF1 on MOT17. In addition, we apply the EG matrix to\n5 different state-of-the-art JDE-based methods and achieve significant\nimprovements in IDF1, HOTA and IDsw metrics, and increase the tracking speed of\nthese methods by about 20%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hualiang Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skating-Mixer: Multimodal MLP for Scoring Figure Skating. (arXiv:2203.03990v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03990","description":"<p>Figure skating scoring is a challenging task because it requires judging\nplayers' technical moves as well as coordination with the background music.\nPrior learning-based work cannot solve it well for two reasons: 1) each move in\nfigure skating changes quickly, hence simply applying traditional frame\nsampling will lose a lot of valuable information, especially in a 3-5 minutes\nlasting video, so an extremely long-range representation learning is necessary;\n2) prior methods rarely considered the critical audio-visual relationship in\ntheir models. Thus, we introduce a multimodal MLP architecture, named\nSkating-Mixer. It extends the MLP-Mixer-based framework into a multimodal\nfashion and effectively learns long-term representations through our designed\nmemory recurrent unit (MRU). Aside from the model, we also collected a\nhigh-quality audio-visual FS1000 dataset, which contains over 1000 videos on 8\ntypes of programs with 7 different rating metrics, overtaking other datasets in\nboth quantity and diversity. Experiments show the proposed method outperforms\nSOTAs over all major metrics on the public Fis-V and our FS1000 dataset. In\naddition, we include an analysis applying our method to recent competitions\nthat occurred in Beijing 2022 Winter Olympic Games, proving our method has\nstrong robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jingfei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuge_M/0/1/0/all/0/1\">Mingchen Zhuge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1\">Tiantian Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yuantai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeltaCNN: End-to-End CNN Inference of Sparse Frame Differences in Videos. (arXiv:2203.03996v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03996","description":"<p>Convolutional neural network inference on video data requires powerful\nhardware for real-time processing. Given the inherent coherence across\nconsecutive frames, large parts of a video typically change little. By skipping\nidentical image regions and truncating insignificant pixel updates,\ncomputational redundancy can in theory be reduced significantly. However, these\ntheoretical savings have been difficult to translate into practice, as sparse\nupdates hamper computational consistency and memory access coherence; which are\nkey for efficiency on real hardware. With DeltaCNN, we present a sparse\nconvolutional neural network framework that enables sparse frame-by-frame\nupdates to accelerate video inference in practice. We provide sparse\nimplementations for all typical CNN layers and propagate sparse feature updates\nend-to-end - without accumulating errors over time. DeltaCNN is applicable to\nall convolutional neural networks without retraining. To the best of our\nknowledge, we are the first to significantly outperform the dense reference,\ncuDNN, in practical settings, achieving speedups of up to 7x with only marginal\ndifferences in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parger_M/0/1/0/all/0/1\">Mathias Parger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chengcheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twigg_C/0/1/0/all/0/1\">Christopher D. Twigg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keskin_C/0/1/0/all/0/1\">Cem Keskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Robert Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinberger_M/0/1/0/all/0/1\">Markus Steinberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration. (arXiv:2203.04006v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04006","description":"<p>Vision-language navigation (VLN) is a challenging task due to its large\nsearching space in the environment. To address this problem, previous works\nhave proposed some methods of fine-tuning a large model that pretrained on\nlarge-scale datasets. However, the conventional fine-tuning methods require\nextra human-labeled navigation data and lack self-exploration capabilities in\nenvironments, which hinders their generalization of unseen scenes. To improve\nthe ability of fast cross-domain adaptation, we propose Prompt-based\nEnvironmental Self-exploration (ProbES), which can self-explore the\nenvironments by sampling trajectories and automatically generates structured\ninstructions via a large-scale cross-modal pretrained model (CLIP). Our method\nfully utilizes the knowledge learned from CLIP to build an in-domain dataset by\nself-exploration without human labeling. Unlike the conventional approach of\nfine-tuning, we introduce prompt-based learning to achieve fast adaptation for\nlanguage embeddings, which substantially improves the learning efficiency by\nleveraging prior knowledge. By automatically synthesizing\ntrajectory-instruction pairs in any environment without human supervision and\nefficient prompt-based learning, our model can adapt to diverse vision-language\nnavigation tasks, including VLN and REVERIE. Both qualitative and quantitative\nresults show that our ProbES significantly improves the generalization ability\nof the navigation model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengda Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lingling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuMLP-Pin: A Dual-MLP-dot-product Permutation-invariant Network for Set Feature Extraction. (arXiv:2203.04007v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04007","description":"<p>Existing permutation-invariant methods can be divided into two categories\naccording to the aggregation scope, i.e. global aggregation and local one.\nAlthough the global aggregation methods, e. g., PointNet and Deep Sets, get\ninvolved in simpler structures, their performance is poorer than the local\naggregation ones like PointNet++ and Point Transformer. It remains an open\nproblem whether there exists a global aggregation method with a simple\nstructure, competitive performance, and even much fewer parameters. In this\npaper, we propose a novel global aggregation permutation-invariant network\nbased on dual MLP dot-product, called DuMLP-Pin, which is capable of being\nemployed to extract features for set inputs, including unordered or\nunstructured pixel, attribute, and point cloud data sets. We strictly prove\nthat any permutation-invariant function implemented by DuMLP-Pin can be\ndecomposed into two or more permutation-equivariant ones in a dot-product way\nas the cardinality of the given input set is greater than a threshold. We also\nshow that the DuMLP-Pin can be viewed as Deep Sets with strong constraints\nunder certain conditions. The performance of DuMLP-Pin is evaluated on several\ndifferent tasks with diverse data sets. The experimental results demonstrate\nthat our DuMLP-Pin achieves the best results on the two classification problems\nfor pixel sets and attribute sets. On both the point cloud classification and\nthe part segmentation, the accuracy of DuMLP-Pin is very close to the so-far\nbest-performing local aggregation method with only a 1-2% difference, while the\nnumber of required parameters is significantly reduced by more than 85% in\nclassification and 69% in segmentation, respectively. The code is publicly\navailable on https://github.com/JaronTHU/DuMLP-Pin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1\">Jiajun Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Ziyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenlei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhidong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Huanjun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evolutionary Neural Cascade Search across Supernetworks. (arXiv:2203.04011v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04011","description":"<p>To achieve excellent performance with modern neural networks, having the\nright network architecture is important. Neural Architecture Search (NAS)\nconcerns the automatic discovery of task-specific network architectures. Modern\nNAS approaches leverage supernetworks whose subnetworks encode candidate neural\nnetwork architectures. These subnetworks can be trained simultaneously,\nremoving the need to train each network from scratch, thereby increasing the\nefficiency of NAS. A recent method called Neural Architecture Transfer (NAT)\nfurther improves the efficiency of NAS for computer vision tasks by using a\nmulti-objective evolutionary algorithm to find high-quality subnetworks of a\nsupernetwork pretrained on ImageNet. Building upon NAT, we introduce ENCAS -\nEvolutionary Neural Cascade Search. ENCAS can be used to search over multiple\npretrained supernetworks to achieve a trade-off front of cascades of different\nneural network architectures, maximizing accuracy while minimizing FLOPS count.\nWe test ENCAS on common computer vision benchmarks (CIFAR-10, CIFAR-100,\nImageNet) and achieve Pareto dominance over previous state-of-the-art NAS\nmodels up to 1.5 GFLOPS. Additionally, applying ENCAS to a pool of 518 publicly\navailable ImageNet classifiers leads to Pareto dominance in all computation\nregimes and to increasing the maximum accuracy from 88.6% to 89.0%, accompanied\nby an 18\\% decrease in computation effort from 362 to 296 GFLOPS. Our code is\navailable at https://github.com/AwesomeLemon/ENCAS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chebykin_A/0/1/0/all/0/1\">Alexander Chebykin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alderliesten_T/0/1/0/all/0/1\">Tanja Alderliesten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosman_P/0/1/0/all/0/1\">Peter A. N. Bosman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Contrastive Learning to Disentangle Whole Slide Image Representations for Glioma Grading. (arXiv:2203.04013v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04013","description":"<p>Whole slide images (WSI) provide valuable phenotypic information for\nhistological assessment and malignancy grading of tumors. The WSI-based\ncomputational pathology promises to provide rapid diagnostic support and\nfacilitate digital health. The most commonly used WSI are derived from\nformalin-fixed paraffin-embedded (FFPE) and frozen sections. Currently, the\nmajority of automatic tumor grading models are developed based on FFPE\nsections, which could be affected by the artifacts introduced by tissue\nprocessing. Here we propose a mutual contrastive learning scheme to integrate\nFFPE and frozen sections and disentangle cross-modality representations for\nglioma grading. We first design a mutual learning scheme to jointly optimize\nthe model training based on FFPE and frozen sections. Further, we develop a\nmulti-modality domain alignment mechanism to ensure semantic consistency in the\nbackbone model training. We finally design a sphere normalized\ntemperature-scaled cross-entropy loss (NT-Xent), which could promote\ncross-modality representation disentangling of FFPE and frozen sections. Our\nexperiments show that the proposed scheme achieves better performance than the\nmodel trained based on each single modality or mixed modalities. The sphere\nNT-Xent loss outperforms other typical metrics loss functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lipei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1\">Yiran Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Price_S/0/1/0/all/0/1\">Stephen Price</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data augmentation with mixtures of max-entropy transformations for filling-level classification. (arXiv:2203.04027v1 [cs.LG])","link":"http://arxiv.org/abs/2203.04027","description":"<p>We address the problem of distribution shifts in test-time data with a\nprincipled data augmentation scheme for the task of content-level\nclassification. In such a task, properties such as shape or transparency of\ntest-time containers (cup or drinking glass) may differ from those represented\nin the training data. Dealing with such distribution shifts using standard\naugmentation schemes is challenging and transforming the training images to\ncover the properties of the test-time instances requires sophisticated image\nmanipulations. We therefore generate diverse augmentations using a family of\nmax-entropy transformations that create samples with new shapes, colors and\nspectral characteristics. We show that such a principled augmentation scheme,\nalone, can replace current approaches that use transfer learning or can be used\nin combination with transfer learning to improve its performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Modas_A/0/1/0/all/0/1\">Apostolos Modas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stage-Aware Feature Alignment Network for Real-Time Semantic Segmentation of Street Scenes. (arXiv:2203.04031v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04031","description":"<p>Over the past few years, deep convolutional neural network-based methods have\nmade great progress in semantic segmentation of street scenes. Some recent\nmethods align feature maps to alleviate the semantic gap between them and\nachieve high segmentation accuracy. However, they usually adopt the feature\nalignment modules with the same network configuration in the decoder and thus\nignore the different roles of stages of the decoder during feature aggregation,\nleading to a complex decoder structure. Such a manner greatly affects the\ninference speed. In this paper, we present a novel Stage-aware Feature\nAlignment Network (SFANet) based on the encoder-decoder structure for real-time\nsemantic segmentation of street scenes. Specifically, a Stage-aware Feature\nAlignment module (SFA) is proposed to align and aggregate two adjacent levels\nof feature maps effectively. In the SFA, by taking into account the unique role\nof each stage in the decoder, a novel stage-aware Feature Enhancement Block\n(FEB) is designed to enhance spatial details and contextual information of\nfeature maps from the encoder. In this way, we are able to address the\nmisalignment problem with a very simple and efficient multi-branch decoder\nstructure. Moreover, an auxiliary training strategy is developed to explicitly\nalleviate the multi-scale object problem without bringing additional\ncomputational costs during the inference phase. Experimental results show that\nthe proposed SFANet exhibits a good balance between accuracy and speed for\nreal-time semantic segmentation of street scenes. In particular, based on\nResNet-18, SFANet respectively obtains 78.1% and 74.7% mean of class-wise\nIntersection-over-Union (mIoU) at inference speeds of 37 FPS and 96 FPS on the\nchallenging Cityscapes and CamVid test datasets by using only a single GTX\n1080Ti GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1\">Xi Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanzi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pretrained StyleGAN. (arXiv:2203.04036v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04036","description":"<p>One-shot talking face generation aims at synthesizing a high-quality talking\nface video from an arbitrary portrait image, driven by a video or an audio\nsegment. One challenging quality factor is the resolution of the output video:\nhigher resolution conveys more details. In this work, we investigate the latent\nfeature space of a pre-trained StyleGAN and discover some excellent spatial\ntransformation properties. Upon the observation, we explore the possibility of\nusing a pre-trained StyleGAN to break through the resolution limit of training\ndatasets. We propose a novel unified framework based on a pre-trained StyleGAN\nthat enables a set of powerful functionalities, i.e., high-resolution video\ngeneration, disentangled control by driving video or audio, and flexible face\nediting. Our framework elevates the resolution of the synthesized talking face\nto 1024*1024 for the first time, even though the training dataset has a lower\nresolution. We design a video-based motion generation module and an audio-based\none, which can be plugged into the framework either individually or jointly to\ndrive the video generation. The predicted motion is used to transform the\nlatent features of StyleGAN for visual animation. To compensate for the\ntransformation distortion, we propose a calibration network as well as a domain\nloss to refine the features. Moreover, our framework allows two types of facial\nediting, i.e., global editing via GAN inversion and intuitive editing based on\n3D morphable models. Comprehensive experiments show superior video quality,\nflexible controllability, and editability over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Fei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cun_X/0/1/0/all/0/1\">Xiaodong Cun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Mingdeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Qingyan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Baoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Multi-Branch Aggregation Network for Real-Time Semantic Segmentation in Street Scenes. (arXiv:2203.04037v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04037","description":"<p>Real-time semantic segmentation, which aims to achieve high segmentation\naccuracy at real-time inference speed, has received substantial attention over\nthe past few years. However, many state-of-the-art real-time semantic\nsegmentation methods tend to sacrifice some spatial details or contextual\ninformation for fast inference, thus leading to degradation in segmentation\nquality. In this paper, we propose a novel Deep Multi-branch Aggregation\nNetwork (called DMA-Net) based on the encoder-decoder structure to perform\nreal-time semantic segmentation in street scenes. Specifically, we first adopt\nResNet-18 as the encoder to efficiently generate various levels of feature maps\nfrom different stages of convolutions. Then, we develop a Multi-branch\nAggregation Network (MAN) as the decoder to effectively aggregate different\nlevels of feature maps and capture the multi-scale information. In MAN, a\nlattice enhanced residual block is designed to enhance feature representations\nof the network by taking advantage of the lattice structure. Meanwhile, a\nfeature transformation block is introduced to explicitly transform the feature\nmap from the neighboring branch before feature aggregation. Moreover, a global\ncontext block is used to exploit the global contextual information. These key\ncomponents are tightly combined and jointly optimized in a unified network.\nExtensive experimental results on the challenging Cityscapes and CamVid\ndatasets demonstrate that our proposed DMA-Net respectively obtains 77.0% and\n73.6% mean Intersection over Union (mIoU) at the inference speed of 46.7 FPS\nand 119.8 FPS by only using a single NVIDIA GTX 1080Ti GPU. This shows that\nDMA-Net provides a good tradeoff between segmentation quality and speed for\nsemantic segmentation in street scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1\">Xi Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Genshun Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1\">Chang Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Biao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanzi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gait Recognition with Mask-based Regularization. (arXiv:2203.04038v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04038","description":"<p>Most gait recognition methods exploit spatial-temporal representations from\nstatic appearances and dynamic walking patterns. However, we observe that many\npart-based methods neglect representations at boundaries. In addition, the\nphenomenon of overfitting on training data is relatively common in gait\nrecognition, which is perhaps due to insufficient data and low-informative gait\nsilhouettes. Motivated by these observations, we propose a novel mask-based\nregularization method named ReverseMask. By injecting perturbation on the\nfeature map, the proposed regularization method helps convolutional\narchitecture learn the discriminative representations and enhances\ngeneralization. Also, we design an Inception-like ReverseMask Block, which has\nthree branches composed of a global branch, a feature dropping branch, and a\nfeature scaling branch. Precisely, the dropping branch can extract fine-grained\nrepresentations when partial activations are zero-outed. Meanwhile, the scaling\nbranch randomly scales the feature map, keeping structural information of\nactivations and preventing overfitting. The plug-and-play Inception-like\nReverseMask block is simple and effective to generalize networks, and it also\nimproves the performance of many state-of-the-art methods. Extensive\nexperiments demonstrate that the ReverseMask regularization help baseline\nachieves higher accuracy and better generalization. Moreover, the baseline with\nInception-like Block significantly outperforms state-of-the-art methods on the\ntwo most popular datasets, CASIA-B and OUMVLP. The source code will be\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chuanfu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Beibei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shunli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">George Q. Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shiqi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape-invariant 3D Adversarial Point Clouds. (arXiv:2203.04041v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04041","description":"<p>Adversary and invisibility are two fundamental but conflict characters of\nadversarial perturbations. Previous adversarial attacks on 3D point cloud\nrecognition have often been criticized for their noticeable point outliers,\nsince they just involve an \"implicit constrain\" like global distance loss in\nthe time-consuming optimization to limit the generated noise. While point cloud\nis a highly structured data format, it is hard to metric and constrain its\nperturbation with a simple loss properly. In this paper, we propose a novel\nPoint-Cloud Sensitivity Map to boost both the efficiency and imperceptibility\nof point perturbations. This map reveals the vulnerability of point cloud\nrecognition models when encountering shape-invariant adversarial noises. These\nnoises are designed along the shape surface with an \"explicit constrain\"\ninstead of extra distance loss. Specifically, we first apply a reversible\ncoordinate transformation on each point of the point cloud input, to reduce one\ndegree of point freedom and limit its movement on the tangent plane. Then we\ncalculate the best attacking direction with the gradients of the transformed\npoint cloud obtained on the white-box model. Finally we assign each point with\na non-negative score to construct the sensitivity map, which benefits both\nwhite-box adversarial invisibility and black-box query-efficiency extended in\nour work. Extensive evaluations prove that our method can achieve the superior\nperformance on various point cloud recognition models, with its satisfying\nadversarial imperceptibility and strong resistance to different point cloud\ndefense settings. Our code is available at: https://github.com/shikiw/SI-Adv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qidong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Erase the Bayer-Filter to See in the Dark. (arXiv:2203.04042v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04042","description":"<p>Low-light image enhancement - a pervasive but challenging problem, plays a\ncentral role in enhancing the visibility of an image captured in a poor\nillumination environment. Due to the fact that not all photons can pass the\nBayer-Filter on the sensor of the color camera, in this work, we first present\na De-Bayer-Filter simulator based on deep neural networks to generate a\nmonochrome raw image from the colored raw image. Next, a fully convolutional\nnetwork is proposed to achieve the low-light image enhancement by fusing\ncolored raw data with synthesized monochrome raw data. Channel-wise attention\nis also introduced to the fusion process to establish a complementary\ninteraction between features from colored and monochrome raw images. To train\nthe convolutional networks, we propose a dataset with monochrome and color raw\npairs named Mono-Colored Raw paired dataset (MCR) collected by using a\nmonochrome camera without Bayer-Filter and a color camera with Bayer-Filter.\nThe proposed pipeline take advantages of the fusion of the virtual monochrome\nand the color raw images and our extensive experiments indicate that\nsignificant improvement can be achieved by leveraging raw sensor data and\ndata-driven learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dong_X/0/1/0/all/0/1\">Xingbo Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1\">Wanyan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miao_Z/0/1/0/all/0/1\">Zhihui Miao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1\">Lan Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jiewen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_Z/0/1/0/all/0/1\">Zhe Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teoh_A/0/1/0/all/0/1\">Andrew Beng Jin Teoh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_J/0/1/0/all/0/1\">Jiajun Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Attention Transformer Network for Multi-Label Image Classification. (arXiv:2203.04049v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04049","description":"<p>Multi-label classification aims to recognize multiple objects or attributes\nfrom images. However, it is challenging to learn from proper label graphs to\neffectively characterize such inter-label correlations or dependencies. Current\nmethods often use the co-occurrence probability of labels based on the training\nset as the adjacency matrix to model this correlation, which is greatly limited\nby the dataset and affects the model's generalization ability. In this paper,\nwe propose a Graph Attention Transformer Network (GATN), a general framework\nfor multi-label image classification that can effectively mine complex\ninter-label relationships. First, we use the cosine similarity based on the\nlabel word embedding as the initial correlation matrix, which can represent\nrich semantic information. Subsequently, we design the graph attention\ntransformer layer to transfer this adjacency matrix to adapt to the current\ndomain. Our extensive experiments have demonstrated that our proposed methods\ncan achieve state-of-the-art performance on three datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shikai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongchao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xin Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jianping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rui_Y/0/1/0/all/0/1\">Yong Rui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVSegFormer: Bird's Eye View Semantic Segmentation From Arbitrary Camera Rigs. (arXiv:2203.04050v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04050","description":"<p>Semantic segmentation in bird's eye view (BEV) is an important task for\nautonomous driving. Though this task has attracted a large amount of research\nefforts, it is still challenging to flexibly cope with arbitrary (single or\nmultiple) camera sensors equipped on the autonomous vehicle. In this paper, we\npresent BEVSegFormer, an effective transformer-based method for BEV semantic\nsegmentation from arbitrary camera rigs. Specifically, our method first encodes\nimage features from arbitrary cameras with a shared backbone. These image\nfeatures are then enhanced by a deformable transformer-based encoder. Moreover,\nwe introduce a BEV transformer decoder module to parse BEV semantic\nsegmentation results. An efficient multi-camera deformable attention unit is\ndesigned to carry out the BEV-to-image view transformation. Finally, the\nqueries are reshaped according the layout of grids in the BEV, and upsampled to\nproduce the semantic segmentation result in a supervised manner. We evaluate\nthe proposed algorithm on the public nuScenes dataset and a self-collected\ndataset. Experimental results show that our method achieves promising\nperformance on BEV semantic segmentation from arbitrary camera rigs. We also\ndemonstrate the effectiveness of each component via ablation study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Lang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhirong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhangjie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Pengpeng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_E/0/1/0/all/0/1\">Erkang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counting with Adaptive Auxiliary Learning. (arXiv:2203.04061v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04061","description":"<p>This paper proposes an adaptive auxiliary task learning based approach for\nobject counting problems. Unlike existing auxiliary task learning based\nmethods, we develop an attention-enhanced adaptively shared backbone network to\nenable both task-shared and task-tailored features learning in an end-to-end\nmanner. The network seamlessly combines standard Convolution Neural Network\n(CNN) and Graph Convolution Network (GCN) for feature extraction and feature\nreasoning among different domains of tasks. Our approach gains enriched\ncontextual information by iteratively and hierarchically fusing the features\nacross different task branches of the adaptive CNN backbone. The whole\nframework pays special attention to the objects' spatial locations and varied\ndensity levels, informed by object (or crowd) segmentation and density level\nsegmentation auxiliary tasks. In particular, thanks to the proposed dilated\ncontrastive density loss function, our network benefits from individual and\nregional context supervision in terms of pixel-independent and pixel-dependent\nfeature learning mechanisms, along with strengthened robustness. Experiments on\nseven challenging multi-domain datasets demonstrate that our method achieves\nsuperior performance to the state-of-the-art auxiliary task learning based\ncounting methods. Our code is made publicly available at:\nhttps://github.com/smallmax00/Counting_With_Adaptive_Auxiliary\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yanda Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bridge_J/0/1/0/all/0/1\">Joshua Bridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Meng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yihong Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yalin Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing General-Purpose Deep-Learning Detection and Segmentation Models with Images from a Lidar as a Camera Sensor. (arXiv:2203.04064v1 [cs.RO])","link":"http://arxiv.org/abs/2203.04064","description":"<p>Over the last decade, robotic perception algorithms have significantly\nbenefited from the rapid advances in deep learning (DL). Indeed, a significant\namount of the autonomy stack of different commercial and research platforms\nrelies on DL for situational awareness, especially vision sensors. This work\nexplores the potential of general-purpose DL perception algorithms,\nspecifically detection and segmentation neural networks, for processing\nimage-like outputs of advanced lidar sensors. Rather than processing the\nthree-dimensional point cloud data, this is, to the best of our knowledge, the\nfirst work to focus on low-resolution images with 360\\textdegree field of view\nobtained with lidar sensors by encoding either depth, reflectivity, or\nnear-infrared light in the image pixels. We show that with adequate\npreprocessing, general-purpose DL models can process these images, opening the\ndoor to their usage in environmental conditions where vision sensors present\ninherent limitations. We provide both a qualitative and quantitative analysis\nof the performance of a variety of neural network architectures. We believe\nthat using DL models built for visual cameras offers significant advantages due\nto the much wider availability and maturity compared to point cloud-based\nperception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xianjia_Y/0/1/0/all/0/1\">Yu Xianjia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimpour_S/0/1/0/all/0/1\">Sahar Salimpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Queralta_J/0/1/0/all/0/1\">Jorge Pe&#xf1;a Queralta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westerlund_T/0/1/0/all/0/1\">Tomi Westerlund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lane Detection with Versatile AtrousFormer and Local Semantic Guidance. (arXiv:2203.04067v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04067","description":"<p>Lane detection is one of the core functions in autonomous driving and has\naroused widespread attention recently. The networks to segment lane instances,\nespecially with bad appearance, must be able to explore lane distribution\nproperties. Most existing methods tend to resort to CNN-based techniques. A few\nhave a try on incorporating the recent adorable, the seq2seq Transformer\n\\cite{transformer}. However, their innate drawbacks of weak global information\ncollection ability and exorbitant computation overhead prohibit a wide range of\nthe further applications. In this work, we propose Atrous Transformer\n(AtrousFormer) to solve the problem. Its variant local AtrousFormer is\ninterleaved into feature extractor to enhance extraction. Their collecting\ninformation first by rows and then by columns in a dedicated manner finally\nequips our network with stronger information gleaning ability and better\ncomputation efficiency. To further improve the performance, we also propose a\nlocal semantic guided decoder to delineate the identities and shapes of lanes\nmore accurately, in which the predicted Gaussian map of the starting point of\neach lane serves to guide the process. Extensive results on three challenging\nbenchmarks (CULane, TuSimple, and BDD100K) show that our network performs\nfavorably against the state of the arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2EC: An End-to-End Contour-based Method for High-Quality High-Speed Instance Segmentation. (arXiv:2203.04074v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04074","description":"<p>Contour-based instance segmentation methods have developed rapidly recently\nbut feature rough and hand-crafted front-end contour initialization, which\nrestricts the model performance, and an empirical and fixed backend\npredicted-label vertex pairing, which contributes to the learning difficulty.\nIn this paper, we introduce a novel contour-based method, named E2EC, for\nhigh-quality instance segmentation. Firstly, E2EC applies a novel learnable\ncontour initialization architecture instead of hand-crafted contour\ninitialization. This consists of a contour initialization module for\nconstructing more explicit learning goals and a global contour deformation\nmodule for taking advantage of all of the vertices' features better. Secondly,\nwe propose a novel label sampling scheme, named multi-direction alignment, to\nreduce the learning difficulty. Thirdly, to improve the quality of the boundary\ndetails, we dynamically match the most appropriate predicted-ground truth\nvertex pairs and propose the corresponding loss function named dynamic matching\nloss. The experiments showed that E2EC can achieve a state-of-the-art\nperformance on the KITTI INStance (KINS) dataset, the Semantic Boundaries\nDataset (SBD), the Cityscapes and the COCO dataset. E2EC is also efficient for\nuse in real-time applications, with an inference speed of 36 fps for 512*512\nimages on an NVIDIA A6000 GPU. Code will be released at\nhttps://github.com/zhang-tao-whu/e2ec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shiqing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shunping Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Distillation Guided Salient Object Detection. (arXiv:2203.04076v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04076","description":"<p>Most existing CNN-based salient object detection methods can identify local\nsegmentation details like hair and animal fur, but often misinterpret the real\nsaliency due to the lack of global contextual information caused by the\nsubjectiveness of the SOD task and the locality of convolution layers.\nMoreover, due to the unrealistically expensive labeling costs, the current\nexisting SOD datasets are insufficient to cover the real data distribution. The\nlimitation and bias of the training data add additional difficulty to fully\nexploring the semantic association between object-to-object and\nobject-to-environment in a given image. In this paper, we propose a semantic\ndistillation guided SOD (SDG-SOD) method that produces accurate results by\nfusing semantically distilled knowledge from generated image captioning into\nthe Vision-Transformer-based SOD framework. SDG-SOD can better uncover\ninter-objects and object-to-environment saliency and cover the gap between the\nsubjective nature of SOD and its expensive labeling. Comprehensive experiments\non five benchmark datasets demonstrate that the SDG-SOD outperforms the\nstate-of-the-art approaches on four evaluation metrics, and largely improves\nthe model performance on DUTS, ECSSD, DUT, HKU-IS, and PASCAL-S datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploration of Various Deep Learning Models for Increased Accuracy in Automatic Polyp Detection. (arXiv:2203.04093v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04093","description":"<p>This paper is created to explore deep learning models and algorithms that\nresults in highest accuracy in detecting polyp on colonoscopy images. Previous\nstudies implemented deep learning using convolution neural network (CNN)\nalgorithm in detecting polyp and non-polyp. Other studies used dropout, and\ndata augmentation algorithm but mostly not checking the overfitting, thus,\ninclude more than four-layer modelss. Rulei Yu et.al from the Institute of\nSoftware, Chinese Academy of Sciences said that transfer learning is better\ntalking about performance or improving the previous used algorithm. Most\nespecially in applying the transfer learning in feature extraction. Series of\nexperiments were conducted with only a minimum of 4 CNN layers applying\nprevious used models and identified the model that produce the highest\npercentage accuracy of 98% among the other models that apply transfer learning.\nFurther studies could use different optimizer to a different CNN modelsto\nincrease accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Isidro_A/0/1/0/all/0/1\">Ariel E. Isidro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fajardo_A/0/1/0/all/0/1\">Arnel C. Fajardo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hernandez_A/0/1/0/all/0/1\">Alexander A. Hernandez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Enhancement Using Latent Prototype for Few-Shot Segmentation. (arXiv:2203.04095v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04095","description":"<p>Few-shot segmentation enables the model to recognize unseen classes with few\nannotated examples. Most existing methods adopt prototype learning\narchitecture, where support prototype vectors are expanded and concatenated\nwith query features to perform conditional segmentation. However, such\nframework potentially focuses more on query features while may neglect the\nsimilarity between support and query features. This paper proposes a\ncontrastive enhancement approach using latent prototypes to leverage latent\nclasses and raise the utilization of similarity information between prototype\nand query features. Specifically, a latent prototype sampling module is\nproposed to generate pseudo-mask and novel prototypes based on features\nsimilarity. The module conveniently conducts end-to-end learning and has no\nstrong dependence on clustering numbers like cluster-based method. Besides, a\ncontrastive enhancement module is developed to drive models to provide\ndifferent predictions with the same query features. Our method can be used as\nan auxiliary module to flexibly integrate into other baselines for a better\nsegmentation performance. Extensive experiments show our approach remarkably\nimproves the performance of state-of-the-art methods for 1-shot and 5-shot\nsegmentation, especially outperforming baseline by 5.9% and 7.3% for 5-shot\ntask on Pascal-5^i and COCO-20^i. Source code is available at\nhttps://github.com/zhaoxiaoyu1995/CELP-Pytorch\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaohu Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer. (arXiv:2203.04099v1 [cs.SD])","link":"http://arxiv.org/abs/2203.04099","description":"<p>This paper presents an audio-visual approach for voice separation which\noutperforms state-of-the-art methods at a low latency in two scenarios: speech\nand singing voice. The model is based on a two-stage network. Motion cues are\nobtained with a lightweight graph convolutional network that processes face\nlandmarks. Then, both audio and motion features are fed to an audio-visual\ntransformer which produces a fairly good estimation of the isolated target\nsource. In a second stage, the predominant voice is enhanced with an audio-only\nnetwork. We present different ablation studies and comparison to\nstate-of-the-art methods. Finally, we explore the transferability of models\ntrained for speech separation in the task of singing voice separation. The\ndemos, code, and weights will be made publicly available at\nhttps://ipcv.github.io/VoViT/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Montesinos_J/0/1/0/all/0/1\">Juan F. Montesinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadandale_V/0/1/0/all/0/1\">Venkatesh S. Kadandale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haro_G/0/1/0/all/0/1\">Gloria Haro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing representations of biological data learned with different AI paradigms, augmenting and cropping strategies. (arXiv:2203.04107v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04107","description":"<p>Recent advances in computer vision and robotics enabled automated large-scale\nbiological image analysis. Various machine learning approaches have been\nsuccessfully applied to phenotypic profiling. However, it remains unclear how\nthey compare in terms of biological feature extraction. In this study, we\npropose a simple CNN architecture and implement 4 different representation\nlearning approaches. We train 16 deep learning setups on the 770k cancer cell\nimages dataset under identical conditions, using different augmenting and\ncropping strategies. We compare the learned representations by evaluating\nmultiple metrics for each of three downstream tasks: i) distance-based\nsimilarity analysis of known drugs, ii) classification of drugs versus\ncontrols, iii) clustering within cell lines. We also compare training times and\nmemory usage. Among all tested setups, multi-crops and random augmentations\ngenerally improved performance across tasks, as expected. Strikingly,\nself-supervised (implicit contrastive learning) models showed competitive\nperformance being up to 11 times faster to train. Self-supervised regularized\nlearning required the most of memory and computation to deliver arguably the\nmost informative features. We observe that no single combination of augmenting\nand cropping strategies consistently results in top performance across tasks\nand recommend prospective research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dmitrenko_A/0/1/0/all/0/1\">Andrei Dmitrenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masiero_M/0/1/0/all/0/1\">Mauro M. Masiero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zamboni_N/0/1/0/all/0/1\">Nicola Zamboni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Classifiers by Constructing Familiar Concepts. (arXiv:2203.04109v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04109","description":"<p>Interpreting a large number of neurons in deep learning is difficult. Our\nproposed `CLAssifier-DECoder' architecture (ClaDec) facilitates the\nunderstanding of the output of an arbitrary layer of neurons or subsets\nthereof. It uses a decoder that transforms the incomprehensible representation\nof the given neurons to a representation that is more similar to the domain a\nhuman is familiar with. In an image recognition problem, one can recognize what\ninformation (or concepts) a layer maintains by contrasting reconstructed images\nof ClaDec with those of a conventional auto-encoder(AE) serving as reference.\nAn extension of ClaDec allows trading comprehensibility and fidelity. We\nevaluate our approach for image classification using convolutional neural\nnetworks. We show that reconstructed visualizations using encodings from a\nclassifier capture more relevant classification information than conventional\nAEs. This holds although AEs contain more information on the original input.\nOur user study highlights that even non-experts can identify a diverse set of\nconcepts contained in images that are relevant (or irrelevant) for the\nclassifier. We also compare against saliency based methods that focus on pixel\nrelevance rather than concepts. We show that ClaDec tends to highlight more\nrelevant input areas to classification though outcomes depend on classifier\narchitecture. Code is at \\url{https://github.com/JohnTailor/ClaDec}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Johannes Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_M/0/1/0/all/0/1\">Michail Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantification of Occlusion Handling Capability of a 3D Human Pose Estimation Framework. (arXiv:2203.04113v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04113","description":"<p>3D human pose estimation using monocular images is an important yet\nchallenging task. Existing 3D pose detection methods exhibit excellent\nperformance under normal conditions however their performance may degrade due\nto occlusion. Recently some occlusion aware methods have also been proposed,\nhowever, the occlusion handling capability of these networks has not yet been\nthoroughly investigated. In the current work, we propose an occlusion-guided 3D\nhuman pose estimation framework and quantify its occlusion handling capability\nby using different protocols. The proposed method estimates more accurate 3D\nhuman poses using 2D skeletons with missing joints as input. Missing joints are\nhandled by introducing occlusion guidance that provides extra information about\nthe absence or presence of a joint. Temporal information has also been\nexploited to better estimate the missing joints. A large number of experiments\nare performed for the quantification of occlusion handling capability of the\nproposed method on three publicly available datasets in various settings\nincluding random missing joints, fixed body parts missing, and complete frames\nmissing, using mean per joint position error criterion. In addition to that,\nthe quality of the predicted 3D poses is also evaluated using action\nclassification performance as a criterion. 3D poses estimated by the proposed\nmethod achieved significantly improved action recognition performance in the\npresence of missing joints. Our experiments demonstrate the effectiveness of\nthe proposed framework for handling the missing joints as well as\nquantification of the occlusion handling capability of the deep neural\nnetworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghafoor_M/0/1/0/all/0/1\">Mehwish Ghafoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1\">Arif Mahmood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A study on joint modeling and data augmentation of multi-modalities for audio-visual scene classification. (arXiv:2203.04114v1 [cs.MM])","link":"http://arxiv.org/abs/2203.04114","description":"<p>In this paper, we propose two techniques, namely joint modeling and data\naugmentation, to improve system performances for audio-visual scene\nclassification (AVSC). We employ pre-trained networks trained only on image\ndata sets to extract video embedding; whereas for audio embedding models, we\ndecide to train them from scratch. We explore different neural network\narchitectures for joint modeling to effectively combine the video and audio\nmodalities. Moreover, data augmentation strategies are investigated to increase\naudio-visual training set size. For the video modality the effectiveness of\nseveral operations in RandAugment is verified. An audio-video joint mixup\nscheme is proposed to further improve AVSC performances. Evaluated on the\ndevelopment set of TAU Urban Audio Visual Scenes 2021, our final system can\nachieve the best accuracy of 94.2% among all single AVSC systems submitted to\nDCASE 2021 Task 1b.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yajian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuzhong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1\">Sabato Marco Siniscalchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chin-Hui Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Polyp Segmentation Network. (arXiv:2203.04118v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04118","description":"<p>Cancer is a disease that occurs as a result of uncontrolled division and\nproliferation of cells. The number of cancer cases has been on the rise over\nthe recent years.. Colon cancer is one of the most common types of cancer in\nthe world. Polyps that can be seen in the large intestine can cause cancer if\nnot removed with early intervention. Deep learning and image segmentation\ntechniques are used to minimize the number of polyps that goes unnoticed by the\nexperts during the diagnosis. Although these techniques give good results, they\nrequire too many parameters. We propose a new model to solve this problem. Our\nproposed model includes less parameters as well as outperforming the success of\nthe state of the art models. In the proposed model, a partial decoder is used\nto reduce the number of parameters while maintaning success. EfficientNetB0,\nwhich gives successfull results as well as requiring few parameters, is used in\nthe encoder part. Since polyps have variable aspect and aspect ratios, an\nasymetric convolution block was used instead of using classic convolution\nblock. Kvasir and CVC-ClinicDB datasets were seperated as training, validation\nand testing, and CVC-ColonDB, ETIS and Endoscene datasets were used for\ntesting. According to the dice metric, our model had the best results with\n%71.8 in the ColonDB test dataset, %89.3 in the EndoScene test dataset and\n%74.8 in the ETIS test dataset. Our model requires a total of 2.626.337\nparameters. When we compare it in the literature, according to similar studies,\nthe model that requires the least parameters is U-Net++ with 9.042.177\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Erol_T/0/1/0/all/0/1\">Tugberk Erol</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarikaya_D/0/1/0/all/0/1\">Duygu Sarikaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment. (arXiv:2203.04121v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04121","description":"<p>Training a generative adversarial network (GAN) with limited data has been a\nchallenging task. A feasible solution is to start with a GAN well-trained on a\nlarge scale source domain and adapt it to the target domain with a few samples,\ntermed as few shot generative model adaption. However, existing methods are\nprone to model overfitting and collapse in extremely few shot setting (less\nthan 10). To solve this problem, we propose a relaxed spatial structural\nalignment method to calibrate the target generative models during the adaption.\nWe design a cross-domain spatial structural consistency loss comprising the\nself-correlation and disturbance correlation consistency loss. It helps align\nthe spatial structural information between the synthesis image pairs of the\nsource and target domains. To relax the cross-domain alignment, we compress the\noriginal latent space of generative models to a subspace. Image pairs generated\nfrom the subspace are pulled closer. Qualitative and quantitative experiments\nshow that our method consistently surpasses the state-of-the-art methods in few\nshot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jiayu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YouTube-GDD: A challenging gun detection dataset with rich contextual information. (arXiv:2203.04129v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04129","description":"<p>An automatic gun detection system can detect potential gun-related violence\nat an early stage that is of paramount importance for citizens security. In the\nwhole system, object detection algorithm is the key to perceive the environment\nso that the system can detect dangerous objects such as pistols and rifles.\nHowever, mainstream deep learning-based object detection algorithms depend\nheavily on large-scale high-quality annotated samples, and the existing gun\ndatasets are characterized by low resolution, little contextual information and\nlittle data volume. To promote the development of security, this work presents\na new challenging dataset called YouTube Gun Detection Dataset (YouTube-GDD).\nOur dataset is collected from 343 high-definition YouTube videos and contains\n5000 well-chosen images, in which 16064 instances of gun and 9046 instances of\nperson are annotated. Compared to other datasets, YouTube-GDD is \"dynamic\",\ncontaining rich contextual information and recording shape changes of the gun\nduring shooting. To build a baseline for gun detection, we evaluate YOLOv5 on\nYouTube-GDD and analyze the influence of additional related annotated\ninformation on gun detection. YouTube-GDD and subsequent updates will be\nreleased at https://github.com/UCAS-GYX/YouTube-GDD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yongxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xingbin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaolin Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeReF: Neural Refractive Field for Fluid Surface Reconstruction and Implicit Representation. (arXiv:2203.04130v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04130","description":"<p>Existing neural reconstruction schemes such as Neural Radiance Field (NeRF)\nare largely focused on modeling opaque objects. We present a novel neural\nrefractive field(NeReF) to recover wavefront of transparent fluids by\nsimultaneously estimating the surface position and normal of the fluid front.\nUnlike prior arts that treat the reconstruction target as a single layer of the\nsurface, NeReF is specifically formulated to recover a volumetric normal field\nwith its corresponding density field. A query ray will be refracted by NeReF\naccording to its accumulated refractive point and normal, and we employ the\ncorrespondences and uniqueness of refracted ray for NeReF optimization. We show\nNeReF, as a global optimization scheme, can more robustly tackle refraction\ndistortions detrimental to traditional methods for correspondence matching.\nFurthermore, the continuous NeReF representation of wavefront enables view\nsynthesis as well as normal integration. We validate our approach on both\nsynthetic and real data and show it is particularly suitable for sparse\nmulti-view acquisition. We hence build a small light field array and experiment\non various surface shapes to demonstrate high fidelity NeReF reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Junming Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Junqing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motron: Multimodal Probabilistic Human Motion Forecasting. (arXiv:2203.04132v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04132","description":"<p>Autonomous systems and humans are increasingly sharing the same space. Robots\nwork side by side or even hand in hand with humans to balance each other's\nlimitations. Such cooperative interactions are ever more sophisticated. Thus,\nthe ability to reason not just about a human's center of gravity position, but\nalso its granular motion is an important prerequisite for human-robot\ninteraction. Though, many algorithms ignore the multimodal nature of humans or\nneglect uncertainty in their motion forecasts. We present Motron, a multimodal,\nprobabilistic, graph-structured model, that captures human's multimodality\nusing probabilistic methods while being able to output deterministic motions\nand corresponding confidence values for each mode. Our model aims to be tightly\nintegrated with the robotic planning-control-interaction loop; outputting\nphysically feasible human motions and being computationally efficient. We\ndemonstrate the performance of our model on several challenging real-world\nmotion forecasting datasets, outperforming a wide array of generative methods\nwhile providing state-of-the-art deterministic motions if required. Both using\nsignificantly less computational power than state-of-the art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_T/0/1/0/all/0/1\">Tim Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryll_M/0/1/0/all/0/1\">Markus Ryll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Easy Ensemble: Simple Deep Ensemble Learning for Sensor-Based Human Activity Recognition. (arXiv:2203.04153v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04153","description":"<p>Sensor-based human activity recognition (HAR) is a paramount technology in\nthe Internet of Things services. HAR using representation learning, which\nautomatically learns a feature representation from raw data, is the mainstream\nmethod because it is difficult to interpret relevant information from raw\nsensor data to design meaningful features. Ensemble learning is a robust\napproach to improve generalization performance; however, deep ensemble learning\nrequires various procedures, such as data partitioning and training multiple\nmodels, which are time-consuming and computationally expensive. In this study,\nwe propose Easy Ensemble (EE) for HAR, which enables the easy implementation of\ndeep ensemble learning in a single model. In addition, we propose input masking\nas a method for diversifying the input for EE. Experiments on a benchmark\ndataset for HAR demonstrated the effectiveness of EE and input masking and\ntheir characteristics compared with conventional ensemble learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_T/0/1/0/all/0/1\">Tatsuhito Hasegawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondo_K/0/1/0/all/0/1\">Kazuma Kondo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Local Preserving and Global Aligning Network for Adversarial Domain Adaptation. (arXiv:2203.04156v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04156","description":"<p>Unsupervised domain adaptation (UDA) requires source domain samples with\nclean ground truth labels during training. Accurately labeling a large number\nof source domain samples is time-consuming and laborious. An alternative is to\nutilize samples with noisy labels for training. However, training with noisy\nlabels can greatly reduce the performance of UDA. In this paper, we address the\nproblem that learning UDA models only with access to noisy labels and propose a\nnovel method called robust local preserving and global aligning network\n(RLPGA). RLPGA improves the robustness of the label noise from two aspects. One\nis learning a classifier by a robust informative-theoretic-based loss function.\nThe other is constructing two adjacency weight matrices and two negative weight\nmatrices by the proposed local preserving module to preserve the local topology\nstructures of input data. We conduct theoretical analysis on the robustness of\nthe proposed RLPGA and prove that the robust informative-theoretic-based loss\nand the local preserving module are beneficial to reduce the empirical risk of\nthe target domain. A series of empirical studies show the effectiveness of our\nproposed RLPGA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1\">Wenwen Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangmeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changwen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding person identification via gait. (arXiv:2203.04179v1 [cs.CR])","link":"http://arxiv.org/abs/2203.04179","description":"<p>Gait recognition is the process of identifying humans from their bipedal\nlocomotion such as walking or running. As such gait data is privacy sensitive\ninformation and should be anonymized. With the rise of more and higher quality\ngait recording techniques, such as depth cameras or motion capture suits, an\nincreasing amount of high-quality gait data becomes available which requires\nanonymization. As a first step towards developing anonymization techniques for\nhigh-quality gait data, we study different aspects of movement data to quantify\ntheir contribution to the gait recognition process. We first extract categories\nof features from the literature on human gait perception and then design\ncomputational experiments for each of the categories which we run against a\ngait recognition system. Our results show that gait anonymization is a\nchallenging process as the data is highly redundant and interdependent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanisch_S/0/1/0/all/0/1\">Simon Hanisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muschter_E/0/1/0/all/0/1\">Evelyn Muschter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzipanagioti_A/0/1/0/all/0/1\">Adamantini Chatzipanagioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu-Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strufe_T/0/1/0/all/0/1\">Thorsten Strufe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tuning-free multi-coil compressed sensing MRI with Parallel Variable Density Approximate Message Passing (P-VDAMP). (arXiv:2203.04180v1 [math.NA])","link":"http://arxiv.org/abs/2203.04180","description":"<p>Purpose: To develop a tuning-free method for multi-coil compressed sensing\nMRI that performs competitively with algorithms with an optimally tuned sparse\nparameter.\n</p>\n<p>Theory: The Parallel Variable Density Approximate Message Passing (P-VDAMP)\nalgorithm is proposed. For Bernoulli random variable density sampling, P-VDAMP\nobeys a \"state evolution\", where the intermediate per-iteration image estimate\nis distributed according to the ground truth corrupted by a Gaussian vector\nwith approximately known covariance. State evolution is leveraged to\nautomatically tune sparse parameters on-the-fly with Stein's Unbiased Risk\nEstimate (SURE).\n</p>\n<p>Methods: P-VDAMP is evaluated on brain, knee and angiogram datasets at\nacceleration factors 5 and 10 and compared with four variants of the Fast\nIterative Shrinkage-Thresholding algorithm (FISTA), including two tuning-free\nvariants from the literature.\n</p>\n<p>Results: The proposed method is found to have a similar reconstruction\nquality and time to convergence as FISTA with an optimally tuned sparse\nweighting.\n</p>\n<p>Conclusions: P-VDAMP is an efficient, robust and principled method for\non-the-fly parameter tuning that is competitive with optimally tuned FISTA and\noffers substantial robustness and reconstruction quality improvements over\ncompeting tuning-free methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Millard_C/0/1/0/all/0/1\">Charles Millard</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chiew_M/0/1/0/all/0/1\">Mark Chiew</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tanner_J/0/1/0/all/0/1\">Jared Tanner</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hess_A/0/1/0/all/0/1\">Aaron T. Hess</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mailhe_B/0/1/0/all/0/1\">Boris Mailhe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective-Supervised Contrastive Learning with Noisy Labels. (arXiv:2203.04181v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04181","description":"<p>Deep networks have strong capacities of embedding data into latent\nrepresentations and finishing following tasks. However, the capacities largely\ncome from high-quality annotated labels, which are expensive to collect. Noisy\nlabels are more affordable, but result in corrupted representations, leading to\npoor generalization performance. To learn robust representations and handle\nnoisy labels, we propose selective-supervised contrastive learning (Sel-CL) in\nthis paper. Specifically, Sel-CL extend supervised contrastive learning\n(Sup-CL), which is powerful in representation learning, but is degraded when\nthere are noisy labels. Sel-CL tackles the direct cause of the problem of\nSup-CL. That is, as Sup-CL works in a \\textit{pair-wise} manner, noisy pairs\nbuilt by noisy labels mislead representation learning. To alleviate the issue,\nwe select confident pairs out of noisy ones for Sup-CL without knowing noise\nrates. In the selection process, by measuring the agreement between learned\nrepresentations and given labels, we first identify confident examples that are\nexploited to build confident pairs. Then, the representation similarity\ndistribution in the built confident pairs is exploited to identify more\nconfident pairs out of noisy pairs. All obtained confident pairs are finally\nused for Sup-CL to enhance representations. Experiments on multiple noisy\ndatasets demonstrate the robustness of the learned representations by our\nmethod, following the state-of-the-art performance. Source codes are available\nat https://github.com/ShikunLi/Sel-CL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shikun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xiaobo Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shiming Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLSeg: Image and Video Segmentation as Multi-Label Classification and Selected-Label Pixel Classification. (arXiv:2203.04187v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04187","description":"<p>For a long period of time, research studies on segmentation have typically\nformulated the task as pixel classification that predicts a class for each\npixel from a set of predefined, fixed number of semantic categories. Yet\nstandard architectures following this formulation will inevitably encounter\nvarious challenges under more realistic settings where the total number of\nsemantic categories scales up (e.g., beyond $1\\rm{k}$ classes). On the other\nhand, a standard image or video usually contains only a small number of\nsemantic categories from the entire label set. Motivated by this intuition, in\nthis paper, we propose to decompose segmentation into two sub-problems: (i)\nimage-level or video-level multi-label classification and (ii) pixel-level\nselected-label classification. Given an input image or video, our framework\nfirst conducts multi-label classification over the large complete label set and\nselects a small set of labels according to the class confidence scores. Then\nthe follow-up pixel-wise classification is only performed among the selected\nsubset of labels. Our approach is conceptually general and can be applied to\nvarious existing segmentation frameworks by simply adding a lightweight\nmulti-label classification branch. We demonstrate the effectiveness of our\nframework with competitive experimental results across four tasks including\nimage semantic segmentation, image panoptic segmentation, video instance\nsegmentation, and video semantic segmentation. Especially, with our MLSeg,\nMask$2$Former gains +$0.8\\%$/+$0.7\\%$/+$0.7\\%$ on ADE$20$K panoptic\nsegmentation/YouTubeVIS $2019$ video instance segmentation/VSPW video semantic\nsegmentation benchmarks respectively. Code will be available\nat:https://github.com/openseg-group/MLSeg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haodi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuhui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiangyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Gating Model for Bias Calibration in Generalized Zero-shot Learning. (arXiv:2203.04195v1 [cs.LG])","link":"http://arxiv.org/abs/2203.04195","description":"<p>Generalized zero-shot learning (GZSL) aims at training a model that can\ngeneralize to unseen class data by only using auxiliary information. One of the\nmain challenges in GZSL is a biased model prediction toward seen classes caused\nby overfitting on only available seen class data during training. To overcome\nthis issue, we propose a two-stream autoencoder-based gating model for GZSL.\nOur gating model predicts whether the query data is from seen classes or unseen\nclasses, and utilizes separate seen and unseen experts to predict the class\nindependently from each other. This framework avoids comparing the biased\nprediction scores for seen classes with the prediction scores for unseen\nclasses. In particular, we measure the distance between visual and attribute\nrepresentations in the latent space and the cross-reconstruction space of the\nautoencoder. These distances are utilized as complementary features to\ncharacterize unseen classes at different levels of data abstraction. Also, the\ntwo-stream autoencoder works as a unified framework for the gating model and\nthe unseen expert, which makes the proposed method computationally efficient.\nWe validate our proposed method in four benchmark image recognition datasets.\nIn comparison with other state-of-the-art methods, we achieve the best harmonic\nmean accuracy in SUN and AWA2, and the second best in CUB and AWA1.\nFurthermore, our base model requires at least 20% less number of model\nparameters than state-of-the-art methods relying on generative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1\">Gukyeong Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlRegib_G/0/1/0/all/0/1\">Ghassan AlRegib</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trustable Co-label Learning from Multiple Noisy Annotators. (arXiv:2203.04199v1 [cs.LG])","link":"http://arxiv.org/abs/2203.04199","description":"<p>Supervised deep learning depends on massive accurately annotated examples,\nwhich is usually impractical in many real-world scenarios. A typical\nalternative is learning from multiple noisy annotators. Numerous earlier works\nassume that all labels are noisy, while it is usually the case that a few\ntrusted samples with clean labels are available. This raises the following\nimportant question: how can we effectively use a small amount of trusted data\nto facilitate robust classifier learning from multiple annotators? This paper\nproposes a data-efficient approach, called \\emph{Trustable Co-label Learning}\n(TCL), to learn deep classifiers from multiple noisy annotators when a small\nset of trusted data is available. This approach follows the coupled-view\nlearning manner, which jointly learns the data classifier and the label\naggregator. It effectively uses trusted data as a guide to generate trustable\nsoft labels (termed co-labels). A co-label learning can then be performed by\nalternately reannotating the pseudo labels and refining the classifiers. In\naddition, we further improve TCL for a special complete data case, where each\ninstance is labeled by all annotators and the label aggregator is represented\nby multilayer neural networks to enhance model capacity. Extensive experiments\non synthetic and real datasets clearly demonstrate the effectiveness and\nrobustness of the proposed approach. Source code is available at\nhttps://github.com/ShikunLi/TCL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shikun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jiyong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shiming Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant. (arXiv:2203.04203v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04203","description":"<p>A long-standing goal of intelligent assistants such as AR glasses/robots has\nbeen to assist users in affordance-centric real-world scenarios, such as \"how\ncan I run the microwave for 1 minute?\". However, there is still no clear task\ndefinition and suitable benchmarks. In this paper, we define a new task called\nAffordance-centric Question-driven Task Completion, where the AI assistant\nshould learn from instructional videos and scripts to guide the user\nstep-by-step. To support the task, we constructed AssistQ, a new dataset\ncomprising 529 question-answer samples derived from 100 newly filmed\nfirst-person videos. Each question should be completed with multi-step\nguidances by inferring from visual details (e.g., buttons' position) and\ntextural details (e.g., actions like press/turn). To address this unique task,\nwe developed a Question-to-Actions (Q2A) model that significantly outperforms\nseveral baseline methods while still having large room for improvement. We\nexpect our task and dataset to advance Egocentric AI Assistant's development.\nOur project page is available at: https://showlab.github.io/assistq\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_B/0/1/0/all/0/1\">Benita Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Joya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">You Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Stan Weixian Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_D/0/1/0/all/0/1\">Dongxing Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Monocular Depth Estimation through Guided Decoding. (arXiv:2203.04206v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04206","description":"<p>We present a lightweight encoder-decoder archi- tecture for monocular depth\nestimation, specifically designed for embedded platforms. Our main contribution\nis the Guided Upsampling Block (GUB) for building the decoder of our model.\nMotivated by the concept of guided image filtering, GUB relies on the image to\nguide the decoder on upsampling the feature representation and the depth map\nreconstruction, achieving high resolution results with fine-grained details.\nBased on multiple GUBs, our model outperforms the related methods on the NYU\nDepth V2 dataset in terms of accuracy while delivering up to 35.1 fps on the\nNVIDIA Jetson Nano and up to 144.5 fps on the NVIDIA Xavier NX. Similarly, on\nthe KITTI dataset, inference is possible with up to 23.7 fps on the Jetson Nano\nand 102.9 fps on the Xavier NX. Our code and models are made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1\">Michael Rudolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawoud_Y/0/1/0/all/0/1\">Youssef Dawoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guldenring_R/0/1/0/all/0/1\">Ronja G&#xfc;ldenring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalpantidis_L/0/1/0/all/0/1\">Lazaros Nalpantidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Universal Texture Synthesis by Combining Texton Broadcasting with Noise Injection in StyleGAN-2. (arXiv:2203.04221v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04221","description":"<p>We present a new approach for universal texture synthesis by incorporating a\nmulti-scale texton broadcasting module in the StyleGAN-2 framework. The texton\nbroadcasting module introduces an inductive bias, enabling generation of\nbroader range of textures, from those with regular structures to completely\nstochastic ones. To train and evaluate the proposed approach, we construct a\ncomprehensive high-resolution dataset that captures the diversity of natural\ntextures as well as stochastic variations within each perceptually uniform\ntexture. Experimental results demonstrate that the proposed approach yields\nsignificantly better quality textures than the state of the art. The ultimate\ngoal of this work is a comprehensive understanding of texture space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1\">Gaurav Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_T/0/1/0/all/0/1\">Thrasyvoulos N. Pappas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Face Identification in a 2D Wireframe Projection of a Manifold Object. (arXiv:2203.04229v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04229","description":"<p>In computer-aided design (CAD) systems, 2D line drawings are commonly used to\nillustrate 3D object designs. To reconstruct the 3D models depicted by a single\n2D line drawing, an important key is finding the edge loops in the line drawing\nwhich correspond to the actual faces of the 3D object. In this paper, we\napproach the classical problem of face identification from a novel data-driven\npoint of view. We cast it as a sequence generation problem: starting from an\narbitrary edge, we adopt a variant of the popular Transformer model to predict\nthe edges associated with the same face in a natural order. This allows us to\navoid searching the space of all possible edge loops with various hand-crafted\nrules and heuristics as most existing methods do, deal with challenging cases\nsuch as curved surfaces and nested edge loops, and leverage additional cues\nsuch as face types. We further discuss how possibly imperfect predictions can\nbe used for 3D object reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kehan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zihan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Lightweight and Detector-free 3D Single Object Tracker on Point Clouds. (arXiv:2203.04232v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04232","description":"<p>Recent works on 3D single object tracking treat the tracking as a\ntarget-specific 3D detection task, where an off-the-shelf 3D detector is\ncommonly employed for tracking. However, it is non-trivial to perform accurate\ntarget-specific detection since the point cloud of objects in raw LiDAR scans\nis usually sparse and incomplete. In this paper, we address this issue by\nexplicitly leveraging temporal motion cues and propose DMT, a Detector-free\nMotion prediction based 3D Tracking network that totally removes the usage of\ncomplicated 3D detectors, which is lighter, faster, and more accurate than\nprevious trackers. Specifically, the motion prediction module is firstly\nintroduced to estimate a potential target center of the current frame in a\npoint-cloud free way. Then, an explicit voting module is proposed to directly\nregress the 3D box from the estimated target center. Extensive experiments on\nKITTI and NuScenes datasets demonstrate that our DMT, without applying any\ncomplicated 3D detectors, can still achieve better performance (~10%\nimprovement on the NuScenes dataset) and faster tracking speed (i.e., 72 FPS)\nthan state-of-the-art approaches. Our codes will be released publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiangqiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stilla_U/0/1/0/all/0/1\">Uwe Stilla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Semi-Supervised Learning for Video Action Detection. (arXiv:2203.04251v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04251","description":"<p>In this work, we focus on semi-supervised learning for video action detection\nwhich utilizes both labeled as well as unlabeled data. We propose a simple\nend-to-end consistency based approach which effectively utilizes the unlabeled\ndata. Video action detection requires both, action class prediction as well as\na spatio-temporal localization of actions. Therefore, we investigate two types\nof constraints, classification consistency, and spatio-temporal consistency.\nThe presence of predominant background and static regions in a video makes it\nchallenging to utilize spatio-temporal consistency for action detection. To\naddress this, we propose two novel regularization constraints for\nspatio-temporal consistency; 1) temporal coherency, and 2) gradient smoothness.\nBoth these aspects exploit the temporal continuity of action in videos and are\nfound to be effective for utilizing unlabeled videos for action detection. We\ndemonstrate the effectiveness of the proposed approach on two different action\ndetection benchmark datasets, UCF101-24 and JHMDB-21. In addition, we also show\nthe effectiveness of the proposed approach for video object segmentation on the\nYoutube-VOS dataset which demonstrates its generalization capability to other\ntasks. The proposed approach achieves competitive performance by using merely\n20% of annotations on UCF101-24 when compared with recent fully supervised\nmethods. On UCF101-24, it improves the score by +8.9% and +11% at 0.5 f-mAP and\nv-mAP respectively, compared to supervised approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Akash Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh Singh Rawat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap. (arXiv:2203.04275v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04275","description":"<p>This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural\nNetwork (CNN) for pose estimation of noncooperative spacecraft across domain\ngap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared\nmulti-scale feature encoder and multiple prediction heads that perform\ndifferent tasks on a shared feature output. These tasks are all related to\ndetection and pose estimation of a target spacecraft from an image, such as\nprediction of pre-defined satellite keypoints, direct pose regression, and\nbinary segmentation of the satellite foreground. It is shown that by jointly\ntraining on different yet related tasks with extensive data augmentations on\nsynthetic images only, the shared encoder learns features that are common\nacross image domains that have fundamentally different visual characteristics\ncompared to synthetic images. This work also introduces Online Domain\nRefinement (ODR) which refines the parameters of the normalization layers of\nSPNv2 on the target domain images online at deployment. Specifically, ODR\nperforms self-supervised entropy minimization of the predicted satellite\nforeground, thereby improving the CNN's performance on the target domain images\nwithout their pose labels and with minimal computational efforts. The GitHub\nrepository for SPNv2 will be made available in the near future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Tae Ha Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmico_S/0/1/0/all/0/1\">Simone D&#x27;Amico</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences. (arXiv:2203.04279v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04279","description":"<p>We propose Probabilistic Warp Consistency, a weakly-supervised learning\nobjective for semantic matching. Our approach directly supervises the dense\nmatching scores predicted by the network, encoded as a conditional probability\ndistribution. We first construct an image triplet by applying a known warp to\none of the images in a pair depicting different instances of the same object\nclass. Our probabilistic learning objectives are then derived using the\nconstraints arising from the resulting image triplet. We further account for\nocclusion and background clutter present in real image pairs by extending our\nprobabilistic output space with a learnable unmatched state. To supervise it,\nwe design an objective between image pairs depicting different object classes.\nWe validate our method by applying it to four recent semantic matching\narchitectures. Our weakly-supervised approach sets a new state-of-the-art on\nfour challenging semantic matching benchmarks. Lastly, we demonstrate that our\nobjective also brings substantial improvements in the strongly-supervised\nregime, when combined with keypoint annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_P/0/1/0/all/0/1\">Prune Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proximal PanNet: A Model-Based Deep Network for Pansharpening. (arXiv:2203.04286v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04286","description":"<p>Recently, deep learning techniques have been extensively studied for\npansharpening, which aims to generate a high resolution multispectral (HRMS)\nimage by fusing a low resolution multispectral (LRMS) image with a high\nresolution panchromatic (PAN) image. However, existing deep learning-based\npansharpening methods directly learn the mapping from LRMS and PAN to HRMS.\nThese network architectures always lack sufficient interpretability, which\nlimits further performance improvements. To alleviate this issue, we propose a\nnovel deep network for pansharpening by combining the model-based methodology\nwith the deep learning method. Firstly, we build an observation model for\npansharpening using the convolutional sparse coding (CSC) technique and design\na proximal gradient algorithm to solve this model. Secondly, we unfold the\niterative algorithm into a deep network, dubbed as Proximal PanNet, by learning\nthe proximal operators using convolutional neural networks. Finally, all the\nlearnable modules can be automatically learned in an end-to-end manner.\nExperimental results on some benchmark datasets show that our network performs\nbetter than other advanced methods both quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiangyong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1\">Wenfei Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation. (arXiv:2203.04287v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04287","description":"<p>This paper proposes a simple transfer learning baseline for sign language\ntranslation. Existing sign language datasets (e.g. PHOENIX-2014T, CSL-Daily)\ncontain only about 10K-20K pairs of sign videos, gloss annotations and texts,\nwhich are an order of magnitude smaller than typical parallel data for training\nspoken language translation models. Data is thus a bottleneck for training\neffective sign language translation models. To mitigate this problem, we\npropose to progressively pretrain the model from general-domain datasets that\ninclude a large amount of external supervision to within-domain datasets.\nConcretely, we pretrain the sign-to-gloss visual network on the general domain\nof human actions and the within-domain of a sign-to-gloss dataset, and pretrain\nthe gloss-to-text translation network on the general domain of a multilingual\ncorpus and the within-domain of a gloss-to-text corpus. The joint model is\nfine-tuned with an additional module named the visual-language mapper that\nconnects the two networks. This simple baseline surpasses the previous\nstate-of-the-art results on two sign language translation benchmarks,\ndemonstrating the effectiveness of transfer learning. With its simplicity and\nstrong performance, this approach can serve as a solid baseline for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yutong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Semi-Supervised Framework for Automatic Pixel-Wise Breast Cancer Grading of Histological Images. (arXiv:1907.01696v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1907.01696","description":"<p>Throughout the world, breast cancer is one of the leading causes of female\ndeath. Recently, deep learning methods are developed to automatically grade\nbreast cancer of histological slides. However, the performance of existing deep\nlearning models is limited due to the lack of large annotated biomedical\ndatasets. One promising way to relieve the annotating burden is to leverage the\nunannotated datasets to enhance the trained model. In this paper, we first\napply active learning method in breast cancer grading, and propose a\nsemi-supervised framework based on expectation maximization (EM) model. The\nproposed EM approach is based on the collaborative filtering among the\nannotated and unannotated datasets. The collaborative filtering method\neffectively extracts useful and credible datasets from the unannotated images.\nResults of pixel-wise prediction of whole-slide images (WSI) demonstrate that\nthe proposed method not only outperforms state-of-art methods, but also\nsignificantly reduces the annotation cost by over 70%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Man_Y/0/1/0/all/0/1\">Yanyuet Man</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiangyun Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingcheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Han Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Graph Transformer Self-Attention Networks. (arXiv:1909.11855v13 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1909.11855","description":"<p>We introduce a transformer-based GNN model, named UGformer, to learn graph\nrepresentations. In particular, we present two UGformer variants, wherein the\nfirst variant (publicized in September 2019) is to leverage the transformer on\na set of sampled neighbors for each input node, while the second (publicized in\nMay 2021) is to leverage the transformer on all input nodes. Experimental\nresults demonstrate that the first UGformer variant achieves state-of-the-art\naccuracies on benchmark datasets for graph classification in both inductive\nsetting and unsupervised transductive setting; and the second UGformer variant\nobtains state-of-the-art accuracies for inductive text classification. The code\nis available at: \\url{https://github.com/daiquocnguyen/Graph-Transformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Dinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geography-Aware Self-Supervised Learning. (arXiv:2011.09980v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.09980","description":"<p>Contrastive learning methods have significantly narrowed the gap between\nsupervised and unsupervised learning on computer vision tasks. In this paper,\nwe explore their application to geo-located datasets, e.g. remote sensing,\nwhere unlabeled data is often abundant but labeled data is scarce. We first\nshow that due to their different characteristics, a non-trivial gap persists\nbetween contrastive and supervised learning on standard benchmarks. To close\nthe gap, we propose novel training methods that exploit the spatio-temporal\nstructure of remote sensing data. We leverage spatially aligned images over\ntime to construct temporal positive pairs in contrastive learning and\ngeo-location to design pre-text tasks. Our experiments show that our proposed\nmethod closes the gap between contrastive and supervised learning on image\nclassification, object detection and semantic segmentation for remote sensing.\nMoreover, we demonstrate that the proposed method can also be applied to\ngeo-tagged ImageNet images, improving downstream performance on various tasks.\nProject Webpage can be found at this link geography-aware-ssl.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayush_K/0/1/0/all/0/1\">Kumar Ayush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzkent_B/0/1/0/all/0/1\">Burak Uzkent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chenlin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanmay_K/0/1/0/all/0/1\">Kumar Tanmay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1\">Marshall Burke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1\">David Lobell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forecasting Characteristic 3D Poses of Human Actions. (arXiv:2011.15079v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.15079","description":"<p>We propose the task of forecasting characteristic 3d poses: from a short\nsequence observation of a person, predict a future 3d pose of that person in a\nlikely action-defining, characteristic pose -- for instance, from observing a\nperson picking up an apple, predict the pose of the person eating the apple.\nPrior work on human motion prediction estimates future poses at fixed time\nintervals. Although easy to define, this frame-by-frame formulation confounds\ntemporal and intentional aspects of human action. Instead, we define a\nsemantically meaningful pose prediction task that decouples the predicted pose\nfrom time, taking inspiration from goal-directed behavior. To predict\ncharacteristic poses, we propose a probabilistic approach that models the\npossible multi-modality in the distribution of likely characteristic poses. We\nthen sample future pose hypotheses from the predicted distribution in an\nautoregressive fashion to model dependencies between joints. To evaluate our\nmethod, we construct a dataset of manually annotated characteristic 3d poses.\nOur experiments with this dataset suggest that our proposed probabilistic\napproach outperforms state-of-the-art methods by 26% on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diller_C/0/1/0/all/0/1\">Christian Diller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1\">Thomas Funkhouser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble deep learning: A review. (arXiv:2104.02395v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.02395","description":"<p>Ensemble learning combines several individual models to obtain better\ngeneralization performance. Currently, deep learning models with multilayer\nprocessing architecture is showing better performance as compared to the\nshallow or traditional classification models. Deep ensemble learning models\ncombine the advantages of both the deep learning models as well as the ensemble\nlearning such that the final model has better generalization performance. This\npaper reviews the state-of-art deep ensemble models and hence serves as an\nextensive summary for the researchers. The ensemble models are broadly\ncategorised into ensemble models like bagging, boosting and stacking, negative\ncorrelation based deep ensemble models, explicit/implicit ensembles,\nhomogeneous /heterogeneous ensemble, decision fusion strategies, unsupervised,\nsemi-supervised, reinforcement learning and online/incremental, multilabel\nbased deep ensemble models. Application of deep ensemble models in different\ndomains is also briefly discussed. Finally, we conclude this paper with some\nfuture recommendations and research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganaie_M/0/1/0/all/0/1\">M.A. Ganaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Minghui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1\">A.K. Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanveer_M/0/1/0/all/0/1\">M. Tanveer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganthan_P/0/1/0/all/0/1\">P.N. Suganthan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Segmentation via Cycle-Consistent Transformer. (arXiv:2106.02320v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02320","description":"<p>Few-shot segmentation aims to train a segmentation model that can fast adapt\nto novel classes with few exemplars. The conventional training paradigm is to\nlearn to make predictions on query images conditioned on the features from\nsupport images. Previous methods only utilized the semantic-level prototypes of\nsupport images as conditional information. These methods cannot utilize all\npixel-wise support information for the query predictions, which is however\ncritical for the segmentation task. In this paper, we focus on utilizing\npixel-wise relationships between support and query images to facilitate the\nfew-shot segmentation task. We design a novel Cycle-Consistent TRansformer\n(CyCTR) module to aggregate pixel-wise support features into query ones. CyCTR\nperforms cross-attention between features from different images, i.e. support\nand query images. We observe that there may exist unexpected irrelevant\npixel-level support features. Directly performing cross-attention may aggregate\nthese features from support to query and bias the query features. Thus, we\npropose using a novel cycle-consistent attention mechanism to filter out\npossible harmful support features and encourage query features to attend to the\nmost informative pixels from support images. Experiments on all few-shot\nsegmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable\nimprovement compared to previous state-of-the-art methods. Specifically, on\nPascal-$5^i$ and COCO-$20^i$ datasets, we achieve 67.5% and 45.6% mIoU for\n5-shot segmentation, outperforming previous state-of-the-art methods by 5.6%\nand 7.1% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gengwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1\">Guoliang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving Deep into the Generalization of Vision Transformers under Distribution Shifts. (arXiv:2106.07617v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07617","description":"<p>Vision Transformers (ViTs) have achieved impressive performance on various\nvision tasks, yet their generalization under distribution shifts (DS) is rarely\nunderstood. In this work, we comprehensively study the out-of-distribution\n(OOD) generalization of ViTs. For systematic investigation, we first present a\ntaxonomy of DS. We then perform extensive evaluations of ViT variants under\ndifferent DS and compare their generalization with Convolutional Neural Network\n(CNN) models. Important observations are obtained: 1) ViTs learn weaker biases\non backgrounds and textures, while they are equipped with stronger inductive\nbiases towards shapes and structures, which is more consistent with human\ncognitive traits. Therefore, ViTs generalize better than CNNs under DS. With\nthe same or less amount of parameters, ViTs are ahead of corresponding CNNs by\nmore than 5% in top-1 accuracy under most types of DS. 2) As the model scale\nincreases, ViTs strengthen these biases and thus gradually narrow the\nin-distribution and OOD performance gap. To further improve the generalization\nof ViTs, we design the Generalization-Enhanced ViTs (GE-ViTs) from the\nperspectives of adversarial learning, information theory, and self-supervised\nlearning. By comprehensively investigating these GE-ViTs and comparing with\ntheir corresponding CNN models, we observe: 1) For the enhanced model, larger\nViTs still benefit more for the OOD generalization. 2) GE-ViTs are more\nsensitive to the hyper-parameters than their corresponding CNN models. We\ndesign a smoother learning strategy to achieve a stable training process and\nobtain performance improvements on OOD data by 4% from vanilla ViTs. We hope\nour comprehensive study could shed light on the design of more generalizable\nlearning architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Daisheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RPR-Net: A Point Cloud-based Rotation-aware Large Scale Place Recognition Network. (arXiv:2108.12790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12790","description":"<p>Point cloud-based large scale place recognition is an important but\nchallenging task for many applications such as Simultaneous Localization and\nMapping (SLAM). Taking the task as a point cloud retrieval problem, previous\nmethods have made delightful achievements. However, how to deal with\ncatastrophic collapse caused by rotation problems is still under-explored. In\nthis paper, to tackle the issue, we propose a novel Point Cloud-based\nRotation-aware Large Scale Place Recognition Network (RPR-Net). In particular,\nto solve the problem, we propose to learn rotation-invariant features in three\nsteps. First, we design three kinds of novel Rotation-Invariant Features\n(RIFs), which are low-level features that can hold the rotation-invariant\nproperty. Second, using these RIFs, we design an attentive module to learn\nrotation-invariant kernels. Third, we apply these kernels to previous point\ncloud features to generate new features, which is the well-known SO(3) mapping\nprocess. By doing so, high-level scene-specific rotation-invariant features can\nbe learned. We call the above process an Attentive Rotation-Invariant\nConvolution (ARIConv). To achieve the place recognition goal, we build RPR-Net,\nwhich takes ARIConv as a basic unit to construct a dense network architecture.\nThen, powerful global descriptors used for retrieval-based place recognition\ncan be sufficiently extracted from RPR-Net. Experimental results on prevalent\ndatasets show that our method achieves comparable results to existing\nstate-of-the-art place recognition models and significantly outperforms other\nrotation-invariant baseline models when solving rotation problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhaoxin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhenbo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiaoyong Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAM-loss: Towards Learning Spatially Discriminative Feature Representations. (arXiv:2109.01359v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01359","description":"<p>The backbone of traditional CNN classifier is generally considered as a\nfeature extractor, followed by a linear layer which performs the\nclassification. We propose a novel loss function, termed as CAM-loss, to\nconstrain the embedded feature maps with the class activation maps (CAMs) which\nindicate the spatially discriminative regions of an image for particular\ncategories. CAM-loss drives the backbone to express the features of target\ncategory and suppress the features of non-target categories or background, so\nas to obtain more discriminative feature representations. It can be simply\napplied in any CNN architecture with neglectable additional parameters and\ncalculations. Experimental results show that CAM-loss is applicable to a\nvariety of network structures and can be combined with mainstream\nregularization methods to improve the performance of image classification. The\nstrong generalization ability of CAM-loss is validated in the transfer learning\nand few shot learning tasks. Based on CAM-loss, we also propose a novel\nCAAM-CAM matching knowledge distillation method. This method directly uses the\nCAM generated by the teacher network to supervise the CAAM generated by the\nstudent network, which effectively improves the accuracy and convergence rate\nof the student network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jiayu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yizeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qisen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning-Based Unified Framework for Red Lesions Detection on Retinal Fundus Images. (arXiv:2109.05021v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.05021","description":"<p>Red-lesions, i.e., microaneurysms (MAs) and hemorrhages (HMs), are the early\nsigns of diabetic retinopathy (DR). The automatic detection of MAs and HMs on\nretinal fundus images is a challenging task. Most of the existing methods\ndetect either only MAs or only HMs because of the difference in their texture,\nsizes, and morphology. Though some methods detect both MAs and HMs, they suffer\nfrom the curse of dimensionality of shape and colors features and fail to\ndetect all shape variations of HMs such as flame-shaped HM. Leveraging the\nprogress in deep learning, we proposed a two-stream red lesions detection\nsystem dealing simultaneously with small and large red lesions. For this\nsystem, we introduced a new ROIs candidates generation method for large red\nlesions fundus images; it is based on blood vessel segmentation and\nmorphological operations, and reduces the computational complexity, and\nenhances the detection accuracy by generating a small number of potential\ncandidates. For detection, we adapted the Faster RCNN framework with two\nstreams. We used pre-trained VGGNet as a bone model and carried out several\nextensive experiments to tune it for vessels segmentation and candidates\ngeneration, and finally learning the appropriate mapping, which yields better\ndetection of the red lesions comparing with the state-of-the-art methods. The\nexperimental results validated the effectiveness of the system in the detection\nof both MAs and HMs; the method yields higher performance for per lesion\ndetection according to sensitivity under 4 FPIs on DiaretDB1-MA and\nDiaretDB1-HM datasets, and 1 FPI on e-ophtha and ROCh datasets than the state\nof the art methods w.r.t. various evaluation metrics. For DR screening, the\nsystem outperforms other methods on DiaretDB1-MA, DiaretDB1-HM, and e-ophtha\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Asiri_N/0/1/0/all/0/1\">Norah Asiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_M/0/1/0/all/0/1\">Muhammad Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adel_F/0/1/0/all/0/1\">Fadwa Al Adel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aboalsamh_H/0/1/0/all/0/1\">Hatim Aboalsamh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Quantization with Code Memory for Unsupervised Image Retrieval. (arXiv:2109.05205v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05205","description":"<p>The high efficiency in computation and storage makes hashing (including\nbinary hashing and quantization) a common strategy in large-scale retrieval\nsystems. To alleviate the reliance on expensive annotations, unsupervised deep\nhashing becomes an important research problem. This paper provides a novel\nsolution to unsupervised deep quantization, namely Contrastive Quantization\nwith Code Memory (MeCoQ). Different from existing reconstruction-based\nstrategies, we learn unsupervised binary descriptors by contrastive learning,\nwhich can better capture discriminative visual semantics. Besides, we uncover\nthat codeword diversity regularization is critical to prevent contrastive\nlearning-based quantization from model degeneration. Moreover, we introduce a\nnovel quantization code memory module that boosts contrastive learning with\nlower feature drift than conventional feature memories. Extensive experiments\non benchmark datasets show that MeCoQ outperforms state-of-the-art methods.\nCode and configurations are publicly available at\nhttps://github.com/gimpong/AAAI22-MeCoQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Tao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SphereFace Revived: Unifying Hyperspherical Face Recognition. (arXiv:2109.05565v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05565","description":"<p>This paper addresses the deep face recognition problem under an open-set\nprotocol, where ideal face features are expected to have smaller maximal\nintra-class distance than minimal inter-class distance under a suitably chosen\nmetric space. To this end, hyperspherical face recognition, as a promising line\nof research, has attracted increasing attention and gradually become a major\nfocus in face recognition research. As one of the earliest works in\nhyperspherical face recognition, SphereFace explicitly proposed to learn face\nembeddings with large inter-class angular margin. However, SphereFace still\nsuffers from severe training instability which limits its application in\npractice. In order to address this problem, we introduce a unified framework to\nunderstand large angular margin in hyperspherical face recognition. Under this\nframework, we extend the study of SphereFace and propose an improved variant\nwith substantially better training stability -- SphereFace-R. Specifically, we\npropose two novel ways to implement the multiplicative margin, and study\nSphereFace-R under three different feature normalization schemes (no feature\nnormalization, hard feature normalization and soft feature normalization). We\nalso propose an implementation strategy -- \"characteristic gradient detachment\"\n-- to stabilize training. Extensive experiments on SphereFace-R show that it is\nconsistently better than or competitive with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yandong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Instance Segmentation with High-Resolution Automotive Radar. (arXiv:2110.01775v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01775","description":"<p>Automotive radar has been widely used in the modern advanced driver\nassistance systems (ADAS) and autonomous driving system as it provides reliable\nenvironmental perception in all-weather conditions with affordable cost.\nHowever, automotive radar usually only plays as an auxiliary sensor since it\nhardly supplies semantic and geometry information due to the sparsity of radar\ndetection points. Nonetheless, as development of high-resolution automotive\nradar in recent years, more advanced perception functionality like instance\nsegmentation which has only been well explored using Lidar point clouds,\nbecomes possible by using automotive radar. Its data comes with rich contexts\nsuch as Radar Cross Section (RCS) and micro-doppler effects which may\npotentially be pertinent, and sometimes can even provide detection when the\nfield of view is completely obscured. Therefore, the effective utilization of\nradar detection points data is an integral part of automotive perception. The\noutcome from instance segmentation could be seen as comparable result of\nclustering, and could be potentially used as the input of tracker for tracking\nthe targets. In this paper, we propose two efficient methods for instance\nsegmentation with radar detection points, one is implemented in an end-to-end\ndeep learning driven fashion using PointNet++ framework, and the other is based\non clustering of the radar detection points with semantic information. Both\napproaches can be further improved by implementing visual multi-layer\nperceptron (MLP). The effectiveness of the proposed methods is verified using\nexperimental results on the recent RadarScenes dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Weiyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Liping Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim2Air - Synthetic aerial dataset for UAV monitoring. (arXiv:2110.05145v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05145","description":"<p>In this paper we propose a novel approach to generate a synthetic aerial\ndataset for application in UAV monitoring. We propose to accentuate shape-based\nobject representation by applying texture randomization. A diverse dataset with\nphotorealism in all parameters such as shape, pose, lighting, scale, viewpoint,\netc. except for atypical textures is created in a 3D modelling software\nBlender. Our approach specifically targets two conditions in aerial images\nwhere texture of objects is difficult to detect, namely challenging\nillumination and objects occupying only a small portion of the image.\nExperimental evaluation of YOLO and Faster R-CNN detectors trained on synthetic\ndata with randomized textures confirmed our approach by increasing the mAP\nvalue (17 and 3.7 percentage points for YOLO; 20 and 1.1 percentage points for\nFaster R-CNN) on two test datasets of real images, both containing UAV-to-UAV\nimages with motion blur. Testing on different domains, we conclude that the\nmore the generalisation ability is put to the test, the more apparent are the\nadvantages of the shape-based representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barisic_A/0/1/0/all/0/1\">Antonella Barisic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petric_F/0/1/0/all/0/1\">Frano Petric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogdan_S/0/1/0/all/0/1\">Stjepan Bogdan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Contrastive Learning for Domain Adaptation. (arXiv:2111.06021v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.06021","description":"<p>Recent feature contrastive learning (FCL) has shown promising performance in\nself-supervised representation learning. For domain adaptation, however, FCL\ncannot show overwhelming gains since the class weights are not involved during\noptimization, which does not guarantee the produced features to be clustered\naround the class weights learned from source data. To tackle this issue, we\npropose a novel probability contrastive learning (PCL) in this paper, which not\nonly produces compact features but also enforces them to be distributed around\nthe class weights. Specifically, we propose to use the output probabilities\nafter softmax to perform contrastive learning instead of the extracted features\nand remove the $\\ell_{2}$ normalization in the traditional FCL. In this way,\nthe probability will approximate the one-hot form, thereby narrowing the\ndistance between the features and the class weights. Our proposed PCL is simple\nand effective. We conduct extensive experiments on two domain adaptation tasks,\ni.e., unsupervised domain adaptation and semi-supervised domain adaptation. The\nresults on multiple datasets demonstrate that our PCL can consistently get\nconsiderable gains and achieves the state-of-the-art performance. In addition,\nour method also obtains considerable gains on semi-supervised tasks when\nlabeled data is scarce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Keyu Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IntraQ: Learning Synthetic Images with Intra-Class Heterogeneity for Zero-Shot Network Quantization. (arXiv:2111.09136v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09136","description":"<p>Learning to synthesize data has emerged as a promising direction in zero-shot\nquantization (ZSQ), which represents neural networks by low-bit integer without\naccessing any of the real data. In this paper, we observe an interesting\nphenomenon of intra-class heterogeneity in real data and show that existing\nmethods fail to retain this property in their synthetic images, which causes a\nlimited performance increase. To address this issue, we propose a novel\nzero-shot quantization method referred to as IntraQ. First, we propose a local\nobject reinforcement that locates the target objects at different scales and\npositions of the synthetic images. Second, we introduce a marginal distance\nconstraint to form class-related features distributed in a coarse area. Lastly,\nwe devise a soft inception loss which injects a soft prior label to prevent the\nsynthetic images from being overfitting to a fixed object. Our IntraQ is\ndemonstrated to well retain the intra-class heterogeneity in the synthetic\nimages and also observed to perform state-of-the-art. For example, compared to\nthe advanced ZSQ, our IntraQ obtains 9.17\\% increase of the top-1 accuracy on\nImageNet when all layers of MobileNetV1 are quantized to 4-bit. Code is at\nhttps://github.com/zysxmu/IntraQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yunshan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1\">Gongrui Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Vocabulary Object Detection with Pseudo Bounding-Box Labels. (arXiv:2111.09452v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09452","description":"<p>Despite great progress in object detection, most existing methods work only\non a limited set of object categories, due to the tremendous human effort\nneeded for instance-level bounding-box annotations of training data. To\nalleviate the problem, recent open vocabulary and zero-shot detection methods\nattempt to detect novel object categories beyond those seen during training.\nThey achieve this goal by training on a pre-defined base categories to induce\ngeneralization to novel objects. However, their potential is still constrained\nby the small set of base categories available for training. To enlarge the set\nof base classes, we propose a method to automatically generate pseudo\nbounding-box annotations of diverse objects from large-scale image-caption\npairs. Our method leverages the localization ability of pre-trained\nvision-language models to generate pseudo bounding-box labels and then directly\nuses them for training object detectors. Experimental results show that our\nmethod outperforms the state-of-the-art (SOTA) open vocabulary object detector\nby 8% AP on COCO novel categories, by 6.3% AP on PASCAL VOC, by 2.3% AP on\nObjects365 and by 2.8% AP on LVIS. Code is available:\nhttps://github.com/salesforce/PB-OVD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1\">Juan Carlos Niebles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Positional Encoder Graph Neural Networks for Geographic Data. (arXiv:2111.10144v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.10144","description":"<p>Graph neural networks (GNNs) provide a powerful and scalable solution for\nmodeling continuous spatial data. However, in the absence of further context on\nthe geometric structure of the data, they often rely on Euclidean distances to\nconstruct the input graphs. This assumption can be improbable in many\nreal-world settings, where the spatial structure is more complex and explicitly\nnon-Euclidean (e.g., road networks). In this paper, we propose PE-GNN, a new\nframework that incorporates spatial context and correlation explicitly into the\nmodels. Building on recent advances in geospatial auxiliary task learning and\nsemantic spatial embeddings, our proposed method (1) learns a context-aware\nvector encoding of the geographic coordinates and (2) predicts spatial\nautocorrelation in the data in parallel with the main task. On spatial\nregression tasks, we show the effectiveness of our approach, improving\nperformance over different state-of-the-art GNN approaches. We also test our\napproach for spatial interpolation, i.e., spatial regression without node\nfeatures, a task that GNNs are currently not competitive at. We observe that\nour approach not only vastly improves over the GNN baselines, but can match\nGaussian processes, the most commonly utilized method for spatial interpolation\nproblems. The code for this study can be accessed via:\nhttps://github.com/konstantinklemmer/pe-gnn\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safir_N/0/1/0/all/0/1\">Nathan Safir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neill_D/0/1/0/all/0/1\">Daniel B Neill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Well Do Sparse Imagenet Models Transfer?. (arXiv:2111.13445v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13445","description":"<p>Transfer learning is a classic paradigm by which models pretrained on large\n\"upstream\" datasets are adapted to yield good results on \"downstream,\"\nspecialized datasets. Generally, it is understood that more accurate models on\nthe \"upstream\" dataset will provide better transfer accuracy \"downstream\". In\nthis work, we perform an in-depth investigation of this phenomenon in the\ncontext of convolutional neural networks (CNNs) trained on the ImageNet\ndataset, which have been pruned - that is, compressed by sparsifiying their\nconnections. Specifically, we consider transfer using unstructured pruned\nmodels obtained by applying several state-of-the-art pruning methods, including\nmagnitude-based, second-order, re-growth and regularization approaches, in the\ncontext of twelve standard transfer tasks. In a nutshell, our study shows that\nsparse models can match or even outperform the transfer performance of dense\nmodels, even at high sparsities, and, while doing so, can lead to significant\ninference and even training speedups. At the same time, we observe and analyze\nsignificant differences in the behaviour of different pruning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1\">Eugenia Iofinova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1\">Alexandra Peste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1\">Mark Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AirDet: Few-Shot Detection without Fine-tuning for Autonomous Exploration. (arXiv:2112.01740v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01740","description":"<p>Few-shot object detection has attracted increasing attention and rapidly\nprogressed in recent years. However, the requirement of an exhaustive offline\nfine-tuning stage in existing methods is time-consuming and significantly\nhinders their usage in online applications such as autonomous exploration of\nlow-power robots. We find that their major limitation is that the little but\nvaluable information from a few support images is not fully exploited. To solve\nthis problem, we propose a brand new architecture, AirDet, and surprisingly\nfind that, by learning class-agnostic relation with the support images in all\nmodules, including cross-scale object proposal network, shots aggregation\nmodule, and localization network, AirDet without fine-tuning achieves\ncomparable or even better results than the exhaustively fine-tuned methods,\nreaching up to 30-40% improvements. We also present solid results of onboard\ntests on real-world exploration data from the DARPA Subterranean Challenge,\nwhich strongly validate the feasibility of AirDet in robotics. To the best of\nour knowledge, AirDet is the first feasible few-shot detection method for\nautonomous exploration of low-power robots. The source code, pre-trained\nmodels, along with the real-world data for exploration, will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_P/0/1/0/all/0/1\">Pranay Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungchan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeing BDD100K in dark: Single-Stage Night-time Object Detection via Continual Fourier Contrastive Learning. (arXiv:2112.02891v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02891","description":"<p>In this paper, we study the lesser explored avenue of object detection at\nnight-time. An object detector trained on abundant labeled daytime images often\nfails to perform well on night images, due to domain gap. As collecting more\nlabeled data from night-time is expensive, unpaired generative image\ntranslation techniques seek to synthesize night-time images. However,\nunrealistic artifacts often arise on the synthetic images. Illuminating\nnight-time inference images also does not work well in practice, as shown in\nour paper. To address these issues, we suggest a novel technique for enhancing\nthe object detector via Contrastive Learning, which tries to group together\nembeddings of similar images. To provide anchor-positive image pairs for\nContrastive Learning, we leverage Fourier Transformation, which is naturally\ngood at preserving the semantics of an image. For practical benefits in\nreal-time applications, we choose the recently proposed YOLOF single-stage\ndetector, which provides a simple and clean encoder-decoder segregation of the\ndetector network. However, merely trying to teach the encoder to perform well\non the auxiliary Contrastive Learning task may lead to catastrophic forgetting\nof the knowledge essential for object detection. Hence, we train the encoder in\na Continual Learning fashion. Our novel method by an elegant training framework\nachieves state-of-the-art performance on the large scale BDD100K dataset, in an\nuniform setting, chosen, to the best of our knowledge, for the first time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_U/0/1/0/all/0/1\">Ujjal Kr Dutta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MASTAF: A Model-Agnostic Spatio-Temporal Attention Fusion Network for Few-shot Video Classification. (arXiv:2112.04585v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04585","description":"<p>We propose MASTAF, a Model-Agnostic Spatio-Temporal Attention Fusion network\nfor few-shot video classification. MASTAF takes input from a general video\nspatial and temporal representation,e.g., using 2D CNN, 3D CNN, and video\nTransformer. Then, to make the most of such representations, we use self- and\ncross-attention models to highlight the critical spatio-temporal region to\nincrease the inter-class distance and decrease the intra-class distance. Last,\nMASTAF applies a lightweight fusion network and a nearest neighbor classifier\nto classify each query video. We demonstrate that MASTAF improves the\nstate-of-the-art performance on three few-shot video classification\nbenchmarks(UCF101, HMDB51, and Something-Something-V2), e.g., by up to 91.6%,\n69.5%, and 60.7% for five-way one-shot video classification, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rex Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huanle Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. (arXiv:2112.10741v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10741","description":"<p>Diffusion models have recently been shown to generate high-quality synthetic\nimages, especially when paired with a guidance technique to trade off diversity\nfor fidelity. We explore diffusion models for the problem of text-conditional\nimage synthesis and compare two different guidance strategies: CLIP guidance\nand classifier-free guidance. We find that the latter is preferred by human\nevaluators for both photorealism and caption similarity, and often produces\nphotorealistic samples. Samples from a 3.5 billion parameter text-conditional\ndiffusion model using classifier-free guidance are favored by human evaluators\nto those from DALL-E, even when the latter uses expensive CLIP reranking.\nAdditionally, we find that our models can be fine-tuned to perform image\ninpainting, enabling powerful text-driven image editing. We train a smaller\nmodel on a filtered dataset and release the code and weights at\nhttps://github.com/openai/glide-text2im.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nichol_A/0/1/0/all/0/1\">Alex Nichol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhariwal_P/0/1/0/all/0/1\">Prafulla Dhariwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_A/0/1/0/all/0/1\">Aditya Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shyam_P/0/1/0/all/0/1\">Pranav Shyam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_P/0/1/0/all/0/1\">Pamela Mishkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGrew_B/0/1/0/all/0/1\">Bob McGrew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutskever_I/0/1/0/all/0/1\">Ilya Sutskever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mark Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity Background Subtraction. (arXiv:2201.04756v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04756","description":"<p>In this paper, we developed the solution of roadside LiDAR object detection\nusing a combination of two unsupervised learning algorithms. The 3D point\nclouds are firstly converted into spherical coordinates and filled into the\nelevation-azimuth matrix using a hash function. After that, the raw LiDAR data\nwere rearranged into new data structures to store the information of range,\nazimuth, and intensity. Then, the Dynamic Mode Decomposition method is applied\nto decompose the LiDAR data into low-rank backgrounds and sparse foregrounds\nbased on intensity channel pattern recognition. The Coarse Fine Triangle\nAlgorithm (CFTA) automatically finds the dividing value to separate the moving\ntargets from static background according to range information. After intensity\nand range background subtraction, the foreground moving objects will be\ndetected using a density-based detector and encoded into the state-space model\nfor tracking. The output of the proposed solution includes vehicle trajectories\nthat can enable many mobility and safety applications. The method was validated\nat both path and point levels and outperformed the state-of-the-art. In\ncontrast to the previous methods that process directly on the scattered and\ndiscrete point clouds, the dynamic classification method can establish the less\nsophisticated linear relationship of the 3D measurement data, which captures\nthe spatial-temporal structure that we often desire.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Peter J. Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Singapore Trusted Media Challenge Dataset. (arXiv:2201.04788v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04788","description":"<p>The development of powerful deep learning technologies has brought about some\nnegative effects to both society and individuals. One such issue is the\nemergence of fake media. To tackle the issue, we have organized the Trusted\nMedia Challenge (TMC) to explore how Artificial Intelligence (AI) technologies\ncould be leveraged to combat fake media.\n</p>\n<p>To enable further research, we are releasing the dataset that we had prepared\nfrom the TMC challenge, consisting of 4,380 fake and 2,563 real videos, with\nvarious video and/or audio manipulation methods employed to produce different\ntypes of fake media. All the videos in the TMC dataset are accompanied with\naudios and have a minimum resolution of 360p. The videos have various\ndurations, background, illumination, and may contain perturbations that mimic\ntransmission errors and compression.\n</p>\n<p>We have also carried out a user study to demonstrate the quality of the TMC\ndataset and to compare the performance of humans and AI models. The results\nshowed that the TMC dataset can fool human participants in many cases, and the\nwinning AI models of the Trusted Media Challenge outperformed humans.\n</p>\n<p>The TMC dataset is available for research purpose upon request via\ntmc-dataset@aisingapore.org.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weiling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_B/0/1/0/all/0/1\">Benjamin Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1\">Stefan Winkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See Kiong Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible Style Image Super-Resolution using Conditional Objective. (arXiv:2201.04898v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04898","description":"<p>Recent studies have significantly enhanced the performance of single-image\nsuper-resolution (SR) using convolutional neural networks (CNNs). While there\ncan be many high-resolution (HR) solutions for a given input, most existing\nCNN-based methods do not explore alternative solutions during the inference. A\ntypical approach to obtaining alternative SR results is to train multiple SR\nmodels with different loss weightings and exploit the combination of these\nmodels. Instead of using multiple models, we present a more efficient method to\ntrain a single adjustable SR model on various combinations of losses by taking\nadvantage of multi-task learning. Specifically, we optimize an SR model with a\nconditional objective during training, where the objective is a weighted sum of\nmultiple perceptual losses at different feature levels. The weights vary\naccording to given conditions, and the set of weights is defined as a style\ncontroller. Also, we present an architecture appropriate for this training\nscheme, which is the Residual-in-Residual Dense Block equipped with spatial\nfeature transformation layers. At the inference phase, our trained model can\ngenerate locally different outputs conditioned on the style control map.\nExtensive experiments show that the proposed SR model produces various\ndesirable reconstructions without artifacts and yields comparable quantitative\nperformance to state-of-the-art SR methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seung Ho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_Y/0/1/0/all/0/1\">Young Su Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. (arXiv:2201.07207v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.07207","description":"<p>Can world knowledge learned by large language models (LLMs) be used to act in\ninteractive environments? In this paper, we investigate the possibility of\ngrounding high-level tasks, expressed in natural language (e.g. \"make\nbreakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While\nprior work focused on learning from explicit step-by-step examples of how to\nact, we surprisingly find that if pre-trained LMs are large enough and prompted\nappropriately, they can effectively decompose high-level tasks into mid-level\nplans without any further training. However, the plans produced naively by LLMs\noften cannot map precisely to admissible actions. We propose a procedure that\nconditions on existing demonstrations and semantically translates the plans to\nadmissible actions. Our evaluation in the recent VirtualHome environment shows\nthat the resulting method substantially improves executability over the LLM\nbaseline. The conducted human evaluation reveals a trade-off between\nexecutability and correctness but shows a promising sign towards extracting\nactionable knowledge from language models. Website at\nhttps://huangwl18.github.io/language-planner\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenlong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-NeRF: Point-based Neural Radiance Fields. (arXiv:2201.08845v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08845","description":"<p>Volumetric neural rendering methods like NeRF generate high-quality view\nsynthesis results but are optimized per-scene leading to prohibitive\nreconstruction time. On the other hand, deep multi-view stereo methods can\nquickly reconstruct scene geometry via direct network inference. Point-NeRF\ncombines the advantages of these two approaches by using neural 3D point\nclouds, with associated neural features, to model a radiance field. Point-NeRF\ncan be rendered efficiently by aggregating neural point features near scene\nsurfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can\nbe initialized via direct inference of a pre-trained deep network to produce a\nneural point cloud; this point cloud can be finetuned to surpass the visual\nquality of NeRF with 30X faster training time. Point-NeRF can be combined with\nother 3D reconstruction methods and handles the errors and outliers in such\nmethods via a novel pruning and growing mechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiangeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zexiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1\">Julien Philip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sai Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhixin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1\">Ulrich Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Object Counting with Similarity-Aware Feature Enhancement. (arXiv:2201.08959v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08959","description":"<p>This work studies the problem of few-shot object counting, which counts the\nnumber of exemplar objects (i.e., described by one or several support images)\noccurring in the query image. The major challenge lies in that the target\nobjects can be densely packed in the query image, making it hard to recognize\nevery single one. To tackle the obstacle, we propose a novel learning block,\nequipped with a similarity comparison module (SCM) and a feature enhancement\nmodule (FEM). Concretely, given a support image and a query image, we first\nderive a score map by comparing their projected features at every spatial\nposition. The score maps regarding all support images are collected together\nand normalized across both the exemplar dimension and the spatial dimensions,\nproducing a reliable similarity map. We then enhance the query feature with the\nsupport features by employing the developed point-wise similarities as the\nweighting coefficients. Such a design encourages the model to inspect the query\nimage by focusing more on the regions akin to the support images, leading to\nmuch clearer boundaries between different objects. Extensive experiments on\nvarious benchmarks and training setups suggest that our method surpasses the\nstate-of-the-art approaches by a sufficiently large margin. For instance, on\nthe very recent large-scale FSC-147 dataset, we beat the second competitor by\nimproving the mean absolute counting error from 22.08 to 14.32 (35%\n$\\uparrow$). Code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhiyuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_X/0/1/0/all/0/1\">Xinyi Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Writer Recognition Using Off-line Handwritten Single Block Characters. (arXiv:2201.10665v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10665","description":"<p>Block characters are often used when filling paper forms for a variety of\npurposes. We investigate if there is biometric information contained within\nindividual digits of handwritten text. In particular, we use personal identity\nnumbers consisting of the six digits of the date of birth, DoB. We evaluate two\nrecognition approaches, one based on handcrafted features that compute contour\ndirectional measurements, and another based on deep features from a ResNet50\nmodel. We use a self-captured database of 317 individuals and 4920 written DoBs\nin total. Results show the presence of identity-related information in a piece\nof handwritten information as small as six digits with the DoB. We also analyze\nthe impact of the amount of enrolment samples, varying its number between one\nand ten. Results with such small amount of data are promising. With ten\nenrolment samples, the Top-1 accuracy with deep features is around 94%, and\nreaches nearly 100% by Top-10. The verification accuracy is more modest, with\nEER&gt;20%with any given feature and enrolment set size, showing that there is\nstill room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagstrom_A/0/1/0/all/0/1\">Adrian Leo Hagstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanikzai_R/0/1/0/all/0/1\">Rustam Stanikzai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1\">Josef Bigun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Issues of TrueDepth Sensor Data for Computer Vision Tasks Across Different iPad Generations. (arXiv:2201.10865v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10865","description":"<p>In 2017 Apple introduced the TrueDepth sensor with the iPhone X release.\nAlthough its primary use case is biometric face recognition, the exploitation\nof accurate depth data for other computer vision tasks like segmentation,\nportrait image generation and metric 3D reconstruction seems natural and lead\nto the development of various applications. In this report, we investigate the\nreliability of TrueDepth data - accessed through two different APIs - on\nvarious devices including different iPhone and iPad generations and reveal two\ndifferent and significant issues on all tested iPads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Urban_S/0/1/0/all/0/1\">Steffen Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindemeier_T/0/1/0/all/0/1\">Thomas Lindemeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbelstein_D/0/1/0/all/0/1\">David Dobbelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haenel_M/0/1/0/all/0/1\">Matthias Haenel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale Point Clouds. (arXiv:2201.12769v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12769","description":"<p>Semantic segmentation of 3D point cloud is an essential task for autonomous\ndriving environment perception. The pipeline of most pointwise point cloud\nsemantic segmentation methods includes points sampling, neighbor searching,\nfeature aggregation, and classification. Neighbor searching method like\nK-nearest neighbors algorithm, KNN, has been widely applied. However, the\ncomplexity of KNN is always a bottleneck of efficiency. In this paper, we\npropose an end-to-end neural architecture, Multiple View Pointwise Net,\nMVP-Net, to efficiently and directly infer large-scale outdoor point cloud\nwithout KNN or any complex pre/postprocessing. Instead, assumption-based space\nfilling curves and multi-rotation of point cloud methods are introduced to\npoint feature aggregation and receptive field expanding. Numerical experiments\nshow that the proposed MVP-Net is 11 times faster than the most efficient\npointwise semantic segmentation method RandLA-Net and achieves the same\naccuracy on the large-scale benchmark SemanticKITTI dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chuanyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Nuo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Han Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shengguang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge-Selective Feature Weaving for Point Cloud Matching. (arXiv:2202.02149v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02149","description":"<p>This paper tackles the problem of accurately matching the points of two 3D\npoint clouds. Most conventional methods improve their performance by extracting\nrepresentative features from each point via deep-learning-based algorithms. On\nthe other hand, the correspondence calculation between the extracted features\nhas not been examined in depth, and non-trainable algorithms (e.g. the Sinkhorn\nalgorithm) are frequently applied. As a result, the extracted features may be\nforcibly fitted to a non-trainable algorithm. Furthermore, the extracted\nfeatures frequently contain stochastically unavoidable errors, which degrades\nthe matching accuracy. In this paper, instead of using a non-trainable\nalgorithm, we propose a differentiable matching network that can be jointly\noptimized with the feature extraction procedure. Our network first constructs\ngraphs with edges connecting the points of each point cloud and then extracts\ndiscriminative edge features by using two main components: a shared set-encoder\nand an edge-selective cross-concatenation. These components enable us to\nsymmetrically consider two point clouds and to extract discriminative edge\nfeatures, respectively. By using the extracted discriminative edge features,\nour network can accurately calculate the correspondence between points. Our\nexperimental results show that the proposed network can significantly improve\nthe performance of point cloud matching. Our code is available at\nhttps://github.com/yanarin/ESFW\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yanagi_R/0/1/0/all/0/1\">Rintaro Yanagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1\">Atsushi Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sone_S/0/1/0/all/0/1\">Shusaku Sone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiba_N/0/1/0/all/0/1\">Naoya Chiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1\">Yoshitaka Ushiku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Attention Network. (arXiv:2202.09741v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09741","description":"<p>While originally designed for natural language processing tasks, the\nself-attention mechanism has recently taken various computer vision areas by\nstorm. However, the 2D nature of images brings three challenges for applying\nself-attention in computer vision. (1) Treating images as 1D sequences neglects\ntheir 2D structures. (2) The quadratic complexity is too expensive for\nhigh-resolution images. (3) It only captures spatial adaptability but ignores\nchannel adaptability. In this paper, we propose a novel large kernel attention\n(LKA) module to enable self-adaptive and long-range correlations in\nself-attention while avoiding the above issues. We further introduce a novel\nneural network based on LKA, namely Visual Attention Network (VAN). While\nextremely simple, VAN outperforms the state-of-the-art vision transformers and\nconvolutional neural networks with a large margin in extensive experiments,\nincluding image classification, object detection, semantic segmentation,\ninstance segmentation, etc. Code is available at\nhttps://github.com/Visual-Attention-Network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Meng-Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng-Ze Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng-Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi-Min Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Effective and Robust Neural Trojan Defenses via Input Filtering. (arXiv:2202.12154v3 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2202.12154","description":"<p>Trojan attacks on deep neural networks are both dangerous and surreptitious.\nOver the past few years, Trojan attacks have advanced from using only a single\ninput-agnostic trigger and targeting only one class to using multiple,\ninput-specific triggers and targeting multiple classes. However, Trojan\ndefenses have not caught up with this development. Most defense methods still\nmake out-of-date assumptions about Trojan triggers and target classes, thus,\ncan be easily circumvented by modern Trojan attacks. To deal with this problem,\nwe propose two novel \"filtering\" defenses called Variational Input Filtering\n(VIF) and Adversarial Input Filtering (AIF) which leverage lossy data\ncompression and adversarial learning respectively to effectively purify all\npotential Trojan triggers in the input at run time without making assumptions\nabout the number of triggers/target classes or the input dependence property of\ntriggers. In addition, we introduce a new defense mechanism called\n\"Filtering-then-Contrasting\" (FtC) which helps avoid the drop in classification\naccuracy on clean data caused by \"filtering\", and combine it with VIF/AIF to\nderive new defenses of this kind. Extensive experimental results and ablation\nstudies show that our proposed defenses significantly outperform well-known\nbaseline defenses in mitigating five advanced Trojan attacks including two\nrecent state-of-the-art while being quite robust to small amounts of training\ndata and large-norm triggers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1\">Kien Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harikumar_H/0/1/0/all/0/1\">Haripriya Harikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_S/0/1/0/all/0/1\">Santu Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susilo_W/0/1/0/all/0/1\">Willy Susilo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Transferable Reward for Query Object Localization with Policy Adaptation. (arXiv:2202.12403v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12403","description":"<p>We propose a reinforcement learning based approach to query object\nlocalization, for which an agent is trained to localize objects of interest\nspecified by a small exemplary set. We learn a transferable reward signal\nformulated using the exemplary set by ordinal metric learning. Our proposed\nmethod enables test-time policy adaptation to new environments where the reward\nsignals are not readily available, and outperforms fine-tuning approaches that\nare limited to annotated images. In addition, the transferable reward allows\nrepurposing the trained agent from one specific class to another class.\nExperiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tingfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shaobo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_M/0/1/0/all/0/1\">Martin Renqiang Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffeomorphic Image Registration with Neural Velocity Field. (arXiv:2202.12498v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12498","description":"<p>Diffeomorphic image registration is a crucial task in medical image analysis.\nRecent learning-based image registration methods utilize convolutional neural\nnetworks (CNN) to learn the spatial transformation between image pairs and\nachieve a fast inference speed. However, these methods often require a large\nnumber of training data to improve their generalization abilities. During the\ntest time, learning-based methods might fail to provide a good registration\nresult, which is likely because of the model overfitting on the training\ndataset. In this paper, we propose a neural representation of continuous\nvelocity field (NeVF) to describe the deformations across two images.\nSpecifically, this neural velocity field assigns a velocity vector to each\npoint in the space, which has higher flexibility in modeling the complex\ndeformation field. Furthermore, we propose a simple sparse-sampling strategy to\nreduce the memory consumption for the diffeomorphic registration. The proposed\nNeVF can also incorporate with a pre-trained learning-based model whose\npredicted deformation is taken as an initial state for optimization. Extensive\nexperiments conducted on two large-scale 3D MR brain scan datasets demonstrate\nthat our proposed method outperforms the state-of-the-art registration methods\nby a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shanlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deying Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiangyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohui Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Effective Subnetworks with Gumebel-Softmax. (arXiv:2202.12986v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12986","description":"<p>Large and performant neural networks are often overparameterized and can be\ndrastically reduced in size and complexity thanks to pruning. Pruning is a\ngroup of methods, which seeks to remove redundant or unnecessary weights or\ngroups of weights in a network. These techniques allow the creation of\nlightweight networks, which are particularly critical in embedded or mobile\napplications. In this paper, we devise an alternative pruning method that\nallows extracting effective subnetworks from larger untrained ones. Our method\nis stochastic and extracts subnetworks by exploring different topologies which\nare sampled using Gumbel Softmax. The latter is also used to train probability\ndistributions which measure the relevance of weights in the sampled topologies.\nThe resulting subnetworks are further enhanced using a highly efficient\nrescaling mechanism that reduces training time and improves performance.\nExtensive experiments conducted on CIFAR10 show the outperformance of our\nsubnetwork extraction method against the related work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dupont_R/0/1/0/all/0/1\">Robin Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alaoui_M/0/1/0/all/0/1\">Mohammed Amine Alaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1\">Hichem Sahbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebois_A/0/1/0/all/0/1\">Alice Lebois</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Bidirectional Arbitrary Image Rescaling: Joint Optimization and Cycle Idempotence. (arXiv:2203.00911v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.00911","description":"<p>Deep learning based single image super-resolution models have been widely\nstudied and superb results are achieved in upscaling low-resolution images with\nfixed scale factor and downscaling degradation kernel. To improve real world\napplicability of such models, there are growing interests to develop models\noptimized for arbitrary upscaling factors. Our proposed method is the first to\ntreat arbitrary rescaling, both upscaling and downscaling, as one unified\nprocess. Using joint optimization of both directions, the proposed model is\nable to learn upscaling and downscaling simultaneously and achieve\nbidirectional arbitrary image rescaling. It improves the performance of current\narbitrary upscaling models by a large margin while at the same time learns to\nmaintain visual perception quality in downscaled images. The proposed model is\nfurther shown to be robust in cycle idempotence test, free of severe\ndegradations in reconstruction accuracy when the downscaling-to-upscaling cycle\nis applied repetitively. This robustness is beneficial for image rescaling in\nthe wild when this cycle could be applied to one image for multiple times. It\nalso performs well on tests with arbitrary large scales and asymmetric scales,\neven when the model is not trained with such tasks. Extensive experiments are\nconducted to demonstrate the superior performance of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_Z/0/1/0/all/0/1\">Zhihong Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_M/0/1/0/all/0/1\">Mingde Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_T/0/1/0/all/0/1\">Tianwei Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recovering 3D Human Mesh from Monocular Images: A Survey. (arXiv:2203.01923v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01923","description":"<p>Estimating human pose and shape from monocular images is a long-standing\nproblem in computer vision. Since the release of statistical body models, 3D\nhuman mesh recovery has been drawing broader attention. With the same goal of\nobtaining well-aligned and physically plausible mesh results, two paradigms\nhave been developed to overcome challenges in the 2D-to-3D lifting process: i)\nan optimization-based paradigm, where different data terms and regularization\nterms are exploited as optimization objectives; and ii) a regression-based\nparadigm, where deep learning techniques are embraced to solve the problem in\nan end-to-end fashion. Meanwhile, continuous efforts are devoted to improving\nthe quality of 3D mesh labels for a wide range of datasets. Though remarkable\nprogress has been achieved in the past decade, the task is still challenging\ndue to flexible body motions, diverse appearances, complex environments, and\ninsufficient in-the-wild annotations. To the best of our knowledge, this is the\nfirst survey to focus on the task of monocular 3D human mesh recovery. We start\nwith the introduction of body models and then elaborate recovery frameworks and\ntraining objectives by providing in-depth analyses of their strengths and\nweaknesses. We also summarize datasets, evaluation metrics, and benchmark\nresults. Open issues and future directions are discussed in the end, hoping to\nmotivate researchers and facilitate their research in this area. A regularly\nupdated project page can be found at https://github.com/tinatiansjz/hmr-survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yating Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated and Generalized Person Re-identification through Domain and Feature Hallucinating. (arXiv:2203.02689v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02689","description":"<p>In this paper, we study the problem of federated domain generalization\n(FedDG) for person re-identification (re-ID), which aims to learn a generalized\nmodel with multiple decentralized labeled source domains. An empirical method\n(FedAvg) trains local models individually and averages them to obtain the\nglobal model for further local fine-tuning or deploying in unseen target\ndomains. One drawback of FedAvg is neglecting the data distributions of other\nclients during local training, making the local model overfit local data and\nproducing a poorly-generalized global model. To solve this problem, we propose\na novel method, called \"Domain and Feature Hallucinating (DFH)\", to produce\ndiverse features for learning generalized local and global models.\nSpecifically, after each model aggregation process, we share the Domain-level\nFeature Statistics (DFS) among different clients without violating data\nprivacy. During local training, the DFS are used to synthesize novel domain\nstatistics with the proposed domain hallucinating, which is achieved by\nre-weighting DFS with random weights. Then, we propose feature hallucinating to\ndiversify local features by scaling and shifting them to the distribution of\nthe obtained novel domain. The synthesized novel features retain the original\npair-wise similarities, enabling us to utilize them to optimize the model in a\nsupervised manner. Extensive experiments verify that the proposed DFH can\neffectively improve the generalization ability of the global model. Our method\nachieves the state-of-the-art performance for FedDG on four large-scale re-ID\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fengxiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhiming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaozi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation. (arXiv:2203.02925v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02925","description":"<p>Weakly supervised temporal action localization aims to localize temporal\nboundaries of actions and simultaneously identify their categories with only\nvideo-level category labels. Many existing methods seek to generate pseudo\nlabels for bridging the discrepancy between classification and localization,\nbut usually only make use of limited contextual information for pseudo label\ngeneration. To alleviate this problem, we propose a representative snippet\nsummarization and propagation framework. Our method seeks to mine the\nrepresentative snippets in each video for propagating information between video\nsnippets to generate better pseudo labels. For each video, its own\nrepresentative snippets and the representative snippets from a memory bank are\npropagated to update the input features in an intra- and inter-video manner.\nThe pseudo labels are generated from the temporal class activation maps of the\nupdated features to rectify the predictions of the main branch. Our method\nobtains superior performance in comparison to the existing methods on two\nbenchmarks, THUMOS14 and ActivityNet1.3, achieving gains as high as 1.2% in\nterms of average mAP on THUMOS14.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Linjiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Texture for Fooling Person Detectors in the Physical World. (arXiv:2203.03373v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03373","description":"<p>Nowadays, cameras equipped with AI systems can capture and analyze images to\ndetect people automatically. However, the AI system can make mistakes when\nreceiving deliberately designed patterns in the real world, i.e., physical\nadversarial examples. Prior works have shown that it is possible to print\nadversarial patches on clothes to evade DNN-based person detectors. However,\nthese adversarial examples could have catastrophic drops in the attack success\nrate when the viewing angle (i.e., the camera's angle towards the object)\nchanges. To perform a multi-angle attack, we propose Adversarial Texture\n(AdvTexture). AdvTexture can cover clothes with arbitrary shapes so that people\nwearing such clothes can hide from person detectors from different viewing\nangles. We propose a generative method, named Toroidal-Cropping-based\nExpandable Generative Attack (TC-EGA), to craft AdvTexture with repetitive\nstructures. We printed several pieces of cloth with AdvTexure and then made\nT-shirts, skirts, and dresses in the physical world. Experiments showed that\nthese clothes could fool person detectors in the physical world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanhao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaopei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Networks for Image Classification and Reinforcement Learning using Graph representations. (arXiv:2203.03457v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.03457","description":"<p>In this paper, we will evaluate the performance of graph neural networks in\ntwo distinct domains: computer vision and reinforcement learning. In the\ncomputer vision section, we seek to learn whether a novel non-redundant\nrepresentation for images as graphs can improve performance over trivial pixel\nto node mapping on a graph-level prediction graph, specifically image\nclassification. For the reinforcement learning section, we seek to learn if\nexplicitly modeling solving a Rubik's cube as a graph problem can improve\nperformance over a standard model-free technique with no inductive bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_D/0/1/0/all/0/1\">David Steiner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}