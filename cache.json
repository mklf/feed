{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-05T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A single speaker is almost all you need for automatic speech recognition. (arXiv:2204.00618v1 [eess.AS])","link":"http://arxiv.org/abs/2204.00618","description":"<p>We explore the use of speech synthesis and voice conversion applied to\naugment datasets for automatic speech recognition (ASR) systems, in scenarios\nwith only one speaker available for the target language. Through extensive\nexperiments, we show that our approach achieves results compared to the\nstate-of-the-art (SOTA) and requires only one speaker in the target language\nduring speech synthesis/voice conversion model training. Finally, we show that\nit is possible to obtain promising results in the training of an ASR model with\nour data augmentation method and only a single real speaker in different target\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shulby_C/0/1/0/all/0/1\">Christopher Shulby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korolev_A/0/1/0/all/0/1\">Alexander Korolev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soares_A/0/1/0/all/0/1\">Anderson da Silva Soares</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aluisio_S/0/1/0/all/0/1\">Sandra Alu&#xed;sio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end multi-talker audio-visual ASR using an active speaker attention module. (arXiv:2204.00652v1 [cs.SD])","link":"http://arxiv.org/abs/2204.00652","description":"<p>This paper presents a new approach for end-to-end audio-visual multi-talker\nspeech recognition. The approach, referred to here as the visual context\nattention model (VCAM), is important because it uses the available video\ninformation to assign decoded text to one of multiple visible faces. This\nessentially resolves the label ambiguity issue associated with most\nmulti-talker modeling approaches which can decode multiple label strings but\ncannot assign the label strings to the correct speakers. This is implemented as\na transformer-transducer based end-to-end model and evaluated using a two\nspeaker audio-visual overlapping speech dataset created from YouTube videos. It\nis shown in the paper that the VCAM model improves performance with respect to\npreviously reported audio-only and audio-visual multi-talker ASR systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rose_R/0/1/0/all/0/1\">Richard Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siohan_O/0/1/0/all/0/1\">Olivier Siohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CipherDAug: Ciphertext based Data Augmentation for Neural Machine Translation. (arXiv:2204.00665v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00665","description":"<p>We propose a novel data-augmentation technique for neural machine translation\nbased on ROT-$k$ ciphertexts. ROT-$k$ is a simple letter substitution cipher\nthat replaces a letter in the plaintext with the $k$th letter after it in the\nalphabet. We first generate multiple ROT-$k$ ciphertexts using different values\nof $k$ for the plaintext which is the source side of the parallel data. We then\nleverage this enciphered training data along with the original parallel data\nvia multi-source training to improve neural machine translation. Our method,\nCipherDAug, uses a co-regularization-inspired training procedure, requires no\nexternal data sources other than the original training data, and uses a\nstandard Transformer to outperform strong data augmentation techniques on\nseveral datasets by a significant margin. This technique combines easily with\nexisting approaches to data augmentation, and yields particularly strong\nresults in low-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kambhatla_N/0/1/0/all/0/1\">Nishant Kambhatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Born_L/0/1/0/all/0/1\">Logan Born</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Anoop Sarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Argument Structure Extraction with Transfer Learning and Active Learning. (arXiv:2204.00707v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00707","description":"<p>The automation of extracting argument structures faces a pair of challenges\non (1) encoding long-term contexts to facilitate comprehensive understanding,\nand (2) improving data efficiency since constructing high-quality argument\nstructures is time-consuming. In this work, we propose a novel context-aware\nTransformer-based argument structure prediction model which, on five different\ndomains, significantly outperforms models that rely on features or only encode\nlimited contexts. To tackle the difficulty of data annotation, we examine two\ncomplementary methods: (i) transfer learning to leverage existing annotated\ndata to boost model performance in a new target domain, and (ii) active\nlearning to strategically identify a small amount of samples for annotation. We\nfurther propose model-independent sample acquisition strategies, which can be\ngeneralized to diverse domains. With extensive experiments, we show that our\nsimple-yet-effective acquisition strategies yield competitive results against\nthree strong comparisons. Combined with transfer learning, substantial F1 score\nboost (5-25) can be further achieved during the early iterations of active\nlearning across domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xinyu Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CharacterBERT and Self-Teaching for Improving the Robustness of Dense Retrievers on Queries with Typos. (arXiv:2204.00716v1 [cs.IR])","link":"http://arxiv.org/abs/2204.00716","description":"<p>Previous work has shown that dense retrievers are not robust to out-of-domain\nand outlier queries, i.e. their effectiveness on these queries is much poorer\nthan what expected. In this paper, we consider a specific instance of such\nqueries: queries that contain typos. We show that a small character level\nperturbation in queries (as caused by typos) highly impacts the effectiveness\nof dense retrievers. We then demonstrate that the root cause of this resides in\nthe input tokenization strategy employed by BERT. In BERT, tokenization is\nperformed using the BERT's WordPiece tokenizer and we show that a token with a\ntypo will significantly change the token distributions obtained after\ntokenization. This distribution change translates to changes in the input\nembeddings passed to the BERT-based query encoder of dense retrievers. We then\nturn our attention to devising dense retriever methods that are robust to such\ntypo queries, while still being as performant as previous methods on queries\nwithout typos. For this, we use CharacterBERT as the backbone encoder and an\nefficient yet effective training method, called Self-Teaching (ST), that\ndistills knowledge from queries without typos into the queries with typos.\nExperimental results show that CharacterBERT in combination with ST achieves\nsignificantly higher effectiveness on queries with typos compared to previous\nmethods. Along with these results and the open-sourced implementation of the\nmethods, we also provide a new passage retrieval dataset consisting of\nreal-world queries with typos and associated relevance assessments on the MS\nMARCO corpus, thus supporting the research community in the investigation of\neffective and robust dense retrievers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shengyao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Simplify with Data Hopelessly Out of Alignment. (arXiv:2204.00741v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00741","description":"<p>We consider whether it is possible to do text simplification without relying\non a \"parallel\" corpus, one that is made up of sentence-by-sentence alignments\nof complex and ground truth simple sentences. To this end, we introduce a\nnumber of concepts, some new and some not, including what we call Conjoined\nTwin Networks, Flip-Flop Auto-Encoders (FFA) and Adversarial Networks (GAN). A\ncomparison is made between Jensen-Shannon (JS-GAN) and Wasserstein GAN, to see\nhow they impact performance, with stronger results for the former. An\nexperiment we conducted with a large dataset derived from Wikipedia found the\nsolid superiority of Twin Networks equipped with FFA and JS-GAN, over the\ncurrent best performing system. Furthermore, we discuss where we stand in a\nrelation to fully supervised methods in the past literature, and highlight with\nexamples qualitative differences that exist among simplified sentences\ngenerated by supervision-free systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nomoto_T/0/1/0/all/0/1\">Tadashi Nomoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating recommendations for entity-oriented exploratory search. (arXiv:2204.00743v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00743","description":"<p>We introduce the task of recommendation set generation for entity-oriented\nexploratory search. Given an input search query which is open-ended or\nunder-specified, the task is to present the user with an easily-understandable\ncollection of query recommendations, with the goal of facilitating domain\nexploration or clarifying user intent. Traditional query recommendation systems\nselect recommendations by identifying salient keywords in retrieved documents,\nor by querying an existing taxonomy or knowledge base for related concepts. In\nthis work, we build a text-to-text model capable of generating a collection of\nrecommendations directly, using the language model as a \"soft\" knowledge base\ncapable of proposing new concepts not found in an existing taxonomy or set of\nretrieved documents. We train the model to generate recommendation sets which\noptimize a cost function designed to encourage comprehensiveness,\ninterestingness, and non-redundancy. In thorough evaluations performed by crowd\nworkers, we confirm the generalizability of our approach and the high quality\nof the generated recommendations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nikita Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems. (arXiv:2204.00763v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00763","description":"<p>Task-oriented dialogue systems (TDSs) are assessed mainly in an offline\nsetting or through human evaluation. The evaluation is often limited to\nsingle-turn or very time-intensive. As an alternative, user simulators that\nmimic user behavior allow us to consider a broad set of user goals to generate\nhuman-like conversations for simulated evaluation. Employing existing user\nsimulators to evaluate TDSs is challenging as user simulators are primarily\ndesigned to optimize dialogue policies for TDSs and have limited evaluation\ncapability. Moreover, the evaluation of user simulators is an open challenge.\nIn this work, we proposes a metaphorical user simulator for endto-end TDS\nevaluation. We also propose a tester-based evaluation framework to generate\nvariants, i.e., dialogue systems with different capabilities. Our user\nsimulator constructs a metaphorical user model that assists the simulator in\nreasoning by referring to prior knowledge when encountering new items. We\nestimate the quality of simulators by checking the simulated interactions\nbetween simulators and variants. Our experiments are conducted using three TDS\ndatasets. The metaphorical user simulator demonstrates better consistency with\nmanual evaluation than Agenda-based simulator and Seq2seq model on three\ndatasets; our tester framework demonstrates efficiency, and our approach\ndemonstrates better generalization and scalability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shuyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CL-XABSA: Contrastive Learning for Cross-lingual Aspect-based Sentiment Analysis. (arXiv:2204.00791v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00791","description":"<p>As an extensive research in the field of Natural language processing (NLP),\naspect-based sentiment analysis (ABSA) is the task of predicting the sentiment\nexpressed in a text relative to the corresponding aspect. Unfortunately, most\nlanguages lack of sufficient annotation resources, thus more and more recent\nresearchers focus on cross-lingual aspect-based sentiment analysis (XABSA).\nHowever, most recent researches only concentrate on cross-lingual data\nalignment instead of model alignment. To this end, we propose a novel\nframework, CL-XABSA: Contrastive Learning for Cross-lingual Aspect-Based\nSentiment Analysis. Specifically, we design two contrastive strategies, token\nlevel contrastive learning of token embeddings (TL-CTE) and sentiment level\ncontrastive learning of token embeddings (SL-CTE), to regularize the semantic\nspace of source and target language to be more uniform. Since our framework can\nreceive datasets in multiple languages during training, our framework can be\nadapted not only for XABSA task, but also for multilingual aspect-based\nsentiment analysis (MABSA). To further improve the performance of our model, we\nperform knowledge distillation technology leveraging data from unlabeled target\nlanguage. In the distillation XABSA task, we further explore the comparative\neffectiveness of different data (source dataset, translated dataset, and\ncode-switched dataset). The results demonstrate that the proposed method has a\ncertain improvement in the three tasks of XABSA, distillation XABSA and MABSA.\nFor reproducibility, our code for this paper is available at\nhttps://github.com/GKLMIP/CL-XABSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaotian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aimin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual-Contrastive Framework for Low-Resource Cross-Lingual Named Entity Recognition. (arXiv:2204.00796v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00796","description":"<p>Cross-lingual Named Entity Recognition (NER) has recently become a research\nhotspot because it can alleviate the data-hungry problem for low-resource\nlanguages. However, few researches have focused on the scenario where the\nsource-language labeled data is also limited in some specific domains. A common\napproach for this scenario is to generate more training data through\ntranslation or generation-based data augmentation method. Unfortunately, we\nfind that simply combining source-language data and the corresponding\ntranslation cannot fully exploit the translated data and the improvements\nobtained are somewhat limited. In this paper, we describe our novel\ndual-contrastive framework ConCNER for cross-lingual NER under the scenario of\nlimited source-language labeled data. Specifically, based on the\nsource-language samples and their translations, we design two contrastive\nobjectives for cross-language NER at different grammatical levels, namely\nTranslation Contrastive Learning (TCL) to close sentence representations\nbetween translated sentence pairs and Label Contrastive Learning (LCL) to close\ntoken representations within the same labels. Furthermore, we utilize knowledge\ndistillation method where the NER model trained above is used as the teacher to\ntrain a student model on unlabeled target-language data to better fit the\ntarget language. We conduct extensive experiments on a wide variety of target\nlanguages, and the results demonstrate that ConCNER tends to outperform\nmultiple baseline methods. For reproducibility, our code for this paper is\navailable at https://github.com/GKLMIP/ConCNER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Factual Accuracy of Abstractive Clinical Text Summarization using Multi-Objective Optimization. (arXiv:2204.00797v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00797","description":"<p>While there has been recent progress in abstractive summarization as applied\nto different domains including news articles, scientific articles, and blog\nposts, the application of these techniques to clinical text summarization has\nbeen limited. This is primarily due to the lack of large-scale training data\nand the messy/unstructured nature of clinical notes as opposed to other domains\nwhere massive training data come in structured or semi-structured form.\nFurther, one of the least explored and critical components of clinical text\nsummarization is factual accuracy of clinical summaries. This is specifically\ncrucial in the healthcare domain, cardiology in particular, where an accurate\nsummary generation that preserves the facts in the source notes is critical to\nthe well-being of a patient. In this study, we propose a framework for\nimproving the factual accuracy of abstractive summarization of clinical text\nusing knowledge-guided multi-objective optimization. We propose to jointly\noptimize three cost functions in our proposed architecture during training:\ngenerative loss, entity loss and knowledge loss and evaluate the proposed\narchitecture on 1) clinical notes of patients with heart failure (HF), which we\ncollect for this study; and 2) two benchmark datasets, Indiana University Chest\nX-ray collection (IU X-Ray), and MIMIC-CXR, that are publicly available. We\nexperiment with three transformer encoder-decoder architectures and demonstrate\nthat optimizing different loss functions leads to improved performance in terms\nof entity-level factual accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alambo_A/0/1/0/all/0/1\">Amanuel Alambo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1\">Tanvi Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thirunarayan_K/0/1/0/all/0/1\">Krishnaprasad Thirunarayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cajita_M/0/1/0/all/0/1\">Mia Cajita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end model for named entity recognition from speech without paired training data. (arXiv:2204.00803v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00803","description":"<p>Recent works showed that end-to-end neural approaches tend to become very\npopular for spoken language understanding (SLU). Through the term end-to-end,\none considers the use of a single model optimized to extract semantic\ninformation directly from the speech signal. A major issue for such models is\nthe lack of paired audio and textual data with semantic annotation. In this\npaper, we propose an approach to build an end-to-end neural model to extract\nsemantic information in a scenario in which zero paired audio data is\navailable. Our approach is based on the use of an external model trained to\ngenerate a sequence of vectorial representations from text. These\nrepresentations mimic the hidden representations that could be generated inside\nan end-to-end automatic speech recognition (ASR) model by processing a speech\nsignal. An SLU neural module is then trained using these representations as\ninput and the annotated text as output. Last, the SLU module replaces the top\nlayers of the ASR model to achieve the construction of the end-to-end model.\nOur experiments on named entity recognition, carried out on the QUAERO corpus,\nshow that this approach is very promising, getting better results than a\ncomparable cascade approach or than the use of synthetic voices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mdhaffar_S/0/1/0/all/0/1\">Salima Mdhaffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duret_J/0/1/0/all/0/1\">Jarod Duret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HLDC: Hindi Legal Documents Corpus. (arXiv:2204.00806v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00806","description":"<p>Many populous countries including India are burdened with a considerable\nbacklog of legal cases. Development of automated systems that could process\nlegal documents and augment legal practitioners can mitigate this. However,\nthere is a dearth of high-quality corpora that is needed to develop such\ndata-driven systems. The problem gets even more pronounced in the case of low\nresource languages such as Hindi. In this resource paper, we introduce the\nHindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents\nin Hindi. Documents are cleaned and structured to enable the development of\ndownstream applications. Further, as a use-case for the corpus, we introduce\nthe task of bail prediction. We experiment with a battery of models and propose\na Multi-Task Learning (MTL) based model for the same. MTL models use\nsummarization as an auxiliary task along with bail prediction as the main task.\nExperiments with different models are indicative of the need for further\nresearch in this area. We release the corpus and model implementation code with\nthis paper: https://github.com/Exploration-Lab/HLDC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1\">Arnav Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhawan_M/0/1/0/all/0/1\">Mudit Dhawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Anmol Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arjun_T/0/1/0/all/0/1\">T.H. Arjun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar_A/0/1/0/all/0/1\">Akshala Bhatnagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_V/0/1/0/all/0/1\">Vibhu Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Amul Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arnab Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1\">Ashutosh Modi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Sequence-to-Tree Generation for Hierarchical Text Classification. (arXiv:2204.00811v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00811","description":"<p>Hierarchical Text Classification (HTC) is a challenging task where a document\ncan be assigned to multiple hierarchically structured categories within a\ntaxonomy. The majority of prior studies consider HTC as a flat multi-label\nclassification problem, which inevitably leads to \"label inconsistency\"\nproblem. In this paper, we formulate HTC as a sequence generation task and\nintroduce a sequence-to-tree framework (Seq2Tree) for modeling the hierarchical\nlabel structure. Moreover, we design a constrained decoding strategy with\ndynamic vocabulary to secure the label consistency of the results. Compared\nwith previous works, the proposed approach achieves significant and consistent\nimprovements on three benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yue Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Longjun Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient comparison of sentence embeddings. (arXiv:2204.00820v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00820","description":"<p>The domain of natural language processing (NLP), which has greatly evolved\nover the last years, has highly benefited from the recent developments in word\nand sentence embeddings. Such embeddings enable the transformation of complex\nNLP tasks, like semantic similarity or Question and Answering (Q\\&amp;A), into much\nsimpler to perform vector comparisons. However, such a problem transformation\nraises new challenges like the efficient comparison of embeddings and their\nmanipulation. In this work, we will discuss about various word and sentence\nembeddings algorithms, we will select a sentence embedding algorithm, BERT, as\nour algorithm of choice and we will evaluate the performance of two vector\ncomparison approaches, FAISS and Elasticsearch, in the specific problem of\nsentence embeddings. According to the results, FAISS outperforms Elasticsearch\nwhen used in a centralized environment with only one node, especially when big\ndatasets are included.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zoupanos_S/0/1/0/all/0/1\">Spyros Zoupanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolovos_S/0/1/0/all/0/1\">Stratis Kolovos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanavos_A/0/1/0/all/0/1\">Athanasios Kanavos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_O/0/1/0/all/0/1\">Orestis Papadimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maragoudakis_M/0/1/0/all/0/1\">Manolis Maragoudakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation. (arXiv:2204.00862v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00862","description":"<p>Existing reference-free metrics have obvious limitations for evaluating\ncontrolled text generation models. Unsupervised metrics can only provide a\ntask-agnostic evaluation result which correlates weakly with human judgments,\nwhereas supervised ones may overfit task-specific data with poor generalization\nability to other datasets. In this paper, we propose an unsupervised\nreference-free metric called CTRLEval, which evaluates controlled text\ngeneration from different aspects by formulating each aspect into multiple text\ninfilling tasks. On top of these tasks, the metric assembles the generation\nprobabilities from a pre-trained language model without any model training.\nExperimental results show that our metric has higher correlations with human\njudgments than other baselines, while obtaining better generalization of\nevaluating generated texts from different models and with different qualities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Online Posterior Alignments for Principled Lexically-Constrained Decoding. (arXiv:2204.00871v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00871","description":"<p>Online alignment in machine translation refers to the task of aligning a\ntarget word to a source word when the target sequence has only been partially\ndecoded. Good online alignments facilitate important applications such as\nlexically constrained translation where user-defined dictionaries are used to\ninject lexical constraints into the translation model. We propose a novel\nposterior alignment technique that is truly online in its execution and\nsuperior in terms of alignment error rates compared to existing methods. Our\nproposed inference technique jointly considers alignment and token\nprobabilities in a principled manner and can be seamlessly integrated within\nexisting constrained beam-search decoding algorithms. On five language pairs,\nincluding two distant language pairs, we achieve consistent drop in alignment\nerror rates. When deployed on seven lexically constrained translation tasks, we\nachieve significant improvements in BLEU specifically around the constrained\npositions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumya Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-VQA : Answering by Interactive Sub Question Sequence. (arXiv:2204.00879v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00879","description":"<p>Most existing approaches to Visual Question Answering (VQA) answer questions\ndirectly, however, people usually decompose a complex question into a sequence\nof simple sub questions and finally obtain the answer to the original question\nafter answering the sub question sequence(SQS). By simulating the process, this\npaper proposes a conversation-based VQA (Co-VQA) framework, which consists of\nthree components: Questioner, Oracle, and Answerer. Questioner raises the sub\nquestions using an extending HRED model, and Oracle answers them one-by-one. An\nAdaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed,\nwhere the question-answer pair is used to update the visual representation\nsequentially. To perform supervised learning for each model, we introduce a\nwell-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2\ndatasets. Experimental results show that our method achieves state-of-the-art\non VQA-CP v2. Further analyses show that SQSs help build direct semantic\nconnections between questions and images, provide question-adaptive\nvariable-length reasoning chains, and with explicit interpretability as well as\nerror traceability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuxi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fangxiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huixing Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging. (arXiv:2204.00885v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00885","description":"<p>Prompting methods recently achieve impressive success in few-shot learning.\nThese methods modify input samples with prompt sentence pieces, and decode\nlabel tokens to map samples to corresponding labels. However, such a paradigm\nis very inefficient for the task of slot tagging. Since slot tagging samples\nare multiple consecutive words in a sentence, the prompting methods have to\nenumerate all n-grams token spans to find all the possible slots, which greatly\nslows down the prediction. To tackle this, we introduce an inverse paradigm for\nprompting. Different from the classic prompts mapping tokens to labels, we\nreversely predict slot values given slot types. Such inverse prompting only\nrequires a one-turn prediction for each slot type and greatly speeds up the\nprediction. Besides, we propose a novel Iterative Prediction Strategy, from\nwhich the model learns to refine predictions by considering the relations\nbetween different slot types. We find, somewhat surprisingly, the proposed\nmethod not only predicts faster but also significantly improves the effect\n(improve over 6.1 F1-scores on 10-shot setting) and achieves new\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yutai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xianzhen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moment-based Adversarial Training for Embodied Language Comprehension. (arXiv:2204.00889v1 [cs.RO])","link":"http://arxiv.org/abs/2204.00889","description":"<p>In this paper, we focus on a vision-and-language task in which a robot is\ninstructed to execute household tasks. Given an instruction such as \"Rinse off\na mug and place it in the coffee maker,\" the robot is required to locate the\nmug, wash it, and put it in the coffee maker. This is challenging because the\nrobot needs to break down the instruction sentences into subgoals and execute\nthem in the correct order. On the ALFRED benchmark, the performance of\nstate-of-the-art methods is still far lower than that of humans. This is\npartially because existing methods sometimes fail to infer subgoals that are\nnot explicitly specified in the instruction sentences. We propose Moment-based\nAdversarial Training (MAT), which uses two types of moments for perturbation\nupdates in adversarial training. We introduce MAT to the embedding spaces of\nthe instruction, subgoals, and state representations to handle their varieties.\nWe validated our method on the ALFRED benchmark, and the results demonstrated\nthat our method outperformed the baseline method for all the metrics on the\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_S/0/1/0/all/0/1\">Shintaro Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-Assisted Semantic Annotation Correction for Emotion-Related Questions. (arXiv:2204.00916v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00916","description":"<p>Annotated data have traditionally been used to provide the input for training\na supervised machine learning (ML) model. However, current pre-trained ML\nmodels for natural language processing (NLP) contain embedded linguistic\ninformation that can be used to inform the annotation process. We use the BERT\nneural language model to feed information back into an annotation task that\ninvolves semantic labelling of dialog behavior in a question-asking game called\nEmotion Twenty Questions (EMO20Q). First we describe the background of BERT,\nthe EMO20Q data, and assisted annotation tasks. Then we describe the methods\nfor fine-tuning BERT for the purpose of checking the annotated labels. To do\nthis, we use the paraphrase task as a way to check that all utterances with the\nsame annotation label are classified as paraphrases of each other. We show this\nmethod to be an effective way to assess and revise annotations of textual user\ndata with complex, utterance-level semantic labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazemzadeh_A/0/1/0/all/0/1\">Abe Kazemzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Local and Global Features in Transformer-based Extreme Multi-label Text Classification. (arXiv:2204.00933v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00933","description":"<p>Extreme multi-label text classification (XMTC) is the task of tagging each\ndocument with the relevant labels from a very large space of predefined\ncategories. Recently, large pre-trained Transformer models have made\nsignificant performance improvements in XMTC, which typically use the embedding\nof the special CLS token to represent the entire document semantics as a global\nfeature vector, and match it against candidate labels. However, we argue that\nsuch a global feature vector may not be sufficient to represent different\ngranularity levels of semantics in the document, and that complementing it with\nthe local word-level features could bring additional gains. Based on this\ninsight, we propose an approach that combines both the local and global\nfeatures produced by Transformer models to improve the prediction power of the\nclassifier. Our experiments show that the proposed model either outperforms or\nis comparable to the state-of-the-art methods on benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruohong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yau-Shian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tom Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1\">Likun Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Dialect Density Estimation for African American English. (arXiv:2204.00967v1 [eess.AS])","link":"http://arxiv.org/abs/2204.00967","description":"<p>In this paper, we explore automatic prediction of dialect density of the\nAfrican American English (AAE) dialect, where dialect density is defined as the\npercentage of words in an utterance that contain characteristics of the\nnon-standard dialect. We investigate several acoustic and language modeling\nfeatures, including the commonly used X-vector representation and ComParE\nfeature set, in addition to information extracted from ASR transcripts of the\naudio files and prosodic information. To address issues of limited labeled\ndata, we use a weakly supervised model to project prosodic and X-vector\nfeatures into low-dimensional task-relevant representations. An XGBoost model\nis then used to predict the speaker's dialect density from these features and\nshow which are most significant during inference. We evaluate the utility of\nthese features both alone and in combination for the given task. This work,\nwhich does not rely on hand-labeled transcripts, is performed on audio segments\nfrom the CORAAL database. We show a significant correlation between our\npredicted and ground truth dialect density measures for AAE speech in this\ndatabase and propose this work as a tool for explaining and mitigating bias in\nspeech technology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Johnson_A/0/1/0/all/0/1\">Alexander Johnson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Everson_K/0/1/0/all/0/1\">Kevin Everson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravi_V/0/1/0/all/0/1\">Vijay Ravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gladney_A/0/1/0/all/0/1\">Anissa Gladney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alwan_A/0/1/0/all/0/1\">Abeer Alwan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question-Driven Graph Fusion Network For Visual Question Answering. (arXiv:2204.00975v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00975","description":"<p>Existing Visual Question Answering (VQA) models have explored various visual\nrelationships between objects in the image to answer complex questions, which\ninevitably introduces irrelevant information brought by inaccurate object\ndetection and text grounding. To address the problem, we propose a\nQuestion-Driven Graph Fusion Network (QD-GFN). It first models semantic,\nspatial, and implicit visual relations in images by three graph attention\nnetworks, then question information is utilized to guide the aggregation\nprocess of the three graphs, further, our QD-GFN adopts an object filtering\nmechanism to remove question-irrelevant objects contained in the image.\nExperiment results demonstrate that our QD-GFN outperforms the prior\nstate-of-the-art on both VQA 2.0 and VQA-CP v2 datasets. Further analysis shows\nthat both the novel graph aggregation method and object filtering mechanism\nplay a significant role in improving the performance of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuxi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuncong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fangxiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Speech Based End-to-End Automated Speech Recognition (ASR) for Indian-English Accents. (arXiv:2204.00977v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00977","description":"<p>Automated Speech Recognition (ASR) is an interdisciplinary application of\ncomputer science and linguistics that enable us to derive the transcription\nfrom the uttered speech waveform. It finds several applications in Military\nlike High-performance fighter aircraft, helicopters, air-traffic controller.\nOther than military speech recognition is used in healthcare, persons with\ndisabilities and many more. ASR has been an active research area. Several\nmodels and algorithms for speech to text (STT) have been proposed. One of the\nmost recent is Mozilla Deep Speech, it is based on the Deep Speech research\npaper by Baidu. Deep Speech is a state-of-art speech recognition system is\ndeveloped using end-to-end deep learning, it is trained using well-optimized\nRecurrent Neural Network (RNN) training system utilizing multiple Graphical\nProcessing Units (GPUs). This training is mostly done using American-English\naccent datasets, which results in poor generalizability to other English\naccents. India is a land of vast diversity. This can even be seen in the\nspeech, there are several English accents which vary from state to state. In\nthis work, we have used transfer learning approach using most recent Deep\nSpeech model i.e., deepspeech-0.9.3 to develop an end-to-end speech recognition\nsystem for Indian-English accents. This work utilizes fine-tuning and data\nargumentation to further optimize and improve the Deep Speech ASR system. Indic\nTTS data of Indian-English accents is used for transfer learning and\nfine-tuning the pre-trained Deep Speech model. A general comparison is made\namong the untrained model, our trained model and other available speech\nrecognition services for Indian-English Accents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_P/0/1/0/all/0/1\">Priyank Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_B/0/1/0/all/0/1\">Bilal Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension. (arXiv:2204.00996v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00996","description":"<p>Multilingual pre-trained models are able to zero-shot transfer knowledge from\nrich-resource to low-resource languages in machine reading comprehension (MRC).\nHowever, inherent linguistic discrepancies in different languages could make\nanswer spans predicted by zero-shot transfer violate syntactic constraints of\nthe target language. In this paper, we propose a novel multilingual MRC\nframework equipped with a Siamese Semantic Disentanglement Model (SSDM) to\ndisassociate semantics from syntax in representations learned by multilingual\npre-trained models. To explicitly transfer only semantic knowledge to the\ntarget language, we propose two groups of losses tailored for semantic and\nsyntactic encoding and disentanglement. Experimental results on three\nmultilingual MRC datasets (i.e., XQuAD, MLQA, and TyDi QA) demonstrate the\neffectiveness of our proposed approach over models based on mBERT and XLM-100.\nCode is available at:https://github.com/wulinjuan/SSDM_MRC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_i/0/1/0/all/0/1\">injuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shaojuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zhiqiang Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiyong Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Efficiently Acquiring Annotations for Multilingual Models. (arXiv:2204.01016v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01016","description":"<p>When tasked with supporting multiple languages for a given problem, two\napproaches have arisen: training a model for each language with the annotation\nbudget divided equally among them, and training on a high-resource language\nfollowed by zero-shot transfer to the remaining languages. In this work, we\nshow that the strategy of joint learning across multiple languages using a\nsingle model performs substantially better than the aforementioned\nalternatives. We also demonstrate that active learning provides additional,\ncomplementary benefits. We show that this simple approach enables the model to\nbe data efficient by allowing it to arbitrate its annotation budget to query\nlanguages it is less certain on. We illustrate the effectiveness of our\nproposed method on a diverse set of tasks: a classification task with 4\nlanguages, a sequence tagging task with 4 languages and a dependency parsing\ntask with 5 languages. Our proposed method, whilst simple, substantially\noutperforms the other viable alternatives for building a model in a\nmultilingual setting under constrained budgets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Barun Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task2Dial: A Novel Task and Dataset for Commonsense enhanced Task-based Dialogue Grounded in Documents. (arXiv:2204.01061v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01061","description":"<p>This paper proposes a novel task on commonsense-enhanced task-based dialogue\ngrounded in documents and describes the Task2Dial dataset, a novel dataset of\ndocument-grounded task-based dialogues, where an Information Giver (IG)\nprovides instructions (by consulting a document) to an Information Follower\n(IF), so that the latter can successfully complete the task. In this unique\nsetting, the IF can ask clarification questions which may not be grounded in\nthe underlying document and require commonsense knowledge to be answered. The\nTask2Dial dataset poses new challenges: (1) its human reference texts show more\nlexical richness and variation than other document-grounded dialogue datasets;\n(2) generating from this set requires paraphrasing as instructional responses\nmight have been modified from the underlying document; (3) requires commonsense\nknowledge, since questions might not necessarily be grounded in the document;\n(4) generating requires planning based on context, as task steps need to be\nprovided in order. The Task2Dial dataset contains dialogues with an average\n$18.15$ number of turns and 19.79 tokens per turn, as compared to 12.94 and 12\nrespectively in existing datasets. As such, learning from this dataset promises\nmore natural, varied and less template-like system utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strathearn_C/0/1/0/all/0/1\">Carl Strathearn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkatzia_D/0/1/0/all/0/1\">Dimitra Gkatzia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A sequence-to-sequence approach for document-level relation extraction. (arXiv:2204.01098v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01098","description":"<p>Motivated by the fact that many relations cross the sentence boundary, there\nhas been increasing interest in document-level relation extraction (DocRE).\nDocRE requires integrating information within and across sentences, capturing\ncomplex interactions between mentions of entities. Most existing methods are\npipeline-based, requiring entities as input. However, jointly learning to\nextract entities and relations can improve performance and be more efficient\ndue to shared parameters and training steps. In this paper, we develop a\nsequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE\n(entity extraction, coreference resolution and relation extraction) end-to-end,\nreplacing a pipeline of task-specific components. Using a simple strategy we\ncall entity hinting, we compare our approach to existing pipeline-based methods\non several popular biomedical datasets, in some cases exceeding their\nperformance. We also report the first end-to-end results on these datasets for\nfuture comparison. Finally, we demonstrate that, under our model, an end-to-end\napproach outperforms a pipeline-based approach. Our code, data and trained\nmodels are available at {\\small{\\url{https://github.com/johngiorgi/seq2rel}}}.\nAn online demo is available at\n{\\small{\\url{https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py}}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1\">John Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bader_G/0/1/0/all/0/1\">Gary D. Bader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pragmatic constraints and pronoun reference disambiguation: the possible and the impossible. (arXiv:2204.01166v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01166","description":"<p>Pronoun disambiguation in understanding text and discourse often requires the\napplication of both general pragmatic knowledge and context-specific\ninformation. In AI and linguistics research, this has mostly been studied in\ncases where the referent is explicitly stated in the preceding text nearby.\nHowever, pronouns in natural text often refer to entities, collections, or\nevents that are only implicit mentioned previously; in those cases the need to\nuse pragmatic knowledge to disambiguate becomes much more acute and the\ncharacterization of the knowledge becomes much more difficult. Extended\nliterary texts at times employ both extremely complex patterns of reference and\nextremely rich and subtle forms of knowledge. Indeed, it is occasionally\npossible to have a pronoun that is far separated from its referent in a text.\nIn the opposite direction, pronoun use is affected by considerations of focus\nof attention and by formal constraints such as a preference for parallel\nsyntactic structures; these can be so strong that no pragmatic knowledge\nsuffices to overrule them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1\">Ernest Davis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01171","description":"<p>Current language generation models suffer from issues such as repetition,\nincoherence, and hallucinations. An often-repeated hypothesis is that this\nbrittleness of generation models is caused by the training and the generation\nprocedure mismatch, also referred to as exposure bias. In this paper, we verify\nthis hypothesis by analyzing exposure bias from an imitation learning\nperspective. We show that exposure bias leads to an accumulation of errors,\nanalyze why perplexity fails to capture this accumulation, and empirically show\nthat this accumulation results in poor generation quality. Source code to\nreproduce these experiments is available at\nhttps://github.com/kushalarora/quantifying_exposure_bias\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_K/0/1/0/all/0/1\">Kushal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asri_L/0/1/0/all/0/1\">Layla El Asri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahuleyan_H/0/1/0/all/0/1\">Hareesh Bahuleyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PERFECT: Prompt-free and Efficient Few-shot Learning with Language Models. (arXiv:2204.01172v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01172","description":"<p>Current methods for few-shot fine-tuning of pretrained masked language models\n(PLMs) require carefully engineered prompts and verbalizers for each new task\nto convert examples into a cloze-format that the PLM can score. In this work,\nwe propose PERFECT, a simple and efficient method for few-shot fine-tuning of\nPLMs without relying on any such handcrafting, which is highly effective given\nas few as 32 data points. PERFECT makes two key design choices: First, we show\nthat manually engineered task prompts can be replaced with task-specific\nadapters that enable sample-efficient fine-tuning and reduce memory and storage\ncosts by roughly factors of 5 and 100, respectively. Second, instead of using\nhandcrafted verbalizers, we learn new multi-token label embeddings during\nfine-tuning, which are not tied to the model vocabulary and which allow us to\navoid complex auto-regressive decoding. These embeddings are not only learnable\nfrom limited data but also enable nearly 100x faster training and inference.\nExperiments on a wide range of few-shot NLP tasks demonstrate that PERFECT,\nwhile being simple and efficient, also outperforms existing state-of-the-art\nfew-shot learning methods. Our code is publicly available at\nhttps://github.com/rabeehk/perfect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahabadi_R/0/1/0/all/0/1\">Rabeeh Karimi Mahabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1\">Marzieh Saeidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_L/0/1/0/all/0/1\">Lambert Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Veselin Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1\">Majid Yazdani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Part-of-Speech Tagger for Yiddish: First Steps in Tagging the Yiddish Book Center Corpus. (arXiv:2204.01175v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01175","description":"<p>We describe the construction and evaluation of a part-of-speech tagger for\nYiddish (the first one, to the best of our knowledge). This is the first step\nin a larger project of automatically assigning part-of-speech tags and\nsyntactic structure to Yiddish text for purposes of linguistic research. We\ncombine two resources for the current work - an 80K word subset of the Penn\nParsed Corpus of Historical Yiddish (PPCHY) (Santorini, 2021) and 650 million\nwords of OCR'd Yiddish text from the Yiddish Book Center (YBC). We compute word\nembeddings on the YBC corpus, and these embeddings are used with a tagger model\ntrained and evaluated on the PPCHY. Yiddish orthography in the YBC corpus has\nmany spelling inconsistencies, and we present some evidence that even simple\nnon-contextualized embeddings are able to capture the relationships among\nspelling variants without the need to first \"standardize\" the corpus. We\nevaluate the tagger performance on a 10-fold cross-validation split, with and\nwithout the embeddings, showing that the embeddings improve tagger performance.\nHowever, a great deal of work remains to be done, and we conclude by discussing\nsome next steps, including the need for additional annotated training and test\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulick_S/0/1/0/all/0/1\">Seth Kulick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryant_N/0/1/0/all/0/1\">Neville Ryant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santorini_B/0/1/0/all/0/1\">Beatrice Santorini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallenberg_J/0/1/0/all/0/1\">Joel Wallenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Text Generation via Variational Encoder-Decoder Models with Gaussian Process Priors. (arXiv:2204.01227v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01227","description":"<p>Generating high quality texts with high diversity is important for many NLG\napplications, but current methods mostly focus on building deterministic models\nto generate higher quality texts and do not provide many options for promoting\ndiversity. In this work, we present a novel latent structured variable model to\ngenerate high quality texts by enriching contextual representation learning of\nencoder-decoder models. Specifically, we introduce a stochastic function to map\ndeterministic encoder hidden states into random context variables. The proposed\nstochastic function is sampled from a Gaussian process prior to (1) provide\ninfinite number of joint Gaussian distributions of random context variables\n(diversity-promoting) and (2) explicitly model dependency between context\nvariables (accurate-encoding). To address the learning challenge of Gaussian\nprocesses, we propose an efficient variational inference approach to\napproximate the posterior distribution of random context variables. We evaluate\nour method in two typical text generation tasks: paraphrase generation and text\nstyle transfer. Experimental results on benchmark datasets demonstrate that our\nmethod improves the generation quality and diversity compared with other\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianqiao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Joint Speech-Text Embeddings for Semantic Matching. (arXiv:2204.01235v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01235","description":"<p>Embeddings play an important role in many recent end-to-end solutions for\nlanguage processing problems involving more than one data modality. Although\nthere has been some effort to understand the properties of single-modality\nembedding spaces, particularly that of text, their cross-modal counterparts are\nless understood. In this work, we study a joint speech-text embedding space\ntrained for semantic matching by minimizing the distance between paired\nutterance and transcription inputs. This was done through dual encoders in a\nteacher-student model setup, with a pretrained language model acting as the\nteacher and a transformer-based speech encoder as the student. We extend our\nmethod to incorporate automatic speech recognition through both pretraining and\nmultitask scenarios and found that both approaches improve semantic matching.\nMultiple techniques were utilized to analyze and evaluate cross-modal semantic\nalignment of the embeddings: a quantitative retrieval accuracy metric,\nzero-shot classification to investigate generalizability, and probing of the\nencoders to observe the extent of knowledge transfer from one modality to\nanother.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huzaifah_M/0/1/0/all/0/1\">Muhammad Huzaifah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukanov_I/0/1/0/all/0/1\">Ivan Kukanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pragmatics-Centered Evaluation Framework for Natural Language Understanding. (arXiv:1907.08672v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1907.08672","description":"<p>New models for natural language understanding have recently made an\nunparalleled amount of progress, which has led some researchers to suggest that\nthe models induce universal text representations. However, current benchmarks\nare predominantly targeting semantic phenomena; we make the case that\npragmatics needs to take center stage in the evaluation of natural language\nunderstanding. We introduce PragmEval, a new benchmark for the evaluation of\nnatural language understanding, that unites 11 pragmatics-focused evaluation\ndatasets for English. PragmEval can be used as supplementary training data in a\nmulti-task learning setup, and is publicly available, alongside the code for\ngathering and preprocessing the datasets. Using our evaluation suite, we show\nthat natural language inference, a widely used pretraining task, does not\nresult in genuinely universal representations, which presents a new challenge\nfor multi-task learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sileo_D/0/1/0/all/0/1\">Damien Sileo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Van_de_Cruys_T/0/1/0/all/0/1\">Tim Van-de-Cruys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradel_C/0/1/0/all/0/1\">Camille Pradel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_P/0/1/0/all/0/1\">Philippe Muller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perspective-corrected Spatial Referring Expression Generation for Human-Robot Interaction. (arXiv:2104.01558v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2104.01558","description":"<p>Intelligent robots designed to interact with humans in real scenarios need to\nbe able to refer to entities actively by natural language. In spatial referring\nexpression generation, the ambiguity is unavoidable due to the diversity of\nreference frames, which will lead to an understanding gap between humans and\nrobots. To narrow this gap, in this paper, we propose a novel\nperspective-corrected spatial referring expression generation (PcSREG) approach\nfor human-robot interaction by considering the selection of reference frames.\nThe task of referring expression generation is simplified into the process of\ngenerating diverse spatial relation units. First, we pick out all landmarks in\nthese spatial relation units according to the entropy of preference and allow\nits updating through a stack model. Then all possible referring expressions are\ngenerated according to different reference frame strategies. Finally, we\nevaluate every expression using a probabilistic referring expression resolution\nmodel and find the best expression that satisfies both of the appropriateness\nand effectiveness. We implement the proposed approach on a robot system and\nempirical experiments show that our approach can generate more effective\nspatial referring expressions for practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingjiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chengli Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chunlin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Word-Level Semantic Representation via Dependency Structure for Expressive Text-to-Speech Synthesis. (arXiv:2104.06835v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06835","description":"<p>Exploiting rich linguistic information in raw text is crucial for expressive\ntext-to-speech (TTS). As large scale pre-trained text representation develops,\nbidirectional encoder representations from Transformers (BERT) has been proven\nto embody semantic information and employed to TTS recently. However, original\nor simply fine-tuned BERT embeddings still cannot provide sufficient semantic\nknowledge that expressive TTS models should take into account. In this paper,\nwe propose a word-level semantic representation enhancing method based on\ndependency structure and pre-trained BERT embedding. The BERT embedding of each\nword is reprocessed considering its specific dependencies and related words in\nthe sentence, to generate more effective semantic representation for TTS. To\nbetter utilize the dependency structure, relational gated graph network (RGGN)\nis introduced to make semantic information flow and aggregate through the\ndependency structure. The experimental results show that the proposed method\ncan further improve the naturalness and expressiveness of synthesized speeches\non both Mandarin and English datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yixuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Changhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingbei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yanyao Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation. (arXiv:2104.08704v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08704","description":"<p>Large pretrained generative models like GPT-3 often suffer from hallucinating\nnon-existent or incorrect content, which undermines their potential merits in\nreal applications. Existing work usually attempts to detect these\nhallucinations based on a corresponding oracle reference at a sentence or\ndocument level. However ground-truth references may not be readily available\nfor many free-form text generation applications, and sentence- or\ndocument-level detection may fail to provide the fine-grained signals that\nwould prevent fallacious content in real time. As a first step to addressing\nthese issues, we propose a novel token-level, reference-free hallucination\ndetection task and an associated annotated dataset named HaDes (HAllucination\nDEtection dataSet). To create this dataset, we first perturb a large number of\ntext segments extracted from English language Wikipedia, and then verify these\nwith crowd-sourced annotations. To mitigate label imbalance during annotation,\nwe utilize an iterative model-in-loop strategy. We conduct comprehensive data\nanalyses and create multiple baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockett_C/0/1/0/all/0/1\">Chris Brockett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1\">Bill Dolan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OntoED: Low-resource Event Detection with Ontology Embedding. (arXiv:2105.10922v4 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2105.10922","description":"<p>Event Detection (ED) aims to identify event trigger words from a given text\nand classify it into an event type. Most of current methods to ED rely heavily\non training instances, and almost ignore the correlation of event types. Hence,\nthey tend to suffer from data scarcity and fail to handle new unseen event\ntypes. To address these problems, we formulate ED as a process of event\nontology population: linking event instances to pre-defined event types in\nevent ontology, and propose a novel ED framework entitled OntoED with ontology\nembedding. We enrich event ontology with linkages among event types, and\nfurther induce more event-event correlations. Based on the event ontology,\nOntoED can leverage and propagate correlation knowledge, particularly from\ndata-rich to data-poor event types. Furthermore, OntoED can be applied to new\nunseen event types, by establishing linkages to existing ones. Experiments\nindicate that OntoED is more predominant and robust than previous approaches to\nED, especially in data-scarce scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT Learns to Teach: Knowledge Distillation with Meta Learning. (arXiv:2106.04570v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.04570","description":"<p>We present Knowledge Distillation with Meta Learning (MetaDistil), a simple\nyet effective alternative to traditional knowledge distillation (KD) methods\nwhere the teacher model is fixed during training. We show the teacher network\ncan learn to better transfer knowledge to the student network (i.e., learning\nto teach) with the feedback from the performance of the distilled student\nnetwork in a meta learning framework. Moreover, we introduce a pilot update\nmechanism to improve the alignment between the inner-learner and meta-learner\nin meta learning algorithms that focus on an improved inner-learner.\nExperiments on various benchmarks show that MetaDistil can yield significant\nimprovements compared with traditional KD algorithms and is less sensitive to\nthe choice of different student capacity and hyperparameters, facilitating the\nuse of KD on different tasks and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MarIA: Spanish Language Models. (arXiv:2107.07253v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.07253","description":"<p>This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as\nwell as the corresponding performance evaluations. Both models were pre-trained\nusing the largest Spanish corpus known to date, with a total of 570GB of clean\nand deduplicated text processed for this work, compiled from the web crawlings\nperformed by the National Library of Spain from 2009 to 2019. We extended the\ncurrent evaluation datasets with an extractive Question Answering dataset and\nour models outperform the existing Spanish models across tasks and settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silveira_Ocampo_J/0/1/0/all/0/1\">Joaqu&#xed;n Silveira-Ocampo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1\">Carme Armentano-Oller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1\">Carlos Rodriguez-Penagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond NED: Fast and Effective Search Space Reduction for Complex Question Answering over Knowledge Bases. (arXiv:2108.08597v9 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08597","description":"<p>Answering complex questions over knowledge bases (KB-QA) faces huge input\ndata with billions of facts, involving millions of entities and thousands of\npredicates. For efficiency, QA systems first reduce the answer search space by\nidentifying a set of facts that is likely to contain all answers and relevant\ncues. The most common technique for doing this is to apply named entity\ndisambiguation (NED) systems to the question, and retrieve KB facts for the\ndisambiguated entities. This work presents CLOCQ, an efficient method that\nprunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses\na top-k query processor over score-ordered lists of KB items that combine\nsignals about lexical matching, relevance to the question, coherence among\ncandidate items, and connectivity in the KB graph. Experiments with two recent\nQA benchmarks for complex questions demonstrate the superiority of CLOCQ over\nstate-of-the-art baselines with respect to answer presence, size of the search\nspace, and runtimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1\">Philipp Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12584","description":"<p>In recent times, there has been definitive progress in the field of NLP, with\nits applications growing as the utility of our language models increases with\nadvances in their performance. However, these models require a large amount of\ncomputational power and data to train, consequently leading to large carbon\nfootprints. Therefore, it is imperative that we study the carbon efficiency and\nlook for alternatives to reduce the overall environmental impact of training\nmodels, in particular large language models. In our work, we assess the\nperformance of models for machine translation, across multiple language pairs\nto assess the difference in computational power required to train these models\nfor each of these language pairs and examine the various components of these\nmodels to analyze aspects of our pipeline that can be optimized to reduce these\ncarbon emissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yusuf_M/0/1/0/all/0/1\">Mirza Yusuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surana_P/0/1/0/all/0/1\">Praatibh Surana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gauri Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Input Length Matters: Improving RNN-T and MWER Training for Long-form Telephony Speech Recognition. (arXiv:2110.03841v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.03841","description":"<p>End-to-end models have achieved state-of-the-art results on several automatic\nspeech recognition tasks. However, they perform poorly when evaluated on\nlong-form data, e.g., minutes long conversational telephony audio. One reason\nthe model fails on long-form speech is that it has only seen short utterances\nduring training. In this paper we study the effect of training utterance length\non the word error rate (WER) for RNN-transducer (RNN-T) model. We compare two\nwidely used training objectives, log loss (or RNN-T loss) and minimum word\nerror rate (MWER) loss. We conduct experiments on telephony datasets in four\nlanguages. Our experiments show that for both losses, the WER on long-form\nspeech reduces substantially as the training utterance length increases. The\naverage relative WER gain is 15.7% for log loss and 8.8% for MWER loss. When\ntraining on short utterances, MWER loss leads to a lower WER than the log loss.\nSuch difference between the two losses diminishes when the input length\nincreases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyun Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yanwei Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doutre_T/0/1/0/all/0/1\">Thibault Doutre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haghani_P/0/1/0/all/0/1\">Parisa Haghani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Liangliang Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MReD: A Meta-Review Dataset for Structure-Controllable Text Generation. (arXiv:2110.07474v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07474","description":"<p>When directly using existing text generation datasets for controllable\ngeneration, we are facing the problem of not having the domain knowledge and\nthus the aspects that could be controlled are limited. A typical example is\nwhen using CNN/Daily Mail dataset for controllable text summarization, there is\nno guided information on the emphasis of summary sentences. A more useful text\ngenerator should leverage both the input text and the control signal to guide\nthe generation, which can only be built with a deep understanding of the domain\nknowledge. Motivated by this vision, our paper introduces a new text generation\ndataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its\n45k meta-review sentences are manually annotated with one of the 9 carefully\ndefined categories, including abstract, strength, decision, etc. We present\nexperimental results on start-of-the-art summarization models, and propose\nmethods for structure-controlled generation with both extractive and\nabstractive models using our annotated data. By exploring various settings and\nanalyzing the model behavior with respect to the control signal, we demonstrate\nthe challenges of our proposed task and the values of our dataset MReD.\nMeanwhile, MReD also allows us to have a better understanding of the\nmeta-review domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenhui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition. (arXiv:2110.07480v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07480","description":"<p>Nested entities are observed in many domains due to their compositionality,\nwhich cannot be easily recognized by the widely-used sequence labeling\nframework. A natural solution is to treat the task as a span classification\nproblem. To learn better span representation and increase classification\nperformance, it is crucial to effectively integrate heterogeneous factors\nincluding inside tokens, boundaries, labels, and related spans which could be\ncontributing to nested entities recognition. To fuse these heterogeneous\nfactors, we propose a novel triaffine mechanism including triaffine attention\nand scoring. Triaffine attention uses boundaries and labels as queries and uses\ninside tokens and related spans as keys and values for span representations.\nTriaffine scoring interacts with boundaries and span representations for\nclassification. Experiments show that our proposed method outperforms previous\nspan-based methods, achieves the state-of-the-art $F_1$ scores on nested NER\ndatasets GENIA and KBP2017, and shows comparable results on ACE2004 and\nACE2005.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeToxy: A Large-Scale Multimodal Dataset for Toxicity Classification in Spoken Utterances. (arXiv:2110.07592v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07592","description":"<p>Toxic speech, also known as hate speech, is regarded as one of the crucial\nissues plaguing online social media today. Most recent work on toxic speech\ndetection is constrained to the modality of text and written conversations with\nvery limited work on toxicity detection from spoken utterances or using the\nmodality of speech. In this paper, we introduce a new dataset DeToxy, the first\npublicly available toxicity annotated dataset for the English language. DeToxy\nis sourced from various openly available speech databases and consists of over\n2 million utterances. We believe that our dataset would act as a benchmark for\nthe relatively new and un-explored Spoken Language Processing task of detecting\ntoxicity from spoken utterances and boost further research in this space.\nFinally, we also provide strong unimodal baselines for our dataset and compare\ntraditional two-step and E2E approaches. Our experiments show that in the case\nof spoken utterances, text-based approaches are largely dependent on gold\nhuman-annotated transcripts for their performance and also suffer from the\nproblem of keyword bias. However, the presence of speech files in DeToxy helps\nfacilitates the development of E2E speech models which alleviate both the\nabove-stated problems by better capturing speech clues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepcha_S/0/1/0/all/0/1\">Samden Lepcha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakshi_S/0/1/0/all/0/1\">S Sakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Curriculum Learning for AMR Parsing. (arXiv:2110.07855v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07855","description":"<p>Abstract Meaning Representation (AMR) parsing aims to translate sentences to\nsemantic representation with a hierarchical structure, and is recently\nempowered by pretrained sequence-to-sequence models. However, there exists a\ngap between their flat training objective (i.e., equally treats all output\ntokens) and the hierarchical AMR structure, which limits the model\ngeneralization. To bridge this gap, we propose a Hierarchical Curriculum\nLearning (HCL) framework with Structure-level (SC) and Instance-level Curricula\n(IC). SC switches progressively from core to detail AMR semantic elements while\nIC transits from structure-simple to -complex AMR instances during training.\nThrough these two warming-up processes, HCL reduces the difficulty of learning\ncomplex structures, thus the flat model can better adapt to the AMR hierarchy.\nExtensive experiments on AMR2.0, AMR3.0, structure-complex and\nout-of-distribution situations verify the effectiveness of HCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark. (arXiv:2110.08466v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08466","description":"<p>Dialogue safety problems severely limit the real-world deployment of neural\nconversational models and have attracted great research interests recently.\nHowever, dialogue safety problems remain under-defined and the corresponding\ndataset is scarce. We propose a taxonomy for dialogue safety specifically\ndesigned to capture unsafe behaviors in human-bot dialogue settings, with\nfocuses on context-sensitive unsafety, which is under-explored in prior works.\nTo spur research in this direction, we compile DiaSafety, a dataset with rich\ncontext-sensitive unsafe examples. Experiments show that existing safety\nguarding tools fail severely on our dataset. As a remedy, we train a dialogue\nsafety classifier to provide a strong baseline for context-sensitive dialogue\nunsafety detection. With our classifier, we perform safety evaluations on\npopular conversational models and show that existing dialogue systems still\nexhibit concerning context-sensitive safety problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangxuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiawen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiale Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models. (arXiv:2110.08527v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08527","description":"<p>Recent work has shown pre-trained language models capture social biases from\nthe large amounts of text they are trained on. This has attracted attention to\ndeveloping techniques that mitigate such biases. In this work, we perform an\nempirical survey of five recently proposed bias mitigation techniques:\nCounterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace\nProjection, Self-Debias, and SentenceDebias. We quantify the effectiveness of\neach technique using three intrinsic bias benchmarks while also measuring the\nimpact of these techniques on a model's language modeling ability, as well as\nits performance on downstream NLU tasks. We experimentally find that: (1)\nSelf-Debias is the strongest debiasing technique, obtaining improved scores on\nall bias benchmarks; (2) Current debiasing techniques perform less consistently\nwhen mitigating non-gender biases; And (3) improvements on bias benchmarks such\nas StereoSet and CrowS-Pairs by using debiasing strategies are often\naccompanied by a decrease in language modeling ability, making it difficult to\ndetermine whether the bias mitigation was effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meade_N/0/1/0/all/0/1\">Nicholas Meade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poole_Dayan_E/0/1/0/all/0/1\">Elinor Poole-Dayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DECAR: Deep Clustering for learning general-purpose Audio Representations. (arXiv:2110.08895v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.08895","description":"<p>In this paper, we introduce DECAR (DEep Clustering for learning\ngeneral-purpose Audio Representations), a self-supervised pre-training approach\nfor learning general-purpose audio representations. Our system is based on\nclustering: it utilizes an offline clustering step to produce pseudo-labels and\ntrains the network with a classification loss supervised by these\npseudo-labels. We develop on top of recent advances in self-supervised learning\nfor computer vision and design a lightweight, easy-to-use, self-supervised\npre-training scheme for learning audio representations. We pre-train DECAR\nembeddings on a balanced subset of the large-scale AudioSet dataset and FSD50K\nand evaluate our representations on the LAPE Benchmark consisting of 11\ndownstream classification tasks, including speech, music, animal sounds, and\nacoustic scenes. Experimental results show that DECAR achieves results\ncompetitive to the state-of-the-art on both linear evaluation and transfer\nlearning evaluation paradigms across all the downstream tasks in LAPE and\nperforms better than other prior-art in literature with just 15% of the total\namount of data available for pre-training. Furthermore, we conduct ablation\nstudies identifying key design choices and also make all our code and\npre-trained models publicly available\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katta_S/0/1/0/all/0/1\">Sandesh V Katta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Referring Video Object Segmentation with Multimodal Transformers. (arXiv:2111.14821v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14821","description":"<p>The referring video object segmentation task (RVOS) involves segmentation of\na text-referred object instance in the frames of a given video. Due to the\ncomplex nature of this multimodal task, which combines text reasoning, video\nunderstanding, instance segmentation and tracking, existing approaches\ntypically rely on sophisticated pipelines in order to tackle it. In this paper,\nwe propose a simple Transformer-based approach to RVOS. Our framework, termed\nMultimodal Tracking Transformer (MTTR), models the RVOS task as a sequence\nprediction problem. Following recent advancements in computer vision and\nnatural language processing, MTTR is based on the realization that video and\ntext can be processed together effectively and elegantly by a single multimodal\nTransformer model. MTTR is end-to-end trainable, free of text-related inductive\nbias components and requires no additional mask-refinement post-processing\nsteps. As such, it simplifies the RVOS pipeline considerably compared to\nexisting methods. Evaluation on standard benchmarks reveals that MTTR\nsignificantly outperforms previous art across multiple metrics. In particular,\nMTTR shows impressive +5.7 and +5.0 mAP gains on the A2D-Sentences and\nJHMDB-Sentences datasets respectively, while processing 76 frames per second.\nIn addition, we report strong results on the public validation set of\nRefer-YouTube-VOS, a more challenging RVOS dataset that has yet to receive the\nattention of researchers. The code to reproduce our experiments is available at\nhttps://github.com/mttr2021/MTTR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Botach_A/0/1/0/all/0/1\">Adam Botach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheltonozhskii_E/0/1/0/all/0/1\">Evgenii Zheltonozhskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baskin_C/0/1/0/all/0/1\">Chaim Baskin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge. (arXiv:2112.08619v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08619","description":"<p>Humans usually have conversations by making use of prior knowledge about a\ntopic and background information of the people whom they are talking to.\nHowever, existing conversational agents and datasets do not consider such\ncomprehensive information, and thus they have a limitation in generating the\nutterances where the knowledge and persona are fused properly. To address this\nissue, we introduce a call For Customized conversation (FoCus) dataset where\nthe customized answers are built with the user's persona and Wikipedia\nknowledge. To evaluate the abilities to make informative and customized\nutterances of pre-trained language models, we utilize BART and GPT-2 as well as\ntransformer-based models. We assess their generation abilities with automatic\nscores and conduct human evaluations for qualitative results. We examine\nwhether the model reflects adequate persona and knowledge with our proposed two\nsub-tasks, persona grounding (PG) and knowledge grounding (KG). Moreover, we\nshow that the utterances of our data are constructed with the proper knowledge\nand persona through grounding quality assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yoonna Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Jungwoo Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hur_Y/0/1/0/all/0/1\">Yuna Hur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_D/0/1/0/all/0/1\">Dongsuk Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Suhyune Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeonsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Donghoon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-driven Semantic Segmentation. (arXiv:2201.03546v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03546","description":"<p>We present LSeg, a novel model for language-driven semantic image\nsegmentation. LSeg uses a text encoder to compute embeddings of descriptive\ninput labels (e.g., \"grass\" or \"building\") together with a transformer-based\nimage encoder that computes dense per-pixel embeddings of the input image. The\nimage encoder is trained with a contrastive objective to align pixel embeddings\nto the text embedding of the corresponding semantic class. The text embeddings\nprovide a flexible label representation in which semantically similar labels\nmap to similar regions in the embedding space (e.g., \"cat\" and \"furry\"). This\nallows LSeg to generalize to previously unseen categories at test time, without\nretraining or even requiring a single additional training sample. We\ndemonstrate that our approach achieves highly competitive zero-shot performance\ncompared to existing zero- and few-shot semantic segmentation methods, and even\nmatches the accuracy of traditional segmentation algorithms when a fixed label\nset is provided. Code and demo are available at\nhttps://github.com/isl-org/lang-seg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranftl_R/0/1/0/all/0/1\">Ren&#xe9; Ranftl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Stop Sets on Stopping Active Learning for Text Classification. (arXiv:2201.05460v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2201.05460","description":"<p>Active learning is an increasingly important branch of machine learning and a\npowerful technique for natural language processing. The main advantage of\nactive learning is its potential to reduce the amount of labeled data needed to\nlearn high-performing models. A vital aspect of an effective active learning\nalgorithm is the determination of when to stop obtaining additional labeled\ndata. Several leading state-of-the-art stopping methods use a stop set to help\nmake this decision. However, there has been relatively less attention given to\nthe choice of stop set than to the stopping algorithms that are applied on the\nstop set. Different choices of stop sets can lead to significant differences in\nstopping method performance. We investigate the impact of different stop set\nchoices on different stopping methods. This paper shows the choice of the stop\nset can have a significant impact on the performance of stopping methods and\nthe impact is different for stability-based methods from that on\nconfidence-based methods. Furthermore, the unbiased representative stop sets\nsuggested by original authors of methods work better than the systematically\nbiased stop sets used in recently published work, and stopping methods based on\nstabilizing predictions have stronger performance than confidence-based\nstopping methods when unbiased representative stop sets are used. We provide\nthe largest quantity of experimental results on the impact of stop sets to\ndate. The findings are important for helping to illuminate the impact of this\nimportant aspect of stopping methods that has been under-considered in recently\npublished work and that can have a large practical impact on the performance of\nstopping methods for important semantic computing applications such as\ntechnology assisted review and text classification more broadly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurlandski_L/0/1/0/all/0/1\">Luke Kurlandski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloodgood_M/0/1/0/all/0/1\">Michael Bloodgood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer. (arXiv:2202.07543v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07543","description":"<p>Memes are prevalent on the internet and continue to grow and evolve alongside\nour culture. An automatic understanding of memes propagating on the internet\ncan shed light on the general sentiment and cultural attitudes of people. In\nthis work, we present team BLUE's solution for the second edition of the\nMEMOTION shared task. We showcase two approaches for meme classification (i.e.\nsentiment, humour, offensive, sarcasm and motivation levels) using a text-only\nmethod using BERT, and a Multi-Modal-Multi-Task transformer network that\noperates on both the meme image and its caption to output the final scores. In\nboth approaches, we leverage state-of-the-art pretrained models for text (BERT,\nSentence Transformer) and image processing (EfficientNetV4, CLIP). Through our\nefforts, we obtain first place in task A, second place in task B and third\nplace in task C. In addition, our team obtained the highest average score for\nall three tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iordache_I/0/1/0/all/0/1\">Ioan-Bogdan Iordache</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reward Modeling for Mitigating Toxicity in Transformer-based Language Models. (arXiv:2202.09662v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.09662","description":"<p>Transformer-based language models are able to generate fluent text and be\nefficiently adapted across various natural language generation tasks. However,\nlanguage models that are pretrained on large unlabeled web text corpora have\nbeen shown to suffer from degenerating toxic content and social bias behaviors,\nconsequently hindering their safe deployment. Various detoxification methods\nwere proposed to mitigate the language model's toxicity; however, these methods\nstruggled to detoxify language models when conditioned on prompts that contain\nspecific social identities related to gender, race, or religion. In this study,\nwe propose Reinforce-Detoxify; A reinforcement learning-based method for\nmitigating toxicity in language models. We address the challenge of safety in\nlanguage models and propose a new reward model that is able to detect toxic\ncontent and mitigate unintended bias towards social identities in toxicity\nprediction. The experiments demonstrate that the Reinforce-Detoxify method for\nlanguage model detoxification outperforms existing detoxification approaches in\nautomatic evaluation metrics, indicating the ability of our approach in\nlanguage model detoxification and less prone to unintended bias toward social\nidentities in generated content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faal_F/0/1/0/all/0/1\">Farshid Faal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_K/0/1/0/all/0/1\">Ketra Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jia Yuan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking and Refining the Distinct Metric. (arXiv:2202.13587v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13587","description":"<p>Distinct-$n$ score\\cite{Li2016} is a widely used automatic metric for\nevaluating diversity in language generation tasks. However, we observed that\nthe original approach for calculating distinct scores has evident biases that\ntend to assign higher penalties to longer sequences. We refine the calculation\nof distinct scores by scaling the number of distinct tokens based on their\nexpectations. We provide both empirical and theoretical evidence to show that\nour method effectively removes the biases existing in the original distinct\nscore. Our experiments show that our proposed metric,\n\\textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human\njudgment in evaluating response diversity. To foster future research, we\nprovide an example implementation at\n\\url{https://github.com/lsy641/Expectation-Adjusted-Distinct}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1\">Sahand Sabour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language. (arXiv:2203.03598v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03598","description":"<p>Learning to classify video data from classes not included in the training\ndata, i.e. video-based zero-shot learning, is challenging. We conjecture that\nthe natural alignment between the audio and visual modalities in video data\nprovides a rich training signal for learning discriminative multi-modal\nrepresentations. Focusing on the relatively underexplored task of audio-visual\nzero-shot learning, we propose to learn multi-modal representations from\naudio-visual data using cross-modal attention and exploit textual label\nembeddings for transferring knowledge from seen classes to unseen classes.\nTaking this one step further, in our generalised audio-visual zero-shot\nlearning setting, we include all the training classes in the test-time search\nspace which act as distractors and increase the difficulty while making the\nsetting more realistic. Due to the lack of a unified benchmark in this domain,\nwe introduce a (generalised) zero-shot learning benchmark on three audio-visual\ndatasets of varying sizes and difficulty, VGGSound, UCF, and ActivityNet,\nensuring that the unseen test classes do not appear in the dataset used for\nsupervised training of the backbone deep models. Comparing multiple relevant\nand recent methods, we demonstrate that our proposed AVCA model achieves\nstate-of-the-art performance on all three datasets. Code and data are available\nat \\url{https://github.com/ExplainableML/AVCA-GZSL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mercea_O/0/1/0/all/0/1\">Otniel-Bogdan Mercea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesch_L/0/1/0/all/0/1\">Lukas Riesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1\">A. Sophia Koepke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIE-SQL: History Information Enhanced Network for Context-Dependent Text-to-SQL Semantic Parsing. (arXiv:2203.07376v2 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2203.07376","description":"<p>Recently, context-dependent text-to-SQL semantic parsing which translates\nnatural language into SQL in an interaction process has attracted a lot of\nattention. Previous works leverage context-dependence information either from\ninteraction history utterances or the previous predicted SQL queries but fail\nin taking advantage of both since of the mismatch between natural language and\nlogic-form SQL. In this work, we propose a History Information Enhanced\ntext-to-SQL model (HIE-SQL) to exploit context-dependence information from both\nhistory utterances and the last predicted SQL query. In view of the mismatch,\nwe treat natural language and SQL as two modalities and propose a bimodal\npre-trained model to bridge the gap between them. Besides, we design a\nschema-linking graph to enhance connections from utterances and the SQL query\nto the database schema. We show our history information enhanced methods\nimprove the performance of HIE-SQL by a significant margin, which achieves new\nstate-of-the-art results on the two context-dependent text-to-SQL benchmarks,\nthe SparC and CoSQL datasets, at the writing time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanzhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Baohua Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changshan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLSP 2021 - ViMRC Challenge: Vietnamese Machine Reading Comprehension. (arXiv:2203.11400v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.11400","description":"<p>One of the emerging research trends in natural language understanding is\nmachine reading comprehension (MRC) which is the task to find answers to human\nquestions based on textual data. Existing Vietnamese datasets for MRC research\nconcentrate solely on answerable questions. However, in reality, questions can\nbe unanswerable for which the correct answer is not stated in the given textual\ndata. To address the weakness, we provide the research community with a\nbenchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question\nanswering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a\nbenchmark dataset for the challenge on Vietnamese MRC at the Eighth Workshop on\nVietnamese Language and Speech Processing (VLSP 2021). This task attracted 77\nparticipant teams from 34 universities and other organizations. In this\narticle, we present details of the organization of the challenge, an overview\nof the methods employed by shared-task participants, and the results. The\nhighest performances are 77.24% in F1-score and 67.43% in Exact Match on the\nprivate test set. The Vietnamese MRC systems proposed by the top 3 teams use\nXLM-RoBERTa, a powerful pre-trained language model based on the transformer\narchitecture. The UIT-ViQuAD 2.0 dataset motivates researchers to further\nexplore the Vietnamese machine reading comprehension task and related tasks\nsuch as question answering, question generation, and natural language\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Quoc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Luan Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1\">Tin Van Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Text-to-Speech Pipeline, Evaluation Methodology, and Initial Fine-Tuning Results for Child Speech Synthesis. (arXiv:2203.11562v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.11562","description":"<p>Speech synthesis has come a long way as current text-to-speech (TTS) models\ncan now generate natural human-sounding speech. However, most of the TTS\nresearch focuses on using adult speech data and there has been very limited\nwork done on child speech synthesis. This study developed and validated a\ntraining pipeline for fine-tuning state-of-the-art (SOTA) neural TTS models\nusing child speech datasets. This approach adopts a multi-speaker TTS retuning\nworkflow to provide a transfer-learning pipeline. A publicly available child\nspeech dataset was cleaned to provide a smaller subset of approximately 19\nhours, which formed the basis of our fine-tuning experiments. Both subjective\nand objective evaluations were performed using a pretrained MOSNet for\nobjective evaluation and a novel subjective framework for mean opinion score\n(MOS) evaluations. Subjective evaluations achieved the MOS of 3.95 for speech\nintelligibility, 3.89 for voice naturalness, and 3.96 for voice consistency.\nObjective evaluation using a pretrained MOSNet showed a strong correlation\nbetween real and synthetic child voices. Speaker similarity was also verified\nby calculating the cosine similarity between the embeddings of utterances. An\nautomatic speech recognition (ASR) model is also used to provide a word error\nrate (WER) comparison between the real and synthetic child voices. The final\ntrained TTS model was able to synthesize child-like speech from reference audio\nsamples as short as 5 seconds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rishabh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiwere_M/0/1/0/all/0/1\">Mariam Yiwere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigioi_D/0/1/0/all/0/1\">Dan Bigioi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1\">Peter Corcoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucu_H/0/1/0/all/0/1\">Horia Cucu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.11933","description":"<p>Vision-language models can encode societal biases and stereotypes, but there\nare challenges to measuring and mitigating these harms. Prior proposed bias\nmeasurements lack robustness and feature degradation occurs when mitigating\nbias without access to pretraining data. We address both of these challenges in\nthis paper: First, we evaluate different bias measures and propose the use of\nretrieval metrics to image-text representations via a bias measuring framework.\nSecond, we investigate debiasing methods and show that optimizing for\nadversarial loss via learnable token embeddings minimizes various bias measures\nwithout substantially degrading feature representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berg_H/0/1/0/all/0/1\">Hugo Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1\">Siobhan Mackenzie Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1\">Yash Bhalgat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wonsuk Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bain_M/0/1/0/all/0/1\">Max Bain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mix and Match: Learning-free Controllable Text Generation using Energy Language Models. (arXiv:2203.13299v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13299","description":"<p>Recent work on controlled text generation has either required attribute-based\nfine-tuning of the base language model (LM), or has restricted the\nparameterization of the attribute discriminator to be compatible with the base\nautoregressive LM. In this work, we propose Mix and Match LM, a global\nscore-based alternative for controllable text generation that combines\narbitrary pre-trained black-box models for achieving the desired attributes in\nthe generated text without involving any fine-tuning or structural assumptions\nabout the black-box models. We interpret the task of controllable generation as\ndrawing samples from an energy-based model whose energy values are a linear\ncombination of scores from black-box models that are separately responsible for\nfluency, the control attribute, and faithfulness to any conditioning context.\nWe use a Metropolis-Hastings sampling scheme to sample from this energy-based\nmodel using bidirectional context and global attribute features. We validate\nthe effectiveness of our approach on various controlled generation and\nstyle-based text revision tasks by outperforming recently proposed methods that\ninvolve extra training, fine-tuning, or restrictive assumptions over the form\nof models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_K/0/1/0/all/0/1\">Kartik Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Roadmap for Big Model. (arXiv:2203.14101v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.14101","description":"<p>With the rapid development of deep learning, training Big Models (BMs) for\nmultiple downstream tasks becomes a popular paradigm. Researchers have achieved\nvarious outcomes in the construction of BMs and the BM application in many\nfields. At present, there is a lack of research work that sorts out the overall\nprogress of BMs and guides the follow-up research. In this paper, we cover not\nonly the BM technologies themselves but also the prerequisites for BM training\nand applications with BMs, dividing the BM review into four parts: Resource,\nModels, Key Technologies and Application. We introduce 16 specific BM-related\ntopics in those four parts, they are Data, Knowledge, Computing System,\nParallel Training System, Language Model, Vision Model, Multi-modal Model,\nTheory&amp;Interpretability, Commonsense Reasoning, Reliability&amp;Security,\nGovernance, Evaluation, Machine Translation, Text Generation, Dialogue and\nProtein Research. In each topic, we summarize clearly the current studies and\npropose some future research directions. At the end of this paper, we conclude\nthe further development of BMs in a more general view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yangxiao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhou Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiaao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yizhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Cong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lingxiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chence Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zuobai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaoyu Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zijun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Weicheng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zixuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaojun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zheni Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenzhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, et al. (35 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Example-based Hypernetworks for Out-of-Distribution Generalization. (arXiv:2203.14276v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.14276","description":"<p>While Natural Language Processing (NLP) algorithms keep reaching\nunprecedented milestones, out-of-distribution generalization is still\nchallenging. In this paper we address the problem of multi-source adaptation to\nunknown domains: Given labeled data from multiple source domains, we aim to\ngeneralize to data drawn from target domains that are unknown to the algorithm\nat training time. We present an algorithmic framework based on example-based\nHypernetwork adaptation: Given an input example, a T5 encoder-decoder first\ngenerates a unique signature which embeds this example in the semantic space of\nthe source domains, and this signature is then fed into a Hypernetwork which\ngenerates the weights of the task classifier. In an advanced version of our\nmodel, the learned signature also serves for improving the representation of\nthe input example. In experiments with two tasks, sentiment classification and\nnatural language inference, across 29 adaptation settings, our algorithms\nsubstantially outperform existing algorithms for this adaptation setup. To the\nbest of our knowledge, this is the first time Hypernetworks are applied to\ndomain adaptation or in example-based manner in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Volk_T/0/1/0/all/0/1\">Tomer Volk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1\">Eyal Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amosy_O/0/1/0/all/0/1\">Ohad Amosy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANNA: Enhanced Language Representation for Question Answering. (arXiv:2203.14507v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.14507","description":"<p>Pre-trained language models have brought significant improvements in\nperformance in a variety of natural language processing tasks. Most existing\nmodels performing state-of-the-art results have shown their approaches in the\nseparate perspectives of data processing, pre-training tasks, neural network\nmodeling, or fine-tuning. In this paper, we demonstrate how the approaches\naffect performance individually, and that the language model performs the best\nresults on a specific question answering task when those approaches are jointly\nconsidered in pre-training models. In particular, we propose an extended\npre-training task, and a new neighbor-aware mechanism that attends neighboring\ntokens more to capture the richness of context for pre-training language\nmodeling. Our best model achieves new state-of-the-art results of 95.7\\% F1 and\n90.6\\% EM on SQuAD 1.1 and also outperforms existing pre-trained language\nmodels such as RoBERTa, ALBERT, ELECTRA, and XLNet on the SQuAD 2.0 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jun_C/0/1/0/all/0/1\">Changwook Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hansol Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_M/0/1/0/all/0/1\">Myoseop Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jooyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_K/0/1/0/all/0/1\">Kyungkoo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1\">Kyunghoon Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrate Lattice-Free MMI into End-to-End Speech Recognition. (arXiv:2203.15614v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.15614","description":"<p>In automatic speech recognition (ASR) research, discriminative criteria have\nachieved superior performance in DNN-HMM systems. Given this success, the\nadoption of discriminative criteria is promising to boost the performance of\nend-to-end (E2E) ASR systems. With this motivation, previous works have\nintroduced the minimum Bayesian risk (MBR, one of the discriminative criteria)\ninto E2E ASR systems. However, the effectiveness and efficiency of the\nMBR-based methods are compromised: the MBR criterion is only used in system\ntraining, which creates a mismatch between training and decoding; the\non-the-fly decoding process in MBR-based methods results in the need for\npre-trained models and slow training speeds. To this end, novel algorithms are\nproposed in this work to integrate another widely used discriminative\ncriterion, lattice-free maximum mutual information (LF-MMI), into E2E ASR\nsystems not only in the training stage but also in the decoding process. The\nproposed LF-MMI training and decoding methods show their effectiveness on two\nwidely used E2E frameworks: Attention-Based Encoder-Decoders (AEDs) and Neural\nTransducers (NTs). Compared with MBR-based methods, the proposed LF-MMI method:\nmaintains the consistency between training and decoding; eschews the on-the-fly\ndecoding process; trains from randomly initialized models with superior\ntraining efficiency. Experiments suggest that the LF-MMI method outperforms its\nMBR counterparts and consistently leads to statistically significant\nperformance improvements on various frameworks and datasets from 30 hours to\n14.3k hours. The proposed method achieves state-of-the-art (SOTA) results on\nAishell-1 (CER 4.10%) and Aishell-2 (CER 5.02%) datasets. Code is released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jinchuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1\">Chao Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding. (arXiv:2203.16487v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16487","description":"<p>In this paper, we propose Generalized Aggressive Decoding (GAD) -- a novel\ndecoding paradigm for speeding up autoregressive translation without quality\nloss, through the collaboration of autoregressive and non-autoregressive\ntranslation (NAT) of the Transformer. At each decoding iteration, GAD\naggressively decodes a number of tokens in parallel as a draft with NAT and\nthen verifies them in the autoregressive manner, where only the tokens that\npass the verification are kept as decoded tokens. GAD can achieve the same\nperformance as autoregressive translation but much more efficiently because\nboth NAT drafting and autoregressive verification are fast due to parallel\ncomputing. We conduct experiments in the WMT14 English-German translation task\nand confirm that the vanilla GAD yields exactly the same results as greedy\ndecoding with an around 3x speedup, and that its variant (GAD++) with an\nadvanced verification strategy not only outperforms the greedy translation and\neven achieves the comparable translation quality with the beam search result,\nbut also further improves the decoding speed, resulting in an around 5x speedup\nover autoregressive translation. Our models and codes are available at\nhttps://github.com/hemingkx/Generalized-Aggressive-Decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Heming Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the factors affecting usefulness of Self-Supervised Pre-trained Representations for Speech Recognition. (arXiv:2203.16973v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16973","description":"<p>Self-supervised learning (SSL) to learn high-level speech representations has\nbeen a popular approach to building Automatic Speech Recognition (ASR) systems\nin low-resource settings. However, the common assumption made in literature is\nthat a considerable amount of unlabeled data is available for the same domain\nor language that can be leveraged for SSL pre-training, which we acknowledge is\nnot feasible in a real-world setting. In this paper, as part of the Interspeech\nGram Vaani ASR challenge, we try to study the effect of domain, language,\ndataset size, and other aspects of our upstream pre-training SSL data on the\nfinal performance low-resource downstream ASR task. We also build on the\ncontinued pre-training paradigm to study the effect of prior knowledge\npossessed by models trained using SSL. Extensive experiments and studies reveal\nthat the performance of ASR systems is susceptible to the data used for SSL\npre-training. Their performance improves with an increase in similarity and\nvolume of pre-training data. We believe our work will be helpful to the speech\ncommunity in building better ASR systems in low-resource settings and steer\nresearch towards improving generalization in SSL-based pre-training for speech\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_L/0/1/0/all/0/1\">Lodagala V S V Durga Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Visual explanations for polyp detection: How medical doctors assess intrinsic versus extrinsic explanations. (arXiv:2204.00617v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00617","description":"<p>Deep learning has in recent years achieved immense success in all areas of\ncomputer vision and has the potential of assisting medical doctors in analyzing\nvisual content for disease and other abnormalities. However, the current state\nof deep learning is very much a black box, making medical professionals highly\nskeptical about integrating these methods into clinical practice. Several\nmethods have been proposed in order to shine some light onto these black boxes,\nbut there is no consensus on the opinion of the medical doctors that will\nconsume these explanations. This paper presents a study asking medical doctors\nabout their opinion of current state-of-the-art explainable artificial\nintelligence methods when applied to a gastrointestinal disease detection use\ncase. We compare two different categories of explanation methods, intrinsic and\nextrinsic, and gauge their opinion of the current value of these explanations.\nThe results indicate that intrinsic explanations are preferred and that\nexplanation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hicks_S/0/1/0/all/0/1\">Steven Hicks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Storaas_A/0/1/0/all/0/1\">Andrea Stor&#xe5;s</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael Riegler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Midoglu_C/0/1/0/all/0/1\">Cise Midoglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammou_M/0/1/0/all/0/1\">Malek Hammou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1\">Thomas de Lange</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strumke_I/0/1/0/all/0/1\">Inga Str&#xfc;mke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Lymph Node Detection in T2 MRI using Neural Networks. (arXiv:2204.00622v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00622","description":"<p>Purpose: Identification of abdominal Lymph Nodes (LN) that are suspicious for\nmetastasis in T2 Magnetic Resonance Imaging (MRI) scans is critical for staging\nof lymphoproliferative diseases. Prior work on LN detection has been limited to\nspecific anatomical regions of the body (pelvis, rectum) in single MR slices.\nTherefore, the development of a universal approach to detect LN in full T2 MRI\nvolumes is highly desirable.\n</p>\n<p>Methods: In this study, a Computer Aided Detection (CAD) pipeline to\nuniversally identify abdominal LN in volumetric T2 MRI using neural networks is\nproposed. First, we trained various neural network models for detecting LN:\nFaster RCNN with and without Hard Negative Example Mining (HNEM), FCOS,\nFoveaBox, VFNet, and Detection Transformer (DETR). Next, we show that the\nstate-of-the-art (SOTA) VFNet model with Adaptive Training Sample Selection\n(ATSS) outperforms Faster RCNN with HNEM. Finally, we ensembled models that\nsurpassed a 45% mAP threshold. We found that the VFNet model and one-stage\nmodel ensemble can be interchangeably used in the CAD pipeline.\n</p>\n<p>Results: Experiments on 122 test T2 MRI volumes revealed that VFNet achieved\na 51.1% mAP and 78.7% recall at 4 false positives (FP) per volume, while the\none-stage model ensemble achieved a mAP of 52.3% and sensitivity of 78.7% at\n4FP.\n</p>\n<p>Conclusion: Our contribution is a CAD pipeline that detects LN in T2 MRI\nvolumes, resulting in a sensitivity improvement of $\\sim$14 points over the\ncurrent SOTA method for LN detection (sensitivity of 78.7% at 4 FP vs. 64.6% at\n5 FP per volume).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mathai_T/0/1/0/all/0/1\">Tejas Sudharshan Mathai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Sungwon Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_T/0/1/0/all/0/1\">Thomas C. Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Summers_R/0/1/0/all/0/1\">Ronald M. Summers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Image Super-Resolution with Deep Modeling of Image Statistics. (arXiv:2204.00623v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00623","description":"<p>Modeling statistics of image priors is useful for image super-resolution, but\nlittle attention has been paid from the massive works of deep learning-based\nmethods. In this work, we propose a Bayesian image restoration framework, where\nnatural image statistics are modeled with the combination of smoothness and\nsparsity priors. Concretely, firstly we consider an ideal image as the sum of a\nsmoothness component and a sparsity residual, and model real image degradation\nincluding blurring, downscaling, and noise corruption. Then, we develop a\nvariational Bayesian approach to infer their posteriors. Finally, we implement\nthe variational approach for single image super-resolution (SISR) using deep\nneural networks, and propose an unsupervised training strategy. The experiments\non three image restoration tasks, \\textit{i.e.,} ideal SISR, realistic SISR,\nand real-world SISR, demonstrate that our method has superior model\ngeneralizability against varying noise levels and degradation kernels and is\neffective in unsupervised SISR. The code and resulting models are released via\n\\url{https://zmiclab.github.io/projects.html}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_S/0/1/0/all/0/1\">Shangqi Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable and Interpretable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning. (arXiv:2204.00624v1 [cs.LG])","link":"http://arxiv.org/abs/2204.00624","description":"<p>In this paper, we propose an explainable and interpretable diabetic\nretinopathy (ExplainDR) classification model based on neural-symbolic learning.\nTo gain explainability, a highlevel symbolic representation should be\nconsidered in decision making. Specifically, we introduce a human-readable\nsymbolic representation, which follows a taxonomy style of diabetic retinopathy\ncharacteristics related to eye health conditions to achieve explainability. We\nthen include humanreadable features obtained from the symbolic representation\nin the disease prediction. Experimental results on a diabetic retinopathy\nclassification dataset show that our proposed ExplainDR method exhibits\npromising performance when compared to that from state-of-the-art methods\napplied to the IDRiD dataset, while also providing interpretability and\nexplainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1\">Se-In Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girard_M/0/1/0/all/0/1\">Michael J.A. Girard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiery_A/0/1/0/all/0/1\">Alexandre H. Thiery</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Neural Acoustic Fields. (arXiv:2204.00628v1 [cs.SD])","link":"http://arxiv.org/abs/2204.00628","description":"<p>Our environment is filled with rich and dynamic acoustic information. When we\nwalk into a cathedral, the reverberations as much as appearance inform us of\nthe sanctuary's wide open space. Similarly, as an object moves around us, we\nexpect the sound emitted to also exhibit this movement. While recent advances\nin learned implicit functions have led to increasingly higher quality\nrepresentations of the visual world, there have not been commensurate advances\nin learning spatial auditory representations. To address this gap, we introduce\nNeural Acoustic Fields (NAFs), an implicit representation that captures how\nsounds propagate in a physical scene. By modeling acoustic propagation in a\nscene as a linear time-invariant system, NAFs learn to continuously map all\nemitter and listener location pairs to a neural impulse response function that\ncan then be applied to arbitrary sounds. We demonstrate that the continuous\nnature of NAFs enables us to render spatial acoustics for a listener at an\narbitrary location, and can predict sound propagation at novel locations. We\nfurther show that the representation learned by NAFs can help improve visual\nlearning with sparse views. Finally, we show that a representation informative\nof scene structure emerges during the learning of NAFs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_A/0/1/0/all/0/1\">Andrew Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarr_M/0/1/0/all/0/1\">Michael J. Tarr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopTemp: Parsing Precipitate Structure from Temper Topology. (arXiv:2204.00629v1 [cond-mat.mtrl-sci])","link":"http://arxiv.org/abs/2204.00629","description":"<p>Technological advances are in part enabled by the development of novel\nmanufacturing processes that give rise to new materials or material property\nimprovements. Development and evaluation of new manufacturing methodologies is\nlabor-, time-, and resource-intensive expensive due to complex, poorly defined\nrelationships between advanced manufacturing process parameters and the\nresulting microstructures. In this work, we present a topological\nrepresentation of temper (heat-treatment) dependent material micro-structure,\nas captured by scanning electron microscopy, called TopTemp. We show that this\ntopological representation is able to support temper classification of\nmicrostructures in a data limited setting, generalizes well to previously\nunseen samples, is robust to image perturbations, and captures domain\ninterpretable features. The presented work outperforms conventional deep\nlearning baselines and is a first step towards improving understanding of\nprocess parameters and resulting material properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Kassab_L/0/1/0/all/0/1\">Lara Kassab</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Howland_S/0/1/0/all/0/1\">Scott Howland</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kvinge_H/0/1/0/all/0/1\">Henry Kvinge</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kappagantula_K/0/1/0/all/0/1\">Keerti Sahithi Kappagantula</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Emerson_T/0/1/0/all/0/1\">Tegan Emerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extremely Low-light Image Enhancement with Scene Text Restoration. (arXiv:2204.00630v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00630","description":"<p>Deep learning-based methods have made impressive progress in enhancing\nextremely low-light images - the image quality of the reconstructed images has\ngenerally improved. However, we found out that most of these methods could not\nsufficiently recover the image details, for instance, the texts in the scene.\nIn this paper, a novel image enhancement framework is proposed to precisely\nrestore the scene texts, as well as the overall quality of the image\nsimultaneously under extremely low-light images conditions. Mainly, we employed\na self-regularised attention map, an edge map, and a novel text detection loss.\nIn addition, leveraging synthetic low-light images is beneficial for image\nenhancement on the genuine ones in terms of text detection. The quantitative\nand qualitative experimental results have shown that the proposed model\noutperforms state-of-the-art methods in image restoration, text detection, and\ntext spotting on See In the Dark and ICDAR15 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hsu_P/0/1/0/all/0/1\">Pohao Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1\">Che-Tsung Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ng_C/0/1/0/all/0/1\">Chun Chet Ng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kew_J/0/1/0/all/0/1\">Jie-Long Kew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_M/0/1/0/all/0/1\">Mei Yih Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_S/0/1/0/all/0/1\">Shang-Hong Lai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1\">Chee Seng Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zach_C/0/1/0/all/0/1\">Christopher Zach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation. (arXiv:2204.00631v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00631","description":"<p>Vision Transformers (ViT)s have recently become popular due to their\noutstanding modeling capabilities, in particular for capturing long-range\ninformation, and scalability to dataset and model sizes which has led to\nstate-of-the-art performance in various computer vision and medical image\nanalysis tasks. In this work, we introduce a unified framework consisting of\ntwo architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder\nand Convolutional Neural Network (CNN) and transformer-based decoders. In the\nproposed model, the encoder is linked to the decoder via skip connections at\nfive different resolutions with deep supervision. The design of proposed\narchitecture allows for meeting a wide range of trade-off requirements between\naccuracy and computational cost. In addition, we present a methodology for\nself-supervised pre-training of the encoder backbone via learning to predict\nrandomly masked volumetric tokens using contextual information of visible\ntokens. We pre-train our framework on a cohort of $5050$ CT images, gathered\nfrom publicly available CT datasets, and present a systematic investigation of\nvarious components such as masking ratio and patch size that affect the\nrepresentation learning capability and performance of downstream tasks. We\nvalidate the effectiveness of our pre-training approach by fine-tuning and\ntesting our model on liver and liver tumor segmentation task using the Medical\nSegmentation Decathlon (MSD) dataset and achieve state-of-the-art performance\nin terms of various segmentation metrics. To demonstrate its generalizability,\nwe train and test the model on BraTS 21 dataset for brain tumor segmentation\nusing MRI images and outperform other methods in terms of Dice score. Code:\nhttps://github.com/Project-MONAI/research-contributions\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIMBAR: Single Image-Based Scene Relighting For Effective Data Augmentation For Automated Driving Vision Tasks. (arXiv:2204.00644v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00644","description":"<p>Real-world autonomous driving datasets comprise of images aggregated from\ndifferent drives on the road. The ability to relight captured scenes to unseen\nlighting conditions, in a controllable manner, presents an opportunity to\naugment datasets with a richer variety of lighting conditions, similar to what\nwould be encountered in the real-world. This paper presents a novel image-based\nrelighting pipeline, SIMBAR, that can work with a single image as input. To the\nbest of our knowledge, there is no prior work on scene relighting leveraging\nexplicit geometric representations from a single image. We present qualitative\ncomparisons with prior multi-view scene relighting baselines. To further\nvalidate and effectively quantify the benefit of leveraging SIMBAR for data\naugmentation for automated driving vision tasks, object detection and tracking\nexperiments are conducted with a state-of-the-art method, a Multiple Object\nTracking Accuracy (MOTA) of 93.3% is achieved with CenterTrack on\nSIMBAR-augmented KITTI - an impressive 9.0% relative improvement over the\nbaseline MOTA of 85.6% with CenterTrack on original KITTI, both models trained\nfrom scratch and tested on Virtual KITTI. For more details and SIMBAR relit\ndatasets, please visit our project website (https://simbarv1.github.io/).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianling Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_N/0/1/0/all/0/1\">Nathan Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syed_A/0/1/0/all/0/1\">Ameerah Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhasin_R/0/1/0/all/0/1\">Rohan Bhasin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaipuria_N/0/1/0/all/0/1\">Nikita Jaipuria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Neonatal Face Detection in Real-world Clinical Settings. (arXiv:2204.00655v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00655","description":"<p>Current face detection algorithms are extremely generalized and can obtain\ndecent accuracy when detecting the adult faces. These approaches are\ninsufficient when handling outlier cases, for example when trying to detect the\nface of a neonate infant whose face composition and expressions are relatively\ndifferent than that of the adult. It is furthermore difficult when applied to\ndetect faces in a complicated setting such as the Neonate Intensive Care Unit.\nBy training a state-of-the-art face detection model, You-Only-Look-Once, on a\nproprietary dataset containing labelled neonate faces in a clinical setting,\nthis work achieves near real time neonate face detection. Our preliminary\nfindings show an accuracy of 68.7%, compared to the off the shelf solution\nwhich detected neonate faces with an accuracy of 7.37%. Although further\nexperiments are needed to validate our model, our results are promising and\nprove the feasibility of detecting neonatal faces in challenging real-world\nsettings. The robust and real-time detection of neonatal faces would benefit\nwide range of automated systems (e.g., pain recognition and surveillance) who\ncurrently suffer from the time and effort due to the necessity of manual\nannotations. To benefit the research community, we make our trained weights\npublicly available at github(https://github.com/ja05haus/trained_neonate_face).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hausmann_J/0/1/0/all/0/1\">Jacqueline Hausmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salekin_M/0/1/0/all/0/1\">Md Sirajus Salekin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldgof_D/0/1/0/all/0/1\">Dmitry Goldgof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistency driven Sequential Transformers Attention Model for Partially Observable Scenes. (arXiv:2204.00656v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00656","description":"<p>Most hard attention models initially observe a complete scene to locate and\nsense informative glimpses, and predict class-label of a scene based on\nglimpses. However, in many applications (e.g., aerial imaging), observing an\nentire scene is not always feasible due to the limited time and resources\navailable for acquisition. In this paper, we develop a Sequential Transformers\nAttention Model (STAM) that only partially observes a complete image and\npredicts informative glimpse locations solely based on past glimpses. We design\nour agent using DeiT-distilled and train it with a one-step actor-critic\nalgorithm. Furthermore, to improve classification performance, we introduce a\nnovel training objective, which enforces consistency between the class\ndistribution predicted by a teacher model from a complete image and the class\ndistribution predicted by our agent using glimpses. When the agent senses only\n4% of the total image area, the inclusion of the proposed consistency loss in\nour training objective yields 3% and 8% higher accuracy on ImageNet and fMoW\ndatasets, respectively. Moreover, our agent outperforms previous\nstate-of-the-art by observing nearly 27% and 42% fewer pixels in glimpses on\nImageNet and fMoW.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rangrej_S/0/1/0/all/0/1\">Samrudhdhi B. Rangrej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinidhi_C/0/1/0/all/0/1\">Chetan L. Srinidhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">James J. Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hazard Detection And Avoidance For The Nova-C Lander. (arXiv:2204.00660v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00660","description":"<p>In early 2022, Intuitive Machines' NOVA-C Lander will touch down on the lunar\nsurface becoming the first commercial endeavor to visit a celestial body.\nNOVA-C will deliver six payloads to the lunar surface with various scientific\nand engineering objectives, ushering in a new era of commercial space\nexploration and utilization. However, to safely accomplish the mission, the\nNOVA-C lander must ensure its landing site is free of hazards larger than 30 cm\nand the slope of local terrain at touchdown is less than 10 degrees off\nvertical. To accomplish this, NOVA-C utilizes Intuitive Machines' precision\nnavigation system, coupled with machine vision algorithms for scene reduction\nand landing site characterization. A unique aspect to the NOVA-C approach is\nthe real-time nature of the hazard detection and avoidance algorithms--which\nare performed 400 meters above and down range of the intended landing site and\ncompleted within 15 seconds. In this paper, we review the theoretical\nfoundations for the hazard detection and avoidance algorithms, describe the\npractical challenges of implementation on the NOVA-C flight computer, and\npresent test and analysis results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Getchius_J/0/1/0/all/0/1\">Joel Getchius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renshaw_D/0/1/0/all/0/1\">Devin Renshaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posada_D/0/1/0/all/0/1\">Daniel Posada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_T/0/1/0/all/0/1\">Troy Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lillian Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molina_G/0/1/0/all/0/1\">Giovanni Molina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Audio-Video Modalities from Image Captions. (arXiv:2204.00679v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00679","description":"<p>A major challenge in text-video and text-audio retrieval is the lack of\nlarge-scale training data. This is unlike image-captioning, where datasets are\nin the order of millions of samples. To close this gap we propose a new video\nmining pipeline which involves transferring captions from image captioning\ndatasets to video clips with no additional manual effort. Using this pipeline,\nwe create a new large-scale, weakly labelled audio-video captioning dataset\nconsisting of millions of paired clips and captions. We show that training a\nmultimodal transformed based model on this data achieves competitive\nperformance on video retrieval and video captioning, matching or even\noutperforming HowTo100M pretraining with 20x fewer clips. We also show that our\nmined clips are suitable for text-audio pretraining, and achieve state of the\nart results for the task of audio retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_P/0/1/0/all/0/1\">Paul Hongsuck Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seybold_B/0/1/0/all/0/1\">Bryan Seybold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauth_A/0/1/0/all/0/1\">Anja Hauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manen_S/0/1/0/all/0/1\">Santiago Manen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkeleVision: Towards Adversarial Resiliency of Person Tracking with Multi-Task Learning. (arXiv:2204.00734v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00734","description":"<p>Person tracking using computer vision techniques has wide ranging\napplications such as autonomous driving, home security and sports analytics.\nHowever, the growing threat of adversarial attacks raises serious concerns\nregarding the security and reliability of such techniques. In this work, we\nstudy the impact of multi-task learning (MTL) on the adversarial robustness of\nthe widely used SiamRPN tracker, in the context of person tracking.\nSpecifically, we investigate the effect of jointly learning with semantically\nanalogous tasks of person tracking and human keypoint detection. We conduct\nextensive experiments with more powerful adversarial attacks that can be\nphysically realizable, demonstrating the practical value of our approach. Our\nempirical study with simulated as well as real-world datasets reveals that\ntraining with MTL consistently makes it harder to attack the SiamRPN tracker,\ncompared to typically training only on the single task of person tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1\">Nilaksh Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sheng-Yun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What to look at and where: Semantic and Spatial Refined Transformer for detecting human-object interactions. (arXiv:2204.00746v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00746","description":"<p>We propose a novel one-stage Transformer-based semantic and spatial refined\ntransformer (SSRT) to solve the Human-Object Interaction detection task, which\nrequires to localize humans and objects, and predicts their interactions.\nDifferently from previous Transformer-based HOI approaches, which mostly focus\nat improving the design of the decoder outputs for the final detection, SSRT\nintroduces two new modules to help select the most relevant object-action pairs\nwithin an image and refine the queries' representation using rich semantic and\nspatial features. These enhancements lead to state-of-the-art results on the\ntwo most popular HOI benchmarks: V-COCO and HICO-DET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iftekhar_A/0/1/0/all/0/1\">A S M Iftekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_K/0/1/0/all/0/1\">Kaustav Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Homography Loss for Monocular 3D Object Detection. (arXiv:2204.00754v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00754","description":"<p>Monocular 3D object detection is an essential task in autonomous driving.\nHowever, most current methods consider each 3D object in the scene as an\nindependent training sample, while ignoring their inherent geometric relations,\nthus inevitably resulting in a lack of leveraging spatial constraints. In this\npaper, we propose a novel method that takes all the objects into consideration\nand explores their mutual relationships to help better estimate the 3D boxes.\nMoreover, since 2D detection is more reliable currently, we also investigate\nhow to use the detected 2D boxes as guidance to globally constrain the\noptimization of the corresponding predicted 3D boxes. To this end, a\ndifferentiable loss function, termed as Homography Loss, is proposed to achieve\nthe goal, which exploits both 2D and 3D information, aiming at balancing the\npositional relationships between different objects by global constraints, so as\nto obtain more accurately predicted 3D boxes. Thanks to the concise design, our\nloss function is universal and can be plugged into any mature monocular 3D\ndetector, while significantly boosting the performance over their baseline.\nExperiments demonstrate that our method yields the best performance (Nov. 2021)\ncompared with the other state-of-the-arts by a large margin on KITTI 3D\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bojian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lubin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhiyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do learned representations respect causal relationships?. (arXiv:2204.00762v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00762","description":"<p>Data often has many semantic attributes that are causally associated with\neach other. But do attribute-specific learned representations of data also\nrespect the same causal relations? We answer this question in three steps.\nFirst, we introduce NCINet, an approach for observational causal discovery from\nhigh-dimensional data. It is trained purely on synthetically generated\nrepresentations and can be applied to real representations, and is specifically\ndesigned to mitigate the domain gap between the two. Second, we apply NCINet to\nidentify the causal relations between image representations of different pairs\nof attributes with known and unknown causal relations between the labels. For\nthis purpose, we consider image representations learned for predicting\nattributes on the 3D Shapes, CelebA, and the CASIA-WebFace datasets, which we\nannotate with multiple multi-class attributes. Third, we analyze the effect on\nthe underlying causal relation between learned representations induced by\nvarious design choices in representation learning. Our experiments indicate\nthat (1) NCINet significantly outperforms existing observational causal\ndiscovery approaches for estimating the causal relation between pairs of random\nsamples, both in the presence and absence of an unobserved confounder, (2)\nunder controlled scenarios, learned representations can indeed satisfy the\nunderlying causal relations between their respective labels, and (3) the causal\nrelations are positively correlated with the predictive capability of the\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1\">Vishnu Naresh Boddeti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAD: A Large-scale Dataset towards Airport Detection in Synthetic Aperture Radar Images. (arXiv:2204.00790v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00790","description":"<p>Airports have an important role in both military and civilian domains. The\nsynthetic aperture radar (SAR) based airport detection has received increasing\nattention in recent years. However, due to the high cost of SAR imaging and\nannotation process, there is no publicly available SAR dataset for airport\ndetection. As a result, deep learning methods have not been fully used in\nairport detection tasks. To provide a benchmark for airport detection research\nin SAR images, this paper introduces a large-scale SAR Airport Dataset (SAD).\nIn order to adequately reflect the demands of real world applications, it\ncontains 624 SAR images from Sentinel 1B and covers 104 airfield instances with\ndifferent scales, orientations and shapes. The experiments of multiple deep\nlearning approach on this dataset proves its effectiveness. It developing\nstate-of-the-art airport area detection algorithms or other relevant tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daochang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1\">Deliang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongsheng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IR-GAN: Image Manipulation with Linguistic Instruction by Increment Reasoning. (arXiv:2204.00792v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00792","description":"<p>Conditional image generation is an active research topic including text2image\nand image translation.\n</p>\n<p>Recently image manipulation with linguistic instruction brings new challenges\nof multimodal conditional generation.\n</p>\n<p>However, traditional conditional image generation models mainly focus on\ngenerating high-quality and visually realistic images, and lack resolving the\npartial consistency between image and instruction.\n</p>\n<p>To address this issue, we propose an Increment Reasoning Generative\nAdversarial Network (IR-GAN), which aims to reason the consistency between\nvisual increment in images and semantic increment in instructions.\n</p>\n<p>First, we introduce the word-level and instruction-level instruction encoders\nto learn user's intention from history-correlated instructions as semantic\nincrement.\n</p>\n<p>Second, we embed the representation of semantic increment into that of source\nimage for generating target image, where source image plays the role of\nreferring auxiliary.\n</p>\n<p>Finally, we propose a reasoning discriminator to measure the consistency\nbetween visual increment and semantic increment, which purifies user's\nintention and guarantees the good logic of generated target image.\n</p>\n<p>Extensive experiments and visualization conducted on two datasets show the\neffectiveness of IR-GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenhuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jincan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaofei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qianqian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R(Det)^2: Randomized Decision Routing for Object Detection. (arXiv:2204.00794v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00794","description":"<p>In the paradigm of object detection, the decision head is an important part,\nwhich affects detection performance significantly. Yet how to design a\nhigh-performance decision head remains to be an open issue. In this paper, we\npropose a novel approach to combine decision trees and deep neural networks in\nan end-to-end learning manner for object detection. First, we disentangle the\ndecision choices and prediction values by plugging soft decision trees into\nneural networks. To facilitate effective learning, we propose randomized\ndecision routing with node selective and associative losses, which can boost\nthe feature representative learning and network decision simultaneously.\nSecond, we develop the decision head for object detection with narrow branches\nto generate the routing probabilities and masks, for the purpose of obtaining\ndivergent decisions from different nodes. We name this approach as the\nrandomized decision routing for object detection, abbreviated as R(Det)$^2$.\nExperiments on MS-COCO dataset demonstrate that R(Det)$^2$ is effective to\nimprove the detection performance. Equipped with existing detectors, it\nachieves $1.4\\sim 3.6$\\% AP improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Ya-Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Coherent Video Cartoonization with Perceptual Motion Consistency. (arXiv:2204.00795v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00795","description":"<p>In recent years, creative content generations like style transfer and neural\nphoto editing have attracted more and more attention. Among these,\ncartoonization of real-world scenes has promising applications in entertainment\nand industry. Different from image translations focusing on improving the style\neffect of generated images, video cartoonization has additional requirements on\nthe temporal consistency. In this paper, we propose a spatially-adaptive\nsemantic alignment framework with perceptual motion consistency for coherent\nvideo cartoonization in an unsupervised manner. The semantic alignment module\nis designed to restore deformation of semantic structure caused by spatial\ninformation lost in the encoder-decoder architecture. Furthermore, we devise\nthe spatio-temporal correlative map as a style-independent, global-aware\nregularization on the perceptual motion consistency. Deriving from similarity\nmeasurement of high-level features in photo and cartoon frames, it captures\nglobal semantic information beyond raw pixel-value in optical flow. Besides,\nthe similarity measurement disentangles temporal relationships from\ndomain-specific style properties, which helps regularize the temporal\nconsistency without hurting style effects of cartoon images. Qualitative and\nquantitative experiments demonstrate our method is able to generate highly\nstylistic and temporal consistent cartoon videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenhuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huajie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_D/0/1/0/all/0/1\">Dandan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RFVTM: A Recovery and Filtering Vertex Trichotomy Matching for Remote Sensing Image Registration. (arXiv:2204.00818v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00818","description":"<p>Reliable feature point matching is a vital yet challenging process in\nfeature-based image registration. In this paper,a robust feature point matching\nalgorithm called Recovery and Filtering Vertex Trichotomy Matching (RFVTM) is\nproposed to remove outliers and retain sufficient inliers for remote sensing\nimages. A novel affine invariant descriptor called vertex trichotomy descriptor\nis proposed on the basis of that geometrical relations between any of vertices\nand lines are preserved after affine transformations, which is constructed by\nmapping each vertex into trichotomy sets. The outlier removals in Vertex\nTrichotomy Matching (VTM) are implemented by iteratively comparing the\ndisparity of corresponding vertex trichotomy descriptors. Some inliers\nmistakenly validated by a large amount of outliers are removed in VTM\niterations, and several residual outliers close to correct locations cannot be\nexcluded with the same graph structures. Therefore, a recovery and filtering\nstrategy is designed to recover some inliers based on identical vertex\ntrichotomy descriptors and restricted transformation errors. Assisted with the\nadditional recovered inliers, residual outliers can also be filtered out during\nthe process of reaching identical graph for the expanded vertex sets.\nExperimental results demonstrate the superior performance on precision and\nstability of this algorithm under various conditions, such as remote sensing\nimages with large transformations, duplicated patterns, or inconsistent\nspectral content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_M/0/1/0/all/0/1\">Ming Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_B/0/1/0/all/0/1\">Bowen An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yongpeng Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luong_H/0/1/0/all/0/1\">Huynh Van Luong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaup_A/0/1/0/all/0/1\">Andr&#xe9; Kaup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Aware Domain Generalized Segmentation. (arXiv:2204.00822v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00822","description":"<p>Deep models trained on source domain lack generalization when evaluated on\nunseen target domains with different data distributions. The problem becomes\neven more pronounced when we have no access to target domain samples for\nadaptation. In this paper, we address domain generalized semantic segmentation,\nwhere a segmentation model is trained to be domain-invariant without using any\ntarget domain data. Existing approaches to tackle this problem standardize data\ninto a unified distribution. We argue that while such a standardization\npromotes global normalization, the resulting features are not discriminative\nenough to get clear segmentation boundaries. To enhance separation between\ncategories while simultaneously promoting domain invariance, we propose a\nframework including two novel modules: Semantic-Aware Normalization (SAN) and\nSemantic-Aware Whitening (SAW). Specifically, SAN focuses on category-level\ncenter alignment between features from different image styles, while SAW\nenforces distributed alignment for the already center-aligned features. With\nthe help of SAN and SAW, we encourage both intra-category compactness and\ninter-category separability. We validate our approach through extensive\nexperiments on widely-used datasets (i.e. GTAV, SYNTHIA, Cityscapes, Mapillary\nand BDDS). Our approach shows significant improvements over existing\nstate-of-the-art on various backbone networks. Code is available at\nhttps://github.com/leolyj/SAN-SAW\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Duo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Convolutional Re-parameterization. (arXiv:2204.00826v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00826","description":"<p>Structural re-parameterization has drawn increasing attention in various\ncomputer vision tasks. It aims at improving the performance of deep models\nwithout introducing any inference-time cost. Though efficient during inference,\nsuch models rely heavily on the complicated training-time blocks to achieve\nhigh accuracy, leading to large extra training cost. In this paper, we present\nonline convolutional re-parameterization (OREPA), a two-stage pipeline, aiming\nto reduce the huge training overhead by squeezing the complex training-time\nblock into a single convolution. To achieve this goal, we introduce a linear\nscaling layer for better optimizing the online blocks. Assisted with the\nreduced training cost, we also explore some more effective re-param components.\nCompared with the state-of-the-art re-param models, OREPA is able to save the\ntraining-time memory cost by about 70% and accelerate the training speed by\naround 2x. Meanwhile, equipped with OREPA, the models outperform previous\nmethods on ImageNet by up to +0.6%.We also conduct experiments on object\ndetection and semantic segmentation and show consistent improvements on the\ndownstream tasks. Codes are available at\nhttps://github.com/JUGGHM/OREPA_CVPR2022 .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Mu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Junyi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_J/0/1/0/all/0/1\">Jiashen Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_B/0/1/0/all/0/1\">Baisheng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xiaojin Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xiansheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Registration of Images with Inconsistent Content Through Line-Support Region Segmentation and Geometrical Outlier Removal. (arXiv:2204.00832v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00832","description":"<p>The implementation of automatic image registration is still difficult in\nvarious applications. In this paper, an automatic image registration approach\nthrough line-support region segmentation and geometrical outlier removal\n(ALRS-GOR) is proposed. This new approach is designed to address the problems\nassociated with the registration of images with affine deformations and\ninconsistent content, such as remote sensing images with different spectral\ncontent or noise interference, or map images with inconsistent annotations. To\nbegin with, line-support regions, namely a straight region whose points share\nroughly the same image gradient angle, are extracted to address the issues of\ninconsistent content existing in images. To alleviate the incompleteness of\nline segments, an iterative strategy with multi-resolution is employed to\npreserve global structures that are masked at full resolution by image details\nor noise. Then, Geometrical Outlier Removal (GOR) is developed to provide\nreliable feature point matching, which is based on affineinvariant geometrical\nclassifications for corresponding matches initialized by SIFT. The candidate\noutliers are selected by comparing the disparity of accumulated classifications\namong all matches, instead of conventional methods which only rely on local\ngeometrical relations. Various image sets have been considered in this paper\nfor the evaluation of the proposed approach, including aerial images with\nsimulated affine deformations, remote sensing optical and synthetic aperture\nradar images taken at different situations (multispectral, multisensor, and\nmultitemporal), and map images with inconsistent annotations. Experimental\nresults demonstrate the superior performance of the proposed method over the\nexisting approaches for the whole data set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_M/0/1/0/all/0/1\">Ming Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yongpeng Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_S/0/1/0/all/0/1\">Shengda Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_F/0/1/0/all/0/1\">Fan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_B/0/1/0/all/0/1\">Bowen An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaup_A/0/1/0/all/0/1\">Andr&#xe9; Kaup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00833","description":"<p>Pixel synthesis is a promising research paradigm for image generation, which\ncan well exploit pixel-wise prior knowledge for generation. However, existing\nmethods still suffer from excessive memory footprint and computation overhead.\nIn this paper, we propose a progressive pixel synthesis network towards\nefficient image generation, coined as PixelFolder. Specifically, PixelFolder\nformulates image generation as a progressive pixel regression problem and\nsynthesizes images by a multi-stage paradigm, which can greatly reduce the\noverhead caused by large tensor transformations. In addition, we introduce\nnovel pixel folding operations to further improve model efficiency while\nmaintaining pixel-wise prior knowledge for end-to-end regression. With these\ninnovative designs, we greatly reduce the expenditure of pixel synthesis, e.g.,\nreducing 90% computation and 57% parameters compared to the latest pixel\nsynthesis method called CIPS. To validate our approach, we conduct extensive\nexperiments on two benchmark datasets, namely FFHQ and LSUN Church. The\nexperimental results show that with much less expenditure, PixelFolder obtains\nnew state-of-the-art (SOTA) performance on two benchmark datasets, i.e., 3.77\nFID and 2.45 FID on FFHQ and LSUN Church, respectively. Meanwhile, PixelFolder\nis also more efficient than the SOTA methods like StyleGAN2, reducing about 74%\ncomputation and 36% parameters, respectively. These results greatly validate\nthe effectiveness of the proposed PixelFolder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rotated Object Detection via Scale-invariant Mahalanobis Distance in Aerial Images. (arXiv:2204.00840v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00840","description":"<p>Rotated object detection in aerial images is a meaningful yet challenging\ntask as objects are densely arranged and have arbitrary orientations. The\neight-parameter (coordinates of box vectors) methods in rotated object\ndetection usually use ln-norm losses (L1 loss, L2 loss, and smooth L1 loss) as\nloss functions. As ln-norm losses are mainly based on non-scale-invariant\nMinkowski distance, using ln-norm losses will lead to inconsistency with the\ndetection metric rotational Intersection-over-Union (IoU) and training\ninstability. To address the problems, we use Mahalanobis distance to calculate\nloss between the predicted and the target box vertices' vectors, proposing a\nnew loss function called Mahalanobis Distance Loss (MDL) for eight-parameter\nrotated object detection. As Mahalanobis distance is scale-invariant, MDL is\nmore consistent with detection metric than ln-norm losses and more stable\nduring training. To alleviate the problem of boundary discontinuity like all\nother eight-parameter methods, we further take the minimum loss value to make\nMDL continuous at boundary cases. We achieve state-of-art performance on\nDOTA-v1.0 with the proposed method MDL. Furthermore, with the comparative\nexperiment of smooth L1 loss under the same condi-tion, we find that MDL\nperforms better in rotated object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Siyang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruijie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Neon Beam: Robust Physical-World Adversarial Attack to DNNs. (arXiv:2204.00853v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00853","description":"<p>In the physical world, light affects the performance of deep neural networks.\nNowadays, many products based on deep neural network have been put into daily\nlife. There are few researches on the effect of light on the performance of\ndeep neural network models. However, the adversarial perturbations generated by\nlight may have extremely dangerous effects on these systems. In this work, we\npropose an attack method called adversarial neon beam (AdvNB), which can\nexecute the physical attack by obtaining the physical parameters of adversarial\nneon beams with very few queries. Experiments show that our algorithm can\nachieve advanced attack effect in both digital test and physical test. In the\ndigital environment, 99.3% attack success rate was achieved, and in the\nphysical environment, 100% attack success rate was achieved. Compared with the\nmost advanced physical attack methods, our method can achieve better physical\nperturbation concealment. In addition, by analyzing the experimental data, we\nreveal some new phenomena brought about by the adversarial neon beam attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chengyin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiliwalidi_K/0/1/0/all/0/1\">Kalibinuer Tiliwalidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Acoustic-to-articulatory Inversion based on Speech Decomposition and Auxiliary Feature. (arXiv:2204.00873v1 [cs.SD])","link":"http://arxiv.org/abs/2204.00873","description":"<p>Acoustic-to-articulatory inversion (AAI) is to obtain the movement of\narticulators from speech signals. Until now, achieving a speaker-independent\nAAI remains a challenge given the limited data. Besides, most current works\nonly use audio speech as input, causing an inevitable performance bottleneck.\nTo solve these problems, firstly, we pre-train a speech decomposition network\nto decompose audio speech into speaker embedding and content embedding as the\nnew personalized speech features to adapt to the speaker-independent case.\nSecondly, to further improve the AAI, we propose a novel auxiliary feature\nnetwork to estimate the lip auxiliary features from the above personalized\nspeech features. Experimental results on three public datasets show that,\ncompared with the state-of-the-art only using the audio speech feature, the\nproposed method reduces the average RMSE by 0.25 and increases the average\ncorrelation coefficient by 2.0% in the speaker-dependent case. More\nimportantly, the average RMSE decreases by 0.29 and the average correlation\ncoefficient increases by 5.0% in the speaker-independent case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianrong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Longxuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Ruiguo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moment-based Adversarial Training for Embodied Language Comprehension. (arXiv:2204.00889v1 [cs.RO])","link":"http://arxiv.org/abs/2204.00889","description":"<p>In this paper, we focus on a vision-and-language task in which a robot is\ninstructed to execute household tasks. Given an instruction such as \"Rinse off\na mug and place it in the coffee maker,\" the robot is required to locate the\nmug, wash it, and put it in the coffee maker. This is challenging because the\nrobot needs to break down the instruction sentences into subgoals and execute\nthem in the correct order. On the ALFRED benchmark, the performance of\nstate-of-the-art methods is still far lower than that of humans. This is\npartially because existing methods sometimes fail to infer subgoals that are\nnot explicitly specified in the instruction sentences. We propose Moment-based\nAdversarial Training (MAT), which uses two types of moments for perturbation\nupdates in adversarial training. We introduce MAT to the embedding spaces of\nthe instruction, subgoals, and state representations to handle their varieties.\nWe validated our method on the ALFRED benchmark, and the results demonstrated\nthat our method outperformed the baseline method for all the metrics on the\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_S/0/1/0/all/0/1\">Shintaro Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Free Lunch to Person Re-identification: Learning from Automatically Generated Noisy Tracklets. (arXiv:2204.00891v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00891","description":"<p>A series of unsupervised video-based re-identification (re-ID) methods have\nbeen proposed to solve the problem of high labor cost required to annotate\nre-ID datasets. But their performance is still far lower than the supervised\ncounterparts. In the mean time, clean datasets without noise are used in these\nmethods, which is not realistic. In this paper, we propose to tackle this\nproblem by learning re-ID models from automatically generated person tracklets\nby multiple objects tracking (MOT) algorithm. To this end, we design a\ntracklet-based multi-level clustering (TMC) framework to effectively learn the\nre-ID model from the noisy person tracklets. First, intra-tracklet isolation to\nreduce ID switch noise within tracklets; second, alternates between using\ninter-tracklet association to eliminate ID fragmentation noise and network\ntraining using the pseudo label. Extensive experiments on MARS with various\nmanually generated noises show the effectiveness of the proposed framework.\nSpecifically, the proposed framework achieved mAP 53.4% and rank-1 63.7% on the\nsimulated tracklets with strongest noise, even outperforming the best existing\nmethod on clean tracklets. Based on the results, we believe that building re-ID\nmodels from automatically generated noisy tracklets is a reasonable approach\nand will also be an important way to make re-ID models feasible in real-world\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teng_H/0/1/0/all/0/1\">Hehan Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuchen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mix-up Self-Supervised Learning for Contrast-agnostic Applications. (arXiv:2204.00901v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00901","description":"<p>Contrastive self-supervised learning has attracted significant research\nattention recently. It learns effective visual representations from unlabeled\ndata by embedding augmented views of the same image close to each other while\npushing away embeddings of different images. Despite its great success on\nImageNet classification, COCO object detection, etc., its performance degrades\non contrast-agnostic applications, e.g., medical image classification, where\nall images are visually similar to each other. This creates difficulties in\noptimizing the embedding space as the distance between images is rather small.\nTo solve this issue, we present the first mix-up self-supervised learning\nframework for contrast-agnostic applications. We address the low variance\nacross images based on cross-domain mix-up and build the pretext task based on\ntwo synergistic objectives: image reconstruction and transparency prediction.\nExperimental results on two benchmark datasets validate the effectiveness of\nour method, where an improvement of 2.5% ~ 7.4% in top-1 accuracy was obtained\ncompared to existing self-supervised learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yifang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roger Zimmermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Algebraic Fitting for Multiple Circle Primitives Extraction from Raw Point Clouds. (arXiv:2204.00920v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00920","description":"<p>The shape of circle is one of fundamental geometric primitives of man-made\nengineering objects. Thus, extraction of circles from scanned point clouds is a\nquite important task in 3D geometry data processing. However, existing circle\nextraction methods either are sensitive to the quality of raw point clouds when\nclassifying circle-boundary points, or require well-designed fitting functions\nwhen regressing circle parameters. To relieve the challenges, we propose an\nend-to-end Point Cloud Circle Algebraic Fitting Network (Circle-Net) based on a\nsynergy of deep circle-boundary point feature learning and weighted algebraic\nfitting. First, we design a circle-boundary learning module, which considers\nlocal and global neighboring contexts of each point, to detect all potential\ncircle-boundary points. Second, we develop a deep feature based circle\nparameter learning module for weighted algebraic fitting, without designing any\nweight metric, to avoid the influence of outliers during fitting. Unlike most\nof the cutting-edge circle extraction wisdoms, the proposed\nclassification-and-fitting modules are originally co-trained with a\ncomprehensive loss to enhance the quality of extracted circles.Comparisons on\nthe established dataset and real-scanned point clouds exhibit clear\nimprovements of Circle-Net over SOTAs in terms of both noise-robustness and\nextraction accuracy. We will release our code, model, and data for both\ntraining and evaluation on GitHub upon publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zeyong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Honghua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word separation in continuous sign language using isolated signs and post-processing. (arXiv:2204.00923v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00923","description":"<p>Continuous Sign Language Recognition (CSLR) is a long challenging task in\nComputer Vision due to the difficulties in detecting the explicit boundaries\nbetween the words in a sign sentence. To deal with this challenge, we propose a\ntwo-stage model. In the first stage, the predictor model, which includes a\ncombination of CNN, SVD, and LSTM, is trained with the isolated signs. In the\nsecond stage, we apply a post-processing algorithm to the Softmax outputs\nobtained from the first part of the model in order to separate the isolated\nsigns in the continuous signs. Due to the lack of a large dataset, including\nboth the sign sequences and the corresponding isolated signs, two public\ndatasets in Isolated Sign Language Recognition (ISLR), RKS-PERSIANSIGN and\nASLVID, are used for evaluation. Results of the continuous sign videos confirm\nthe efficiency of the proposed model to deal with isolated sign boundaries\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rastgoo_R/0/1/0/all/0/1\">Razieh Rastgoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1\">Kourosh Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image. (arXiv:2204.00928v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00928","description":"<p>Despite the rapid development of Neural Radiance Field (NeRF), the necessity\nof dense covers largely prohibits its wider applications. While several recent\nworks have attempted to address this issue, they either operate with sparse\nviews (yet still, a few of them) or on simple objects/scenes. In this work, we\nconsider a more ambitious task: training neural radiance field, over\nrealistically complex visual scenes, by \"looking only once\", i.e., using only a\nsingle view. To attain this goal, we present a Single View NeRF (SinNeRF)\nframework consisting of thoughtfully designed semantic and geometry\nregularizations. Specifically, SinNeRF constructs a semi-supervised learning\nprocess, where we introduce and propagate geometry pseudo labels and semantic\npseudo labels to guide the progressive training process. Extensive experiments\nare conducted on complex scene benchmarks, including NeRF synthetic dataset,\nLocal Light Field Fusion dataset, and DTU dataset. We show that even without\npre-training on multi-view datasets, SinNeRF can yield photo-realistic\nnovel-view synthesis results. Under the single image setting, SinNeRF\nsignificantly outperforms the current state-of-the-art NeRF baselines in all\ncases. Project page: https://vita-group.github.io/SinNeRF/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dejia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhiwen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A-ACT: Action Anticipation through Cycle Transformations. (arXiv:2204.00942v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00942","description":"<p>While action anticipation has garnered a lot of research interest recently,\nmost of the works focus on anticipating future action directly through observed\nvisual cues only. In this work, we take a step back to analyze how the human\ncapability to anticipate the future can be transferred to machine learning\nalgorithms. To incorporate this ability in intelligent systems a question worth\npondering upon is how exactly do we anticipate? Is it by anticipating future\nactions from past experiences? Or is it by simulating possible scenarios based\non cues from the present? A recent study on human psychology explains that, in\nanticipating an occurrence, the human brain counts on both systems. In this\nwork, we study the impact of each system for the task of action anticipation\nand introduce a paradigm to integrate them in a learning framework. We believe\nthat intelligent systems designed by leveraging the psychological anticipation\nmodels will do a more nuanced job at the task of human action prediction.\nFurthermore, we introduce cyclic transformation in the temporal dimension in\nfeature and semantic label space to instill the human ability of reasoning of\npast actions based on the predicted future. Experiments on Epic-Kitchen,\nBreakfast, and 50Salads dataset demonstrate that the action anticipation model\nlearned using a combination of the two systems along with the cycle\ntransformation performs favorably against various state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akash Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1\">Liefeng Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TripleNet: A Low Computing Power Platform of Low-Parameter Network. (arXiv:2204.00943v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00943","description":"<p>With the excellent performance of deep learning technology in the field of\ncomputer vision, convolutional neural network (CNN) architecture has become the\nmain backbone of computer vision task technology. With the widespread use of\nmobile devices, neural network models based on platforms with low computing\npower are gradually being paid attention. This paper proposes a lightweight\nconvolutional neural network model, TripleNet, an improved convolutional neural\nnetwork based on HarDNet and ThreshNet, inheriting the advantages of small\nmemory usage and low power consumption of the mentioned two models. TripleNet\nuses three different convolutional layers combined into a new model\narchitecture, which has less number of parameters than that of HarDNet and\nThreshNet. CIFAR-10 and SVHN datasets were used for image classification by\nemploying HarDNet, ThreshNet, and our proposed TripleNet for verification.\nExperimental results show that, compared with HarDNet, TripleNet's parameters\nare reduced by 66% and its accuracy rate is increased by 18%; compared with\nThreshNet, TripleNet's parameters are reduced by 37% and its accuracy rate is\nincreased by 5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_R/0/1/0/all/0/1\">Rui-Yang Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_J/0/1/0/all/0/1\">Jia-Hao Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1\">Jen-Shiun Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Minimal Path Method with Embedded CNN. (arXiv:2204.00944v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00944","description":"<p>We propose Path-CNN, a method for the segmentation of centerlines of tubular\nstructures by embedding convolutional neural networks (CNNs) into the\nprogressive minimal path method. Minimal path methods are widely used for\ntopology-aware centerline segmentation, but usually these methods rely on weak,\nhand-tuned image features. In contrast, CNNs use strong image features which\nare learned automatically from images. But CNNs usually do not take the\ntopology of the results into account, and often require a large amount of\nannotations for training. We integrate CNNs into the minimal path method, so\nthat both techniques benefit from each other: CNNs employ learned image\nfeatures to improve the determination of minimal paths, while the minimal path\nmethod ensures the correct topology of the segmented centerlines, provides\nstrong geometric priors to increase the performance of CNNs, and reduces the\namount of annotations for the training of CNNs significantly. Our method has\nlower hardware requirements than many recent methods. Qualitative and\nquantitative comparison with other methods shows that Path-CNN achieves better\nperformance, especially when dealing with tubular structures with complex\nshapes in challenging environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wei Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching Feature Sets for Few-Shot Image Classification. (arXiv:2204.00949v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00949","description":"<p>In image classification, it is common practice to train deep networks to\nextract a single feature vector per input image. Few-shot classification\nmethods also mostly follow this trend. In this work, we depart from this\nestablished direction and instead propose to extract sets of feature vectors\nfor each image. We argue that a set-based representation intrinsically builds a\nricher representation of images from the base classes, which can subsequently\nbetter transfer to the few-shot classes. To do so, we propose to adapt existing\nfeature extractors to instead produce sets of feature vectors from images. Our\napproach, dubbed SetFeat, embeds shallow self-attention mechanisms inside\nexisting encoder architectures. The attention modules are lightweight, and as\nsuch our method results in encoders that have approximately the same number of\nparameters as their original versions. During training and inference, a\nset-to-set matching metric is used to perform image classification. The\neffectiveness of our proposed architecture and metrics is demonstrated via\nthorough experiments on standard few-shot datasets -- namely miniImageNet,\ntieredImageNet, and CUB -- in both the 1- and 5-shot scenarios. In all cases\nbut one, our method outperforms the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afrasiyabi_A/0/1/0/all/0/1\">Arman Afrasiyabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1\">Hugo Larochelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Lalonde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1\">Christian Gagn&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sentinel-2 multi-year, multi-country benchmark dataset for crop classification and segmentation with deep learning. (arXiv:2204.00951v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00951","description":"<p>In this work we introduce Sen4AgriNet, a Sentinel-2 based time series multi\ncountry benchmark dataset, tailored for agricultural monitoring applications\nwith Machine and Deep Learning. Sen4AgriNet dataset is annotated from farmer\ndeclarations collected via the Land Parcel Identification System (LPIS) for\nharmonizing country wide labels. These declarations have only recently been\nmade available as open data, allowing for the first time the labeling of\nsatellite imagery from ground truth data. We proceed to propose and standardise\na new crop type taxonomy across Europe that address Common Agriculture Policy\n(CAP) needs, based on the Food and Agriculture Organization (FAO) Indicative\nCrop Classification scheme. Sen4AgriNet is the only multi-country, multi-year\ndataset that includes all spectral information. It is constructed to cover the\nperiod 2016-2020 for Catalonia and France, while it can be extended to include\nadditional countries. Currently, it contains 42.5 million parcels, which makes\nit significantly larger than other available archives. We extract two\nsub-datasets to highlight its value for diverse Deep Learning applications; the\nObject Aggregated Dataset (OAD) and the Patches Assembled Dataset (PAD). OAD\ncapitalizes zonal statistics of each parcel, thus creating a powerful\nlabel-to-features instance for classification algorithms. On the other hand,\nPAD structure generalizes the classification problem to parcel extraction and\nsemantic segmentation and labeling. The PAD and OAD are examined under three\ndifferent scenarios to showcase and model the effects of spatial and temporal\nvariability across different years and different countries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sykas_D/0/1/0/all/0/1\">Dimitrios Sykas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sdraka_M/0/1/0/all/0/1\">Maria Sdraka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zografakis_D/0/1/0/all/0/1\">Dimitrios Zografakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1\">Ioannis Papoutsis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaFace: Quality Adaptive Margin for Face Recognition. (arXiv:2204.00964v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00964","description":"<p>Recognition in low quality face datasets is challenging because facial\nattributes are obscured and degraded. Advances in margin-based loss functions\nhave resulted in enhanced discriminability of faces in the embedding space.\nFurther, previous studies have studied the effect of adaptive losses to assign\nmore importance to misclassified (hard) examples. In this work, we introduce\nanother aspect of adaptiveness in the loss function, namely the image quality.\nWe argue that the strategy to emphasize misclassified samples should be\nadjusted according to their image quality. Specifically, the relative\nimportance of easy or hard samples should be based on the sample's image\nquality. We propose a new loss function that emphasizes samples of different\ndifficulties based on their image quality. Our method achieves this in the form\nof an adaptive margin function by approximating the image quality with feature\nnorms. Extensive experiments show that our method, AdaFace, improves the face\nrecognition performance over the state-of-the-art (SoTA) on four datasets\n(IJB-B, IJB-C, IJB-S and TinyFace). Code and models are released in\nhttps://github.com/mk-minchul/AdaFace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minchul Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DST: Dynamic Substitute Training for Data-free Black-box Attack. (arXiv:2204.00972v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00972","description":"<p>With the wide applications of deep neural network models in various computer\nvision tasks, more and more works study the model vulnerability to adversarial\nexamples. For data-free black box attack scenario, existing methods are\ninspired by the knowledge distillation, and thus usually train a substitute\nmodel to learn knowledge from the target model using generated data as input.\nHowever, the substitute model always has a static network structure, which\nlimits the attack ability for various target models and tasks. In this paper,\nwe propose a novel dynamic substitute training attack method to encourage\nsubstitute model to learn better and faster from the target model.\nSpecifically, a dynamic substitute structure learning strategy is proposed to\nadaptively generate optimal substitute model structure via a dynamic gate\naccording to different target models and tasks. Moreover, we introduce a\ntask-driven graph-based structure information learning constrain to improve the\nquality of generated training data, and facilitate the substitute model\nlearning structural relationships from the target model multiple outputs.\nExtensive experiments have been conducted to verify the efficacy of the\nproposed attack method, which can achieve better performance compared with the\nstate-of-the-art competitors on several datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuelin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kernel Extreme Learning Machine Optimized by the Sparrow Search Algorithm for Hyperspectral Image Classification. (arXiv:2204.00973v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00973","description":"<p>To improve the classification performance and generalization ability of the\nhyperspectral image classification algorithm, this paper uses Multi-Scale Total\nVariation (MSTV) to extract the spectral features, local binary pattern (LBP)\nto extract spatial features, and feature superposition to obtain the fused\nfeatures of hyperspectral images. A new swarm intelligence optimization method\nwith high convergence and strong global search capability, the Sparrow Search\nAlgorithm (SSA), is used to optimize the kernel parameters and regularization\ncoefficients of the Kernel Extreme Learning Machine (KELM). In summary, a\nmultiscale fusion feature hyperspectral image classification method (MLS-KELM)\nis proposed in this paper. The Indian Pines, Pavia University and Houston 2013\ndatasets were selected to validate the classification performance of MLS-KELM,\nand the method was applied to ZY1-02D hyperspectral data. The experimental\nresults show that MLS-KELM has better classification performance and\ngeneralization ability compared with other popular classification methods, and\nMLS-KELM shows its strong robustness in the small sample case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhixin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiawei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_K/0/1/0/all/0/1\">Kehua Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Global Shutter: Learn to Restore Video from a Rolling Shutter Camera with Global Reset Feature. (arXiv:2204.00974v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00974","description":"<p>Most computer vision systems assume distortion-free images as inputs. The\nwidely used rolling-shutter (RS) image sensors, however, suffer from geometric\ndistortion when the camera and object undergo motion during capture. Extensive\nresearches have been conducted on correcting RS distortions. However, most of\nthe existing work relies heavily on the prior assumptions of scenes or motions.\nBesides, the motion estimation steps are either oversimplified or\ncomputationally inefficient due to the heavy flow warping, limiting their\napplicability. In this paper, we investigate using rolling shutter with a\nglobal reset feature (RSGR) to restore clean global shutter (GS) videos. This\nfeature enables us to turn the rectification problem into a deblur-like one,\ngetting rid of inaccurate and costly explicit motion estimation. First, we\nbuild an optic system that captures paired RSGR/GS videos. Second, we develop a\nnovel algorithm incorporating spatial and temporal designs to correct the\nspatial-varying RSGR distortion. Third, we demonstrate that existing\nimage-to-image translation algorithms can recover clean GS videos from\ndistorted RSGR inputs, yet our algorithm achieves the best performance with the\nspecific designs. Our rendered results are not only visually appealing but also\nbeneficial to downstream tasks. Compared to the state-of-the-art RS solution,\nour RSGR solution is superior in both effectiveness and efficiency. Considering\nit is easy to realize without changing the hardware, we believe our RSGR\nsolution can potentially replace the RS solution in taking distortion-free\nvideos with low noise and low budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhixiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_S/0/1/0/all/0/1\">Shin&#x27;ichi Satoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question-Driven Graph Fusion Network For Visual Question Answering. (arXiv:2204.00975v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00975","description":"<p>Existing Visual Question Answering (VQA) models have explored various visual\nrelationships between objects in the image to answer complex questions, which\ninevitably introduces irrelevant information brought by inaccurate object\ndetection and text grounding. To address the problem, we propose a\nQuestion-Driven Graph Fusion Network (QD-GFN). It first models semantic,\nspatial, and implicit visual relations in images by three graph attention\nnetworks, then question information is utilized to guide the aggregation\nprocess of the three graphs, further, our QD-GFN adopts an object filtering\nmechanism to remove question-irrelevant objects contained in the image.\nExperiment results demonstrate that our QD-GFN outperforms the prior\nstate-of-the-art on both VQA 2.0 and VQA-CP v2 datasets. Further analysis shows\nthat both the novel graph aggregation method and object filtering mechanism\nplay a significant role in improving the performance of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuxi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuncong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fangxiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BinsFormer: Revisiting Adaptive Bins for Monocular Depth Estimation. (arXiv:2204.00987v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00987","description":"<p>Monocular depth estimation is a fundamental task in computer vision and has\ndrawn increasing attention. Recently, some methods reformulate it as a\nclassification-regression task to boost the model performance, where continuous\ndepth is estimated via a linear combination of predicted probability\ndistributions and discrete bins. In this paper, we present a novel framework\ncalled BinsFormer, tailored for the classification-regression-based depth\nestimation. It mainly focuses on two crucial components in the specific task:\n1) proper generation of adaptive bins and 2) sufficient interaction between\nprobability distribution and bins predictions. To specify, we employ the\nTransformer decoder to generate bins, novelly viewing it as a direct set-to-set\nprediction problem. We further integrate a multi-scale decoder structure to\nachieve a comprehensive understanding of spatial geometry information and\nestimate depth maps in a coarse-to-fine manner. Moreover, an extra scene\nunderstanding query is proposed to improve the estimation accuracy, which turns\nout that models can implicitly learn useful information from an auxiliary\nenvironment classification task. Extensive experiments on the KITTI, NYU, and\nSUN RGB-D datasets demonstrate that BinsFormer surpasses state-of-the-art\nmonocular depth estimation methods with prominent margins. Code and pretrained\nmodels will be made publicly available at\n\\url{https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POS-BERT: Point Cloud One-Stage BERT Pre-Training. (arXiv:2204.00989v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00989","description":"<p>Recently, the pre-training paradigm combining Transformer and masked language\nmodeling has achieved tremendous success in NLP, images, and point clouds, such\nas BERT. However, directly extending BERT from NLP to point clouds requires\ntraining a fixed discrete Variational AutoEncoder (dVAE) before pre-training,\nwhich results in a complex two-stage method called Point-BERT. Inspired by BERT\nand MoCo, we propose POS-BERT, a one-stage BERT pre-training method for point\nclouds. Specifically, we use the mask patch modeling (MPM) task to perform\npoint cloud pre-training, which aims to recover masked patches information\nunder the supervision of the corresponding tokenizer output. Unlike Point-BERT,\nits tokenizer is extra-trained and frozen. We propose to use the dynamically\nupdated momentum encoder as the tokenizer, which is updated and outputs the\ndynamic supervision signal along with the training process. Further, in order\nto learn high-level semantic representation, we combine contrastive learning to\nmaximize the class token consistency between different transformation point\nclouds. Extensive experiments have demonstrated that POS-BERT can extract\nhigh-quality pre-training features and promote downstream tasks to improve\nperformance. Using the pre-training model without any fine-tuning to extract\nfeatures and train linear SVM on ModelNet40, POS-BERT achieves the\nstate-of-the-art classification accuracy, which exceeds Point-BERT by 3.5\\%. In\naddition, our approach has significantly improved many downstream tasks, such\nas fine-tuned classification, few-shot classification, part segmentation. The\ncode and trained-models will be available at:\n\\url{https://github.com/fukexue/POS-BERT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kexue Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">ShaoLei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Manning Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Vision Transformers by Revisiting High-frequency Components. (arXiv:2204.00993v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00993","description":"<p>The transformer models have shown promising effectiveness in dealing with\nvarious vision tasks. However, compared with training Convolutional Neural\nNetwork (CNN) models, training Vision Transformer (ViT) models is more\ndifficult and relies on the large-scale training set. To explain this\nobservation we make a hypothesis that ViT models are less effective in\ncapturing the high-frequency components of images than CNN models, and verify\nit by a frequency analysis. Inspired by this finding, we first investigate the\neffects of existing techniques for improving ViT models from a new frequency\nperspective, and find that the success of some techniques (e.g., RandAugment)\ncan be attributed to the better usage of the high-frequency components. Then,\nto compensate for this insufficient ability of ViT models, we propose HAT,\nwhich directly augments high-frequency components of images via adversarial\ntraining. We show that HAT can consistently boost the performance of various\nViT models (e.g., +1.2% for ViT-B, +0.5% for Swin-B), and especially enhance\nthe advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the\nsuperiority can also be maintained on out-of-distribution data and transferred\nto downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiawang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-aware Attention for Image Inpainting. (arXiv:2204.01004v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01004","description":"<p>Recent attention-based image inpainting methods have made inspiring progress\nby modeling long-range dependencies within a single image. However, they tend\nto generate blurry contents since the correlation between each pixel pairs is\nalways misled by ill-predicted features in holes. To handle this problem, we\npropose a novel region-aware attention (RA) module. By avoiding the directly\ncalculating corralation between each pixel pair in a single samples and\nconsidering the correlation between different samples, the misleading of\ninvalid information in holes can be avoided. Meanwhile, a learnable region\ndictionary (LRD) is introduced to store important information in the entire\ndataset, which not only simplifies correlation modeling, but also avoids\ninformation redundancy. By applying RA in our architecture, our methodscan\ngenerate semantically plausible results with realistic details. Extensive\nexperiments on CelebA, Places2 and Paris StreetView datasets validate the\nsuperiority of our method compared with existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhilin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chujun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zhenyu Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuesheng Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gastrointestinal Polyps and Tumors Detection Based on Multi-scale Feature-fusion with WCE Sequences. (arXiv:2204.01012v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01012","description":"<p>Wireless Capsule Endoscopy(WCE) has been widely used for the screening of\ngastrointestinal(GI) diseases, especially the small intestine, due to its\nadvantages of non-invasive and painless imaging of the entire digestive\ntract.However, the huge amount of image data captured by WCE makes manual\nreading a process that requires a huge amount of tasks and can easily lead to\nmissed detection and false detection of lesions.Therefore, In this paper, we\npropose a \\textbf{T}wo-stage \\textbf{M}ulti-scale \\textbf{F}eature-fusion\nlearning network(\\textbf{TMFNet}) to automatically detect small intestinal\npolyps and tumors in WCE image sequences. Specifically, TMFNet consists of\nlesion detection network and lesion identification network. Among them, the\nformer improves the feature extraction module and detection module based on the\ntraditional Faster R-CNN network, and readjusts the parameters of the anchor in\nthe region proposal network(RPN) module;the latter combines residual structure\nand feature pyramid structure are used to build a small intestinal lesion\nrecognition network based on feature fusion, for reducing the false positive\nrate of the former and improve the overall accuracy.We used 22,335 WCE images\nin the experiment, with a total of 123,092 lesion regions used to train the\ndetection framework of this paper. In the experiment, the detection framework\nis trained and tested on the real WCE image dataset provided by the hospital\ngastroenterology department. The sensitivity, false positive and accuracy of\nthe final model on the RPM are 98.81$\\%$, 7.43$\\%$ and 92.57$\\%$,\nrespectively.Meanwhile,the corresponding results on the lesion images were\n98.75$\\%$, 5.62$\\%$ and 94.39$\\%$. The algorithm model proposed in this paper\nis obviously superior to other detection algorithms in detection effect and\nperformance\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Falin_Z/0/1/0/all/0/1\">Zhuo Falin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haihua_L/0/1/0/all/0/1\">Liu Haihua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ning_P/0/1/0/all/0/1\">Pan Ning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for Repetitive Action Counting. (arXiv:2204.01018v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01018","description":"<p>Counting repetitive actions are widely seen in human activities such as\nphysical exercise. Existing methods focus on performing repetitive action\ncounting in short videos, which is tough for dealing with longer videos in more\nrealistic scenarios. In the data-driven era, the degradation of such\ngeneralization capability is mainly attributed to the lack of long video\ndatasets. To complement this margin, we introduce a new large-scale repetitive\naction counting dataset covering a wide variety of video lengths, along with\nmore realistic situations where action interruption or action inconsistencies\noccur in the video. Besides, we also provide a fine-grained annotation of the\naction cycles instead of just counting annotation along with a numerical value.\nSuch a dataset contains 1,451 videos with about 20,000 annotations, which is\nmore challenging. For repetitive action counting towards more realistic\nscenarios, we further propose encoding multi-scale temporal correlation with\ntransformers that can take into account both performance and efficiency.\nFurthermore, with the help of fine-grained annotation of action cycles, we\npropose a density map regression-based method to predict the action period,\nwhich yields better performance with sufficient interpretability. Our proposed\nmethod outperforms state-of-the-art methods on all datasets and also achieves\nbetter performance on the unseen dataset without fine-tuning. The dataset and\ncode are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huazhang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Sixun Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiqun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Dongze Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes. (arXiv:2204.01026v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01026","description":"<p>Accurately detecting and tracking pedestrians in 3D space is challenging due\nto large variations in rotations, poses and scales. The situation becomes even\nworse for dense crowds with severe occlusions. However, existing benchmarks\neither only provide 2D annotations, or have limited 3D annotations with\nlow-density pedestrian distribution, making it difficult to build a reliable\npedestrian perception system especially in crowded scenes. To better evaluate\npedestrian perception algorithms in crowded scenarios, we introduce a\nlarge-scale multimodal dataset,STCrowd. Specifically, in STCrowd, there are a\ntotal of 219 K pedestrian instances and 20 persons per frame on average, with\nvarious levels of occlusion. We provide synchronized LiDAR point clouds and\ncamera images as well as their corresponding 3D labels and joint IDs. STCrowd\ncan be used for various tasks, including LiDAR-only, image-only, and\nsensor-fusion based pedestrian detection and tracking. We provide baselines for\nmost of the tasks. In addition, considering the property of sparse global\ndistribution and density-varying local distribution of pedestrians, we further\npropose a novel method, Density-aware Hierarchical heatmap Aggregation (DHA),\nto enhance pedestrian perception in crowded scenes. Extensive experiments show\nthat our new method achieves state-of-the-art performance for pedestrian\ndetection on various datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cong_P/0/1/0/all/0/1\">Peishan Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_F/0/1/0/all/0/1\">Feng Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yiming Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xidong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yuenan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruigang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distortion-Aware Self-Supervised 360{\\deg} Depth Estimation from A Single Equirectangular Projection Image. (arXiv:2204.01027v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01027","description":"<p>360{\\deg} images are widely available over the last few years. This paper\nproposes a new technique for single 360{\\deg} image depth prediction under open\nenvironments. Depth prediction from a 360{\\deg} single image is not easy for\ntwo reasons. One is the limitation of supervision datasets - the currently\navailable dataset is limited to indoor scenes. The other is the problems caused\nby Equirectangular Projection Format (ERP), commonly used for 360{\\deg} images,\nthat are coordinate and distortion. There is only one method existing that uses\ncube map projection to produce six perspective images and apply self-supervised\nlearning using motion pictures for perspective depth prediction to deal with\nthese problems. Different from the existing method, we directly use the ERP\nformat. We propose a framework of direct use of ERP with coordinate conversion\nof correspondences and distortion-aware upsampling module to deal with the ERP\nrelated problems and extend a self-supervised learning method for open\nenvironments. For the experiments, we firstly built a dataset for the\nevaluation, and quantitatively evaluate the depth prediction in outdoor scenes.\nWe show that it outperforms the state-of-the-art technique\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_Y/0/1/0/all/0/1\">Yuya Hasegawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoshi_I/0/1/0/all/0/1\">Ikehata Satoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_K/0/1/0/all/0/1\">Kiyoharu Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style-Based Global Appearance Flow for Virtual Try-On. (arXiv:2204.01046v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01046","description":"<p>Image-based virtual try-on aims to fit an in-shop garment into a clothed\nperson image. To achieve this, a key step is garment warping which spatially\naligns the target garment with the corresponding body parts in the person\nimage. Prior methods typically adopt a local appearance flow estimation model.\nThey are thus intrinsically susceptible to difficult body poses/occlusions and\nlarge mis-alignments between person and garment images (see\nFig.~\\ref{fig:fig1}). To overcome this limitation, a novel global appearance\nflow estimation model is proposed in this work. For the first time, a StyleGAN\nbased architecture is adopted for appearance flow estimation. This enables us\nto take advantage of a global style vector to encode a whole-image context to\ncope with the aforementioned challenges. To guide the StyleGAN flow generator\nto pay more attention to local garment deformation, a flow refinement module is\nintroduced to add local context. Experiment results on a popular virtual try-on\nbenchmark show that our method achieves new state-of-the-art performance. It is\nparticularly effective in a `in-the-wild' application scenario where the\nreference image is full-body resulting in a large mis-alignment with the\ngarment image (Fig.~\\ref{fig:fig1} Top). Code is available at:\n\\url{https://github.com/SenHe/Flow-Style-VTON}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In Rain or Shine: Understanding and Overcoming Dataset Bias for Improving Robustness Against Weather Corruptions for Autonomous Vehicles. (arXiv:2204.01062v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01062","description":"<p>Several popular computer vision (CV) datasets, specifically employed for\nObject Detection (OD) in autonomous driving tasks exhibit biases due to a range\nof factors including weather and lighting conditions. These biases may impair a\nmodel's generalizability, rendering it ineffective for OD in novel and unseen\ndatasets. Especially, in autonomous driving, it may prove extremely high risk\nand unsafe for the vehicle and its surroundings. This work focuses on\nunderstanding these datasets better by identifying such \"good-weather\" bias.\nMethods to mitigate such bias which allows the OD models to perform better and\nimprove the robustness are also demonstrated. A simple yet effective OD\nframework for studying bias mitigation is proposed. Using this framework, the\nperformance on popular datasets is analyzed and a significant difference in\nmodel performance is observed. Additionally, a knowledge transfer technique and\na synthetic image corruption technique are proposed to mitigate the identified\nbias. Finally, using the DAWN dataset, the findings are validated on the OD\ntask, demonstrating the effectiveness of our techniques in mitigating\nreal-world \"good-weather\" bias. The experiments show that the proposed\ntechniques outperform baseline methods by averaged fourfold improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marathe_A/0/1/0/all/0/1\">Aboli Marathe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walambe_R/0/1/0/all/0/1\">Rahee Walambe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotecha_K/0/1/0/all/0/1\">Ketan Kotecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Deepak Kumar Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework. (arXiv:2204.01080v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01080","description":"<p>In this paper, a computation efficient regression framework is presented for\nestimating the 6D pose of rigid objects from a single RGB-D image, which is\napplicable to handling symmetric objects. This framework is designed in a\nsimple architecture that efficiently extracts point-wise features from RGB-D\ndata using a fully convolutional network, called XYZNet, and directly regresses\nthe 6D pose without any post refinement. In the case of symmetric object, one\nobject has multiple ground-truth poses, and this one-to-many relationship may\nlead to estimation ambiguity. In order to solve this ambiguity problem, we\ndesign a symmetry-invariant pose distance metric, called average (maximum)\ngrouped primitives distance or A(M)GPD. The proposed A(M)GPD loss can make the\nregression network converge to the correct state, i.e., all minima in the\nA(M)GPD loss surface are mapped to the correct poses. Extensive experiments on\nYCB-Video and T-LESS datasets demonstrate the proposed framework's\nsubstantially superior performance in top accuracy and low computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_N/0/1/0/all/0/1\">Ningkai Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Wanshui Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1\">Naoto Yokoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faces: AI Blitz XIII Solutions. (arXiv:2204.01081v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01081","description":"<p>AI Blitz XIII Faces challenge hosted on www.aicrowd.com platform consisted of\nfive problems: Sentiment Classification, Age Prediction, Mask Prediction, Face\nRecognition, and Face De-Blurring. Our team GLaDOS took second place. Here we\npresent our solutions and results. Code implementation:\nhttps://github.com/ndrwmlnk/ai-blitz-xiii\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melnik_A/0/1/0/all/0/1\">Andrew Melnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbulut_E/0/1/0/all/0/1\">Eren Akbulut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_J/0/1/0/all/0/1\">Jannik Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loos_K/0/1/0/all/0/1\">Kira Loos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buettner_M/0/1/0/all/0/1\">Michael Buettner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenze_T/0/1/0/all/0/1\">Tobias Lenze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarially robust segmentation models learn perceptually-aligned gradients. (arXiv:2204.01099v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01099","description":"<p>The effects of adversarial training on semantic segmentation networks has not\nbeen thoroughly explored. While previous work has shown that\nadversarially-trained image classifiers can be used to perform image synthesis,\nwe have yet to understand how best to leverage an adversarially-trained\nsegmentation network to do the same. Using a simple optimizer, we demonstrate\nthat adversarially-trained semantic segmentation networks can be used to\nperform image inpainting and generation. Our experiments demonstrate that\nadversarially-trained segmentation networks are more robust and indeed exhibit\nperceptually-aligned gradients which help in producing plausible image\ninpaintings. We seek to place additional weight behind the hypothesis that\nadversarially robust models exhibit gradients that are more\nperceptually-aligned with human vision. Through image synthesis, we argue that\nperceptually-aligned gradients promote a better understanding of a neural\nnetwork's learned representations and aid in making neural networks more\ninterpretable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sandoval_Segura_P/0/1/0/all/0/1\">Pedro Sandoval-Segura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adjusting for Bias with Procedural Data. (arXiv:2204.01108v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01108","description":"<p>3D softwares are now capable of producing highly realistic images that look\nnearly indistinguishable from the real images. This raises the question: can\nreal datasets be enhanced with 3D rendered data? We investigate this question.\nIn this paper we demonstrate the use of 3D rendered data, procedural, data for\nthe adjustment of bias in image datasets. We perform error analysis of images\nof animals which shows that the misclassification of some animal breeds is\nlargely a data issue. We then create procedural images of the poorly classified\nbreeds and that model further trained on procedural data can better classify\npoorly performing breeds on real data. We believe that this approach can be\nused for the enhancement of visual data for any underrepresented group,\nincluding rare diseases, or any data bias potentially improving the accuracy\nand fairness of models. We find that the resulting representations rival or\neven out-perform those learned directly from real data, but that good\nperformance requires care in the 3D rendered procedural data generation. 3D\nimage dataset can be viewed as a compressed and organized copy of a real\ndataset, and we envision a future where more and more procedural data\nproliferate while datasets become increasingly unwieldy, missing, or private.\nThis paper suggests several techniques for dealing with visual representation\nlearning in such a future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shesh Narayan Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Nicholas Bear Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion. (arXiv:2204.01139v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01139","description":"<p>Dense 3D reconstruction from a stream of depth images is the key to many\nmixed reality and robotic applications. Although methods based on Truncated\nSigned Distance Function (TSDF) Fusion have advanced the field over the years,\nthe TSDF volume representation is confronted with striking a balance between\nthe robustness to noisy measurements and maintaining the level of detail. We\npresent Bi-level Neural Volume Fusion (BNV-Fusion), which leverages recent\nadvances in neural implicit representations and neural rendering for dense 3D\nreconstruction. In order to incrementally integrate new depth maps into a\nglobal neural implicit representation, we propose a novel bi-level fusion\nstrategy that considers both efficiency and reconstruction quality by design.\nWe evaluate the proposed method on multiple datasets quantitatively and\nqualitatively, demonstrating a significant improvement over existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kejie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yansong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Adrian Prisacariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indoor Navigation Assistance for Visually Impaired People via Dynamic SLAM and Panoptic Segmentation with an RGB-D Sensor. (arXiv:2204.01154v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01154","description":"<p>Exploring an unfamiliar indoor environment and avoiding obstacles is\nchallenging for visually impaired people. Currently, several approaches achieve\nthe avoidance of static obstacles based on the mapping of indoor scenes. To\nsolve the issue of distinguishing dynamic obstacles, we propose an assistive\nsystem with an RGB-D sensor to detect dynamic information of a scene. Once the\nsystem captures an image, panoptic segmentation is performed to obtain the\nprior dynamic object information. With sparse feature points extracted from\nimages and the depth information, poses of the user can be estimated. After the\nego-motion estimation, the dynamic object can be identified and tracked. Then,\nposes and speed of tracked dynamic objects can be estimated, which are passed\nto the users through acoustic feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1\">Wenyan Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaworek_G/0/1/0/all/0/1\">Gerhard Jaworek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Karin M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape-Pose Disentanglement using SE(3)-equivariant Vector Neurons. (arXiv:2204.01159v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01159","description":"<p>We introduce an unsupervised technique for encoding point clouds into a\ncanonical shape representation, by disentangling shape and pose. Our encoder is\nstable and consistent, meaning that the shape encoding is purely\npose-invariant, while the extracted rotation and translation are able to\nsemantically align different input shapes of the same class to a common\ncanonical pose. Specifically, we design an auto-encoder based on Vector Neuron\nNetworks, a rotation-equivariant neural network, whose layers we extend to\nprovide translation-equivariance in addition to rotation-equivariance only. The\nresulting encoder produces pose-invariant shape encoding by construction,\nenabling our approach to focus on learning a consistent canonical pose for a\nclass of objects. Quantitative and qualitative experiments validate the\nsuperior stability and consistency of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katzir_O/0/1/0/all/0/1\">Oren Katzir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1\">Dani Lischinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Temporal Relations on Radar Perception for Autonomous Driving. (arXiv:2204.01184v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01184","description":"<p>We consider the object recognition problem in autonomous driving using\nautomotive radar sensors. Comparing to Lidar sensors, radar is cost-effective\nand robust in all-weather conditions for perception in autonomous driving.\nHowever, radar signals suffer from low angular resolution and precision in\nrecognizing surrounding objects. To enhance the capacity of automotive radar,\nin this work, we exploit the temporal information from successive ego-centric\nbird-eye-view radar image frames for radar object recognition. We leverage the\nconsistency of an object's existence and attributes (size, orientation, etc.),\nand propose a temporal relational layer to explicitly model the relations\nbetween objects within successive radar images. In both object detection and\nmultiple object tracking, we show the superiority of our method compared to\nseveral baseline approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peizhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berntorp_K/0/1/0/all/0/1\">Karl Berntorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting a kNN-based Image Classification System with High-capacity Storage. (arXiv:2204.01186v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01186","description":"<p>In existing image classification systems that use deep neural networks, the\nknowledge needed for image classification is implicitly stored in model\nparameters. If users want to update this knowledge, then they need to fine-tune\nthe model parameters. Moreover, users cannot verify the validity of inference\nresults or evaluate the contribution of knowledge to the results. In this\npaper, we investigate a system that stores knowledge for image classification,\nsuch as image feature maps, labels, and original images, not in model\nparameters but in external high-capacity storage. Our system refers to the\nstorage like a database when classifying input images. To increase knowledge,\nour system updates the database instead of fine-tuning model parameters, which\navoids catastrophic forgetting in incremental learning scenarios. We revisit a\nkNN (k-Nearest Neighbor) classifier and employ it in our system. By analyzing\nthe neighborhood samples referred by the kNN algorithm, we can interpret how\nknowledge learned in the past is used for inference results. Our system\nachieves 79.8% top-1 accuracy on the ImageNet dataset without fine-tuning model\nparameters after pretraining, and 90.8% accuracy on the Split CIFAR-100 dataset\nin the task incremental learning setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1\">Kengo Nakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1\">Youyang Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyashita_D/0/1/0/all/0/1\">Daisuke Miyashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maki_A/0/1/0/all/0/1\">Asuka Maki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Chieh Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deguchi_J/0/1/0/all/0/1\">Jun Deguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Sliced Wasserstein on Images: From Vectorization to Convolution. (arXiv:2204.01188v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01188","description":"<p>The conventional sliced Wasserstein is defined between two probability\nmeasures that have realizations as vectors. When comparing two probability\nmeasures over images, practitioners first need to vectorize images and then\nproject them to one-dimensional space by using matrix multiplication between\nthe sample matrix and the projection matrix. After that, the sliced Wasserstein\nis evaluated by averaging the two corresponding one-dimensional projected\nprobability measures. However, this approach has two limitations. The first\nlimitation is that the spatial structure of images is not captured efficiently\nby the vectorization step; therefore, the later slicing process becomes harder\nto gather the discrepancy information. The second limitation is memory\ninefficiency since each slicing direction is a vector that has the same\ndimension as the images. To address these limitations, we propose novel slicing\nmethods for sliced Wasserstein between probability measures over images that\nare based on the convolution operators. We derive convolution sliced\nWasserstein (CSW) and its variants via incorporating stride, dilation, and\nnon-linear activation function into the convolution operators. We investigate\nthe metricity of CSW as well as its sample complexity, its computational\ncomplexity, and its connection to conventional sliced Wasserstein distances.\nFinally, we demonstrate the favorable performance of CSW over the conventional\nsliced Wasserstein in comparing probability measures over images and in\ntraining deep generative modeling on images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Change Detection Based on Image Reconstruction Loss. (arXiv:2204.01200v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01200","description":"<p>To train the change detector, bi-temporal images taken at different times in\nthe same area are used. However, collecting labeled bi-temporal images is\nexpensive and time consuming. To solve this problem, various unsupervised\nchange detection methods have been proposed, but they still require unlabeled\nbi-temporal images. In this paper, we propose unsupervised change detection\nbased on image reconstruction loss using only unlabeled single temporal single\nimage. The image reconstruction model is trained to reconstruct the original\nsource image by receiving the source image and the photometrically transformed\nsource image as a pair. During inference, the model receives bi-temporal images\nas the input, and tries to reconstruct one of the inputs. The changed region\nbetween bi-temporal images shows high reconstruction loss. Our change detector\nshowed significant performance in various change detection benchmark datasets\neven though only a single temporal single source image was used. The code and\ntrained models will be publicly available for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noh_H/0/1/0/all/0/1\">Hyeoncheol Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1\">Jingi Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongchan Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Mask R-CNN Model to Segment Heterogeneous Brain Tumors through Image Subtraction. (arXiv:2204.01201v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01201","description":"<p>The segmentation of diseases is a popular topic explored by researchers in\nthe field of machine learning. Brain tumors are extremely dangerous and require\nthe utmost precision to segment for a successful surgery. Patients with tumors\nusually take 4 MRI scans, T1, T1gd, T2, and FLAIR, which are then sent to\nradiologists to segment and analyze for possible future surgery. To create a\nsecond segmentation, it would be beneficial to both radiologists and patients\nin being more confident in their conclusions. We propose using a method\nperformed by radiologists called image segmentation and applying it to machine\nlearning models to prove a better segmentation. Using Mask R-CNN, its ResNet\nbackbone being pre-trained on the RSNA pneumonia detection challenge dataset,\nwe can train a model on the Brats2020 Brain Tumor dataset. Center for\nBiomedical Image Computing &amp; Analytics provides MRI data on patients with and\nwithout brain tumors and the corresponding segmentations. We can see how well\nthe method of image subtraction works by comparing it to models without image\nsubtraction through DICE coefficient (F1 score), recall, and precision on the\nuntouched test set. Our model performed with a DICE coefficient of 0.75 in\ncomparison to 0.69 without image subtraction. To further emphasize the\nusefulness of image subtraction, we compare our final model to current\nstate-of-the-art models to segment tumors from MRI scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singh_S/0/1/0/all/0/1\">Sanskriti Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribute Prototype Network for Any-Shot Learning. (arXiv:2204.01208v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01208","description":"<p>Any-shot image classification allows to recognize novel classes with only a\nfew or even zero samples. For the task of zero-shot learning, visual attributes\nhave been shown to play an important role, while in the few-shot regime, the\neffect of attributes is under-explored. To better transfer attribute-based\nknowledge from seen to unseen classes, we argue that an image representation\nwith integrated attribute localization ability would be beneficial for\nany-shot, i.e. zero-shot and few-shot, image classification tasks. To this end,\nwe propose a novel representation learning framework that jointly learns\ndiscriminative global and local features using only class-level attributes.\nWhile a visual-semantic embedding layer learns global features, local features\nare learned through an attribute prototype network that simultaneously\nregresses and decorrelates attributes from intermediate features. Furthermore,\nwe introduce a zoom-in module that localizes and crops the informative regions\nto encourage the network to learn informative features explicitly. We show that\nour locality augmented image representations achieve a new state-of-the-art on\nchallenging benchmarks, i.e. CUB, AWA2, and SUN. As an additional benefit, our\nmodel points to the visual evidence of the attributes in an image, confirming\nthe improved attribute localization ability of our image representation. The\nattribute localization is evaluated quantitatively with ground truth part\nannotations, qualitatively with visualizations, and through well-designed user\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yongqin Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiuniu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rediscovery of the Effectiveness of Standard Convolution for Lightweight Face Detection. (arXiv:2204.01209v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01209","description":"<p>This paper analyses the design choices of face detection architecture that\nimprove efficiency between computation cost and accuracy. Specifically, we\nre-examine the effectiveness of the standard convolutional block as a\nlightweight backbone architecture on face detection. Unlike the current\ntendency of lightweight architecture design, which heavily utilizes depthwise\nseparable convolution layers, we show that heavily channel-pruned standard\nconvolution layer can achieve better accuracy and inference speed when using a\nsimilar parameter size. This observation is supported by the analyses\nconcerning the characteristics of the target data domain, face. Based on our\nobservation, we propose to employ ResNet with a highly reduced channel, which\nsurprisingly allows high efficiency compared to other mobile-friendly networks\n(e.g., MobileNet-V1,-V2,-V3). From the extensive experiments, we show that the\nproposed backbone can replace that of the state-of-the-art face detector with a\nfaster inference speed. Also, we further propose a new feature aggregation\nmethod maximizing the detection performance. Our proposed detector EResFD\nobtained 80.4% mAP on WIDER FACE Hard subset which only takes 37.7 ms for VGA\nimage inference in on CPU. Code will be available at\nhttps://github.com/clovaai/EResFD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Joonhyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Joonsang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-Teaching for Unsupervised Domain Adaptation and Expansion. (arXiv:2204.01210v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01210","description":"<p>Unsupervised Domain Adaptation (UDA) is known to trade a model's performance\non a source domain for improving its performance on a target domain. To resolve\nthe issue, Unsupervised Domain Expansion (UDE) has been proposed recently to\nadapt the model for the target domain as UDA does, and in the meantime maintain\nits performance on the source domain. For both UDA and UDE, a model tailored to\na given domain, let it be the source or the target domain, is assumed to well\nhandle samples from the given domain. We question the assumption by reporting\nthe existence of cross-domain visual ambiguity: Due to the lack of a crystally\nclear boundary between the two domains, samples from one domain can be visually\nclose to the other domain. We exploit this finding and accordingly propose in\nthis paper Co-Teaching (CT) that consists of knowledge distillation based CT\n(kdCT) and mixup based CT (miCT). Specifically, kdCT transfers knowledge from a\nleader-teacher network and an assistant-teacher network to a student network,\nso the cross-domain visual ambiguity will be better handled by the student.\nMeanwhile, miCT further enhances the generalization ability of the student.\nComprehensive experiments on two image-classification benchmarks and two\ndriving-scene-segmentation benchmarks justify the viability of the proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1\">Kaibin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Q/0/1/0/all/0/1\">Qijie Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Animatable Neural Radiance Fields from Monocular RGB-D. (arXiv:2204.01218v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01218","description":"<p>This paper aims at representing animatable photo-realistic humans under novel\nviews and poses. Recent work has shown significant progress with dynamic scenes\nby exploring shared canonical neural radiance fields. However learning a\nuser-controlled model for novel poses remains a challenging task. To tackle\nthis problem, we introduce a novel method to integrate observations across\nframes and encode the appearance at each individual frame by utilizing the\nhuman pose that models the body shape and point clouds which cover partial part\nof the human as the input. Specifically, our method simultaneously learns a\nshared set of latent codes anchored to the human pose among frames, and learns\nan appearance-dependent code anchored to incomplete point clouds generated by\nmonocular RGB-D at each frame. A human pose-based code models the shape of the\nperformer whereas a point cloud based code predicts details and reasons about\nmissing structures at the unseen poses. To further recover non-visible regions\nin query frames, we utilize a temporal transformer to integrate features of\npoints in query frames and tracked body points from automatically-selected key\nframes. Experiments on various sequences of humans in motion show that our\nmethod significantly outperforms existing works under unseen poses and novel\nviews given monocular RGB-D videos as input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiantian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarafianos_N/0/1/0/all/0/1\">Nikolaos Sarafianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soft Threshold Ternary Networks. (arXiv:2204.01234v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01234","description":"<p>Large neural networks are difficult to deploy on mobile devices because of\nintensive computation and storage. To alleviate it, we study ternarization, a\nbalance between efficiency and accuracy that quantizes both weights and\nactivations into ternary values. In previous ternarized neural networks, a hard\nthreshold {\\Delta} is introduced to determine quantization intervals. Although\nthe selection of {\\Delta} greatly affects the training results, previous works\nestimate {\\Delta} via an approximation or treat it as a hyper-parameter, which\nis suboptimal. In this paper, we present the Soft Threshold Ternary Networks\n(STTN), which enables the model to automatically determine quantization\nintervals instead of depending on a hard threshold. Concretely, we replace the\noriginal ternary kernel with the addition of two binary kernels at training\ntime, where ternary values are determined by the combination of two\ncorresponding binary values. At inference time, we add up the two binary\nkernels to obtain a single ternary kernel. Our method dramatically outperforms\ncurrent state-of-the-arts, lowering the performance gap between full-precision\nnetworks and extreme low bit networks. Experiments on ImageNet with ResNet-18\n(Top-1 66.2%) achieves new state-of-the-art.\n</p>\n<p>Update: In this version, we further fine-tune the experimental\nhyperparameters and training procedure. The latest STTN shows that ResNet-18\nwith ternary weights and ternary activations achieves up to 68.2% Top-1\naccuracy on ImageNet. Code is available at: github.com/WeixiangXu/STTN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weixiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianli Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peisong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Focus-aware Positional Queries for Semantic Segmentation. (arXiv:2204.01244v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01244","description":"<p>Most of the latest top semantic segmentation approaches are based on vision\nTransformers, particularly DETR-like frameworks, which employ a set of queries\nin the Transformer decoder. Each query is composed of a content query that\npreserves semantic information and a positional query that provides positional\nguidance for aggregating the query-specific context. However, the positional\nqueries in the Transformer decoder layers are typically represented as fixed\nlearnable weights, which often encode dataset statistics for segments and can\nbe inaccurate for individual samples. Therefore, in this paper, we propose to\ngenerate positional queries dynamically conditioned on the cross-attention\nscores and the localization information of the preceding layer. By doing so,\neach query is aware of its previous focus, thus providing more accurate\npositional guidance and encouraging the cross-attention consistency across the\ndecoder layers. In addition, we also propose an efficient way to deal with\nhigh-resolution cross-attention by dynamically determining the contextual\ntokens based on the low-resolution cross-attention maps to perform local\nrelation aggregation. Our overall framework termed FASeg (Focus-Aware semantic\nSegmentation) provides a simple yet effective solution for semantic\nsegmentation. Extensive experiments on ADE20K and Cityscapes show that our\nFASeg achieves state-of-the-art performance, e.g., obtaining 48.3% and 49.6%\nmIoU respectively for single-scale inference on ADE20K validation set with\nResNet-50 and Swin-T backbones, and barely increases the computation\nconsumption from Mask2former. Source code will be made publicly available at\nhttps://github.com/zip-group/FASeg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Rendering for Synthetic Aperture Radar Imagery. (arXiv:2204.01248v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01248","description":"<p>There is rising interest in integrating signal and image processing pipelines\ninto deep learning training to incorporate more domain knowledge. This can lead\nto deep neural networks that are trained more robustly and with limited data,\nas well as the capability to solve ill-posed inverse problems. In particular,\nthere is rising interest in differentiable rendering, which allows explicitly\nmodeling geometric priors and constraints in the optimization pipeline using\nfirst-order methods such as backpropagation. Existing efforts in differentiable\nrendering have focused on imagery from electro-optical sensors, particularly\nconventional RGB-imagery. In this work, we propose an approach for\ndifferentiable rendering of Synthetic Aperture Radar (SAR) imagery, which\ncombines methods from 3D computer graphics with neural rendering. We\ndemonstrate the approach on the inverse graphics problem of 3D Object\nReconstruction from limited SAR imagery using high-fidelity simulated SAR data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wilmanski_M/0/1/0/all/0/1\">Michael Wilmanski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tamir_J/0/1/0/all/0/1\">Jonathan Tamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BatchFormerV2: Exploring Sample Relationships for Dense Representation Learning. (arXiv:2204.01254v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01254","description":"<p>Attention mechanisms have been very popular in deep neural networks, where\nthe Transformer architecture has achieved great success in not only natural\nlanguage processing but also visual recognition applications. Recently, a new\nTransformer module, applying on batch dimension rather than spatial/channel\ndimension, i.e., BatchFormer [18], has been introduced to explore sample\nrelationships for overcoming data scarcity challenges. However, it only works\nwith image-level representations for classification. In this paper, we devise a\nmore general batch Transformer module, BatchFormerV2, which further enables\nexploring sample relationships for dense representation learning. Specifically,\nwhen applying the proposed module, it employs a two-stream pipeline during\ntraining, i.e., either with or without a BatchFormerV2 module, where the\nbatchformer stream can be removed for testing. Therefore, the proposed method\nis a plug-and-play module and can be easily integrated into different vision\nTransformers without any extra inference cost. Without bells and whistles, we\nshow the effectiveness of the proposed method for a variety of popular visual\nrecognition tasks, including image classification and two important dense\nprediction tasks: object detection and panoptic segmentation. Particularly,\nBatchFormerV2 consistently improves current DETR-based detection methods (e.g.,\nDETR, Deformable-DETR, Conditional DETR, and SMCA) by over 1.3%. Code will be\nmade publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct Dense Pose Estimation. (arXiv:2204.01263v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01263","description":"<p>Dense human pose estimation is the problem of learning dense correspondences\nbetween RGB images and the surfaces of human bodies, which finds various\napplications, such as human body reconstruction, human pose transfer, and human\naction recognition. Prior dense pose estimation methods are all based on Mask\nR-CNN framework and operate in a top-down manner of first attempting to\nidentify a bounding box for each person and matching dense correspondences in\neach bounding box. Consequently, these methods lack robustness due to their\ncritical dependence on the Mask R-CNN detection, and the runtime increases\ndrastically as the number of persons in the image increases. We therefore\npropose a novel alternative method for solving the dense pose estimation\nproblem, called Direct Dense Pose (DDP). DDP first predicts the instance mask\nand global IUV representation separately and then combines them together. We\nalso propose a simple yet effective 2D temporal-smoothing scheme to alleviate\nthe temporal jitters when dealing with video data. Experiments demonstrate that\nDDP overcomes the limitations of previous top-down baseline methods and\nachieves competitive accuracy. In addition, DDP is computationally more\nefficient than previous dense pose estimation methods, and it reduces jitters\nwhen applied to a video sequence, which is a problem plaguing the previous\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liqian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Implicit Scene Completion. (arXiv:2204.01264v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01264","description":"<p>We propose a probabilistic shape completion method extended to the continuous\ngeometry of large-scale 3D scenes. Real-world scans of 3D scenes suffer from a\nconsiderable amount of missing data cluttered with unsegmented objects. The\nproblem of shape completion is inherently ill-posed, and high-quality result\nrequires scalable solutions that consider multiple possible outcomes. We employ\nthe Generative Cellular Automata that learns the multi-modal distribution and\ntransform the formulation to process large-scale continuous geometry. The local\ncontinuous shape is incrementally generated as a sparse voxel embedding, which\ncontains the latent code for each occupied cell. We formally derive that our\ntraining objective for the sparse voxel embedding maximizes the variational\nlower bound of the complete shape distribution and therefore our progressive\ngeneration constitutes a valid generative model. Experiments show that our\nmodel successfully generates diverse plausible scenes faithful to the input,\nespecially when the input suffers from a significant amount of missing data. We\nalso demonstrate that our approach outperforms deterministic models even in\nless ambiguous cases with a small amount of missing data, which infers that\nprobabilistic formulation is crucial for high-quality geometry completion on\ninput scans exhibiting any levels of completeness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongsu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Changwoon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_I/0/1/0/all/0/1\">Inbum Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Min Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modality Associative Bridging through Memory: Speech Sound Recollected from Face Video. (arXiv:2204.01265v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01265","description":"<p>In this paper, we introduce a novel audio-visual multi-modal bridging\nframework that can utilize both audio and visual information, even with\nuni-modal inputs. We exploit a memory network that stores source (i.e., visual)\nand target (i.e., audio) modal representations, where source modal\nrepresentation is what we are given, and target modal representations are what\nwe want to obtain from the memory network. We then construct an associative\nbridge between source and target memories that considers the interrelationship\nbetween the two memories. By learning the interrelationship through the\nassociative bridge, the proposed bridging framework is able to obtain the\ntarget modal representations inside the memory network, even with the source\nmodal input only, and it provides rich information for its downstream tasks. We\napply the proposed framework to two tasks: lip reading and speech\nreconstruction from silent video. Through the proposed associative bridge and\nmodality-specific memories, each task knowledge is enriched with the recalled\naudio context, achieving state-of-the-art performance. We also verify that the\nassociative bridge properly relates the source and target memories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Joanna Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Se Jin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1\">Yong Man Ro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FoV-Net: Field-of-View Extrapolation Using Self-Attention and Uncertainty. (arXiv:2204.01267v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01267","description":"<p>The ability to make educated predictions about their surroundings, and\nassociate them with certain confidence, is important for intelligent systems,\nlike autonomous vehicles and robots. It allows them to plan early and decide\naccordingly. Motivated by this observation, in this paper we utilize\ninformation from a video sequence with a narrow field-of-view to infer the\nscene at a wider field-of-view. To this end, we propose a temporally consistent\nfield-of-view extrapolation framework, namely FoV-Net, that: (1) leverages 3D\ninformation to propagate the observed scene parts from past frames; (2)\naggregates the propagated multi-frame information using an attention-based\nfeature aggregation module and a gated self-attention module, simultaneously\nhallucinating any unobserved scene parts; and (3) assigns an interpretable\nuncertainty value at each pixel. Extensive experiments show that FoV-Net does\nnot only extrapolate the temporally consistent wide field-of-view scene better\nthan existing alternatives, but also provides the associated uncertainty which\nmay benefit critical decision-making downstream applications. Project page is\nat <a href=\"http://charliememory.github.io/RAL21_FoV.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liqian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Monocular Visual Odometry Using Learned Depth. (arXiv:2204.01268v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01268","description":"<p>Monocular visual odometry (VO) is an important task in robotics and computer\nvision. Thus far, how to build accurate and robust monocular VO systems that\ncan work well in diverse scenarios remains largely unsolved. In this paper, we\npropose a framework to exploit monocular depth estimation for improving VO. The\ncore of our framework is a monocular depth estimation module with a strong\ngeneralization capability for diverse scenes. It consists of two separate\nworking modes to assist the localization and mapping. With a single monocular\nimage input, the depth estimation module predicts a relative depth to help the\nlocalization module on improving the accuracy. With a sparse depth map and an\nRGB image input, the depth estimation module can generate accurate\nscale-consistent depth for dense mapping. Compared with current learning-based\nVO methods, our method demonstrates a stronger generalization ability to\ndiverse scenes. More significantly, our framework is able to boost the\nperformances of existing geometry-based VO methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Libo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengrong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust and Reproducible Active Learning Using Neural Networks. (arXiv:2002.09564v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2002.09564","description":"<p>Active learning (AL) is a promising ML paradigm that has the potential to\nparse through large unlabeled data and help reduce annotation cost in domains\nwhere labelling data can be prohibitive. Recently proposed neural network based\nAL methods use different heuristics to accomplish this goal. In this study, we\ndemonstrate that under identical experimental settings, different types of AL\nalgorithms (uncertainty based, diversity-based, and committee based) produce an\ninconsistent gain over random sampling baseline. Through a variety of\nexperiments, controlling for sources of stochasticity, we show that variance in\nperformance metrics achieved by AL algorithms can lead to results that are not\nconsistent with the previously reported results. We also found that under\nstrong regularization, AL methods show marginal or no advantage over the random\nsampling baseline under a variety of experimental conditions. Finally, we\nconclude with a set of recommendations on how to assess the results using a new\nAL algorithm to ensure results are reproducible and robust under changes in\nexperimental conditions. We share our codes to facilitate AL evaluations. We\nbelieve our findings and recommendations will help advance reproducible\nresearch in AL using neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munjal_P/0/1/0/all/0/1\">Prateek Munjal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_N/0/1/0/all/0/1\">Nasir Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sourati_J/0/1/0/all/0/1\">Jamshid Sourati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shadab Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Critical Assessment of Transfer Learning for Medical Image Segmentation with Fully Convolutional Neural Networks. (arXiv:2006.00356v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.00356","description":"<p>Transfer learning is widely used for training machine learning models. Here,\nwe study the role of transfer learning for training fully convolutional\nnetworks (FCNs) for medical image segmentation. Our experiments show that\nalthough transfer learning reduces the training time on the target task, the\nimprovement in segmentation accuracy is highly task/data-dependent. Larger\nimprovements in accuracy are observed when the segmentation task is more\nchallenging and the target training data is smaller. We observe that\nconvolutional filters of an FCN change little during training for medical image\nsegmentation, and still look random at convergence. We further show that quite\naccurate FCNs can be built by freezing the encoder section of the network at\nrandom values and only training the decoder section. At least for medical image\nsegmentation, this finding challenges the common belief that the encoder\nsection needs to learn data/task-specific representations. We examine the\nevolution of FCN representations to gain a better insight into the effects of\ntransfer learning on the training dynamics. Our analysis shows that although\nFCNs trained via transfer learning learn different representations than FCNs\ntrained with random initialization, the variability among FCNs trained via\ntransfer learning can be as high as that among FCNs trained with random\ninitialization. Moreover, feature reuse is not restricted to the early encoder\nlayers; rather, it can be more significant in deeper layers. These findings\noffer new insights and suggest alternative ways of training FCNs for medical\nimage segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karimi_D/0/1/0/all/0/1\">Davood Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warfield_S/0/1/0/all/0/1\">Simon K. Warfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholipour_A/0/1/0/all/0/1\">Ali Gholipour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tropical time series, iterated-sums signatures and quasisymmetric functions. (arXiv:2009.08443v3 [math.RA] UPDATED)","link":"http://arxiv.org/abs/2009.08443","description":"<p>Aiming for a systematic feature-extraction from time series, we introduce the\niterated-sums signature over arbitrary commutative semirings. The case of the\ntropical semiring is a central, and our motivating example. It leads to\nfeatures of (real-valued) time series that are not easily available using\nexisting signature-type objects. We demonstrate how the signature extracts\nchronological aspects of a time series, and that its calculation is possible in\nlinear time. We identify quasisymmetric expressions over semirings as the\nappropriate framework for iterated-sums signatures over semiring-valued time\nseries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Diehl_J/0/1/0/all/0/1\">Joscha Diehl</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ebrahimi_Fard_K/0/1/0/all/0/1\">Kurusch Ebrahimi-Fard</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tapia_N/0/1/0/all/0/1\">Nikolas Tapia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCW-Net: Single Image Deraining with Multi-level Connections and Wide Regional Non-local Blocks. (arXiv:2009.13990v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.13990","description":"<p>A recent line of convolutional neural network-based works has succeeded in\ncapturing rain streaks. However, difficulties in detailed recovery still\nremain. In this paper, we present a multi-level connection and wide regional\nnon-local block network (MCW-Net) to properly restore the original background\ntextures in rainy images. Unlike existing encoder-decoder-based image deraining\nmodels that improve performance with additional branches, MCW-Net improves\nperformance by maximizing information utilization without additional branches\nthrough the following two proposed methods. The first method is a multi-level\nconnection that repeatedly connects multi-level features of the encoder network\nto the decoder network. Multi-level connection encourages the decoding process\nto use the feature information of all levels. In multi-level connection,\nchannel-wise attention is considered to learn which level of features is\nimportant in the decoding process of the current level. The second method is a\nwide regional non-local block. As rain streaks primarily exhibit a vertical\ndistribution, we divide the grid of the image into horizontally-wide patches\nand apply a non-local operation to each region to explore the rich rain-free\nbackground information. Experimental results on both synthetic and real-world\nrainy datasets demonstrate that the proposed model significantly outperforms\nexisting state-of-the-art models. Furthermore, the results of the joint\nderaining and segmentation experiment prove that our model contributes\neffectively to other vision tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Yeachan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Myeongho Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myungjoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Accurate Active Camera Localization. (arXiv:2012.04263v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04263","description":"<p>In this work, we solve the problem of active camera localization, which\ncontrols the camera movements actively to achieve an accurate camera pose. The\npast solutions are mostly based on Markov Localization, which reduces the\nposition-wise camera uncertainty for localization. These approaches localize\nthe camera in the discrete pose space and are agnostic to the\nlocalization-driven scene property, which restrict the camera pose accuracy in\nthe coarse scale. We propose to overcome these limitations via a novel active\ncamera localization algorithm, composed of a passive and an active localization\nmodule. The former one optimizes the camera pose in the continuous pose space\nby establishing the point-wise camera-world correspondences. The latter one\nexplicitly models the scene and camera uncertainty components to plan the right\npath for accurate camera pose estimation. We validate our algorithm on the\nchallenging localization scenarios from both synthetic and scanned real-world\nindoor scenes. Experimental results demonstrate that our algorithm outperforms\nboth the state-of-the-art Markov Localization based approach and other compared\napproaches on the fine-scale camera pose accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qihang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yingda Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Siyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoquan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.04830","description":"<p>Cataracts are the leading cause of visual impairment and blindness globally.\nOver the years, researchers have achieved significant progress in developing\nstate-of-the-art machine learning techniques for automatic cataract\nclassification and grading, aiming to prevent cataracts early and improve\nclinicians' diagnosis efficiency. This survey provides a comprehensive survey\nof recent advances in machine learning techniques for cataract\nclassification/grading based on ophthalmic images. We summarize existing\nliterature from two research directions: conventional machine learning methods\nand deep learning methods. This survey also provides insights into existing\nworks of both merits and limitations. In addition, we discuss several\nchallenges of automatic cataract classification/grading based on machine\nlearning techniques and present possible solutions to these challenges for\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Z/0/1/0/all/0/1\">Zunjie Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1\">Jiansheng Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Higashita_R/0/1/0/all/0/1\">Risa Higashita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonlinear Evolutionary PDE-Based Refinement of Optical Flow. (arXiv:2102.00487v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.00487","description":"<p>The goal of this paper is to propose two nonlinear variational models for\nobtaining a refined motion estimation from an image sequence. Both the proposed\nmodels can be considered as a part of a generalized framework for an accurate\nestimation of physics-based flow fields such as rotational and fluid flow. The\nfirst model is novel in the sense that it is divided into two phases: the first\nphase obtains a crude estimate of the optical flow and then the second phase\nrefines this estimate using additional constraints. The correctness of this\nmodel is proved using an evolutionary PDE approach. The second model achieves\nthe same refinement as the first model, but in a standard manner, using a\nsingle functional. A special feature of our models is that they permit us to\nprovide efficient numerical implementations through the first-order primal-dual\nChambolle-Pock scheme. Both the models are compared in the context of accurate\nestimation of angle by performing an anisotropic regularization of the\ndivergence and curl of the flow respectively. We observe that, although both\nthe models obtain the same level of accuracy, the two-phase model is more\nefficient. In fact, we empirically demonstrate that the single-phase and the\ntwo-phase models have convergence rates of order $O(1/N^2)$ and $O(1/N)$\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doshi_H/0/1/0/all/0/1\">Hirak Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiran_N/0/1/0/all/0/1\">N. Uday Kiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subspace-Based Feature Fusion From Hyperspectral And Multispectral Image For Land Cover Classification. (arXiv:2102.11228v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2102.11228","description":"<p>In remote sensing, hyperspectral (HS) and multispectral (MS) image fusion\nhave emerged as a synthesis tool to improve the data set resolution. However,\nconventional image fusion methods typically degrade the performance of the land\ncover classification. In this paper, a feature fusion method from HS and MS\nimages for pixel-based classification is proposed. More precisely, the proposed\nmethod first extracts spatial features from the MS image using morphological\nprofiles. Then, the feature fusion model assumes that both the extracted\nmorphological profiles and the HS image can be described as a feature matrix\nlying in different subspaces. An algorithm based on combining alternating\noptimization (AO) and the alternating direction method of multipliers (ADMM) is\ndeveloped to solve efficiently the feature fusion problem. Finally, extensive\nsimulations were run to evaluate the performance of the proposed feature fusion\napproach for two data sets. In general, the proposed approach exhibits a\ncompetitive performance compared to other feature extraction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ramirez_J/0/1/0/all/0/1\">Juan Ram&#xed;rez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vargas_H/0/1/0/all/0/1\">H&#xe9;ctor Vargas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martinez_J/0/1/0/all/0/1\">Jos&#xe9; Ignacio Mart&#xed;nez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arguello_H/0/1/0/all/0/1\">Henry Arguello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolution-Free Medical Image Segmentation using Transformers. (arXiv:2102.13645v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2102.13645","description":"<p>Like other applications in computer vision, medical image segmentation has\nbeen most successfully addressed using deep learning models that rely on the\nconvolution operation as their main building block. Convolutions enjoy\nimportant properties such as sparse interactions, weight sharing, and\ntranslation equivariance. These properties give convolutional neural networks\n(CNNs) a strong and useful inductive bias for vision tasks. In this work we\nshow that a different method, based entirely on self-attention between\nneighboring image patches and without any convolution operations, can achieve\ncompetitive or better results. Given a 3D image block, our network divides it\ninto $n^3$ 3D patches, where $n=3 \\text{ or } 5$ and computes a 1D embedding\nfor each patch. The network predicts the segmentation map for the center patch\nof the block based on the self-attention between these patch embeddings. We\nshow that the proposed model can achieve segmentation accuracies that are\nbetter than the state of the art CNNs on three datasets. We also propose\nmethods for pre-training this model on large corpora of unlabeled images. Our\nexperiments show that with pre-training the advantage of our proposed network\nover CNNs can be significant when labeled training data is small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Karimi_D/0/1/0/all/0/1\">Davood Karimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vasylechko_S/0/1/0/all/0/1\">Serge Vasylechko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gholipour_A/0/1/0/all/0/1\">Ali Gholipour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Part Segmentation through Unsupervised Domain Adaptation from Synthetic Vehicles. (arXiv:2103.14098v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14098","description":"<p>Part segmentations provide a rich and detailed part-level description of\nobjects. However, their annotation requires an enormous amount of work, which\nmakes it difficult to apply standard deep learning methods. In this paper, we\npropose the idea of learning part segmentation through unsupervised domain\nadaptation (UDA) from synthetic data. We first introduce UDA-Part, a\ncomprehensive part segmentation dataset for vehicles that can serve as an\nadequate benchmark for UDA (https://qliu24.github.io/udapart). In UDA-Part, we\nlabel parts on 3D CAD models which enables us to generate a large set of\nannotated synthetic images. We also annotate parts on a number of real images\nto provide a real test set. Secondly, to advance the adaptation of part models\ntrained from the synthetic data to the real images, we introduce a new UDA\nalgorithm that leverages the object's spatial structure to guide the adaptation\nprocess. Our experimental results on two real test datasets confirm the\nsuperiority of our approach over existing works, and demonstrate the promise of\nlearning part segmentation for general objects from synthetic data. We believe\nour dataset provides a rich testbed to study UDA for part segmentation and will\nhelp to significantly push forward research in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhishuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zizhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mengqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoding Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jiteng Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Weichao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Object-level Supervision for Instance Segmentation with Pixel Embeddings. (arXiv:2103.14572v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14572","description":"<p>Most state-of-the-art instance segmentation methods have to be trained on\ndensely annotated images. While difficult in general, this requirement is\nespecially daunting for biomedical images, where domain expertise is often\nrequired for annotation and no large public data collections are available for\npre-training. We propose to address the dense annotation bottleneck by\nintroducing a proposal-free segmentation approach based on non-spatial\nembeddings, which exploits the structure of the learned embedding space to\nextract individual instances in a differentiable way. The segmentation loss can\nthen be applied directly to instances and the overall pipeline can be trained\nin a fully- or weakly supervised manner. We consider the challenging case of\npositive-unlabeled supervision, where a novel self-supervised consistency loss\nis introduced for the unlabeled parts of the training data. We evaluate the\nproposed method on 2D and 3D segmentation problems in different microscopy\nmodalities as well as on the Cityscapes and CVPPP instance segmentation\nbenchmarks, achieving state-of-the-art results on the latter. The code is\navailable at: https://github.com/kreshuklab/spoco\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolny_A/0/1/0/all/0/1\">Adrian Wolny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pape_C/0/1/0/all/0/1\">Constantin Pape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreshuk_A/0/1/0/all/0/1\">Anna Kreshuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Self-Supervised Learning. (arXiv:2103.14653v3 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2103.14653","description":"<p>The resurgence of self-supervised learning, whereby a deep learning model\ngenerates its own supervisory signal from the data, promises a scalable way to\ntackle the dramatically increasing size of real-world data sets without human\nannotation. However, the staggering computational complexity of these methods\nis such that for state-of-the-art performance, classical hardware requirements\nrepresent a significant bottleneck to further progress. Here we take the first\nsteps to understanding whether quantum neural networks could meet the demand\nfor more powerful architectures and test its effectiveness in\nproof-of-principle hybrid experiments. Interestingly, we observe a numerical\nadvantage for the learning of visual representations using small-scale quantum\nneural networks over equivalently structured classical networks, even when the\nquantum circuits are sampled with only 100 shots. Furthermore, we apply our\nbest quantum model to classify unseen images on the ibmq\\_paris quantum\ncomputer and find that current noisy devices can already achieve equal accuracy\nto the equivalent classical model on downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Jaderberg_B/0/1/0/all/0/1\">Ben Jaderberg</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Anderson_L/0/1/0/all/0/1\">Lewis W. Anderson</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kiffner_M/0/1/0/all/0/1\">Martin Kiffner</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Jaksch_D/0/1/0/all/0/1\">Dieter Jaksch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clean Images are Hard to Reblur: Exploiting the Ill-Posed Inverse Task for Dynamic Scene Deblurring. (arXiv:2104.12665v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.12665","description":"<p>The goal of dynamic scene deblurring is to remove the motion blur in a given\nimage. Typical learning-based approaches implement their solutions by\nminimizing the L1 or L2 distance between the output and the reference sharp\nimage. Recent attempts adopt visual recognition features in training to improve\nthe perceptual quality. However, those features are primarily designed to\ncapture high-level contexts rather than low-level structures such as\nblurriness. Instead, we propose a more direct way to make images sharper by\nexploiting the inverse task of deblurring, namely, reblurring. Reblurring\namplifies the remaining blur to rebuild the original blur, however, a\nwell-deblurred clean image with zero-magnitude blur is hard to reblur. Thus, we\ndesign two types of reblurring loss functions for better deblurring. The\nsupervised reblurring loss at training stage compares the amplified blur\nbetween the deblurred and the sharp images. The self-supervised reblurring loss\nat inference stage inspects if there noticeable blur remains in the deblurred.\nOur experimental results on large-scale benchmarks and real images demonstrate\nthe effectiveness of the reblurring losses in improving the perceptual quality\nof the deblurred images in terms of NIQE and LPIPS scores as well as visual\nsharpness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nah_S/0/1/0/all/0/1\">Seungjun Nah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Son_S/0/1/0/all/0/1\">Sanghyun Son</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Jaerin Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Skeleton-based Action Recognition. (arXiv:2104.13586v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13586","description":"<p>Human skeleton, as a compact representation of human action, has received\nincreasing attention in recent years. Many skeleton-based action recognition\nmethods adopt graph convolutional networks (GCN) to extract features on top of\nhuman skeletons. Despite the positive results shown in previous works,\nGCN-based methods are subject to limitations in robustness, interoperability,\nand scalability. In this work, we propose PoseC3D, a new approach to\nskeleton-based action recognition, which relies on a 3D heatmap stack instead\nof a graph sequence as the base representation of human skeletons. Compared to\nGCN-based methods, PoseC3D is more effective in learning spatiotemporal\nfeatures, more robust against pose estimation noises, and generalizes better in\ncross-dataset settings. Also, PoseC3D can handle multiple-person scenarios\nwithout additional computation cost, and its features can be easily integrated\nwith other modalities at early fusion stages, which provides a great design\nspace to further boost the performance. On four challenging datasets, PoseC3D\nconsistently obtains superior performance, when used alone on skeletons and in\ncombination with the RGB modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Haodong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Does Contrastive Visual Representation Learning Work?. (arXiv:2105.05837v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.05837","description":"<p>Recent self-supervised representation learning techniques have largely closed\nthe gap between supervised and unsupervised learning on ImageNet\nclassification. While the particulars of pretraining on ImageNet are now\nrelatively well understood, the field still lacks widely accepted best\npractices for replicating this success on other datasets. As a first step in\nthis direction, we study contrastive self-supervised learning on four diverse\nlarge-scale datasets. By looking through the lenses of data quantity, data\ndomain, data quality, and task granularity, we provide new insights into the\nnecessary conditions for successful self-supervised learning. Our key findings\ninclude observations such as: (i) the benefit of additional pretraining data\nbeyond 500k images is modest, (ii) adding pretraining images from another\ndomain does not lead to more general representations, (iii) corrupted\npretraining images have a disparate impact on supervised and self-supervised\npretraining, and (iv) contrastive learning lags far behind supervised learning\non fine-grained visual classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cole_E/0/1/0/all/0/1\">Elijah Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1\">Kimberly Wilber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Person Extreme Motion Prediction. (arXiv:2105.08825v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.08825","description":"<p>Human motion prediction aims to forecast future poses given a sequence of\npast 3D skeletons. While this problem has recently received increasing\nattention, it has mostly been tackled for single humans in isolation. In this\npaper, we explore this problem when dealing with humans performing\ncollaborative tasks, we seek to predict the future motion of two interacted\npersons given two sequences of their past skeletons. We propose a novel cross\ninteraction attention mechanism that exploits historical information of both\npersons, and learns to predict cross dependencies between the two pose\nsequences. Since no dataset to train such interactive situations is available,\nwe collected ExPI (Extreme Pose Interaction), a new lab-based person\ninteraction dataset of professional dancers performing Lindy-hop dancing\nactions, which contains 115 sequences with 30K frames annotated with 3D body\nposes and shapes. We thoroughly evaluate our cross interaction network on ExPI\nand show that both in short- and long-term predictions, it consistently\noutperforms state-of-the-art methods for single-person motion prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_X/0/1/0/all/0/1\">Xiaoyu Bie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch Slimming for Efficient Vision Transformers. (arXiv:2106.02852v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02852","description":"<p>This paper studies the efficiency problem for visual transformers by\nexcavating redundant calculation in given networks. The recent transformer\narchitecture has demonstrated its effectiveness for achieving excellent\nperformance on a series of computer vision tasks. However, similar to that of\nconvolutional neural networks, the huge computational cost of vision\ntransformers is still a severe issue. Considering that the attention mechanism\naggregates different patches layer-by-layer, we present a novel patch slimming\napproach that discards useless patches in a top-down paradigm. We first\nidentify the effective patches in the last layer and then use them to guide the\npatch selection process of previous layers. For each layer, the impact of a\npatch on the final output feature is approximated and patches with less impact\nwill be removed. Experimental results on benchmark datasets demonstrate that\nthe proposed method can significantly reduce the computational costs of vision\ntransformers without affecting their performances. For example, over 45% FLOPs\nof the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the\nImageNet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.03743","description":"<p>We investigate the reasons for the performance degradation incurred with\nbatch-independent normalization. We find that the prototypical techniques of\nlayer normalization and instance normalization both induce the appearance of\nfailure modes in the neural network's pre-activations: (i) layer normalization\ninduces a collapse towards channel-wise constant functions; (ii) instance\nnormalization induces a lack of variability in instance statistics, symptomatic\nof an alteration of the expressivity. To alleviate failure mode (i) without\naggravating failure mode (ii), we introduce the technique \"Proxy Normalization\"\nthat normalizes post-activations using a proxy distribution. When combined with\nlayer normalization or group normalization, this batch-independent\nnormalization emulates batch normalization's behavior and consistently matches\nor exceeds its performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Labatie_A/0/1/0/all/0/1\">Antoine Labatie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masters_D/0/1/0/all/0/1\">Dominic Masters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1\">Zach Eaton-Rosen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1\">Carlo Luschi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT Learns to Teach: Knowledge Distillation with Meta Learning. (arXiv:2106.04570v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.04570","description":"<p>We present Knowledge Distillation with Meta Learning (MetaDistil), a simple\nyet effective alternative to traditional knowledge distillation (KD) methods\nwhere the teacher model is fixed during training. We show the teacher network\ncan learn to better transfer knowledge to the student network (i.e., learning\nto teach) with the feedback from the performance of the distilled student\nnetwork in a meta learning framework. Moreover, we introduce a pilot update\nmechanism to improve the alignment between the inner-learner and meta-learner\nin meta learning algorithms that focus on an improved inner-learner.\nExperiments on various benchmarks show that MetaDistil can yield significant\nimprovements compared with traditional KD algorithms and is less sensitive to\nthe choice of different student capacity and hyperparameters, facilitating the\nuse of KD on different tasks and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverting Adversarially Robust Networks for Image Synthesis. (arXiv:2106.06927v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06927","description":"<p>Despite unconditional feature inversion being the foundation of many image\nsynthesis applications, training an inverter demands a high computational\nbudget, large decoding capacity and imposing conditions such as autoregressive\npriors. To address these limitations, we propose the use of adversarially\nrobust representations as a perceptual primitive for feature inversion. We\ntrain an adversarially robust encoder to extract disentangled and\nperceptually-aligned image representations, making them easily invertible. By\ntraining a simple generator with the mirror architecture of the encoder, we\nachieve superior reconstruction quality and generalization over standard\nmodels. Based on this, we propose an adversarially robust autoencoder and\ndemonstrate its improved performance on anomaly detection, style transfer and\nimage denoising tasks. Comparisons against recent learn-based methods show that\nour model attains improved performance with significantly less complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Gomez_R/0/1/0/all/0/1\">Renan A. Rojas-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_R/0/1/0/all/0/1\">Raymond A. Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1\">Minh N. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?. (arXiv:2106.11297v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11297","description":"<p>In this paper, we introduce a novel visual representation learning which\nrelies on a handful of adaptively learned tokens, and which is applicable to\nboth image and video understanding tasks. Instead of relying on hand-designed\nsplitting strategies to obtain visual tokens and processing a large number of\ndensely sampled patches for attention, our approach learns to mine important\ntokens in visual data. This results in efficiently and effectively finding a\nfew important visual tokens and enables modeling of pairwise attention between\nsuch tokens, over a longer temporal horizon for videos, or the spatial content\nin images. Our experiments demonstrate strong performance on several\nchallenging benchmarks for both image and video recognition tasks. Importantly,\ndue to our tokens being adaptive, we accomplish competitive results at\nsignificantly reduced compute amount. We obtain comparable results to the\nstate-of-the-arts on ImageNet while being computationally more efficient. We\nalso confirm the effectiveness of the approach on multiple video datasets,\nincluding Kinetics-400, Kinetics-600, Charades, and AViD.\n</p>\n<p>The code is available at:\nhttps://github.com/google-research/scenic/tree/main/scenic/projects/token_learner\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1\">AJ Piergiovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis. (arXiv:2106.11485v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11485","description":"<p>High-resolution satellite imagery has proven useful for a broad range of\ntasks, including measurement of global human population, local economic\nlivelihoods, and biodiversity, among many others. Unfortunately,\nhigh-resolution imagery is both infrequently collected and expensive to\npurchase, making it hard to efficiently and effectively scale these downstream\ntasks over both time and space. We propose a new conditional pixel synthesis\nmodel that uses abundant, low-cost, low-resolution imagery to generate accurate\nhigh-resolution imagery at locations and times in which it is unavailable. We\nshow that our model attains photo-realistic sample quality and outperforms\ncompeting baselines on a key downstream task -- object counting -- particularly\nin geographic locations where conditions on the ground are changing rapidly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yutong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_N/0/1/0/all/0/1\">Nicholas Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">William Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chenlin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1\">Marshall Burke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1\">David B. Lobell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Bottlenecks for Multimodal Fusion. (arXiv:2107.00135v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00135","description":"<p>Humans perceive the world by concurrently processing and fusing\nhigh-dimensional inputs from multiple modalities such as vision and audio.\nMachine perception models, in stark contrast, are typically modality-specific\nand optimised for unimodal benchmarks, and hence late-stage fusion of final\nrepresentations or predictions from each modality (`late-fusion') is still a\ndominant paradigm for multimodal video classification. Instead, we introduce a\nnovel transformer based architecture that uses `fusion bottlenecks' for\nmodality fusion at multiple layers. Compared to traditional pairwise\nself-attention, our model forces information between different modalities to\npass through a small number of bottleneck latents, requiring the model to\ncollate and condense the most relevant information in each modality and only\nshare what is necessary. We find that such a strategy improves fusion\nperformance, at the same time reducing computational cost. We conduct thorough\nablation studies, and achieve state-of-the-art results on multiple audio-visual\nclassification benchmarks including Audioset, Epic-Kitchens and VGGSound. All\ncode and models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_A/0/1/0/all/0/1\">Aren Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imaging dynamics beneath turbid media via parallelized single-photon detection. (arXiv:2107.01422v3 [physics.optics] UPDATED)","link":"http://arxiv.org/abs/2107.01422","description":"<p>Noninvasive optical imaging through dynamic scattering media has numerous\nimportant biomedical applications but still remains a challenging task. While\nstandard diffuse imaging methods measure optical absorption or fluorescent\nemission, it is also well-established that the temporal correlation of\nscattered coherent light diffuses through tissue much like optical intensity.\nFew works to date, however, have aimed to experimentally measure and process\nsuch temporal correlation data to demonstrate deep-tissue video reconstruction\nof decorrelation dynamics. In this work, we utilize a single-photon avalanche\ndiode (SPAD) array camera to simultaneously monitor the temporal dynamics of\nspeckle fluctuations at the single-photon level from 12 different phantom\ntissue surface locations delivered via a customized fiber bundle array. We then\napply a deep neural network to convert the acquired single-photon measurements\ninto video of scattering dynamics beneath rapidly decorrelating tissue\nphantoms. We demonstrate the ability to reconstruct images of transient\n(0.1-0.4s) dynamic events occurring up to 8 mm beneath a decorrelating tissue\nphantom with millimeter-scale resolution, and highlight how our model can\nflexibly extend to monitor flow speed within buried phantom vessels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Xu_S/0/1/0/all/0/1\">Shiqi Xu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1\">Wenhui Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jonsson_J/0/1/0/all/0/1\">Joakim Jonsson</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Qian_R/0/1/0/all/0/1\">Ruobing Qian</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Konda_P/0/1/0/all/0/1\">Pavan Chandra Konda</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhou_K/0/1/0/all/0/1\">Kevin C. Zhou</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kreiss_L/0/1/0/all/0/1\">Lucas Kreiss</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Berrocal_E/0/1/0/all/0/1\">Edouard Berrocal</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Horstmeyer_R/0/1/0/all/0/1\">Roarke Horstmeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing, Reconstructing, and Simulating: the UrbanScene3D Dataset. (arXiv:2107.04286v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.04286","description":"<p>We present UrbanScene3D, a large-scale data platform for research of urban\nscene perception and reconstruction. UrbanScene3D contains over 128k\nhigh-resolution images covering 16 scenes including large-scale real urban\nregions and synthetic cities with 136 km^2 area in total. The dataset also\ncontains high-precision LiDAR scans and hundreds of image sets with different\nobservation patterns, which provide a comprehensive benchmark to design and\nevaluate aerial path planning and 3D reconstruction algorithms. In addition,\nthe dataset, which is built on Unreal Engine and Airsim simulator together with\nthe manually annotated unique instance label for each building in the dataset,\nenables the generation of all kinds of data, e.g., 2D depth maps, 2D/3D\nbounding boxes, and 3D point cloud/mesh segmentations, etc. The simulator with\nphysical engine and lighting system not only produce variety of data but also\nenable users to simulate cars or drones in the proposed urban environment for\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liqiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yilin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xingguang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Ke Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal MRI Reconstruction Assisted with Spatial Alignment Network. (arXiv:2108.05603v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.05603","description":"<p>In clinical practice, multi-modal magnetic resonance imaging (MRI) with\ndifferent contrasts is usually acquired in a single study to assess different\nproperties of the same region of interest in the human body. The whole\nacquisition process can be accelerated by having one or more modalities\nunder-sampled in the $k$-space. Recent research has shown that, considering the\nredundancy between different modalities, a target MRI modality under-sampled in\nthe $k$-space can be more efficiently reconstructed with a fully-sampled\nreference MRI modality. However, we find that the performance of the\naforementioned multi-modal reconstruction can be negatively affected by subtle\nspatial misalignment between different modalities, which is actually common in\nclinical practice. In this paper, we improve the quality of multi-modal\nreconstruction by compensating for such spatial misalignment with a spatial\nalignment network. First, our spatial alignment network estimates the\ndisplacement between the fully-sampled reference and the under-sampled target\nimages, and warps the reference image accordingly. Then, the aligned\nfully-sampled reference image joins the multi-modal reconstruction of the\nunder-sampled target image. Also, considering the contrast difference between\nthe target and reference images, we have designed a\ncross-modality-synthesis-based registration loss in combination with the\nreconstruction loss, to jointly train the spatial alignment network and the\nreconstruction network. The experiments on both clinical MRI and multi-coil\n$k$-space raw data demonstrate the superiority and robustness of the\nmulti-modal MRI reconstruction empowered with our spatial alignment network.\nOur code is publicly available at\n\\url{https://github.com/woxuankai/SpatialAlignmentNetwork}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xuan_K/0/1/0/all/0/1\">Kai Xuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_L/0/1/0/all/0/1\">Lei Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoqian Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lichi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_S/0/1/0/all/0/1\">Shu Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full-resolution quality assessment for pansharpening. (arXiv:2108.06144v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06144","description":"<p>A reliable quality assessment procedure for pansharpening methods is of\ncritical importance for the development of the related solutions.\nUnfortunately, the lack of ground-truths to be used as guidance for an\nobjective evaluation has pushed the community to resort to two approaches which\ncan also be jointly applied. Hence, two kinds of indexes can be found in the\nliterature: i) reference-based reduced-resolution indexes aimed to assess the\nsynthesis ability; ii) no-reference subjective quality indexes for\nfull-resolution datasets aimed to assess spectral and spatial consistency. Both\nreference-based and no-reference indexes present critical shortcomings which\nmotivate the community to explore new solutions. In this work, we propose an\nalternative no-reference full-resolution assessment framework. On one side we\nintroduce a protocol, namely the reprojection protocol, to take care of the\nspectral consistency issue. On the other side, a new index of the spatial\nconsistency between the pansharpened image and the panchromatic band at full\nresolution is also proposed. Experimental results carried out on different\ndatasets/sensors demonstrate the effectiveness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scarpa_G/0/1/0/all/0/1\">Giuseppe Scarpa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciotola_M/0/1/0/all/0/1\">Matteo Ciotola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Navigation: A Survey and Taxonomy. (arXiv:2108.11544v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11544","description":"<p>Vision-Language Navigation (VLN) tasks require an agent to follow human\nlanguage instructions to navigate in previously unseen environments. This\nchallenging field involving problems in natural language processing, computer\nvision, robotics, etc., has spawn many excellent works focusing on various VLN\ntasks. This paper provides a comprehensive survey and an insightful taxonomy of\nthese tasks based on the different characteristics of language instructions in\nthese tasks. Depending on whether the navigation instructions are given for\nonce or multiple times, this paper divides the tasks into two categories, i.e.,\nsingle-turn and multi-turn tasks. For single-turn tasks, we further subdivide\nthem into goal-oriented and route-oriented based on whether the instructions\ndesignate a single goal location or specify a sequence of multiple locations.\nFor multi-turn tasks, we subdivide them into passive and interactive tasks\nbased on whether the agent is allowed to question the instruction or not. These\ntasks require different capabilities of the agent and entail various model\ndesigns. We identify progress made on the tasks and look into the limitations\nof existing VLN models and task settings. Finally, we discuss several open\nissues of VLN and point out some opportunities in the future, i.e.,\nincorporating knowledge with VLN models and implementing them in the real\nphysical world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wansen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinmeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining. (arXiv:2109.04275v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04275","description":"<p>Despite the potential of multi-modal pre-training to learn highly\ndiscriminative feature representations from complementary data modalities,\ncurrent progress is being slowed by the lack of large-scale modality-diverse\ndatasets. By leveraging the natural suitability of E-commerce, where different\nmodalities capture complementary semantic information, we contribute a\nlarge-scale multi-modal pre-training dataset M5Product. The dataset comprises 5\nmodalities (image, text, table, video, and audio), covers over 6,000 categories\nand 5,000 attributes, and is 500 larger than the largest publicly available\ndataset with a similar number of modalities. Furthermore, M5Product contains\nincomplete modality pairs and noise while also having a long-tailed\ndistribution, resembling most real-world problems. We further propose\nSelf-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework\nthat integrates the different modalities into a unified model through an\nadaptive feature fusion mechanism, where the importance of each modality is\nlearned directly from the modality embeddings and impacts the inter-modality\ncontrastive learning and masked tasks within a multi-modal transformer model.\nWe evaluate the current multi-modal pre-training state-of-the-art approaches\nand benchmark their ability to learn from unlabeled data when faced with the\nlarge number of modalities in the M5Product dataset. We conduct extensive\nexperiments on four downstream tasks and demonstrate the superiority of our\nSCALE model, providing insights into the importance of dataset scale and\ndiversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xunlin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael C. Kampffmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoyong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Minlong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Wide-Angle Portraits Correction by Multi-Scale Transformer. (arXiv:2109.08024v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08024","description":"<p>We propose a semi-supervised network for wide-angle portraits correction.\nWide-angle images often suffer from skew and distortion affected by perspective\ndistortion, especially noticeable at the face regions. Previous deep learning\nbased approaches need the ground-truth correction flow maps for training\nguidance. However, such labels are expensive, which can only be obtained\nmanually. In this work, we design a semi-supervised scheme and build a\nhigh-quality unlabeled dataset with rich scenarios, allowing us to\nsimultaneously use labeled and unlabeled data to improve performance.\nSpecifically, our semi-supervised scheme takes advantage of the consistency\nmechanism, with several novel components such as direction and range\nconsistency (DRC) and regression consistency (RC). Furthermore, different from\nthe existing methods, we propose the Multi-Scale Swin-Unet (MS-Unet) based on\nthe multi-scale swin transformer block (MSTB), which can simultaneously learn\nshort-distance and long-distance information to avoid artifacts. Extensive\nexperiments demonstrate that the proposed method is superior to the\nstate-of-the-art methods and other representative baselines. The source code\nand dataset are available at:\nhttps://github.com/megvii-research/Portraits_Correction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fushun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hua Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoPhaseNN: Unsupervised Physics-aware Deep Learning of 3D Nanoscale Bragg Coherent Diffraction Imaging. (arXiv:2109.14053v2 [physics.app-ph] UPDATED)","link":"http://arxiv.org/abs/2109.14053","description":"<p>The problem of phase retrieval, or the algorithmic recovery of lost phase\ninformation from measured intensity alone, underlies various imaging methods\nfrom astronomy to nanoscale imaging. Traditional methods of phase retrieval are\niterative in nature, and are therefore computationally expensive and time\nconsuming. More recently, deep learning (DL) models have been developed to\neither provide learned priors to iterative phase retrieval or in some cases\ncompletely replace phase retrieval with networks that learn to recover the lost\nphase information from measured intensity alone. However, such models require\nvast amounts of labeled data, which can only be obtained through simulation or\nperforming computationally prohibitive phase retrieval on hundreds of or even\nthousands of experimental datasets. Using a 3D nanoscale X-ray imaging modality\n(Bragg Coherent Diffraction Imaging or BCDI) as a representative technique, we\ndemonstrate AutoPhaseNN, a DL-based approach which learns to solve the phase\nproblem without labeled data. By incorporating the physics of the imaging\ntechnique into the DL model during training, AutoPhaseNN learns to invert 3D\nBCDI data from reciprocal space to real space in a single shot without ever\nbeing shown real space images. Once trained, AutoPhaseNN is about one hundred\ntimes faster than traditional iterative phase retrieval methods while providing\ncomparable image quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chan_H/0/1/0/all/0/1\">Henry Chan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sankaranarayanan_S/0/1/0/all/0/1\">Subramanian Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Balaprakash_P/0/1/0/all/0/1\">Prasanna Balaprakash</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Harder_R/0/1/0/all/0/1\">Ross J. Harder</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cherukara_M/0/1/0/all/0/1\">Mathew J. Cherukara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical Flow Estimation for Spiking Camera. (arXiv:2110.03916v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03916","description":"<p>As a bio-inspired sensor with high temporal resolution, the spiking camera\nhas an enormous potential in real applications, especially for motion\nestimation in high-speed scenes. However, frame-based and event-based methods\nare not well suited to spike streams from the spiking camera due to the\ndifferent data modalities. To this end, we present, SCFlow, a tailored deep\nlearning pipeline to estimate optical flow in high-speed scenes from spike\nstreams. Importantly, a novel input representation is introduced which can\nadaptively remove the motion blur in spike streams according to the prior\nmotion. Further, for training SCFlow, we synthesize two sets of optical flow\ndata for the spiking camera, SPIkingly Flying Things and Photo-realistic\nHigh-speed Motion, denoted as SPIFT and PHM respectively, corresponding to\nrandom high-speed and well-designed scenes. Experimental results show that the\nSCFlow can predict optical flow from spike streams in different high-speed\nscenes. Moreover, SCFlow shows promising generalization on \\textbf{real spike\nstreams}. Codes and datasets refer to\nhttps://github.com/Acnext/Optical-Flow-For-Spiking-Camera.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Liwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Ziluo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Boxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruiqin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Learning from An Expert in Deep Recommendation Systems with Marginal Distance Probability Distribution. (arXiv:2110.06287v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06287","description":"<p>Recommendation systems play an important role in today's digital world. They\nhave found applications in various applications such as music platforms, e.g.,\nSpotify, and movie streaming services, e.g., Netflix. Less research effort has\nbeen devoted to physical exercise recommendation systems. Sedentary lifestyles\nhave become the major driver of several diseases as well as healthcare costs.\nIn this paper, we develop a recommendation system for daily exercise activities\nto users based on their history, profile and similar users. The developed\nrecommendation system uses a deep recurrent neural network with user-profile\nattention and temporal attention mechanisms.\n</p>\n<p>Moreover, exercise recommendation systems are significantly different from\nstreaming recommendation systems in that we are not able to collect click\nfeedback from the participants in exercise recommendation systems. Thus, we\npropose a real-time, expert-in-the-loop active learning procedure. The active\nlearners calculate the uncertainty of the recommender at each time step for\neach user and ask an expert for a recommendation when the certainty is low. In\nthis paper, we derive the probability distribution function of marginal\ndistance, and use it to determine when to ask experts for feedback. Our\nexperimental results on a mHealth dataset show improved accuracy after\nincorporating the real-time active learner with the recommendation system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahyari_A/0/1/0/all/0/1\">Arash Mahyari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirolli_P/0/1/0/all/0/1\">Peter Pirolli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeBlanc_J/0/1/0/all/0/1\">Jacqueline A. LeBlanc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trigger Hunting with a Topological Prior for Trojan Detection. (arXiv:2110.08335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08335","description":"<p>Despite their success and popularity, deep neural networks (DNNs) are\nvulnerable when facing backdoor attacks. This impedes their wider adoption,\nespecially in mission critical applications. This paper tackles the problem of\nTrojan detection, namely, identifying Trojaned models -- models trained with\npoisoned data. One popular approach is reverse engineering, i.e., recovering\nthe triggers on a clean image by manipulating the model's prediction. One major\nchallenge of reverse engineering approach is the enormous search space of\ntriggers. To this end, we propose innovative priors such as diversity and\ntopological simplicity to not only increase the chances of finding the\nappropriate triggers but also improve the quality of the found triggers.\nMoreover, by encouraging a diverse set of trigger candidates, our method can\nperform effectively in cases with unknown target labels. We demonstrate that\nthese priors can significantly improve the quality of the recovered triggers,\nresulting in substantially improved Trojan detection accuracy as validated on\nboth synthetic and publicly available TrojAI benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoling Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cogswell_M/0/1/0/all/0/1\">Michael Cogswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Susmit Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynCoLFinGer: Synthetic Contactless Fingerprint Generator. (arXiv:2110.09144v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09144","description":"<p>We present the first method for synthetic generation of contactless\nfingerprint images, referred to as SynCoLFinGer. To this end, the constituent\ncomponents of contactless fingerprint images regarding capturing, subject\ncharacteristics, and environmental influences are modeled and applied to a\nsynthetically generated ridge pattern using the SFinGe algorithm. The proposed\nmethod is able to generate different synthetic samples corresponding to a\nsingle finger and it can be parameterized to generate contactless fingerprint\nimages of various quality levels. The resemblance of the synthetically\ngenerated contactless fingerprints to real fingerprints is confirmed by\nevaluating biometric sample quality using an adapted NFIQ 2.0 algorithm and\nbiometric utility using a state-of-the-art contactless fingerprint recognition\nsystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Priesnitz_J/0/1/0/all/0/1\">Jannis Priesnitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchmann_N/0/1/0/all/0/1\">Nicolas Buchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Redundancy: Separable Group Convolutional Networks on Lie Groups. (arXiv:2110.13059v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13059","description":"<p>Group convolutional neural networks (G-CNNs) have been shown to increase\nparameter efficiency and model accuracy by incorporating geometric inductive\nbiases. In this work, we investigate the properties of representations learned\nby regular G-CNNs, and show considerable parameter redundancy in group\nconvolution kernels. This finding motivates further weight-tying by sharing\nconvolution kernels over subgroups. To this end, we introduce convolution\nkernels that are separable over the subgroup and channel dimensions. In order\nto obtain equivariance to arbitrary affine Lie groups we provide a continuous\nparameterisation of separable convolution kernels. We evaluate our approach\nacross several vision datasets, and show that our weight sharing leads to\nimproved performance and computational efficiency. In many settings, separable\nG-CNNs outperform their non-separable counterpart, while only using a fraction\nof their training time. In addition, thanks to the increase in computational\nefficiency, we are able to implement G-CNNs equivariant to the\n$\\mathrm{Sim(2)}$ group; the group of dilations, rotations and translations.\n$\\mathrm{Sim(2)}$-equivariance further improves performance on all tasks\nconsidered.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knigge_D/0/1/0/all/0/1\">David M. Knigge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1\">David W. Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik J. Bekkers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHIP: CHannel Independence-based Pruning for Compact Neural Networks. (arXiv:2110.13981v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13981","description":"<p>Filter pruning has been widely used for neural network compression because of\nits enabled practical acceleration. To date, most of the existing filter\npruning works explore the importance of filters via using intra-channel\ninformation. In this paper, starting from an inter-channel perspective, we\npropose to perform efficient filter pruning using Channel Independence, a\nmetric that measures the correlations among different feature maps. The less\nindependent feature map is interpreted as containing less useful\ninformation$/$knowledge, and hence its corresponding filter can be pruned\nwithout affecting model capacity. We systematically investigate the\nquantification metric, measuring scheme and sensitiveness$/$reliability of\nchannel independence in the context of filter pruning. Our evaluation results\nfor different models on various datasets show the superior performance of our\napproach. Notably, on CIFAR-10 dataset our solution can bring $0.90\\%$ and\n$0.94\\%$ accuracy increase over baseline ResNet-56 and ResNet-110 models,\nrespectively, and meanwhile the model size and FLOPs are reduced by $42.8\\%$\nand $47.4\\%$ (for ResNet-56) and $48.3\\%$ and $52.1\\%$ (for ResNet-110),\nrespectively. On ImageNet dataset, our approach can achieve $40.8\\%$ and\n$44.8\\%$ storage and computation reductions, respectively, with $0.15\\%$\naccuracy increase over the baseline ResNet-50 model. The code is available at\nhttps://github.com/Eclipsess/CHIP_NeurIPS2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1\">Yang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Miao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonouz_S/0/1/0/all/0/1\">Saman Zonouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pansharpening by convolutional neural networks in the full resolution framework. (arXiv:2111.08334v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08334","description":"<p>In recent years, there has been a growing interest in deep learning-based\npansharpening. Thus far, research has mainly focused on architectures.\nNonetheless, model training is an equally important issue. A first problem is\nthe absence of ground truths, unavoidable in pansharpening. This is often\naddressed by training networks in a reduced resolution domain and using the\noriginal data as ground truth, relying on an implicit scale invariance\nassumption. However, on full resolution images results are often disappointing,\nsuggesting such invariance not to hold. A further problem is the scarcity of\ntraining data, which causes a limited generalization ability and a poor\nperformance on off-training test images. In this paper, we propose a\nfull-resolution training framework for deep learning-based pansharpening. The\nframework is fully general and can be used for any deep learning-based\npansharpening model. Training takes place in the high-resolution domain,\nrelying only on the original data, thus avoiding any loss of information. To\nensure spectral and spatial fidelity, a suitable two-component loss is defined.\nThe spectral component enforces consistency between the pansharpened output and\nthe low-resolution multispectral input. The spatial component, computed at\nhigh-resolution, maximizes the local correlation between each pansharpened band\nand the panchromatic input. At testing time, the target-adaptive operating\nmodality is adopted, achieving good generalization with a limited computational\noverhead. Experiments carried out on WorldView-3, WorldView-2, and GeoEye-1\nimages show that methods trained with the proposed framework guarantee a pretty\ngood performance in terms of both full-resolution numerical indexes and visual\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciotola_M/0/1/0/all/0/1\">Matteo Ciotola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitale_S/0/1/0/all/0/1\">Sergio Vitale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazza_A/0/1/0/all/0/1\">Antonio Mazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poggi_G/0/1/0/all/0/1\">Giovanni Poggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarpa_G/0/1/0/all/0/1\">Giuseppe Scarpa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's About Time: Analog Clock Reading in the Wild. (arXiv:2111.09162v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09162","description":"<p>In this paper, we present a framework for reading analog clocks in natural\nimages or videos. Specifically, we make the following contributions: First, we\ncreate a scalable pipeline for generating synthetic clocks, significantly\nreducing the requirements for the labour-intensive annotations; Second, we\nintroduce a clock recognition architecture based on spatial transformer\nnetworks (STN), which is trained end-to-end for clock alignment and\nrecognition. We show that the model trained on the proposed synthetic dataset\ngeneralises towards real clocks with good accuracy, advocating a Sim2Real\ntraining regime; Third, to further reduce the gap between simulation and real\ndata, we leverage the special property of time, i.e. uniformity, to generate\nreliable pseudo-labels on real unlabelled clock videos, and show that training\non these videos offers further improvements while still requiring zero manual\nannotations. Lastly, we introduce three benchmark datasets based on COCO, Open\nImages, and The Clock movie, totalling 4,472 images with clocks, with full\nannotations for time, accurate to the minute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Charig Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discrete Representations Strengthen Vision Transformer Robustness. (arXiv:2111.10493v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10493","description":"<p>Vision Transformer (ViT) is emerging as the state-of-the-art architecture for\nimage recognition. While recent studies suggest that ViTs are more robust than\ntheir convolutional counterparts, our experiments find that ViTs trained on\nImageNet are overly reliant on local textures and fail to make adequate use of\nshape information. ViTs thus have difficulties generalizing to\nout-of-distribution, real-world data. To address this deficiency, we present a\nsimple and effective architecture modification to ViT's input layer by adding\ndiscrete tokens produced by a vector-quantized encoder. Different from the\nstandard continuous pixel tokens, discrete tokens are invariant under small\nperturbations and contain less information individually, which promote ViTs to\nlearn global information that is invariant. Experimental results demonstrate\nthat adding discrete representation on four architecture variants strengthens\nViT robustness by up to 12% across seven ImageNet robustness benchmarks while\nmaintaining the performance on ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chengzhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukthankar_R/0/1/0/all/0/1\">Rahul Sukthankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essa_I/0/1/0/all/0/1\">Irfan Essa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferability Estimation using Bhattacharyya Class Separability. (arXiv:2111.12780v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12780","description":"<p>Transfer learning has become a popular method for leveraging pre-trained\nmodels in computer vision. However, without performing computationally\nexpensive fine-tuning, it is difficult to quantify which pre-trained source\nmodels are suitable for a specific target task, or, conversely, to which tasks\na pre-trained source model can be easily adapted to. In this work, we propose\nGaussian Bhattacharyya Coefficient (GBC), a novel method for quantifying\ntransferability between a source model and a target dataset. In a first step we\nembed all target images in the feature space defined by the source model, and\nrepresent them with per-class Gaussians. Then, we estimate their pairwise class\nseparability using the Bhattacharyya coefficient, yielding a simple and\neffective measure of how well the source model transfers to the target task. We\nevaluate GBC on image classification tasks in the context of dataset and\narchitecture selection. Further, we also perform experiments on the more\ncomplex semantic segmentation transferability estimation task. We demonstrate\nthat GBC outperforms state-of-the-art transferability metrics on most\nevaluation criteria in the semantic segmentation settings, matches the\nperformance of top methods for dataset transferability in image classification,\nand performs best on architecture selection problems for image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandy_M/0/1/0/all/0/1\">Michal P&#xe1;ndy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agostinelli_A/0/1/0/all/0/1\">Andrea Agostinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uijlings_J/0/1/0/all/0/1\">Jasper Uijlings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensink_T/0/1/0/all/0/1\">Thomas Mensink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SurfEmb: Dense and Continuous Correspondence Distributions for Object Pose Estimation with Learnt Surface Embeddings. (arXiv:2111.13489v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13489","description":"<p>We present an approach to learn dense, continuous 2D-3D correspondence\ndistributions over the surface of objects from data with no prior knowledge of\nvisual ambiguities like symmetry. We also present a new method for 6D pose\nestimation of rigid objects using the learnt distributions to sample, score and\nrefine pose hypotheses. The correspondence distributions are learnt with a\ncontrastive loss, represented in object-specific latent spaces by an\nencoder-decoder query model and a small fully connected key model. Our method\nis unsupervised with respect to visual ambiguities, yet we show that the query-\nand key models learn to represent accurate multi-modal surface distributions.\nOur pose estimation method improves the state-of-the-art significantly on the\ncomprehensive BOP Challenge, trained purely on synthetic data, even compared\nwith methods trained on real data. The project site is at\nhttps://surfemb.github.io/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haugaard_R/0/1/0/all/0/1\">Rasmus Laurvig Haugaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_A/0/1/0/all/0/1\">Anders Glent Buch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Relations are Equal: Mining Informative Labels for Scene Graph Generation. (arXiv:2111.13517v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13517","description":"<p>Scene graph generation (SGG) aims to capture a wide variety of interactions\nbetween pairs of objects, which is essential for full scene understanding.\nExisting SGG methods trained on the entire set of relations fail to acquire\ncomplex reasoning about visual and textual correlations due to various biases\nin training data. Learning on trivial relations that indicate generic spatial\nconfiguration like 'on' instead of informative relations such as 'parked on'\ndoes not enforce this complex reasoning, harming generalization. To address\nthis problem, we propose a novel framework for SGG training that exploits\nrelation labels based on their informativeness. Our model-agnostic training\nprocedure imputes missing informative relations for less informative samples in\nthe training data and trains a SGG model on the imputed labels along with\nexisting annotations. We show that this approach can successfully be used in\nconjunction with state-of-the-art SGG methods and improves their performance\nsignificantly in multiple metrics on the standard Visual Genome benchmark.\nFurthermore, we obtain considerable improvements for unseen triplets in a more\nchallenging zero-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Arushi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1\">Basura Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Referring Video Object Segmentation with Multimodal Transformers. (arXiv:2111.14821v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14821","description":"<p>The referring video object segmentation task (RVOS) involves segmentation of\na text-referred object instance in the frames of a given video. Due to the\ncomplex nature of this multimodal task, which combines text reasoning, video\nunderstanding, instance segmentation and tracking, existing approaches\ntypically rely on sophisticated pipelines in order to tackle it. In this paper,\nwe propose a simple Transformer-based approach to RVOS. Our framework, termed\nMultimodal Tracking Transformer (MTTR), models the RVOS task as a sequence\nprediction problem. Following recent advancements in computer vision and\nnatural language processing, MTTR is based on the realization that video and\ntext can be processed together effectively and elegantly by a single multimodal\nTransformer model. MTTR is end-to-end trainable, free of text-related inductive\nbias components and requires no additional mask-refinement post-processing\nsteps. As such, it simplifies the RVOS pipeline considerably compared to\nexisting methods. Evaluation on standard benchmarks reveals that MTTR\nsignificantly outperforms previous art across multiple metrics. In particular,\nMTTR shows impressive +5.7 and +5.0 mAP gains on the A2D-Sentences and\nJHMDB-Sentences datasets respectively, while processing 76 frames per second.\nIn addition, we report strong results on the public validation set of\nRefer-YouTube-VOS, a more challenging RVOS dataset that has yet to receive the\nattention of researchers. The code to reproduce our experiments is available at\nhttps://github.com/mttr2021/MTTR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Botach_A/0/1/0/all/0/1\">Adam Botach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheltonozhskii_E/0/1/0/all/0/1\">Evgenii Zheltonozhskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baskin_C/0/1/0/all/0/1\">Chaim Baskin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessment of Data Consistency through Cascades of Independently Recurrent Inference Machines for fast and robust accelerated MRI reconstruction. (arXiv:2111.15498v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.15498","description":"<p>Machine Learning methods can learn how to reconstruct Magnetic Resonance\nImages and thereby accelerate acquisition, which is of paramount importance to\nthe clinical workflow. Physics-informed networks incorporate the forward model\nof accelerated MRI reconstruction in the learning process. With increasing\nnetwork complexity, robustness is not ensured when reconstructing data unseen\nduring training. We aim to embed data consistency (DC) in deep networks while\nbalancing the degree of network complexity. While doing so, we will assess\nwhether either explicit or implicit enforcement of DC in varying network\narchitectures is preferred to optimize performance. We propose a scheme called\nCascades of Independently Recurrent Inference Machines (CIRIM) to assess DC\nthrough unrolled optimization. Herein we assess DC both implicitly by gradient\ndescent and explicitly by a designed term. Extensive comparison of the CIRIM to\nCS as well as to other methods is performed: the E2EVN, CascadeNet, KIKINet,\nLPDNet, RIM, IRIM, and UNet. Models were trained and evaluated on T1-weighted\nand FLAIR contrast brain data, and T2-weighted knee data. Both 1D and 2D\nundersampling patterns were evaluated. Robustness was tested by reconstructing\n7.5x prospectively undersampled 3D FLAIR MRI data of Multiple Sclerosis (MS)\npatients with white matter lesions. The CIRIM performed best when implicitly\nenforcing DC, while the E2EVN required an explicit DC formulation. In\nreconstructing MS patient data, prospectively acquired with a sampling pattern\nunseen during model training, the CIRIM maintained lesion contrast while\nefficiently denoising the images. The CIRIM showed highly promising\ngeneralization capabilities maintaining a very fair trade-off between\nreconstructed image quality and fast reconstruction times, which is crucial in\nthe clinical workflow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Karkalousos_D/0/1/0/all/0/1\">D. Karkalousos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noteboom_S/0/1/0/all/0/1\">S. Noteboom</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hulst_H/0/1/0/all/0/1\">H. E. Hulst</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vos_F/0/1/0/all/0/1\">F.M. Vos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Caan_M/0/1/0/all/0/1\">M.W.A. Caan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Background Activation Suppression for Weakly Supervised Object Localization. (arXiv:2112.00580v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00580","description":"<p>Weakly supervised object localization (WSOL) aims to localize objects using\nonly image-level labels. Recently a new paradigm has emerged by generating a\nforeground prediction map (FPM) to achieve localization task. Existing\nFPM-based methods use cross-entropy (CE) to evaluate the foreground prediction\nmap and to guide the learning of generator. We argue for using activation value\nto achieve more efficient learning. It is based on the experimental observation\nthat, for a trained network, CE converges to zero when the foreground mask\ncovers only part of the object region. While activation value increases until\nthe mask expands to the object boundary, which indicates that more object areas\ncan be learned by using activation value. In this paper, we propose a\nBackground Activation Suppression (BAS) method. Specifically, an Activation Map\nConstraint module (AMC) is designed to facilitate the learning of generator by\nsuppressing the background activation value. Meanwhile, by using the foreground\nregion guidance and the area constraint, BAS can learn the whole region of the\nobject. In the inference phase, we consider the prediction maps of different\ncategories together to obtain the final localization results. Extensive\nexperiments show that BAS achieves significant and consistent improvement over\nthe baseline methods on the CUB-200-2011 and ILSVRC datasets. Code and models\nare available at https://github.com/wpy1999/BAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Pingyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperInverter: Improving StyleGAN Inversion via Hypernetwork. (arXiv:2112.00719v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00719","description":"<p>Real-world image manipulation has achieved fantastic progress in recent years\nas a result of the exploration and utilization of GAN latent spaces. GAN\ninversion is the first step in this pipeline, which aims to map the real image\nto the latent code faithfully. Unfortunately, the majority of existing GAN\ninversion methods fail to meet at least one of the three requirements listed\nbelow: high reconstruction quality, editability, and fast inference. We present\na novel two-phase strategy in this research that fits all requirements at the\nsame time. In the first phase, we train an encoder to map the input image to\nStyleGAN2 $\\mathcal{W}$-space, which was proven to have excellent editability\nbut lower reconstruction quality. In the second phase, we supplement the\nreconstruction ability in the initial phase by leveraging a series of\nhypernetworks to recover the missing information during inversion. These two\nsteps complement each other to yield high reconstruction quality thanks to the\nhypernetwork branch and excellent editability due to the inversion done in the\n$\\mathcal{W}$-space. Our method is entirely encoder-based, resulting in\nextremely fast inference. Extensive experiments on two challenging datasets\ndemonstrate the superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tan M. Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1\">Anh Tuan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_R/0/1/0/all/0/1\">Rang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GANSeg: Learning to Segment by Unsupervised Hierarchical Image Generation. (arXiv:2112.01036v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01036","description":"<p>Segmenting an image into its parts is a frequent preprocess for high-level\nvision tasks such as image editing. However, annotating masks for supervised\ntraining is expensive. Weakly-supervised and unsupervised methods exist, but\nthey depend on the comparison of pairs of images, such as from multi-views,\nframes of videos, and image augmentation, which limits their applicability. To\naddress this, we propose a GAN-based approach that generates images conditioned\non latent masks, thereby alleviating full or weak annotations required in\nprevious approaches. We show that such mask-conditioned image generation can be\nlearned faithfully when conditioning the masks in a hierarchical manner on\nlatent keypoints that define the position of parts explicitly. Without\nrequiring supervision of masks or points, this strategy increases robustness to\nviewpoint and object positions changes. It also lets us generate image-mask\npairs for training a segmentation network, which outperforms the\nstate-of-the-art unsupervised segmentation methods on established benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingzhe He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1\">Bastian Wandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OW-DETR: Open-world Detection Transformer. (arXiv:2112.01513v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01513","description":"<p>Open-world object detection (OWOD) is a challenging computer vision problem,\nwhere the task is to detect a known set of object categories while\nsimultaneously identifying unknown objects. Additionally, the model must\nincrementally learn new classes that become known in the next training\nepisodes. Distinct from standard object detection, the OWOD setting poses\nsignificant challenges for generating quality candidate proposals on\npotentially unknown objects, separating the unknown objects from the background\nand detecting diverse unknown objects. Here, we introduce a novel end-to-end\ntransformer-based framework, OW-DETR, for open-world object detection. The\nproposed OW-DETR comprises three dedicated components namely, attention-driven\npseudo-labeling, novelty classification and objectness scoring to explicitly\naddress the aforementioned OWOD challenges. Our OW-DETR explicitly encodes\nmulti-scale contextual information, possesses less inductive bias, enables\nknowledge transfer from known classes to the unknown class and can better\ndiscriminate between unknown objects and background. Comprehensive experiments\nare performed on two benchmarks: MS-COCO and PASCAL VOC. The extensive\nablations reveal the merits of our proposed contributions. Further, our model\noutperforms the recently introduced OWOD approach, ORE, with absolute gains\nranging from 1.8% to 3.3% in terms of unknown recall on MS-COCO. In the case of\nincremental object detection, OW-DETR outperforms the state-of-the-art for all\nsettings on PASCAL VOC. Our code is available at\nhttps://github.com/akshitac8/OW-DETR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshita Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Sanath Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_K/0/1/0/all/0/1\">K J Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E$^2$(GO)MOTION: Motion Augmented Event Stream for Egocentric Action Recognition. (arXiv:2112.03596v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03596","description":"<p>Event cameras are novel bio-inspired sensors, which asynchronously capture\npixel-level intensity changes in the form of \"events\". Due to their sensing\nmechanism, event cameras have little to no motion blur, a very high temporal\nresolution and require significantly less power and memory than traditional\nframe-based cameras. These characteristics make them a perfect fit to several\nreal-world applications such as egocentric action recognition on wearable\ndevices, where fast camera motion and limited power challenge traditional\nvision sensors. However, the ever-growing field of event-based vision has, to\ndate, overlooked the potential of event cameras in such applications. In this\npaper, we show that event data is a very valuable modality for egocentric\naction recognition. To do so, we introduce N-EPIC-Kitchens, the first\nevent-based camera extension of the large-scale EPIC-Kitchens dataset. In this\ncontext, we propose two strategies: (i) directly processing event-camera data\nwith traditional video-processing architectures (E$^2$(GO)) and (ii) using\nevent-data to distill optical flow information (E$^2$(GO)MO). On our proposed\nbenchmark, we show that event data provides a comparable performance to RGB and\noptical flow, yet without any additional flow computation at deploy time, and\nan improved performance of up to 4% with respect to RGB only information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plizzari_C/0/1/0/all/0/1\">Chiara Plizzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Planamente_M/0/1/0/all/0/1\">Mirco Planamente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goletto_G/0/1/0/all/0/1\">Gabriele Goletto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cannici_M/0/1/0/all/0/1\">Marco Cannici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gusso_E/0/1/0/all/0/1\">Emanuele Gusso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1\">Matteo Matteucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's Behind the Couch? Directed Ray Distance Functions (DRDF) for 3D Scene Reconstruction. (arXiv:2112.04481v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04481","description":"<p>We present an approach for full 3D scene reconstruction from a single unseen\nimage. We train on dataset of realistic non-watertight scans of scenes. Our\napproach predicts a distance function, since these have shown promise in\nhandling complex topologies and large spaces. We identify and analyze two key\nchallenges for predicting such image conditioned distance functions that have\nprevented their success on real 3D scene data. First, we show that predicting a\nconventional scene distance from an image requires reasoning over a large\nreceptive field. Second, we analytically show that the optimal output of the\nnetwork trained to predict these distance functions does not obey all the\ndistance function properties. We propose an alternate distance function, the\nDirected Ray Distance Function (DRDF), that tackles both challenges. We show\nthat a deep network trained to predict DRDFs outperforms all other methods\nquantitatively and qualitatively on 3D reconstruction from single image on\nMatterport3D, 3DFront, and ScanNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_N/0/1/0/all/0/1\">Nilesh Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Justin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1\">David F. Fouhey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Point Transformer. (arXiv:2112.04702v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04702","description":"<p>The recent success of neural networks enables a better interpretation of 3D\npoint clouds, but processing a large-scale 3D scene remains a challenging\nproblem. Most current approaches divide a large-scale scene into small regions\nand combine the local predictions together. However, this scheme inevitably\ninvolves additional stages for pre- and post-processing and may also degrade\nthe final output due to predictions in a local perspective. This paper\nintroduces Fast Point Transformer that consists of a new lightweight\nself-attention layer. Our approach encodes continuous 3D coordinates, and the\nvoxel hashing-based architecture boosts computational efficiency. The proposed\nmethod is demonstrated with 3D semantic segmentation and 3D detection. The\naccuracy of our approach is competitive to the best voxel-based method, and our\nnetwork achieves 129 times faster inference time than the state-of-the-art,\nPoint Transformer, with a reasonable accuracy trade-off in 3D semantic\nsegmentation on S3DIS dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yoonwoo Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized Spatio-Temporal Contrastive Learning with Self-Supervision. (arXiv:2112.05181v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05181","description":"<p>Modern self-supervised learning algorithms typically enforce persistency of\ninstance representations across views. While being very effective on learning\nholistic image and video representations, such an objective becomes sub-optimal\nfor learning spatio-temporally fine-grained features in videos, where scenes\nand instances evolve through space and time. In this paper, we present\nContextualized Spatio-Temporal Contrastive Learning (ConST-CL) to effectively\nlearn spatio-temporally fine-grained video representations via\nself-supervision. We first design a region-based pretext task which requires\nthe model to transform in-stance representations from one view to another,\nguided by context features. Further, we introduce a simple network design that\nsuccessfully reconciles the simultaneous learning process of both holistic and\nlocal representations. We evaluate our learned representations on a variety of\ndownstream tasks and show that ConST-CL achieves competitive results on 6\ndatasets, including Kinetics, UCF, HMDB, AVA-Kinetics, AVA and OTB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liangzhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroff_F/0/1/0/all/0/1\">Florian Schroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1\">Hartwig Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes. (arXiv:2112.05298v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05298","description":"<p>Building embodied intelligent agents that can interact with 3D indoor\nenvironments has received increasing research attention in recent years. While\nmost works focus on single-object or agent-object visual functionality and\naffordances, our work proposes to study a new kind of visual relationship that\nis also important to perceive and model -- inter-object functional\nrelationships (e.g., a switch on the wall turns on or off the light, a remote\ncontrol operates the TV). Humans often spend little or no effort to infer these\nrelationships, even when entering a new room, by using our strong prior\nknowledge (e.g., we know that buttons control electrical devices) or using only\na few exploratory interactions in cases of uncertainty (e.g., multiple switches\nand lights in the same room). In this paper, we take the first step in building\nAI system learning inter-object functional relationships in 3D indoor\nenvironments with key technical contributions of modeling prior knowledge by\ntraining over large-scale scenes and designing interactive policies for\neffectively exploring the training scenes and quickly adapting to novel test\nscenes. We create a new benchmark based on the AI2Thor and PartNet datasets and\nperform extensive experiments that prove the effectiveness of our proposed\nmethod. Results show that our model successfully learns priors and\nfast-interactive-adaptation strategies for exploring inter-object functional\nrelationships in complex 3D scenes. Several ablation studies further validate\nthe usefulness of each proposed module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimedia Datasets for Anomaly Detection: A Review. (arXiv:2112.05410v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05410","description":"<p>Multimedia anomaly datasets play a crucial role in automated surveillance.\nThey have a wide range of applications expanding from outlier objects/\nsituation detection to the detection of life-threatening events. For more than\n1.5 decades, this field has attracted a lot of research attention, and as a\nresult, more and more datasets dedicated to anomalous actions and object\ndetection have been developed. Tapping these public anomaly datasets enable\nresearchers to generate and compare various anomaly detection frameworks with\nthe same input data. This paper presents a comprehensive survey on a variety of\nvideo, audio, as well as audio-visual datasets based on the application of\nanomaly detection. This survey aims to address the lack of a comprehensive\ncomparison and analysis of multimedia public datasets based on anomaly\ndetection. Also, it can assist researchers in selecting the best available\ndataset for bench-marking frameworks. Additionally, we discuss gaps in the\nexisting dataset and insights for future direction towards developing\nmultimodal anomaly detection datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumari_P/0/1/0/all/0/1\">Pratibha Kumari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedi_A/0/1/0/all/0/1\">Anterpreet Kaur Bedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_M/0/1/0/all/0/1\">Mukesh Saini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I M Avatar: Implicit Morphable Head Avatars from Videos. (arXiv:2112.07471v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07471","description":"<p>Traditional 3D morphable face models (3DMMs) provide fine-grained control\nover expression but cannot easily capture geometric and appearance details.\nNeural volumetric representations approach photorealism but are hard to animate\nand do not generalize well to unseen expressions. To tackle this problem, we\npropose IMavatar (Implicit Morphable avatar), a novel method for learning\nimplicit head avatars from monocular videos. Inspired by the fine-grained\ncontrol mechanisms afforded by conventional 3DMMs, we represent the expression-\nand pose- related deformations via learned blendshapes and skinning fields.\nThese attributes are pose-independent and can be used to morph the canonical\ngeometry and texture fields given novel expression and pose parameters. We\nemploy ray marching and iterative root-finding to locate the canonical surface\nintersection for each pixel. A key contribution is our novel analytical\ngradient formulation that enables end-to-end training of IMavatars from videos.\nWe show quantitatively and qualitatively that our method improves geometry and\ncovers a more complete expression space compared to state-of-the-art methods.\nCode, video, and data can be found at\nhttps://ait.ethz.ch/projects/2022/IMavatar/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yufeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrevaya_V/0/1/0/all/0/1\">Victoria Fern&#xe1;ndez Abrevaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1\">Marcel C. B&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing the Deep: Finding Class Specific Filters in Deep CNNs. (arXiv:2112.07719v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07719","description":"<p>Interpretability of Deep Neural Networks has become a major area of\nexploration. Although these networks have achieved state of the art accuracy in\nmany tasks, it is extremely difficult to interpret and explain their decisions.\nIn this work we analyze the final and penultimate layers of Deep Convolutional\nNetworks and provide an efficient method for identifying subsets of features\nthat contribute most towards the network's decision for a class. We demonstrate\nthat the number of such features per class is much lower in comparison to the\ndimension of the final layer and therefore the decision surface of Deep CNNs\nlies on a low dimensional manifold and is proportional to the network depth.\nOur methods allow to decompose the final layer into separate subspaces which is\nfar more interpretable and has a lower computational cost as compared to the\nfinal layer of the full network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Badola_A/0/1/0/all/0/1\">Akshay Badola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_C/0/1/0/all/0/1\">Cherian Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanabhan_V/0/1/0/all/0/1\">Vineet Padmanabhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_R/0/1/0/all/0/1\">Rajendra Lal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Pursuit: Building a Space of Objects via Discriminative Weight Generation. (arXiv:2112.07954v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07954","description":"<p>We propose a framework to continuously learn object-centric representations\nfor visual learning and understanding. Existing object-centric representations\neither rely on supervisions that individualize objects in the scene, or perform\nunsupervised disentanglement that can hardly deal with complex scenes in the\nreal world. To mitigate the annotation burden and relax the constraints on the\nstatistical complexity of the data, our method leverages interactions to\neffectively sample diverse variations of an object and the corresponding\ntraining signals while learning the object-centric representations. Throughout\nlearning, objects are streamed one by one in random order with unknown\nidentities, and are associated with latent codes that can synthesize\ndiscriminative weights for each object through a convolutional hypernetwork.\nMoreover, re-identification of learned objects and forgetting prevention are\nemployed to make the learning process efficient and robust. We perform an\nextensive study of the key features of the proposed framework and analyze the\ncharacteristics of the learned representations. Furthermore, we demonstrate the\ncapability of the proposed framework in learning representations that can\nimprove label efficiency in downstream tasks. Our code and trained models are\nmade publicly available at: https://github.com/pptrick/Object-Pursuit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chuanyu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yueqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnweaveNet: Unweaving Activity Stories. (arXiv:2112.10194v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10194","description":"<p>Our lives can be seen as a complex weaving of activities; we switch from one\nactivity to another, to maximise our achievements or in reaction to demands\nplaced upon us. Observing a video of unscripted daily activities, we parse the\nvideo into its constituent activity threads through a process we call\nunweaving. To accomplish this, we introduce a video representation explicitly\ncapturing activity threads called a thread bank, along with a neural controller\ncapable of detecting goal changes and resuming of past activities, together\nforming UnweaveNet. We train and evaluate UnweaveNet on sequences from the\nunscripted egocentric dataset EPIC-KITCHENS. We propose and showcase the\nefficacy of pretraining UnweaveNet in a self-supervised manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Price_W/0/1/0/all/0/1\">Will Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1\">Dima Damen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface-Aligned Neural Radiance Fields for Controllable 3D Human Synthesis. (arXiv:2201.01683v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01683","description":"<p>We propose a new method for reconstructing controllable implicit 3D human\nmodels from sparse multi-view RGB videos. Our method defines the neural scene\nrepresentation on the mesh surface points and signed distances from the surface\nof a human body mesh. We identify an indistinguishability issue that arises\nwhen a point in 3D space is mapped to its nearest surface point on a mesh for\nlearning surface-aligned neural scene representation. To address this issue, we\npropose projecting a point onto a mesh surface using a barycentric\ninterpolation with modified vertex normals. Experiments with the ZJU-MoCap and\nHuman3.6M datasets show that our approach achieves a higher quality in a\nnovel-view and novel-pose synthesis than existing methods. We also demonstrate\nthat our method easily supports the control of body shape and clothes. Project\npage: https://pfnet-research.github.io/surface-aligned-nerf/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianhan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_Y/0/1/0/all/0/1\">Yasuhiro Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumoto_E/0/1/0/all/0/1\">Eiichi Matsumoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Amplitude SAR Imagery Splicing Localization. (arXiv:2201.02409v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02409","description":"<p>Synthetic Aperture Radar (SAR) images are a valuable asset for a wide variety\nof tasks. In the last few years, many websites have been offering them for free\nin the form of easy to manage products, favoring their widespread diffusion and\nresearch work in the SAR field. The drawback of these opportunities is that\nsuch images might be exposed to forgeries and manipulations by malicious users,\nraising new concerns about their integrity and trustworthiness. Up to now, the\nmultimedia forensics literature has proposed various techniques to localize\nmanipulations in natural photographs, but the integrity assessment of SAR\nimages was never investigated. This task poses new challenges, since SAR images\nare generated with a processing chain completely different from that of natural\nphotographs. This implies that many forensics methods developed for natural\nimages are not guaranteed to succeed. In this paper, we investigate the problem\nof amplitude SAR imagery splicing localization. Our goal is to localize regions\nof an amplitude SAR image that have been copied and pasted from another image,\npossibly undergoing some kind of editing in the process. To do so, we leverage\na Convolutional Neural Network (CNN) to extract a fingerprint highlighting\ninconsistencies in the processing traces of the analyzed input. Then, we\nexamine this fingerprint to produce a binary tampering mask indicating the\npixel region under splicing attack. Results show that our proposed method,\ntailored to the nature of SAR signals, provides better performances than\nstate-of-the-art forensic tools developed for natural images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cannas_E/0/1/0/all/0/1\">Edoardo Daniele Cannas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonettini_N/0/1/0/all/0/1\">Nicol&#xf2; Bonettini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandelli_S/0/1/0/all/0/1\">Sara Mandelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bestagini_P/0/1/0/all/0/1\">Paolo Bestagini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tubaro_S/0/1/0/all/0/1\">Stefano Tubaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relieving Long-tailed Instance Segmentation via Pairwise Class Balance. (arXiv:2201.02784v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02784","description":"<p>Long-tailed instance segmentation is a challenging task due to the extreme\nimbalance of training samples among classes. It causes severe biases of the\nhead classes (with majority samples) against the tailed ones. This renders \"how\nto appropriately define and alleviate the bias\" one of the most important\nissues. Prior works mainly use label distribution or mean score information to\nindicate a coarse-grained bias. In this paper, we explore to excavate the\nconfusion matrix, which carries the fine-grained misclassification details, to\nrelieve the pairwise biases, generalizing the coarse one. To this end, we\npropose a novel Pairwise Class Balance (PCB) method, built upon a confusion\nmatrix which is updated during training to accumulate the ongoing prediction\npreferences. PCB generates fightback soft labels for regularization during\ntraining. Besides, an iterative learning paradigm is developed to support a\nprogressive and smooth regularization in such debiasing. PCB can be plugged and\nplayed to any existing method as a complement. Experimental results on LVIS\ndemonstrate that our method achieves state-of-the-art performance without bells\nand whistles. Superior results across various architectures show the\ngeneralization ability. The code and trained models are available at\nhttps://github.com/megvii-research/PCB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yin-Yin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peizhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiu-Shen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAXIM: Multi-Axis MLP for Image Processing. (arXiv:2201.02973v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02973","description":"<p>Recent progress on Transformers and multi-layer perceptron (MLP) models\nprovide new network architectural designs for computer vision tasks. Although\nthese models proved to be effective in many vision tasks such as image\nrecognition, there remain challenges in adapting them for low-level vision. The\ninflexibility to support high-resolution images and limitations of local\nattention are perhaps the main bottlenecks. In this work, we present a\nmulti-axis MLP based architecture called MAXIM, that can serve as an efficient\nand flexible general-purpose vision backbone for image processing tasks. MAXIM\nuses a UNet-shaped hierarchical structure and supports long-range interactions\nenabled by spatially-gated MLPs. Specifically, MAXIM contains two MLP-based\nbuilding blocks: a multi-axis gated MLP that allows for efficient and scalable\nspatial mixing of local and global visual cues, and a cross-gating block, an\nalternative to cross-attention, which accounts for cross-feature conditioning.\nBoth these modules are exclusively based on MLPs, but also benefit from being\nboth global and `fully-convolutional', two properties that are desirable for\nimage processing. Our extensive experimental results show that the proposed\nMAXIM model achieves state-of-the-art performance on more than ten benchmarks\nacross a range of image processing tasks, including denoising, deblurring,\nderaining, dehazing, and enhancement while requiring fewer or comparable\nnumbers of parameters and FLOPs than competitive models. The source code and\ntrained models will be available at\n\\url{https://github.com/google-research/maxim}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzhong Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan Bovik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yinxiao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-driven Semantic Segmentation. (arXiv:2201.03546v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03546","description":"<p>We present LSeg, a novel model for language-driven semantic image\nsegmentation. LSeg uses a text encoder to compute embeddings of descriptive\ninput labels (e.g., \"grass\" or \"building\") together with a transformer-based\nimage encoder that computes dense per-pixel embeddings of the input image. The\nimage encoder is trained with a contrastive objective to align pixel embeddings\nto the text embedding of the corresponding semantic class. The text embeddings\nprovide a flexible label representation in which semantically similar labels\nmap to similar regions in the embedding space (e.g., \"cat\" and \"furry\"). This\nallows LSeg to generalize to previously unseen categories at test time, without\nretraining or even requiring a single additional training sample. We\ndemonstrate that our approach achieves highly competitive zero-shot performance\ncompared to existing zero- and few-shot semantic segmentation methods, and even\nmatches the accuracy of traditional segmentation algorithms when a fixed label\nset is provided. Code and demo are available at\nhttps://github.com/isl-org/lang-seg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranftl_R/0/1/0/all/0/1\">Ren&#xe9; Ranftl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Saliency based Feature Fusion Model for EEG Emotion Estimation. (arXiv:2201.03891v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03891","description":"<p>Among the different modalities to assess emotion, electroencephalogram (EEG),\nrepresenting the electrical brain activity, achieved motivating results over\nthe last decade. Emotion estimation from EEG could help in the diagnosis or\nrehabilitation of certain diseases. In this paper, we propose a dual model\nconsidering two different representations of EEG feature maps: 1) a sequential\nbased representation of EEG band power, 2) an image-based representation of the\nfeature vectors. We also propose an innovative method to combine the\ninformation based on a saliency analysis of the image-based model to promote\njoint learning of both model parts. The model has been evaluated on four\npublicly available datasets: SEED-IV, SEED, DEAP and MPED. The achieved results\noutperform results from state-of-the-art approaches for three of the proposed\ndatasets with a lower standard deviation that reflects higher stability. For\nsake of reproducibility, the codes and models proposed in this paper are\navailable at https://github.com/VDelv/Emotion-EEG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delvigne_V/0/1/0/all/0/1\">Victor Delvigne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facchini_A/0/1/0/all/0/1\">Antoine Facchini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wannous_H/0/1/0/all/0/1\">Hazem Wannous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutoit_T/0/1/0/all/0/1\">Thierry Dutoit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ris_L/0/1/0/all/0/1\">Laurence Ris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandeborre_J/0/1/0/all/0/1\">Jean-Philippe Vandeborre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Captcha Attack: Turning Captchas Against Humanity. (arXiv:2201.04014v3 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2201.04014","description":"<p>Nowadays, people generate and share massive content on online platforms\n(e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook\nusers posted around 150 thousand photos every minute. Content moderators\nconstantly monitor these online platforms to prevent the spreading of\ninappropriate content (e.g., hate speech, nudity images). Based on deep\nlearning (DL) advances, Automatic Content Moderators (ACM) help human\nmoderators handle high data volume. Despite their advantages, attackers can\nexploit weaknesses of DL components (e.g., preprocessing, model) to affect\ntheir performance. Therefore, an attacker can leverage such techniques to\nspread inappropriate content by evading ACM.\n</p>\n<p>In this work, we propose CAPtcha Attack (CAPA), an adversarial technique that\nallows users to spread inappropriate text online by evading ACM controls. CAPA,\nby generating custom textual CAPTCHAs, exploits ACM's careless design\nimplementations and internal procedures vulnerabilities. We test our attack on\nreal-world ACM, and the results confirm the ferocity of our simple yet\neffective attack, reaching up to a 100% evasion success in most cases. At the\nsame time, we demonstrate the difficulties in designing CAPA mitigations,\nopening new challenges in CAPTCHAs research area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajola_L/0/1/0/all/0/1\">Luca Pajola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tricomi_P/0/1/0/all/0/1\">Pier Paolo Tricomi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-free Online Test-time Adaptation. (arXiv:2201.05718v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05718","description":"<p>Training state-of-the-art vision models has become prohibitively expensive\nfor researchers and practitioners. For the sake of accessibility and resource\nreuse, it is important to focus on adapting these models to a variety of\ndownstream scenarios. An interesting and practical paradigm is online test-time\nadaptation, according to which training data is inaccessible, no labelled data\nfrom the test distribution is available, and adaptation can only happen at test\ntime and on a handful of samples. In this paper, we investigate how test-time\nadaptation methods fare for a number of pre-trained models on a variety of\nreal-world scenarios, significantly extending the way they have been originally\nevaluated. We show that they perform well only in narrowly-defined experimental\nsetups and sometimes fail catastrophically when their hyperparameters are not\nselected for the same scenario in which they are being tested. Motivated by the\ninherent uncertainty around the conditions that will ultimately be encountered\nat test time, we propose a particularly \"conservative\" approach, which\naddresses the problem with a Laplacian Adjusted Maximum-likelihood Estimation\n(LAME) objective. By adapting the model's output (not its parameters), and\nsolving our objective with an efficient concave-convex procedure, our approach\nexhibits a much higher average accuracy across scenarios than existing methods,\nwhile being notably faster and have a much lower memory footprint. The code is\navailable at https://github.com/fiveai/LAME.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1\">Malik Boudiaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_R/0/1/0/all/0/1\">Romain Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertinetto_L/0/1/0/all/0/1\">Luca Bertinetto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Weakly Supervised Pre-Training of Visual Perception Models. (arXiv:2201.08371v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08371","description":"<p>Model pre-training is a cornerstone of modern visual recognition systems.\nAlthough fully supervised pre-training on datasets like ImageNet is still the\nde-facto standard, recent studies suggest that large-scale weakly supervised\npre-training can outperform fully supervised approaches. This paper revisits\nweakly-supervised pre-training of models using hashtag supervision with modern\nversions of residual networks and the largest-ever dataset of images and\ncorresponding hashtags. We study the performance of the resulting models in\nvarious transfer-learning settings including zero-shot transfer. We also\ncompare our models with those obtained via large-scale self-supervised\nlearning. We find our weakly-supervised models to be very competitive across\nall settings, and find they substantially outperform their self-supervised\ncounterparts. We also include an investigation into whether our models learned\npotentially troubling associations or stereotypes. Overall, our results provide\na compelling argument for the use of weakly supervised learning in the\ndevelopment of visual recognition systems. Our models, Supervised Weakly\nthrough hashtAGs (SWAG), are available publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mannat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gustafson_L/0/1/0/all/0/1\">Laura Gustafson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adcock_A/0/1/0/all/0/1\">Aaron Adcock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reis_V/0/1/0/all/0/1\">Vinicius de Freitas Reis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedik_B/0/1/0/all/0/1\">Bugra Gedik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosaraju_R/0/1/0/all/0/1\">Raj Prateek Kosaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Dhruv Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1\">Ross Girshick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollar_P/0/1/0/all/0/1\">Piotr Doll&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaten_L/0/1/0/all/0/1\">Laurens van der Maaten</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Pixel Trajectories with Multiscale Contrastive Random Walks. (arXiv:2201.08379v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08379","description":"<p>A range of video modeling tasks, from optical flow to multiple object\ntracking, share the same fundamental challenge: establishing space-time\ncorrespondence. Yet, approaches that dominate each space differ. We take a step\ntowards bridging this gap by extending the recent contrastive random walk\nformulation to much denser, pixel-level space-time graphs. The main\ncontribution is introducing hierarchy into the search problem by computing the\ntransition matrix between two frames in a coarse-to-fine manner, forming a\nmultiscale contrastive random walk when extended in time. This establishes a\nunified technique for self-supervised learning of optical flow, keypoint\ntracking, and video object segmentation. Experiments demonstrate that, for each\nof these tasks, the unified model achieves performance competitive with strong\nself-supervised approaches specific to that task. Project webpage:\nhttps://jasonbian97.github.io/flowwalk\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1\">Zhangxing Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabri_A/0/1/0/all/0/1\">Allan Jabri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owens_A/0/1/0/all/0/1\">Andrew Owens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast MRI Reconstruction: How Powerful Transformers Are?. (arXiv:2201.09400v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.09400","description":"<p>Magnetic resonance imaging (MRI) is a widely used non-radiative and\nnon-invasive method for clinical interrogation of organ structures and\nmetabolism, with an inherently long scanning time. Methods by k-space\nundersampling and deep learning based reconstruction have been popularised to\naccelerate the scanning process. This work focuses on investigating how\npowerful transformers are for fast MRI by exploiting and comparing different\nnovel network architectures. In particular, a generative adversarial network\n(GAN) based Swin transformer (ST-GAN) was introduced for the fast MRI\nreconstruction. To further preserve the edge and texture information, edge\nenhanced GAN based Swin transformer (EES-GAN) and texture enhanced GAN based\nSwin transformer (TES-GAN) were also developed, where a dual-discriminator GAN\nstructure was applied. We compared our proposed GAN based transformers,\nstandalone Swin transformer and other convolutional neural networks based GAN\nmodel in terms of the evaluation metrics PSNR, SSIM and FID. We showed that\ntransformers work well for the MRI reconstruction from different undersampling\nconditions. The utilisation of GAN's adversarial structure improves the quality\nof images reconstructed when undersampled for 30% or higher. The code is\npublicly available at https://github.com/ayanglab/SwinGANMR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Jiahao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yinzhe Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Huanjun Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning. (arXiv:2201.09671v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.09671","description":"<p>Since severe droughts are occurring more frequently and lengthening the dry\nseason in the Amazon Rainforest, it is important to detect wildfires promptly\nand forecast possible spread for effective suppression response. Though\ncomputer vision researchers have applied algorithms to automatically detect\nwildfires, current models are computationally expensive and not versatile\nenough for the low technology conditions of South American wildfire hotspots.\nThis comprehensive deep learning study first trains a Fully Convolutional\nNeural Network with skip connections on multispectral Landsat 8 images of\nEcuador and the Galapagos. The model uses Green and Short-wave Infrared (SWIR)\nbands as inputs to predict each image's corresponding pixel-level binary fire\nmask. This model achieves a 0.962 validation F2 score and a 0.932 F2 score on\ntest data from Guyana and Suriname. Afterward, image segmentation is conducted\non the Cirrus band using K-Means Clustering to simplify continuous pixel values\ninto three discrete classes representing differing degrees of cirrus cloud\ncontamination. Two additional Convolutional Neural Networks are trained to\nclassify the presence of a wildfire using these segmented cirrus images. The\n\"experimental\" model trained on the segmented inputs and SWIR data achieves a\nbinary accuracy that is 2.306% higher than that of the \"benchmark model\" that\nis trained only on SWIR data. The difference in performance has a p-value of\n0.00968. This proof of concept reveals that feature simplification can improve\nthe performance of wildfire detection models. Overall, the software built in\nthis study is useful for early and accurate detection of wildfires in South\nAmerica.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Christopher Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Approach for Digital Color Reconstruction of Lenticular Films. (arXiv:2202.05270v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.05270","description":"<p>We propose the first accurate digitization and color reconstruction process\nfor historical lenticular film that is robust to artifacts. Lenticular films\nemerged in the 1920s and were one of the first technologies that permitted to\ncapture full color information in motion. The technology leverages an RGB\nfilter and cylindrical lenticules embossed on the film surface to encode the\ncolor in the horizontal spatial dimension of the image. To project the pictures\nthe encoding process was reversed using an appropriate analog device. In this\nwork, we introduce an automated, fully digital pipeline to process the scan of\nlenticular films and colorize the image. Our method merges deep learning with a\nmodel-based approach in order to maximize the performance while making sure\nthat the reconstructed colored images truthfully match the encoded color\ninformation. Our model employs different strategies to achieve an effective\ncolor reconstruction, in particular (i) we use data augmentation to create a\nrobust lenticule segmentation network, (ii) we fit the lenticules raster\nprediction to obtain a precise vectorial lenticule localization, and (iii) we\ntrain a colorization network that predicts interpolation coefficients in order\nto obtain a truthful colorization. We validate the proposed method on a\nlenticular film dataset and compare it to other approaches. Since no colored\ngroundtruth is available as reference, we conduct a user study to validate our\nmethod in a subjective manner. The results of the study show that the proposed\nmethod is largely preferred with respect to other existing and baseline\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+DAronco_S/0/1/0/all/0/1\">Stefano D&#x27;Aronco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trumpy_G/0/1/0/all/0/1\">Giorgio Trumpy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pfluger_D/0/1/0/all/0/1\">David Pfluger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Deep Learning Techniques for the Analysis of COVID-19 and their usability for Detecting Omicron. (arXiv:2202.06372v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.06372","description":"<p>The Coronavirus (COVID-19) outbreak in December 2019 has become an ongoing\nthreat to humans worldwide, creating a health crisis that infected millions of\nlives, as well as devastating the global economy. Deep learning (DL) techniques\nhave proved helpful in analysis and delineation of infectious regions in\nradiological images in a timely manner. This paper makes an in-depth survey of\nDL techniques and draws a taxonomy based on diagnostic strategies and learning\napproaches. DL techniques are systematically categorized into classification,\nsegmentation, and multi-stage approaches for COVID-19 diagnosis at image and\nregion level analysis. Each category includes pre-trained and custom-made\nConvolutional Neural Network architectures for detecting COVID-19 infection in\nradiographic imaging modalities; X-Ray, and Computer Tomography (CT).\nFurthermore, a discussion is made on challenges in developing diagnostic\ntechniques such as cross-platform interoperability and examining imaging\nmodality. Similarly, a review of the various methodologies and performance\nmeasures used in these techniques is also presented. This survey provides an\ninsight into the promising areas of research in DL for analyzing radiographic\nimages, and further accelerates the research in designing customized DL based\ndiagnostic tools for effectively dealing with new variants of COVID-19 and\nemerging challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1\">Asifullah Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1\">Saddam Hussain Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saif_M/0/1/0/all/0/1\">Mahrukh Saif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Batool_A/0/1/0/all/0/1\">Asiya Batool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sohail_A/0/1/0/all/0/1\">Anabia Sohail</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Waleed Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Convolutional Networks for Multi-modality Medical Imaging: Methods, Architectures, and Clinical Applications. (arXiv:2202.08916v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.08916","description":"<p>Image-based characterization and disease understanding involve integrative\nanalysis of morphological, spatial, and topological information across\nbiological scales. The development of graph convolutional networks (GCNs) has\ncreated the opportunity to address this information complexity via graph-driven\narchitectures, since GCNs can perform feature aggregation, interaction, and\nreasoning with remarkable flexibility and efficiency. These GCNs capabilities\nhave spawned a new wave of research in medical imaging analysis with the\noverarching goal of improving quantitative disease understanding, monitoring,\nand diagnosis. Yet daunting challenges remain for designing the important\nimage-to-graph transformation for multi-modality medical imaging and gaining\ninsights into model interpretation and enhanced clinical decision support. In\nthis review, we present recent GCNs developments in the context of medical\nimage analysis including imaging data from radiology and histopathology. We\ndiscuss the fast-growing use of graph network architectures in medical image\nanalysis to improve disease diagnosis and patient outcomes in clinical\npractice. To foster cross-disciplinary research, we present GCNs technical\nadvancements, emerging medical applications, identify common challenges in the\nuse of image-based GCNs and their extensions in model interpretation,\nlarge-scale benchmarks that promise to transform the scope of medical image\nstudies and related graph-driven medical research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ding_K/0/1/0/all/0/1\">Kexin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zichen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_C/0/1/0/all/0/1\">Corey W. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitri N. Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knock, knock. Who's there? -- Identifying football player jersey numbers with synthetic data. (arXiv:2203.00734v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00734","description":"<p>Automatic player identification is an essential and complex task in sports\nvideo analysis. Different strategies have been devised over the years, but\nidentification based on jersey numbers is one of the most common approaches\ngiven its versatility and relative simplicity. However, automatic detection of\njersey numbers is still challenging due to changing camera angles, low video\nresolution, small object size in wide-range shots and transient changes in the\nplayer's posture and movement. In this paper we present a novel approach for\njersey number identification in a small, highly imbalanced dataset from the\nSeattle Seahawks practice videos. Our results indicate that simple models can\nachieve an acceptable performance on the jersey number detection task and that\nsynthetic data can improve the performance dramatically (accuracy increase of\n~9% overall, ~18% on low frequency numbers) making our approach achieve state\nof the art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhargavi_D/0/1/0/all/0/1\">Divya Bhargavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coyotl_E/0/1/0/all/0/1\">Erika Pelaez Coyotl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_S/0/1/0/all/0/1\">Sia Gholami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Common Corruptions and Data Augmentation. (arXiv:2203.01441v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01441","description":"<p>We introduce a set of image transformations that can be used as corruptions\nto evaluate the robustness of models as well as data augmentation mechanisms\nfor training neural networks. The primary distinction of the proposed\ntransformations is that, unlike existing approaches such as Common Corruptions,\nthe geometry of the scene is incorporated in the transformations -- thus\nleading to corruptions that are more likely to occur in the real world. We also\nintroduce a set of semantic corruptions (e.g. natural object occlusions). We\nshow these transformations are `efficient' (can be computed on-the-fly),\n`extendable' (can be applied on most image datasets), expose vulnerability of\nexisting models, and can effectively make models more robust when employed as\n`3D data augmentation' mechanisms. The evaluations on several tasks and\ndatasets suggest incorporating 3D information into benchmarking and training\nopens up a promising direction for robustness research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kar_O/0/1/0/all/0/1\">O&#x11f;uzhan Fatih Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_T/0/1/0/all/0/1\">Teresa Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1\">Andrei Atanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_A/0/1/0/all/0/1\">Amir Zamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AugHover-Net: Augmenting Hover-net for Nucleus Segmentation and Classification. (arXiv:2203.03415v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.03415","description":"<p>Nuclei segmentation and classification have been a challenge in digital\npathology due to the specific domain characteristics. First, annotating a\nlarge-scale dataset is quite consuming. It requires specific domain knowledge\nand large efforts. Second, some nuclei are clustered together and hard to\nsegment from each other. Third, the classes are often extremely unbalanced. As\nin Lizard, the number of epithelial nuclei is around 67 times larger than the\nnumber of eosinophil nuclei. Fourth, the nuclei often exhibit high inter-class\nsimilarity and intra-class variability. Connective nuclei may look very\ndifferent from each other while some of them share a similar shape with the\nepithelial ones. Last but not least, pathological patches may have very\ndifferent color distributions among different datasets. Thus, a large-scale\ngenerally annotated dataset and a specially-designed algorithm are needed to\nsolve this problem. The CoNIC challenge aims to promote the automatic\nsegmentation and classification task and requires researchers to develop\nalgorithms that perform segmentation, classification, and counting of 6\ndifferent types of nuclei with the large-scale annotated dataset: Lizard. Due\nto the 60-minute time limit, the algorithm has to be simple and quick. In this\npaper, we briefly describe the final method we used in the CoNIC challenge. Our\nalgorithm is based on Hover-Net and we added several modifications to it to\nimprove its performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wenhua Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language. (arXiv:2203.03598v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03598","description":"<p>Learning to classify video data from classes not included in the training\ndata, i.e. video-based zero-shot learning, is challenging. We conjecture that\nthe natural alignment between the audio and visual modalities in video data\nprovides a rich training signal for learning discriminative multi-modal\nrepresentations. Focusing on the relatively underexplored task of audio-visual\nzero-shot learning, we propose to learn multi-modal representations from\naudio-visual data using cross-modal attention and exploit textual label\nembeddings for transferring knowledge from seen classes to unseen classes.\nTaking this one step further, in our generalised audio-visual zero-shot\nlearning setting, we include all the training classes in the test-time search\nspace which act as distractors and increase the difficulty while making the\nsetting more realistic. Due to the lack of a unified benchmark in this domain,\nwe introduce a (generalised) zero-shot learning benchmark on three audio-visual\ndatasets of varying sizes and difficulty, VGGSound, UCF, and ActivityNet,\nensuring that the unseen test classes do not appear in the dataset used for\nsupervised training of the backbone deep models. Comparing multiple relevant\nand recent methods, we demonstrate that our proposed AVCA model achieves\nstate-of-the-art performance on all three datasets. Code and data are available\nat \\url{https://github.com/ExplainableML/AVCA-GZSL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mercea_O/0/1/0/all/0/1\">Otniel-Bogdan Mercea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesch_L/0/1/0/all/0/1\">Lukas Riesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1\">A. Sophia Koepke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Distinctive Margin toward Active Domain Adaptation. (arXiv:2203.05738v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.05738","description":"<p>Despite plenty of efforts focusing on improving the domain adaptation ability\n(DA) under unsupervised or few-shot semi-supervised settings, recently the\nsolution of active learning started to attract more attention due to its\nsuitability in transferring model in a more practical way with limited\nannotation resource on target data. Nevertheless, most active learning methods\nare not inherently designed to handle domain gap between data distribution, on\nthe other hand, some active domain adaptation methods (ADA) usually requires\ncomplicated query functions, which is vulnerable to overfitting. In this work,\nwe propose a concise but effective ADA method called\nSelect-by-Distinctive-Margin (SDM), which consists of a maximum margin loss and\na margin sampling algorithm for data selection. We provide theoretical analysis\nto show that SDM works like a Support Vector Machine, storing hard examples\naround decision boundaries and exploiting them to find informative and\ntransferable data. In addition, we propose two variants of our method, one is\ndesigned to adaptively adjust the gradient from margin loss, the other boosts\nthe selectivity of margin sampling by taking the gradient direction into\naccount. We benchmark SDM with standard active learning setting, demonstrating\nour algorithm achieves competitive results with good data scalability. Code is\navailable at https://github.com/TencentYoutuResearch/ActiveLearning-SDM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Ming Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zekun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhenye Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhongyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_M/0/1/0/all/0/1\">Mingmin Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06717","description":"<p>We revisit large kernel design in modern convolutional neural networks\n(CNNs). Inspired by recent advances in vision transformers (ViTs), in this\npaper, we demonstrate that using a few large convolutional kernels instead of a\nstack of small kernels could be a more powerful paradigm. We suggested five\nguidelines, e.g., applying re-parameterized large depth-wise convolutions, to\ndesign efficient high-performance large-kernel CNNs. Following the guidelines,\nwe propose RepLKNet, a pure CNN architecture whose kernel size is as large as\n31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the\nperformance gap between CNNs and ViTs, e.g., achieving comparable or superior\nresults than Swin Transformer on ImageNet and a few typical downstream tasks,\nwith lower latency. RepLKNet also shows nice scalability to big data and large\nmodels, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K,\nwhich is very competitive among the state-of-the-arts with similar model sizes.\nOur study further reveals that, in contrast to small-kernel CNNs, large-kernel\nCNNs have much larger effective receptive fields and higher shape bias rather\nthan texture bias. Code &amp; models at\nhttps://github.com/megvii-research/RepLKNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yizhuang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Minimal Sufficient Representation in Contrastive Learning. (arXiv:2203.07004v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07004","description":"<p>Contrastive learning between different views of the data achieves outstanding\nsuccess in the field of self-supervised representation learning and the learned\nrepresentations are useful in broad downstream tasks. Since all supervision\ninformation for one view comes from the other view, contrastive learning\napproximately obtains the minimal sufficient representation which contains the\nshared information and eliminates the non-shared information between views.\nConsidering the diversity of the downstream tasks, it cannot be guaranteed that\nall task-relevant information is shared between views. Therefore, we assume the\nnon-shared task-relevant information cannot be ignored and theoretically prove\nthat the minimal sufficient representation in contrastive learning is not\nsufficient for the downstream tasks, which causes performance degradation. This\nreveals a new problem that the contrastive learning models have the risk of\nover-fitting to the shared information between views. To alleviate this\nproblem, we propose to increase the mutual information between the\nrepresentation and input as regularization to approximately introduce more\ntask-relevant information, since we cannot utilize any downstream task\ninformation during training. Extensive experiments verify the rationality of\nour analysis and the effectiveness of our method. It significantly improves the\nperformance of several classic contrastive learning models in downstream tasks.\nOur code is available at https://github.com/Haoqing-Wang/InfoCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi-Hong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation. (arXiv:2203.09811v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09811","description":"<p>Scene Graph Generation, which generally follows a regular encoder-decoder\npipeline, aims to first encode the visual contents within the given image and\nthen parse them into a compact summary graph. Existing SGG approaches generally\nnot only neglect the insufficient modality fusion between vision and language,\nbut also fail to provide informative predicates due to the biased relationship\npredictions, leading SGG far from practical. Towards this end, in this paper,\nwe first present a novel Stacked Hybrid-Attention network, which facilitates\nthe intra-modal refinement as well as the inter-modal interaction, to serve as\nthe encoder. We then devise an innovative Group Collaborative Learning strategy\nto optimize the decoder. Particularly, based upon the observation that the\nrecognition capability of one classifier is limited towards an extremely\nunbalanced dataset, we first deploy a group of classifiers that are expert in\ndistinguishing different subsets of classes, and then cooperatively optimize\nthem from two aspects to promote the unbiased SGG. Experiments conducted on VG\nand GQA datasets demonstrate that, we not only establish a new state-of-the-art\nin the unbiased metric, but also nearly double the performance compared with\ntwo baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xingning Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_T/0/1/0/all/0/1\">Tian Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuemeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianlong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Morphological Feature Perturbations for Calibrated Semi-Supervised Segmentation. (arXiv:2203.10196v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10196","description":"<p>We propose MisMatch, a novel consistency-driven semi-supervised segmentation\nframework which produces predictions that are invariant to learnt feature\nperturbations. MisMatch consists of an encoder and a two-head decoders. One\ndecoder learns positive attention to the foreground regions of interest (RoI)\non unlabelled images thereby generating dilated features. The other decoder\nlearns negative attention to the foreground on the same unlabelled images\nthereby generating eroded features. We then apply a consistency regularisation\non the paired predictions. MisMatch outperforms state-of-the-art\nsemi-supervised methods on a CT-based pulmonary vessel segmentation task and a\nMRI-based brain tumour segmentation task. In addition, we show that the\neffectiveness of MisMatch comes from better model calibration than its\nsupervised learning counterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mou-Cheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu-Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blumberg_S/0/1/0/all/0/1\">Stefano B Blumberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_F/0/1/0/all/0/1\">Frederick J Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+deGroot_M/0/1/0/all/0/1\">Marius deGroot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C. Alexander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oxtoby_N/0/1/0/all/0/1\">Neil P. Oxtoby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_J/0/1/0/all/0/1\">Joseph Jacob</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving anatomical plausibility in medical image segmentation via hybrid graph neural networks: applications to chest x-ray analysis. (arXiv:2203.10977v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.10977","description":"<p>Anatomical segmentation is a fundamental task in medical image computing,\ngenerally tackled with fully convolutional neural networks which produce dense\nsegmentation masks. These models are often trained with loss functions such as\ncross-entropy or Dice, which assume pixels to be independent of each other,\nthus ignoring topological errors and anatomical inconsistencies. We address\nthis limitation by moving from pixel-level to graph representations, which\nallow to naturally incorporate anatomical constraints by construction. To this\nend, we introduce HybridGNet, an encoder-decoder neural architecture that\nleverages standard convolutions for image feature encoding and graph\nconvolutional neural networks (GCNNs) to decode plausible representations of\nanatomical structures. We also propose a novel image-to-graph skip connection\nlayer which allows localized features to flow from standard convolutional\nblocks to GCNN blocks, and show that it improves segmentation accuracy. The\nproposed architecture is extensively evaluated in a variety of domain shift and\nimage occlusion scenarios, and audited considering different types of\ndemographic domain shift. Our comprehensive experimental setup compares\nHybridGNet with other landmark and pixel-based models for anatomical\nsegmentation in chest x-ray images, and shows that it produces anatomically\nplausible results in challenging scenarios where other models tend to fail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gaggion_N/0/1/0/all/0/1\">Nicol&#xe1;s Gaggion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansilla_L/0/1/0/all/0/1\">Lucas Mansilla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mosquera_C/0/1/0/all/0/1\">Candelaria Mosquera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milone_D/0/1/0/all/0/1\">Diego H. Milone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1\">Enzo Ferrante</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.11692","description":"<p>This manuscript describes the panoptic segmentation method we devised for our\nsubmission to the CONIC challenge at ISBI 2022. Key features of our method are\na weighted loss that we specifically engineered for semantic segmentation of\nhighly imbalanced cell types, and an existing state-of-the art nuclei instance\nsegmentation model, which we combine in a Hovernet-like architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rumberger_J/0/1/0/all/0/1\">Josef Lorenz Rumberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumann_E/0/1/0/all/0/1\">Elias Baumann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hirsch_P/0/1/0/all/0/1\">Peter Hirsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Janowczyk_A/0/1/0/all/0/1\">Andrew Janowczyk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zlobec_I/0/1/0/all/0/1\">Inti Zlobec</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainmueller_D/0/1/0/all/0/1\">Dagmar Kainmueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.11933","description":"<p>Vision-language models can encode societal biases and stereotypes, but there\nare challenges to measuring and mitigating these harms. Prior proposed bias\nmeasurements lack robustness and feature degradation occurs when mitigating\nbias without access to pretraining data. We address both of these challenges in\nthis paper: First, we evaluate different bias measures and propose the use of\nretrieval metrics to image-text representations via a bias measuring framework.\nSecond, we investigate debiasing methods and show that optimizing for\nadversarial loss via learnable token embeddings minimizes various bias measures\nwithout substantially degrading feature representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berg_H/0/1/0/all/0/1\">Hugo Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1\">Siobhan Mackenzie Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1\">Yash Bhalgat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wonsuk Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bain_M/0/1/0/all/0/1\">Max Bain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Repairing Group-Level Errors for DNNs Using Weighted Regularization. (arXiv:2203.13612v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.13612","description":"<p>Deep Neural Networks (DNNs) have been widely used in software making\ndecisions impacting people's lives. However, they have been found to exhibit\nsevere erroneous behaviors that may lead to unfortunate outcomes. Previous work\nshows that such misbehaviors often occur due to class property violations\nrather than errors on a single image. Although methods for detecting such\nerrors have been proposed, fixing them has not been studied so far. Here, we\npropose a generic method called Weighted Regularization (WR) consisting of five\nconcrete methods targeting the error-producing classes to fix the DNNs. In\nparticular, it can repair confusion error and bias error of DNN models for both\nsingle-label and multi-label image classifications. A confusion error happens\nwhen a given DNN model tends to confuse between two classes. Each method in WR\nassigns more weights at a stage of DNN retraining or inference to mitigate the\nconfusion between target pair. A bias error can be fixed similarly. We evaluate\nand compare the proposed methods along with baselines on six widely-used\ndatasets and architecture combinations. The results suggest that WR methods\nhave different trade-offs but under each setting at least one WR method can\ngreatly reduce confusion/bias errors at a very limited cost of the overall\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Ziyuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuchi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sweeney_C/0/1/0/all/0/1\">Conor J.Sweeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.13883","description":"<p>As social media platforms are evolving from text-based forums into\nmulti-modal environments, the nature of misinformation in social media is also\nchanging accordingly. Taking advantage of the fact that visual modalities such\nas images and videos are more favorable and attractive to the users, and\ntextual contents are sometimes skimmed carelessly, misinformation spreaders\nhave recently targeted contextual correlations between modalities e.g., text\nand image. Thus, many research efforts have been put into development of\nautomatic techniques for detecting possible cross-modal discordances in\nweb-based media. In this work, we aim to analyze, categorize and identify\nexisting approaches in addition to challenges and shortcomings they face in\norder to unearth new opportunities in furthering the research in the field of\nmulti-modal misinformation detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1\">Sara Abdali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning with Position-Aware Neurons. (arXiv:2203.14666v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14666","description":"<p>Federated Learning (FL) fuses collaborative models from local nodes without\ncentralizing users' data. The permutation invariance property of neural\nnetworks and the non-i.i.d. data across clients make the locally updated\nparameters imprecisely aligned, disabling the coordinate-based parameter\naveraging. Traditional neurons do not explicitly consider position information.\nHence, we propose Position-Aware Neurons (PANs) as an alternative, fusing\nposition-related values (i.e., position encodings) into neuron outputs. PANs\ncouple themselves to their positions and minimize the possibility of\ndislocation, even updating on heterogeneous data. We turn on/off PANs to\ndisable/enable the permutation invariance property of neural networks. PANs are\ntightly coupled with positions when applied to FL, making parameters across\nclients pre-aligned and facilitating coordinate-based parameter averaging. PANs\nare algorithm-agnostic and could universally improve existing FL algorithms.\nFurthermore, \"FL with PANs\" is simple to implement and computationally\nfriendly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin-Chun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi-Chu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shaoming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinchuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition. (arXiv:2203.14779v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14779","description":"<p>Multimodal emotion recognition has recently gained much attention since it\ncan leverage diverse and complementary relationships over multiple modalities\n(e.g., audio, visual, biosignals, etc.), and can provide some robustness to\nnoisy modalities. Most state-of-the-art methods for audio-visual (A-V) fusion\nrely on recurrent networks or conventional attention mechanisms that do not\neffectively leverage the complementary nature of A-V modalities. In this paper,\nwe focus on dimensional emotion recognition based on the fusion of facial and\nvocal modalities extracted from videos. Specifically, we propose a joint\ncross-attention model that relies on the complementary relationships to extract\nthe salient features across A-V modalities, allowing for accurate prediction of\ncontinuous values of valence and arousal. The proposed fusion model efficiently\nleverages the inter-modal relationships, while reducing the heterogeneity\nbetween the features. In particular, it computes the cross-attention weights\nbased on correlation between the combined feature representation and individual\nmodalities. By deploying the combined A-V feature representation into the\ncross-attention module, the performance of our fusion module improves\nsignificantly over the vanilla cross-attention module. Experimental results on\nvalidation-set videos from the AffWild2 dataset indicate that our proposed A-V\nfusion model provides a cost-effective solution that can outperform\nstate-of-the-art approaches. The code is available on GitHub:\nhttps://github.com/praveena2j/JointCrossAttentional-AV-Fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajasekar_G/0/1/0/all/0/1\">Gnana Praveen Rajasekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_W/0/1/0/all/0/1\">Wheidima Carneiro de Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullah_N/0/1/0/all/0/1\">Nasib Ullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aslam_H/0/1/0/all/0/1\">Haseeb Aslam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeeshan_O/0/1/0/all/0/1\">Osama Zeeshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denorme_T/0/1/0/all/0/1\">Th&#xe9;o Denorme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koerich_A/0/1/0/all/0/1\">Alessandro Koerich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardinal_P/0/1/0/all/0/1\">Patrick Cardinal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Semantic Segmentation: A Prototype View. (arXiv:2203.15102v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15102","description":"<p>Prevalent semantic segmentation solutions, despite their different network\ndesigns (FCN based or attention based) and mask decoding strategies (parametric\nsoftmax based or pixel-query based), can be placed in one category, by\nconsidering the softmax weights or query vectors as learnable class prototypes.\nIn light of this prototype view, this study uncovers several limitations of\nsuch parametric segmentation regime, and proposes a nonparametric alternative\nbased on non-learnable prototypes. Instead of prior methods learning a single\nweight/query vector for each class in a fully parametric manner, our model\nrepresents each class as a set of non-learnable prototypes, relying solely on\nthe mean features of several training pixels within that class. The dense\nprediction is thus achieved by nonparametric nearest prototype retrieving. This\nallows our model to directly shape the pixel embedding space, by optimizing the\narrangement between embedded pixels and anchored prototypes. It is able to\nhandle arbitrary number of classes with a constant amount of learnable\nparameters. We empirically show that, with FCN based and attention based\nsegmentation models (i.e., HRNet, Swin, SegFormer) and backbones (i.e., ResNet,\nHRNet, Swin, MiT), our nonparametric framework yields compelling results over\nseveral datasets (i.e., ADE20K, Cityscapes, COCO-Stuff), and performs well in\nthe large-vocabulary situation. We expect this work will provoke a rethink of\nthe current de facto semantic segmentation model design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Camera-Conditioned Stable Feature Generation for Isolated Camera Supervised Person Re-IDentification. (arXiv:2203.15210v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15210","description":"<p>To learn camera-view invariant features for person Re-IDentification (Re-ID),\nthe cross-camera image pairs of each person play an important role. However,\nsuch cross-view training samples could be unavailable under the ISolated Camera\nSupervised (ISCS) setting, e.g., a surveillance system deployed across distant\nscenes. To handle this challenging problem, a new pipeline is introduced by\nsynthesizing the cross-camera samples in the feature space for model training.\nSpecifically, the feature encoder and generator are end-to-end optimized under\na novel method, Camera-Conditioned Stable Feature Generation (CCSFG). Its joint\nlearning procedure raises concern on the stability of generative model\ntraining. Therefore, a new feature generator, $\\sigma$-Regularized Conditional\nVariational Autoencoder ($\\sigma$-Reg.~CVAE), is proposed with theoretical and\nexperimental analysis on its robustness. Extensive experiments on two ISCS\nperson Re-ID datasets demonstrate the superiority of our CCSFG to the\ncompetitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wenhang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Ancong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaobin Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Feature Alignment and Mutual Information Maximization for Video-Based Human Pose Estimation. (arXiv:2203.15227v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15227","description":"<p>Multi-frame human pose estimation has long been a compelling and fundamental\nproblem in computer vision. This task is challenging due to fast motion and\npose occlusion that frequently occur in videos. State-of-the-art methods strive\nto incorporate additional visual evidences from neighboring frames (supporting\nframes) to facilitate the pose estimation of the current frame (key frame). One\naspect that has been obviated so far, is the fact that current methods directly\naggregate unaligned contexts across frames. The spatial-misalignment between\npose features of the current frame and neighboring frames might lead to\nunsatisfactory results. More importantly, existing approaches build upon the\nstraightforward pose estimation loss, which unfortunately cannot constrain the\nnetwork to fully leverage useful information from neighboring frames. To tackle\nthese problems, we present a novel hierarchical alignment framework, which\nleverages coarse-to-fine deformations to progressively update a neighboring\nframe to align with the current frame at the feature level. We further propose\nto explicitly supervise the knowledge extraction from neighboring frames,\nguaranteeing that useful complementary cues are extracted. To achieve this\ngoal, we theoretically analyzed the mutual information between the frames and\narrived at a loss that maximizes the task-relevant mutual information. These\nallow us to rank No.1 in the Multi-frame Person Pose Estimation Challenge on\nbenchmark dataset PoseTrack2017, and obtain state-of-the-art performance on\nbenchmarks Sub-JHMDB and Pose-Track2018. Our code is released at\nhttps://github. com/Pose-Group/FAMI-Pose, hoping that it will be useful to the\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Runyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yixing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SepViT: Separable Vision Transformer. (arXiv:2203.15380v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15380","description":"<p>Vision Transformers have witnessed prevailing success in a series of vision\ntasks. However, they often require enormous amount of computations to achieve\nhigh performance, which is burdensome to deploy on resource-constrained\ndevices. To address these issues, we draw lessons from depthwise separable\nconvolution and imitate its ideology to design the Separable Vision\nTransformer, abbreviated as SepViT. SepViT helps to carry out the information\ninteraction within and among the windows via a depthwise separable\nself-attention. The novel window token embedding and grouped self-attention are\nemployed to model the attention relationship among windows with negligible\ncomputational cost and capture a long-range visual dependencies of multiple\nwindows, respectively. Extensive experiments on various benchmark tasks\ndemonstrate SepViT can achieve state-of-the-art results in terms of trade-off\nbetween accuracy and latency. Among them, SepViT achieves 84.0% top-1 accuracy\non ImageNet-1K classification while decreasing the latency by 40%, compared to\nthe ones with similar accuracy (e.g., CSWin, PVTV2). As for the downstream\nvision tasks, SepViT with fewer FLOPs can achieve 50.4% mIoU on ADE20K semantic\nsegmentation task, 47.5 AP on the RetinaNet-based COCO detection task, 48.7 box\nAP and 43.9 mask AP on Mask R-CNN-based COCO detection and segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Min Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Shiping Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection. (arXiv:2203.15793v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15793","description":"<p>Unsupervised Domain Adaptation (UDA) is an effective approach to tackle the\nissue of domain shift. Specifically, UDA methods try to align the source and\ntarget representations to improve the generalization on the target domain.\nFurther, UDA methods work under the assumption that the source data is\naccessible during the adaptation process. However, in real-world scenarios, the\nlabelled source data is often restricted due to privacy regulations, data\ntransmission constraints, or proprietary data concerns. The Source-Free Domain\nAdaptation (SFDA) setting aims to alleviate these concerns by adapting a\nsource-trained model for the target domain without requiring access to the\nsource data. In this paper, we explore the SFDA setting for the task of\nadaptive object detection. To this end, we propose a novel training strategy\nfor adapting a source-trained object detector to the target domain without\nsource data. More precisely, we design a novel contrastive loss to enhance the\ntarget representations by exploiting the objects relations for a given target\ndomain input. These object instance relations are modelled using an Instance\nRelation Graph (IRG) network, which are then used to guide the contrastive\nrepresentation learning. In addition, we utilize a student-teacher based\nknowledge distillation strategy to avoid overfitting to the noisy pseudo-labels\ngenerated by the source-trained model. Extensive experiments on multiple object\ndetection benchmark datasets show that the proposed approach is able to\nefficiently adapt source-trained object detectors to the target domain,\noutperforming previous state-of-the-art domain adaptive detection methods. Code\nis available at https://github.com/Vibashan/irg-sfda.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1\">Vibashan VS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1\">Poojan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PP-YOLOE: An evolved version of YOLO. (arXiv:2203.16250v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16250","description":"<p>In this report, we present PP-YOLOE, an industrial state-of-the-art object\ndetector with high performance and friendly deployment. We optimize on the\nbasis of the previous PP-YOLOv2, using anchor-free paradigm, more powerful\nbackbone and neck equipped with CSPRepResStage, ET-head and dynamic label\nassignment algorithm TAL. We provide s/m/l/x models for different practice\nscenarios. As a result, PP-YOLOE-l achieves 51.4 mAP on COCO test-dev and 78.1\nFPS on Tesla V100, yielding a remarkable improvement of (+1.9 AP, +13.35% speed\nup) and (+1.3 AP, +24.96% speed up), compared to the previous state-of-the-art\nindustrial models PP-YOLOv2 and YOLOX respectively. Further, PP-YOLOE inference\nspeed achieves 149.2 FPS with TensorRT and FP16-precision. We also conduct\nextensive experiments to verify the effectiveness of our designs. Source code\nand pre-trained models are available at\nhttps://github.com/PaddlePaddle/PaddleDetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shangliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1\">Wenyu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1\">Qinyao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Cheng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kaipeng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_Q/0/1/0/all/0/1\">Qingqing Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shengyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_B/0/1/0/all/0/1\">Baohua Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Improved Lightweight YOLOv5 Model Based on Attention Mechanism for Face Mask Detection. (arXiv:2203.16506v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16506","description":"<p>Coronavirus 2019 has brought severe challenges to social stability and public\nhealth worldwide. One effective way of curbing the epidemic is to require\npeople to wear masks in public places and monitor mask-wearing states by\nutilizing suitable automatic detectors. However, existing deep learning based\nmodels struggle to simultaneously achieve the requirements of both high\nprecision and real-time performance. To solve this problem, we propose an\nimproved lightweight face mask detector based on YOLOv5, which can achieve an\nexcellent balance of precision and speed. Firstly, a novel backbone\nShuffleCANet that combines ShuffleNetV2 network with Coordinate Attention\nmechanism is proposed as the backbone. Afterwards, an efficient path aggression\nnetwork BiFPN is applied as the feature fusion neck. Furthermore, the\nlocalization loss is replaced with alpha-CIoU in model training phase to obtain\nhigher-quality anchors. Some valuable strategies such as data augmentation,\nadaptive image scaling, and anchor cluster operation are also utilized.\nExperimental results on AIZOO face mask dataset show the superiority of the\nproposed model. Compared with the original YOLOv5, the proposed model increases\nthe inference speed by 28.3% while still improving the precision by 0.58%. It\nachieves the best mean average precision of 95.2% compared with other seven\nexisting models, which is 4.4% higher than the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Sheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ball 3D localization from a single calibrated image. (arXiv:2204.00003v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00003","description":"<p>Ball 3D localization in team sports has various applications including\nautomatic offside detection in soccer, or shot release localization in\nbasketball. Today, this task is either resolved by using expensive multi-views\nsetups, or by restricting the analysis to ballistic trajectories. In this work,\nwe propose to address the task on a single image from a calibrated monocular\ncamera by estimating ball diameter in pixels and use the knowledge of real ball\ndiameter in meters. This approach is suitable for any game situation where the\nball is (even partly) visible. To achieve this, we use a small neural network\ntrained on image patches around candidates generated by a conventional ball\ndetector. Besides predicting ball diameter, our network outputs the confidence\nof having a ball in the image patch. Validations on 3 basketball datasets\nreveals that our model gives remarkable predictions on ball 3D localization. In\naddition, through its confidence output, our model improves the detection rate\nby filtering the candidates produced by the detector. The contributions of this\nwork are (i) the first model to address 3D ball localization on a single image,\n(ii) an effective method for ball 3D annotation from single calibrated images,\n(iii) a high quality 3D ball evaluation dataset annotated from a single\nviewpoint. In addition, the code to reproduce this research is be made freely\navailable at https://github.com/gabriel-vanzandycke/deepsport.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zandycke_G/0/1/0/all/0/1\">Gabriel Van Zandycke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vleeschouwer_C/0/1/0/all/0/1\">Christophe De Vleeschouwer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAT-Det: Contrastively Augmented Transformer for Multi-modal 3D Object Detection. (arXiv:2204.00325v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00325","description":"<p>In autonomous driving, LiDAR point-clouds and RGB images are two major data\nmodalities with complementary cues for 3D object detection. However, it is\nquite difficult to sufficiently use them, due to large inter-modal\ndiscrepancies. To address this issue, we propose a novel framework, namely\nContrastively Augmented Transformer for multi-modal 3D object Detection\n(CAT-Det). Specifically, CAT-Det adopts a two-stream structure consisting of a\nPointformer (PT) branch, an Imageformer (IT) branch along with a Cross-Modal\nTransformer (CMT) module. PT, IT and CMT jointly encode intra-modal and\ninter-modal long-range contexts for representing an object, thus fully\nexploring multi-modal information for detection. Furthermore, we propose an\neffective One-way Multi-modal Data Augmentation (OMDA) approach via\nhierarchical contrastive learning at both the point and object levels,\nsignificantly improving the accuracy only by augmenting point-clouds, which is\nfree from complex generation of paired samples of the two modalities. Extensive\nexperiments on the KITTI benchmark show that CAT-Det achieves a new\nstate-of-the-art, highlighting its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DFNet: Enhance Absolute Pose Regression with Direct Feature Matching. (arXiv:2204.00559v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00559","description":"<p>We introduce a camera relocalization pipeline that combines absolute pose\nregression (APR) and direct feature matching. Existing photometric-based\nmethods have trouble on scenes with large photometric distortions, e.g. outdoor\nenvironments. By incorporating an exposure-adaptive novel view synthesis, our\nmethods can successfully address the challenges. Moreover, by introducing\ndomain-invariant feature matching, our solution can improve pose regression\naccuracy while using semi-supervised learning on unlabeled data. In particular,\nthe pipeline consists of two components, Novel View Synthesizer and FeatureNet\n(DFNet). The former synthesizes novel views compensating for changes in\nexposure and the latter regresses camera poses and extracts robust features\nthat bridge the domain gap between real images and synthetic ones. We show that\ndomain invariant feature matching effectively enhances camera pose estimation\nboth in indoor and outdoor scenes. Hence, our method achieves a\nstate-of-the-art accuracy by outperforming existing single-image APR methods by\nas much as 56%, comparable to 3D structure-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Adrian Prisacariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}