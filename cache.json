{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-19T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis. (arXiv:2207.07706v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07706","description":"<p>Representational Similarity Analysis is a method from cognitive neuroscience,\nwhich helps in comparing representations from two different sources of data. In\nthis paper, we propose using Representational Similarity Analysis to probe the\nsemantic grounding in language models of code. We probe representations from\nthe CodeBERT model for semantic grounding by using the data from the IBM\nCodeNet dataset. Through our experiments, we show that current pre-training\nmethods do not induce semantic grounding in language models of code, and\ninstead focus on optimizing form-based patterns. We also show that even a\nlittle amount of fine-tuning on semantically relevant tasks increases the\nsemantic grounding in CodeBERT significantly. Our ablations with the input\nmodality to the CodeBERT model show that using bimodal inputs (code and natural\nlanguage) over unimodal inputs (only code) gives better semantic grounding and\nsample efficiency during semantic fine-tuning. Finally, our experiments with\nsemantic perturbations in code reveal that CodeBERT is able to robustly\ndistinguish between semantically correct and incorrect code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naik_S/0/1/0/all/0/1\">Shounak Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Swati Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baths_V/0/1/0/all/0/1\">Veeky Baths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sotto Voce: Federated Speech Recognition with Differential Privacy Guarantees. (arXiv:2207.07816v1 [cs.CR])","link":"http://arxiv.org/abs/2207.07816","description":"<p>Speech data is expensive to collect, and incredibly sensitive to its sources.\nIt is often the case that organizations independently collect small datasets\nfor their own use, but often these are not performant for the demands of\nmachine learning. Organizations could pool these datasets together and jointly\nbuild a strong ASR system; sharing data in the clear, however, comes with\ntremendous risk, in terms of intellectual property loss as well as loss of\nprivacy of the individuals who exist in the dataset. In this paper, we offer a\npotential solution for learning an ML model across multiple organizations where\nwe can provide mathematical guarantees limiting privacy loss. We use a\nFederated Learning approach built on a strong foundation of Differential\nPrivacy techniques. We apply these to a senone classification prototype and\ndemonstrate that the model improves with the addition of private data while\nstill respecting privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shoemate_M/0/1/0/all/0/1\">Michael Shoemate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jett_K/0/1/0/all/0/1\">Kevin Jett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowan_E/0/1/0/all/0/1\">Ethan Cowan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colbath_S/0/1/0/all/0/1\">Sean Colbath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honaker_J/0/1/0/all/0/1\">James Honaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthukumar_P/0/1/0/all/0/1\">Prasanna Muthukumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model. (arXiv:2207.07934v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07934","description":"<p>Text response generation for multimodal task-oriented dialog systems, which\naims to generate the proper text response given the multimodal context, is an\nessential yet challenging task. Although existing efforts have achieved\ncompelling success, they still suffer from two pivotal limitations: 1) overlook\nthe benefit of generative pre-training, and 2) ignore the textual context\nrelated knowledge. To address these limitations, we propose a novel dual\nknowledge-enhanced generative pretrained language model for multimodal\ntask-oriented dialog systems (DKMD), consisting of three key components: dual\nknowledge selection, dual knowledge-enhanced context learning, and\nknowledge-enhanced response generation. To be specific, the dual knowledge\nselection component aims to select the related knowledge according to both\ntextual and visual modalities of the given context. Thereafter, the dual\nknowledge-enhanced context learning component targets seamlessly integrating\nthe selected knowledge into the multimodal context learning from both global\nand local perspectives, where the cross-modal semantic relation is also\nexplored. Moreover, the knowledge-enhanced response generation component\ncomprises a revised BART decoder, where an additional dot-product\nknowledge-decoder attention sub-layer is introduced for explicitly utilizing\nthe knowledge to advance the text response generation. Extensive experiments on\na public dataset verify the superiority of the proposed DKMD over\nstate-of-the-art competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuemeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liqiang Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Linmei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v1 [cs.CL])","link":"http://arxiv.org/abs/2207.08012","description":"<p>Human beings use compositionality to generalise from past experiences to\nactual or fictive, novel experiences. To do so, we separate our experiences\ninto fundamental atomic components. These atomic components can then be\nrecombined in novel ways to support our ability to imagine and engage with\nnovel experiences. We frame this as the ability to learn to generalise\ncompositionally. And, we will refer to behaviours making use of this ability as\ncompositional learning behaviours (CLBs).\n</p>\n<p>A central problem to learning CLBs is the resolution of a binding problem\n(BP) (by learning to, firstly, segregate the supportive stimulus components\nfrom the observation of multiple stimuli, and then, combine them in a single\nepisodic experience). While it is another feat of intelligence that human\nbeings perform with ease, it is not the case for state-of-the-art artificial\nagents.\n</p>\n<p>Thus, in order to build artificial agents able to collaborate with human\nbeings, we propose to develop a novel benchmark to investigate agents'\nabilities to exhibit CLBs by solving a domain-agnostic version of the BP. We\ntake inspiration from the language emergence and grounding framework of\nreferential games and propose a meta-learning extension of referential games,\nentitled Meta-Referential Games, and use this framework to build our benchmark,\nthat we name Symbolic Behaviour Benchmark (S2B).\n</p>\n<p>While it has the potential to test for more symbolic behaviours, rather than\nsolely CLBs, in the present paper, though, we solely focus on the single-agent\nlanguage grounding task that tests for CLBs. We provide baseline results for\nit, using state-of-the-art RL agents, and show that our proposed benchmark is a\ncompelling challenge that we hope will spur the research community towards\ndeveloping more capable artificial agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Denamganai_K/0/1/0/all/0/1\">Kevin Denamgana&#xef;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Missaoui_S/0/1/0/all/0/1\">Sondess Missaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">James Alfred Walker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Explainability in NLP: Analyzing and Calculating Word Saliency through Word Properties. (arXiv:2207.08083v1 [cs.CL])","link":"http://arxiv.org/abs/2207.08083","description":"<p>The wide use of black-box models in natural language processing brings great\nchallenges to the understanding of the decision basis, the trustworthiness of\nthe prediction results, and the improvement of the model performance. The words\nin text samples have properties that reflect their semantics and contextual\ninformation, such as the part of speech, the position, etc. These properties\nmay have certain relationships with the word saliency, which is of great help\nfor studying the explainability of the model predictions. In this paper, we\nexplore the relationships between the word saliency and the word properties.\nAccording to the analysis results, we further establish a mapping model,\nSeq2Saliency, from the words in a text sample and their properties to the\nsaliency values based on the idea of sequence tagging. In addition, we\nestablish a new dataset called PrSalM, which contains each word in the text\nsamples, the word properties, and the word saliency values. The experimental\nevaluations are conducted to analyze the saliency of words with different\nproperties. The effectiveness of the Seq2Saliency model is verified.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jialiang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Zhitao Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Longfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Context Pattern Generation for Entity Set Expansion. (arXiv:2207.08087v1 [cs.CL])","link":"http://arxiv.org/abs/2207.08087","description":"<p>Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various NLP and IR\ndownstream applications have benefited from ESE due to its ability to discover\nknowledge. Although existing bootstrapping methods have achieved great\nprogress, most of them still rely on manually pre-defined context patterns. A\nnon-negligible shortcoming of the pre-defined context patterns is that they\ncannot be flexibly generalized to all kinds of semantic classes, and we call\nthis phenomenon as \"semantic sensitivity\". To address this problem, we devise a\ncontext pattern generation module that utilizes autoregressive language models\n(e.g., GPT-2) to automatically generate high-quality context patterns for\nentities. In addition, we propose the GAPA, a novel ESE framework that\nleverages the aforementioned GenerAted PAtterns to expand target entities.\nExtensive experiments and detailed analyses on three widely used datasets\ndemonstrate the effectiveness of our method. All the codes of our experiments\nwill be available for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shulin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect-specific Context Modeling for Aspect-based Sentiment Analysis. (arXiv:2207.08099v1 [cs.CL])","link":"http://arxiv.org/abs/2207.08099","description":"<p>Aspect-based sentiment analysis (ABSA) aims at predicting sentiment polarity\n(SC) or extracting opinion span (OE) expressed towards a given aspect. Previous\nwork in ABSA mostly relies on rather complicated aspect-specific feature\ninduction. Recently, pretrained language models (PLMs), e.g., BERT, have been\nused as context modeling layers to simplify the feature induction structures\nand achieve state-of-the-art performance. However, such PLM-based context\nmodeling can be not that aspect-specific. Therefore, a key question is left\nunder-explored: how the aspect-specific context can be better modeled through\nPLMs? To answer the question, we attempt to enhance aspect-specific context\nmodeling with PLM in a non-intrusive manner. We propose three aspect-specific\ninput transformations, namely aspect companion, aspect prompt, and aspect\nmarker. Informed by these transformations, non-intrusive aspect-specific PLMs\ncan be achieved to promote the PLM to pay more attention to the aspect-specific\ncontext in a sentence. Additionally, we craft an adversarial benchmark for ABSA\n(advABSA) to see how aspect-specific modeling can impact model robustness.\nExtensive experimental results on standard and adversarial benchmarks for SC\nand OE demonstrate the effectiveness and robustness of the proposed method,\nyielding new state-of-the-art performance on OE and competitive performance on\nSC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multibias-mitigated and Sentiment Knowledge Enriched Transformer for Debiasing in Multimodal Conversational Emotion Recognition. (arXiv:2207.08104v1 [cs.CL])","link":"http://arxiv.org/abs/2207.08104","description":"<p>Multimodal emotion recognition in conversations (mERC) is an active research\ntopic in natural language processing (NLP), which aims to predict human's\nemotional states in communications of multiple modalities, e,g., natural\nlanguage and facial gestures. Innumerable implicit prejudices and\npreconceptions fill human language and conversations, leading to the question\nof whether the current data-driven mERC approaches produce a biased error. For\nexample, such approaches may offer higher emotional scores on the utterances by\nfemales than males. In addition, the existing debias models mainly focus on\ngender or race, where multibias mitigation is still an unexplored task in mERC.\nIn this work, we take the first step to solve these issues by proposing a\nseries of approaches to mitigate five typical kinds of bias in textual\nutterances (i.e., gender, age, race, religion and LGBTQ+) and visual\nrepresentations (i.e, gender and age), followed by a Multibias-Mitigated and\nsentiment Knowledge Enriched bi-modal Transformer (MMKET). Comprehensive\nexperimental results show the effectiveness of the proposed model and prove\nthat the debias operation has a great impact on the classification performance\nfor mERC. We hope our study will benefit the development of bias mitigation in\nmERC and related emotion studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinglin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yazhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"United States Politicians' Tone Became More Negative with 2016 Primary Campaigns. (arXiv:2207.08112v1 [cs.CL])","link":"http://arxiv.org/abs/2207.08112","description":"<p>There is a widespread belief that the tone of US political language has\nbecome more negative recently, in particular when Donald Trump entered\npolitics. At the same time, there is disagreement as to whether Trump changed\nor merely continued previous trends. To date, data-driven evidence regarding\nthese questions is scarce, partly due to the difficulty of obtaining a\ncomprehensive, longitudinal record of politicians' utterances. Here we apply\npsycholinguistic tools to a novel, comprehensive corpus of 24 million quotes\nfrom online news attributed to 18,627 US politicians in order to analyze how\nthe tone of US politicians' language evolved between 2008 and 2020. We show\nthat, whereas the frequency of negative emotion words had decreased\ncontinuously during Obama's tenure, it suddenly and lastingly increased with\nthe 2016 primary campaigns, by 1.6 pre-campaign standard deviations, or 8% of\nthe pre-campaign mean, in a pattern that emerges across parties. The effect\nsize drops by 40% when omitting Trump's quotes, and by 50% when averaging over\nspeakers rather than quotes, implying that prominent speakers, and Trump in\nparticular, have disproportionately, though not exclusively, contributed to the\nrise in negative language. This work provides the first large-scale data-driven\nevidence of a drastic shift toward a more negative political tone following\nTrump's campaign start as a catalyst, with important implications for the\ndebate about the state of US politics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulz_J/0/1/0/all/0/1\">Jonathan K&#xfc;lz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spitz_A/0/1/0/all/0/1\">Andreas Spitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Akel_A/0/1/0/all/0/1\">Ahmad Abu-Akel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Span-based Joint Entity and Relation Extraction via Squence Tagging Mechanism. (arXiv:2105.10080v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.10080","description":"<p>Span-based joint extraction simultaneously conducts named entity recognition\n(NER) and relation extraction (RE) in text span form. Recent studies have shown\nthat token labels can convey crucial task-specific information and enrich token\nsemantics. However, as far as we know, due to completely abstain from sequence\ntagging mechanism, all prior span-based work fails to use token label\nin-formation. To solve this problem, we pro-pose Sequence Tagging enhanced\nSpan-based Network (STSN), a span-based joint extrac-tion network that is\nenhanced by token BIO label information derived from sequence tag-ging based\nNER. By stacking multiple atten-tion layers in depth, we design a deep neu-ral\narchitecture to build STSN, and each atten-tion layer consists of three basic\nattention units. The deep neural architecture first learns seman-tic\nrepresentations for token labels and span-based joint extraction, and then\nconstructs in-formation interactions between them, which also realizes\nbidirectional information interac-tions between span-based NER and RE.\nFur-thermore, we extend the BIO tagging scheme to make STSN can extract\noverlapping en-tity. Experiments on three benchmark datasets show that our\nmodel consistently outperforms previous optimal models by a large margin,\ncreating new state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1\">Bin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huijun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Differential Privacy for Language Modeling. (arXiv:2108.12944v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12944","description":"<p>With the increasing applications of language models, it has become crucial to\nprotect these models from leaking private information. Previous work has\nattempted to tackle this challenge by training RNN-based language models with\ndifferential privacy guarantees. However, applying classical differential\nprivacy to language models leads to poor model performance as the underlying\nprivacy notion is over-pessimistic and provides undifferentiated protection for\nall tokens in the data. Given that the private information in natural language\nis sparse (for example, the bulk of an email might not carry personally\nidentifiable information), we propose a new privacy notion, selective\ndifferential privacy, to provide rigorous privacy guarantees on the sensitive\nportion of the data to improve model utility. To realize such a new notion, we\ndevelop a corresponding privacy mechanism, Selective-DPSGD, for RNN-based\nlanguage models. Besides language modeling, we also apply the method to a more\nconcrete application--dialog systems. Experiments on both language modeling and\ndialog system building show that the proposed privacy-preserving mechanism\nachieves better utilities while remaining safe under various privacy attacks\ncompared to the baselines. The data and code are released at\nhttps://github.com/wyshi/lm_privacy to facilitate future research .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_A/0/1/0/all/0/1\">Aiqi Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Evan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation. (arXiv:2109.05729v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05729","description":"<p>In this paper, we take the advantage of previous pre-trained models (PTMs)\nand propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different\nfrom previous Chinese PTMs, CPT is designed to utilize the shared knowledge\nbetween natural language understanding (NLU) and natural language generation\n(NLG) to boost the performance. CPT consists of three parts: a shared encoder,\nan understanding decoder, and a generation decoder. Two specific decoders with\na shared encoder are pre-trained with masked language modeling (MLM) and\ndenoising auto-encoding (DAE) tasks, respectively. With the partially shared\narchitecture and multi-task pre-training, CPT can (1) learn specific knowledge\nof both NLU or NLG tasks with two decoders and (2) be fine-tuned flexibly that\nfully exploits the potential of the model. Moreover, the unbalanced Transformer\nsaves the computational and storage cost, which makes CPT competitive and\ngreatly accelerates the inference of text generation. Experimental results on a\nwide range of Chinese NLU and NLG tasks show the effectiveness of CPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhichao Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_L/0/1/0/all/0/1\">Li Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.05679","description":"<p>Differentially Private (DP) learning has seen limited success for building\nlarge deep learning models of text, and attempts at straightforwardly applying\nDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have\nresulted in large performance drops and high computational overhead. We show\nthat this performance drop can be mitigated with (1) the use of large\npretrained models; (2) hyperparameters that suit DP optimization; and (3)\nfine-tuning objectives aligned with the pretraining procedure. With these\nfactors set right, we obtain private NLP models that outperform\nstate-of-the-art private training approaches and strong non-private baselines\n-- by directly fine-tuning pretrained models with DP optimization on\nmoderately-sized corpora. To address the computational challenge of running\nDP-SGD with large Transformers, we propose a memory saving technique that\nallows clipping in DP-SGD to run without instantiating per-example gradients\nfor any layer in the model. The technique enables privately training\nTransformers with almost the same memory cost as non-private training at a\nmodest run-time overhead. Contrary to conventional wisdom that DP optimization\nfails at learning high-dimensional models (due to noise that scales with\ndimension) empirical results reveal that private learning with pretrained\nmodels tends to not suffer from dimension-dependent performance degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Amortized Noisy Channel Neural Machine Translation. (arXiv:2112.08670v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08670","description":"<p>Noisy channel models have been especially effective in neural machine\ntranslation (NMT). However, recent approaches like \"beam search and rerank\"\n(BSR) incur significant computation overhead during inference, making\nreal-world application infeasible. We aim to study if it is possible to build\nan amortized noisy channel NMT model such that when we do greedy decoding\nduring inference, the translation accuracy matches that of BSR in terms of\nreward (based on the source-to-target log probability and the target-to-source\nlog probability) and quality (based on BLEU and BLEURT). We attempt three\napproaches to train the new model: knowledge distillation, one-step-deviation\nimitation learning, and Q learning. The first approach obtains the noisy\nchannel signal from a pseudo-corpus, and the latter two approaches aim to\noptimize toward a noisy-channel MT reward directly. For all three approaches,\nthe generated translations fail to achieve rewards comparable to BSR, but the\ntranslation quality approximated by BLEU and BLEURT is similar to the quality\nof BSR-produced translations. Additionally, all three approaches speed up\ninference by 1-2 orders of magnitude.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Richard Yuanzhe Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Intensity and its Control for Emotional Voice Conversion. (arXiv:2201.03967v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2201.03967","description":"<p>Emotional voice conversion (EVC) seeks to convert the emotional state of an\nutterance while preserving the linguistic content and speaker identity. In EVC,\nemotions are usually treated as discrete categories overlooking the fact that\nspeech also conveys emotions with various intensity levels that the listener\ncan perceive. In this paper, we aim to explicitly characterize and control the\nintensity of emotion. We propose to disentangle the speaker style from\nlinguistic content and encode the speaker style into a style embedding in a\ncontinuous space that forms the prototype of emotion embedding. We further\nlearn the actual emotion encoder from an emotion-labelled database and study\nthe use of relative attributes to represent fine-grained emotion intensity. To\nensure emotional intelligibility, we incorporate emotion classification loss\nand emotion embedding similarity loss into the training of the EVC network. As\ndesired, the proposed network controls the fine-grained emotion intensity in\nthe output speech. Through both objective and subjective evaluations, we\nvalidate the effectiveness of the proposed network for emotional expressiveness\nand emotion intensity control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sisman_B/0/1/0/all/0/1\">Berrak Sisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_R/0/1/0/all/0/1\">Rajib Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages. (arXiv:2201.11732v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11732","description":"<p>Reliable evaluation benchmarks designed for replicability and\ncomprehensiveness have driven progress in machine learning. Due to the lack of\na multilingual benchmark, however, vision-and-language research has mostly\nfocused on English language tasks. To fill this gap, we introduce the\nImage-Grounded Language Understanding Evaluation benchmark. IGLUE brings\ntogether - by both aggregating pre-existing datasets and creating new ones -\nvisual question answering, cross-modal retrieval, grounded reasoning, and\ngrounded entailment tasks across 20 diverse languages. Our benchmark enables\nthe evaluation of multilingual multimodal models for transfer learning, not\nonly in a zero-shot setting, but also in newly defined few-shot learning\nsetups. Based on the evaluation of the available state-of-the-art models, we\nfind that translate-test transfer is superior to zero-shot transfer and that\nfew-shot learning is hard to harness for many tasks. Moreover, downstream\nperformance is partially explained by the amount of available unlabelled\ntextual data for pretraining, and only weakly by the typological distance of\ntarget-source languages. We hope to encourage future research efforts in this\narea by releasing the benchmark to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Vision-Language Pre-Trained Models. (arXiv:2202.10936v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10936","description":"<p>As transformer evolves, pre-trained models have advanced at a breakneck pace\nin recent years. They have dominated the mainstream techniques in natural\nlanguage processing (NLP) and computer vision (CV). How to adapt pre-training\nto the field of Vision-and-Language (V-L) learning and improve downstream task\nperformance becomes a focus of multimodal learning. In this paper, we review\nthe recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the\ncore content, we first briefly introduce several ways to encode raw images and\ntexts to single-modal embeddings before pre-training. Then, we dive into the\nmainstream architectures of VL-PTMs in modeling the interaction between text\nand image representations. We further present widely-used pre-training tasks,\nand then we introduce some common downstream tasks. We finally conclude this\npaper and present some promising research directions. Our survey aims to\nprovide researchers with synthesis and pointer to related research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PALI-NLP at SemEval-2022 Task 4: Discriminative Fine-tuning of Transformers for Patronizing and Condescending Language Detection. (arXiv:2203.04616v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.04616","description":"<p>Patronizing and condescending language (PCL) has a large harmful impact and\nis difficult to detect, both for human judges and existing NLP systems. At\nSemEval-2022 Task 4, we propose a novel Transformer-based model and its\nensembles to accurately understand such language context for PCL detection. To\nfacilitate comprehension of the subtle and subjective nature of PCL, two\nfine-tuning strategies are applied to capture discriminative features from\ndiverse linguistic behaviour and categorical distribution. The system achieves\nremarkable results on the official ranking, including 1st in Subtask 1 and 5th\nin Subtask 2. Extensive experiments on the task demonstrate the effectiveness\nof our system and its strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dou Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiyang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mengfei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Meizhi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lianxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1\">Yang Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaofeng Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compression of Generative Pre-trained Language Models via Quantization. (arXiv:2203.10705v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10705","description":"<p>The increasing size of generative Pre-trained Language Models (PLMs) has\ngreatly increased the demand for model compression. Despite various methods to\ncompress BERT or its variants, there are few attempts to compress generative\nPLMs, and the underlying difficulty remains unclear. In this paper, we compress\ngenerative PLMs by quantization. We find that previous quantization methods\nfail on generative tasks due to the \\textit{homogeneous word embeddings} caused\nby reduced capacity, and \\textit{varied distribution of weights}.\nCorrespondingly, we propose a token-level contrastive distillation to learn\ndistinguishable word embeddings, and a module-wise dynamic scaling to make\nquantizers adaptive to different modules. Empirical results on various tasks\nshow that our proposed method outperforms the state-of-the-art compression\nmethods on generative PLMs by a clear margin. With comparable performance with\nthe full-precision models, we achieve 14.4x and 13.4x compression rates on\nGPT-2 and BART, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chaofan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Ngai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks. (arXiv:2203.12257v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12257","description":"<p>Traditionally, a debate usually requires a manual preparation process,\nincluding reading plenty of articles, selecting the claims, identifying the\nstances of the claims, seeking the evidence for the claims, etc. As the AI\ndebate attracts more attention these years, it is worth exploring the methods\nto automate the tedious process involved in the debating system. In this work,\nwe introduce a comprehensive and large dataset named IAM, which can be applied\nto a series of argument mining tasks, including claim extraction, stance\nclassification, evidence extraction, etc. Our dataset is collected from over 1k\narticles related to 123 topics. Near 70k sentences in the dataset are fully\nannotated based on their argument properties (e.g., claims, stances, evidence,\netc.). We further propose two new integrated argument mining tasks associated\nwith the debate preparation process: (1) claim extraction with stance\nclassification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a\npipeline approach and an end-to-end method for each integrated task separately.\nPromising experimental results are reported to show the values and challenges\nof our proposed tasks, and motivate future research on argument mining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruidan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Event Linking to Wikidata. (arXiv:2204.06535v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06535","description":"<p>We present a task of multilingual linking of events to a knowledge base. We\nautomatically compile a large-scale dataset for this task, comprising of 1.8M\nmentions across 44 languages referring to over 10.9K events from Wikidata. We\npropose two variants of the event linking task: 1) multilingual, where event\ndescriptions are from the same language as the mention, and 2) crosslingual,\nwhere all event descriptions are in English. On the two proposed tasks, we\ncompare multiple event linking systems including BM25+ (Lv and Zhai, 2011) and\nmultilingual adaptations of the biencoder and crossencoder architectures from\nBLINK (Wu et al., 2020). In our experiments on the two task variants, we find\nboth biencoder and crossencoder models significantly outperform the BM25+\nbaseline. Our results also indicate that the crosslingual task is in general\nmore challenging than the multilingual task. To test the out-of-domain\ngeneralization of the proposed linking systems, we additionally create a\nWikinews-based evaluation set. We present qualitative analysis highlighting\nvarious aspects captured by the proposed dataset, including the need for\ntemporal reasoning over context and tackling diverse event descriptions across\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pratapa_A/0/1/0/all/0/1\">Adithya Pratapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rishubh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Retrieve Videos by Asking Questions. (arXiv:2205.05739v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05739","description":"<p>The majority of traditional text-to-video retrieval systems operate in static\nenvironments, i.e., there is no interaction between the user and the agent\nbeyond the initial textual query provided by the user. This can be sub-optimal\nif the initial query has ambiguities, which would lead to many falsely\nretrieved videos. To overcome this limitation, we propose a novel framework for\nVideo Retrieval using Dialog (ViReD), which enables the user to interact with\nan AI agent via multiple rounds of dialog, where the user refines retrieved\nresults by answering questions generated by an AI agent. Our novel multimodal\nquestion generator learns to ask questions that maximize the subsequent video\nretrieval performance using (i) the video candidates retrieved during the last\nround of interaction with the user and (ii) the text-based dialog history\ndocumenting all previous interactions, to generate questions that incorporate\nboth visual and linguistic cues relevant to video retrieval. Furthermore, to\ngenerate maximally informative questions, we propose an Information-Guided\nSupervision (IGS), which guides the question generator to ask questions that\nwould boost subsequent video retrieval accuracy. We validate the effectiveness\nof our interactive ViReD framework on the AVSD dataset, showing that our\ninteractive method performs significantly better than traditional\nnon-interactive video retrieval systems. We also demonstrate that our proposed\napproach generalizes to the real-world settings that involve interactions with\nreal humans, thus, demonstrating the robustness and generality of our framework\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1\">Avinash Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1\">Junier Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Dementia Detection in Spoken Language. (arXiv:2206.12879v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.12879","description":"<p>Dementia is a growing problem as our society ages, and detection methods are\noften invasive and expensive. Recent deep-learning techniques can offer a\nfaster diagnosis and have shown promising results. However, they require large\namounts of labelled data which is not easily available for the task of dementia\ndetection. One effective solution to sparse data problems is data augmentation,\nthough the exact methods need to be selected carefully. To date, there has been\nno empirical study of data augmentation on Alzheimer's disease (AD) datasets\nfor NLP and speech processing. In this work, we investigate data augmentation\ntechniques for the task of AD detection and perform an empirical evaluation of\nthe different approaches on two kinds of models for both the text and audio\ndomains. We use a transformer-based model for both domains, and SVM and Random\nForest models for the text and audio domains, respectively. We generate\nadditional samples using traditional as well as deep learning based methods and\nshow that data augmentation improves performance for both the text- and\naudio-based models and that such results are comparable to state-of-the-art\nresults on the popular ADReSS set, with carefully crafted architectures and\nfeatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hledikova_A/0/1/0/all/0/1\">Anna Hl&#xe9;dikov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woszczyk_D/0/1/0/all/0/1\">Dominika Woszczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akman_A/0/1/0/all/0/1\">Alican Akman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demetriou_S/0/1/0/all/0/1\">Soteris Demetriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. (arXiv:2207.01206v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.01206","description":"<p>Existing benchmarks for grounding language in interactive environments either\nlack real-world linguistic elements, or prove difficult to scale up due to\nsubstantial human involvement in the collection of data or feedback signals. To\nbridge this gap, we develop WebShop -- a simulated e-commerce website\nenvironment with $1.18$ million real-world products and $12,087$ crowd-sourced\ntext instructions. Given a text instruction specifying a product requirement,\nan agent needs to navigate multiple types of webpages and issue diverse actions\nto find, customize, and purchase an item. WebShop provides several challenges\nfor language grounding including understanding compositional instructions,\nquery (re-)formulation, comprehending and acting on noisy text in webpages, and\nperforming strategic exploration. We collect over $1,600$ human demonstrations\nfor the task, and train and evaluate a diverse range of agents using\nreinforcement learning, imitation learning, and pre-trained image and language\nmodels. Our best model achieves a task success rate of $29\\%$, which\noutperforms rule-based heuristics ($9.6\\%$) but is far lower than human expert\nperformance ($59\\%$). We also analyze agent and human trajectories and ablate\nvarious model components to provide insights for developing future agents with\nstronger language understanding and decision making abilities. Finally, we show\nthat agents trained on WebShop exhibit non-trivial sim-to-real transfer when\nevaluated on amazon.com and ebay.com, indicating the potential value of WebShop\nin developing practical web-based agents that can operate in the wild.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Howard Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">John Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIA 2022 Shared Task Submission: Leveraging Entity Representations, Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question Answering. (arXiv:2207.01940v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.01940","description":"<p>We describe our two-stage system for the Multilingual Information Access\n(MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The\nfirst stage consists of multilingual passage retrieval with a hybrid dense and\nsparse retrieval strategy. The second stage consists of a reader which outputs\nthe answer from the top passages returned by the first stage. We show the\nefficacy of using a multilingual language model with entity representations in\npretraining, sparse retrieval signals to help dense retrieval, and\nFusion-in-Decoder. On the development set, we obtain 43.46 F1 on XOR-TyDi QA\nand 21.99 F1 on MKQA, for an average F1 score of 32.73. On the test set, we\nobtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an average F1 score of\n31.61. We improve over the official baseline by over 4 F1 points on both the\ndevelopment and test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhucheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanabhan_S/0/1/0/all/0/1\">Sarguna Janani Padmanabhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Value of Gazetteer in Chinese Named Entity Recognition. (arXiv:2207.02802v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.02802","description":"<p>Gazetteer is widely used in Chinese named entity recognition (NER) to enhance\nspan boundary detection and type classification. However, to further understand\nthe generalizability and effectiveness of gazetteers, the NLP community still\nlacks a systematic analysis of the gazetteer-enhanced NER model. In this paper,\nwe first re-examine the effectiveness several common practices of the\ngazetteer-enhanced NER models and carry out a series of detailed analysis to\nevaluate the relationship between the model performance and the gazetteer\ncharacteristics, which can guide us to build a more suitable gazetteer. The\nfindings of this paper are as follows: (1) the gazetteer improves most of the\nsituations that the traditional NER model datasets are difficult to learn. (2)\nthe performance of model greatly benefits from the high-quality pre-trained\nlexeme embeddings. (3) a good gazetteer should cover more entities that can be\nmatched in both the training set and testing set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianglong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangji Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiangang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bojia Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Win-Win Cooperation: Bundling Sequence and Span Models for Named Entity Recognition. (arXiv:2207.03300v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.03300","description":"<p>For Named Entity Recognition (NER), sequence labeling-based and span-based\nparadigms are quite different. Previous research has demonstrated that the two\nparadigms have clear complementary advantages, but few models have attempted to\nleverage these advantages in a single NER model as far as we know. In our\nprevious work, we proposed a paradigm known as Bundling Learning (BL) to\naddress the above problem. The BL paradigm bundles the two NER paradigms,\nenabling NER models to jointly tune their parameters by weighted summing each\nparadigm's training loss. However, three critical issues remain unresolved:\nWhen does BL work? Why does BL work? Can BL enhance the existing\nstate-of-the-art (SOTA) NER models? To address the first two issues, we\nimplement three NER models, involving a sequence labeling-based model--SeqNER,\na span-based NER model--SpanNER, and BL-NER that bundles SeqNER and SpanNER\ntogether. We draw two conclusions regarding the two issues based on the\nexperimental results on eleven NER datasets from five domains. We then apply BL\nto five existing SOTA NER models to investigate the third issue, consisting of\nthree sequence labeling-based models and two span-based models. Experimental\nresults indicate that BL consistently enhances their performance, suggesting\nthat it is possible to construct a new SOTA NER system by incorporating BL into\nthe current SOTA system. Moreover, we find that BL reduces both entity boundary\nand type prediction errors. In addition, we compare two commonly used labeling\ntagging methods as well as three types of span semantic representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1\">Bin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huijun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying public values and spatial conflicts in urban planning. (arXiv:2207.04719v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2207.04719","description":"<p>Identifying the diverse and often competing values of citizens, and resolving\nthe consequent public value conflicts, are of significant importance for\ninclusive and integrated urban development. Scholars have highlighted that\nrelational, value-laden urban space gives rise to many diverse conflicts that\nvary both spatially and temporally. Although notions of public value conflicts\nhave been conceived in theory, there are very few empirical studies that\nidentify such values and their conflicts in urban space. Building on public\nvalue theory and using a case-study mixed-methods approach, this paper proposes\na new approach to empirically investigate public value conflicts in urban\nspace. Using unstructured participatory data of 4,528 citizen contributions\nfrom a Public Participation Geographic Information Systems in Hamburg, Germany,\nnatural language processing and spatial clustering techniques are used to\nidentify areas of potential value conflicts. Four expert workshops assess and\ninterpret these quantitative findings. Integrating both quantitative and\nqualitative results, 19 general public values and a total of 9 archetypical\nconflicts are identified. On the basis of these results, this paper proposes a\nnew conceptual tool of Public Value Spheres that extends the theoretical notion\nof public-value conflicts and helps to further account for the value-laden\nnature of urban space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herzog_R/0/1/0/all/0/1\">Rico H. Herzog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_J/0/1/0/all/0/1\">Juliana E. Gon&#xe7;alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slingerland_G/0/1/0/all/0/1\">Geertje Slingerland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinhans_R/0/1/0/all/0/1\">Reinout Kleinhans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prang_H/0/1/0/all/0/1\">Holger Prang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brazier_F/0/1/0/all/0/1\">Frances Brazier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_T/0/1/0/all/0/1\">Trivik Verma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Induction enabling Recommending and Trend Analysis: A Corporate Research Community Use Case. (arXiv:2207.05188v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2207.05188","description":"<p>A research division plays an important role of driving innovation in an\norganization. Drawing insights, following trends, keeping abreast of new\nresearch, and formulating strategies are increasingly becoming more challenging\nfor both researchers and executives as the amount of information grows in both\nvelocity and volume. In this paper we present a use case of how a corporate\nresearch community, IBM Research, utilizes Semantic Web technologies to induce\na unified Knowledge Graph from both structured and textual data obtained by\nintegrating various applications used by the community related to research\nprojects, academic papers, datasets, achievements and recognition. In order to\nmake the Knowledge Graph more accessible to application developers, we\nidentified a set of common patterns for exploiting the induced knowledge and\nexposed them as APIs. Those patterns were born out of user research which\nidentified the most valuable use cases or user pain points to be alleviated. We\noutline two distinct scenarios: recommendation and analytics for business use.\nWe will discuss these scenarios in detail and provide an empirical evaluation\non entity recommendation specifically. The methodology used and the lessons\nlearned from this work can be applied to other organizations facing similar\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sava_M/0/1/0/all/0/1\">Mike Sava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md Faisal Mahbub Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yachbes_I/0/1/0/all/0/1\">Irene Yachbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gidh_A/0/1/0/all/0/1\">Aditya Gidh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duckwitz_J/0/1/0/all/0/1\">Jillian Duckwitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nisar_K/0/1/0/all/0/1\">Kovit Nisar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1\">Michael Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models (Mostly) Know What They Know. (arXiv:2207.05221v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.05221","description":"<p>We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conerly_T/0/1/0/all/0/1\">Tom Conerly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henighan_T/0/1/0/all/0/1\">Tom Henighan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1\">Nicholas Schiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_Z/0/1/0/all/0/1\">Zac Hatfield Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1\">Nova DasSarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Johnson_E/0/1/0/all/0/1\">Eli Tran-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnston_S/0/1/0/all/0/1\">Scott Johnston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Showk_S/0/1/0/all/0/1\">Sheer El-Showk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_A/0/1/0/all/0/1\">Andy Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhage_N/0/1/0/all/0/1\">Nelson Elhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hume_T/0/1/0/all/0/1\">Tristan Hume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anna Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuntao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Sam Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1\">Deep Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1\">Danny Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_J/0/1/0/all/0/1\">Josh Jacobson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kernion_J/0/1/0/all/0/1\">Jackson Kernion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1\">Shauna Kravec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovitt_L/0/1/0/all/0/1\">Liane Lovitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1\">Kamal Ndousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsson_C/0/1/0/all/0/1\">Catherine Olsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ringer_S/0/1/0/all/0/1\">Sam Ringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Dario Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1\">Tom Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_B/0/1/0/all/0/1\">Ben Mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olah_C/0/1/0/all/0/1\">Chris Olah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Localisation And Imaging Methods for Moving Target Ghost Imaging Radar Based On Correlation Intensity Weighting. (arXiv:2207.07649v1 [eess.IV])","link":"http://arxiv.org/abs/2207.07649","description":"<p>Ghost imaging radar is a new system of gaze imaging radar with high detection\nsensitivity, super-resolution and better anti-interference performance, but the\nrelative motion between the radar system and the target will make the target\nimaging deteriorate. This paper proposes to perform absolute position\nlocalisation of a single target in the field of view by weighting the\ncorrelation strength of a single frame image of rough target, and to compensate\ntranslation of the reference arm speckle according to the localisation and\ntracking trajectory to accumulate the rough image into a high quality image.\nThe proposed correlation intensity weighted localization and tracking imaging\nmethod has been verified by simulation to be able to locate and image targets\nin the field of view well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuliang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging. (arXiv:2207.07697v1 [cs.LG])","link":"http://arxiv.org/abs/2207.07697","description":"<p>Fine-tuning models on edge devices like mobile phones would enable\nprivacy-preserving personalization over sensitive data. However, edge training\nhas historically been limited to relatively small models with simple\narchitectures because training is both memory and energy intensive. We present\nPOET, an algorithm to enable training large neural networks on memory-scarce\nbattery-operated edge devices. POET jointly optimizes the integrated search\nsearch spaces of rematerialization and paging, two algorithms to reduce the\nmemory consumption of backpropagation. Given a memory budget and a run-time\nconstraint, we formulate a mixed-integer linear program (MILP) for\nenergy-optimal training. Our approach enables training significantly larger\nmodels on embedded devices while reducing energy consumption while not\nmodifying mathematical correctness of backpropagation. We demonstrate that it\nis possible to fine-tune both ResNet-18 and BERT within the memory constraints\nof a Cortex-M class embedded device while outperforming current edge training\nmethods in energy efficiency. POET is an open-source project available at\nhttps://github.com/ShishirPatil/poet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1\">Shishir G. Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Paras Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_P/0/1/0/all/0/1\">Prabal Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Untrained, physics-informed neural networks for structured illumination microscopy. (arXiv:2207.07705v1 [eess.IV])","link":"http://arxiv.org/abs/2207.07705","description":"<p>In recent years there has been great interest in using deep neural networks\n(DNN) for super-resolution image reconstruction including for structured\nillumination microscopy (SIM). While these methods have shown very promising\nresults, they all rely on data-driven, supervised training strategies that need\na large number of ground truth images, which is experimentally difficult to\nrealize. For SIM imaging, there exists a need for a flexible, general, and\nopen-source reconstruction method that can be readily adapted to different\nforms of structured illumination. We demonstrate that we can combine a deep\nneural network with the forward model of the structured illumination process to\nreconstruct sub-diffraction images without training data. The resulting\nphysics-informed neural network (PINN) can be optimized on a single set of\ndiffraction limited sub-images and thus doesn't require any training set. We\nshow with simulated and experimental data that this PINN can be applied to a\nwide variety of SIM methods by simply changing the known illumination patterns\nused in the loss function and can achieve resolution improvements that match\nwell with theoretical expectations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Burns_Z/0/1/0/all/0/1\">Zachary Burns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaowei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Focal Loss: Asking Your Discriminator for Hard Examples. (arXiv:2207.07739v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07739","description":"<p>Focal Loss has reached incredible popularity as it uses a simple technique to\nidentify and utilize hard examples to achieve better performance on\nclassification. However, this method does not easily generalize outside of\nclassification tasks, such as in keypoint detection. In this paper, we propose\na novel adaptation of Focal Loss for keypoint detection tasks, called\nAdversarial Focal Loss (AFL). AFL not only is semantically analogous to Focal\nloss, but also works as a plug-and-chug upgrade for arbitrary loss functions.\nWhile Focal Loss requires output from a classifier, AFL leverages a separate\nadversarial network to produce a difficulty score for each input. This\ndifficulty score can then be used to dynamically prioritize learning on hard\nexamples, even in absence of a classifier. In this work, we show AFL's\neffectiveness in enhancing existing methods in keypoint detection and verify\nits capability to re-weigh examples based on difficulty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaomeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1\">Michael Potter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hsi-Ming Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soni_R/0/1/0/all/0/1\">Ravi Soni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human keypoint detection for close proximity human-robot interaction. (arXiv:2207.07742v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07742","description":"<p>We study the performance of state-of-the-art human keypoint detectors in the\ncontext of close proximity human-robot interaction. The detection in this\nscenario is specific in that only a subset of body parts such as hands and\ntorso are in the field of view. In particular, (i) we survey existing datasets\nwith human pose annotation from the perspective of close proximity images and\nprepare and make publicly available a new Human in Close Proximity (HiCP)\ndataset; (ii) we quantitatively and qualitatively compare state-of-the-art\nhuman whole-body 2D keypoint detection methods (OpenPose, MMPose, AlphaPose,\nDetectron2) on this dataset; (iii) since accurate detection of hands and\nfingers is critical in applications with handovers, we evaluate the performance\nof the MediaPipe hand detector; (iv) we deploy the algorithms on a humanoid\nrobot with an RGB-D camera on its head and evaluate the performance in 3D human\nkeypoint detection. A motion capture system is used as reference.\n</p>\n<p>The best performing whole-body keypoint detectors in close proximity were\nMMPose and AlphaPose, but both had difficulty with finger detection. Thus, we\npropose a combination of MMPose or AlphaPose for the body and MediaPipe for the\nhands in a single framework providing the most accurate and robust detection.\nWe also analyse the failure modes of individual detectors -- for example, to\nwhat extent the absence of the head of the person in the image degrades\nperformance. Finally, we demonstrate the framework in a scenario where a\nhumanoid robot interacting with a person uses the detected 3D keypoints for\nwhole-body avoidance maneuvers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Docekal_J/0/1/0/all/0/1\">Jan Docekal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozlivek_J/0/1/0/all/0/1\">Jakub Rozlivek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_M/0/1/0/all/0/1\">Matej Hoffmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HOME: High-Order Mixed-Moment-based Embedding for Representation Learning. (arXiv:2207.07743v1 [cs.LG])","link":"http://arxiv.org/abs/2207.07743","description":"<p>Minimum redundancy among different elements of an embedding in a latent space\nis a fundamental requirement or major preference in representation learning to\ncapture intrinsic informational structures. Current self-supervised learning\nmethods minimize a pair-wise covariance matrix to reduce the feature redundancy\nand produce promising results. However, such representation features of\nmultiple variables may contain the redundancy among more than two feature\nvariables that cannot be minimized via the pairwise regularization. Here we\npropose the High-Order Mixed-Moment-based Embedding (HOME) strategy to reduce\nthe redundancy between any sets of feature variables, which is to our best\nknowledge the first attempt to utilize high-order statistics/information in\nthis context. Multivariate mutual information is minimum if and only if\nmultiple variables are mutually independent, which suggests the necessary\nconditions of factorized mixed moments among multiple variables. Based on these\nstatistical and information theoretic principles, our general HOME framework is\npresented for self-supervised representation learning. Our initial experiments\nshow that a simple version in the form of a three-order HOME scheme already\nsignificantly outperforms the current two-order baseline method (i.e., Barlow\nTwins) in terms of the linear evaluation on representation features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESFPNet: efficient deep learning architecture for real-time lesion segmentation in autofluorescence bronchoscopic video. (arXiv:2207.07759v1 [eess.IV])","link":"http://arxiv.org/abs/2207.07759","description":"<p>Lung cancer tends to be detected at an advanced stage, resulting in a high\npatient mortality rate. Thus, recent research has focused on early disease\ndetection. Lung cancer generally first appears as lesions developing within the\nbronchial epithelium of the airway walls. Bronchoscopy is the procedure of\nchoice for effective noninvasive bronchial lesion detection. In particular,\nautofluorescence bronchoscopy (AFB) discriminates the autofluorescence\nproperties of normal and diseased tissue, whereby lesions appear reddish brown\nin AFB video frames, while normal tissue appears green. Because recent studies\nshow AFB's ability for high lesion sensitivity, it has become a potentially\npivotal method during the standard bronchoscopic airway exam for early-stage\nlung cancer detection. Unfortunately, manual inspection of AFB video is\nextremely tedious and error-prone, while limited effort has been expended\ntoward potentially more robust automatic AFB lesion detection and segmentation.\nWe propose a real-time deep learning architecture ESFPNet for robust detection\nand segmentation of bronchial lesions from an AFB video stream. The\narchitecture features an encoder structure that exploits pretrained Mix\nTransformer (MiT) encoders and a stage-wise feature pyramid (ESFP) decoder\nstructure. Results from AFB videos derived from lung cancer patient airway\nexams indicate that our approach gives mean Dice index and IOU values of 0.782\nand 0.658, respectively, while having a processing throughput of 27 frames/sec.\nThese values are superior to results achieved by other competing architectures\nthat use Mix transformers or CNN-based encoders. Moreover, the superior\nperformance on the ETIS-LaribPolypDB dataset demonstrates its potential\napplicability to other domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chang_Q/0/1/0/all/0/1\">Qi Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahmad_D/0/1/0/all/0/1\">Danish Ahmad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toth_J/0/1/0/all/0/1\">Jennifer Toth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bascom_R/0/1/0/all/0/1\">Rebecca Bascom</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Higgins_W/0/1/0/all/0/1\">William E. Higgins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Long-Term Spatial-Temporal Graphs for Active Speaker Detection. (arXiv:2207.07783v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07783","description":"<p>Active speaker detection (ASD) in videos with multiple speakers is a\nchallenging task as it requires learning effective audiovisual features and\nspatial-temporal correlations over long temporal windows. In this paper, we\npresent SPELL, a novel spatial-temporal graph learning framework that can solve\ncomplex tasks such as ASD. To this end, each person in a video frame is first\nencoded in a unique node for that frame. Nodes corresponding to a single person\nacross frames are connected to encode their temporal dynamics. Nodes within a\nframe are also connected to encode inter-person relationships. Thus, SPELL\nreduces ASD to a node classification task. Importantly, SPELL is able to reason\nover long temporal contexts for all nodes without relying on computationally\nexpensive fully connected graph neural networks. Through extensive experiments\non the AVA-ActiveSpeaker dataset, we demonstrate that learning graph-based\nrepresentations can significantly improve the active speaker detection\nperformance owing to its explicit spatial and temporal structure. SPELL\noutperforms all previous state-of-the-art approaches while requiring\nsignificantly lower memory and computational resources. Our code is publicly\navailable at https://github.com/SRA2/SPELL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_K/0/1/0/all/0/1\">Kyle Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sourya Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Subarna Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_T/0/1/0/all/0/1\">Tanaya Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Somdeb Majumdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Desirable Decision Boundary by Moderate-Margin Adversarial Training. (arXiv:2207.07793v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07793","description":"<p>Adversarial training, as one of the most effective defense methods against\nadversarial attacks, tends to learn an inclusive decision boundary to increase\nthe robustness of deep learning models. However, due to the large and\nunnecessary increase in the margin along adversarial directions, adversarial\ntraining causes heavy cross-over between natural examples and adversarial\nexamples, which is not conducive to balancing the trade-off between robustness\nand natural accuracy. In this paper, we propose a novel adversarial training\nscheme to achieve a better trade-off between robustness and natural accuracy.\nIt aims to learn a moderate-inclusive decision boundary, which means that the\nmargins of natural examples under the decision boundary are moderate. We call\nthis scheme Moderate-Margin Adversarial Training (MMAT), which generates\nfiner-grained adversarial examples to mitigate the cross-over problem. We also\ntake advantage of logits from a teacher model that has been well-trained to\nguide the learning of our model. Finally, MMAT achieves high natural accuracy\nand robustness under both black-box and white-box attacks. On SVHN, for\nexample, state-of-the-art robustness and natural accuracy are achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaoyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yaguan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianchang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiang Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chunming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaileh_W/0/1/0/all/0/1\">Wassim Swaileh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RCRN: Real-world Character Image Restoration Network via Skeleton Extraction. (arXiv:2207.07795v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07795","description":"<p>Constructing high-quality character image datasets is challenging because\nreal-world images are often affected by image degradation. There are\nlimitations when applying current image restoration methods to such real-world\ncharacter images, since (i) the categories of noise in character images are\ndifferent from those in general images; (ii) real-world character images\nusually contain more complex image degradation, e.g., mixed noise at different\nnoise levels. To address these problems, we propose a real-world character\nrestoration network (RCRN) to effectively restore degraded character images,\nwhere character skeleton information and scale-ensemble feature extraction are\nutilized to obtain better restoration performance. The proposed method consists\nof a skeleton extractor (SENet) and a character image restorer (CiRNet). SENet\naims to preserve the structural consistency of the character and normalize\ncomplex noise. Then, CiRNet reconstructs clean images from degraded character\nimages and their skeletons. Due to the lack of benchmarks for real-world\ncharacter image restoration, we constructed a dataset containing 1,606\ncharacter images with real-world degradation to evaluate the validity of the\nproposed method. The experimental results demonstrate that RCRN outperforms\nstate-of-the-art methods quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Daqian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_X/0/1/0/all/0/1\">Xiaolei Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_H/0/1/0/all/0/1\">Hao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CARBEN: Composite Adversarial Robustness Benchmark. (arXiv:2207.07797v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07797","description":"<p>Prior literature on adversarial attack methods has mainly focused on\nattacking with and defending against a single threat model, e.g., perturbations\nbounded in Lp ball. However, multiple threat models can be combined into\ncomposite perturbations. One such approach, composite adversarial attack (CAA),\nnot only expands the perturbable space of the image, but also may be overlooked\nby current modes of robustness evaluation. This paper demonstrates how CAA's\nattack order affects the resulting image, and provides real-time inferences of\ndifferent models, which will facilitate users' configuration of the parameters\nof the attack level and their rapid evaluation of model prediction. A\nleaderboard to benchmark adversarial robustness against CAA is also introduced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsiung_L/0/1/0/all/0/1\">Lei Hsiung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Yun Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1\">Tsung-Yi Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CharFormer: A Glyph Fusion based Attentive Framework for High-precision Character Image Denoising. (arXiv:2207.07798v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07798","description":"<p>Degraded images commonly exist in the general sources of character images,\nleading to unsatisfactory character recognition results. Existing methods have\ndedicated efforts to restoring degraded character images. However, the\ndenoising results obtained by these methods do not appear to improve character\nrecognition performance. This is mainly because current methods only focus on\npixel-level information and ignore critical features of a character, such as\nits glyph, resulting in character-glyph damage during the denoising process. In\nthis paper, we introduce a novel generic framework based on glyph fusion and\nattention mechanisms, i.e., CharFormer, for precisely recovering character\nimages without changing their inherent glyphs. Unlike existing frameworks,\nCharFormer introduces a parallel target task for capturing additional\ninformation and injecting it into the image denoising backbone, which will\nmaintain the consistency of character glyphs during character image denoising.\nMoreover, we utilize attention-based networks for global-local feature\ninteraction, which will help to deal with blind denoising and enhance denoising\nperformance. We compare CharFormer with state-of-the-art methods on multiple\ndatasets. The experimental results show the superiority of CharFormer\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Daqian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_X/0/1/0/all/0/1\">Xiaolei Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lida Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1\">Yang Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Granularity-Unified Representations for Text-to-Image Person Re-identification. (arXiv:2207.07802v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07802","description":"<p>Text-to-image person re-identification (ReID) aims to search for pedestrian\nimages of an interested identity via textual descriptions. It is challenging\ndue to both rich intra-modal variations and significant inter-modal gaps.\nExisting works usually ignore the difference in feature granularity between the\ntwo modalities, i.e., the visual features are usually fine-grained while\ntextual features are coarse, which is mainly responsible for the large\ninter-modal gaps. In this paper, we propose an end-to-end framework based on\ntransformers to learn granularity-unified representations for both modalities,\ndenoted as LGUR. LGUR framework contains two modules: a Dictionary-based\nGranularity Alignment (DGA) module and a Prototype-based Granularity\nUnification (PGU) module. In DGA, in order to align the granularities of two\nmodalities, we introduce a Multi-modality Shared Dictionary (MSD) to\nreconstruct both visual and textual features. Besides, DGA has two important\nfactors, i.e., the cross-modality guidance and the foreground-centric\nreconstruction, to facilitate the optimization of MSD. In PGU, we adopt a set\nof shared and learnable prototypes as the queries to extract diverse and\nsemantically aligned features for both modalities in the granularity-unified\nfeature space, which further promotes the ReID performance. Comprehensive\nexperiments show that our LGUR consistently outperforms state-of-the-arts by\nlarge margins on both CUHK-PEDES and ICFG-PEDES datasets. Code will be released\nat https://github.com/ZhiyinShao-H/LGUR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhiyin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhifeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Spatial-Spectral Autoencoders Are Excellent Hyperspectral Defenders. (arXiv:2207.07803v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07803","description":"<p>Deep learning methodology contributes a lot to the development of\nhyperspectral image (HSI) analysis community. However, it also makes HSI\nanalysis systems vulnerable to adversarial attacks. To this end, we propose a\nmasked spatial-spectral autoencoder (MSSA) in this paper under self-supervised\nlearning theory, for enhancing the robustness of HSI analysis systems. First, a\nmasked sequence attention learning module is conducted to promote the inherent\nrobustness of HSI analysis systems along spectral channel. Then, we develop a\ngraph convolutional network with learnable graph structure to establish global\npixel-wise combinations.In this way, the attack effect would be dispersed by\nall the related pixels among each combination, and a better defense performance\nis achievable in spatial aspect.Finally, to improve the defense transferability\nand address the problem of limited labelled samples, MSSA employs spectra\nreconstruction as a pretext task and fits the datasets in a self-supervised\nmanner.Comprehensive experiments over three benchmarks verify the effectiveness\nof MSSA in comparison with the state-of-the-art hyperspectral classification\nmethods and representative adversarial defense strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiahao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bin_K/0/1/0/all/0/1\">Kangcheng Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_P/0/1/0/all/0/1\">Ping Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-calibrating Photometric Stereo by Neural Inverse Rendering. (arXiv:2207.07815v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07815","description":"<p>This paper tackles the task of uncalibrated photometric stereo for 3D object\nreconstruction, where both the object shape, object reflectance, and lighting\ndirections are unknown. This is an extremely difficult task, and the challenge\nis further compounded with the existence of the well-known generalized\nbas-relief (GBR) ambiguity in photometric stereo. Previous methods to resolve\nthis ambiguity either rely on an overly simplified reflectance model, or assume\nspecial light distribution. We propose a new method that jointly optimizes\nobject shape, light directions, and light intensities, all under general\nsurfaces and lights assumptions. The specularities are used explicitly to solve\nuncalibrated photometric stereo via a neural inverse rendering process. We\ngradually fit specularities from shiny to rough using novel progressive\nspecular bases. Our method leverages a physically based rendering equation by\nminimizing the reconstruction error on a per-object-basis. Our method\ndemonstrates state-of-the-art accuracy in light estimation and shape recovery\non real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bagging Regional Classification Activation Maps for Weakly Supervised Object Localization. (arXiv:2207.07818v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07818","description":"<p>Classification activation map (CAM), utilizing the classification structure\nto generate pixel-wise localization maps, is a crucial mechanism for weakly\nsupervised object localization (WSOL). However, CAM directly uses the\nclassifier trained on image-level features to locate objects, making it prefers\nto discern global discriminative factors rather than regional object cues. Thus\nonly the discriminative locations are activated when feeding pixel-level\nfeatures into this classifier. To solve this issue, this paper elaborates a\nplug-and-play mechanism called BagCAMs to better project a well-trained\nclassifier for the localization task without refining or re-training the\nbaseline structure. Our BagCAMs adopts a proposed regional localizer generation\n(RLG) strategy to define a set of regional localizers and then derive them from\na well-trained classifier. These regional localizers can be viewed as the base\nlearner that only discerns region-wise object factors for localization tasks,\nand their results can be effectively weighted by our BagCAMs to form the final\nlocalization map. Experiments indicate that adopting our proposed BagCAMs can\nimprove the performance of baseline WSOL methods to a great extent and obtains\nstate-of-the-art performance on three WSOL benchmarks. Code are released at\nhttps://github.com/zh460045050/BagCAMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lujia Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yunfei You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanye Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Cross-Set Few-Shot Learning via Learning Compact and Aligned Representations. (arXiv:2207.07826v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07826","description":"<p>Few-shot learning (FSL) aims to recognize novel queries with only a few\nsupport samples through leveraging prior knowledge from a base dataset. In this\npaper, we consider the domain shift problem in FSL and aim to address the\ndomain gap between the support set and the query set. Different from previous\ncross-domain FSL work (CD-FSL) that considers the domain shift between base and\nnovel classes, the new problem, termed cross-domain cross-set FSL (CDSC-FSL),\nrequires few-shot learners not only to adapt to the new domain, but also to be\nconsistent between different domains within each novel class. To this end, we\npropose a novel approach, namely stabPA, to learn prototypical compact and\ncross-domain aligned representations, so that the domain shift and few-shot\nlearning can be addressed simultaneously. We evaluate our approach on two new\nCDCS-FSL benchmarks built from the DomainNet and Office-Home datasets\nrespectively. Remarkably, our approach outperforms multiple elaborated\nbaselines by a large margin, e.g., improving 5-shot accuracy by 6.0 points on\naverage on DomainNet. Code is available at\nhttps://github.com/WentaoChen0813/CDCS-FSL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wentao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Memory-driven Transformer for Multivariate Long Sequence Time-series Forecasting. (arXiv:2207.07827v1 [cs.LG])","link":"http://arxiv.org/abs/2207.07827","description":"<p>Multivariate long sequence time-series forecasting (M-LSTF) is a practical\nbut challenging problem. Unlike traditional timer-series forecasting tasks,\nM-LSTF tasks are more challenging from two aspects: 1) M-LSTF models need to\nlearn time-series patterns both within and between multiple time features; 2)\nUnder the rolling forecasting setting, the similarity between two consecutive\ntraining samples increases with the increasing prediction length, which makes\nmodels more prone to overfitting. In this paper, we propose a generalizable\nmemory-driven Transformer to target M-LSTF problems. Specifically, we first\npropose a global-level memory component to drive the forecasting procedure by\nintegrating multiple time-series features. In addition, we adopt a progressive\nfashion to train our model to increase its generalizability, in which we\ngradually introduce Bernoulli noises to training samples. Extensive experiments\nhave been performed on five different datasets across multiple fields.\nExperimental results demonstrate that our approach can be seamlessly plugged\ninto varying Transformer-based models to improve their performances up to\nroughly 30%. Particularly, this is the first work to specifically focus on the\nM-LSTF tasks to the best of our knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structural Prior Guided Generative Adversarial Transformers for Low-Light Image Enhancement. (arXiv:2207.07828v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07828","description":"<p>We propose an effective Structural Prior guided Generative Adversarial\nTransformer (SPGAT) to solve low-light image enhancement. Our SPGAT mainly\ncontains a generator with two discriminators and a structural prior estimator\n(SPE). The generator is based on a U-shaped Transformer which is used to\nexplore non-local information for better clear image restoration. The SPE is\nused to explore useful structures from images to guide the generator for better\nstructural detail estimation. To generate more realistic images, we develop a\nnew structural prior guided adversarial learning method by building the skip\nconnections between the generator and discriminators so that the discriminators\ncan better discriminate between real and fake features. Finally, we propose a\nparallel windows-based Swin Transformer block to aggregate different level\nhierarchical features for high-quality image restoration. Experimental results\ndemonstrate that the proposed SPGAT performs favorably against recent\nstate-of-the-art methods on both synthetic and real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinshan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval. (arXiv:2207.07852v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07852","description":"<p>Text-Video retrieval is a task of great practical value and has received\nincreasing attention, among which learning spatial-temporal video\nrepresentation is one of the research hotspots. The video encoders in the\nstate-of-the-art video retrieval models usually directly adopt the pre-trained\nvision backbones with the network structure fixed, they therefore can not be\nfurther improved to produce the fine-grained spatial-temporal video\nrepresentation. In this paper, we propose Token Shift and Selection Network\n(TS2-Net), a novel token shift and selection transformer architecture, which\ndynamically adjusts the token sequence and selects informative tokens in both\ntemporal and spatial dimensions from input video samples. The token shift\nmodule temporally shifts the whole token features back-and-forth across\nadjacent frames, to preserve the complete token representation and capture\nsubtle movements. Then the token selection module selects tokens that\ncontribute most to local spatial semantics. Based on thorough experiments, the\nproposed TS2-Net achieves state-of-the-art performance on major text-video\nretrieval benchmarks, including new records on MSRVTT, VATEX, LSMDC,\nActivityNet, and DiDeMo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_P/0/1/0/all/0/1\">Pengfei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Luhui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shengming Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Lottery Ticket Hypothesis for Self-attention in Convolutional Neural Network. (arXiv:2207.07858v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07858","description":"<p>Recently many plug-and-play self-attention modules (SAMs) are proposed to\nenhance the model generalization by exploiting the internal information of deep\nconvolutional neural networks (CNNs). In general, previous works ignore where\nto plug in the SAMs since they connect the SAMs individually with each block of\nthe entire CNN backbone for granted, leading to incremental computational cost\nand the number of parameters with the growth of network depth. However, we\nempirically find and verify some counterintuitive phenomena that: (a)\nConnecting the SAMs to all the blocks may not always bring the largest\nperformance boost, and connecting to partial blocks would be even better; (b)\nAdding the SAMs to a CNN may not always bring a performance boost, and instead\nit may even harm the performance of the original CNN backbone. Therefore, we\narticulate and demonstrate the Lottery Ticket Hypothesis for Self-attention\nNetworks: a full self-attention network contains a subnetwork with sparse\nself-attention connections that can (1) accelerate inference, (2) reduce extra\nparameter increment, and (3) maintain accuracy. In addition to the empirical\nevidence, this hypothesis is also supported by our theoretical evidence.\nFurthermore, we propose a simple yet effective reinforcement-learning-based\nmethod to search the ticket, i.e., the connection scheme that satisfies the\nthree above-mentioned conditions. Extensive experiments on widely-used\nbenchmark datasets and popular self-attention networks show the effectiveness\nof our method. Besides, our experiments illustrate that our searched ticket has\nthe capacity of transferring to some vision tasks, e.g., crowd counting and\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransGrasp: Grasp Pose Estimation of a Category of Objects by Transferring Grasps from Only One Labeled Instance. (arXiv:2207.07861v1 [cs.RO])","link":"http://arxiv.org/abs/2207.07861","description":"<p>Grasp pose estimation is an important issue for robots to interact with the\nreal world. However, most of existing methods require exact 3D object models\navailable beforehand or a large amount of grasp annotations for training. To\navoid these problems, we propose TransGrasp, a category-level grasp pose\nestimation method that predicts grasp poses of a category of objects by\nlabeling only one object instance. Specifically, we perform grasp pose transfer\nacross a category of objects based on their shape correspondences and propose a\ngrasp pose refinement module to further fine-tune grasp pose of grippers so as\nto ensure successful grasps. Experiments demonstrate the effectiveness of our\nmethod on achieving high-quality grasps with the transferred grasp poses. Our\ncode is available at https://github.com/yanjh97/TransGrasp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">Hongtao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wanli Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic dataset generation for specific object detection. (arXiv:2207.07867v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07867","description":"<p>In the past decade, object detection tasks are defined mostly by large public\ndatasets. However, building object detection datasets is not scalable due to\ninefficient image collecting and labeling. Furthermore, most labels are still\nin the form of bounding boxes, which provide much less information than the\nreal human visual system. In this paper, we present a method to synthesize\nobject-in-scene images, which can preserve the objects' detailed features\nwithout bringing irrelevant information. In brief, given a set of images\ncontaining a target object, our algorithm first trains a model to find an\napproximate center of the object as an anchor, then makes an outline regression\nto estimate its boundary, and finally blends the object into a new scene. Our\nresult shows that in the synthesized image, the boundaries of objects blend\nvery well with the background. Experiments also show that SOTA segmentation\nmodels work well with our synthesized data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaotian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Leiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLOSE: Curriculum Learning On the Sharing Extent Towards Better One-shot NAS. (arXiv:2207.07868v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07868","description":"<p>One-shot Neural Architecture Search (NAS) has been widely used to discover\narchitectures due to its efficiency. However, previous studies reveal that\none-shot performance estimations of architectures might not be well correlated\nwith their performances in stand-alone training because of the excessive\nsharing of operation parameters (i.e., large sharing extent) between\narchitectures. Thus, recent methods construct even more over-parameterized\nsupernets to reduce the sharing extent. But these improved methods introduce a\nlarge number of extra parameters and thus cause an undesirable trade-off\nbetween the training costs and the ranking quality. To alleviate the above\nissues, we propose to apply Curriculum Learning On Sharing Extent (CLOSE) to\ntrain the supernet both efficiently and effectively. Specifically, we train the\nsupernet with a large sharing extent (an easier curriculum) at the beginning\nand gradually decrease the sharing extent of the supernet (a harder\ncurriculum). To support this training strategy, we design a novel supernet\n(CLOSENet) that decouples the parameters from operations to realize a flexible\nsharing scheme and adjustable sharing extent. Extensive experiments demonstrate\nthat CLOSE can obtain a better ranking quality across different computational\nbudget constraints than other one-shot supernets, and is able to discover\nsuperior architectures when combined with various search strategies. Code is\navailable at https://github.com/walkerning/aw_nas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zixuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xuefei Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiashu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yiping Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuhan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huazhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CA-SpaceNet: Counterfactual Analysis for 6D Pose Estimation in Space. (arXiv:2207.07869v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07869","description":"<p>Reliable and stable 6D pose estimation of uncooperative space objects plays\nan essential role in on-orbit servicing and debris removal missions.\nConsidering that the pose estimator is sensitive to background interference,\nthis paper proposes a counterfactual analysis framework named CASpaceNet to\ncomplete robust 6D pose estimation of the spaceborne targets under complicated\nbackground. Specifically, conventional methods are adopted to extract the\nfeatures of the whole image in the factual case. In the counterfactual case, a\nnon-existent image without the target but only the background is imagined. Side\neffect caused by background interference is reduced by counterfactual analysis,\nwhich leads to unbiased prediction in final results. In addition, we also carry\nout lowbit-width quantization for CA-SpaceNet and deploy part of the framework\nto a Processing-In-Memory (PIM) accelerator on FPGA. Qualitative and\nquantitative results demonstrate the effectiveness and efficiency of our\nproposed method. To our best knowledge, this paper applies causal inference and\nnetwork quantization to the 6D pose estimation of space-borne targets for the\nfirst time. The code is available at\nhttps://github.com/Shunli-Wang/CA-SpaceNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shunli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaibing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Bo Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Liuzhen Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_P/0/1/0/all/0/1\">Peng Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chixiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihua Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeFSAC: Neurally Filtered Minimal Samples. (arXiv:2207.07872v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07872","description":"<p>Since RANSAC, a great deal of research has been devoted to improving both its\naccuracy and run-time. Still, only a few methods aim at recognizing invalid\nminimal samples early, before the often expensive model estimation and quality\ncalculation are done. To this end, we propose NeFSAC, an efficient algorithm\nfor neural filtering of motion-inconsistent and poorly-conditioned minimal\nsamples. We train NeFSAC to predict the probability of a minimal sample leading\nto an accurate relative pose, only based on the pixel coordinates of the image\ncorrespondences. Our neural filtering model learns typical motion patterns of\nsamples which lead to unstable poses, and regularities in the possible motions\nto favour well-conditioned and likely-correct samples. The novel lightweight\narchitecture implements the main invariants of minimal samples for pose\nestimation, and a novel training scheme addresses the problem of extreme class\nimbalance. NeFSAC can be plugged into any existing RANSAC-based pipeline. We\nintegrate it into USAC and show that it consistently provides strong speed-ups\neven under extreme train-test domain gaps - for example, the model trained for\nthe autonomous driving scenario works on PhotoTourism too. We tested NeFSAC on\nmore than 100k image pairs from three publicly available real-world datasets\nand found that it leads to one order of magnitude speed-up, while often finding\nmore accurate results than USAC alone. The source code is available at\nhttps://github.com/cavalli1234/NeFSAC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cavalli_L/0/1/0/all/0/1\">Luca Cavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1\">Daniel Barath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Importance of Hyperparameters and Data Augmentation for Self-Supervised Learning. (arXiv:2207.07875v1 [cs.LG])","link":"http://arxiv.org/abs/2207.07875","description":"<p>Self-Supervised Learning (SSL) has become a very active area of Deep Learning\nresearch where it is heavily used as a pre-training method for classification\nand other tasks. However, the rapid pace of advancements in this area comes at\na price: training pipelines vary significantly across papers, which presents a\npotentially crucial confounding factor. Here, we show that, indeed, the choice\nof hyperparameters and data augmentation strategies can have a dramatic impact\non performance. To shed light on these neglected factors and help maximize the\npower of SSL, we hyperparameterize these components and optimize them with\nBayesian optimization, showing improvements across multiple datasets for the\nSimSiam SSL approach. Realizing the importance of data augmentations for SSL,\nwe also introduce a new automated data augmentation algorithm, GroupAugment,\nwhich considers groups of augmentations and optimizes the sampling across\ngroups. In contrast to algorithms designed for supervised learning,\nGroupAugment achieved consistently high linear evaluation accuracy across all\ndatasets we considered. Overall, our results indicate the importance and likely\nunderestimated role of data augmentation for SSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1\">Diane Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1\">Fabio Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoll_D/0/1/0/all/0/1\">Danny Stoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1\">Robin Tibor Schirrmeister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1\">Samuel M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clover: Towards A Unified Video-Language Alignment and Fusion Model. (arXiv:2207.07885v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07885","description":"<p>Building a universal video-language model for solving various video\nunderstanding tasks (e.g., text-video retrieval, video question answering) is\nan open challenge to the machine learning field. Towards this goal, most recent\nattempts train the models, usually consisting of uni-modal and cross-modal\nfeature encoders, with supervised or pair-wise contrastive pre-text tasks.\nThough offering attractive generality, the resulted models have to compromise\nbetween efficiency and performance. We argue the flaws are caused by their\npre-training strategies\\textemdash they cannot well align and fuse features\nfrom different modalities simultaneously. We then introduce Clover -- a\nCorrelated Video-Language pre-training method -- towards a universal\nvideo-language model for solving multiple video understanding tasks with\nneither performance nor efficiency compromise. It improves cross-modal feature\nalignment and fusion via a novel tri-modal alignment pre-training task.\nAdditionally, we propose to enhance the tri-modal alignment via incorporating\nlearning from masked samples and a novel pair-wise ranking loss. Clover\ndemonstrates outstanding generality. It establishes new state-of-the-arts on\nmultiple downstream tasks, including three retrieval tasks for both zero-shot\nand fine-tuning settings, and eight video question answering tasks. Codes and\npre-trained models will be released at https://github.com/LeeYN-43/Clover.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jingjia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Should Look at All Objects. (arXiv:2207.07889v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07889","description":"<p>Feature pyramid network (FPN) is one of the key components for object\ndetectors. However, there is a long-standing puzzle for researchers that the\ndetection performance of large-scale objects are usually suppressed after\nintroducing FPN. To this end, this paper first revisits FPN in the detection\nframework and reveals the nature of the success of FPN from the perspective of\noptimization. Then, we point out that the degraded performance of large-scale\nobjects is due to the arising of improper back-propagation paths after\nintegrating FPN. It makes each level of the backbone network only has the\nability to look at the objects within a certain scale range. Based on these\nanalysis, two feasible strategies are proposed to enable each level of the\nbackbone to look at all objects in the FPN-based detection frameworks.\nSpecifically, one is to introduce auxiliary objective functions to make each\nbackbone level directly receive the back-propagation signals of various-scale\nobjects during training. The other is to construct the feature pyramid in a\nmore reasonable way to avoid the irrational back-propagation paths. Extensive\nexperiments on the COCO benchmark validate the soundness of our analysis and\nthe effectiveness of our methods. Without bells and whistles, we demonstrate\nthat our method achieves solid improvements (more than 2%) on various detection\nframeworks: one-stage, two-stage, anchor-based, anchor-free and\ntransformer-based detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhenchao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dongdong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Luchuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Unsupervised Pre-Training for Surgical Operating Room Workflow Analysis. (arXiv:2207.07894v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07894","description":"<p>Data-driven approaches to assist operating room (OR) workflow analysis depend\non large curated datasets that are time consuming and expensive to collect. On\nthe other hand, we see a recent paradigm shift from supervised learning to\nself-supervised and/or unsupervised learning approaches that can learn\nrepresentations from unlabeled datasets. In this paper, we leverage the\nunlabeled data captured in robotic surgery ORs and propose a novel way to fuse\nthe multi-modal data for a single video frame or image. Instead of producing\ndifferent augmentations (or 'views') of the same image or video frame which is\na common practice in self-supervised learning, we treat the multi-modal data as\ndifferent views to train the model in an unsupervised manner via clustering. We\ncompared our method with other state of the art methods and results show the\nsuperior performance of our approach on surgical video activity recognition and\nsemantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jamal_M/0/1/0/all/0/1\">Muhammad Abdullah Jamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohareri_O/0/1/0/all/0/1\">Omid Mohareri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JPerceiver: Joint Perception Network for Depth, Pose and Layout Estimation in Driving Scenes. (arXiv:2207.07895v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07895","description":"<p>Depth estimation, visual odometry (VO), and bird's-eye-view (BEV) scene\nlayout estimation present three critical tasks for driving scene perception,\nwhich is fundamental for motion planning and navigation in autonomous driving.\nThough they are complementary to each other, prior works usually focus on each\nindividual task and rarely deal with all three tasks together. A naive way is\nto accomplish them independently in a sequential or parallel manner, but there\nare many drawbacks, i.e., 1) the depth and VO results suffer from the inherent\nscale ambiguity issue; 2) the BEV layout is directly predicted from the\nfront-view image without using any depth-related information, although the\ndepth map contains useful geometry clues for inferring scene layouts. In this\npaper, we address these issues by proposing a novel joint perception framework\nnamed JPerceiver, which can simultaneously estimate scale-aware depth and VO as\nwell as BEV layout from a monocular video sequence. It exploits the cross-view\ngeometric transformation (CGT) to propagate the absolute scale from the road\nlayout to depth and VO based on a carefully-designed scale loss. Meanwhile, a\ncross-view and cross-modal transfer (CCT) module is devised to leverage the\ndepth clues for reasoning road and vehicle layout through an attention\nmechanism. JPerceiver can be trained in an end-to-end multi-task learning way,\nwhere the CGT scale loss and CCT module promote inter-task knowledge transfer\nto benefit feature learning of each task. Experiments on Argoverse, Nuscenes\nand KITTI show the superiority of JPerceiver over existing methods on all the\nabove three tasks in terms of accuracy, model size, and inference speed. The\ncode and models are available\nat~\\href{https://github.com/sunnyHelen/JPerceiver}{https://github.com/sunnyHelen/JPerceiver}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haimei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Vision-RF Gait Re-identification with Low-cost RGB-D Cameras and mmWave Radars. (arXiv:2207.07896v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07896","description":"<p>Human identification is a key requirement for many applications in everyday\nlife, such as personalized services, automatic surveillance, continuous\nauthentication, and contact tracing during pandemics, etc. This work studies\nthe problem of cross-modal human re-identification (ReID), in response to the\nregular human movements across camera-allowed regions (e.g., streets) and\ncamera-restricted regions (e.g., offices) deployed with heterogeneous sensors.\nBy leveraging the emerging low-cost RGB-D cameras and mmWave radars, we propose\nthe first-of-its-kind vision-RF system for cross-modal multi-person ReID at the\nsame time. Firstly, to address the fundamental inter-modality discrepancy, we\npropose a novel signature synthesis algorithm based on the observed specular\nreflection model of a human body. Secondly, an effective cross-modal deep\nmetric learning model is introduced to deal with interference caused by\nunsynchronized data across radars and cameras. Through extensive experiments in\nboth indoor and outdoor environments, we demonstrate that our proposed system\nis able to achieve ~92.5% top-1 accuracy and ~97.5% top-5 accuracy out of 56\nvolunteers. We also show that our proposed system is able to robustly\nreidentify subjects even when multiple subjects are present in the sensors'\nfield of view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1\">Dongjiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenchao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPSN: Superpixel Prototype Sampling Network for RGB-D Salient Object Detection. (arXiv:2207.07898v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07898","description":"<p>RGB-D salient object detection (SOD) has been in the spotlight recently\nbecause it is an important preprocessing operation for various vision tasks.\nHowever, despite advances in deep learning-based methods, RGB-D SOD is still\nchallenging due to the large domain gap between an RGB image and the depth map\nand low-quality depth maps. To solve this problem, we propose a novel\nsuperpixel prototype sampling network (SPSN) architecture. The proposed model\nsplits the input RGB image and depth map into component superpixels to generate\ncomponent prototypes. We design a prototype sampling network so that the\nnetwork only samples prototypes corresponding to salient objects. In addition,\nwe propose a reliance selection module to recognize the quality of each RGB and\ndepth feature map and adaptively weight them in proportion to their\nreliability. The proposed method makes the model robust to inconsistencies\nbetween RGB images and depth maps and eliminates the influence of non-salient\nobjects. Our method is evaluated on five popular datasets, achieving\nstate-of-the-art performance. We prove the effectiveness of the proposed method\nthrough comparative experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chaewon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Suhwan Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Adaptive Reasoning for Monocular 3D Multi-Person Pose Estimation. (arXiv:2207.07900v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07900","description":"<p>Inter-person occlusion and depth ambiguity make estimating the 3D poses of\nmonocular multiple persons as camera-centric coordinates a challenging problem.\nTypical top-down frameworks suffer from high computational redundancy with an\nadditional detection stage. By contrast, the bottom-up methods enjoy low\ncomputational costs as they are less affected by the number of humans. However,\nmost existing bottom-up methods treat camera-centric 3D human pose estimation\nas two unrelated subtasks: 2.5D pose estimation and camera-centric depth\nestimation. In this paper, we propose a unified model that leverages the mutual\nbenefits of both these subtasks. Within the framework, a robust structured 2.5D\npose estimation is designed to recognize inter-person occlusion based on depth\nrelationships. Additionally, we develop an end-to-end geometry-aware depth\nreasoning method that exploits the mutual benefits of both 2.5D pose and\ncamera-centric root depths. This method first uses 2.5D pose and geometry\ninformation to infer camera-centric root depths in a forward pass, and then\nexploits the root depths to further improve representation learning of 2.5D\npose estimation in a backward pass. Further, we designed an adaptive fusion\nscheme that leverages both visual perception and body geometry to alleviate\ninherent depth ambiguity issues. Extensive experiments demonstrate the\nsuperiority of our proposed model over a wide range of bottom-up methods. Our\naccuracy is even competitive with top-down counterparts. Notably, our model\nruns much faster than existing bottom-up and top-down methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juze Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Ye Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-branch Hybrid Learning Network for Unbiased Scene Graph Generation. (arXiv:2207.07913v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07913","description":"<p>The current studies of Scene Graph Generation (SGG) focus on solving the\nlong-tailed problem for generating unbiased scene graphs. However, most\nde-biasing methods overemphasize the tail predicates and underestimate head\nones throughout training, thereby wrecking the representation ability of head\npredicate features. Furthermore, these impaired features from head predicates\nharm the learning of tail predicates. In fact, the inference of tail predicates\nheavily depends on the general patterns learned from head ones, e.g., \"standing\non\" depends on \"on\". Thus, these de-biasing SGG methods can neither achieve\nexcellent performance on tail predicates nor satisfying behaviors on head ones.\nTo address this issue, we propose a Dual-branch Hybrid Learning network (DHL)\nto take care of both head predicates and tail ones for SGG, including a\nCoarse-grained Learning Branch (CLB) and a Fine-grained Learning Branch (FLB).\nSpecifically, the CLB is responsible for learning expertise and robust features\nof head predicates, while the FLB is expected to predict informative tail\npredicates. Furthermore, DHL is equipped with a Branch Curriculum Schedule\n(BCS) to make the two branches work well together. Experiments show that our\napproach achieves a new state-of-the-art performance on VG and GQA datasets and\nmakes a trade-off between the performance of tail predicates and head ones.\nMoreover, extensive experiments on two downstream tasks (i.e., Image Captioning\nand Sentence-to-Graph Retrieval) further verify the generalization and\npracticability of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chaofan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xinyu Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_P/0/1/0/all/0/1\">Pengpeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1\">Abdulmotaleb El Saddik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative Kernel Convolution Network for Multi-Label Ophthalmic Disease Detection on Imbalanced Fundus Image Dataset. (arXiv:2207.07918v1 [eess.IV])","link":"http://arxiv.org/abs/2207.07918","description":"<p>It is feasible to recognize the presence and seriousness of eye disease by\ninvestigating the progressions in retinal biological structure. Fundus\nexamination is a diagnostic procedure to examine the biological structure and\nanomaly of the eye. Ophthalmic diseases like glaucoma, diabetic retinopathy,\nand cataract are the main reason for visual impairment around the world. Ocular\nDisease Intelligent Recognition (ODIR-5K) is a benchmark structured fundus\nimage dataset utilized by researchers for multi-label multi-disease\nclassification of fundus images. This work presents a discriminative kernel\nconvolution network (DKCNet), which explores discriminative region-wise\nfeatures without adding extra computational cost. DKCNet is composed of an\nattention block followed by a squeeze and excitation (SE) block. The attention\nblock takes features from the backbone network and generates discriminative\nfeature attention maps. The SE block takes the discriminative feature maps and\nimproves channel interdependencies. Better performance of DKCNet is observed\nwith InceptionResnet backbone network for multi-label classification of ODIR-5K\nfundus images with 96.08 AUC, 94.28 F1-score and 0.81 kappa score. The proposed\nmethod splits the common target label for an eye pair based on the diagnostic\nkeyword. Based on these labels oversampling and undersampling is done to\nresolve class imbalance. To check the biasness of proposed model towards\ntraining data, the model trained on ODIR dataset is tested on three publicly\navailable benchmark datasets. It is found to give good performance on\ncompletely unseen fundus images also.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhati_A/0/1/0/all/0/1\">Amit Bhati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gour_N/0/1/0/all/0/1\">Neha Gour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khanna_P/0/1/0/all/0/1\">Pritee Khanna</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ojha_A/0/1/0/all/0/1\">Aparajita Ojha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable vision transformer enabled convolutional neural network for plant disease identification: PlantXViT. (arXiv:2207.07919v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07919","description":"<p>Plant diseases are the primary cause of crop losses globally, with an impact\non the world economy. To deal with these issues, smart agriculture solutions\nare evolving that combine the Internet of Things and machine learning for early\ndisease detection and control. Many such systems use vision-based machine\nlearning methods for real-time disease detection and diagnosis. With the\nadvancement in deep learning techniques, new methods have emerged that employ\nconvolutional neural networks for plant disease detection and identification.\nAnother trend in vision-based deep learning is the use of vision transformers,\nwhich have proved to be powerful models for classification and other problems.\nHowever, vision transformers have rarely been investigated for plant pathology\napplications. In this study, a Vision Transformer enabled Convolutional Neural\nNetwork model called \"PlantXViT\" is proposed for plant disease identification.\nThe proposed model combines the capabilities of traditional convolutional\nneural networks with the Vision Transformers to efficiently identify a large\nnumber of plant diseases for several crops. The proposed model has a\nlightweight structure with only 0.8 million trainable parameters, which makes\nit suitable for IoT-based smart agriculture services. The performance of\nPlantXViT is evaluated on five publicly available datasets. The proposed\nPlantXViT network performs better than five state-of-the-art methods on all\nfive datasets. The average accuracy for recognising plant diseases is shown to\nexceed 93.55%, 92.59%, and 98.33% on Apple, Maize, and Rice datasets,\nrespectively, even under challenging background conditions. The efficiency in\nterms of explainability of the proposed model is evaluated using\ngradient-weighted class activation maps and Local Interpretable Model Agnostic\nExplanation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_P/0/1/0/all/0/1\">Poornima Singh Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_P/0/1/0/all/0/1\">Pritee Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheorey_T/0/1/0/all/0/1\">Tanuja Sheorey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Aparajita Ojha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN-based Euler's Elastica Inpainting with Deep Energy and Deep Image Prior. (arXiv:2207.07921v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07921","description":"<p>Euler's elastica constitute an appealing variational image inpainting model.\nIt minimises an energy that involves the total variation as well as the level\nline curvature. These components are transparent and make it attractive for\nshape completion tasks. However, its gradient flow is a singular, anisotropic,\nand nonlinear PDE of fourth order, which is numerically challenging: It is\ndifficult to find efficient algorithms that offer sharp edges and good rotation\ninvariance. As a remedy, we design the first neural algorithm that simulates\ninpainting with Euler's Elastica. We use the deep energy concept which employs\nthe variational energy as neural network loss. Furthermore, we pair it with a\ndeep image prior where the network architecture itself acts as a prior. This\nyields better inpaintings by steering the optimisation trajectory closer to the\ndesired solution. Our results are qualitatively on par with state-of-the-art\nalgorithms on elastica-based shape completion. They combine good rotation\ninvariance with sharp edges. Moreover, we benefit from the high efficiency and\neffortless parallelisation within a neural framework. Our neural elastica\napproach only requires 3x3 central difference stencils. It is thus much simpler\nthan other well-performing algorithms for elastica inpainting. Last but not\nleast, it is unsupervised as it requires no ground truth training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schrader_K/0/1/0/all/0/1\">Karl Schrader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alt_T/0/1/0/all/0/1\">Tobias Alt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weickert_J/0/1/0/all/0/1\">Joachim Weickert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ertel_M/0/1/0/all/0/1\">Michael Ertel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Quality-aware Dynamic Memory for Video Object Segmentation. (arXiv:2207.07922v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07922","description":"<p>Recently, several spatial-temporal memory-based methods have verified that\nstoring intermediate frames and their masks as memory are helpful to segment\ntarget objects in videos. However, they mainly focus on better matching between\nthe current frame and the memory frames without explicitly paying attention to\nthe quality of the memory. Therefore, frames with poor segmentation masks are\nprone to be memorized, which leads to a segmentation mask error accumulation\nproblem and further affect the segmentation performance. In addition, the\nlinear increase of memory frames with the growth of frame number also limits\nthe ability of the models to handle long videos. To this end, we propose a\nQuality-aware Dynamic Memory Network (QDMN) to evaluate the segmentation\nquality of each frame, allowing the memory bank to selectively store accurately\nsegmented frames to prevent the error accumulation problem. Then, we combine\nthe segmentation quality with temporal consistency to dynamically update the\nmemory bank to improve the practicability of the models. Without any bells and\nwhistles, our QDMN achieves new state-of-the-art performance on both DAVIS and\nYouTube-VOS benchmarks. Moreover, extensive experiments demonstrate that the\nproposed Quality Assessment Module (QAM) can be applied to memory-based methods\nas generic plugins and significantly improves performance. Our source code is\navailable at https://github.com/workforai/QDMN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Ran Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Fei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Weihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Lightweight Super-Resolution with Dual Regression Learning. (arXiv:2207.07929v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07929","description":"<p>Deep neural networks have exhibited remarkable performance in image\nsuper-resolution (SR) tasks by learning a mapping from low-resolution (LR)\nimages to high-resolution (HR) images. However, the SR problem is typically an\nill-posed problem and existing methods would come with several limitations.\nFirst, the possible mapping space of SR can be extremely large since there may\nexist many different HR images that can be downsampled to the same LR image. As\na result, it is hard to directly learn a promising SR mapping from such a large\nspace. Second, it is often inevitable to develop very large models with\nextremely high computational cost to yield promising SR performance. In\npractice, one can use model compression techniques to obtain compact models by\nreducing model redundancy. Nevertheless, it is hard for existing model\ncompression methods to accurately identify the redundant components due to the\nextremely large SR mapping space. To alleviate the first challenge, we propose\na dual regression learning scheme to reduce the space of possible SR mappings.\nSpecifically, in addition to the mapping from LR to HR images, we learn an\nadditional dual regression mapping to estimate the downsampling kernel and\nreconstruct LR images. In this way, the dual mapping acts as a constraint to\nreduce the space of possible mappings. To address the second challenge, we\npropose a lightweight dual regression compression method to reduce model\nredundancy in both layer-level and channel-level based on channel pruning.\nSpecifically, we first develop a channel number search method that minimizes\nthe dual regression loss to determine the redundancy of each layer. Given the\nsearched channel numbers, we further exploit the dual regression manner to\nevaluate the importance of channels and prune the redundant ones. Extensive\nexperiments show the effectiveness of our method in obtaining accurate and\nefficient SR models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiezhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zeshuai Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Keypoint Detector and Descriptor for Retinal Image Matching. (arXiv:2207.07932v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07932","description":"<p>For retinal image matching (RIM), we propose SuperRetina, the first\nend-to-end method with jointly trainable keypoint detector and descriptor.\nSuperRetina is trained in a novel semi-supervised manner. A small set of\n(nearly 100) images are incompletely labeled and used to supervise the network\nto detect keypoints on the vascular tree. To attack the incompleteness of\nmanual labeling, we propose Progressive Keypoint Expansion to enrich the\nkeypoint labels at each training epoch. By utilizing a keypoint-based improved\ntriplet loss as its description loss, SuperRetina produces highly\ndiscriminative descriptors at full input image size. Extensive experiments on\nmultiple real-world datasets justify the viability of SuperRetina. Even with\nmanual labeling replaced by auto labeling and thus making the training process\nfully manual-annotation free, SuperRetina compares favorably against a number\nof strong baselines for two RIM tasks, i.e. image registration and identity\nverification. SuperRetina will be open source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiazhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Q/0/1/0/all/0/1\">Qijie Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">Dayong Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistency of Implicit and Explicit Features Matters for Monocular 3D Object Detection. (arXiv:2207.07933v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07933","description":"<p>Monocular 3D object detection is a common solution for low-cost autonomous\nagents to perceive their surrounding environment. Monocular detection has\nprogressed into two categories: (1)Direct methods that infer 3D bounding boxes\ndirectly from a frontal-view image; (2)3D intermedia representation methods\nthat map image features to 3D space for subsequent 3D detection. The second\ncategory is standing out not only because 3D detection forges ahead at the\nmercy of more meaningful and representative features, but because of emerging\nSOTA end-to-end prediction and planning paradigms that require a\nbird's-eye-view feature map from a perception pipeline. However, in\ntransforming to 3D representation, these methods do not guarantee that objects'\nimplicit orientations and locations in latent space are consistent with those\nexplicitly observed in Euclidean space, which will hurt model performance.\nHence, we argue that the consistency of implicit and explicit features matters\nand present a novel monocular detection method, named CIEF, with the first\norientation-aware image backbone to eliminate the disparity of implicit and\nexplicit features in subsequent 3D representation. As a second contribution, we\nintroduce a ray attention mechanism. In contrast to previous methods that\nrepeat features along the projection ray or rely on another intermedia frustum\npoint cloud, we directly transform image features to voxel representations with\nwell-localized features. We also propose a handcrafted gaussian positional\nencoding function that outperforms the sinusoidal encoding function but\nmaintains the benefit of being continuous. CIEF ranked 1st among all reported\nmethods on both 3D and BEV detection benchmark of KITTI at submission time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Ling Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuyang Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Attribute Modeling for Face Super-Resolution. (arXiv:2207.07945v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07945","description":"<p>When a high-resolution (HR) image is degraded into a low-resolution (LR)\nimage, the image loses some of the existing information. Consequently, multiple\nHR images can correspond to the LR image. Most of the existing methods do not\nconsider the uncertainty caused by the stochastic attribute, which can only be\nprobabilistically inferred. Therefore, the predicted HR images are often blurry\nbecause the network tries to reflect all possibilities in a single output\nimage. To overcome this limitation, this paper proposes a novel face\nsuper-resolution (SR) scheme to take into the uncertainty by stochastic\nmodeling. Specifically, the information in LR images is separately encoded into\ndeterministic and stochastic attributes. Furthermore, an Input Conditional\nAttribute Predictor is proposed and separately trained to predict the partially\nalive stochastic attributes from only the LR images. Extensive evaluation shows\nthat the proposed method successfully reduces the uncertainty in the learning\nprocess and outperforms the existing state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hanbyel Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yekang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jaemyung Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Level Set-Based Camera Pose Estimation From Multiple 2D/3D Ellipse-Ellipsoid Correspondences. (arXiv:2207.07953v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07953","description":"<p>In this paper, we propose an object-based camera pose estimation from a\nsingle RGB image and a pre-built map of objects, represented with ellipsoidal\nmodels. We show that contrary to point correspondences, the definition of a\ncost function characterizing the projection of a 3D object onto a 2D object\ndetection is not straightforward. We develop an ellipse-ellipse cost based on\nlevel sets sampling, demonstrate its nice properties for handling partially\nvisible objects and compare its performance with other common metrics. Finally,\nwe show that the use of a predictive uncertainty on the detected ellipses\nallows a fair weighting of the contribution of the correspondences which\nimproves the computed pose. The code is released at\nhttps://gitlab.inria.fr/tangram/level-set-based-camera-pose-estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zins_M/0/1/0/all/0/1\">Matthieu Zins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_G/0/1/0/all/0/1\">Gilles Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1\">Marie-Odile Berger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn-to-Decompose: Cascaded Decomposition Network for Cross-Domain Few-Shot Facial Expression Recognition. (arXiv:2207.07973v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07973","description":"<p>Most existing compound facial expression recognition (FER) methods rely on\nlarge-scale labeled compound expression data for training. However, collecting\nsuch data is labor-intensive and time-consuming. In this paper, we address the\ncompound FER task in the cross-domain few-shot learning (FSL) setting, which\nrequires only a few samples of compound expressions in the target domain.\nSpecifically, we propose a novel cascaded decomposition network (CDNet), which\ncascades several learn-to-decompose modules with shared parameters based on a\nsequential decomposition mechanism, to obtain a transferable feature space. To\nalleviate the overfitting problem caused by limited base classes in our task, a\npartial regularization strategy is designed to effectively exploit the best of\nboth episodic training and batch training. By training across similar tasks on\nmultiple basic expression datasets, CDNet learns the ability of\nlearn-to-decompose that can be easily adapted to identify unseen compound\nexpressions. Extensive experiments on both in-the-lab and in-the-wild compound\nexpression datasets demonstrate the superiority of our proposed CDNet against\nseveral state-of-the-art FSL methods. Code is available at:\nhttps://github.com/zouxinyi0625/CDNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xinyi Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanzi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Guided Bidirectional Attention Network for Human-Object Interaction Detection. (arXiv:2207.07979v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07979","description":"<p>Human Object Interaction (HOI) detection is a challenging task that requires\nto distinguish the interaction between a human-object pair. Attention based\nrelation parsing is a popular and effective strategy utilized in HOI. However,\ncurrent methods execute relation parsing in a \"bottom-up\" manner. We argue that\nthe independent use of the bottom-up parsing strategy in HOI is\ncounter-intuitive and could lead to the diffusion of attention. Therefore, we\nintroduce a novel knowledge-guided top-down attention into HOI, and propose to\nmodel the relation parsing as a \"look and search\" process: execute\nscene-context modeling (i.e. look), and then, given the knowledge of the target\npair, search visual clues for the discrimination of the interaction between the\npair. We implement the process via unifying the bottom-up and top-down\nattention in a single encoder-decoder based model. The experimental results\nshow that our model achieves competitive performance on the V-COCO and HICO-DET\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jingjia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baixiang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras. (arXiv:2207.08000v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08000","description":"<p>We propose DiffuStereo, a novel system using only sparse cameras (8 in this\nwork) for high-quality 3D human reconstruction. At its core is a novel\ndiffusion-based stereo module, which introduces diffusion models, a type of\npowerful generative models, into the iterative stereo matching network. To this\nend, we design a new diffusion kernel and additional stereo constraints to\nfacilitate stereo matching and depth estimation in the network. We further\npresent a multi-level stereo network architecture to handle high-resolution (up\nto 4k) inputs without requiring unaffordable memory footprint. Given a set of\nsparse-view color images of a human, the proposed multi-level diffusion-based\nstereo network can produce highly accurate depth maps, which are then converted\ninto a high-quality 3D human model through an efficient multi-view fusion\nstrategy. Overall, our method enables automatic reconstruction of human models\nwith quality on par to high-end dense-view camera rigs, and this is achieved\nusing a much more light-weight hardware setup. Experiments show that our method\noutperforms state-of-the-art methods by a large margin both qualitatively and\nquantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Ruizhi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zerong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jingxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVGraph: Learning Semantic Graphs from Instructional Videos. (arXiv:2207.08001v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08001","description":"<p>In this work, we focus on generating graphical representations of noisy,\ninstructional videos for video understanding. We propose a self-supervised,\ninterpretable approach that does not require any annotations for graphical\nrepresentations, which would be expensive and time consuming to collect. We\nattempt to overcome \"black box\" learning limitations by presenting Semantic\nVideo Graph or SVGraph, a multi-modal approach that utilizes narrations for\nsemantic interpretability of the learned graphs. SVGraph 1) relies on the\nagreement between multiple modalities to learn a unified graphical structure\nwith the help of cross-modal attention and 2) assigns semantic interpretation\nwith the help of Semantic-Assignment, which captures the semantics from video\nnarration. We perform experiments on multiple datasets and demonstrate the\ninterpretability of SVGraph in semantic graph learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schiappa_M/0/1/0/all/0/1\">Madeline C. Schiappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh S. Rawat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSMTL++: Revisiting Self-Supervised Multi-Task Learning for Video Anomaly Detection. (arXiv:2207.08003v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08003","description":"<p>A self-supervised multi-task learning (SSMTL) framework for video anomaly\ndetection was recently introduced in literature. Due to its highly accurate\nresults, the method attracted the attention of many researchers. In this work,\nwe revisit the self-supervised multi-task learning framework, proposing several\nupdates to the original method. First, we study various detection methods, e.g.\nbased on detecting high-motion regions using optical flow or background\nsubtraction, since we believe the currently used pre-trained YOLOv3 is\nsuboptimal, e.g. objects in motion or objects from unknown classes are never\ndetected. Second, we modernize the 3D convolutional backbone by introducing\nmulti-head self-attention modules, inspired by the recent success of vision\ntransformers. As such, we alternatively introduce both 2D and 3D convolutional\nvision transformer (CvT) blocks. Third, in our attempt to further improve the\nmodel, we study additional self-supervised learning tasks, such as predicting\nsegmentation maps through knowledge distillation, solving jigsaw puzzles,\nestimating body pose through knowledge distillation, predicting masked regions\n(inpainting), and adversarial learning with pseudo-anomalies. We conduct\nexperiments to assess the performance impact of the introduced changes. Upon\nfinding more promising configurations of the framework, dubbed SSMTL++v1 and\nSSMTL++v2, we extend our preliminary experiments to more data sets,\ndemonstrating that our performance gains are consistent across all data sets.\nIn most cases, our results on Avenue, ShanghaiTech and UBnormal raise the\nstate-of-the-art performance to a new level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbalau_A/0/1/0/all/0/1\">Antonio Barbalau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dueholm_J/0/1/0/all/0/1\">Jacob Dueholm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_B/0/1/0/all/0/1\">Bharathkumar Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrollahi_K/0/1/0/all/0/1\">Kamal Nasrollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1\">Thomas B. Moeslund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monitoring Vegetation From Space at Extremely Fine Resolutions via Coarsely-Supervised Smooth U-Net. (arXiv:2207.08022v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08022","description":"<p>Monitoring vegetation productivity at extremely fine resolutions is valuable\nfor real-world agricultural applications, such as detecting crop stress and\nproviding early warning of food insecurity. Solar-Induced Chlorophyll\nFluorescence (SIF) provides a promising way to directly measure plant\nproductivity from space. However, satellite SIF observations are only available\nat a coarse spatial resolution, making it impossible to monitor how individual\ncrop types or farms are doing. This poses a challenging coarsely-supervised\nregression (or downscaling) task; at training time, we only have SIF labels at\na coarse resolution (3km), but we want to predict SIF at much finer spatial\nresolutions (e.g. 30m, a 100x increase). We also have additional\nfine-resolution input features, but the relationship between these features and\nSIF is unknown. To address this, we propose Coarsely-Supervised Smooth U-Net\n(CS-SUNet), a novel method for this coarse supervision setting. CS-SUNet\ncombines the expressive power of deep convolutional networks with novel\nregularization methods based on prior knowledge (such as a smoothness loss)\nthat are crucial for preventing overfitting. Experiments show that CS-SUNet\nresolves fine-grained variations in SIF more accurately than existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Joshua Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Di Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jiaming Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Ying Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1\">Carla P. Gomes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAVA: Language Audio Vision Alignment for Contrastive Video Pre-Training. (arXiv:2207.08024v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08024","description":"<p>Generating representations of video data is of key importance in advancing\nthe field of machine perception. Most current techniques rely on hand-annotated\ndata, which can be difficult to work with, expensive to generate, and hard to\nscale. In this work, we propose a novel learning approach based on contrastive\nlearning, LAVA, which is capable of learning joint language, audio, and video\nrepresentations in a self-supervised manner. We pre-train LAVA on the Kinetics\n700 dataset using transformer encoders to learn representations for each\nmodality. We then demonstrate that LAVA performs competitively with the current\nstate-of-the-art self-supervised and weakly-supervised pretraining techniques\non UCF-101 and HMDB-51 video action recognition while using a fraction of the\nunlabeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gurram_S/0/1/0/all/0/1\">Sumanth Gurram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1\">Andy Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1\">David Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1\">John Canny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of liver cancer detection based on image processing. (arXiv:2207.08032v1 [eess.IV])","link":"http://arxiv.org/abs/2207.08032","description":"<p>Medical imaging is the most important tool for detecting complications in the\ninner body of medicine. Nowadays, with the development of image processing\ntechnology as well as changing the size of photos to higher resolution images\nin the field of digital medical imaging, there is an efficient and accurate\nsystem for segmenting this. Real-world images that for a variety of reasons\nhave poor heterogeneity, noise and contrast are essential. Digital image\nsegmentation in medicine is used for diagnostic and therapeutic analysis, which\nis very helpful for physicians. In this study, we aim at liver cancer\nphotographs, which aim to more accurately detect the lesion or tumor of the\nliver because accurate and timely detection of the tumor is very important in\nthe survival and life of the patient.The aim of this paper is to simplify the\nobnoxious study problems related to the study of MR images. The liver is the\nsecond organ most generic involved by metastatic disease being liver cancer one\nof the prominent causes of death worldwide. Without healthy liver a person\ncannot survive. It is life threatening disease which is very challenging\nperceptible for both medical and engineering technologists. Medical image\nprocessing is used as a non-invasive method to detect tumours. The chances of\nsurvival having liver Tumor highly depends on early detection of Tumor and then\nclassification as cancerous and noncancerous tumours. Image processing\ntechniques for automatic detection of brain are includes pre-processing and\nenhancement, image segmentation, classification and volume calculation, Poly\ntechniques have been developed for the detection of liver Tumor and different\nliver toM oR detection algorithms and methodologies utilized for Tumor\ndiagnosis. Novel methodology for the detection and diagnosis of liver Tumor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Moghimhanjani_M/0/1/0/all/0/1\">Mahmoudreza Moghimhanjani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taghavirashidizadeh_A/0/1/0/all/0/1\">Ali Taghavirashidizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progress and limitations of deep networks to recognize objects in unusual poses. (arXiv:2207.08034v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08034","description":"<p>Deep networks should be robust to rare events if they are to be successfully\ndeployed in high-stakes real-world applications (e.g., self-driving cars). Here\nwe study the capability of deep networks to recognize objects in unusual poses.\nWe create a synthetic dataset of images of objects in unusual orientations, and\nevaluate the robustness of a collection of 38 recent and competitive deep\nnetworks for image classification. We show that classifying these images is\nstill a challenge for all networks tested, with an average accuracy drop of\n29.5% compared to when the objects are presented upright. This brittleness is\nlargely unaffected by various network design choices, such as training losses\n(e.g., supervised vs. self-supervised), architectures (e.g., convolutional\nnetworks vs. transformers), dataset modalities (e.g., images vs. image-text\npairs), and data-augmentation schemes. However, networks trained on very large\ndatasets substantially outperform others, with the best network\ntested$\\unicode{x2014}$Noisy Student EfficentNet-L2 trained on\nJFT-300M$\\unicode{x2014}$showing a relatively small accuracy drop of only 14.5%\non unusual poses. Nevertheless, a visual inspection of the failures of Noisy\nStudent reveals a remaining gap in robustness with the human visual system.\nFurthermore, combining multiple object\ntransformations$\\unicode{x2014}$3D-rotations and\nscaling$\\unicode{x2014}$further degrades the performance of all networks.\nAltogether, our results provide another measurement of the robustness of deep\nnetworks that is important to consider when using them in the real world. Code\nand datasets are available at https://github.com/amro-kamal/ObjectPose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1\">Amro Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deny_S/0/1/0/all/0/1\">St&#xe9;phane Deny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single MR Image Super-Resolution using Generative Adversarial Network. (arXiv:2207.08036v1 [eess.IV])","link":"http://arxiv.org/abs/2207.08036","description":"<p>Spatial resolution of medical images can be improved using super-resolution\nmethods. Real Enhanced Super Resolution Generative Adversarial Network\n(Real-ESRGAN) is one of the recent effective approaches utilized to produce\nhigher resolution images, given input images of lower resolution. In this\npaper, we apply this method to enhance the spatial resolution of 2D MR images.\nIn our proposed approach, we slightly modify the structure of the Real-ESRGAN\nto train 2D Magnetic Resonance images (MRI) taken from the Brain Tumor\nSegmentation Challenge (BraTS) 2018 dataset. The obtained results are validated\nqualitatively and quantitatively by computing SSIM (Structural Similarity Index\nMeasure), NRMSE (Normalized Root Mean Square Error), MAE (Mean Absolute Error),\nand VIF (Visual Information Fidelity) values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rashid_S/0/1/0/all/0/1\">Shawkh Ibne Rashid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shakibapour_E/0/1/0/all/0/1\">Elham Shakibapour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebrahimi_M/0/1/0/all/0/1\">Mehran Ebrahimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIMBA: Discretely Masked Black-Box Attack in Single Object Tracking. (arXiv:2207.08044v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08044","description":"<p>The adversarial attack can force a CNN-based model to produce an incorrect\noutput by craftily manipulating human-imperceptible input. Exploring such\nperturbations can help us gain a deeper understanding of the vulnerability of\nneural networks, and provide robustness to deep learning against miscellaneous\nadversaries. Despite extensive studies focusing on the robustness of image,\naudio, and NLP, works on adversarial examples of visual object tracking --\nespecially in a black-box manner -- are quite lacking. In this paper, we\npropose a novel adversarial attack method to generate noises for single object\ntracking under black-box settings, where perturbations are merely added on\ninitial frames of tracking sequences, which is difficult to be noticed from the\nperspective of a whole video clip. Specifically, we divide our algorithm into\nthree components and exploit reinforcement learning for localizing important\nframe patches precisely while reducing unnecessary computational queries\noverhead. Compared to existing techniques, our method requires fewer queries on\ninitialized frames of a video to manipulate competitive or even better attack\nperformance. We test our algorithm in both long-term and short-term datasets,\nincluding OTB100, VOT2018, UAV123, and LaSOT. Extensive experiments demonstrate\nthe effectiveness of our method on three mainstream types of trackers:\ndiscrimination, Siamese-based, and reinforcement learning-based trackers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiangyu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1\">Wenjie Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fieldsend_J/0/1/0/all/0/1\">Jonathan Fieldsend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDM:Visual Explanations for Neural Networks via Multiple Dynamic Mask. (arXiv:2207.08046v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08046","description":"<p>The active region lookup of a neural network tells us which regions the\nneural network focuses on when making a decision, which gives us a basis for\ninterpretability when the neural network makes a classification decision. We\npropose an algorithm Multiple Dynamic Mask(MDM), which is a general saliency\ngraph query method with interpretability of the inference process. Its proposal\nis based on an assumption: when a picture is input to a neural network that has\nbeen trained, the activation features related to classification will affect the\nclassification results of the neural network, and the features unrelated to\nclassification will hardly affect the classification results of the network.\nMDM: A learning-based end-to-end algorithm for finding regions of interest for\nneural network classification. It has the following advantages: 1. It has the\ninterpretability of the reasoning process. 2. It is universal, it can be used\nfor any neural network and does not depend on the internal structure of the\nneural network. 3. The search performance is better. Because the algorithm is\nbased on learning to generate masks and has the ability to adapt to different\ndata and networks, the performance is better than the method proposed in the\nprevious paper. For the MDM saliency map search algorithm, we experimentally\ncompared the performance indicators of various saliency map search methods and\nthe MDM with ResNet and DenseNet as the trained neural networks. The search\neffect performance of the MDM reached the state of the art. We applied the MDM\nto the interpretable neural network ProtoPNet and XProtoNet, which improved the\ninterpretability of the model and the prototype search performance. We\nvisualize the performance of convolutional neural architecture and Transformer\narchitecture on saliency map search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yitao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longzhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lianghua He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery. (arXiv:2207.08051v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08051","description":"<p>Unsupervised pre-training methods for large vision models have shown to\nenhance performance on downstream supervised tasks. Developing similar\ntechniques for satellite imagery presents significant opportunities as\nunlabelled data is plentiful and the inherent temporal and multi-spectral\nstructure provides avenues to further improve existing pre-training strategies.\nIn this paper, we present SatMAE, a pre-training framework for temporal or\nmulti-spectral satellite imagery based on Masked Autoencoder (MAE). To leverage\ntemporal information, we include a temporal embedding along with independently\nmasking image patches across time. In addition, we demonstrate that encoding\nmulti-spectral data as groups of bands with distinct spectral positional\nencodings is beneficial. Our approach yields strong improvements over previous\nstate-of-the-art techniques, both in terms of supervised learning performance\non benchmark datasets (up to $\\uparrow$ 7\\%), and transfer learning performance\non downstream remote sensing tasks, including land cover classification (up to\n$\\uparrow$ 14\\%) and semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1\">Yezhen Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1\">Samar Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chenlin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Patrick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozi_E/0/1/0/all/0/1\">Erik Rozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yutong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1\">Marshall Burke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1\">David B. Lobell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Humans in RGB-D Data with CNNs. (arXiv:2207.08064v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08064","description":"<p>We address the problem of people detection in RGB-D data where we leverage\ndepth information to develop a region-of-interest (ROI) selection method that\nprovides proposals to two color and depth CNNs. To combine the detections\nproduced by the two CNNs, we propose a novel fusion approach based on the\ncharacteristics of depth images. We also present a new depth-encoding scheme,\nwhich not only encodes depth images into three channels but also enhances the\ninformation for classification. We conduct experiments on a publicly available\nRGB-D people dataset and show that our approach outperforms the baseline models\nthat only use RGB data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paiement_A/0/1/0/all/0/1\">Adeline Paiement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1\">Majid Mirmehdi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Instance Normalization on Fine-Grained Control for Sketch-Based Face Image Generation. (arXiv:2207.08072v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08072","description":"<p>Sketching is an intuitive and effective way for content creation. While\nsignificant progress has been made for photorealistic image generation by using\ngenerative adversarial networks, it remains challenging to take a fine-grained\ncontrol on synthetic content. The instance normalization layer, which is widely\nadopted in existing image translation networks, washes away details in the\ninput sketch and leads to loss of precise control on the desired shape of the\ngenerated face images. In this paper, we comprehensively investigate the effect\nof instance normalization on generating photorealistic face images from\nhand-drawn sketches. We first introduce a visualization approach to analyze the\nfeature embedding for sketches with a group of specific changes. Based on the\nvisual analysis, we modify the instance normalization layers in the baseline\nimage translation model. We elaborate a new set of hand-drawn sketches with 11\ncategories of specially designed changes and conduct extensive experimental\nanalysis. The results and user studies demonstrate that our method markedly\nimprove the quality of synthesized images and the conformance with user\nintention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhihua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuejin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performance degradation of ImageNet trained models by simple image transformations. (arXiv:2207.08079v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08079","description":"<p>ImageNet trained PyTorch models are generally preferred as the off-the-shelf\nmodels for direct use or for initialisation in most computer vision tasks. In\nthis paper, we simply test a representative set of these convolution and\ntransformer based models under many simple image transformations like\nhorizontal shifting, vertical shifting, scaling, rotation, presence of Gaussian\nnoise, cutout, horizontal flip and vertical flip and report the performance\ndrop caused by such transformations. We find that even simple transformations\nlike rotating the image by 10{\\deg} or zooming in by 20% can reduce the top-1\naccuracy of models like ResNet152 by 1%+. The code is available at\nhttps://github.com/harshm121/imagenet-transformation-degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_H/0/1/0/all/0/1\">Harsh Maheshwari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Color Operators for Sequential Image Retouching. (arXiv:2207.08080v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08080","description":"<p>We propose a novel image retouching method by modeling the retouching process\nas performing a sequence of newly introduced trainable neural color operators.\nThe neural color operator mimics the behavior of traditional color operators\nand learns pixelwise color transformation while its strength is controlled by a\nscalar. To reflect the homomorphism property of color operators, we employ\nequivariant mapping and adopt an encoder-decoder structure which maps the\nnon-linear color transformation to a much simpler transformation (i.e.,\ntranslation) in a high dimensional space. The scalar strength of each neural\ncolor operator is predicted using CNN based strength predictors by analyzing\nglobal image statistics. Overall, our method is rather lightweight and offers\nflexible controls. Experiments and user studies on public datasets show that\nour method consistently achieves the best results compared with SOTA methods in\nboth quantitative measures and visual qualities. The code and data will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CATRE: Iterative Point Clouds Alignment for Category-level Object Pose Refinement. (arXiv:2207.08082v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08082","description":"<p>While category-level 9DoF object pose estimation has emerged recently,\nprevious correspondence-based or direct regression methods are both limited in\naccuracy due to the huge intra-category variances in object shape and color,\netc. Orthogonal to them, this work presents a category-level object pose and\nsize refiner CATRE, which is able to iteratively enhance pose estimate from\npoint clouds to produce accurate results. Given an initial pose estimate, CATRE\npredicts a relative transformation between the initial pose and ground truth by\nmeans of aligning the partially observed point cloud and an abstract shape\nprior. In specific, we propose a novel disentangled architecture being aware of\nthe inherent distinctions between rotation and translation/size estimation.\nExtensive experiments show that our approach remarkably outperforms\nstate-of-the-art methods on REAL275, CAMERA25, and LM benchmarks up to a speed\nof ~85.32Hz, and achieves competitive results on category-level tracking. We\nfurther demonstrate that CATRE can perform pose refinement on unseen category.\nCode and trained models are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Threat Model-Agnostic Adversarial Defense using Diffusion Models. (arXiv:2207.08089v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08089","description":"<p>Deep Neural Networks (DNNs) are highly sensitive to imperceptible malicious\nperturbations, known as adversarial attacks. Following the discovery of this\nvulnerability in real-world imaging and vision applications, the associated\nsafety concerns have attracted vast research attention, and many defense\ntechniques have been developed. Most of these defense methods rely on\nadversarial training (AT) -- training the classification network on images\nperturbed according to a specific threat model, which defines the magnitude of\nthe allowed modification. Although AT leads to promising results, training on a\nspecific threat model fails to generalize to other types of perturbations. A\ndifferent approach utilizes a preprocessing step to remove the adversarial\nperturbation from the attacked image. In this work, we follow the latter path\nand aim to develop a technique that leads to robust classifiers across various\nrealizations of threat models. To this end, we harness the recent advances in\nstochastic generative modeling, and means to leverage these for sampling from\nconditional distributions. Our defense relies on an addition of Gaussian i.i.d\nnoise to the attacked image, followed by a pretrained diffusion process -- an\narchitecture that performs a stochastic iterative process over a denoising\nnetwork, yielding a high perceptual quality denoised outcome. The obtained\nrobustness with this stochastic preprocessing step is validated through\nextensive experiments on the CIFAR-10 dataset, showing that our method\noutperforms the leading defense methods under various threat models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blau_T/0/1/0/all/0/1\">Tsachi Blau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1\">Roy Ganz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawar_B/0/1/0/all/0/1\">Bahjat Kawar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1\">Alex Bronstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Temporal Spatial Cubism for Cross-Dataset Skeleton-based Action Recognition. (arXiv:2207.08095v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08095","description":"<p>Rapid progress and superior performance have been achieved for skeleton-based\naction recognition recently. In this article, we investigate this problem under\na cross-dataset setting, which is a new, pragmatic, and challenging task in\nreal-world scenarios. Following the unsupervised domain adaptation (UDA)\nparadigm, the action labels are only available on a source dataset, but\nunavailable on a target dataset in the training stage. Different from the\nconventional adversarial learning-based approaches for UDA, we utilize a\nself-supervision scheme to reduce the domain shift between two skeleton-based\naction datasets. Our inspiration is drawn from Cubism, an art genre from the\nearly 20th century, which breaks and reassembles the objects to convey a\ngreater context. By segmenting and permuting temporal segments or human body\nparts, we design two self-supervised learning classification tasks to explore\nthe temporal and spatial dependency of a skeleton-based action and improve the\ngeneralization ability of the model. We conduct experiments on six datasets for\nskeleton-based action recognition, including three large-scale datasets (NTU\nRGB+D, PKU-MMD, and Kinetics) where new cross-dataset settings and benchmarks\nare established. Extensive results demonstrate that our method outperforms\nstate-of-the-art approaches. The source codes of our model and all the compared\nmethods are available at https://github.com/shanice-l/st-cubism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yansong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xumin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BCS-Net: Boundary, Context and Semantic for Automatic COVID-19 Lung Infection Segmentation from CT Images. (arXiv:2207.08114v1 [eess.IV])","link":"http://arxiv.org/abs/2207.08114","description":"<p>The spread of COVID-19 has brought a huge disaster to the world, and the\nautomatic segmentation of infection regions can help doctors to make diagnosis\nquickly and reduce workload. However, there are several challenges for the\naccurate and complete segmentation, such as the scattered infection area\ndistribution, complex background noises, and blurred segmentation boundaries.\nTo this end, in this paper, we propose a novel network for automatic COVID-19\nlung infection segmentation from CT images, named BCS-Net, which considers the\nboundary, context, and semantic attributes. The BCS-Net follows an\nencoder-decoder architecture, and more designs focus on the decoder stage that\nincludes three progressively Boundary-Context-Semantic Reconstruction (BCSR)\nblocks. In each BCSR block, the attention-guided global context (AGGC) module\nis designed to learn the most valuable encoder features for decoder by\nhighlighting the important spatial and boundary locations and modeling the\nglobal context dependence. Besides, a semantic guidance (SG) unit generates the\nsemantic guidance map to refine the decoder features by aggregating multi-scale\nhigh-level features at the intermediate resolution. Extensive experiments\ndemonstrate that our proposed framework outperforms the existing competitors\nboth qualitatively and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cong_R/0/1/0/all/0/1\">Runmin Cong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Haowei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Q/0/1/0/all/0/1\">Qiuping Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haisheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FloLPIPS: A Bespoke Video Quality Metric for Frame Interpoation. (arXiv:2207.08119v1 [eess.IV])","link":"http://arxiv.org/abs/2207.08119","description":"<p>Video frame interpolation (VFI) serves as a useful tool for many video\nprocessing applications. Recently, it has also been applied in the video\ncompression domain for enhancing both conventional video codecs and\nlearning-based compression architectures. While there has been an increased\nfocus on the development of enhanced frame interpolation algorithms in recent\nyears, the perceptual quality assessment of interpolated content remains an\nopen field of research. In this paper, we present a bespoke full reference\nvideo quality metric for VFI, FloLPIPS, that builds on the popular perceptual\nimage quality metric, LPIPS, which captures the perceptual degradation in\nextracted image feature space. In order to enhance the performance of LPIPS for\nevaluating interpolated content, we re-designed its spatial feature aggregation\nstep by using the temporal distortion (through comparing optical flows) to\nweight the feature difference maps. Evaluated on the BVI-VFI database, which\ncontains 180 test sequences with various frame interpolation artefacts,\nFloLPIPS shows superior correlation performance (with statistical significance)\nwith subjective ground truth over 12 popular quality assessors. To facilitate\nfurther research in VFI quality assessment, our code is publicly available at\nhttps://danielism97.github.io/FloLPIPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Danier_D/0/1/0/all/0/1\">Duolikun Danier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1\">David Bull</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-free Unsupervised Domain Adaptation for Blind Image Quality Assessment. (arXiv:2207.08124v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08124","description":"<p>Existing learning-based methods for blind image quality assessment (BIQA) are\nheavily dependent on large amounts of annotated training data, and usually\nsuffer from a severe performance degradation when encountering the\ndomain/distribution shift problem. Thanks to the development of unsupervised\ndomain adaptation (UDA), some works attempt to transfer the knowledge from a\nlabel-sufficient source domain to a label-free target domain under domain shift\nwith UDA. However, it requires the coexistence of source and target data, which\nmight be impractical for source data due to the privacy or storage issues. In\nthis paper, we take the first step towards the source-free unsupervised domain\nadaptation (SFUDA) in a simple yet efficient manner for BIQA to tackle the\ndomain shift without access to the source data. Specifically, we cast the\nquality assessment task as a rating distribution prediction problem. Based on\nthe intrinsic properties of BIQA, we present a group of well-designed\nself-supervised objectives to guide the adaptation of the BN affine parameters\ntowards the target domain. Among them, minimizing the prediction entropy and\nmaximizing the batch prediction diversity aim to encourage more confident\nresults while avoiding the trivial solution. Besides, based on the observation\nthat the IQA rating distribution of single image follows the Gaussian\ndistribution, we apply Gaussian regularization to the predicted rating\ndistribution to make it more consistent with the nature of human scoring.\nExtensive experimental results under cross-domain scenarios demonstrated the\neffectiveness of our proposed method to mitigate the domain shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shukun An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E-NeRV: Expedite Neural Video Representation with Disentangled Spatial-Temporal Context. (arXiv:2207.08132v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08132","description":"<p>Recently, the image-wise implicit neural representation of videos, NeRV, has\ngained popularity for its promising results and swift speed compared to regular\npixel-wise implicit representations. However, the redundant parameters within\nthe network structure can cause a large model size when scaling up for\ndesirable performance. The key reason of this phenomenon is the coupled\nformulation of NeRV, which outputs the spatial and temporal information of\nvideo frames directly from the frame index input. In this paper, we propose\nE-NeRV, which dramatically expedites NeRV by decomposing the image-wise\nimplicit neural representation into separate spatial and temporal context.\nUnder the guidance of this new formulation, our model greatly reduces the\nredundant model parameters, while retaining the representation ability. We\nexperimentally find that our method can improve the performance to a large\nextent with fewer parameters, resulting in a more than $8\\times$ faster speed\non convergence. Code is available at https://github.com/kyleleey/E-NeRV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zizhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengmeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pi_H/0/1/0/all/0/1\">Huaijin Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kechun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jianbiao Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Out-of-domain GAN Inversion via Differential Activations. (arXiv:2207.08134v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08134","description":"<p>Despite the demonstrated editing capacity in the latent space of a pretrained\nGAN model, inverting real-world images is stuck in a dilemma that the\nreconstruction cannot be faithful to the original input. The main reason for\nthis is that the distributions between training and real-world data are\nmisaligned, and because of that, it is unstable of GAN inversion for real image\nediting. In this paper, we propose a novel GAN prior based editing framework to\ntackle the out-of-domain inversion problem with a composition-decomposition\nparadigm. In particular, during the phase of composition, we introduce a\ndifferential activation module for detecting semantic changes from a global\nperspective, \\ie, the relative gap between the features of edited and unedited\nimages. With the aid of the generated Diff-CAM mask, a coarse reconstruction\ncan intuitively be composited by the paired original and edited images. In this\nway, the attribute-irrelevant regions can be survived in almost whole, while\nthe quality of such an intermediate result is still limited by an unavoidable\nghosting effect. Consequently, in the decomposition phase, we further present a\nGAN prior based deghosting network for separating the final fine edited image\nfrom the coarse reconstruction. Extensive experiments exhibit superiorities\nover the state-of-the-art methods, in terms of qualitative and quantitative\nevaluations. The robustness and flexibility of our method is also validated on\nboth scenarios of single attribute and multi-attribute manipulations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haorui Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tianyi Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Weakly-Supervised Learning Methods for Classification and Localization in Histology Images: A Survey. (arXiv:1909.03354v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1909.03354","description":"<p>Using deep learning models to diagnose cancer from histology data presents\nseveral challenges. Cancer grading and localization of regions of interest\n(ROIs) in these images normally relies on both image- and pixel-level labels,\nthe latter requiring a costly annotation process. Deep weakly-supervised object\nlocalization (WSOL) methods provide different strategies for low-cost training\nof deep learning models. Using only image-class annotations, these methods can\nbe trained to classify an image, and yield class activation maps (CAMs) for ROI\nlocalization. This paper provides a review of state-of-art DL methods for WSOL.\nWe propose a taxonomy where these methods are divided into bottom-up and\ntop-down methods according to the information flow in models. Although the\nlatter have seen limited progress, recent bottom-up methods are currently\ndriving much progress with deep WSOL methods. Early works focused on designing\ndifferent spatial pooling functions. However, these methods reached limited\nlocalization accuracy, and unveiled a major limitation -- the under-activation\nof CAMs which leads to high false negative localization. Subsequent works aimed\nto alleviate this issue and recover complete object. Representative methods\nfrom our taxonomy are evaluated and compared in terms of classification and\nlocalization accuracy on two challenging histology datasets. Overall, the\nresults indicate poor localization performance, particularly for generic\nmethods that were initially designed to process natural images. Methods\ndesigned to address the challenges of histology data yielded good results.\nHowever, all methods suffer from high false positive/negative localization.\nFour key challenges are identified for the application of deep WSOL methods in\nhistology -- under/over activation of CAMs, sensitivity to thresholding, and\nmodel selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering. (arXiv:2009.09213v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.09213","description":"<p>The current high-fidelity generation and high-precision detection of DeepFake\nimages are at an arms race. We believe that producing DeepFakes that are highly\nrealistic and ``detection evasive'' can serve the ultimate goal of improving\nfuture generation DeepFake detection capabilities. In this paper, we propose a\nsimple yet powerful pipeline to reduce the artifact patterns of fake images\nwithout hurting image quality by performing implicit spatial-domain notch\nfiltering. We first demonstrate that frequency-domain notch filtering, although\nfamously shown to be effective in removing periodic noise in the spatial\ndomain, is infeasible for our task at hand due to manual designs required for\nthe notch filters. We, therefore, resort to a learning-based approach to\nreproduce the notch filtering effects, but solely in the spatial domain. We\nadopt a combination of adding overwhelming spatial noise for breaking the\nperiodic noise pattern and deep image filtering to reconstruct the noise-free\nfake images, and we name our method DeepNotch. Deep image filtering provides a\nspecialized filter for each pixel in the noisy image, producing filtered images\nwith high fidelity compared to their DeepFake counterparts. Moreover, we also\nuse the semantic information of the image to generate an adversarial guidance\nmap to add noise intelligently. Our large-scale evaluation on 3 representative\nstate-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)\nhas demonstrated that our technique significantly reduces the accuracy of these\n3 fake image detection methods, 36.79% on average and up to 97.02% in the best\ncase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yihao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1\">Geguang Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Adaptive Negative Envision for Few-Shot Open-Set Recognition. (arXiv:2012.13073v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.13073","description":"<p>We study the problem of few-shot open-set recognition (FSOR), which learns a\nrecognition system capable of both fast adaptation to new classes with limited\nlabeled examples and rejection of unknown negative samples. Traditional\nlarge-scale open-set methods have been shown ineffective for FSOR problem due\nto data limitation. Current FSOR methods typically calibrate few-shot\nclosed-set classifiers to be sensitive to negative samples so that they can be\nrejected via thresholding. However, threshold tuning is a challenging process\nas different FSOR tasks may require different rejection powers. In this paper,\nwe instead propose task-adaptive negative class envision for FSOR to integrate\nthreshold tuning into the learning process. Specifically, we augment the\nfew-shot closed-set classifier with additional negative prototypes generated\nfrom few-shot examples. By incorporating few-shot class correlations in the\nnegative generation process, we are able to learn dynamic rejection boundaries\nfor FSOR tasks. Besides, we extend our method to generalized few-shot open-set\nrecognition (GFSOR), which requires classification on both many-shot and\nfew-shot classes as well as rejection of negative samples. Extensive\nexperiments on public benchmarks validate our methods on both problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiawei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focal and Efficient IOU Loss for Accurate Bounding Box Regression. (arXiv:2101.08158v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.08158","description":"<p>In object detection, bounding box regression (BBR) is a crucial step that\ndetermines the object localization performance. However, we find that most\nprevious loss functions for BBR have two main drawbacks: (i) Both $\\ell_n$-norm\nand IOU-based loss functions are inefficient to depict the objective of BBR,\nwhich leads to slow convergence and inaccurate regression results. (ii) Most of\nthe loss functions ignore the imbalance problem in BBR that the large number of\nanchor boxes which have small overlaps with the target boxes contribute most to\nthe optimization of BBR. To mitigate the adverse effects caused thereby, we\nperform thorough studies to exploit the potential of BBR losses in this paper.\nFirstly, an Efficient Intersection over Union (EIOU) loss is proposed, which\nexplicitly measures the discrepancies of three geometric factors in BBR, i.e.,\nthe overlap area, the central point and the side length. After that, we state\nthe Effective Example Mining (EEM) problem and propose a regression version of\nfocal loss to make the regression process focus on high-quality anchor boxes.\nFinally, the above two parts are combined to obtain a new loss function, namely\nFocal-EIOU loss. Extensive experiments on both synthetic and real datasets are\nperformed. Notable superiorities on both the convergence speed and the\nlocalization accuracy can be achieved over other BBR losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi-Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Weiqiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthesizing MR Image Contrast Enhancement Using 3D High-resolution ConvNets. (arXiv:2104.01592v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.01592","description":"<p>\\textit{Objective:} Gadolinium-based contrast agents (GBCAs) have been widely\nused to better visualize disease in brain magnetic resonance imaging (MRI).\nHowever, gadolinium deposition within the brain and body has raised safety\nconcerns about the use of GBCAs. Therefore, the development of novel approaches\nthat can decrease or even eliminate GBCA exposure while providing similar\ncontrast information would be of significant use clinically. \\textit{Methods:}\nIn this work, we present a deep learning based approach for contrast-enhanced\nT1 synthesis on brain tumor patients. A 3D high-resolution fully convolutional\nnetwork (FCN), which maintains high resolution information through processing\nand aggregates multi-scale information in parallel, is designed to map\npre-contrast MRI sequences to contrast-enhanced MRI sequences. Specifically,\nthree pre-contrast MRI sequences, T1, T2 and apparent diffusion coefficient map\n(ADC), are utilized as inputs and the post-contrast T1 sequences are utilized\nas target output. To alleviate the data imbalance problem between normal\ntissues and the tumor regions, we introduce a local loss to improve the\ncontribution of the tumor regions, which leads to better enhancement results on\ntumors. \\textit{Results:} Extensive quantitative and visual assessments are\nperformed, with our proposed model achieving a PSNR of 28.24dB in the brain and\n21.2dB in tumor regions. \\textit{Conclusion and Significance:} Our results\nsuggest the potential of substituting GBCAs with synthetic contrast images\ngenerated via deep learning. Code is available at\n\\url{https://github.com/chenchao666/Contrast-enhanced-MRI-Synthesis\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raymond_C/0/1/0/all/0/1\">Catalina Raymond</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Speier_B/0/1/0/all/0/1\">Bill Speier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_X/0/1/0/all/0/1\">Xinyu Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cloughesy_T/0/1/0/all/0/1\">Timothy F. Cloughesy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Enzmann_D/0/1/0/all/0/1\">Dieter Enzmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ellingson_B/0/1/0/all/0/1\">Benjamin M. Ellingson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_C/0/1/0/all/0/1\">Corey W. Arnold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CXR Segmentation by AdaIN-based Domain Adaptation and Knowledge Distillation. (arXiv:2104.05892v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.05892","description":"<p>As segmentation labels are scarce, extensive researches have been conducted\nto train segmentation networks with domain adaptation, semi-supervised or\nself-supervised learning techniques to utilize abundant unlabeled dataset.\nHowever, these approaches appear different from each other, so it is not clear\nhow these approaches can be combined for better performance. Inspired by recent\nmulti-domain image translation approaches, here we propose a novel segmentation\nframework using adaptive instance normalization (AdaIN), so that a single\ngenerator is trained to perform both domain adaptation and semi-supervised\nsegmentation tasks via knowledge distillation by simply changing task-specific\nAdaIN codes. Specifically, our framework is designed to deal with difficult\nsituations in chest X-ray radiograph (CXR) segmentation, where labels are only\navailable for normal data, but trained model should be applied to both normal\nand abnormal data. The proposed network demonstrates great generalizability\nunder domain shift and achieves the state-of-the-art performance for abnormal\nCXR segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Oh_Y/0/1/0/all/0/1\">Yujin Oh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Sequential Sampling and Reconstruction for MRI. (arXiv:2105.06460v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.06460","description":"<p>Accelerated MRI shortens acquisition time by subsampling in the measurement\n$\\kappa$-space. Recovering a high-fidelity anatomical image from subsampled\nmeasurements requires close cooperation between two components: (1) a sampler\nthat chooses the subsampling pattern and (2) a reconstructor that recovers\nimages from incomplete measurements. In this paper, we leverage the sequential\nnature of MRI measurements, and propose a fully differentiable framework that\njointly learns a sequential sampling policy simultaneously with a\nreconstruction strategy. This co-designed framework is able to adapt during\nacquisition in order to capture the most informative measurements for a\nparticular target. Experimental results on the fastMRI knee dataset demonstrate\nthat the proposed approach successfully utilizes intermediate information\nduring the sampling process to boost reconstruction performance. In particular,\nour proposed method can outperform the current state-of-the-art learned\n$\\kappa$-space sampling baseline on over 96% of test samples. We also\ninvestigate the individual and collective benefits of the sequential sampling\nand co-design strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yin_T/0/1/0/all/0/1\">Tianwei Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1\">Zihui Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">He Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bouman_K/0/1/0/all/0/1\">Katherine L. Bouman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Recognition in Horses with Convolutional Neural Networks. (arXiv:2105.11953v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11953","description":"<p>Creating intelligent systems capable of recognizing emotions is a difficult\ntask, especially when looking at emotions in animals. This paper describes the\nprocess of designing a \"proof of concept\" system to recognize emotions in\nhorses. This system is formed by two elements, a detector and a model. The\ndetector is a fast region-based convolutional neural network that detects\nhorses in an image. The model is a convolutional neural network that predicts\nthe emotions of those horses. These two elements were trained with multiple\nimages of horses until they achieved high accuracy in their tasks. In total,\n400 images of horses were collected and labeled to train both the detector and\nthe model while 40 were used to test the system. Once the two components were\nvalidated, they were combined into a testable system that would detect equine\nemotions based on established behavioral ethograms indicating emotional affect\nthrough head, neck, ear, muzzle and eye position. The system showed an accuracy\nof 80% on the validation set and 65% on the test set, demonstrating that it is\npossible to predict emotions in animals using autonomous intelligent systems.\nSuch a system has multiple applications including further studies in the\ngrowing field of animal emotions as well as in the veterinary field to\ndetermine the physical welfare of horses or other livestock.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corujo_L/0/1/0/all/0/1\">Luis A. Corujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">Peter A. Gloor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kieson_E/0/1/0/all/0/1\">Emily Kieson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schloesser_T/0/1/0/all/0/1\">Timo Schloesser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Driven Image Style Transfer. (arXiv:2106.00178v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00178","description":"<p>Despite having promising results, style transfer, which requires preparing\nstyle images in advance, may result in lack of creativity and accessibility.\nFollowing human instruction, on the other hand, is the most natural way to\nperform artistic style transfer that can significantly improve controllability\nfor visual effect applications. We introduce a new task, language-driven\nartistic style transfer (LDAST), to manipulate the style of a content image,\nguided by a text. We propose contrastive language visual artist (CLVA) that\nlearns to extract visual semantics from style instructions and accomplish LDAST\nby the patch-wise style discriminator. The discriminator considers the\ncorrelation between language and patches of style images or transferred results\nto jointly embed style instructions. CLVA further compares contrastive pairs of\ncontent images and style instructions to improve the mutual relativeness. The\nresults from the same content image can preserve consistent content structures.\nBesides, they should present analogous style patterns from style instructions\nthat contain similar visual semantics. The experiments show that our CLVA is\neffective and achieves superb transferred results on LDAST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tsu-Jui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatially Invariant Unsupervised 3D Object-Centric Learning and Scene Decomposition. (arXiv:2106.05607v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05607","description":"<p>We tackle the problem of object-centric learning on point clouds, which is\ncrucial for high-level relational reasoning and scalable machine intelligence.\nIn particular, we introduce a framework, SPAIR3D, to factorize a 3D point cloud\ninto a spatial mixture model where each component corresponds to one object. To\nmodel the spatial mixture model on point clouds, we derive the Chamfer Mixture\nLoss, which fits naturally into our variational training pipeline. Moreover, we\nadopt an object-specification scheme that describes each object's location\nrelative to its local voxel grid cell. Such a scheme allows SPAIR3D to model\nscenes with an arbitrary number of objects. We evaluate our method on the task\nof unsupervised scene decomposition. Experimental results demonstrate that\nSPAIR3D has strong scalability and is capable of detecting and segmenting an\nunknown number of objects from a point cloud in an unsupervised manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miaomiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1\">Kee Siong Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Trajectory Prediction via Distribution Discrimination. (arXiv:2107.14204v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.14204","description":"<p>Trajectory prediction is confronted with the dilemma to capture the\nmulti-modal nature of future dynamics with both diversity and accuracy. In this\npaper, we present a distribution discrimination (DisDis) method to predict\npersonalized motion patterns by distinguishing the potential distributions.\nMotivated by that the motion pattern of each person is personalized due to\nhis/her habit, our DisDis learns the latent distribution to represent different\nmotion patterns and optimize it by the contrastive discrimination. This\ndistribution discrimination encourages latent distributions to be more\ndiscriminative. Our method can be integrated with existing multi-modal\nstochastic predictive models as a plug-and-play module to learn the more\ndiscriminative latent distribution. To evaluate the latent distribution, we\nfurther propose a new metric, probability cumulative minimum distance (PCMD)\ncurve, which cumulatively calculates the minimum distance on the sorted\nprobabilities. Experimental results on the ETH and UCY datasets show the\neffectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1\">Nuoxing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liangliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoVideo: An Automated Video Action Recognition System. (arXiv:2108.04212v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04212","description":"<p>Action recognition is an important task for video understanding with broad\napplications. However, developing an effective action recognition solution\noften requires extensive engineering efforts in building and testing different\ncombinations of the modules and their hyperparameters. In this demo, we present\nAutoVideo, a Python system for automated video action recognition. AutoVideo is\nfeatured for 1) highly modular and extendable infrastructure following the\nstandard pipeline language, 2) an exhaustive list of primitives for pipeline\nconstruction, 3) data-driven tuners to save the efforts of pipeline tuning, and\n4) easy-to-use Graphical User Interface (GUI). AutoVideo is released under MIT\nlicense at https://github.com/datamllab/autovideo\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1\">Zaid Pervaiz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sirui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaben Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1\">Kwei-Herng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1\">Mohammad Qazim Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anmoll Kumar Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_A/0/1/0/all/0/1\">Alfredo Costilla Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1\">Na Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind Image Decomposition. (arXiv:2108.11364v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11364","description":"<p>We propose and study a novel task named Blind Image Decomposition (BID),\nwhich requires separating a superimposed image into constituent underlying\nimages in a blind setting, that is, both the source components involved in\nmixing as well as the mixing mechanism are unknown. For example, rain may\nconsist of multiple components, such as rain streaks, raindrops, snow, and\nhaze. Rainy images can be treated as an arbitrary combination of these\ncomponents, some of them or all of them. How to decompose superimposed images,\nlike rainy images, into distinct source components is a crucial step toward\nreal-world vision systems. To facilitate research on this new task, we\nconstruct multiple benchmark datasets, including mixed image decomposition\nacross multiple domains, real-scenario deraining, and joint\nshadow/reflection/watermark removal. Moreover, we propose a simple yet general\nBlind Image Decomposition Network (BIDeN) to serve as a strong baseline for\nfuture work. Experimental results demonstrate the tenability of our benchmarks\nand the effectiveness of BIDeN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junlin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chunyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jie Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Synthetic Anomalies for Self-Supervised Anomaly Detection and Localization. (arXiv:2109.15222v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15222","description":"<p>We introduce a simple and intuitive self-supervision task, Natural Synthetic\nAnomalies (NSA), for training an end-to-end model for anomaly detection and\nlocalization using only normal training data. NSA integrates Poisson image\nediting to seamlessly blend scaled patches of various sizes from separate\nimages. This creates a wide range of synthetic anomalies which are more similar\nto natural sub-image irregularities than previous data-augmentation strategies\nfor self-supervised anomaly detection. We evaluate the proposed method using\nnatural and medical images. Our experiments with the MVTec AD dataset show that\na model trained to localize NSA anomalies generalizes well to detecting\nreal-world a priori unknown types of manufacturing defects. Our method achieves\nan overall detection AUROC of 97.2 outperforming all previous methods that\nlearn without the use of additional datasets. Code available at\nhttps://github.com/hmsch/natural-synthetic-anomalies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schluter_H/0/1/0/all/0/1\">Hannah M. Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jeremy Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Benjamin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Transformer in Federated Setting for Human Activity Recognition. (arXiv:2110.00244v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00244","description":"<p>Human activity recognition (HAR) is a machine learning task with applications\nin many domains including health care, but it has proven a challenging research\nproblem. In health care, it is used mainly as an assistive technology for elder\ncare, often used together with other related technologies such as the Internet\nof Things (IoT) because HAR can be achieved with the help of IoT devices such\nas smartphones, wearables, environmental and on-body sensors. Deep neural\nnetwork techniques like convolutional neural networks (CNNs) and recurrent\nneural networks (RNNs) have been used for HAR, both in centralized and\nfederated settings. However, these techniques have certain limitations: RNNs\ncannot be easily parallelized, CNNs have the limitation of sequence length, and\nboth are computationally expensive. Moreover, the centralized approach has\nprivacy concerns when facing sensitive applications such as healthcare. In this\npaper, to address some of the existing challenges facing HAR, we present a\nnovel one-patch transformer based on inertial sensors that can combine the\nadvantages of RNNs and CNNs without their major limitations. We designed a\ntestbed to collect real-time human activity data and used the data to train and\ntest the proposed transformer-based HAR classifier. We also propose TransFed: a\nfederated learning-based HAR classifier using the proposed transformer to\naddress privacy concerns. The experimental results showed that the proposed\nsolution outperformed the state-of-the-art HAR classifiers based on CNNs and\nRNNs, in both federated and centralized settings. Moreover, the proposed HAR\nclassifier is computationally inexpensive as it uses much fewer parameters than\nexisting CNN/RNN-based classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raza_A/0/1/0/all/0/1\">Ali Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Kim Phuc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehl_L/0/1/0/all/0/1\">Ludovic Koehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benzaidi_K/0/1/0/all/0/1\">Khaled Benzaidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Score-based diffusion models for accelerated MRI. (arXiv:2110.05243v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.05243","description":"<p>Score-based diffusion models provide a powerful way to model images using the\ngradient of the data distribution. Leveraging the learned score function as a\nprior, here we introduce a way to sample data from a conditional distribution\ngiven the measurements, such that the model can be readily used for solving\ninverse problems in imaging, especially for accelerated MRI. In short, we train\na continuous time-dependent score function with denoising score matching. Then,\nat the inference stage, we iterate between numerical SDE solver and data\nconsistency projection step to achieve reconstruction. Our model requires\nmagnitude images only for training, and yet is able to reconstruct\ncomplex-valued data, and even extends to parallel imaging. The proposed method\nis agnostic to sub-sampling patterns, and can be used with any sampling\nschemes. Also, due to its generative nature, our approach can quantify\nuncertainty, which is not possible with standard regression settings. On top of\nall the advantages, our method also has very strong performance, even beating\nthe models trained with full supervision. With extensive experiments, we verify\nthe superiority of our method in terms of quality and practicality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1\">Hyungjin Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Stereo Network with attention thin volume. (arXiv:2110.08556v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08556","description":"<p>We propose an efficient multi-view stereo (MVS) network for infering depth\nvalue from multiple RGB images. Recent studies have shown that mapping the\ngeometric relationship in real space to neural network is an essential topic of\nthe MVS problem. Specifically, these methods focus on how to express the\ncorrespondence between different views by constructing a nice cost volume. In\nthis paper, we propose a more complete cost volume construction approach based\non absorbing previous experience. First of all, we introduce the self-attention\nmechanism to fully aggregate the dominant information from input images and\naccurately model the long-range dependency, so as to selectively aggregate\nreference features. Secondly, we introduce the group-wise correlation to\nfeature aggregation, which greatly reduces the memory and calculation burden.\nMeanwhile, this method enhances the information interaction between different\nfeature channels. With this approach, a more lightweight and efficient cost\nvolume is constructed. Finally we follow the coarse to fine strategy and refine\nthe depth sampling range scale by scale with the help of uncertainty\nestimation. We further combine the previous steps to get the attention thin\nvolume. Quantitative and qualitative experiments are presented to demonstrate\nthe performance of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zihang Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reachability Embeddings: Scalable Self-Supervised Representation Learning from Mobility Trajectories for Multimodal Geospatial Computer Vision. (arXiv:2110.12521v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12521","description":"<p>Self-supervised representation learning techniques utilize large datasets\nwithout semantic annotations to learn meaningful, universal features that can\nbe conveniently transferred to solve a wide variety of downstream supervised\ntasks. In this paper, we propose a self-supervised method for learning\nrepresentations of geographic locations from unlabeled GPS trajectories to\nsolve downstream geospatial computer vision tasks. Tiles resulting from a\nraster representation of the earth's surface are modeled as nodes on a graph or\npixels of an image. GPS trajectories are modeled as allowed Markovian paths on\nthese nodes. A scalable and distributed algorithm is presented to compute\nimage-like tensors, called reachability summaries, of the spatial connectivity\npatterns between tiles and their neighbors implied by the observed Markovian\npaths. A convolutional, contractive autoencoder is trained to learn compressed\nrepresentations, called reachability embeddings, of reachability summaries for\nevery tile. Reachability embeddings serve as task-agnostic, feature\nrepresentations of geographic locations. Using reachability embeddings as pixel\nrepresentations for five different downstream geospatial tasks, cast as\nsupervised semantic segmentation problems, we quantitatively demonstrate that\nreachability embeddings are semantically meaningful representations and result\nin 4-23% gain in performance, while using upto 67% less trajectory data, as\nmeasured using area under the precision-recall curve (AUPRC) metric, when\ncompared to baseline models that use pixel representations that do not account\nfor the spatial connectivity between tiles. Reachability embeddings transform\nsequential, spatiotemporal mobility data into semantically meaningful\nimage-like tensor representations that can be combined with other sources of\nimagery and are designed to facilitate multimodal learning in geospatial\ncomputer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Swetava Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_C/0/1/0/all/0/1\">C. V. Krishnakumar Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1\">Vipul Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Authentication Attacks on Projection-based Cancelable Biometric Schemes. (arXiv:2110.15163v6 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2110.15163","description":"<p>Cancelable biometric schemes aim at generating secure biometric templates by\ncombining user specific tokens, such as password, stored secret or salt, along\nwith biometric data. This type of transformation is constructed as a\ncomposition of a biometric transformation with a feature extraction algorithm.\nThe security requirements of cancelable biometric schemes concern the\nirreversibility, unlinkability and revocability of templates, without losing in\naccuracy of comparison. While several schemes were recently attacked regarding\nthese requirements, full reversibility of such a composition in order to\nproduce colliding biometric characteristics, and specifically presentation\nattacks, were never demonstrated to the best of our knowledge. In this paper,\nwe formalize these attacks for a traditional cancelable scheme with the help of\ninteger linear programming (ILP) and quadratically constrained quadratic\nprogramming (QCQP). Solving these optimization problems allows an adversary to\nslightly alter its fingerprint image in order to impersonate any individual.\nMoreover, in an even more severe scenario, it is possible to simultaneously\nimpersonate several individuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durbet_A/0/1/0/all/0/1\">Axel Durbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lafourcade_P/0/1/0/all/0/1\">Pascal Lafourcade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migdal_D/0/1/0/all/0/1\">Denis Migdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiry_Atighehchi_K/0/1/0/all/0/1\">Kevin Thiry-Atighehchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grollemund_P/0/1/0/all/0/1\">Paul-Marie Grollemund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation. (arXiv:2111.05759v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.05759","description":"<p>Vision-and-Language Navigation (VLN) is a task that an agent is required to\nfollow a language instruction to navigate to the goal position, which relies on\nthe ongoing interactions with the environment during moving. Recent\nTransformer-based VLN methods have made great progress benefiting from the\ndirect connections between visual observations and the language instruction via\nthe multimodal cross-attention mechanism. However, these methods usually\nrepresent temporal context as a fixed-length vector by using an LSTM decoder or\nusing manually designed hidden states to build a recurrent Transformer.\nConsidering a single fixed-length vector is often insufficient to capture\nlong-term temporal context, in this paper, we introduce Multimodal Transformer\nwith Variable-length Memory (MTVM) for visually-grounded natural language\nnavigation by modelling the temporal context explicitly. Specifically, MTVM\nenables the agent to keep track of the navigation trajectory by directly\nstoring previous activations in a memory bank. To further boost the\nperformance, we propose a memory-aware consistency loss to help learn a better\njoint representation of temporal context with random masked instructions. We\nevaluate MTVM on popular R2R and CVDN datasets, and our model improves Success\nRate on R2R unseen validation and test set by 2% each, and reduce Goal Process\nby 1.6m on CVDN test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chuang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching. (arXiv:2111.08919v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08919","description":"<p>Current metrics for video captioning are mostly based on the text-level\ncomparison between reference and candidate captions. However, they have some\ninsuperable drawbacks, e.g., they cannot handle videos without references, and\nthey may result in biased evaluation due to the one-to-many nature of\nvideo-to-text and the neglect of visual relevance. From the human evaluator's\nviewpoint, a high-quality caption should be consistent with the provided video,\nbut not necessarily be similar to the reference in literal or semantics.\nInspired by human evaluation, we propose EMScore (Embedding Matching-based\nscore), a novel reference-free metric for video captioning, which directly\nmeasures similarity between video and candidate captions. Benefit from the\nrecent development of large-scale pre-training models, we exploit a well\npre-trained vision-language model to extract visual and linguistic embeddings\nfor computing EMScore. Specifically, EMScore combines matching scores of both\ncoarse-grained (video and caption) and fine-grained (frames and words) levels,\nwhich takes the overall understanding and detailed characteristics of the video\ninto account. Furthermore, considering the potential information gain, EMScore\ncan be flexibly extended to the conditions where human-labeled references are\navailable. Last but not least, we collect VATEX-EVAL and ActivityNet-FOIl\ndatasets to systematically evaluate the existing metrics. VATEX-EVAL\nexperiments demonstrate that EMScore has higher human correlation and lower\nreference dependency. ActivityNet-FOIL experiment verifies that EMScore can\neffectively identify \"hallucinating\" captions. The datasets will be released to\nfacilitate the development of video captioning metrics. The code is available\nat: https://github.com/ShiYaya/emscore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yaya Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunfeng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Clustering Learning for Large-scale Unsupervised Person Re-identification. (arXiv:2111.10032v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10032","description":"<p>Unsupervised Person Re-identification (U-ReID) with pseudo labeling recently\nreaches a competitive performance compared to fully-supervised ReID methods\nbased on modern clustering algorithms. However, such clustering-based scheme\nbecomes computationally prohibitive for large-scale datasets. How to\nefficiently leverage endless unlabeled data with limited computing resources\nfor better U-ReID is under-explored. In this paper, we make the first attempt\nto the large-scale U-ReID and propose a \"small data for big task\" paradigm\ndubbed Meta Clustering Learning (MCL). MCL only pseudo-labels a subset of the\nentire unlabeled data via clustering to save computing for the first-phase\ntraining. After that, the learned cluster centroids, termed as meta-prototypes\nin our MCL, are regarded as a proxy annotator to softly annotate the rest\nunlabeled data for further polishing the model. To alleviate the potential\nnoisy labeling issue in the polishment phase, we enforce two well-designed loss\nconstraints to promise intra-identity consistency and inter-identity strong\ncorrelation. For multiple widely-used U-ReID benchmarks, our method\nsignificantly saves computational cost while achieving a comparable or even\nbetter performance compared to prior works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Vision Transformers Robust to Patch Perturbations?. (arXiv:2111.10659v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10659","description":"<p>Recent advances in Vision Transformer (ViT) have demonstrated its impressive\nperformance in image classification, which makes it a promising alternative to\nConvolutional Neural Network (CNN). Unlike CNNs, ViT represents an input image\nas a sequence of image patches. The patch-based input image representation\nmakes the following question interesting: How does ViT perform when individual\ninput image patches are perturbed with natural corruptions or adversarial\nperturbations, compared to CNNs? In this work, we study the robustness of ViT\nto patch-wise perturbations. Surprisingly, we find that ViTs are more robust to\nnaturally corrupted patches than CNNs, whereas they are more vulnerable to\nadversarial patches. Furthermore, we discover that the attention mechanism\ngreatly affects the robustness of vision transformers. Specifically, the\nattention module can help improve the robustness of ViT by effectively ignoring\nnatural corrupted patches. However, when ViTs are attacked by an adversary, the\nattention mechanism can be easily fooled to focus more on the adversarially\nperturbed patches and cause a mistake. Based on our analysis, we propose a\nsimple temperature-scaling based method to improve the robustness of ViT\nagainst adversarial patches. Extensive qualitative and quantitative experiments\nare performed to support our findings, understanding, and improvement of ViT\nrobustness to patch-wise perturbations across a set of transformer-based\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jindong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yao Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Vision Transformers. (arXiv:2111.11067v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11067","description":"<p>We study the training of Vision Transformers for semi-supervised image\nclassification. Transformers have recently demonstrated impressive performance\non a multitude of supervised learning tasks. Surprisingly, we show Vision\nTransformers perform significantly worse than Convolutional Neural Networks\nwhen only a small set of labeled data is available. Inspired by this\nobservation, we introduce a joint semi-supervised learning framework,\nSemiformer, which contains a transformer stream, a convolutional stream and a\ncarefully designed fusion module for knowledge sharing between these streams.\nThe convolutional stream is trained on limited labeled data and further used to\ngenerate pseudo labels to supervise the training of the transformer stream on\nunlabeled data. Extensive experiments on ImageNet demonstrate that Semiformer\nachieves 75.5% top-1 accuracy, outperforming the state-of-the-art by a clear\nmargin. In addition, we show, among other things, Semiformer is a general\nframework that is compatible with most modern transformer and convolutional\nneural architectures. Code is available at\nhttps://github.com/wengzejia1/Semiformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zejia Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xitong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-agnostic Object Detection with Multi-modal Transformer. (arXiv:2111.11430v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11430","description":"<p>What constitutes an object? This has been a long-standing question in\ncomputer vision. Towards this goal, numerous learning-free and learning-based\napproaches have been developed to score objectness. However, they generally do\nnot scale well across new domains and novel objects. In this paper, we advocate\nthat existing methods lack a top-down supervision signal governed by\nhuman-understandable semantics. For the first time in literature, we\ndemonstrate that Multi-modal Vision Transformers (MViT) trained with aligned\nimage-text pairs can effectively bridge this gap. Our extensive experiments\nacross various domains and novel objects show the state-of-the-art performance\nof MViTs to localize generic objects in images. Based on the observation that\nexisting MViTs do not include multi-scale feature processing and usually\nrequire longer training schedules, we develop an efficient MViT architecture\nusing multi-scale deformable attention and late vision-language fusion. We show\nthe significance of MViT proposals in a diverse range of applications including\nopen-world object detection, salient and camouflage object detection,\nsupervised and self-supervised detection tasks. Further, MViTs can adaptively\ngenerate proposals given a specific language query and thus offer enhanced\ninteractability. Code: \\url{https://git.io/J1HPY}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maaz_M/0/1/0/all/0/1\">Muhammad Maaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_H/0/1/0/all/0/1\">Hanoona Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1\">Rao Muhammad Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Video Transformers with Spatial-Temporal Token Selection. (arXiv:2111.11591v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11591","description":"<p>Video transformers have achieved impressive results on major video\nrecognition benchmarks, which however suffer from high computational cost. In\nthis paper, we present STTS, a token selection framework that dynamically\nselects a few informative tokens in both temporal and spatial dimensions\nconditioned on input video samples. Specifically, we formulate token selection\nas a ranking problem, which estimates the importance of each token through a\nlightweight scorer network and only those with top scores will be used for\ndownstream evaluation. In the temporal dimension, we keep the frames that are\nmost relevant to the action categories, while in the spatial dimension, we\nidentify the most discriminative region in feature maps without affecting the\nspatial context used in a hierarchical way in most video transformers. Since\nthe decision of token selection is non-differentiable, we employ a\nperturbed-maximum based differentiable Top-K operator for end-to-end training.\nWe mainly conduct extensive experiments on Kinetics-400 with a recently\nintroduced video transformer backbone, MViT. Our framework achieves similar\nresults while requiring 20% less computation. We also demonstrate our approach\nis generic for different transformer architectures and video datasets. Code is\navailable at https://github.com/wangjk666/STTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xitong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hengduo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KTNet: Knowledge Transfer for Unpaired 3D Shape Completion. (arXiv:2111.11976v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11976","description":"<p>Unpaired 3D object completion aims to predict a complete 3D shape from an\nincomplete input without knowing the correspondence between the complete and\nincomplete shapes. In this paper, we propose the novel KTNet to solve this task\nfrom the new perspective of knowledge transfer. KTNet elaborates a\nteacher-assistant-student network to establish multiple knowledge transfer\nprocesses. Specifically, the teacher network takes complete shape as input and\nlearns the knowledge of complete shape. The student network takes the\nincomplete one as input and restores the corresponding complete shape. And the\nassistant modules not only help to transfer the knowledge of complete shape\nfrom the teacher to the student, but also judge the learning effect of the\nstudent network. As a result, KTNet makes use of a more comprehensive\nunderstanding to establish the geometric correspondence between complete and\nincomplete shapes in a perspective of knowledge transfer, which enables more\ndetailed geometric inference for generating high-quality complete shapes. We\nconduct comprehensive experiments on several datasets, and the results show\nthat our method outperforms previous methods of unpaired point cloud completion\nby a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bisheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Lightweight Graph Transformer Network for Human Mesh Reconstruction from 2D Human Pose. (arXiv:2111.12696v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12696","description":"<p>Existing deep learning-based human mesh reconstruction approaches have a\ntendency to build larger networks in order to achieve higher accuracy.\nComputational complexity and model size are often neglected, despite being key\ncharacteristics for practical use of human mesh reconstruction models (e.g.\nvirtual try-on systems). In this paper, we present GTRS, a lightweight\npose-based method that can reconstruct human mesh from 2D human pose. We\npropose a pose analysis module that uses graph transformers to exploit\nstructured and implicit joint correlations, and a mesh regression module that\ncombines the extracted pose feature with the mesh template to reconstruct the\nfinal human mesh. We demonstrate the efficiency and generalization of GTRS by\nextensive evaluations on the Human3.6M and 3DPW datasets. In particular, GTRS\nachieves better accuracy than the SOTA pose-based method Pose2Mesh while only\nusing 10.2% of the parameters (Params) and 2.5% of the FLOPs on the challenging\nin-the-wild 3DPW dataset. Code will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Ce Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendieta_M/0/1/0/all/0/1\">Matias Mendieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1\">Aidong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Perceptual Quality of 2D Animation Interpolation. (arXiv:2111.12792v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12792","description":"<p>Traditional 2D animation is labor-intensive, often requiring animators to\nmanually draw twelve illustrations per second of movement. While automatic\nframe interpolation may ease this burden, 2D animation poses additional\ndifficulties compared to photorealistic video. In this work, we address\nchallenges unexplored in previous animation interpolation systems, with a focus\non improving perceptual quality. Firstly, we propose SoftsplatLite (SSL), a\nforward-warping interpolation architecture with fewer trainable parameters and\nbetter perceptual performance. Secondly, we design a Distance Transform Module\n(DTM) that leverages line proximity cues to correct aberrations in difficult\nsolid-color regions. Thirdly, we define a Restricted Relative Linear\nDiscrepancy metric (RRLD) to automate the previously manual training data\ncollection process. Lastly, we explore evaluation of 2D animation generation\nthrough a user study, and establish that the LPIPS perceptual metric and\nchamfer line distance (CD) are more appropriate measures of quality than PSNR\nand SSIM used in prior art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1\">Matthias Zwicker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint stereo 3D object detection and implicit surface reconstruction. (arXiv:2111.12924v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12924","description":"<p>We present the first learning-based framework for category-level 3D object\ndetection and implicit shape estimation based on a pair of stereo RGB images in\nthe wild. Previous stereo 3D object detection approaches cannot describe the\ncomplete shape details of the detected objects and often fails for the small\nobjects. In contrast, we propose a new progressive approach that can (1)\nperform precise localization as well as provide a complete and\nresolution-agnostic shape description for the detected objects and (2) produce\nsignificantly more accurate orientation predictions for the tiny instances.\nThis approach features a new instance-level network that explicitly models the\nunseen surface hallucination problem using point-based representations and uses\na new geometric representation for orientation refinement. Extensive\nexperiments show that our approach achieves state-of-the-art performance using\nvarious metrics on the KITTI benchmark. Code and pre-trained models will be\navailable at this https URL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Vicinal Space for Unsupervised Domain Adaptation. (arXiv:2111.13353v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13353","description":"<p>Recent unsupervised domain adaptation methods have utilized vicinal space\nbetween the source and target domains. However, the equilibrium collapse of\nlabels, a problem where the source labels are dominant over the target labels\nin the predictions of vicinal instances, has never been addressed. In this\npaper, we propose an instance-wise minimax strategy that minimizes the entropy\nof high uncertainty instances in the vicinal space to tackle the stated\nproblem. We divide the vicinal space into two subspaces through the solution of\nthe minimax problem: contrastive space and consensus space. In the contrastive\nspace, inter-domain discrepancy is mitigated by constraining instances to have\ncontrastive views and labels, and the consensus space reduces the confusion\nbetween intra-domain categories. The effectiveness of our method is\ndemonstrated on public benchmarks, including Office-31, Office-Home, and\nVisDA-C, achieving state-of-the-art performances. We further show that our\nmethod outperforms the current state-of-the-art methods on PACS, which\nindicates that our instance-wise approach works well for multi-source domain\nadaptation as well. Code is available at https://github.com/NaJaeMin92/CoVi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Na_J/0/1/0/all/0/1\">Jaemin Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hyung Jin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonjun Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GMFlow: Learning Optical Flow via Global Matching. (arXiv:2111.13680v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13680","description":"<p>Learning-based optical flow estimation has been dominated with the pipeline\nof cost volume with convolutions for flow regression, which is inherently\nlimited to local correlations and thus is hard to address the long-standing\nchallenge of large displacements. To alleviate this, the state-of-the-art\nframework RAFT gradually improves its prediction quality by using a large\nnumber of iterative refinements, achieving remarkable performance but\nintroducing linearly increasing inference time. To enable both high accuracy\nand efficiency, we completely revamp the dominant flow regression pipeline by\nreformulating optical flow as a global matching problem, which identifies the\ncorrespondences by directly comparing feature similarities. Specifically, we\npropose a GMFlow framework, which consists of three main components: a\ncustomized Transformer for feature enhancement, a correlation and softmax layer\nfor global feature matching, and a self-attention layer for flow propagation.\nWe further introduce a refinement step that reuses GMFlow at higher feature\nresolution for residual flow prediction. Our new framework outperforms\n31-refinements RAFT on the challenging Sintel benchmark, while using only one\nrefinement and running faster, suggesting a new paradigm for accurate and\nefficient optical flow estimation. Code is available at\nhttps://github.com/haofeixu/gmflow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haofei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVA-AVD: Audio-Visual Speaker Diarization in the Wild. (arXiv:2111.14448v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14448","description":"<p>Audio-visual speaker diarization aims at detecting \"who spoke when\" using\nboth auditory and visual signals. Existing audio-visual diarization datasets\nare mainly focused on indoor environments like meeting rooms or news studios,\nwhich are quite different from in-the-wild videos in many scenarios such as\nmovies, documentaries, and audience sitcoms. To develop diarization methods for\nthese challenging videos, we create the AVA Audio-Visual Diarization (AVA-AVD)\ndataset. Our experiments demonstrate that adding AVA-AVD into training set can\nproduce significantly better diarization models for in-the-wild videos despite\nthat the data is relatively small. Moreover, this benchmark is challenging due\nto the diverse scenes, complicated acoustic conditions, and completely\noff-screen speakers. As a first step towards addressing the challenges, we\ndesign the Audio-Visual Relation Network (AVR-Net) which introduces a simple\nyet effective modality mask to capture discriminative information based on face\nvisibility. Experiments show that our method not only can outperform\nstate-of-the-art methods but is more robust as varying the ratio of off-screen\nspeakers. Our data and code has been made publicly available at\nhttps://github.com/showlab/AVA-AVD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_E/0/1/0/all/0/1\">Eric Zhongcong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zeyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsutsui_S/0/1/0/all/0/1\">Satoshi Tsutsui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Sparse Colorization Network for Deep Exemplar-based Colorization. (arXiv:2112.01335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01335","description":"<p>Exemplar-based colorization approaches rely on reference image to provide\nplausible colors for target gray-scale image. The key and difficulty of\nexemplar-based colorization is to establish an accurate correspondence between\nthese two images. Previous approaches have attempted to construct such a\ncorrespondence but are faced with two obstacles. First, using luminance\nchannels for the calculation of correspondence is inaccurate. Second, the dense\ncorrespondence they built introduces wrong matching results and increases the\ncomputation burden. To address these two problems, we propose Semantic-Sparse\nColorization Network (SSCN) to transfer both the global image style and\ndetailed semantic-related colors to the gray-scale image in a coarse-to-fine\nmanner. Our network can perfectly balance the global and local colors while\nalleviating the ambiguous matching problem. Experiments show that our method\noutperforms existing methods in both quantitative and qualitative evaluation\nand achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yunpeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zenghao Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Andong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhengzhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-Aware Semantic-Guided Generative Model for Human Synthesis. (arXiv:2112.01422v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01422","description":"<p>Generative Neural Radiance Field (GNeRF) models, which extract implicit 3D\nrepresentations from 2D images, have recently been shown to produce realistic\nimages representing rigid/semi-rigid objects, such as human faces or cars.\nHowever, they usually struggle to generate high-quality images representing\nnon-rigid objects, such as the human body, which is of a great interest for\nmany computer graphics applications. This paper proposes a 3D-aware\nSemantic-Guided Generative Model (3D-SGAN) for human image synthesis, which\ncombines a GNeRF with a texture generator. The former learns an implicit 3D\nrepresentation of the human body and outputs a set of 2D semantic segmentation\nmasks. The latter transforms these semantic masks into a real image, adding a\nrealistic texture to the human appearance. Without requiring additional 3D\ninformation, our model can learn 3D human representations with a\nphoto-realistic, controllable generation. Our experiments on the DeepFashion\ndataset show that 3D-SGAN significantly outperforms the most recent baselines.\nThe code is available at https://github.com/zhangqianhui/3DSGAN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jichao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1\">Enver Sangineto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siarohin_A/0/1/0/all/0/1\">Aliaksandr Siarohin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Channel Encoding Transformer for Point Cloud Analysis. (arXiv:2112.02507v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02507","description":"<p>Transformer plays an increasingly important role in various computer vision\nareas and remarkable achievements have also been made in point cloud analysis.\nSince they mainly focus on point-wise transformer, an adaptive channel encoding\ntransformer is proposed in this paper. Specifically, a channel convolution\ncalled Transformer-Conv is designed to encode the channel. It can encode\nfeature channels by capturing the potential relationship between coordinates\nand features. Compared with simply assigning attention weight to each channel,\nour method aims to encode the channel adaptively. In addition, our network\nadopts the neighborhood search method of low-level and high-level dual semantic\nreceptive fields to improve the performance. Extensive experiments show that\nour method is superior to state-of-the-art point cloud classification and\nsegmentation methods on three benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoquan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hezhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jianwei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolyphonicFormer: Unified Query Learning for Depth-aware Video Panoptic Segmentation. (arXiv:2112.02582v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02582","description":"<p>The Depth-aware Video Panoptic Segmentation (DVPS) is a new challenging\nvision problem that aims to predict panoptic segmentation and depth in a video\nsimultaneously. The previous work solves this task by extending the existing\npanoptic segmentation method with an extra dense depth prediction and instance\ntracking head. However, the relationship between the depth and panoptic\nsegmentation is not well explored -- simply combining existing methods leads to\ncompetition and needs carefully weight balancing. In this paper, we present\nPolyphonicFormer, a vision transformer to unify these sub-tasks under the DVPS\ntask and lead to more robust results. Our principal insight is that the depth\ncan be harmonized with the panoptic segmentation with our proposed new paradigm\nof predicting instance level depth maps with object queries. Then the\nrelationship between the two tasks via query-based learning is explored. From\nthe experiments, we demonstrate the benefits of our design from both depth\nestimation and panoptic segmentation aspects. Since each thing query also\nencodes the instance-wise information, it is natural to perform tracking\ndirectly with appearance learning. Our method achieves state-of-the-art results\non two DVPS datasets (Semantic KITTI, Cityscapes), and ranks 1st on the\nICCV-2021 BMTT Challenge video + depth track. Code is available at\nhttps://github.com/HarborYuan/PolyphonicFormer .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haobo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GreenPCO: An Unsupervised Lightweight Point Cloud Odometry Method. (arXiv:2112.04054v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04054","description":"<p>Visual odometry aims to track the incremental motion of an object using the\ninformation captured by visual sensors. In this work, we study the point cloud\nodometry problem, where only the point cloud scans obtained by the LiDAR (Light\nDetection And Ranging) are used to estimate object's motion trajectory. A\nlightweight point cloud odometry solution is proposed and named the green point\ncloud odometry (GreenPCO) method. GreenPCO is an unsupervised learning method\nthat predicts object motion by matching features of consecutive point cloud\nscans. It consists of three steps. First, a geometry-aware point sampling\nscheme is used to select discriminant points from the large point cloud.\nSecond, the view is partitioned into four regions surrounding the object, and\nthe PointHop++ method is used to extract point features. Third, point\ncorrespondences are established to estimate object motion between two\nconsecutive scans. Experiments on the KITTI dataset are conducted to\ndemonstrate the effectiveness of the GreenPCO method. It is observed that\nGreenPCO outperforms benchmarking deep learning methods in accuracy while it\nhas a significantly smaller model size and less training time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadam_P/0/1/0/all/0/1\">Pranav Kadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiahao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformaly -- Two (Feature Spaces) Are Better Than One. (arXiv:2112.04185v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04185","description":"<p>Anomaly detection is a well-established research area that seeks to identify\nsamples outside of a predetermined distribution. An anomaly detection pipeline\nis comprised of two main stages: (1) feature extraction and (2) normality score\nassignment. Recent papers used pre-trained networks for feature extraction\nachieving state-of-the-art results. However, the use of pre-trained networks\ndoes not fully-utilize the normal samples that are available at train time.\nThis paper suggests taking advantage of this information by using\nteacher-student training. In our setting, a pretrained teacher network is used\nto train a student network on the normal training samples. Since the student\nnetwork is trained only on normal samples, it is expected to deviate from the\nteacher network in abnormal cases. This difference can serve as a complementary\nrepresentation to the pre-trained feature vector. Our method -- Transformaly --\nexploits a pre-trained Vision Transformer (ViT) to extract both feature\nvectors: the pre-trained (agnostic) features and the teacher-student\n(fine-tuned) features. We report state-of-the-art AUROC results in both the\ncommon unimodal setting, where one class is considered normal and the rest are\nconsidered abnormal, and the multimodal setting, where all classes but one are\nconsidered normal, and just one class is considered abnormal. The code is\navailable at https://github.com/MatanCohen1/Transformaly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_M/0/1/0/all/0/1\">Matan Jacob Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1\">Shai Avidan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Marine Bubble Flow Quantification Using Wide-Baseline Stereo Photogrammetry. (arXiv:2112.07414v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07414","description":"<p>Reliable quantification of natural and anthropogenic gas release (e.g.\\\nCO$_2$, methane) from the seafloor into the water column, and potentially to\nthe atmosphere, is a challenging task. While ship-based echo sounders such as\nsingle beam and multibeam systems allow detection of free gas, bubbles, in the\nwater even from a great distance, exact quantification utilizing the\nhydroacoustic data requires additional parameters such as rise speed and bubble\nsize distribution. Optical methods are complementary in the sense that they can\nprovide high temporal and spatial resolution of single bubbles or bubble\nstreams from close distance. In this contribution we introduce a complete\ninstrument and evaluation method for optical bubble stream characterization\ntargeted at flows of up to 100ml/min and bubbles with a few millimeters radius.\nThe dedicated instrument employs a high-speed deep sea capable stereo camera\nsystem that can record terabytes of bubble imagery when deployed at a seep site\nfor later automated analysis. Bubble characteristics can be obtained for short\nsequences, then relocating the instrument to other locations, or in autonomous\nmode of definable intervals up to several days, in order to capture bubble flow\nvariations due to e.g. tide dependent pressure changes or reservoir depletion.\nBeside reporting the steps to make bubble characterization robust and\nautonomous, we carefully evaluate the reachable accuracy to be in the range of\n1-2\\% of the bubble radius and propose a novel auto-calibration procedure that,\ndue to the lack of point correspondences, uses only the silhouettes of bubbles.\nThe system has been operated successfully in 1000m water depth at the Cascadia\nmargin offshore Oregon to assess methane fluxes from various seep locations.\nBesides sample results we also report failure cases and lessons learnt during\ndeployment and method development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+She_M/0/1/0/all/0/1\">Mengkun She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_T/0/1/0/all/0/1\">Tim Wei&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_P/0/1/0/all/0/1\">Peter Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greinert_J/0/1/0/all/0/1\">Jens Greinert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koser_K/0/1/0/all/0/1\">Kevin K&#xf6;ser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AFDetV2: Rethinking the Necessity of the Second Stage for Object Detection from Point Clouds. (arXiv:2112.09205v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09205","description":"<p>There have been two streams in the 3D detection from point clouds:\nsingle-stage methods and two-stage methods. While the former is more\ncomputationally efficient, the latter usually provides better detection\naccuracy. By carefully examining the two-stage approaches, we have found that\nif appropriately designed, the first stage can produce accurate box regression.\nIn this scenario, the second stage mainly rescores the boxes such that the\nboxes with better localization get selected. From this observation, we have\ndevised a single-stage anchor-free network that can fulfill these requirements.\nThis network, named AFDetV2, extends the previous work by incorporating a\nself-calibrated convolution block in the backbone, a keypoint auxiliary\nsupervision, and an IoU prediction branch in the multi-task head. As a result,\nthe detection accuracy is drastically boosted in the single-stage. To evaluate\nour approach, we have conducted extensive experiments on the Waymo Open Dataset\nand the nuScenes Dataset. We have observed that our AFDetV2 achieves the\nstate-of-the-art results on these two datasets, superior to all the prior arts,\nincluding both the single-stage and the two-stage 3D detectors. AFDetV2 won the\n1st place in the Real-Time 3D Detection of the Waymo Open Dataset Challenge\n2021. In addition, a variant of our model AFDetV2-Base was entitled the \"Most\nEfficient Model\" by the Challenge Sponsor, showing a superior computational\nefficiency. To demonstrate the generality of this single-stage method, we have\nalso applied it to the first stage of the two-stage networks. Without\nexception, the results show that with the strengthened backbone and the\nrescoring approach, the second stage refinement is no longer needed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yihan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuangzhuang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Runzhou Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wenxin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Li Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Vision-Language Pre-training with Limited Resources. (arXiv:2112.09331v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09331","description":"<p>Pioneering dual-encoder pre-training works (e.g., CLIP and ALIGN) have\nrevealed the potential of aligning multi-modal representations with contrastive\nlearning. However, these works require a tremendous amount of data and\ncomputational resources (e.g., billion-level web data and hundreds of GPUs),\nwhich prevent researchers with limited resources from reproduction and further\nexploration. To this end, we propose a stack of novel methods, which\nsignificantly cut down the heavy resource dependency and allow us to conduct\ndual-encoder multi-modal representation alignment with limited resources.\nBesides, we provide a reproducible baseline of competitive results, namely\nZeroVL, with only 14M publicly accessible academic datasets and 8 V100 GPUs.\nAdditionally, we collect 100M web data for pre-training, and achieve comparable\nor superior results than state-of-the-art methods, further proving the\neffectiveness of our methods on large-scale data. We hope that this work will\nprovide useful data points and experience for future research in contrastive\nvision-language pre-training. Code is available at\nhttps://github.com/zerovl/ZeroVL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Quan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Boyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Weidong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshie_O/0/1/0/all/0/1\">Osamu Yoshie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Visual Tracking with Exemplar Transformers. (arXiv:2112.09686v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09686","description":"<p>The design of more complex and powerful neural network models has\nsignificantly advanced the state-of-the-art in visual object tracking. These\nadvances can be attributed to deeper networks, or the introduction of new\nbuilding blocks, such as transformers. However, in the pursuit of increased\ntracking performance, runtime is often hindered. Furthermore, efficient\ntracking architectures have received surprisingly little attention. In this\npaper, we introduce the Exemplar Transformer, a transformer module utilizing a\nsingle instance level attention layer for realtime visual object tracking.\nE.T.Track, our visual tracker that incorporates Exemplar Transformer modules,\nruns at 47 FPS on a CPU. This is up to 8x faster than other transformer-based\nmodels. When compared to lightweight trackers that can operate in realtime on\nstandard CPUs, E.T.Track consistently outperforms all other methods on the\nLaSOT, OTB-100, NFS, TrackingNet, and VOT-ST2020 datasets. The code will be\nmade publicly available upon publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blatter_P/0/1/0/all/0/1\">Philippe Blatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1\">Menelaos Kanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iSegFormer: Interactive Segmentation via Transformers with Application to 3D Knee MR Images. (arXiv:2112.11325v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11325","description":"<p>We propose iSegFormer, a memory-efficient transformer that combines a Swin\ntransformer with a lightweight multilayer perceptron (MLP) decoder. With the\nefficient Swin transformer blocks for hierarchical self-attention and the\nsimple MLP decoder for aggregating both local and global attention, iSegFormer\nlearns powerful representations while achieving high computational\nefficiencies. Specifically, we apply iSegFormer to interactive 3D medical image\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenlin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yining Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages. (arXiv:2201.11732v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11732","description":"<p>Reliable evaluation benchmarks designed for replicability and\ncomprehensiveness have driven progress in machine learning. Due to the lack of\na multilingual benchmark, however, vision-and-language research has mostly\nfocused on English language tasks. To fill this gap, we introduce the\nImage-Grounded Language Understanding Evaluation benchmark. IGLUE brings\ntogether - by both aggregating pre-existing datasets and creating new ones -\nvisual question answering, cross-modal retrieval, grounded reasoning, and\ngrounded entailment tasks across 20 diverse languages. Our benchmark enables\nthe evaluation of multilingual multimodal models for transfer learning, not\nonly in a zero-shot setting, but also in newly defined few-shot learning\nsetups. Based on the evaluation of the available state-of-the-art models, we\nfind that translate-test transfer is superior to zero-shot transfer and that\nfew-shot learning is hard to harness for many tasks. Moreover, downstream\nperformance is partially explained by the amount of available unlabelled\ntextual data for pretraining, and only weakly by the typological distance of\ntarget-source languages. We hope to encourage future research efforts in this\narea by releasing the benchmark to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedMed-ATL: Misaligned Unpaired Brain Image Synthesis via Affine Transform Loss. (arXiv:2201.12589v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.12589","description":"<p>The existence of completely aligned and paired multi-modal neuroimaging data\nhas proved its effectiveness in the diagnosis of brain diseases. However,\ncollecting the full set of well-aligned and paired data is impractical, since\nthe practical difficulties may include high cost, long time acquisition, image\ncorruption, and privacy issues. Previously, the misaligned unpaired\nneuroimaging data (termed as MUD) are generally treated as noisy label.\nHowever, such a noisy label-based method fail to accomplish well when\nmisaligned data occurs distortions severely. For example, the angle of rotation\nis different. In this paper, we propose a novel federated self-supervised\nlearning (FedMed) for brain image synthesis. An affine transform loss (ATL) was\nformulated to make use of severely distorted images without violating privacy\nlegislation for the hospital. We then introduce a new data augmentation\nprocedure for self-supervised training and fed it into three auxiliary heads,\nnamely auxiliary rotation, auxiliary translation and auxiliary scaling heads.\nThe proposed method demonstrates the advanced performance in both the quality\nof our synthesized results under a severely misaligned and unpaired data\nsetting, and better stability than other GAN-based algorithms. The proposed\nmethod also reduces the demand for deformable registration while encouraging to\nleverage the misaligned and unpaired data. Experimental results verify the\noutstanding performance of our learning paradigm compared to other\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinbao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_G/0/1/0/all/0/1\">Guoyang Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yawen Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1\">Yaochu Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MINER: Multiscale Implicit Neural Representations. (arXiv:2202.03532v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03532","description":"<p>We introduce a new neural signal model designed for efficient high-resolution\nrepresentation of large-scale signals. The key innovation in our multiscale\nimplicit neural representation (MINER) is an internal representation via a\nLaplacian pyramid, which provides a sparse multiscale decomposition of the\nsignal that captures orthogonal parts of the signal across scales. We leverage\nthe advantages of the Laplacian pyramid by representing small disjoint patches\nof the pyramid at each scale with a small MLP. This enables the capacity of the\nnetwork to adaptively increase from coarse to fine scales, and only represent\nparts of the signal with strong signal energy. The parameters of each MLP are\noptimized from coarse-to-fine scale which results in faster approximations at\ncoarser scales, thereby ultimately an extremely fast training process. We apply\nMINER to a range of large-scale signal representation tasks, including\ngigapixel images and very large point clouds, and demonstrate that it requires\nfewer than 25% of the parameters, 33% of the memory footprint, and 10% of the\ncomputation time of competing techniques such as ACORN to reach the same\nrepresentation accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saragadam_V/0/1/0/all/0/1\">Vishwanath Saragadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jasper Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_G/0/1/0/all/0/1\">Guha Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeraraghavan_A/0/1/0/all/0/1\">Ashok Veeraraghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NIMBLE: A Non-rigid Hand Model with Bones and Muscles. (arXiv:2202.04533v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04533","description":"<p>Emerging Metaverse applications demand reliable, accurate, and photorealistic\nreproductions of human hands to perform sophisticated operations as if in the\nphysical world. While real human hand represents one of the most intricate\ncoordination between bones, muscle, tendon, and skin, state-of-the-art\ntechniques unanimously focus on modeling only the skeleton of the hand. In this\npaper, we present NIMBLE, a novel parametric hand model that includes the\nmissing key components, bringing 3D hand model to a new level of realism. We\nfirst annotate muscles, bones and skins on the recent Magnetic Resonance\nImaging hand (MRI-Hand) dataset and then register a volumetric template hand\nonto individual poses and subjects within the dataset. NIMBLE consists of 20\nbones as triangular meshes, 7 muscle groups as tetrahedral meshes, and a skin\nmesh. Via iterative shape registration and parameter learning, it further\nproduces shape blend shapes, pose blend shapes, and a joint regressor. We\ndemonstrate applying NIMBLE to modeling, rendering, and visual inference tasks.\nBy enforcing the inner bones and muscles to match anatomic and kinematic rules,\nNIMBLE can animate 3D hands to new poses at unprecedented realism. To model the\nappearance of skin, we further construct a photometric HandStage to acquire\nhigh-quality textures and normal maps to model wrinkles and palm print.\nFinally, NIMBLE also benefits learning-based hand pose and shape estimation by\neither synthesizing rich data or acting directly as a differentiable layer in\nthe inference network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zesong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yingwenqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Network for Cell Tracking in Microscopy Videos. (arXiv:2202.04731v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04731","description":"<p>We present a novel graph neural network (GNN) approach for cell tracking in\nhigh-throughput microscopy videos. By modeling the entire time-lapse sequence\nas a direct graph where cell instances are represented by its nodes and their\nassociations by its edges, we extract the entire set of cell trajectories by\nlooking for the maximal paths in the graph. This is accomplished by several key\ncontributions incorporated into an end-to-end deep learning framework. We\nexploit a deep metric learning algorithm to extract cell feature vectors that\ndistinguish between instances of different biological cells and assemble same\ncell instances. We introduce a new GNN block type which enables a mutual update\nof node and edge feature vectors, thus facilitating the underlying message\npassing process. The message passing concept, whose extent is determined by the\nnumber of GNN blocks, is of fundamental importance as it enables the `flow' of\ninformation between nodes and edges much behind their neighbors in consecutive\nframes. Finally, we solve an edge classification problem and use the identified\nactive edges to construct the cells' tracks and lineage trees. We demonstrate\nthe strengths of the proposed cell tracking approach by applying it to 2D and\n3D datasets of different cell types, imaging setups, and experimental\nconditions. We show that our framework outperforms current state-of-the-art\nmethods on most of the evaluated datasets. The code is available at our\nrepository: https://github.com/talbenha/cell-tracker-gnn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Haim_T/0/1/0/all/0/1\">Tal Ben-Haim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_T/0/1/0/all/0/1\">Tammy Riklin Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FILM: Frame Interpolation for Large Motion. (arXiv:2202.04901v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04901","description":"<p>We present a frame interpolation algorithm that synthesizes multiple\nintermediate frames from two input images with large in-between motion. Recent\nmethods use multiple networks to estimate optical flow or depth and a separate\nnetwork dedicated to frame synthesis. This is often complex and requires scarce\noptical flow or depth ground-truth. In this work, we present a single unified\nnetwork, distinguished by a multi-scale feature extractor that shares weights\nat all scales, and is trainable from frames alone. To synthesize crisp and\npleasing frames, we propose to optimize our network with the Gram matrix loss\nthat measures the correlation difference between feature maps. Our approach\noutperforms state-of-the-art methods on the Xiph large motion benchmark. We\nalso achieve higher scores on Vimeo-90K, Middlebury and UCF101, when comparing\nto methods that use perceptual losses. We study the effect of weight sharing\nand of training with datasets of increasing motion range. Finally, we\ndemonstrate our model's effectiveness in synthesizing high quality and\ntemporally coherent videos on a challenging near-duplicate photos dataset.\nCodes and pre-trained models are available at https://film-net.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reda_F/0/1/0/all/0/1\">Fitsum Reda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontkanen_J/0/1/0/all/0/1\">Janne Kontkanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabellion_E/0/1/0/all/0/1\">Eric Tabellion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Deqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantofaru_C/0/1/0/all/0/1\">Caroline Pantofaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curless_B/0/1/0/all/0/1\">Brian Curless</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D2ADA: Dynamic Density-aware Active Domain Adaptation for Semantic Segmentation. (arXiv:2202.06484v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06484","description":"<p>In the field of domain adaptation, a trade-off exists between the model\nperformance and the number of target domain annotations. Active learning,\nmaximizing model performance with few informative labeled data, comes in handy\nfor such a scenario. In this work, we present D2ADA, a general active domain\nadaptation framework for semantic segmentation. To adapt the model to the\ntarget domain with minimum queried labels, we propose acquiring labels of the\nsamples with high probability density in the target domain yet with low\nprobability density in the source domain, complementary to the existing source\ndomain labeled data. To further facilitate labeling efficiency, we design a\ndynamic scheduling policy to adjust the labeling budgets between domain\nexploration and model uncertainty over time. Extensive experiments show that\nour method outperforms existing active learning and domain adaptation baselines\non two benchmarks, GTA5 -&gt; Cityscapes and SYNTHIA -&gt; Cityscapes. With less than\n5% target domain annotations, our method reaches comparable results with that\nof full supervision. Our code is publicly available at\nhttps://github.com/tsunghan-wu/D2ADA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liou_Y/0/1/0/all/0/1\">Yi-Syuan Liou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shao-Ji Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tung-I Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Chih Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Adversarial Examples in Remote Sensing: Methodology and Benchmark. (arXiv:2202.07054v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07054","description":"<p>Deep neural networks have achieved great success in many important remote\nsensing tasks. Nevertheless, their vulnerability to adversarial examples should\nnot be neglected. In this study, we systematically analyze the universal\nadversarial examples in remote sensing data for the first time, without any\nknowledge from the victim model. Specifically, we propose a novel black-box\nadversarial attack method, namely Mixup-Attack, and its simple variant\nMixcut-Attack, for remote sensing data. The key idea of the proposed methods is\nto find common vulnerabilities among different networks by attacking the\nfeatures in the shallow layer of a given surrogate model. Despite their\nsimplicity, the proposed methods can generate transferable adversarial examples\nthat deceive most of the state-of-the-art deep neural networks in both scene\nclassification and semantic segmentation tasks with high success rates. We\nfurther provide the generated universal adversarial examples in the dataset\nnamed UAE-RS, which is the first dataset that provides black-box adversarial\nsamples in the remote sensing field. We hope UAE-RS may serve as a benchmark\nthat helps researchers to design deep neural networks with strong resistance\ntoward adversarial attacks in the remote sensing field. Codes and the UAE-RS\ndataset are available online (https://github.com/YonghaoXu/UAE-RS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yonghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1\">Pedram Ghamisi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification. (arXiv:2202.07570v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07570","description":"<p>Progress in digital pathology is hindered by high-resolution images and the\nprohibitive cost of exhaustive localized annotations. The commonly used\nparadigm to categorize pathology images is patch-based processing, which often\nincorporates multiple instance learning (MIL) to aggregate local patch-level\nrepresentations yielding image-level prediction. Nonetheless, diagnostically\nrelevant regions may only take a small fraction of the whole tissue, and\ncurrent MIL-based approaches often process images uniformly, discarding the\ninter-patches interactions. To alleviate these issues, we propose ScoreNet, a\nnew efficient transformer that exploits a differentiable recommendation stage\nto extract discriminative image regions and dedicate computational resources\naccordingly. The proposed transformer leverages the local and global attention\nof a few dynamically recommended high-resolution regions at an efficient\ncomputational cost. We further introduce a novel mixing data-augmentation,\nnamely ScoreMix, by leveraging the image's semantic distribution to guide the\ndata mixing and produce coherent sample-label pairs. ScoreMix is embarrassingly\nsimple and mitigates the pitfalls of previous augmentations, which assume a\nuniform semantic distribution and risk mislabeling the samples. Thorough\nexperiments and ablation studies on three breast cancer histology datasets of\nHaematoxylin &amp; Eosin (H&amp;E) have validated the superiority of our approach over\nprior arts, including transformer-based models on tumour regions-of-interest\n(TRoIs) classification. ScoreNet equipped with proposed ScoreMix augmentation\ndemonstrates better generalization capabilities and achieves new\nstate-of-the-art (SOTA) results with only 50% of the data compared to other\nmixing augmentation variants. Finally, ScoreNet yields high efficacy and\noutperforms SOTA efficient transformers, namely TransPath and SwinTransformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stegmuller_T/0/1/0/all/0/1\">Thomas Stegm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1\">Behzad Bozorgtabar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spahr_A/0/1/0/all/0/1\">Antoine Spahr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1\">Jean-Philippe Thiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"V2X-Sim: Multi-Agent Collaborative Perception Dataset and Benchmark for Autonomous Driving. (arXiv:2202.08449v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08449","description":"<p>Vehicle-to-everything (V2X) communication techniques enable the collaboration\nbetween vehicles and many other entities in the neighboring environment, which\ncould fundamentally improve the perception system for autonomous driving.\nHowever, the lack of a public dataset significantly restricts the research\nprogress of collaborative perception. To fill this gap, we present V2X-Sim, a\ncomprehensive simulated multi-agent perception dataset for V2X-aided autonomous\ndriving. V2X-Sim provides: (1) \\hl{multi-agent} sensor recordings from the\nroad-side unit (RSU) and multiple vehicles that enable collaborative\nperception, (2) multi-modality sensor streams that facilitate multi-modality\nperception, and (3) diverse ground truths that support various perception\ntasks. Meanwhile, we build an open-source testbed and provide a benchmark for\nthe state-of-the-art collaborative perception algorithms on three tasks,\nincluding detection, tracking and segmentation. V2X-Sim seeks to stimulate\ncollaborative perception research for autonomous driving before realistic\ndatasets become widely available. Our dataset and code are available at\n\\url{https://ai4ce.github.io/V2X-Sim/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Dekun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Ziyan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chen Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Vision-Language Pre-Trained Models. (arXiv:2202.10936v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10936","description":"<p>As transformer evolves, pre-trained models have advanced at a breakneck pace\nin recent years. They have dominated the mainstream techniques in natural\nlanguage processing (NLP) and computer vision (CV). How to adapt pre-training\nto the field of Vision-and-Language (V-L) learning and improve downstream task\nperformance becomes a focus of multimodal learning. In this paper, we review\nthe recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the\ncore content, we first briefly introduce several ways to encode raw images and\ntexts to single-modal embeddings before pre-training. Then, we dive into the\nmainstream architectures of VL-PTMs in modeling the interaction between text\nand image representations. We further present widely-used pre-training tasks,\nand then we introduce some common downstream tasks. We finally conclude this\npaper and present some promising research directions. Our survey aims to\nprovide researchers with synthesis and pointer to related research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GroupViT: Semantic Segmentation Emerges from Text Supervision. (arXiv:2202.11094v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11094","description":"<p>Grouping and recognition are important components of visual scene\nunderstanding, e.g., for object detection and semantic segmentation. With\nend-to-end deep learning systems, grouping of image regions usually happens\nimplicitly via top-down supervision from pixel-level recognition labels.\nInstead, in this paper, we propose to bring back the grouping mechanism into\ndeep networks, which allows semantic segments to emerge automatically with only\ntext supervision. We propose a hierarchical Grouping Vision Transformer\n(GroupViT), which goes beyond the regular grid structure representation and\nlearns to group image regions into progressively larger arbitrary-shaped\nsegments. We train GroupViT jointly with a text encoder on a large-scale\nimage-text dataset via contrastive losses. With only text supervision and\nwithout any pixel-level annotations, GroupViT learns to group together semantic\nregions and successfully transfers to the task of semantic segmentation in a\nzero-shot manner, i.e., without any further fine-tuning. It achieves a\nzero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on\nPASCAL Context datasets, and performs competitively to state-of-the-art\ntransfer-learning methods requiring greater levels of supervision. We\nopen-source our code at https://github.com/NVlabs/GroupViT .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1\">Wonmin Byeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1\">Thomas Breuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LF-VIO: A Visual-Inertial-Odometry Framework for Large Field-of-View Cameras with Negative Plane. (arXiv:2202.12613v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12613","description":"<p>Visual-inertial-odometry has attracted extensive attention in the field of\nautonomous driving and robotics. The size of Field of View (FoV) plays an\nimportant role in Visual-Odometry (VO) and Visual-Inertial-Odometry (VIO), as a\nlarge FoV enables to perceive a wide range of surrounding scene elements and\nfeatures. However, when the field of the camera reaches the negative half\nplane, one cannot simply use [u,v,1]^T to represent the image feature points\nanymore. To tackle this issue, we propose LF-VIO, a real-time VIO framework for\ncameras with extremely large FoV. We leverage a three-dimensional vector with\nunit length to represent feature points, and design a series of algorithms to\novercome this challenge. To address the scarcity of panoramic visual odometry\ndatasets with ground-truth location and pose, we present the PALVIO dataset,\ncollected with a Panoramic Annular Lens (PAL) system with an entire FoV of\n360{\\deg}x(40{\\deg}-120{\\deg}) and an IMU sensor. With a comprehensive variety\nof experiments, the proposed LF-VIO is verified on both the established PALVIO\nbenchmark and a public fisheye camera dataset with a FoV of\n360{\\deg}x(0{\\deg}-93.5{\\deg}). LF-VIO outperforms state-of-the-art\nvisual-inertial-odometry methods. Our dataset and code are made publicly\navailable at https://github.com/flysoaryun/LF-VIO\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminability-Transferability Trade-Off: An Information-Theoretic Perspective. (arXiv:2203.03871v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03871","description":"<p>This work simultaneously considers the discriminability and transferability\nproperties of deep representations in the typical supervised learning task,\ni.e., image classification. By a comprehensive temporal analysis, we observe a\ntrade-off between these two properties. The discriminability keeps increasing\nwith the training progressing while the transferability intensely diminishes in\nthe later training period.\n</p>\n<p>From the perspective of information-bottleneck theory, we reveal that the\nincompatibility between discriminability and transferability is attributed to\nthe over-compression of input information. More importantly, we investigate why\nand how the InfoNCE loss can alleviate the over-compression, and further\npresent a learning framework, named contrastive temporal coding~(CTC), to\ncounteract the over-compression and alleviate the incompatibility. Extensive\nexperiments validate that CTC successfully mitigates the incompatibility,\nyielding discriminative and transferable representations. Noticeable\nimprovements are achieved on the image classification task and challenging\ntransfer learning tasks. We hope that this work will raise the significance of\nthe transferability property in the conventional supervised learning setting.\nCode is available at https://github.com/DTennant/dt-tradeoff.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Quan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhao-Min Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Borui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Renjie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiajun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Boyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshie_O/0/1/0/all/0/1\">Osamu Yoshie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GaitEdge: Beyond Plain End-to-end Gait Recognition for Better Practicality. (arXiv:2203.03972v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03972","description":"<p>Gait is one of the most promising biometrics to identify individuals at a\nlong distance. Although most previous methods have focused on recognizing the\nsilhouettes, several end-to-end methods that extract gait features directly\nfrom RGB images perform better. However, we demonstrate that these end-to-end\nmethods may inevitably suffer from the gait-irrelevant noises, i.e., low-level\ntexture and colorful information. Experimentally, we design the cross-domain\nevaluation to support this view. In this work, we propose a novel end-to-end\nframework named GaitEdge which can effectively block gait-irrelevant\ninformation and release end-to-end training potential. Specifically, GaitEdge\nsynthesizes the output of the pedestrian segmentation network and then feeds it\nto the subsequent recognition network, where the synthetic silhouettes consist\nof trainable edges of bodies and fixed interiors to limit the information that\nthe recognition network receives. Besides, GaitAlign for aligning silhouettes\nis embedded into the GaitEdge without losing differentiability. Experimental\nresults on CASIA-B and our newly built TTG-200 indicate that GaitEdge\nsignificantly outperforms the previous methods and provides a more practical\nend-to-end paradigm. All the source code are available at\nhttps://github.com/ShiqiYu/OpenGait.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Junhao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Saihui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chuanfu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shiqi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backbone is All Your Need: A Simplified Architecture for Visual Object Tracking. (arXiv:2203.05328v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05328","description":"<p>Exploiting a general-purpose neural architecture to replace hand-wired\ndesigns or inductive biases has recently drawn extensive interest. However,\nexisting tracking approaches rely on customized sub-modules and need prior\nknowledge for architecture selection, hindering the tracking development in a\nmore general system. This paper presents a Simplified Tracking architecture\n(SimTrack) by leveraging a transformer backbone for joint feature extraction\nand interaction. Unlike existing Siamese trackers, we serialize the input\nimages and concatenate them directly before the one-branch backbone. Feature\ninteraction in the backbone helps to remove well-designed interaction modules\nand produce a more efficient and effective framework. To reduce the information\nloss from down-sampling in vision transformers, we further propose a foveal\nwindow strategy, providing more diverse input patches with acceptable\ncomputational costs. Our SimTrack improves the baseline with 2.5%/2.6% AUC\ngains on LaSOT/TNL2K and gets results competitive with other specialized\ntracking algorithms without bells and whistles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Lei Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1\">Qiuhong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Weihao Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction-Guided Distillation for Dense Object Detection. (arXiv:2203.05469v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05469","description":"<p>Real-world object detection models should be cheap and accurate. Knowledge\ndistillation (KD) can boost the accuracy of a small, cheap detection model by\nleveraging useful information from a larger teacher model. However, a key\nchallenge is identifying the most informative features produced by the teacher\nfor distillation. In this work, we show that only a very small fraction of\nfeatures within a ground-truth bounding box are responsible for a teacher's\nhigh detection performance. Based on this, we propose Prediction-Guided\nDistillation (PGD), which focuses distillation on these key predictive regions\nof the teacher and yields considerable gains in performance over many existing\nKD baselines. In addition, we propose an adaptive weighting scheme over the key\nregions to smooth out their influence and achieve even better performance. Our\nproposed approach outperforms current state-of-the-art KD baselines on a\nvariety of advanced one-stage detection architectures. Specifically, on the\nCOCO dataset, our method achieves between +3.1% and +4.6% AP improvement using\nResNet-101 and ResNet-50 as the teacher and student backbones, respectively. On\nthe CrowdHuman dataset, we achieve +3.2% and +2.0% improvements in MR and AP,\nalso using these backbones. Our code is available at\nhttps://github.com/ChenhongyiYang/PGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenhongyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochal_M/0/1/0/all/0/1\">Mateusz Ochal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1\">Amos Storkey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crowley_E/0/1/0/all/0/1\">Elliot J. Crowley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VPFusion: Joint 3D Volume and Pixel-Aligned Feature Fusion for Single and Multi-view 3D Reconstruction. (arXiv:2203.07553v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07553","description":"<p>We introduce a unified single and multi-view neural implicit 3D\nreconstruction framework VPFusion. VPFusion attains high-quality reconstruction\nusing both - 3D feature volume to capture 3D-structure-aware context, and\npixel-aligned image features to capture fine local detail. Existing approaches\nuse RNN, feature pooling, or attention computed independently in each view for\nmulti-view fusion. RNNs suffer from long-term memory loss and permutation\nvariance, while feature pooling or independently computed attention leads to\nrepresentation in each view being unaware of other views before the final\npooling step. In contrast, we show improved multi-view feature fusion by\nestablishing transformer-based pairwise view association. In particular, we\npropose a novel interleaved 3D reasoning and pairwise view association\narchitecture for feature volume fusion across different views. Using this\nstructure-aware and multi-view-aware feature volume, we show improved 3D\nreconstruction performance compared to existing methods. VPFusion improves the\nreconstruction quality further by also incorporating pixel-aligned local image\nfeatures to capture fine detail. We verify the effectiveness of VPFusion on the\nShapeNet and ModelNet datasets, where we outperform or perform on-par the\nstate-of-the-art single and multi-view 3D shape reconstruction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_J/0/1/0/all/0/1\">Jisan Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frahm_J/0/1/0/all/0/1\">Jan-Michael Frahm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverted Pyramid Multi-task Transformer for Dense Scene Understanding. (arXiv:2203.07997v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07997","description":"<p>Multi-task dense scene understanding is a thriving research domain that\nrequires simultaneous perception and reasoning on a series of correlated tasks\nwith pixel-wise prediction. Most existing works encounter a severe limitation\nof modeling in the locality due to heavy utilization of convolution operations,\nwhile learning interactions and inference in a global spatial-position and\nmulti-task context is critical for this problem. In this paper, we propose a\nnovel end-to-end Inverted Pyramid multi-task Transformer (InvPT) to perform\nsimultaneous modeling of spatial positions and multiple tasks in a unified\nframework. To the best of our knowledge, this is the first work that explores\ndesigning a transformer structure for multi-task dense prediction for scene\nunderstanding. Besides, it is widely demonstrated that a higher spatial\nresolution is remarkably beneficial for dense predictions, while it is very\nchallenging for existing transformers to go deeper with higher resolutions due\nto huge complexity to large spatial size. InvPT presents an efficient\nUP-Transformer block to learn multi-task feature interaction at gradually\nincreased resolutions, which also incorporates effective self-attention message\npassing and multi-scale feature aggregation to produce task-specific prediction\nat a high resolution. Our method achieves superior multi-task performance on\nNYUD-v2 and PASCAL-Context datasets respectively, and significantly outperforms\nprevious state-of-the-arts. The code is available at\nhttps://github.com/prismformore/InvPT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hanrong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation. (arXiv:2203.09836v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09836","description":"<p>Most recent 6D object pose estimation methods, including unsupervised ones,\nrequire many real training images. Unfortunately, for some applications, such\nas those in space or deep under water, acquiring real images, even unannotated,\nis virtually impossible. In this paper, we propose a method that can be trained\nsolely on synthetic images, or optionally using a few additional real ones.\nGiven a rough pose estimate obtained from a first network, it uses a second\nnetwork to predict a dense 2D correspondence field between the image rendered\nusing the rough pose and the real image and infers the required pose\ncorrection. This approach is much less sensitive to the domain shift between\nsynthetic and real images than state-of-the-art methods. It performs on par\nwith methods that require annotated real images for training when not using\nany, and outperforms them considerably when using as few as twenty real images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yinlin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FAR: Fourier Aerial Video Recognition. (arXiv:2203.10694v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10694","description":"<p>We present an algorithm, Fourier Activity Recognition (FAR), for UAV video\nactivity recognition. Our formulation uses a novel Fourier object\ndisentanglement method to innately separate out the human agent (which is\ntypically small) from the background. Our disentanglement technique operates in\nthe frequency domain to characterize the extent of temporal change of spatial\npixels, and exploits convolution-multiplication properties of Fourier transform\nto map this representation to the corresponding object-background entangled\nfeatures obtained from the network. To encapsulate contextual information and\nlong-range space-time dependencies, we present a novel Fourier Attention\nalgorithm, which emulates the benefits of self-attention by modeling the\nweighted outer product in the frequency domain. Our Fourier attention\nformulation uses much fewer computations than self-attention. We have evaluated\nour approach on multiple UAV datasets including UAV Human RGB, UAV Human Night,\nDrone Action, and NEC Drone. We demonstrate a relative improvement of 8.02% -\n38.69% in top-1 accuracy and up to 3 times faster over prior works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kothandaraman_D/0/1/0/all/0/1\">Divya Kothandaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1\">Tianrui Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Sean Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compression of Generative Pre-trained Language Models via Quantization. (arXiv:2203.10705v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10705","description":"<p>The increasing size of generative Pre-trained Language Models (PLMs) has\ngreatly increased the demand for model compression. Despite various methods to\ncompress BERT or its variants, there are few attempts to compress generative\nPLMs, and the underlying difficulty remains unclear. In this paper, we compress\ngenerative PLMs by quantization. We find that previous quantization methods\nfail on generative tasks due to the \\textit{homogeneous word embeddings} caused\nby reduced capacity, and \\textit{varied distribution of weights}.\nCorrespondingly, we propose a token-level contrastive distillation to learn\ndistinguishable word embeddings, and a module-wise dynamic scaling to make\nquantizers adaptive to different modules. Empirical results on various tasks\nshow that our proposed method outperforms the state-of-the-art compression\nmethods on generative PLMs by a clear margin. With comparable performance with\nthe full-precision models, we achieve 14.4x and 13.4x compression rates on\nGPT-2 and BART, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chaofan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Ngai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer. (arXiv:2203.10790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10790","description":"<p>The vanilla self-attention mechanism inherently relies on pre-defined and\nsteadfast computational dimensions. Such inflexibility restricts it from\npossessing context-oriented generalization that can bring more contextual cues\nand global representations. To mitigate this issue, we propose a Scalable\nSelf-Attention (SSA) mechanism that leverages two scaling factors to release\ndimensions of query, key, and value matrices while unbinding them with the\ninput. This scalability fetches context-oriented generalization and enhances\nobject sensitivity, which pushes the whole network into a more effective\ntrade-off state between accuracy and cost. Furthermore, we propose an\nInteractive Window-based Self-Attention (IWSA), which establishes interaction\nbetween non-overlapping regions by re-merging independent value tokens and\naggregating spatial information from adjacent windows. By stacking the SSA and\nIWSA alternately, the Scalable Vision Transformer (ScalableViT) achieves\nstate-of-the-art performance in general-purpose vision tasks. For example,\nScalableViT-S outperforms Twins-SVT-S by 1.4% and Swin-T by 1.8% on ImageNet-1K\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hailong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yansong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Min Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Heads or Tails: Towards Semantically Consistent Visual Counterfactuals. (arXiv:2203.12892v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12892","description":"<p>A visual counterfactual explanation replaces image regions in a query image\nwith regions from a distractor image such that the system's decision on the\ntransformed image changes to the distractor class. In this work, we present a\nnovel framework for computing visual counterfactual explanations based on two\nkey ideas. First, we enforce that the replaced and replacer regions contain the\nsame semantic part, resulting in more semantically consistent explanations.\nSecond, we use multiple distractor images in a computationally efficient way\nand obtain more discriminative explanations with fewer region replacements. Our\napproach is 27 % more semantically consistent and an order of magnitude faster\nthan a competing method on three fine-grained image recognition datasets. We\nhighlight the utility of our counterfactuals over existing works through\nmachine teaching experiments where we teach humans to classify different bird\nspecies. We also complement our explanations with the vocabulary of parts and\nattributes that contributed the most to the system's decision. In this task as\nwell, we obtain state-of-the-art results when using our counterfactual\nexplanations relative to existing works, reinforcing the importance of\nsemantically consistent explanations. Source code is available at\nhttps://github.com/facebookresearch/visual-counterfactuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vandenhende_S/0/1/0/all/0/1\">Simon Vandenhende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Dhruv Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiyaram_D/0/1/0/all/0/1\">Deepti Ghadiyaram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Perturbation-Constrained Adversarial Attack for Evaluating the Robustness of Optical Flow. (arXiv:2203.13214v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13214","description":"<p>Recent optical flow methods are almost exclusively judged in terms of\naccuracy, while their robustness is often neglected. Although adversarial\nattacks offer a useful tool to perform such an analysis, current attacks on\noptical flow methods focus on real-world attacking scenarios rather than a\nworst case robustness assessment. Hence, in this work, we propose a novel\nadversarial attack - the Perturbation-Constrained Flow Attack (PCFA) - that\nemphasizes destructivity over applicability as a real-world attack. PCFA is a\nglobal attack that optimizes adversarial perturbations to shift the predicted\nflow towards a specified target flow, while keeping the L2 norm of the\nperturbation below a chosen bound. Our experiments demonstrate PCFA's\napplicability in white- and black-box settings, and show it finds stronger\nadversarial samples than previous attacks. Based on these strong samples, we\nprovide the first joint ranking of optical flow methods considering both\nprediction quality and adversarial robustness, which reveals state-of-the-art\nmethods to be particularly vulnerable. Code is available at\nhttps://github.com/cv-stuttgart/PCFA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmalfuss_J/0/1/0/all/0/1\">Jenny Schmalfuss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholze_P/0/1/0/all/0/1\">Philipp Scholze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruhn_A/0/1/0/all/0/1\">Andr&#xe9;s Bruhn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptDet: Towards Open-vocabulary Detection using Uncurated Images. (arXiv:2203.16513v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16513","description":"<p>The goal of this work is to establish a scalable pipeline for expanding an\nobject detector towards novel/unseen categories, using zero manual annotations.\nTo achieve that, we make the following four contributions: (i) in pursuit of\ngeneralisation, we propose a two-stage open-vocabulary object detector, where\nthe class-agnostic object proposals are classified with a text encoder from\npre-trained visual-language model; (ii) To pair the visual latent space (of RPN\nbox proposals) with that of the pre-trained text encoder, we propose the idea\nof regional prompt learning to align the textual embedding space with regional\nvisual object features; (iii) To scale up the learning procedure towards\ndetecting a wider spectrum of objects, we exploit the available online resource\nvia a novel self-training framework, which allows to train the proposed\ndetector on a large corpus of noisy uncurated web images. Lastly, (iv) to\nevaluate our proposed detector, termed as PromptDet, we conduct extensive\nexperiments on the challenging LVIS and MS-COCO dataset. PromptDet shows\nsuperior performance over existing approaches with fewer additional training\nimages and zero manual annotations whatsoever. Project page with code:\nhttps://fcjian.github.io/promptdet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chengjian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yujie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zequn Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haibing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaolin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Scene Understanding via Disentangled Instance Mesh Reconstruction. (arXiv:2203.16832v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16832","description":"<p>Semantic scene reconstruction from point cloud is an essential and\nchallenging task for 3D scene understanding. This task requires not only to\nrecognize each instance in the scene, but also to recover their geometries\nbased on the partial observed point cloud. Existing methods usually attempt to\ndirectly predict occupancy values of the complete object based on incomplete\npoint cloud proposals from a detection-based backbone. However, this framework\nalways fails to reconstruct high fidelity mesh due to the obstruction of\nvarious detected false positive object proposals and the ambiguity of\nincomplete point observations for learning occupancy values of complete\nobjects. To circumvent the hurdle, we propose a Disentangled Instance Mesh\nReconstruction (DIMR) framework for effective point scene understanding. A\nsegmentation-based backbone is applied to reduce false positive object\nproposals, which further benefits our exploration on the relationship between\nrecognition and reconstruction. Based on the accurate proposals, we leverage a\nmesh-aware latent code space to disentangle the processes of shape completion\nand mesh generation, relieving the ambiguity caused by the incomplete point\nobservations. Furthermore, with access to the CAD model pool at test time, our\nmodel can also be used to improve the reconstruction quality by performing mesh\nretrieval without extra training. We thoroughly evaluate the reconstructed mesh\nquality with multiple metrics, and demonstrate the superiority of our method on\nthe challenging ScanNet dataset. Code is available at\n\\url{https://github.com/ashawkey/dimr}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaxiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Gang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining. (arXiv:2204.02393v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02393","description":"<p>Deep visuomotor policy learning, which aims to map raw visual observation to\naction, achieves promising results in control tasks such as robotic\nmanipulation and autonomous driving. However, it requires a huge number of\nonline interactions with the training environment, which limits its real-world\napplication. Compared to the popular unsupervised feature learning for visual\nrecognition, feature pretraining for visuomotor control tasks is much less\nexplored. In this work, we aim to pretrain policy representations for driving\ntasks by watching hours-long uncurated YouTube videos. Specifically, we train\nan inverse dynamic model with a small amount of labeled data and use it to\npredict action labels for all the YouTube video frames. A new contrastive\npolicy pretraining method is then developed to learn action-conditioned\nfeatures from the video frames with pseudo action labels. Experiments show that\nthe resulting action-conditioned features obtain substantial improvements for\nthe downstream reinforcement learning and imitation learning tasks,\noutperforming the weights pretrained from previous unsupervised learning\nmethods and ImageNet pretrained weight. Code, model weights, and data are\navailable at: https://metadriverse.github.io/ACO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qihang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhenghao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BMD: A General Class-balanced Multicentric Dynamic Prototype Strategy for Source-free Domain Adaptation. (arXiv:2204.02811v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02811","description":"<p>Source-free Domain Adaptation (SFDA) aims to adapt a pre-trained source model\nto the unlabeled target domain without accessing the well-labeled source data,\nwhich is a much more practical setting due to the data privacy, security, and\ntransmission issues. To make up for the absence of source data, most existing\nmethods introduced feature prototype based pseudo-labeling strategies to\nrealize self-training model adaptation. However, feature prototypes are\nobtained by instance-level predictions based feature clustering, which is\ncategory-biased and tends to result in noisy labels since the visual domain\ngaps between source and target are usually different between categories. In\naddition, we found that a monocentric feature prototype may be ineffective to\nrepresent each category and introduce negative transfer, especially for those\nhard-transfer data. To address these issues, we propose a general\nclass-Balanced Multicentric Dynamic prototype (BMD) strategy for the SFDA task.\nSpecifically, for each target category, we first introduce a global inter-class\nbalanced sampling strategy to aggregate potential representative target\nsamples. Then, we design an intra-class multicentric clustering strategy to\nachieve more robust and representative prototypes generation. In contrast to\nexisting strategies that update the pseudo label at a fixed training period, we\nfurther introduce a dynamic pseudo labeling strategy to incorporate network\nupdate information during model adaptation. Extensive experiments show that the\nproposed model-agnostic BMD strategy significantly improves representative SFDA\nmethods to yield new state-of-the-art results. The code is available at\nhttps://github.com/ispc-lab/BMD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Sanqing Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhijun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChildCI Framework: Analysis of Motor and Cognitive Development in Children-Computer Interaction for Age Detection. (arXiv:2204.04236v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2204.04236","description":"<p>This article presents a comprehensive analysis of the different tests\nproposed in the recent ChildCI framework, proving its potential for generating\na better understanding of children's neuromotor and cognitive development along\ntime, as well as their possible application in other research areas such as\ne-Health and e-Learning. In particular, we propose a set of over 100 global\nfeatures related to motor and cognitive aspects of the children interaction\nwith mobile devices, some of them collected and adapted from the literature.\nFurthermore, we analyse the robustness and discriminative power of the proposed\nfeature set including experimental results for the task of children age group\ndetection based on their motor and cognitive behaviors. Two different scenarios\nare considered in this study: i) single-test scenario, and ii) multiple-test\nscenario. Results over 93% accuracy are achieved using the publicly available\nChildCIdb_v1 database (over 400 children from 18 months to 8 years old),\nproving the high correlation of children's age with the way they interact with\nmobile devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_J/0/1/0/all/0/1\">Juan Carlos Ruiz-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herreros_Rodriguez_J/0/1/0/all/0/1\">Jaime Herreros-Rodriguez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v9 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06718","description":"<p>Convolutional neural network (CNN) has achieved impressive success in\ncomputer vision during the past few decades. The image convolution operation\nhelps CNNs to get good performance on image-related tasks. However, the image\nconvolution has high computation complexity and hard to be implemented. This\npaper proposes the CEMNet, which can be trained in the frequency domain. The\nmost important motivation of this research is that we can use the\nstraightforward element-wise multiplication operation to replace the image\nconvolution in the frequency domain based on the Cross-Correlation Theorem,\nwhich obviously reduces the computation complexity. We further introduce a\nWeight Fixation mechanism to alleviate the problem of over-fitting, and analyze\nthe working behavior of Batch Normalization, Leaky ReLU, and Dropout in the\nfrequency domain to design their counterparts for CEMNet. Also, to deal with\ncomplex inputs brought by Discrete Fourier Transform, we design a two-branches\nnetwork structure for CEMNet. Experimental results imply that CEMNet achieves\ngood performance on MNIST and CIFAR-10 databases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hengyue Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two Decades of Colorization and Decolorization for Images and Videos. (arXiv:2204.13322v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13322","description":"<p>Colorization is a computer-aided process, which aims to give color to a gray\nimage or video. It can be used to enhance black-and-white images, including\nblack-and-white photos, old-fashioned films, and scientific imaging results. On\nthe contrary, decolorization is to convert a color image or video into a\ngrayscale one. A grayscale image or video refers to an image or video with only\nbrightness information without color information. It is the basis of some\ndownstream image processing applications such as pattern recognition, image\nsegmentation, and image enhancement. Different from image decolorization, video\ndecolorization should not only consider the image contrast preservation in each\nvideo frame, but also respect the temporal and spatial consistency between\nvideo frames. Researchers were devoted to develop decolorization methods by\nbalancing spatial-temporal consistency and algorithm efficiency. With the\nprevalance of the digital cameras and mobile phones, image and video\ncolorization and decolorization have been paid more and more attention by\nresearchers. This paper gives an overview of the progress of image and video\ncolorization and decolorization methods in the last two decades.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiguang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Color Transfer and Style Transfer for Images and Videos. (arXiv:2204.13339v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13339","description":"<p>Image or video appearance features (e.g., color, texture, tone, illumination,\nand so on) reflect one's visual perception and direct impression of an image or\nvideo. Given a source image (video) and a target image (video), the image\n(video) color transfer technique aims to process the color of the source image\nor video (note that the source image or video is also referred to the reference\nimage or video in some literature) to make it look like that of the target\nimage or video, i.e., transferring the appearance of the target image or video\nto that of the source image or video, which can thereby change one's perception\nof the source image or video. As an extension of color transfer, style transfer\nrefers to rendering the content of a target image or video in the style of an\nartist with either a style sample or a set of images through a style transfer\nmodel. As an emerging field, the study of style transfer has attracted the\nattention of a large number of researchers. After decades of development, it\nhas become a highly interdisciplinary research with a variety of artistic\nexpression styles can be achieved. This paper provides an overview of color\ntransfer and style transfer methods over the past years.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiguang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Retrieve Videos by Asking Questions. (arXiv:2205.05739v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05739","description":"<p>The majority of traditional text-to-video retrieval systems operate in static\nenvironments, i.e., there is no interaction between the user and the agent\nbeyond the initial textual query provided by the user. This can be sub-optimal\nif the initial query has ambiguities, which would lead to many falsely\nretrieved videos. To overcome this limitation, we propose a novel framework for\nVideo Retrieval using Dialog (ViReD), which enables the user to interact with\nan AI agent via multiple rounds of dialog, where the user refines retrieved\nresults by answering questions generated by an AI agent. Our novel multimodal\nquestion generator learns to ask questions that maximize the subsequent video\nretrieval performance using (i) the video candidates retrieved during the last\nround of interaction with the user and (ii) the text-based dialog history\ndocumenting all previous interactions, to generate questions that incorporate\nboth visual and linguistic cues relevant to video retrieval. Furthermore, to\ngenerate maximally informative questions, we propose an Information-Guided\nSupervision (IGS), which guides the question generator to ask questions that\nwould boost subsequent video retrieval accuracy. We validate the effectiveness\nof our interactive ViReD framework on the AVSD dataset, showing that our\ninteractive method performs significantly better than traditional\nnon-interactive video retrieval systems. We also demonstrate that our proposed\napproach generalizes to the real-world settings that involve interactions with\nreal humans, thus, demonstrating the robustness and generality of our framework\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1\">Avinash Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1\">Junier Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-mentoring: a new deep learning pipeline to train a self-supervised U-net for few-shot learning of bio-artificial capsule segmentation. (arXiv:2205.10840v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10840","description":"<p>Background: Accurate segmentation of microscopic structures such as\nbio-artificial capsules in microscopy imaging is a prerequisite to the\ncomputer-aided understanding of important biomechanical phenomenons.\nState-of-the-art segmentation performances are achieved by deep neural networks\nand related data-driven approaches. Training these networks from only a few\nannotated examples is challenging while producing manually annotated images\nthat provide supervision is tedious.\n</p>\n<p>Method: Recently, self-supervision, i.e. designing a neural pipeline\nproviding synthetic or indirect supervision, has proved to significantly\nincrease generalization performances of models trained on few shots. The\nobjective of this paper is to introduce one such neural pipeline in the context\nof micro-capsule image segmentation. Our method leverages the rather simple\ncontent of these images so that a trainee network can be mentored by a referee\nnetwork which has been previously trained on synthetically generated pairs of\ncorrupted/correct region masks.\n</p>\n<p>Results: Challenging experimental setups are investigated. They involve from\nonly 3 to 10 annotated images along with moderately large amounts of\nunannotated images. In a bio-artificial capsule dataset, our approach\nconsistently and drastically improves accuracy. We also show that the learnt\nreferee network is transferable to another Glioblastoma cell dataset and that\nit can be efficiently coupled with data augmentation strategies.\n</p>\n<p>Conclusions: Experimental results show that very significant accuracy\nincrements are obtained by the proposed pipeline, leading to the conclusion\nthat the self-supervision mechanism introduced in this paper has the potential\nto replace human annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deleruyelle_A/0/1/0/all/0/1\">Arnaud Deleruyelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Versari_C/0/1/0/all/0/1\">Cristian Versari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1\">John Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfReformer: Self-Refined Network with Transformer for Salient Object Detection. (arXiv:2205.11283v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11283","description":"<p>The global and local contexts significantly contribute to the integrity of\npredictions in Salient Object Detection (SOD). Unfortunately, existing methods\nstill struggle to generate complete predictions with fine details. There are\ntwo major problems in conventional approaches: first, for global context,\nhigh-level CNN-based encoder features cannot effectively catch long-range\ndependencies, resulting in incomplete predictions. Second, downsampling the\nground truth to fit the size of predictions will introduce inaccuracy as the\nground truth details are lost during interpolation or pooling. Thus, in this\nwork, we developed a Transformer-based network and framed a supervised task for\na branch to learn the global context information explicitly. Besides, we adopt\nPixel Shuffle from Super-Resolution (SR) to reshape the predictions back to the\nsize of ground truth instead of the reverse. Thus details in the ground truth\nare untouched. In addition, we developed a two-stage Context Refinement Module\n(CRM) to fuse global context and automatically locate and refine the local\ndetails in the predictions. The proposed network can guide and correct itself\nbased on the global and local context generated, thus is named, Self-Refined\nTransformer (SelfReformer). Extensive experiments and evaluation results on\nfive benchmark datasets demonstrate the outstanding performance of the network,\nand we achieved the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_Y/0/1/0/all/0/1\">Yi Ke Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Learning with Multi-Query Transformer for Dense Prediction. (arXiv:2205.14354v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14354","description":"<p>Previous multi-task dense prediction studies developed complex pipelines such\nas multi-modal distillations in multiple stages or searching for task\nrelational contexts for each task. The core insight beyond these methods is to\nmaximize the mutual effects between each task. Inspired by the recent\nquery-based Transformers, we propose a simpler pipeline named Multi-Query\nTransformer (MQTransformer) that is equipped with multiple queries from\ndifferent tasks to facilitate the reasoning among multiple tasks and simplify\nthe cross task pipeline. Instead of modeling the dense per-pixel context among\ndifferent tasks, we seek a task-specific proxy to perform cross-task reasoning\nvia multiple queries where each query encodes the task-related context. The\nMQTransformer is composed of three key components: shared encoder, cross task\nattention and shared decoder. We first model each task with a task-relevant and\nscale-aware query, and then both the image feature output by the feature\nextractor and the task-relevant query feature are fed into the shared encoder,\nthus encoding the query feature from the image feature. Secondly, we design a\ncross task attention module to reason the dependencies among multiple tasks and\nfeature scales from two perspectives including different tasks of the same\nscale and different scales of the same task. Then we use a shared decoder to\ngradually refine the image features with the reasoned query features from\ndifferent tasks. Extensive experiment results on two dense prediction datasets\n(NYUD-v2 and PASCAL-Context) show that the proposed method is an effective\napproach and achieves the state-of-the-art result.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haobo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-Training for Unsupervised Domain Adaptation of Semantic Segmentation Models. (arXiv:2205.15781v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15781","description":"<p>Semantic image segmentation is addressed by training deep models. Since\nsupervised training draws to a curse of human-based image labeling, using\nsynthetic images with automatically generated ground truth together with\nunlabeled real-world images is a promising alternative. This implies to address\nan unsupervised domain adaptation (UDA) problem. In this paper, we proposed a\nnew co-training process for synth-to-real UDA of semantic segmentation models.\nFirst, we design a self-training procedure which provides two initial models.\nThen, we keep training these models in a collaborative manner for obtaining the\nfinal model. The overall process treats the deep models as black boxes and\ndrives their collaboration at the level of pseudo-labeled target images, i.e.,\nneither modifying loss functions is required, nor explicit feature alignment.\nWe test our proposal on standard synthetic and real-world datasets. Our\nco-training shows improvements of 15-20 percentage points of mIoU over\nbaselines, so establishing new state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_J/0/1/0/all/0/1\">Jose L. G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villalonga_G/0/1/0/all/0/1\">Gabriel Villalonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Antonio M. L&#xf3;pez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Invariant Visual Representations for Compositional Zero-Shot Learning. (arXiv:2206.00415v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.00415","description":"<p>Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositions\nusing knowledge learned from seen attribute-object compositions in the training\nset. Previous works mainly project an image and a composition into a common\nembedding space to measure their compatibility score. However, both attributes\nand objects share the visual representations learned above, leading the model\nto exploit spurious correlations and bias towards seen pairs. Instead, we\nreconsider CZSL as an out-of-distribution generalization problem. If an object\nis treated as a domain, we can learn object-invariant features to recognize the\nattributes attached to any object reliably. Similarly, attribute-invariant\nfeatures can also be learned when recognizing the objects with attributes as\ndomains. Specifically, we propose an invariant feature learning framework to\nalign different domains at the representation and gradient levels to capture\nthe intrinsic characteristics associated with the tasks. Experiments on two\nCZSL benchmarks demonstrate that the proposed method significantly outperforms\nthe previous state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kongming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Ruoyi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turning a Curse Into a Blessing: Enabling Clean-Data-Free Defenses by Model Inversion. (arXiv:2206.07018v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07018","description":"<p>It is becoming increasingly common to utilize pre-trained models provided by\nthird parties due to their convenience. At the same time, however, these models\nmay be vulnerable to both poisoning and evasion attacks. We introduce an\nalgorithmic framework that can mitigate potential security vulnerabilities in a\npre-trained model when clean data from its training distribution is unavailable\nto the defender. The framework reverse-engineers samples from a given\npre-trained model. The resulting synthetic samples can then be used as a\nsubstitute for clean data to perform various defenses. We consider two\nimportant attack scenarios -- backdoor attacks and evasion attacks -- to\nshowcase the utility of synthesized samples. For both attacks, we show that\nwhen supplied with our synthetic data, the state-of-the-art defenses perform\ncomparably or sometimes even better than the case when it's supplied with the\nsame amount of clean data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Won Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN2X: Non-Lambertian Inverse Rendering of Image GANs. (arXiv:2206.09244v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09244","description":"<p>2D images are observations of the 3D physical world depicted with the\ngeometry, material, and illumination components. Recovering these underlying\nintrinsic components from 2D images, also known as inverse rendering, usually\nrequires a supervised setting with paired images collected from multiple\nviewpoints and lighting conditions, which is resource-demanding. In this work,\nwe present GAN2X, a new method for unsupervised inverse rendering that only\nuses unpaired images for training. Unlike previous Shape-from-GAN approaches\nthat mainly focus on 3D shapes, we take the first attempt to also recover\nnon-Lambertian material properties by exploiting the pseudo paired data\ngenerated by a GAN. To achieve precise inverse rendering, we devise a\nspecularity-aware neural surface representation that continuously models the\ngeometry and material properties. A shading-based refinement technique is\nadopted to further distill information in the target image and recover more\nfine details. Experiments demonstrate that GAN2X can accurately decompose 2D\nimages to 3D shape, albedo, and specular properties for different object\ncategories, and achieves the state-of-the-art performance for unsupervised\nsingle-view 3D face reconstruction. We also show its applications in downstream\ntasks including real image editing and lifting 2D GANs to decomposed 3D GANs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ayush Tewari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GNN-PMB: A Simple but Effective Online 3D Multi-Object Tracker without Bells and Whistles. (arXiv:2206.10255v3 [eess.SY] UPDATED)","link":"http://arxiv.org/abs/2206.10255","description":"<p>Multi-object tracking (MOT) is among crucial applications in modern advanced\ndriver assistance systems (ADAS) and autonomous driving (AD) systems. Global\nnearest neighbor (GNN) filter, as the earliest random vector Bayesian tracking\nframework, has been adopted in most of state-of-the-arts trackers and widely\naccepted in the automotive industry. With the development of random finite set\n(RFS) theory, the RFS Bayesian filters have been applied in MOT tasks recently.\nHowever, their usefulness in the real traffic for ADAS and AD application is\nstill open to doubt. In this paper, we firstly demonstrate the latest RFS\nBayesian tracking framework could be superior to typical random vector Bayesian\ntracking framework like GNN, via a systematic comparative study of both\ntraditional random vector Bayesian filters with rule-based heuristic track\nmaintenance and RFS Bayesian filters on nuScenes validation dataset. Then, we\npropose a RFS-based tracker, namely Poisson multi-Bernoulli filter using the\nglobal nearest neighbor (GNN-PMB), for LiDAR-based MOT tasks. This GNN-PMB\ntracker is simple to use but can achieve competitive results on nuScenes\ndataset. Specifically, the proposed GNN-PMB tracker outperforms most of the\nstate-of-the-art LiDAR-only trackers and LiDAR and camera fusion-based\ntrackers, ranks the 3rd among all LiDAR-only trackers on nuScenes tracking task\nleader board1 at the time of submission. Our code is available at\nhttps://github.com/chisyliu/gnn pmb tracker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_L/0/1/0/all/0/1\">Liping Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxuan Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_B/0/1/0/all/0/1\">Bing Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_Q/0/1/0/all/0/1\">Qing-Long Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SearchMorph:Multi-scale Correlation Iterative Network for Deformable Registration. (arXiv:2206.13076v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13076","description":"<p>Deformable image registration can obtain dynamic information about images,\nwhich is of great significance in medical image analysis. The unsupervised deep\nlearning registration method can quickly achieve high registration accuracy\nwithout labels. However, these methods generally suffer from uncorrelated\nfeatures, poor ability to register large deformations and details, and\nunnatural deformation fields. To address the issues above, we propose an\nunsupervised multi-scale correlation iterative registration network\n(SearchMorph). In the proposed network, we introduce a correlation layer to\nstrengthen the relevance between features and construct a correlation pyramid\nto provide multi-scale relevance information for the network. We also design a\ndeformation field iterator, which improves the ability of the model to register\ndetails and large deformations through the search module and GRU while ensuring\nthat the deformation field is realistic. We use single-temporal brain MR images\nand multi-temporal echocardiographic sequences to evaluate the model's ability\nto register large deformations and details. The experimental results\ndemonstrate that the method in this paper achieves the highest registration\naccuracy and the lowest folding point ratio using a short elapsed time to\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shuxin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zhemin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shunmin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_A/0/1/0/all/0/1\">Alex Noel Joseph Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yibiao Rong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAM/CAD Point Cloud Part Segmentation via Few-Shot Learning. (arXiv:2207.01218v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.01218","description":"<p>3D part segmentation is an essential step in advanced CAM/CAD workflow.\nPrecise 3D segmentation contributes to lower defective rate of work-pieces\nproduced by the manufacturing equipment (such as computer controlled CNCs),\nthereby improving work efficiency and attaining the attendant economic\nbenefits. A large class of existing works on 3D model segmentation are mostly\nbased on fully-supervised learning, which trains the AI models with large,\nannotated datasets. However, the disadvantage is that the resulting models from\nthe fully-supervised learning methodology are highly reliant on the\ncompleteness of the available dataset, and its generalization ability is\nrelatively poor to new unknown segmentation types (i.e. further additional\nnovel classes). In this work, we propose and develop a noteworthy few-shot\nlearning-based approach for effective part segmentation in CAM/CAD; and this is\ndesigned to significantly enhance its generalization ability and flexibly adapt\nto new segmentation tasks by using only relatively rather few samples. As a\nresult, it not only reduces the requirements for the usually unattainable and\nexhaustive completeness of supervision datasets, but also improves the\nflexibility for real-world applications. As further improvement and innovation,\nwe additionally adopt the transform net and the center loss block in the\nnetwork. These characteristics serve to improve the comprehension for 3D\nfeatures of the various possible instances of the whole work-piece and ensure\nthe close distribution of the same class in feature space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiahui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Haiyue Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1\">Haoren Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mamun_A/0/1/0/all/0/1\">Abdullah Al Mamun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prahlad_V/0/1/0/all/0/1\">Vadakkepat Prahlad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_T/0/1/0/all/0/1\">Tong Heng Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding contrastive unsupervised features to cluster in- and out-of-distribution noise in corrupted image datasets. (arXiv:2207.01573v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01573","description":"<p>Using search engines for web image retrieval is a tempting alternative to\nmanual curation when creating an image dataset, but their main drawback remains\nthe proportion of incorrect (noisy) samples retrieved. These noisy samples have\nbeen evidenced by previous works to be a mixture of in-distribution (ID)\nsamples, assigned to the incorrect category but presenting similar visual\nsemantics to other classes in the dataset, and out-of-distribution (OOD)\nimages, which share no semantic correlation with any category from the dataset.\nThe latter are, in practice, the dominant type of noisy images retrieved. To\ntackle this noise duality, we propose a two stage algorithm starting with a\ndetection step where we use unsupervised contrastive feature learning to\nrepresent images in a feature space. We find that the alignment and uniformity\nprinciples of contrastive learning allow OOD samples to be linearly separated\nfrom ID samples on the unit hypersphere. We then spectrally embed the\nunsupervised representations using a fixed neighborhood size and apply an\noutlier sensitive clustering at the class level to detect the clean and OOD\nclusters as well as ID noisy outliers. We finally train a noise robust neural\nnetwork that corrects ID noise to the correct category and utilizes OOD samples\nin a guided contrastive objective, clustering them to improve low-level\nfeatures. Our algorithm improves the state-of-the-art results on synthetic\nnoise image datasets as well as real-world web-crawled data. Our work is fully\nreproducible github.com/PaulAlbert31/SNCF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albert_P/0/1/0/all/0/1\">Paul Albert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arazo_E/0/1/0/all/0/1\">Eric Arazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1\">Noel E. O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Network Binarization via Contrastive Learning. (arXiv:2207.02970v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02970","description":"<p>Neural network binarization accelerates deep models by quantizing their\nweights and activations into 1-bit. However, there is still a huge performance\ngap between Binary Neural Networks (BNNs) and their full-precision (FP)\ncounterparts. As the quantization error caused by weights binarization has been\nreduced in earlier works, the activations binarization becomes the major\nobstacle for further improvement of the accuracy. BNN characterises a unique\nand interesting structure, where the binary and latent FP activations exist in\nthe same forward pass (i.e., $\\text{Binarize}(\\mathbf{a}_F) = \\mathbf{a}_B$).\nTo mitigate the information degradation caused by the binarization operation\nfrom FP to binary activations, we establish a novel contrastive learning\nframework while training BNNs through the lens of Mutual Information (MI)\nmaximization. MI is introduced as the metric to measure the information shared\nbetween binary and FP activations, which assists binarization with contrastive\nlearning. Specifically, the representation ability of the BNNs is greatly\nstrengthened via pulling the positive pairs with binary and FP activations from\nthe same input samples, as well as pushing negative pairs from different\nsamples (the number of negative pairs can be exponentially large). This\nbenefits the downstream tasks, not only classification but also segmentation\nand depth estimation, etc. The experimental results show that our method can be\nimplemented as a pile-up module on existing state-of-the-art binarization\nmethods and can remarkably improve the performance over them on CIFAR-10/100\nand ImageNet, in addition to the great generalization ability on NYUD-v2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yuzhang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Ziliang Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Power of Transfer Learning in Agricultural Applications: AgriNet. (arXiv:2207.03881v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.03881","description":"<p>Advances in deep learning and transfer learning have paved the way for\nvarious automation classification tasks in agriculture, including plant\ndiseases, pests, weeds, and plant species detection. However, agriculture\nautomation still faces various challenges, such as the limited size of datasets\nand the absence of plant-domain-specific pretrained models. Domain specific\npretrained models have shown state of art performance in various computer\nvision tasks including face recognition and medical imaging diagnosis. In this\npaper, we propose AgriNet dataset, a collection of 160k agricultural images\nfrom more than 19 geographical locations, several images captioning devices,\nand more than 423 classes of plant species and diseases. We also introduce\nAgriNet models, a set of pretrained models on five ImageNet architectures:\nVGG16, VGG19, Inception-v3, InceptionResNet-v2, and Xception. AgriNet-VGG19\nachieved the highest classification accuracy of 94 % and the highest F1-score\nof 92%. Additionally, all proposed models were found to accurately classify the\n423 classes of plant species, diseases, pests, and weeds with a minimum\naccuracy of 87% for the Inception-v3 model.Finally, experiments to evaluate of\nsuperiority of AgriNet models compared to ImageNet models were conducted on two\nexternal datasets: pest and plant diseases dataset from Bangladesh and a plant\ndiseases dataset from Kashmir.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahili_Z/0/1/0/all/0/1\">Zahraa Al Sahili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awad_M/0/1/0/all/0/1\">Mariette Awad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoMER: Modeling Coverage for Transformer-based Handwritten Mathematical Expression Recognition. (arXiv:2207.04410v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04410","description":"<p>The Transformer-based encoder-decoder architecture has recently made\nsignificant advances in recognizing handwritten mathematical expressions.\nHowever, the transformer model still suffers from the lack of coverage problem,\nmaking its expression recognition rate (ExpRate) inferior to its RNN\ncounterpart. Coverage information, which records the alignment information of\nthe past steps, has proven effective in the RNN models. In this paper, we\npropose CoMER, a model that adopts the coverage information in the transformer\ndecoder. Specifically, we propose a novel Attention Refinement Module (ARM) to\nrefine the attention weights with past alignment information without hurting\nits parallelism. Furthermore, we take coverage information to the extreme by\nproposing self-coverage and cross-coverage, which utilize the past alignment\ninformation from the current and previous layers. Experiments show that CoMER\nimproves the ExpRate by 0.61%/2.09%/1.59% compared to the current\nstate-of-the-art model, and reaches 59.33%/59.81%/62.97% on the CROHME\n2014/2016/2019 test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Liangcai Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence. (arXiv:2207.04630v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2207.04630","description":"<p>Ten years into the revival of deep networks and artificial intelligence, we\npropose a theoretical framework that sheds light on understanding deep networks\nwithin a bigger picture of Intelligence in general. We introduce two\nfundamental principles, Parsimony and Self-consistency, that address two\nfundamental questions regarding Intelligence: what to learn and how to learn,\nrespectively. We believe the two principles are the cornerstones for the\nemergence of Intelligence, artificial or natural. While these two principles\nhave rich classical roots, we argue that they can be stated anew in entirely\nmeasurable and computable ways. More specifically, the two principles lead to\nan effective and efficient computational framework, compressive closed-loop\ntranscription, that unifies and explains the evolution of modern deep networks\nand many artificial intelligence practices. While we mainly use modeling of\nvisual data as an example, we believe the two principles will unify\nunderstanding of broad families of autonomous intelligent systems and provide a\nframework for understanding the brain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_D/0/1/0/all/0/1\">Doris Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Heung-Yeung Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Contextual Relationships for Cervical Abnormal Cell Detection. (arXiv:2207.04693v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04693","description":"<p>Cervical abnormal cell detection is a challenging task as the morphological\ndiscrepancies between abnormal and normal cells are usually subtle. To\ndetermine whether a cervical cell is normal or abnormal, cytopathologists\nalways take surrounding cells as references to identify its abnormality. To\nmimic these behaviors, we propose to explore contextual relationships to boost\nthe performance of cervical abnormal cell detection. Specifically, both\ncontextual relationships between cells and cell-to-global images are exploited\nto enhance features of each region of interest (RoI) proposals. Accordingly,\ntwo modules, dubbed as RoI-relationship attention module (RRAM) and global RoI\nattention module (GRAM), are developed and their combination strategies are\nalso investigated. We establish a strong baseline by using Double-Head Faster\nR-CNN with feature pyramid network (FPN) and integrate our RRAM and GRAM into\nit to validate the effectiveness of the proposed modules. Experiments conducted\non a large cervical cell detection dataset reveal that the introduction of RRAM\nand GRAM both achieves better average precision (AP) than the baseline methods.\nMoreover, when cascading RRAM and GRAM, our method outperforms the\nstate-of-the-art (SOTA) methods. Furthermore, we also show the proposed feature\nenhancing scheme can facilitate both image-level and smear-level\nclassification. The code and trained models are publicly available at\nhttps://github.com/CVIU-CSU/CR4CACD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yixiong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shuo Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1\">Hulin Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Liyan Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianxin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer. (arXiv:2207.04808v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04808","description":"<p>In this paper, we aim to devise a universally versatile style transfer method\ncapable of performing artistic, photo-realistic, and video style transfer\njointly, without seeing videos during training. Previous single-frame methods\nassume a strong constraint on the whole image to maintain temporal consistency,\nwhich could be violated in many cases. Instead, we make a mild and reasonable\nassumption that global inconsistency is dominated by local inconsistencies and\ndevise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local\npatches. CCPL can preserve the coherence of the content source during style\ntransfer without degrading stylization. Moreover, it owns a neighbor-regulating\nmechanism, resulting in a vast reduction of local distortions and considerable\nvisual quality improvement. Aside from its superior performance on versatile\nstyle transfer, it can be easily extended to other tasks, such as\nimage-to-image translation. Besides, to better fuse content and style features,\nwe propose Simple Covariance Transformation (SCT) to effectively align\nsecond-order statistics of the content feature with the style feature.\nExperiments demonstrate the effectiveness of the resulting model for versatile\nstyle transfer, when armed with CCPL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zijie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Junping Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CANF-VC: Conditional Augmented Normalizing Flows for Video Compression. (arXiv:2207.05315v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05315","description":"<p>This paper presents an end-to-end learning-based video compression system,\ntermed CANF-VC, based on conditional augmented normalizing flows (CANF). Most\nlearned video compression systems adopt the same hybrid-based coding\narchitecture as the traditional codecs. Recent research on conditional coding\nhas shown the sub-optimality of the hybrid-based coding and opens up\nopportunities for deep generative models to take a key role in creating new\ncoding frameworks. CANF-VC represents a new attempt that leverages the\nconditional ANF to learn a video generative model for conditional inter-frame\ncoding. We choose ANF because it is a special type of generative model, which\nincludes variational autoencoder as a special case and is able to achieve\nbetter expressiveness. CANF-VC also extends the idea of conditional coding to\nmotion coding, forming a purely conditional coding framework. Extensive\nexperimental results on commonly used datasets confirm the superiority of\nCANF-VC to the state-of-the-art methods. The source code of CANF-VC is\navailable at https://github.com/NYCU-MAPL/CANF-VC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_Y/0/1/0/all/0/1\">Yung-Han Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chih-Peng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gnutti_A/0/1/0/all/0/1\">Alessandro Gnutti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Hsiao Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Graph Transformer for Video Question Answering. (arXiv:2207.05342v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05342","description":"<p>This paper proposes a Video Graph Transformer (VGT) model for Video Quetion\nAnswering (VideoQA). VGT's uniqueness are two-fold: 1) it designs a dynamic\ngraph transformer module which encodes video by explicitly capturing the visual\nobjects, their relations, and dynamics for complex spatio-temporal reasoning;\nand 2) it exploits disentangled video and text Transformers for relevance\ncomparison between the video and text to perform QA, instead of entangled\ncross-modal Transformer for answer classification. Vision-text communication is\ndone by additional cross-modal interaction modules. With more reasonable video\nencoding and QA solution, we show that VGT can achieve much better performances\non VideoQA tasks that challenge dynamic relation reasoning than prior arts in\nthe pretraining-free scenario. Its performances even surpass those models that\nare pretrained with millions of external data. We further show that VGT can\nalso benefit a lot from self-supervised cross-modal pretraining, yet with\norders of magnitude smaller data. These results clearly demonstrate the\neffectiveness and superiority of VGT, and reveal its potential for more\ndata-efficient pretraining. With comprehensive analyses and some heuristic\nobservations, we hope that VGT can promote VQA research beyond coarse\nrecognition/description towards fine-grained relation reasoning in realistic\nvideos. Our code is available at https://github.com/sail-sg/VGT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Junbin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compound Prototype Matching for Few-shot Action Recognition. (arXiv:2207.05515v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05515","description":"<p>Few-shot action recognition aims to recognize novel action classes using only\na small number of labeled training samples. In this work, we propose a novel\napproach that first summarizes each video into compound prototypes consisting\nof a group of global prototypes and a group of focused prototypes, and then\ncompares video similarity based on the prototypes. Each global prototype is\nencouraged to summarize a specific aspect from the entire video, for example,\nthe start/evolution of the action. Since no clear annotation is provided for\nthe global prototypes, we use a group of focused prototypes to focus on certain\ntimestamps in the video. We compare video similarity by matching the compound\nprototypes between the support and query videos. The global prototypes are\ndirectly matched to compare videos from the same perspective, for example, to\ncompare whether two actions start similarly. For the focused prototypes, since\nactions have various temporal variations in the videos, we apply bipartite\nmatching to allow the comparison of actions with different temporal positions\nand shifts. Experiments demonstrate that our proposed method achieves\nstate-of-the-art results on multiple benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lijin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSP-Former: Multi-Scale Projection Transformer for Single Image Desnowing. (arXiv:2207.05621v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05621","description":"<p>Image restoration of snow scenes in severe weather is a difficult task. Snow\nimages have complex degradations and are cluttered over clean images, changing\nthe distribution of clean images. The previous methods based on CNNs are\nchallenging to remove perfectly in restoring snow scenes due to their local\ninductive biases' lack of a specific global modeling ability. In this paper, we\napply the vision transformer to the task of snow removal from a single image.\nSpecifically, we propose a parallel network architecture split along the\nchannel, performing local feature refinement and global information modeling\nseparately. We utilize a channel shuffle operation to combine their respective\nstrengths to enhance network performance. Second, we propose the MSP module,\nwhich utilizes multi-scale avgpool to aggregate information of different sizes\nand simultaneously performs multi-scale projection self-attention on multi-head\nself-attention to improve the representation ability of the model under\ndifferent scale degradations. Finally, we design a lightweight and simple local\ncapture module, which can refine the local capture capability of the model.\n</p>\n<p>In the experimental part, we conduct extensive experiments to demonstrate the\nsuperiority of our method. We compared the previous snow removal methods on\nthree snow scene datasets. The experimental results show that our method\nsurpasses the state-of-the-art methods with fewer parameters and computation.\nWe achieve substantial growth by 1.99dB and SSIM 0.03 on the CSD test dataset.\nOn the SRRS and Snow100K datasets, we also increased PSNR by 2.47dB and 1.62dB\ncompared with the Transweather approach and improved by 0.03 in SSIM. In the\nvisual comparison section, our MSP-Former also achieves better visual effects\nthan existing methods, proving the usability of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1\">Taodong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Erkang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A new database of Houma Alliance Book ancient handwritten characters and its baseline algorithm. (arXiv:2207.05993v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05993","description":"<p>The Houma Alliance Book is one of the national treasures of the Museum in\nShanxi Museum Town in China. It has great historical significance in\nresearching ancient history. To date, the research on the Houma Alliance Book\nhas been staying in the identification of paper documents, which is inefficient\nto identify and difficult to display, study and publicize. Therefore, the\ndigitization of the recognized ancient characters of Houma League can\neffectively improve the efficiency of recognizing ancient characters and\nprovide more reliable technical support and text data. This paper proposes a\nnew database of Houma Alliance Book ancient handwritten characters and a\nmulti-modal fusion method to recognize ancient handwritten characters. In the\ndatabase, 297 classes and 3,547 samples of Houma Alliance ancient handwritten\ncharacters are collected from the original book collection and by human\nimitative writing. Furthermore, the decision-level classifier fusion strategy\nis applied to fuse three well-known deep neural network architectures for\nancient handwritten character recognition. Experiments are performed on our new\ndatabase. The experimental results first provide the baseline result of the new\ndatabase to the research community and then demonstrate the efficiency of our\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yabo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zekai Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_X/0/1/0/all/0/1\">Xiuyan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaohua Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliminating Gradient Conflict in Reference-based Line-Art Colorization. (arXiv:2207.06095v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06095","description":"<p>Reference-based line-art colorization is a challenging task in computer\nvision. The color, texture, and shading are rendered based on an abstract\nsketch, which heavily relies on the precise long-range dependency modeling\nbetween the sketch and reference. Popular techniques to bridge the cross-modal\ninformation and model the long-range dependency employ the attention mechanism.\nHowever, in the context of reference-based line-art colorization, several\ntechniques would intensify the existing training difficulty of attention, for\ninstance, self-supervised training protocol and GAN-based losses. To understand\nthe instability in training, we detect the gradient flow of attention and\nobserve gradient conflict among attention branches. This phenomenon motivates\nus to alleviate the gradient issue by preserving the dominant gradient branch\nwhile removing the conflict ones. We propose a novel attention mechanism using\nthis training strategy, Stop-Gradient Attention (SGA), outperforming the\nattention baseline by a large margin with better training stability. Compared\nwith state-of-the-art modules in line-art colorization, our approach\ndemonstrates significant improvements in Fr\\'echet Inception Distance (FID, up\nto 27.21%) and structural similarity index measure (SSIM, up to 25.67%) on\nseveral benchmarks. The code of SGA is available at\nhttps://github.com/kunkun0w0/SGA .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhengyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DynaST: Dynamic Sparse Transformer for Exemplar-Guided Image Generation. (arXiv:2207.06124v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06124","description":"<p>One key challenge of exemplar-guided image generation lies in establishing\nfine-grained correspondences between input and guided images. Prior approaches,\ndespite the promising results, have relied on either estimating dense attention\nto compute per-point matching, which is limited to only coarse scales due to\nthe quadratic memory cost, or fixing the number of correspondences to achieve\nlinear complexity, which lacks flexibility. In this paper, we propose a dynamic\nsparse attention based Transformer model, termed Dynamic Sparse Transformer\n(DynaST), to achieve fine-level matching with favorable efficiency. The heart\nof our approach is a novel dynamic-attention unit, dedicated to covering the\nvariation on the optimal number of tokens one position should focus on.\nSpecifically, DynaST leverages the multi-layer nature of Transformer structure,\nand performs the dynamic attention scheme in a cascaded manner to refine\nmatching results and synthesize visually-pleasing outputs. In addition, we\nintroduce a unified training objective for DynaST, making it a versatile\nreference-based image translation framework for both supervised and\nunsupervised scenarios. Extensive experiments on three applications,\npose-guided person image generation, edge-based face synthesis, and undistorted\nimage style transfer, demonstrate that DynaST achieves superior performance in\nlocal details, outperforming the state of the art while reducing the\ncomputational cost significantly. Our code is available at\nhttps://github.com/Huage001/DynaST\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jingwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Organic Priors in Non-Rigid Structure from Motion. (arXiv:2207.06262v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06262","description":"<p>This paper advocates the use of organic priors in classical non-rigid\nstructure from motion (NRSfM). By organic priors, we mean invaluable\nintermediate prior information intrinsic to the NRSfM matrix factorization\ntheory. It is shown that such priors reside in the factorized matrices, and\nquite surprisingly, existing methods generally disregard them. The paper's main\ncontribution is to put forward a simple, methodical, and practical method that\ncan effectively exploit such organic priors to solve NRSfM. The proposed method\ndoes not make assumptions other than the popular one on the low-rank shape and\noffers a reliable solution to NRSfM under orthographic projection. Our work\nreveals that the accessibility of organic priors is independent of the camera\nmotion and shape deformation type. Besides that, the paper provides insights\ninto the NRSfM factorization -- both in terms of shape and motion -- and is the\nfirst approach to show the benefit of single rotation averaging for NRSfM.\nFurthermore, we outline how to effectively recover motion and non-rigid 3D\nshape using the proposed organic prior based approach and demonstrate results\nthat outperform prior-free NRSfM performance by a significant margin. Finally,\nwe present the benefits of our method via extensive experiments and evaluations\non several benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PyMAF-X: Towards Well-aligned Full-body Model Regression from Monocular Images. (arXiv:2207.06400v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06400","description":"<p>We present PyMAF-X, a regression-based approach to recovering a full-body\nparametric model from a single image. This task is very challenging since minor\nparametric deviation may lead to noticeable misalignment between the estimated\nmesh and the input image. Moreover, when integrating part-specific estimations\nto the full-body model, existing solutions tend to either degrade the alignment\nor produce unnatural wrist poses. To address these issues, we propose a\nPyramidal Mesh Alignment Feedback (PyMAF) loop in our regression network for\nwell-aligned human mesh recovery and extend it as PyMAF-X for the recovery of\nexpressive full-body models. The core idea of PyMAF is to leverage a feature\npyramid and rectify the predicted parameters explicitly based on the mesh-image\nalignment status. Specifically, given the currently predicted parameters,\nmesh-aligned evidence will be extracted from finer-resolution features\naccordingly and fed back for parameter rectification. To enhance the alignment\nperception, an auxiliary dense supervision is employed to provide mesh-image\ncorrespondence guidance while spatial alignment attention is introduced to\nenable the awareness of the global contexts for our network. When extending\nPyMAF for full-body mesh recovery, an adaptive integration strategy is proposed\nin PyMAF-X to produce natural wrist poses while maintaining the well-aligned\nperformance of the part-specific estimations. The efficacy of our approach is\nvalidated on several benchmark datasets for body-only and full-body mesh\nrecovery, where PyMAF and PyMAF-X effectively improve the mesh-image alignment\nand achieve new state-of-the-art results. The project page with code and video\nresults can be found at https://www.liuyebin.com/pymaf-x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yating Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengcheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Liang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-Efficient Deep Learning Framework for Segmentation and Classification of Histopathology Images. (arXiv:2207.06489v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.06489","description":"<p>The current study of cell architecture of inflammation in histopathology\nimages commonly performed for diagnosis and research purposes excludes a lot of\ninformation available on the biopsy slide. In autoimmune diseases, major\noutstanding research questions remain regarding which cell types participate in\ninflammation at the tissue level,and how they interact with each other. While\nthese questions can be partially answered using traditional methods, artificial\nintelligence approaches for segmentation and classification provide a much more\nefficient method to understand the architecture of inflammation in autoimmune\ndisease, holding a great promise for novel insights. In this paper, we\nempirically develop deep learning approaches that uses dermatomyositis biopsies\nof human tissue to detect and identify inflammatory cells. Our approach\nimproves classification performance by 26% and segmentation performance by 5%.\nWe also propose a novel post-processing autoencoder architecture that improves\nsegmentation performance by an additional 3%. We have open-sourced our approach\nand architecture at https://github.com/pranavsinghps1/DEDL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singh_P/0/1/0/all/0/1\">Pranav Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cirrone_J/0/1/0/all/0/1\">Jacopo Cirrone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lipschitz Continuity Retained Binary Neural Network. (arXiv:2207.06540v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.06540","description":"<p>Relying on the premise that the performance of a binary neural network can be\nlargely restored with eliminated quantization error between full-precision\nweight vectors and their corresponding binary vectors, existing works of\nnetwork binarization frequently adopt the idea of model robustness to reach the\naforementioned objective. However, robustness remains to be an ill-defined\nconcept without solid theoretical support. In this work, we introduce the\nLipschitz continuity, a well-defined functional property, as the rigorous\ncriteria to define the model robustness for BNN. We then propose to retain the\nLipschitz continuity as a regularization term to improve the model robustness.\nParticularly, while the popular Lipschitz-involved regularization methods often\ncollapse in BNN due to its extreme sparsity, we design the Retention Matrices\nto approximate spectral norms of the targeted weight matrices, which can be\ndeployed as the approximation for the Lipschitz constant of BNNs without the\nexact Lipschitz constant computation (NP-hard). Our experiments prove that our\nBNN-specific regularization method can effectively strengthen the robustness of\nBNN (testified on ImageNet-C), achieving state-of-the-art performance on CIFAR\nand ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yuzhang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_B/0/1/0/all/0/1\">Bin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Ziliang Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEXTER: An end-to-end system to extract table contents from electronic medical health documents. (arXiv:2207.06823v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06823","description":"<p>In this paper, we propose DEXTER, an end to end system to extract information\nfrom tables present in medical health documents, such as electronic health\nrecords (EHR) and explanation of benefits (EOB). DEXTER consists of four\nsub-system stages: i) table detection ii) table type classification iii) cell\ndetection; and iv) cell content extraction. We propose a two-stage transfer\nlearning-based approach using CDeC-Net architecture along with Non-Maximal\nsuppression for table detection. We design a conventional computer vision-based\napproach for table type classification and cell detection using parameterized\nkernels based on image size for detecting rows and columns. Finally, we extract\nthe text from the detected cells using pre-existing OCR engine Tessaract. To\nevaluate our system, we manually annotated a sample of the real-world medical\ndataset (referred to as Meddata) consisting of wide variations of documents (in\nterms of appearance) covering different table structures, such as bordered,\npartially bordered, borderless, or coloured tables. We experimentally show that\nDEXTER outperforms the commercially available Amazon Textract and Microsoft\nAzure Form Recognizer systems on the annotated real-world medical dataset\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+PR_N/0/1/0/all/0/1\">Nandhinee PR</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamoorthy_H/0/1/0/all/0/1\">Harinath Krishnamoorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivatsan_K/0/1/0/all/0/1\">Koushik Srivatsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Anil Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santhiappan_S/0/1/0/all/0/1\">Sudarsun Santhiappan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Bypasses Are Better Vision Transformer Adapters. (arXiv:2207.07039v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07039","description":"<p>The pretrain-then-finetune paradigm has been widely adopted in computer\nvision. But as the size of Vision Transformer (ViT) grows exponentially, the\nfull finetuning becomes prohibitive in view of the heavier storage overhead.\nMotivated by parameter-efficient transfer learning (PETL) on language\ntransformers, recent studies attempt to insert lightweight adaptation modules\n(e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune\nthese modules while the pretrained weights are frozen. However, these modules\nwere originally proposed to finetune language models. Although ported well to\nViT, their design lacks prior knowledge for visual tasks. In this paper, we\npropose to construct Convolutional Bypasses (Convpass) in ViT as adaptation\nmodules, introducing only a small amount (less than 0.5% of model parameters)\nof trainable parameters to adapt the large ViT. Different from other PETL\nmethods, Convpass benefits from the hard-coded inductive bias of convolutional\nlayers and thus is more suitable for visual tasks, especially in the low-data\nregime. Experimental results on VTAB-1k benchmark and few-shot learning\ndatasets demonstrate that Convpass outperforms current language-oriented\nadaptation modules, demonstrating the necessity to tailor vision-oriented\nadaptation modules for vision models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jie_S/0/1/0/all/0/1\">Shibo Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi-Hong Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Grand Unification of Object Tracking. (arXiv:2207.07078v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07078","description":"<p>We present a unified method, termed Unicorn, that can simultaneously solve\nfour tracking problems (SOT, MOT, VOS, MOTS) with a single network using the\nsame model parameters. Due to the fragmented definitions of the object tracking\nproblem itself, most existing trackers are developed to address a single or\npart of tasks and overspecialize on the characteristics of specific tasks. By\ncontrast, Unicorn provides a unified solution, adopting the same input,\nbackbone, embedding, and head across all tracking tasks. For the first time, we\naccomplish the great unification of the tracking network architecture and\nlearning paradigm. Unicorn performs on-par or better than its task-specific\ncounterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17,\nBDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will\nserve as a solid step towards the general vision model. Code is available at\nhttps://github.com/MasterBin-IIAU/Unicorn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model. (arXiv:2207.07115v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07115","description":"<p>We present XMem, a video object segmentation architecture for long videos\nwith unified feature memory stores inspired by the Atkinson-Shiffrin memory\nmodel. Prior work on video object segmentation typically only uses one type of\nfeature memory. For videos longer than a minute, a single feature memory model\ntightly links memory consumption and accuracy. In contrast, following the\nAtkinson-Shiffrin model, we develop an architecture that incorporates multiple\nindependent yet deeply-connected feature memory stores: a rapidly updated\nsensory memory, a high-resolution working memory, and a compact thus sustained\nlong-term memory. Crucially, we develop a memory potentiation algorithm that\nroutinely consolidates actively used working memory elements into the long-term\nmemory, which avoids memory explosion and minimizes performance decay for\nlong-term prediction. Combined with a new memory reading mechanism, XMem\ngreatly exceeds state-of-the-art performance on long-video datasets while being\non par with state-of-the-art methods (that do not work on long videos) on\nshort-video datasets. Code is available at https://hkchengrex.github.io/XMem\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Ho Kei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander G. Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupling Recognition from Detection: Single Shot Self-Reliant Scene Text Spotter. (arXiv:2207.07253v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07253","description":"<p>Typical text spotters follow the two-stage spotting strategy: detect the\nprecise boundary for a text instance first and then perform text recognition\nwithin the located text region. While such strategy has achieved substantial\nprogress, there are two underlying limitations. 1) The performance of text\nrecognition depends heavily on the precision of text detection, resulting in\nthe potential error propagation from detection to recognition. 2) The RoI\ncropping which bridges the detection and recognition brings noise from\nbackground and leads to information loss when pooling or interpolating from\nfeature maps. In this work we propose the single shot Self-Reliant Scene Text\nSpotter (SRSTS), which circumvents these limitations by decoupling recognition\nfrom detection. Specifically, we conduct text detection and recognition in\nparallel and bridge them by the shared positive anchor point. Consequently, our\nmethod is able to recognize the text instances correctly even though the\nprecise text boundaries are challenging to detect. Additionally, our method\nreduces the annotation cost for text detection substantially. Extensive\nexperiments on regular-shaped benchmark and arbitrary-shaped benchmark\ndemonstrate that our SRSTS compares favorably to previous state-of-the-art\nspotters in terms of both accuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingjing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_P/0/1/0/all/0/1\">Pengyuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-Preserving Face Recognition with Learnable Privacy Budgets in Frequency Domain. (arXiv:2207.07316v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07316","description":"<p>Face recognition technology has been used in many fields due to its high\nrecognition accuracy, including the face unlocking of mobile devices, community\naccess control systems, and city surveillance. As the current high accuracy is\nguaranteed by very deep network structures, facial images often need to be\ntransmitted to third-party servers with high computational power for inference.\nHowever, facial images visually reveal the user's identity information. In this\nprocess, both untrusted service providers and malicious users can significantly\nincrease the risk of a personal privacy breach. Current privacy-preserving\napproaches to face recognition are often accompanied by many side effects, such\nas a significant increase in inference time or a noticeable decrease in\nrecognition accuracy. This paper proposes a privacy-preserving face recognition\nmethod using differential privacy in the frequency domain. Due to the\nutilization of differential privacy, it offers a guarantee of privacy in\ntheory. Meanwhile, the loss of accuracy is very slight. This method first\nconverts the original image to the frequency domain and removes the direct\ncomponent termed DC. Then a privacy budget allocation method can be learned\nbased on the loss of the back-end face recognition network within the\ndifferential privacy framework. Finally, it adds the corresponding noise to the\nfrequency domain features. Our method performs very well with several classical\nface recognition test sets according to the extensive experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jiazhen Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuge Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xingkun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">ShengChuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Instances as 1D Kernels. (arXiv:2207.07372v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07372","description":"<p>We introduce a 3D instance representation, termed instance kernels, where\ninstances are represented by one-dimensional vectors that encode the semantic,\npositional, and shape information of 3D instances. We show that instance\nkernels enable easy mask inference by simply scanning kernels over the entire\nscenes, avoiding the heavy reliance on proposals or heuristic clustering\nalgorithms in standard 3D instance segmentation pipelines. The idea of instance\nkernel is inspired by recent success of dynamic convolutions in 2D/3D instance\nsegmentation. However, we find it non-trivial to represent 3D instances due to\nthe disordered and unstructured nature of point cloud data, e.g., poor instance\nlocalization can significantly degrade instance representation. To remedy this,\nwe construct a novel 3D instance encoding paradigm. First, potential instance\ncentroids are localized as candidates. Then, a candidate merging scheme is\ndevised to simultaneously aggregate duplicated candidates and collect context\naround the merged centroids to form the instance kernels. Once instance kernels\nare available, instance masks can be reconstructed via dynamic convolutions\nwhose weights are conditioned on instance kernels. The whole pipeline is\ninstantiated with a dynamic kernel network (DKNet). Results show that DKNet\noutperforms the state of the arts on both ScanNetV2 and S3DIS datasets with\nbetter instance localization. Code is available:\nhttps://github.com/W1zheng/DKNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yizheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Min Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Shuaiyuan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiguo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Weicai Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning. (arXiv:2207.07601v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07601","description":"<p>Many existing autonomous driving paradigms involve a multi-stage discrete\npipeline of tasks. To better predict the control signals and enhance user\nsafety, an end-to-end approach that benefits from joint spatial-temporal\nfeature learning is desirable. While there are some pioneering works on\nLiDAR-based input or implicit design, in this paper we formulate the problem in\nan interpretable vision-based setting. In particular, we propose a\nspatial-temporal feature learning scheme towards a set of more representative\nfeatures for perception, prediction and planning tasks simultaneously, which is\ncalled ST-P3. Specifically, an egocentric-aligned accumulation technique is\nproposed to preserve geometry information in 3D space before the bird's eye\nview transformation for perception; a dual pathway modeling is devised to take\npast motion variations into account for future prediction; a temporal-based\nrefinement unit is introduced to compensate for recognizing vision-based\nelements for planning. To the best of our knowledge, we are the first to\nsystematically investigate each part of an interpretable end-to-end\nvision-based autonomous driving system. We benchmark our approach against\nprevious state-of-the-arts on both open-loop nuScenes dataset as well as\nclosed-loop CARLA simulation. The results show the effectiveness of our method.\nSource code, model and protocol details are made publicly available at\nhttps://github.com/OpenPerceptionX/ST-P3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengchao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Penghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}