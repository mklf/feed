{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-09T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Selecting Seed Words for Wordle using Character Statistics. (arXiv:2202.03457v1 [cs.CL])","link":"http://arxiv.org/abs/2202.03457","description":"<p>Wordle, a word guessing game rose to global popularity in the January of\n2022. The goal of the game is to guess a five-letter English word within six\ntries. Each try provides the player with hints by means of colour changing\ntiles which inform whether or not a given character is part of the solution as\nwell as, in cases where it is part of the solution, whether or not it is in the\ncorrect placement. Numerous attempts have been made to find the best starting\nword and best strategy to solve the daily wordle. This study uses character\nstatistics of five-letter words to determine the best three starting words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Spam Detection using Transfer Learning of BERT Model. (arXiv:2202.03480v1 [cs.CL])","link":"http://arxiv.org/abs/2202.03480","description":"<p>Deep learning transformer models become important by training on text data\nbased on self-attention mechanisms. This manuscript demonstrated a novel\nuniversal spam detection model using pre-trained Google's Bidirectional Encoder\nRepresentations from Transformers (BERT) base uncased models with four datasets\nby efficiently classifying ham or spam emails in real-time scenarios. Different\nmethods for Enron, Spamassain, Lingspam, and Spamtext message classification\ndatasets, were used to train models individually in which a single model was\nobtained with acceptable performance on four datasets. The Universal Spam\nDetection Model (USDM) was trained with four datasets and leveraged\nhyperparameters from each model. The combined model was finetuned with the same\nhyperparameters from these four models separately. When each model using its\ncorresponding dataset, an F1-score is at and above 0.9 in individual models. An\noverall accuracy reached 97%, with an F1 score of 0.96. Research results and\nimplications were discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tida_V/0/1/0/all/0/1\">Vijay Srinivas Tida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_S/0/1/0/all/0/1\">Sonya Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximation Algorithms for ROUND-UFP and ROUND-SAP. (arXiv:2202.03492v1 [cs.DS])","link":"http://arxiv.org/abs/2202.03492","description":"<p>We study ROUND-UFP and ROUND-SAP, two generalizations of the classical BIN\nPACKING problem that correspond to the unsplittable flow problem on a path\n(UFP) and the storage allocation problem (SAP), respectively. We are given a\npath with capacities on its edges and a set of tasks where for each task we are\ngiven a demand and a subpath. In ROUND-UFP, the goal is to find a packing of\nall tasks into a minimum number of copies (rounds) of the given path such that\nfor each copy, the total demand of tasks on any edge does not exceed the\ncapacity of the respective edge. In ROUND-SAP, the tasks are considered to be\nrectangles and the goal is to find a non-overlapping packing of these\nrectangles into a minimum number of rounds such that all rectangles lie\ncompletely below the capacity profile of the edges.\n</p>\n<p>We show that in contrast to BIN PACKING, both the problems do not admit an\nasymptotic polynomial-time approximation scheme (APTAS), even when all edge\ncapacities are equal. However, for this setting, we obtain asymptotic\n$(2+\\varepsilon)$-approximations for both problems. For the general case, we\nobtain an $O(\\log\\log n)$-approximation algorithm and an\n$O(\\log\\log\\frac{1}{\\delta})$-approximation under $(1+\\delta)$-resource\naugmentation for both problems. For the intermediate setting of the no\nbottleneck assumption (i.e., the maximum task demand is at most the minimum\nedge capacity), we obtain absolute $12$- and asymptotic\n$(16+\\varepsilon)$-approximation algorithms for ROUND-UFP and ROUND-SAP,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kar_D/0/1/0/all/0/1\">Debajyoti Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Arindam Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiese_A/0/1/0/all/0/1\">Andreas Wiese</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling. (arXiv:2202.03543v1 [eess.AS])","link":"http://arxiv.org/abs/2202.03543","description":"<p>In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge\nand SUPERB benchmark. Our submissions are based on the recently proposed\nFaST-VGS model, which is a Transformer-based model that learns to associate raw\nspeech waveforms with semantically related images, all without the use of any\ntranscriptions of the speech. Additionally, we introduce a novel extension of\nthis model, FaST-VGS+, which is learned in a multi-task fashion with a masked\nlanguage modeling objective in addition to the visual grounding objective. On\nZeroSpeech 2021, we show that our models perform competitively on the ABX task,\noutperform all other concurrent submissions on the Syntactic and Semantic\ntasks, and nearly match the best system on the Lexical task. On the SUPERB\nbenchmark, we show that our models also achieve strong performance, in some\ncases even outperforming the popular wav2vec2.0 model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Language Models Learn Position-Role Mappings?. (arXiv:2202.03611v1 [cs.CL])","link":"http://arxiv.org/abs/2202.03611","description":"<p>How is knowledge of position-role mappings in natural language learned? We\nexplore this question in a computational setting, testing whether a variety of\nwell-performing pertained language models (BERT, RoBERTa, and DistilBERT)\nexhibit knowledge of these mappings, and whether this knowledge persists across\nalternations in syntactic, structural, and lexical alternations. In Experiment\n1, we show that these neural models do indeed recognize distinctions between\ntheme and recipient roles in ditransitive constructions, and that these\ndistinct patterns are shared across construction type. We strengthen this\nfinding in Experiment 2 by showing that fine-tuning these language models on\nnovel theme- and recipient-like tokens in one paradigm allows the models to\nmake correct predictions about their placement in other paradigms, suggesting\nthat the knowledge of these mappings is shared rather than independently\nlearned. We do, however, observe some limitations of this generalization when\ntasks involve constructions with novel ditransitive verbs, hinting at a degree\nof lexical specificity which underlies model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petty_J/0/1/0/all/0/1\">Jackson Petty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_M/0/1/0/all/0/1\">Michael Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_R/0/1/0/all/0/1\">Robert Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HistBERT: A Pre-trained Language Model for Diachronic Lexical Semantic Analysis. (arXiv:2202.03612v1 [cs.CL])","link":"http://arxiv.org/abs/2202.03612","description":"<p>Contextualized word embeddings have demonstrated state-of-the-art performance\nin various natural language processing tasks including those that concern\nhistorical semantic change. However, language models such as BERT was trained\nprimarily on contemporary corpus data. To investigate whether training on\nhistorical corpus data improves diachronic semantic analysis, we present a\npre-trained BERT-based language model, HistBERT, trained on the balanced Corpus\nof Historical American English. We examine the effectiveness of our approach by\ncomparing the performance of the original BERT and that of HistBERT, and we\nreport promising results in word similarity and semantic shift analysis. Our\nwork suggests that the effectiveness of contextual embeddings in diachronic\nsemantic analysis is dependent on the temporal profile of the input text and\ncare should be taken in applying this methodology to study historical semantic\nchange.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Wenjun Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Property-Based Tests in Natural Language. (arXiv:2202.03616v1 [cs.SE])","link":"http://arxiv.org/abs/2202.03616","description":"<p>We consider a new approach to generate tests from natural language. Rather\nthan relying on machine learning or templated extraction from structured\ncomments, we propose to apply classic ideas from linguistics to translate\nnatural-language sentences into executable tests. This paper explores the\napplication of combinatory categorial grammars (CCGs) to generating\nproperty-based tests. Our prototype is able to generate tests from English\ndescriptions for each example in a textbook chapter on property-based testing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gordon_C/0/1/0/all/0/1\">Colin S. Gordon</a> (Drexel University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey of Hallucination in Natural Language Generation. (arXiv:2202.03629v1 [cs.CL])","link":"http://arxiv.org/abs/2202.03629","description":"<p>Natural Language Generation (NLG) has improved exponentially in recent years\nthanks to the development of deep learning technologies such as\nTransformer-based language models. This advancement has led to more fluent and\ncoherent natural language generation, naturally leading to development in\ndownstream tasks such as abstractive summarization, dialogue generation and\ndata-to-text generation. However, it is also investigated that such generation\nincludes hallucinated texts, which makes the performances of text generation\nfail to meet users' expectations in many real-world scenarios. In order to\naddress this issue, studies in evaluation and mitigation methods of\nhallucinations have been presented in various tasks, but have not been reviewed\nin a combined manner. In this survey, we provide a broad overview of the\nresearch progress and challenges in the hallucination problem of NLG. The\nsurvey is organized into two big divisions: (i) a general overview of metrics,\nmitigation methods, and future directions; (ii) task-specific research progress\nfor hallucinations in a large set of downstream tasks: abstractive\nsummarization, dialogue generation, generative question answering, data-to-text\ngeneration, and machine translation. This survey could facilitate collaborative\nefforts among researchers in these tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A two-step approach to leverage contextual data: speech recognition in air-traffic communications. (arXiv:2202.03725v1 [cs.CL])","link":"http://arxiv.org/abs/2202.03725","description":"<p>Automatic Speech Recognition (ASR), as the assistance of speech communication\nbetween pilots and air-traffic controllers, can significantly reduce the\ncomplexity of the task and increase the reliability of transmitted information.\nASR application can lead to a lower number of incidents caused by\nmisunderstanding and improve air traffic management (ATM) efficiency.\nEvidently, high accuracy predictions, especially, of key information, i.e.,\ncallsigns and commands, are required to minimize the risk of errors. We prove\nthat combining the benefits of ASR and Natural Language Processing (NLP)\nmethods to make use of surveillance data (i.e. additional modality) helps to\nconsiderably improve the recognition of callsigns (named entity). In this\npaper, we investigate a two-step callsign boosting approach: (1) at the 1 step\n(ASR), weights of probable callsign n-grams are reduced in G.fst and/or in the\ndecoding FST (lattices), (2) at the 2 step (NLP), callsigns extracted from the\nimproved recognition outputs with Named Entity Recognition (NER) are correlated\nwith the surveillance data to select the most suitable one. Boosting callsign\nn-grams with the combination of ASR and NLP methods eventually leads up to\n53.7% of an absolute, or 60.4% of a relative, improvement in callsign\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Seyyed Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic features of object concepts generated with GPT-3. (arXiv:2202.03753v1 [cs.CL])","link":"http://arxiv.org/abs/2202.03753","description":"<p>Semantic features have been playing a central role in investigating the\nnature of our conceptual representations. Yet the enormous time and effort\nrequired to empirically sample and norm features from human raters has\nrestricted their use to a limited set of manually curated concepts. Given\nrecent promising developments with transformer-based language models, here we\nasked whether it was possible to use such models to automatically generate\nmeaningful lists of properties for arbitrary object concepts and whether these\nmodels would produce features similar to those found in humans. To this end, we\nprobed a GPT-3 model to generate semantic features for 1,854 objects and\ncompared automatically-generated features to existing human feature norms.\nGPT-3 generated many more features than humans, yet showed a similar\ndistribution in the types of generated features. Generated feature norms\nrivaled human norms in predicting similarity, relatedness, and category\nmembership, while variance partitioning demonstrated that these predictions\nwere driven by similar variance in humans and GPT-3. Together, these results\nhighlight the potential of large language models to capture important facets of\nhuman knowledge and yield a new approach for automatically generating\ninterpretable feature sets, thus drastically expanding the potential use of\nsemantic features in psychological and linguistic studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_H/0/1/0/all/0/1\">Hannes Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebart_M/0/1/0/all/0/1\">Martin N. Hebart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Structure with Undirected Neural Networks. (arXiv:2202.03760v1 [cs.LG])","link":"http://arxiv.org/abs/2202.03760","description":"<p>Neural networks are powerful function estimators, leading to their status as\na paradigm of choice for modeling structured data. However, unlike other\nstructured representations that emphasize the modularity of the problem --\ne.g., factor graphs -- neural networks are usually monolithic mappings from\ninputs to outputs, with a fixed computation order. This limitation prevents\nthem from capturing different directions of computation and interaction between\nthe modeled variables.\n</p>\n<p>In this paper, we combine the representational strengths of factor graphs and\nof neural networks, proposing undirected neural networks (UNNs): a flexible\nframework for specifying computations that can be performed in any order. For\nparticular choices, our proposed models subsume and extend many existing\narchitectures: feed-forward, recurrent, self-attention networks, auto-encoders,\nand networks with implicit layers. We demonstrate the effectiveness of\nundirected neural architectures, both unstructured and structured, on a range\nof tasks: tree-constrained dependency parsing, convolutional image\nclassification, and sequence completion with attention. By varying the\ncomputation order, we show how a single UNN can be used both as a classifier\nand a prototype generator, and how it can fill in missing parts of an input\nsequence, making them a promising field for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihaylova_T/0/1/0/all/0/1\">Tsvetomila Mihaylova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1\">Vlad Niculae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Multi-Token Fairness in Text Classification. (arXiv:2202.03792v1 [cs.CL])","link":"http://arxiv.org/abs/2202.03792","description":"<p>The counterfactual token generation has been limited to perturbing only a\nsingle token in texts that are generally short and single sentences. These\ntokens are often associated with one of many sensitive attributes. With limited\ncounterfactuals generated, the goal to achieve invariant nature for machine\nlearning classification models towards any sensitive attribute gets bounded,\nand the formulation of Counterfactual Fairness gets narrowed. In this paper, we\novercome these limitations by solving root problems and opening bigger domains\nfor understanding. We have curated a resource of sensitive tokens and their\ncorresponding perturbation tokens, even extending the support beyond\ntraditionally used sensitive attributes like \\textit{Age}, \\textit{Gender}, and\n\\textit{Race} to \\textit{Nationality}, \\textit{Disability}, and\n\\textit{Religion}. The concept of Counterfactual Generation has been extended\nto multi-token support valid over all forms of texts and documents. We define\nthe method of generating counterfactuals by perturbing multiple sensitive\ntokens as \\textbf{Counterfactual Multi-token Generation}. The method has been\nconceptualized to showcase significant performance improvement over\nsingle-token methods and validated over multiple benchmark datasets. The\nemendation in counterfactual generation propagates in achieving improved\n\\textbf{Counterfactual Multi-token Fairness}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lohia_P/0/1/0/all/0/1\">Pranay Lohia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What are the best systems? New perspectives on NLP Benchmarking. (arXiv:2202.03799v1 [cs.CL])","link":"http://arxiv.org/abs/2202.03799","description":"<p>In Machine Learning, a benchmark refers to an ensemble of datasets associated\nwith one or multiple metrics together with a way to aggregate different systems\nperformances. They are instrumental in (i) assessing the progress of new\nmethods along different axes and (ii) selecting the best systems for practical\nuse. This is particularly the case for NLP with the development of large\npre-trained models (e.g. GPT, BERT) that are expected to generalize well on a\nvariety of tasks. While the community mainly focused on developing new datasets\nand metrics, there has been little interest in the aggregation procedure, which\nis often reduced to a simple average over various performance measures.\nHowever, this procedure can be problematic when the metrics are on a different\nscale, which may lead to spurious conclusions. This paper proposes a new\nprocedure to rank systems based on their performance across different tasks.\nMotivated by the social choice theory, the final system ordering is obtained\nthrough aggregating the rankings induced by each task and is theoretically\ngrounded. We conduct extensive numerical experiments (on over 270k scores) to\nassess the soundness of our approach both on synthetic and real scores (e.g.\nGLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method\nyields different conclusions on state-of-the-art systems than the\nmean-aggregation procedure while being both more reliable and robust.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noiry_N/0/1/0/all/0/1\">Nathan Noiry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irurozki_E/0/1/0/all/0/1\">Ekhine Irurozki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clemencon_S/0/1/0/all/0/1\">Stephan Clemencon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TimeLMs: Diachronic Language Models from Twitter. (arXiv:2202.03829v1 [cs.CL])","link":"http://arxiv.org/abs/2202.03829","description":"<p>Despite its importance, the time variable has been largely neglected in the\nNLP and language model literature. In this paper, we present TimeLMs, a set of\nlanguage models specialized on diachronic Twitter data. We show that a\ncontinual learning strategy contributes to enhancing Twitter-based language\nmodels' capacity to deal with future and out-of-distribution tweets, while\nmaking them competitive with standardized and more monolithic benchmarks. We\nalso perform a number of qualitative analyses showing how they cope with trends\nand peaks in activity involving specific named entities or concept drift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loureiro_D/0/1/0/all/0/1\">Daniel Loureiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbieri_F/0/1/0/all/0/1\">Francesco Barbieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_L/0/1/0/all/0/1\">Leonardo Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anke_L/0/1/0/all/0/1\">Luis Espinosa Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable N-gram Objective on Abstractive Summarization. (arXiv:2202.04003v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04003","description":"<p>ROUGE is a standard automatic evaluation metric based on n-grams for\nsequence-to-sequence tasks, while cross-entropy loss is an essential objective\nof neural network language model that optimizes at a unigram level. We present\ndifferentiable n-gram objectives, attempting to alleviate the discrepancy\nbetween training criterion and evaluating criterion. The objective maximizes\nthe probabilistic weight of matched sub-sequences, and the novelty of our work\nis the objective weights the matched sub-sequences equally and does not ceil\nthe number of matched sub-sequences by the ground truth count of n-grams in\nreference sequence. We jointly optimize cross-entropy loss and the proposed\nobjective, providing decent ROUGE score enhancement over abstractive\nsummarization dataset CNN/DM and XSum, outperforming alternative n-gram\nobjectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wensheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingjin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating question answering and text-to-SQL in Portuguese. (arXiv:2202.04048v1 [cs.CL])","link":"http://arxiv.org/abs/2202.04048","description":"<p>Deep learning transformers have drastically improved systems that\nautomatically answer questions in natural language. However, different\nquestions demand different answering techniques; here we propose, build and\nvalidate an architecture that integrates different modules to answer two\ndistinct kinds of queries. Our architecture takes a free-form natural language\ntext and classifies it to send it either to a Neural Question Answering\nReasoner or a Natural Language parser to SQL. We implemented a complete system\nfor the Portuguese language, using some of the main tools available for the\nlanguage and translating training and testing datasets. Experiments show that\nour system selects the appropriate answering method with high accuracy (over\n99\\%), thus validating a modular question answering strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jose_M/0/1/0/all/0/1\">Marcos Menon Jos&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_M/0/1/0/all/0/1\">Marcelo Archanjo Jos&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maua_D/0/1/0/all/0/1\">Denis Deratani Mau&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cozman_F/0/1/0/all/0/1\">F&#xe1;bio Gagliardi Cozman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers. (arXiv:2202.04053v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04053","description":"<p>Generating images from textual descriptions has gained a lot of attention.\nRecently, DALL-E, a multimodal transformer language model, and its variants\nhave shown high-quality text-to-image generation capabilities with a simple\narchitecture and training objective, powered by large-scale training data and\ncomputation. However, despite the interesting image generation results, there\nhas not been a detailed analysis on how to evaluate such models. In this work,\nwe investigate the reasoning capabilities and social biases of such\ntext-to-image generative transformers in detail. First, we measure four visual\nreasoning skills: object recognition, object counting, color recognition, and\nspatial relation understanding. For this, we propose PaintSkills, a diagnostic\ndataset and evaluation toolkit that measures these four visual reasoning\nskills. Second, we measure the text alignment and quality of the generated\nimages based on pretrained image captioning, image-text retrieval, and image\nclassification models. Third, we assess social biases in the models. For this,\nwe suggest evaluation of gender and racial biases of text-to-image generation\nmodels based on a pretrained image-text retrieval model and human evaluation.\nIn our experiments, we show that recent text-to-image models perform better in\nrecognizing and counting objects than recognizing colors and understanding\nspatial relations, while there exists a large gap between model performances\nand oracle accuracy on all skills. Next, we demonstrate that recent\ntext-to-image models learn specific gender/racial biases from web image-text\npairs. We also show that our automatic evaluations of visual reasoning skills\nand gender bias are highly correlated with human judgments. We hope our work\nwill help guide future progress in improving text-to-image models on visual\nreasoning skills and social biases. Code and data at:\nhttps://github.com/j-min/DallEval\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zala_A/0/1/0/all/0/1\">Abhay Zala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-Class: Text Classification with Extremely Weak Supervision. (arXiv:2010.12794v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12794","description":"<p>In this paper, we explore text classification with extremely weak\nsupervision, i.e., only relying on the surface text of class names. This is a\nmore challenging setting than the seed-driven weak supervision, which allows a\nfew seed words per class. We opt to attack this problem from a representation\nlearning perspective -- ideal document representations should lead to nearly\nthe same results between clustering and the desired classification. In\nparticular, one can classify the same corpus differently (e.g., based on topics\nand locations), so document representations should be adaptive to the given\nclass names. We propose a novel framework X-Class to realize the adaptive\nrepresentations. Specifically, we first estimate class representations by\nincrementally adding the most similar word to each class until inconsistency\narises. Following a tailored mixture of class attention mechanisms, we obtain\nthe document representation via a weighted average of contextualized word\nrepresentations. With the prior of each document assigned to its nearest class,\nwe then cluster and align the documents to classes. Finally, we pick the most\nconfident documents from each cluster to train a text classifier. Extensive\nexperiments demonstrate that X-Class can rival and even outperform seed-driven\nweakly supervised methods on 7 benchmark datasets. Our dataset and code are\nreleased at https://github.com/ZihanWangKi/XClass/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekala_D/0/1/0/all/0/1\">Dheeraj Mekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Average\" Approximates \"First Principal Component\"? An Empirical Analysis on Representations from Neural Language Models. (arXiv:2104.08673v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08673","description":"<p>Contextualized representations based on neural language models have furthered\nthe state of the art in various NLP tasks. Despite its great success, the\nnature of such representations remains a mystery. In this paper, we present an\nempirical property of these representations -- \"average\" approximates \"first\nprincipal component\". Specifically, experiments show that the average of these\nrepresentations shares almost the same direction as the first principal\ncomponent of the matrix whose columns are these representations. We believe\nthis explains why the average representation is always a simple yet strong\nbaseline. Our further examinations show that this property also holds in more\nchallenging scenarios, for example, when the representations are from a model\nright after its random initialization. Therefore, we conjecture that this\nproperty is intrinsic to the distribution of representations and not\nnecessarily related to the input structure. We realize that these\nrepresentations empirically follow a normal distribution for each dimension,\nand by assuming this is true, we demonstrate that the empirical property can be\nin fact derived mathematically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noised Consistency Training for Text Summarization. (arXiv:2105.13635v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.13635","description":"<p>Neural abstractive summarization methods often require large quantities of\nlabeled training data. However, labeling large amounts of summarization data is\noften prohibitive due to time, financial, and expertise constraints, which has\nlimited the usefulness of summarization systems to practical applications. In\nthis paper, we argue that this limitation can be overcome by a semi-supervised\napproach: consistency training which is to leverage large amounts of unlabeled\ndata to improve the performance of supervised learning over a small corpus. The\nconsistency regularization semi-supervised learning can regularize model\npredictions to be invariant to small noise applied to input articles. By adding\nnoised unlabeled corpus to help regularize consistency training, this framework\nobtains comparative performance without using the full dataset. In particular,\nwe have verified that leveraging large amounts of unlabeled data decently\nimproves the performance of supervised learning over an insufficient labeled\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junnan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1\">Qianren Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongdong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Sexism Detection with Multilingual Transformer Models. (arXiv:2106.04908v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.04908","description":"<p>Sexism has become an increasingly major problem on social networks during the\nlast years. The first shared task on sEXism Identification in Social neTworks\n(EXIST) at IberLEF 2021 is an international competition in the field of Natural\nLanguage Processing (NLP) with the aim to automatically identify sexism in\nsocial media content by applying machine learning methods. Thereby sexism\ndetection is formulated as a coarse (binary) classification problem and a\nfine-grained classification task that distinguishes multiple types of sexist\ncontent (e.g., dominance, stereotyping, and objectification). This paper\npresents the contribution of the AIT_FHSTP team at the EXIST2021 benchmark for\nboth tasks. To solve the tasks we applied two multilingual transformer models,\none based on multilingual BERT and one based on XLM-R. Our approach uses two\ndifferent strategies to adapt the transformers to the detection of sexist\ncontent: first, unsupervised pre-training with additional data and second,\nsupervised fine-tuning with additional and augmented data. For both tasks our\nbest model is XLM-R with unsupervised pre-training on the EXIST data and\nadditional datasets and fine-tuning on the provided dataset. The best run for\nthe binary classification (task 1) achieves a macro F1-score of 0.7752 and\nscores 5th rank in the benchmark; for the multiclass classification (task 2)\nour best submission scores 6th rank with a macro F1-score of 0.5589.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schutz_M/0/1/0/all/0/1\">Mina Sch&#xfc;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boeck_J/0/1/0/all/0/1\">Jaqueline Boeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakhovets_D/0/1/0/all/0/1\">Daria Liakhovets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slijepcevic_D/0/1/0/all/0/1\">Djordje Slijep&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchknopf_A/0/1/0/all/0/1\">Armin Kirchknopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hecht_M/0/1/0/all/0/1\">Manuel Hecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogensperger_J/0/1/0/all/0/1\">Johannes Bogensperger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlarb_S/0/1/0/all/0/1\">Sven Schlarb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_A/0/1/0/all/0/1\">Alexander Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeppelzauer_M/0/1/0/all/0/1\">Matthias Zeppelzauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Conditional End-to-End ASR with CTC and Multi-Granular Subword Units. (arXiv:2110.04109v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.04109","description":"<p>In end-to-end automatic speech recognition (ASR), a model is expected to\nimplicitly learn representations suitable for recognizing a word-level\nsequence. However, the huge abstraction gap between input acoustic signals and\noutput linguistic tokens makes it challenging for a model to learn the\nrepresentations. In this work, to promote the word-level representation\nlearning in end-to-end ASR, we propose a hierarchical conditional model that is\nbased on connectionist temporal classification (CTC). Our model is trained by\nauxiliary CTC losses applied to intermediate layers, where the vocabulary size\nof each target subword sequence is gradually increased as the layer becomes\nclose to the word-level output. Here, we make each level of sequence prediction\nexplicitly conditioned on the previous sequences predicted at lower levels.\nWith the proposed approach, we expect the proposed model to learn the\nword-level representations effectively by exploiting a hierarchy of linguistic\nstructures. Experimental results on LibriSpeech-{100h, 960h} and TEDLIUM2\ndemonstrate that the proposed model improves over a standard CTC-based model\nand other competitive models from prior work. We further analyze the results to\nconfirm the effectiveness of the intended representation learning with our\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karube_K/0/1/0/all/0/1\">Keita Karube</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ogawa_T/0/1/0/all/0/1\">Tetsuji Ogawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobayashi_T/0/1/0/all/0/1\">Tetsunori Kobayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Would Jiminy Cricket Do? Towards Agents That Behave Morally. (arXiv:2110.13136v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.13136","description":"<p>When making everyday decisions, people are guided by their conscience, an\ninternal sense of right and wrong. By contrast, artificial agents are currently\nnot endowed with a moral sense. As a consequence, they may learn to behave\nimmorally when trained on environments that ignore moral concerns, such as\nviolent video games. With the advent of generally capable agents that pretrain\non many environments, it will become necessary to mitigate inherited biases\nfrom environments that teach immoral behavior. To facilitate the development of\nagents that avoid causing wanton harm, we introduce Jiminy Cricket, an\nenvironment suite of 25 text-based adventure games with thousands of diverse,\nmorally salient scenarios. By annotating every possible game state, the Jiminy\nCricket environments robustly evaluate whether agents can act morally while\nmaximizing reward. Using models with commonsense moral knowledge, we create an\nelementary artificial conscience that assesses and guides agents. In extensive\nexperiments, we find that the artificial conscience approach can steer agents\ntowards moral behavior without sacrificing performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1\">Mantas Mazeika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Sahil Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Christine Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_J/0/1/0/all/0/1\">Jesus Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Funnelling: Ensemble Learning and Heterogeneous Document Embeddings for Cross-Lingual Text Classification. (arXiv:2110.14764v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.14764","description":"<p>\\emph{Funnelling} (Fun) is a recently proposed method for cross-lingual text\nclassification (CLTC) based on a two-tier learning ensemble for heterogeneous\ntransfer learning (HTL). In this ensemble method, 1st-tier classifiers, each\nworking on a different and language-dependent feature space, return a vector of\ncalibrated posterior probabilities (with one dimension for each class) for each\ndocument, and the final classification decision is taken by a metaclassifier\nthat uses this vector as its input. The metaclassifier can thus exploit\nclass-class correlations, and this (among other things) gives Fun an edge over\nCLTC systems in which these correlations cannot be brought to bear. In this\npaper we describe \\emph{Generalized Funnelling} (gFun), a generalization of Fun\nconsisting of an HTL architecture in which 1st-tier components can be arbitrary\n\\emph{view-generating functions}, i.e., language-dependent functions that each\nproduce a language-independent representation (\"view\") of the (monolingual)\ndocument. We describe an instance of gFun in which the metaclassifier receives\nas input a vector of calibrated posterior probabilities (as in Fun) aggregated\nto other embedded representations that embody other types of correlations, such\nas word-class correlations (as encoded by \\emph{Word-Class Embeddings}),\nword-word correlations (as encoded by \\emph{Multilingual Unsupervised or\nSupervised Embeddings}), and word-context correlations (as encoded by\n\\emph{multilingual BERT}). We show that this instance of \\textsc{gFun}\nsubstantially improves over Fun and over state-of-the-art baselines, by\nreporting experimental results obtained on two large, standard datasets for\nmultilingual multilabel text classification. Our code that implements gFun is\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrotti_A/0/1/0/all/0/1\">Andrea Pedrotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Enactivist account of Mind Reading in Natural Language Understanding. (arXiv:2111.06179v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.06179","description":"<p>In this paper we apply our understanding of the radical enactivist agenda to\nthe classic AI-hard problem of Natural Language Understanding. When Turing\ndevised his famous test the assumption was that a computer could use language\nand the challenge would be to mimic human intelligence. It turned out playing\nchess and formal logic were easy compared to understanding what people say. The\ntechniques of good old-fashioned AI (GOFAI) assume symbolic representation is\nthe core of reasoning and by that paradigm human communication consists of\ntransferring representations from one mind to another. However, one finds that\nrepresentations appear in another's mind, without appearing in the intermediary\nlanguage. People communicate by mind reading it seems. Systems with speech\ninterfaces such as Alexa and Siri are of course common, but they are limited.\nRather than adding mind reading skills, we introduced a \"cheat\" that enabled\nour systems to fake it. The cheat is simple and only slightly interesting to\ncomputer scientists and not at all interesting to philosophers. However,\nreading about the enactivist idea that we \"directly perceive\" the intentions of\nothers, our cheat took on a new light and in this paper look again at how\nnatural language understanding might actually work between humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1\">Peter Wallis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving language models by retrieving from trillions of tokens. (arXiv:2112.04426v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.04426","description":"<p>We enhance auto-regressive language models by conditioning on document chunks\nretrieved from a large corpus, based on local similarity with preceding tokens.\nWith a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO)\nobtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite\nusing 25$\\times$ fewer parameters. After fine-tuning, RETRO performance\ntranslates to downstream knowledge-intensive tasks such as question answering.\nRETRO combines a frozen Bert retriever, a differentiable encoder and a chunked\ncross-attention mechanism to predict tokens based on an order of magnitude more\ndata than what is typically consumed during training. We typically train RETRO\nfrom scratch, yet can also rapidly RETROfit pre-trained transformers with\nretrieval and still achieve good performance. Our work opens up new avenues for\nimproving language models through explicit memory at unprecedented scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensch_A/0/1/0/all/0/1\">Arthur Mensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jordan Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Trevor Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutherford_E/0/1/0/all/0/1\">Eliza Rutherford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millican_K/0/1/0/all/0/1\">Katie Millican</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driessche_G/0/1/0/all/0/1\">George van den Driessche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lespiau_J/0/1/0/all/0/1\">Jean-Baptiste Lespiau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damoc_B/0/1/0/all/0/1\">Bogdan Damoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_A/0/1/0/all/0/1\">Aidan Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1\">Diego de Las Casas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_A/0/1/0/all/0/1\">Aurelia Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ring_R/0/1/0/all/0/1\">Roman Ring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigan_T/0/1/0/all/0/1\">Tom Hennigan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Saffron Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggiore_L/0/1/0/all/0/1\">Loren Maggiore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1\">Chris Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassirer_A/0/1/0/all/0/1\">Albin Cassirer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andy Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paganini_M/0/1/0/all/0/1\">Michela Paganini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1\">Geoffrey Irving</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1\">Simon Osindero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1\">Karen Simonyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rae_J/0/1/0/all/0/1\">Jack W. Rae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifre_L/0/1/0/all/0/1\">Laurent Sifre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Black-Box Tuning for Language-Model-as-a-Service. (arXiv:2201.03514v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03514","description":"<p>Extremely large pre-trained language models (PTMs) such as GPT-3 are usually\nreleased as a service. It allows users to design task-specific prompts to query\nthe PTMs through some black-box APIs. In such a scenario, which we call\nLanguage-Model-as-a-Service (LMaaS), the gradients of PTMs are usually\nunavailable. Can we optimize the task prompts by only accessing the model\ninference APIs? This paper proposes the black-box tuning framework to optimize\nthe continuous prompt prepended to the input text via derivative-free\noptimization. Instead of optimizing in the original high-dimensional prompt\nspace, which is intractable for traditional derivative-free optimization, we\nperform optimization in a randomly generated subspace due to the low intrinsic\ndimensionality of large PTMs. The experimental results show that the black-box\ntuning with RoBERTa on a few labeled samples not only significantly outperforms\nmanual prompt and GPT-3's in-context learning, but also surpasses the\ngradient-based counterparts, i.e. prompt tuning and full model tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hong Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evidence-aware Fake News Detection with Graph Neural Networks. (arXiv:2201.06885v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06885","description":"<p>The prevalence and perniciousness of fake news has been a critical issue on\nthe Internet, which stimulates the development of automatic fake news detection\nin turn. In this paper, we focus on the evidence-based fake news detection,\nwhere several evidences are utilized to probe the veracity of news (i.e., a\nclaim). Most previous methods first employ sequential models to embed the\nsemantic information and then capture the claim-evidence interaction based on\ndifferent attention mechanisms. Despite their effectiveness, they still suffer\nfrom two main weaknesses. Firstly, due to the inherent drawbacks of sequential\nmodels, they fail to integrate the relevant information that is scattered far\napart in evidences for veracity checking. Secondly, they neglect much redundant\ninformation contained in evidences that may be useless or even harmful. To\nsolve these problems, we propose a unified Graph-based sEmantic sTructure\nmining framework, namely GET in short. Specifically, different from the\nexisting work that treats claims and evidences as sequences, we model them as\ngraph-structured data and capture the long-distance semantic dependency among\ndispersed relevant snippets via neighborhood propagation. After obtaining\ncontextual semantic information, our model reduces information redundancy by\nperforming graph structure learning. Finally, the fine-grained semantic\nrepresentations are fed into the downstream claim-evidence interaction module\nfor predictions. Comprehensive experiments have demonstrated the superiority of\nGET over the state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weizhi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning Over Multiple Domains in Natural Language Tasks. (arXiv:2202.00254v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00254","description":"<p>Studies of active learning traditionally assume the target and source data\nstem from a single domain. However, in realistic applications, practitioners\noften require active learning with multiple sources of out-of-distribution\ndata, where it is unclear a priori which data sources will help or hurt the\ntarget domain. We survey a wide variety of techniques in active learning (AL),\ndomain shift detection (DS), and multi-domain sampling to examine this\nchallenging setting for question answering and sentiment analysis. We ask (1)\nwhat family of methods are effective for this task? And, (2) what properties of\nselected examples and domains achieve strong results? Among 18 acquisition\nfunctions from 4 families of methods, we find H-Divergence methods, and\nparticularly our proposed variant DAL-E, yield effective results, averaging\n2-3% improvements over the random baseline. We also show the importance of a\ndiverse allocation of domains, as well as room-for-improvement of existing\nmethods on both domain and example selection. Our findings yield the first\ncomprehensive analysis of both existing and novel methods for practitioners\nfaced with multi-domain active learning for natural language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reisler_J/0/1/0/all/0/1\">Julia Reisler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_E/0/1/0/all/0/1\">Edward Greg Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Andrew Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_N/0/1/0/all/0/1\">Nikhil Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuBois_C/0/1/0/all/0/1\">Chris DuBois</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Aspect-Based Sentiment Analysis. (arXiv:2202.01924v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.01924","description":"<p>Aspect-based sentiment analysis (ABSA) typically requires in-domain annotated\ndata for supervised training/fine-tuning. It is a big challenge to scale ABSA\nto a large number of new domains. This paper aims to train a unified model that\ncan perform zero-shot ABSA without using any annotated data for a new domain.\nWe propose a method called contrastive post-training on review Natural Language\nInference (CORN). Later ABSA tasks can be cast into NLI for zero-shot transfer.\nWe evaluate CORN on ABSA tasks, ranging from aspect extraction (AE), aspect\nsentiment classification (ASC), to end-to-end aspect-based sentiment analysis\n(E2E ABSA), which show ABSA can be conducted without any human annotated ABSA\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02113","description":"<p>Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKGC/tree/main/GenKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEAPMood: Light and Efficient Architecture to Predict Mood with Genetic Algorithm driven Hyperparameter Tuning. (arXiv:2202.02522v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02522","description":"<p>Accurate and automatic detection of mood serves as a building block for use\ncases like user profiling which in turn power applications such as advertising,\nrecommendation systems, and many more. One primary source indicative of an\nindividual's mood is textual data. While there has been extensive research on\nemotion recognition, the field of mood prediction has been barely explored. In\naddition, very little work is done in the area of on-device inferencing, which\nis highly important from the user privacy point of view. In this paper, we\npropose for the first time, an on-device deep learning approach for mood\nprediction from textual data, LEAPMood. We use a novel on-device\ndeployment-focused objective function for hyperparameter tuning based on the\nGenetic Algorithm (GA) and optimize the parameters concerning both performance\nand size. LEAPMood consists of Emotion Recognition in Conversion (ERC) as the\nfirst building block followed by mood prediction using K-means clustering. We\nshow that using a combination of character embedding, phonetic hashing, and\nattention along with Conditional Random Fields (CRF), results in a performance\nclosely comparable to that of the current State-Of-the-Art with a significant\nreduction in model size (&gt; 90%) for the task of ERC. We achieve a Micro F1\nscore of 62.05% with a memory footprint of a mere 1.67MB on the DailyDialog\ndataset. Furthermore, we curate a dataset for the task of mood prediction\nachieving a Macro F1-score of 72.12% with LEAPMood.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_H/0/1/0/all/0/1\">Harichandana B S S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sumit Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moving Other Way: Exploring Word Mover Distance Extensions. (arXiv:2202.03119v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03119","description":"<p>The word mover's distance (WMD) is a popular semantic similarity metric for\ntwo texts. This position paper studies several possible extensions of WMD. We\nexperiment with the frequency of words in the corpus as a weighting factor and\nthe geometry of the word vector space. We validate possible extensions of WMD\non six document classification datasets. Some proposed extensions show better\nresults in terms of the k-nearest neighbor classification error than WMD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smirnov_I/0/1/0/all/0/1\">Ilya Smirnov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Backdoor Defense via Decoupling the Training Process. (arXiv:2202.03423v1 [cs.CR])","link":"http://arxiv.org/abs/2202.03423","description":"<p>Recent studies have revealed that deep neural networks (DNNs) are vulnerable\nto backdoor attacks, where attackers embed hidden backdoors in the DNN model by\npoisoning a few training samples. The attacked model behaves normally on benign\nsamples, whereas its prediction will be maliciously changed when the backdoor\nis activated. We reveal that poisoned samples tend to cluster together in the\nfeature space of the attacked DNN model, which is mostly due to the end-to-end\nsupervised training paradigm. Inspired by this observation, we propose a novel\nbackdoor defense via decoupling the original end-to-end training process into\nthree stages. Specifically, we first learn the backbone of a DNN model via\n\\emph{self-supervised learning} based on training samples without their labels.\nThe learned backbone will map samples with the same ground-truth label to\nsimilar locations in the feature space. Then, we freeze the parameters of the\nlearned backbone and train the remaining fully connected layers via standard\ntraining with all (labeled) training samples. Lastly, to further alleviate\nside-effects of poisoned samples in the second stage, we remove labels of some\n`low-credible' samples determined based on the learned model and conduct a\n\\emph{semi-supervised fine-tuning} of the whole model. Extensive experiments on\nmultiple benchmark datasets and DNN models verify that the proposed defense is\neffective in reducing backdoor threats while preserving high accuracy in\npredicting benign samples. Our code is available at\n\\url{https://github.com/SCLBD/DBD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kunzhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Baoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kui Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performance Evaluation of Infrared Image Enhancement Techniques. (arXiv:2202.03427v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03427","description":"<p>Infrared (IR) images are widely used in many fields such as medical imaging,\nobject tracking, astronomy and military purposes for securing borders. Infrared\nimages can be captured day or night based on the type of capturing device. The\ncapturing devices use electromagnetic radiation with longer wavelengths. There\nare several types of IR radiation based on the range of wavelength and\ncorresponding frequency. Due to noising and other artifacts, IR images are not\nclearly visible. In this paper, we present a complete up-todate survey on IR\nimaging enhancement techniques. The survey includes IR radiation types and\ndevices and existing IR datasets. The survey covers spatial enhancement\ntechniques, frequency-domain based enhancement techniques and Deep\nlearning-based techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gaber_R/0/1/0/all/0/1\">Rania Gaber</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_A/0/1/0/all/0/1\">AbdElmgied Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_K/0/1/0/all/0/1\">Kareem Ahmed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Topology-Attention ConvLSTM Network and Its Application to EM Images. (arXiv:2202.03430v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03430","description":"<p>Structural accuracy of segmentation is important for finescale structures in\nbiomedical images. We propose a novel TopologyAttention ConvLSTM Network\n(TACNet) for 3D image segmentation in order to achieve high structural accuracy\nfor 3D segmentation tasks. Specifically, we propose a Spatial\nTopology-Attention (STA) module to process a 3D image as a stack of 2D image\nslices and adopt ConvLSTM to leverage contextual structure information from\nadjacent slices. In order to effectively transfer topology-critical information\nacross slices, we propose an Iterative-Topology Attention (ITA) module that\nprovides a more stable topology-critical map for segmentation. Quantitative and\nqualitative results show that our proposed method outperforms various baselines\nin terms of topology-aware evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jiaqi Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoling Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsai_C/0/1/0/all/0/1\">Chialing Tsai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inference of captions from histopathological patches. (arXiv:2202.03432v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03432","description":"<p>Computational histopathology has made significant strides in the past few\nyears, slowly getting closer to clinical adoption. One area of benefit would be\nthe automatic generation of diagnostic reports from H\\&amp;E-stained whole slide\nimages which would further increase the efficiency of the pathologists' routine\ndiagnostic workflows. In this study, we compiled a dataset (PatchGastricADC22)\nof histopathological captions of stomach adenocarcinoma endoscopic biopsy\nspecimens, which we extracted from diagnostic reports and paired with patches\nextracted from the associated whole slide images. The dataset contains a\nvariety of gastric adenocarcinoma subtypes. We trained a baseline\nattention-based model to predict the captions from features extracted from the\npatches and obtained promising results. We make the captioned dataset of 262K\npatches publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tsuneki_M/0/1/0/all/0/1\">Masayuki Tsuneki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kanavati_F/0/1/0/all/0/1\">Fahdi Kanavati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Coarse-to-fine Morphological Approach With Knowledge-based Rules and Self-adapting Correction for Lung Nodules Segmentation. (arXiv:2202.03433v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03433","description":"<p>The segmentation module which precisely outlines the nodules is a crucial\nstep in a computer-aided diagnosis(CAD) system. The most challenging part of\nsuch a module is how to achieve high accuracy of the segmentation, especially\nfor the juxtapleural, non-solid and small nodules. In this research, we present\na coarse-to-fine methodology that greatly improves the thresholding method\nperformance with a novel self-adapting correction algorithm and effectively\nremoves noisy pixels with well-defined knowledge-based principles. Compared\nwith recent strong morphological baselines, our algorithm, by combining dataset\nfeatures, achieves state-of-the-art performance on both the public LIDC-IDRI\ndataset (DSC 0.699) and our private LC015 dataset (DSC 0.760) which closely\napproaches the SOTA deep learning-based models' performances. Furthermore,\nunlike most available morphological methods that can only segment the isolated\nand well-circumscribed nodules accurately, the precision of our method is\ntotally independent of the nodule type or diameter, proving its applicability\nand generality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1\">Xinliang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_J/0/1/0/all/0/1\">Jiayin Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mai_J/0/1/0/all/0/1\">Juanyun Mai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_Y/0/1/0/all/0/1\">Yanbo Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Minghao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Linyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diao_Z/0/1/0/all/0/1\">Zhaoqi Diao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jianyu Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_J/0/1/0/all/0/1\">Jian You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_A/0/1/0/all/0/1\">Airu Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_X/0/1/0/all/0/1\">Xiangcheng Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_J/0/1/0/all/0/1\">Jinsheng Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_H/0/1/0/all/0/1\">Hua Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal data generation with a deep metric variational autoencoder. (arXiv:2202.03434v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03434","description":"<p>We present a deep metric variational autoencoder for multi-modal data\ngeneration. The variational autoencoder employs triplet loss in the latent\nspace, which allows for conditional data generation by sampling in the latent\nspace within each class cluster. The approach is evaluated on a multi-modal\ndataset consisting of otoscopy images of the tympanic membrane with\ncorresponding wideband tympanometry measurements. The modalities in this\ndataset are correlated, as they represent different aspects of the state of the\nmiddle ear, but they do not present a direct pixel-to-pixel correlation. The\napproach shows promising results for the conditional generation of pairs of\nimages and tympanograms, and will allow for efficient data augmentation of data\nfrom multi-modal sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sundgaard_J/0/1/0/all/0/1\">Josefine Vilsb&#xf8;ll Sundgaard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hannemose_M/0/1/0/all/0/1\">Morten Rieger Hannemose</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laugesen_S/0/1/0/all/0/1\">S&#xf8;ren Laugesen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bray_P/0/1/0/all/0/1\">Peter Bray</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harte_J/0/1/0/all/0/1\">James Harte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamide_Y/0/1/0/all/0/1\">Yosuke Kamide</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanaka_C/0/1/0/all/0/1\">Chiemi Tanaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paulsen_R/0/1/0/all/0/1\">Rasmus R. Paulsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christensen_A/0/1/0/all/0/1\">Anders Nymark Christensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PatClArC: Using Pattern Concept Activation Vectors for Noise-Robust Model Debugging. (arXiv:2202.03482v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03482","description":"<p>State-of-the-art machine learning models are commonly (pre-)trained on large\nbenchmark datasets. These often contain biases, artifacts, or errors that have\nremained unnoticed in the data collection process and therefore fail in\nrepresenting the real world truthfully. This can cause models trained on these\ndatasets to learn undesired behavior based upon spurious correlations, e.g.,\nthe existence of a copyright tag in an image. Concept Activation Vectors (CAV)\nhave been proposed as a tool to model known concepts in latent space and have\nbeen used for concept sensitivity testing and model correction. Specifically,\nclass artifact compensation (ClArC) corrects models using CAVs to represent\ndata artifacts in feature space linearly. Modeling CAVs with filters of linear\nmodels, however, causes a significant influence of the noise portion within the\ndata, as recent work proposes the unsuitability of linear model filters to find\nthe signal direction in the input, which can be avoided by instead using\npatterns. In this paper we propose Pattern Concept Activation Vectors (PCAV)\nfor noise-robust concept representations in latent space. We demonstrate that\npattern-based artifact modeling has beneficial effects on the application of\nCAVs as a means to remove influence of confounding features from models via the\nClArC framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pahde_F/0/1/0/all/0/1\">Frederik Pahde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_L/0/1/0/all/0/1\">Leander Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anders_C/0/1/0/all/0/1\">Christopher J. Anders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1\">Sebastian Lapuschkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Random Ferns for Semantic Segmentation of PolSAR Images. (arXiv:2202.03498v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03498","description":"<p>Random Ferns -- as a less known example of Ensemble Learning -- have been\nsuccessfully applied in many Computer Vision applications ranging from keypoint\nmatching to object detection. This paper extends the Random Fern framework to\nthe semantic segmentation of polarimetric synthetic aperture radar images. By\nusing internal projections that are defined over the space of Hermitian\nmatrices, the proposed classifier can be directly applied to the polarimetric\ncovariance matrices without the need to explicitly compute predefined image\nfeatures. Furthermore, two distinct optimization strategies are proposed: The\nfirst based on pre-selection and grouping of internal binary features before\nthe creation of the classifier; and the second based on iteratively improving\nthe properties of a given Random Fern. Both strategies are able to boost the\nperformance by filtering features that are either redundant or have a low\ninformation content and by grouping correlated features to best fulfill the\nindependence assumptions made by the Random Fern classifier. Experiments show\nthat results can be achieved that are similar to a more complex Random Forest\nmodel and competitive to a deep learning baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pengchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansch_R/0/1/0/all/0/1\">Ronny H&#xe4;nsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scribble-based Boundary-aware Network for Weakly Supervised Salient Object Detection in Remote Sensing Images. (arXiv:2202.03501v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03501","description":"<p>Existing CNNs-based salient object detection (SOD) heavily depends on the\nlarge-scale pixel-level annotations, which is labor-intensive, time-consuming,\nand expensive. By contrast, the sparse annotations become appealing to the\nsalient object detection community. However, few efforts are devoted to\nlearning salient object detection from sparse annotations, especially in the\nremote sensing field. In addition, the sparse annotation usually contains\nscanty information, which makes it challenging to train a well-performing\nmodel, resulting in its performance largely lagging behind the fully-supervised\nmodels. Although some SOD methods adopt some prior cues to improve the\ndetection performance, they usually lack targeted discrimination of object\nboundaries and thus provide saliency maps with poor boundary localization. To\nthis end, in this paper, we propose a novel weakly-supervised salient object\ndetection framework to predict the saliency of remote sensing images from\nsparse scribble annotations. To implement it, we first construct the\nscribble-based remote sensing saliency dataset by relabelling an existing\nlarge-scale SOD dataset with scribbles, namely S-EOR dataset. After that, we\npresent a novel scribble-based boundary-aware network (SBA-Net) for remote\nsensing salient object detection. Specifically, we design a boundary-aware\nmodule (BAM) to explore the object boundary semantics, which is explicitly\nsupervised by the high-confidence object boundary (pseudo) labels generated by\nthe boundary label generation (BLG) module, forcing the model to learn features\nthat highlight the object structure and thus boosting the boundary localization\nof objects. Then, the boundary semantics are integrated with high-level\nfeatures to guide the salient object detection under the supervision of\nscribble labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tian-Zhu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huai-Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hang Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrated Multiscale Domain Adaptive YOLO. (arXiv:2202.03527v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03527","description":"<p>The area of domain adaptation has been instrumental in addressing the domain\nshift problem encountered by many applications. This problem arises due to the\ndifference between the distributions of source data used for training in\ncomparison with target data used during realistic testing scenarios. In this\npaper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO)\nframework that employs multiple domain adaptation paths and corresponding\ndomain classifiers at different scales of the recently introduced YOLOv4 object\ndetector. Building on our baseline multiscale DAYOLO framework, we introduce\nthree novel deep learning architectures for a Domain Adaptation Network (DAN)\nthat generates domain-invariant features. In particular, we propose a\nProgressive Feature Reduction (PFR), a Unified Classifier (UC), and an\nIntegrated architecture. We train and test our proposed DAN architectures in\nconjunction with YOLOv4 using popular datasets. Our experiments show\nsignificant improvements in object detection performance when training YOLOv4\nusing the proposed MS-DAYOLO architectures and when tested on target data for\nautonomous driving applications. Moreover, MS-DAYOLO framework achieves an\norder of magnitude real-time speed improvement relative to Faster R-CNN\nsolutions while providing comparable object detection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hnewa_M/0/1/0/all/0/1\">Mazin Hnewa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radha_H/0/1/0/all/0/1\">Hayder Radha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MINER: Multiscale Implicit Neural Representations. (arXiv:2202.03532v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03532","description":"<p>We introduce a new neural signal representation designed for the efficient\nhigh-resolution representation of large-scale signals. The key innovation in\nour multiscale implicit neural representation (MINER) is an internal\nrepresentation via a Laplacian pyramid, which provides a sparse multiscale\nrepresentation of the signal that captures orthogonal parts of the signal\nacross scales. We leverage the advantages of the Laplacian pyramid by\nrepresenting small disjoint patches of the pyramid at each scale with a tiny\nMLP. This enables the capacity of the network to adaptively increase from\ncoarse to fine scales, and only represent parts of the signal with strong\nsignal energy. The parameters of each MLP are optimized from coarse-to-fine\nscale which results in faster approximations at coarser scales, thereby\nultimately an extremely fast training process. We apply MINER to a range of\nlarge-scale signal representation tasks, including gigapixel images and very\nlarge point clouds, and demonstrate that it requires fewer than 25% of the\nparameters, 33% of the memory footprint, and 10% of the computation time of\ncompeting techniques such as ACORN to reach the same representation error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saragadam_V/0/1/0/all/0/1\">Vishwanath Saragadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jasper Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_G/0/1/0/all/0/1\">Guha Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeraraghavan_A/0/1/0/all/0/1\">Ashok Veeraraghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SliTraNet: Automatic Detection of Slide Transitions in Lecture Videos using Convolutional Neural Networks. (arXiv:2202.03540v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03540","description":"<p>With the increasing number of online learning material in the web, search for\nspecific content in lecture videos can be time consuming. Therefore, automatic\nslide extraction from the lecture videos can be helpful to give a brief\noverview of the main content and to support the students in their studies. For\nthis task, we propose a deep learning method to detect slide transitions in\nlectures videos. We first process each frame of the video by a heuristic-based\napproach using a 2-D convolutional neural network to predict transition\ncandidates. Then, we increase the complexity by employing two 3-D convolutional\nneural networks to refine the transition candidates. Evaluation results\ndemonstrate the effectiveness of our method in finding slide transitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sindel_A/0/1/0/all/0/1\">Aline Sindel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_A/0/1/0/all/0/1\">Abner Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seung Hee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christlein_V/0/1/0/all/0/1\">Vincent Christlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling. (arXiv:2202.03543v1 [eess.AS])","link":"http://arxiv.org/abs/2202.03543","description":"<p>In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge\nand SUPERB benchmark. Our submissions are based on the recently proposed\nFaST-VGS model, which is a Transformer-based model that learns to associate raw\nspeech waveforms with semantically related images, all without the use of any\ntranscriptions of the speech. Additionally, we introduce a novel extension of\nthis model, FaST-VGS+, which is learned in a multi-task fashion with a masked\nlanguage modeling objective in addition to the visual grounding objective. On\nZeroSpeech 2021, we show that our models perform competitively on the ABX task,\noutperform all other concurrent submissions on the Syntactic and Semantic\ntasks, and nearly match the best system on the Lexical task. On the SUPERB\nbenchmark, we show that our models also achieve strong performance, in some\ncases even outperforming the popular wav2vec2.0 model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LwPosr: Lightweight Efficient Fine-Grained Head Pose Estimation. (arXiv:2202.03544v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03544","description":"<p>This paper presents a lightweight network for head pose estimation (HPE)\ntask. While previous approaches rely on convolutional neural networks, the\nproposed network \\textit{LwPosr} uses mixture of depthwise separable\nconvolutional (DSC) and transformer encoder layers which are structured in two\nstreams and three stages to provide fine-grained regression for predicting head\nposes. The quantitative and qualitative demonstration is provided to show that\nthe proposed network is able to learn head poses efficiently while using less\nparameter space. Extensive ablations are conducted using three open-source\ndatasets namely 300W-LP, AFLW2000, and BIWI datasets. To our knowledge, (1)\n\\textit{LwPosr} is the lightest network proposed for estimating head poses\ncompared to both keypoints-based and keypoints-free approaches; (2) it sets a\nbenchmark for both overperforming the previous lightweight network on mean\nabsolute error and on reducing number of parameters; (3) it is first of its\nkind to use mixture of DSCs and transformer encoders for HPE. This approach is\nsuitable for mobile devices which require lightweight networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_N/0/1/0/all/0/1\">Naina Dhingra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HeadPosr: End-to-end Trainable Head Pose Estimation using Transformer Encoders. (arXiv:2202.03548v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03548","description":"<p>In this paper, HeadPosr is proposed to predict the head poses using a single\nRGB image. \\textit{HeadPosr} uses a novel architecture which includes a\ntransformer encoder. In concrete, it consists of: (1) backbone; (2) connector;\n(3) transformer encoder; (4) prediction head. The significance of using a\ntransformer encoder for HPE is studied. An extensive ablation study is\nperformed on varying the (1) number of encoders; (2) number of heads; (3)\ndifferent position embeddings; (4) different activations; (5) input channel\nsize, in a transformer used in HeadPosr. Further studies on using: (1)\ndifferent backbones, (2) using different learning rates are also shown. The\nelaborated experiments and ablations studies are conducted using three\ndifferent open-source widely used datasets for HPE, i.e., 300W-LP, AFLW2000,\nand BIWI datasets. Experiments illustrate that \\textit{HeadPosr} outperforms\nall the state-of-art methods including both the landmark-free and the others\nbased on using landmark or depth estimation on the AFLW2000 dataset and BIWI\ndatasets when trained with 300W-LP. It also outperforms when averaging the\nresults from the compared datasets, hence setting a benchmark for the problem\nof HPE, also demonstrating the effectiveness of using transformers over the\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_N/0/1/0/all/0/1\">Naina Dhingra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aladdin: Joint Atlas Building and Diffeomorphic Registration Learning with Pairwise Alignment. (arXiv:2202.03563v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03563","description":"<p>Atlas building and image registration are important tasks for medical image\nanalysis. Once one or multiple atlases from an image population have been\nconstructed, commonly (1) images are warped into an atlas space to study\nintra-subject or inter-subject variations or (2) a possibly probabilistic atlas\nis warped into image space to assign anatomical labels. Atlas estimation and\nnonparametric transformations are computationally expensive as they usually\nrequire numerical optimization. Additionally, previous approaches for atlas\nbuilding often define similarity measures between a fuzzy atlas and each\nindividual image, which may cause alignment difficulties because a fuzzy atlas\ndoes not exhibit clear anatomical structures in contrast to the individual\nimages. This work explores using a convolutional neural network (CNN) to\njointly predict the atlas and a stationary velocity field (SVF)\nparameterization for diffeomorphic image registration with respect to the\natlas. Our approach does not require affine pre-registrations and utilizes\npairwise image alignment losses to increase registration accuracy. We evaluate\nour model on 3D knee magnetic resonance images (MRI) from the OAI-ZIB dataset.\nOur results show that the proposed framework achieves better performance than\nother state-of-the-art image registration algorithms, allows for end-to-end\ntraining, and for fast inference at test time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ding_Z/0/1/0/all/0/1\">Zhipeng Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate super-resolution low-field brain MRI. (arXiv:2202.03564v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03564","description":"<p>The recent introduction of portable, low-field MRI (LF-MRI) into the clinical\nsetting has the potential to transform neuroimaging. However, LF-MRI is limited\nby lower resolution and signal-to-noise ratio, leading to incomplete\ncharacterization of brain regions. To address this challenge, recent advances\nin machine learning facilitate the synthesis of higher resolution images\nderived from one or multiple lower resolution scans. Here, we report the\nextension of a machine learning super-resolution (SR) algorithm to synthesize 1\nmm isotropic MPRAGE-like scans from LF-MRI T1-weighted and T2-weighted\nsequences. Our initial results on a paired dataset of LF and high-field (HF,\n1.5T-3T) clinical scans show that: (i) application of available automated\nsegmentation tools directly to LF-MRI images falters; but (ii) segmentation\ntools succeed when applied to SR images with high correlation to gold standard\nmeasurements from HF-MRI (e.g., r = 0.85 for hippocampal volume, r = 0.84 for\nthe thalamus, r = 0.92 for the whole cerebrum). This work demonstrates\nproof-of-principle post-processing image enhancement from lower resolution\nLF-MRI sequences. These results lay the foundation for future work to enhance\nthe detection of normal and abnormal image findings at LF and ultimately\nimprove the diagnostic performance of LF-MRI. Our tools are publicly available\non FreeSurfer (surfer.nmr.mgh.harvard.edu/).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schleicher_R/0/1/0/all/0/1\">Riana Schleicher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laguna_S/0/1/0/all/0/1\">Sonia Laguna</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Billot_B/0/1/0/all/0/1\">Benjamin Billot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schaefer_P/0/1/0/all/0/1\">Pamela Schaefer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McKaig_B/0/1/0/all/0/1\">Brenna McKaig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goldstein_J/0/1/0/all/0/1\">Joshua N. Goldstein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheth_K/0/1/0/all/0/1\">Kevin N. Sheth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rosen_M/0/1/0/all/0/1\">Matthew S. Rosen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kimberly_W/0/1/0/all/0/1\">W. Taylor Kimberly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phase-Stretch Adaptive Gradient-Field Extractor (PAGE). (arXiv:2202.03570v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03570","description":"<p>Phase-Stretch Adaptive Gradient-Field Extractor (PAGE) is an edge detection\nalgorithm that is inspired by physics of electromagnetic diffraction and\ndispersion. A computational imaging algorithm, it identifies edges, their\norientations and sharpness in a digital image where the image brightness\nchanges abruptly. Edge detection is a basic operation performed by the eye and\nis crucial to visual perception. PAGE embeds an original image into a set of\nfeature maps that can be used for object representation and classification. The\nalgorithm performs exceptionally well as an edge and texture extractor in low\nlight level and low contrast images. This manuscript is prepared to support the\nopen-source code which is being simultaneously made available within the GitHub\nrepository https://github.com/JalaliLabUCLA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+MacPhee_C/0/1/0/all/0/1\">Callen MacPhee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suthar_M/0/1/0/all/0/1\">Madhuri Suthar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jalali_B/0/1/0/all/0/1\">Bahram Jalali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metal Artifact Reduction with Intra-Oral Scan Data for 3D Low Dose Maxillofacial CBCT Modeling. (arXiv:2202.03571v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03571","description":"<p>Low-dose dental cone beam computed tomography (CBCT) has been increasingly\nused for maxillofacial modeling. However, the presence of metallic inserts,\nsuch as implants, crowns, and dental filling, causes severe streaking and\nshading artifacts in a CBCT image and loss of the morphological structures of\nthe teeth, which consequently prevents accurate segmentation of bones. A\ntwo-stage metal artifact reduction method is proposed for accurate 3D low-dose\nmaxillofacial CBCT modeling, where a key idea is to utilize explicit tooth\nshape prior information from intra-oral scan data whose acquisition does not\nrequire any extra radiation exposure. In the first stage, an image-to-image\ndeep learning network is employed to mitigate metal-related artifacts. To\nimprove the learning ability, the proposed network is designed to take\nadvantage of the intra-oral scan data as side-inputs and perform multi-task\nlearning of auxiliary tooth segmentation. In the second stage, a 3D\nmaxillofacial model is constructed by segmenting the bones from the dental CBCT\nimage corrected in the first stage. For accurate bone segmentation, weighted\nthresholding is applied, wherein the weighting region is determined depending\non the geometry of the intra-oral scan data. Because acquiring a paired\ntraining dataset of metal-artifact-free and metal artifact-affected dental CBCT\nimages is challenging in clinical practice, an automatic method of generating a\nrealistic dataset according to the CBCT physics model is introduced. Numerical\nsimulations and clinical experiments show the feasibility of the proposed\nmethod, which takes advantage of tooth surface information from intra-oral scan\ndata in 3D low dose maxillofacial CBCT modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hyun_C/0/1/0/all/0/1\">Chang Min Hyun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bayaraa_T/0/1/0/all/0/1\">Taigyntuya Bayaraa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yun_H/0/1/0/all/0/1\">Hye Sun Yun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jang_T/0/1/0/all/0/1\">Tae Jun Jang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_H/0/1/0/all/0/1\">Hyoung Suk Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seo_J/0/1/0/all/0/1\">Jin Keun Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Prediction Problem Archive. (arXiv:2202.03574v1 [cs.LG])","link":"http://arxiv.org/abs/2202.03574","description":"<p>Structured prediction problems are one of the fundamental tools in machine\nlearning. In order to facilitate algorithm development for their numerical\nsolution, we collect in one place a large number of datasets in easy to read\nformats for a diverse set of problem classes. We provide archival links to\ndatasets, description of the considered problems and problem formats, and a\nshort summary of problem characteristics including size, number of instances\netc. For reference we also give a non-exhaustive selection of algorithms\nproposed in the literature for their solution. We hope that this central\nrepository will make benchmarking and comparison to established works easier.\nWe welcome submission of interesting new datasets and algorithms for inclusion\nin our archive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hornakova_A/0/1/0/all/0/1\">Andrea Hornakova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roetzer_P/0/1/0/all/0/1\">Paul Roetzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1\">Ahmed Abbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations. (arXiv:2202.03576v1 [cs.LG])","link":"http://arxiv.org/abs/2202.03576","description":"<p>Owing much to the revolution of information technology, the recent progress\nof deep learning benefits incredibly from the vastly enhanced access to data\navailable in various digital formats. However, in certain scenarios, people may\nnot want their data being used for training commercial models and thus studied\nhow to attack the learnability of deep learning models. Previous works on\nlearnability attack only consider the goal of preventing unauthorized\nexploitation on the specific dataset but not the process of restoring the\nlearnability for authorized cases. To tackle this issue, this paper introduces\nand investigates a new concept called \"learnability lock\" for controlling the\nmodel's learnability on a specific dataset with a special key. In particular,\nwe propose adversarial invertible transformation, that can be viewed as a\nmapping from image to image, to slightly modify data samples so that they\nbecome \"unlearnable\" by machine learning models with negligible loss of visual\nfeatures. Meanwhile, one can unlock the learnability of the dataset and train\nmodels normally using the corresponding key. The proposed learnability lock\nleverages class-wise perturbation that applies a universal transformation\nfunction on data samples of the same label. This ensures that the learnability\ncan be easily restored with a simple inverse transformation while remaining\ndifficult to be detected or reverse-engineered. We empirically demonstrate the\nsuccess and practicability of our method on visual classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Weiqi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs. (arXiv:2202.03583v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03583","description":"<p>Chest X-ray images are one of the most common medical diagnosis techniques to\nidentify different thoracic diseases. However, identification of pathologies in\nX-ray images requires skilled manpower and are often cited as a time-consuming\ntask with varied level of interpretation, particularly in cases where the\nidentification of disease only by images is difficult for human eyes. With\nrecent achievements of deep learning in image classification, its application\nin disease diagnosis has been widely explored. This research project presents a\nmulti-label disease diagnosis model of chest x-rays. Using Dense Convolutional\nNeural Network (DenseNet), the diagnosis system was able to obtain high\nclassification predictions. The model obtained the highest AUC score of 0.896\nfor condition Cardiomegaly and the lowest AUC score for Nodule, 0.655. The\nmodel also localized the parts of the chest radiograph that indicated the\npresence of each pathology using GRADCAM, thus contributing to the model\ninterpretability of a deep learning algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhusal_D/0/1/0/all/0/1\">Dipkamal Bhusal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Panday_D/0/1/0/all/0/1\">Dr. Sanjeeb Prasad Panday</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair SA: Sensitivity Analysis for Fairness in Face Recognition. (arXiv:2202.03586v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03586","description":"<p>As the use of deep learning in high impact domains becomes ubiquitous, it is\nincreasingly important to assess the resilience of models. One such high impact\ndomain is that of face recognition, with real world applications involving\nimages affected by various degradations, such as motion blur or high exposure.\nMoreover, images captured across different attributes, such as gender and race,\ncan also challenge the robustness of a face recognition algorithm. While\ntraditional summary statistics suggest that the aggregate performance of face\nrecognition models has continued to improve, these metrics do not directly\nmeasure the robustness or fairness of the models. Visual Psychophysics\nSensitivity Analysis (VPSA) [1] provides a way to pinpoint the individual\ncauses of failure by way of introducing incremental perturbations in the data.\nHowever, perturbations may affect subgroups differently. In this paper, we\npropose a new fairness evaluation based on robustness in the form of a generic\nframework that extends VPSA. With this framework, we can analyze the ability of\na model to perform fairly for different subgroups of a population affected by\nperturbations, and pinpoint the exact failure modes for a subgroup by measuring\ntargeted robustness. With the increasing focus on the fairness of models, we\nuse face recognition as an example application of our framework and propose to\ncompactly visualize the fairness analysis of a model via AUC matrices. We\nanalyze the performance of common face recognition models and empirically show\nthat certain subgroups are at a disadvantage when images are perturbed, thereby\nuncovering trends that were not visible using the model's performance on\nsubgroups without perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Aparna R. Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suau_X/0/1/0/all/0/1\">Xavier Suau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_N/0/1/0/all/0/1\">Nivedha Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zappella_L/0/1/0/all/0/1\">Luca Zappella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostoloff_N/0/1/0/all/0/1\">Nicholas Apostoloff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model and predict age and sex in healthy subjects using brain white matter features: A deep learning approach. (arXiv:2202.03595v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03595","description":"<p>The human brain's white matter (WM) structure is of immense interest to the\nscientific community. Diffusion MRI gives a powerful tool to describe the brain\nWM structure noninvasively. To potentially enable monitoring of age-related\nchanges and investigation of sex-related brain structure differences on the\nmapping between the brain connectome and healthy subjects' age and sex, we\nextract fiber-cluster-based diffusion features and predict sex and age with a\nnovel ensembled neural network classifier. We conduct experiments on the Human\nConnectome Project (HCP) young adult dataset and show that our model achieves\n94.82% accuracy in sex prediction and 2.51 years MAE in age prediction. We also\nshow that the fractional anisotropy (FA) is the most predictive of sex, while\nthe number of fibers is the most predictive of age and the combination of\ndifferent features can improve the model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pieper_S/0/1/0/all/0/1\">Steve Pieper</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makris_N/0/1/0/all/0/1\">Nikos Makris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rathi_Y/0/1/0/all/0/1\">Yogesh Rathi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wells_W/0/1/0/all/0/1\">William Wells III</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ODonnell_L/0/1/0/all/0/1\">Lauren J. O&#x27;Donnell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOST-Net: A Memory Oriented Style Transfer Network for Face Sketch Synthesis. (arXiv:2202.03596v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03596","description":"<p>Face sketch synthesis has been widely used in multi-media entertainment and\nlaw enforcement. Despite the recent developments in deep neural networks,\naccurate and realistic face sketch synthesis is still a challenging task due to\nthe diversity and complexity of human faces. Current image-to-image\ntranslation-based face sketch synthesis frequently encounters over-fitting\nproblems when it comes to small-scale datasets. To tackle this problem, we\npresent an end-to-end Memory Oriented Style Transfer Network (MOST-Net) for\nface sketch synthesis which can produce high-fidelity sketches with limited\ndata. Specifically, an external self-supervised dynamic memory module is\nintroduced to capture the domain alignment knowledge in the long term. In this\nway, our proposed model could obtain the domain-transfer ability by\nestablishing the durable relationship between faces and corresponding sketches\non the feature level. Furthermore, we design a novel Memory Refinement Loss (MR\nLoss) for feature alignment in the memory module, which enhances the accuracy\nof memory slots in an unsupervised manner. Extensive experiments on the CUFS\nand the CUFSF datasets show that our MOST-Net achieves state-of-the-art\nperformance, especially in terms of the Structural Similarity Index(SSIM).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_F/0/1/0/all/0/1\">Fan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xingqun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Scene BERT: Improving object detection by searching for challenging groups of data. (arXiv:2202.03651v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03651","description":"<p>Modern computer vision applications rely on learning-based perception modules\nparameterized with neural networks for tasks like object detection. These\nmodules frequently have low expected error overall but high error on atypical\ngroups of data due to biases inherent in the training process. In building\nautonomous vehicles (AV), this problem is an especially important challenge\nbecause their perception modules are crucial to the overall system performance.\nAfter identifying failures in AV, a human team will comb through the associated\ndata to group perception failures that share common causes. More data from\nthese groups is then collected and annotated before retraining the model to fix\nthe issue. In other words, error groups are found and addressed in hindsight.\nOur main contribution is a pseudo-automatic method to discover such groups in\nforesight by performing causal interventions on simulated scenes. To keep our\ninterventions on the data manifold, we utilize masked language models. We\nverify that the prioritized groups found via intervention are challenging for\nthe object detector and show that retraining with data collected from these\ngroups helps inordinately compared to adding more IID data. We also plan to\nrelease software to run interventions in simulated scenes, which we hope will\nbenefit the causality community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Resnick_C/0/1/0/all/0/1\">Cinjon Resnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1\">Or Litany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1\">Amlan Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1\">Karsten Kreis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_J/0/1/0/all/0/1\">James Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Understand Masked Autoencoders. (arXiv:2202.03670v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03670","description":"<p>\"Masked Autoencoders (MAE) Are Scalable Vision Learners\" revolutionizes the\nself-supervised learning that not only achieves the state-of-the-art for image\npretraining, but also is a milestone that bridged the gap between the visual\nand linguistic masked autoencoding (BERT-style) pretrainings. However, to our\nknowledge, to date there are no theoretical perspectives to explain the\npowerful expressivity of MAE. In this paper, we, for the first time, propose a\nunified theoretical framework that provides a mathematical understanding for\nMAE. Particularly, we explain the patch-based attention approaches of MAE using\nan integral kernel under a non-overlapping domain decomposition setting. To\nhelp the researchers to further grasp the main reasons of the great success of\nMAE, based on our framework, we contribute five questions and answer them by\ninsights from operator theory with mathematical rigor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shuhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David A. Clifton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAD-RADS Scoring using Deep Learning and Task-Specific Centerline Labeling. (arXiv:2202.03671v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03671","description":"<p>With coronary artery disease (CAD) persisting to be one of the leading causes\nof death worldwide, interest in supporting physicians with algorithms to speed\nup and improve diagnosis is high. In clinical practice, the severeness of CAD\nis often assessed with a coronary CT angiography (CCTA) scan and manually\ngraded with the CAD-Reporting and Data System (CAD-RADS) score. The clinical\nquestions this score assesses are whether patients have CAD or not (rule-out)\nand whether they have severe CAD or not (hold-out). In this work, we reach new\nstate-of-the-art performance for automatic CAD-RADS scoring. We propose using\nseverity-based label encoding, test time augmentation (TTA) and model\nensembling for a task-specific deep learning architecture. Furthermore, we\nintroduce a novel task- and model-specific, heuristic coronary segment\nlabeling, which subdivides coronary trees into consistent parts across\npatients. It is fast, robust, and easy to implement. We were able to raise the\npreviously reported area under the receiver operating characteristic curve\n(AUC) from 0.914 to 0.942 in the rule-out and from 0.921 to 0.950 in the\nhold-out task respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Denzinger_F/0/1/0/all/0/1\">Felix Denzinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wels_M/0/1/0/all/0/1\">Michael Wels</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taubmann_O/0/1/0/all/0/1\">Oliver Taubmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gulsun_M/0/1/0/all/0/1\">Mehmet A. G&#xfc;ls&#xfc;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schobinger_M/0/1/0/all/0/1\">Max Sch&#xf6;binger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andre_F/0/1/0/all/0/1\">Florian Andr&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buss_S/0/1/0/all/0/1\">Sebastian J. Buss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorich_J/0/1/0/all/0/1\">Johannes G&#xf6;rich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suhling_M/0/1/0/all/0/1\">Michael S&#xfc;hling</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trained Model in Supervised Deep Learning is a Conditional Risk Minimizer. (arXiv:2202.03674v1 [cs.LG])","link":"http://arxiv.org/abs/2202.03674","description":"<p>We proved that a trained model in supervised deep learning minimizes the\nconditional risk for each input (Theorem 2.1). This property provided insights\ninto the behavior of trained models and established a connection between\nsupervised and unsupervised learning in some cases. In addition, when the\nlabels are intractable but can be written as a conditional risk minimizer, we\nproved an equivalent form of the original supervised learning problem with\naccessible labels (Theorem 2.2). We demonstrated that many existing works, such\nas Noise2Score, Noise2Noise and score function estimation can be explained by\nour theorem. Moreover, we derived a property of classification problem with\nnoisy labels using Theorem 2.1 and validated it using MNIST dataset.\nFurthermore, We proposed a method to estimate uncertainty in image\nsuper-resolution based on Theorem 2.2 and validated it using ImageNet dataset.\nOur code is available on github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yutong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dufan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Quanzheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Image Descriptor with Aggregated Semantic Skeleton Representation for Long-term Visual Place Recognition. (arXiv:2202.03677v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03677","description":"<p>In a Simultaneous Localization and Mapping (SLAM) system, a loop-closure can\neliminate accumulated errors, which is accomplished by Visual Place Recognition\n(VPR), a task that retrieves the current scene from a set of pre-stored\nsequential images through matching specific scene-descriptors. In urban scenes,\nthe appearance variation caused by seasons and illumination has brought great\nchallenges to the robustness of scene descriptors. Semantic segmentation images\ncan not only deliver the shape information of objects but also their categories\nand spatial relations that will not be affected by the appearance variation of\nthe scene. Innovated by the Vector of Locally Aggregated Descriptor (VLAD), in\nthis paper, we propose a novel image descriptor with aggregated semantic\nskeleton representation (SSR), dubbed SSR-VLAD, for the VPR under drastic\nappearance-variation of environments. The SSR-VLAD of one image aggregates the\nsemantic skeleton features of each category and encodes the spatial-temporal\ndistribution information of the image semantic information. We conduct a series\nof experiments on three public datasets of challenging urban scenes. Compared\nwith four state-of-the-art VPR methods- CoHOG, NetVLAD, LOST-X, and\nRegion-VLAD, VPR by matching SSR-VLAD outperforms those methods and maintains\ncompetitive real-time performance at the same time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiwei_N/0/1/0/all/0/1\">Nie Jiwei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joe_Mei_F/0/1/0/all/0/1\">Feng Joe-Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dingyu_X/0/1/0/all/0/1\">Xue Dingyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_P/0/1/0/all/0/1\">Pan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Liu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_H/0/1/0/all/0/1\">Hu Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_C/0/1/0/all/0/1\">Cheng Shuai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality Metric Guided Portrait Line Drawing Generation from Unpaired Training Data. (arXiv:2202.03678v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03678","description":"<p>Face portrait line drawing is a unique style of art which is highly abstract\nand expressive. However, due to its high semantic constraints, many existing\nmethods learn to generate portrait drawings using paired training data, which\nis costly and time-consuming to obtain. In this paper, we propose a novel\nmethod to automatically transform face photos to portrait drawings using\nunpaired training data with two new features; i.e., our method can (1) learn to\ngenerate high quality portrait drawings in multiple styles using a single\nnetwork and (2) generate portrait drawings in a \"new style\" unseen in the\ntraining data. To achieve these benefits, we (1) propose a novel quality metric\nfor portrait drawings which is learned from human perception, and (2) introduce\na quality loss to guide the network toward generating better looking portrait\ndrawings. We observe that existing unpaired translation methods such as\nCycleGAN tend to embed invisible reconstruction information indiscriminately in\nthe whole drawings due to significant information imbalance between the photo\nand portrait drawing domains, which leads to important facial features missing.\nTo address this problem, we propose a novel asymmetric cycle mapping that\nenforces the reconstruction information to be visible and only embedded in the\nselected facial regions. Along with localized discriminators for important\nfacial regions, our method well preserves all important facial features in the\ngenerated drawings. Generator dissection further explains that our model learns\nto incorporate face semantic information during drawing generation. Extensive\nexperiments including a user study show that our model outperforms\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1\">Ran Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosin_P/0/1/0/all/0/1\">Paul L. Rosin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Inter-Channel Correlation for Diversity-preserved KnowledgeDistillation. (arXiv:2202.03680v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03680","description":"<p>Knowledge Distillation has shown very promising abil-ity in transferring\nlearned representation from the largermodel (teacher) to the smaller one\n(student).Despitemany efforts, prior methods ignore the important role\nofretaining inter-channel correlation of features, leading tothe lack of\ncapturing intrinsic distribution of the featurespace and sufficient diversity\nproperties of features in theteacher network.To solve the issue, we propose\nthenovel Inter-Channel Correlation for Knowledge Distillation(ICKD), with which\nthe diversity and homology of the fea-ture space of the student network can\nalign with that ofthe teacher network. The correlation between these\ntwochannels is interpreted as diversity if they are irrelevantto each other,\notherwise homology. Then the student isrequired to mimic the correlation within\nits own embed-ding space. In addition, we introduce the grid-level\ninter-channel correlation, making it capable of dense predictiontasks.\nExtensive experiments on two vision tasks, includ-ing ImageNet classification\nand Pascal VOC segmentation,demonstrate the superiority of our ICKD, which\nconsis-tently outperforms many existing methods, advancing thestate-of-the-art\nin the fields of Knowledge Distillation. Toour knowledge, we are the first\nmethod based on knowl-edge distillation boosts ResNet18 beyond 72% Top-1\nac-curacy on ImageNet classification. Code is available\nat:https://github.com/ADLab-AutoDrive/ICKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingle Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sihao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Network Comparison Study of Deep Activation Feature Discriminability with Novel Objects. (arXiv:2202.03695v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03695","description":"<p>Feature extraction has always been a critical component of the computer\nvision field. More recently, state-of-the-art computer visions algorithms have\nincorporated Deep Neural Networks (DNN) in feature extracting roles, creating\nDeep Convolutional Activation Features (DeCAF). The transferability of DNN\nknowledge domains has enabled the wide use of pretrained DNN feature extraction\nfor applications with novel object classes, especially those with limited\ntraining data. This study analyzes the general discriminability of novel object\nvisual appearances encoded into the DeCAF space of six of the leading visual\nrecognition DNN architectures. The results of this study characterize the\nMahalanobis distances and cosine similarities between DeCAF object manifolds\nacross two visual object tracking benchmark data sets. The backgrounds\nsurrounding each object are also included as an object classes in the manifold\nanalysis, providing a wider range of novel classes. This study found that\ndifferent network architectures led to different network feature focuses that\nmust to be considered in the network selection process. These results are\ngenerated from the VOT2015 and UAV123 benchmark data sets; however, the\nproposed methods can be applied to efficiently compare estimated network\nperformance characteristics for any labeled visual data set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karnes_M/0/1/0/all/0/1\">Michael Karnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Paced Imbalance Rectification for Class Incremental Learning. (arXiv:2202.03703v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03703","description":"<p>Exemplar-based class-incremental learning is to recognize new classes while\nnot forgetting old ones, whose samples can only be saved in limited memory. The\nratio fluctuation of new samples to old exemplars, which is caused by the\nvariation of memory capacity at different environments, will bring challenges\nto stabilize the incremental optimization process. To address this problem, we\npropose a novel self-paced imbalance rectification scheme, which dynamically\nmaintains the incremental balance during the representation learning phase.\nSpecifically, our proposed scheme consists of a frequency compensation strategy\nthat adjusts the logits margin between old and new classes with the\ncorresponding number ratio to strengthen the expression ability of the old\nclasses, and an inheritance transfer strategy to reduce the representation\nconfusion by estimating the similarity of different classes in the old\nembedding space. Furthermore, a chronological attenuation mechanism is proposed\nto mitigate the repetitive optimization of the older classes at multiple\nstep-wise increments. Extensive experiments on three benchmarks demonstrate\nstable incremental performance, significantly outperforming the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's Cracking? A Review and Analysis of Deep Learning Methods for Structural Crack Segmentation, Detection and Quantification. (arXiv:2202.03714v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03714","description":"<p>Surface cracks are a very common indicator of potential structural faults.\nTheir early detection and monitoring is an important factor in structural\nhealth monitoring. Left untreated, they can grow in size over time and require\nexpensive repairs or maintenance. With recent advances in computer vision and\ndeep learning algorithms, the automatic detection and segmentation of cracks\nfor this monitoring process have become a major topic of interest. This review\naims to give researchers an overview of the published work within the field of\ncrack analysis algorithms that make use of deep learning. It outlines the\nvarious tasks that are solved through applying computer vision algorithms to\nsurface cracks in a structural health monitoring setting and also provides\nin-depth reviews of recent fully, semi and unsupervised approaches that perform\ncrack classification, detection, segmentation and quantification. Additionally,\nthis review also highlights popular datasets used for cracks and the metrics\nthat are used to evaluate the performance of those algorithms. Finally,\npotential research gaps are outlined and further research directions are\nprovided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konig_J/0/1/0/all/0/1\">Jacob K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_M/0/1/0/all/0/1\">Mark Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannion_M/0/1/0/all/0/1\">Mike Mannion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrie_P/0/1/0/all/0/1\">Peter Barrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morison_G/0/1/0/all/0/1\">Gordon Morison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binary Neural Networks as a general-propose compute paradigm for on-device computer vision. (arXiv:2202.03716v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03716","description":"<p>For binary neural networks (BNNs) to become the mainstream on-device computer\nvision algorithm, they must achieve a superior speed-vs-accuracy tradeoff than\n8-bit quantization and establish a similar degree of general applicability in\nvision tasks. To this end, we propose a BNN framework comprising 1) a\nminimalistic inference scheme for hardware-friendliness, 2) an\nover-parameterized training scheme for high accuracy, and 3) a simple procedure\nto adapt to different vision tasks. The resultant framework overtakes 8-bit\nquantization in the speed-vs-accuracy tradeoff for classification, detection,\nsegmentation, super-resolution and matching: our BNNs not only retain the\naccuracy levels of their 8-bit baselines but also showcase 1.3-2.4$\\times$\nfaster FPS on mobile CPUs. Similar conclusions can be drawn for prototypical\nsystolic-array-based AI accelerators, where our BNNs promise 2.8-7$\\times$\nfewer execution cycles than 8-bit and 2.1-2.7$\\times$ fewer cycles than\nalternative BNN designs. These results suggest that the time for large-scale\nBNN adoption could be upon us.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_G/0/1/0/all/0/1\">Guhong Nie</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lirui Xiao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Menglong Zhu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chu_D/0/1/0/all/0/1\">Dongliang Chu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yue Shen</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kang Yang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Li Du</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a> (1) ((1) DJI Innovations Inc, (2) School of Electronic Science and Engineering, Nanjing University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hair Color Digitization through Imaging and Deep Inverse Graphics. (arXiv:2202.03723v1 [cs.GR])","link":"http://arxiv.org/abs/2202.03723","description":"<p>Hair appearance is a complex phenomenon due to hair geometry and how the\nlight bounces on different hair fibers. For this reason, reproducing a specific\nhair color in a rendering environment is a challenging task that requires\nmanual work and expert knowledge in computer graphics to tune the result\nvisually. While current hair capture methods focus on hair shape estimation\nmany applications could benefit from an automated method for capturing the\nappearance of a physical hair sample, from augmented/virtual reality to hair\ndying development. Building on recent advances in inverse graphics and material\ncapture using deep neural networks, we introduce a novel method for hair color\ndigitization. Our proposed pipeline allows capturing the color appearance of a\nphysical hair sample and renders synthetic images of hair with a similar\nappearance, simulating different hair styles and/or lighting environments.\nSince rendering realistic hair images requires path-tracing rendering, the\nconventional inverse graphics approach based on differentiable rendering is\nuntractable. Our method is based on the combination of a controlled imaging\ndevice, a path-tracing renderer, and an inverse graphics model based on\nself-supervised machine learning, which does not require to use differentiable\nrendering to be trained. We illustrate the performance of our hair digitization\nmethod on both real and synthetic images and show that our approach can\naccurately capture and render hair color.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kips_R/0/1/0/all/0/1\">Robin Kips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bokaris_P/0/1/0/all/0/1\">Panagiotis-Alexandros Bokaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perrot_M/0/1/0/all/0/1\">Matthieu Perrot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloch_I/0/1/0/all/0/1\">Isabelle Bloch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigating to Objects in Unseen Environments by Distance Prediction. (arXiv:2202.03735v1 [cs.RO])","link":"http://arxiv.org/abs/2202.03735","description":"<p>Object Goal Navigation (ObjectNav) task is to navigate an agent to an object\ninstance in unseen environments. The traditional navigation paradigm plans the\nshortest path on a pre-built map. Inspired by this, we propose an object goal\nnavigation framework, which could directly perform path planning based on an\nestimated distance map. Specifically, our model takes a birds-eye-view semantic\nmap as input, and estimates the distance from the map cells to the target\nobject based on the learned prior knowledge. With the estimated distance map,\nthe agent could explore the environment and navigate to the target objects\nbased on either human-designed or learned navigation policy. Empirical results\nin visually realistic simulation environments show that the proposed method\noutperforms a wide range of baselines on success rate and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minzhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Binglei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Breast Cancer Screening Techniques: Thermography and Electrical Impedance Tomography. (arXiv:2202.03737v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03737","description":"<p>Breast cancer is a disease that threatens many women's life, thus, early and\naccurate detection plays a key role in reducing the mortality rate. Mammography\nstands as the reference technique for breast cancer screening; nevertheless,\nmany countries still lack access to mammograms due to economic, social, and\ncultural issues. Last advances in computational tools, infrared cameras, and\ndevices for bio-impedance quantification allowed the development of parallel\ntechniques like thermography, infrared imaging, and electrical impedance\ntomography, these being faster, reliable and cheaper. In the last decades,\nthese have been considered as complement procedures for breast cancer\ndiagnosis, where many studies concluded that false positive and false negative\nrates are greatly reduced. This work aims to review the last breakthroughs\nabout the three above-mentioned techniques describing the benefits of mixing\nseveral computational skills to obtain a better global performance. In\naddition, we provide a comparison between several machine learning techniques\napplied to breast cancer diagnosis going from logistic regression, decision\ntrees, and random forest to artificial, deep, and convolutional neural\nnetworks. Finally, it is mentioned several recommendations for 3D breast\nsimulations, pre-processing techniques, biomedical devices in the research\nfield, prediction of tumor location and size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zerhouni_N/0/1/0/all/0/1\">N. Zerhouni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masry_Z/0/1/0/all/0/1\">Z. Al Masry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Devalland_C/0/1/0/all/0/1\">C. Devalland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varnier_C/0/1/0/all/0/1\">C. Varnier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistency-Regularized Region-Growing Network for Semantic Segmentation of Urban Scenes with Point-Level Annotations. (arXiv:2202.03740v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03740","description":"<p>Deep learning algorithms have obtained great success in semantic segmentation\nof very high-resolution (VHR) images. Nevertheless, training these models\ngenerally requires a large amount of accurate pixel-wise annotations, which is\nvery laborious and time-consuming to collect. To reduce the annotation burden,\nthis paper proposes a consistency-regularized region-growing network (CRGNet)\nto achieve semantic segmentation of VHR images with point-level annotations.\nThe key idea of CRGNet is to iteratively select unlabeled pixels with high\nconfidence to expand the annotated area from the original sparse points.\nHowever, since there may exist some errors and noises in the expanded\nannotations, directly learning from them may mislead the training of the\nnetwork. To this end, we further propose the consistency regularization\nstrategy, where a base classifier and an expanded classifier are employed.\nSpecifically, the base classifier is supervised by the original sparse\nannotations, while the expanded classifier aims to learn from the expanded\nannotations generated by the base classifier with the region-growing mechanism.\nThe consistency regularization is thereby achieved by minimizing the\ndiscrepancy between the predictions from both the base and the expanded\nclassifiers. We find such a simple regularization strategy is yet very useful\nto control the quality of the region-growing mechanism. Extensive experiments\non two benchmark datasets demonstrate that the proposed CRGNet significantly\noutperforms the existing state-of-the-art methods. Codes and pre-trained models\nwill be available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yonghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1\">Pedram Ghamisi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STC: Spatio-Temporal Contrastive Learning for Video Instance Segmentation. (arXiv:2202.03747v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03747","description":"<p>Video Instance Segmentation (VIS) is a task that simultaneously requires\nclassification, segmentation, and instance association in a video. Recent VIS\napproaches rely on sophisticated pipelines to achieve this goal, including\nRoI-related operations or 3D convolutions. In contrast, we present a simple and\nefficient single-stage VIS framework based on the instance segmentation method\nCondInst by adding an extra tracking head. To improve instance association\naccuracy, a novel bi-directional spatio-temporal contrastive learning strategy\nfor tracking embedding across frames is proposed. Moreover, an instance-wise\ntemporal consistency scheme is utilized to produce temporally coherent results.\nExperiments conducted on the YouTube-VIS-2019, YouTube-VIS-2021, and OVIS-2021\ndatasets validate the effectiveness and efficiency of the proposed method. We\nhope the proposed framework can serve as a simple and strong alternative for\nmany other instance-level video association tasks. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengkai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhangxuan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jinlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Addressing Data Scarcity in Multimodal User State Recognition by Combining Semi-Supervised and Supervised Learning. (arXiv:2202.03775v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03775","description":"<p>Detecting mental states of human users is crucial for the development of\ncooperative and intelligent robots, as it enables the robot to understand the\nuser's intentions and desires. Despite their importance, it is difficult to\nobtain a large amount of high quality data for training automatic recognition\nalgorithms as the time and effort required to collect and label such data is\nprohibitively high. In this paper we present a multimodal machine learning\napproach for detecting dis-/agreement and confusion states in a human-robot\ninteraction environment, using just a small amount of manually annotated data.\nWe collect a data set by conducting a human-robot interaction study and develop\na novel preprocessing pipeline for our machine learning approach. By combining\nsemi-supervised and supervised architectures, we are able to achieve an average\nF1-score of 81.1\\% for dis-/agreement detection with a small amount of labeled\ndata and a large unlabeled data set, while simultaneously increasing the\nrobustness of the model compared to the supervised approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voss_H/0/1/0/all/0/1\">Hendric Vo&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wersing_H/0/1/0/all/0/1\">Heiko Wersing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_S/0/1/0/all/0/1\">Stefan Kopp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCR: Smooth Contour Regression with Geometric Priors. (arXiv:2202.03784v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03784","description":"<p>While object detection methods traditionally make use of pixel-level masks or\nbounding boxes, alternative representations such as polygons or active contours\nhave recently emerged. Among them, methods based on the regression of Fourier\nor Chebyshev coefficients have shown high potential on freeform objects. By\ndefining object shapes as polar functions, they are however limited to\nstar-shaped domains. We address this issue with SCR: a method that captures\nresolution-free object contours as complex periodic functions. The method\noffers a good compromise between accuracy and compactness thanks to the design\nof efficient geometric shape priors. We benchmark SCR on the popular COCO 2017\ninstance segmentation dataset, and show its competitiveness against existing\nalgorithms in the field. In addition, we design a compact version of our\nnetwork, which we benchmark on embedded hardware with a wide range of power\ntargets, achieving up to real-time performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahl_G/0/1/0/all/0/1\">Gaetan Bahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniel_L/0/1/0/all/0/1\">Lionel Daniel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lafarge_F/0/1/0/all/0/1\">Florent Lafarge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space. (arXiv:2202.03800v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03800","description":"<p>Face clustering has attracted rising research interest recently to take\nadvantage of massive amounts of face images on the web. State-of-the-art\nperformance has been achieved by Graph Convolutional Networks (GCN) due to\ntheir powerful representation capacity. However, existing GCN-based methods\nbuild face graphs mainly according to kNN relations in the feature space, which\nmay lead to a lot of noise edges connecting two faces of different classes. The\nface features will be polluted when messages pass along these noise edges, thus\ndegrading the performance of GCNs. In this paper, a novel algorithm named\nAda-NETS is proposed to cluster faces by constructing clean graphs for GCNs. In\nAda-NETS, each face is transformed to a new structure space, obtaining robust\nfeatures by considering face features of the neighbour images. Then, an\nadaptive neighbour discovery strategy is proposed to determine a proper number\nof edges connecting to each face image. It significantly reduces the noise\nedges while maintaining the good ones to build a graph with clean yet rich\nedges for GCNs to cluster faces. Experiments on multiple public clustering\ndatasets show that Ada-NETS significantly outperforms current state-of-the-art\nmethods, proving its superiority and generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaobin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Senzhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">YuQi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Plug-in Module for Fine-Grained Visual Classification. (arXiv:2202.03822v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03822","description":"<p>Visual classification can be divided into coarse-grained and fine-grained\nclassification. Coarse-grained classification represents categories with a\nlarge degree of dissimilarity, such as the classification of cats and dogs,\nwhile fine-grained classification represents classifications with a large\ndegree of similarity, such as cat species, bird species, and the makes or\nmodels of vehicles. Unlike coarse-grained visual classification, fine-grained\nvisual classification often requires professional experts to label data, which\nmakes data more expensive. To meet this challenge, many approaches propose to\nautomatically find the most discriminative regions and use local features to\nprovide more precise features. These approaches only require image-level\nannotations, thereby reducing the cost of annotation. However, most of these\nmethods require two- or multi-stage architectures and cannot be trained\nend-to-end. Therefore, we propose a novel plug-in module that can be integrated\nto many common backbones, including CNN-based or Transformer-based networks to\nprovide strongly discriminative regions. The plugin module can output\npixel-level feature maps and fuse filtered features to enhance fine-grained\nvisual classification. Experimental results show that the proposed plugin\nmodule outperforms state-of-the-art approaches and significantly improves the\naccuracy to 92.77\\% and 92.83\\% on CUB200-2011 and NABirds, respectively. We\nhave released our source code in Github\nhttps://github.com/chou141253/FGVC-PIM.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chou_P/0/1/0/all/0/1\">Po-Yung Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Cheng-Hung Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_W/0/1/0/all/0/1\">Wen-Chung Kao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Pitfalls of Using the Residual Error as Anomaly Score. (arXiv:2202.03826v1 [eess.IV])","link":"http://arxiv.org/abs/2202.03826","description":"<p>Many current state-of-the-art methods for anomaly localization in medical\nimages rely on calculating a residual image between a potentially anomalous\ninput image and its \"healthy\" reconstruction. As the reconstruction of the\nunseen anomalous region should be erroneous, this yields large residuals as a\nscore to detect anomalies in medical images. However, this assumption does not\ntake into account residuals resulting from imperfect reconstructions of the\nmachine learning models used. Such errors can easily overshadow residuals of\ninterest and therefore strongly question the use of residual images as scoring\nfunction. Our work explores this fundamental problem of residual images in\ndetail. We theoretically define the problem and thoroughly evaluate the\ninfluence of intensity and texture of anomalies against the effect of imperfect\nreconstructions in a series of experiments. Code and experiments are available\nunder https://github.com/FeliMe/residual-score-pitfalls\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meissen_F/0/1/0/all/0/1\">Felix Meissen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wiestler_B/0/1/0/all/0/1\">Benedikt Wiestler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Multi-Task Learning Framework of Real-Time Drone Supervision for Crowd Counting. (arXiv:2202.03843v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03843","description":"<p>In this paper, a novel Unified Multi-Task Learning Framework of Real-Time\nDrone Supervision for Crowd Counting (MFCC) is proposed, which utilizes an\nimage fusion network architecture to fuse images from the visible and thermal\ninfrared image, and a crowd counting network architecture to estimate the\ndensity map. The purpose of our framework is to fuse two modalities, including\nvisible and thermal infrared images captured by drones in real-time, that\nexploit the complementary information to accurately count the dense population\nand then automatically guide the flight of the drone to supervise the dense\ncrowd. To this end, we propose the unified multi-task learning framework for\ncrowd counting for the first time and re-design the unified training loss\nfunctions to align the image fusion network and crowd counting network. We also\ndesign the Assisted Learning Module (ALM) to fuse the density map feature to\nthe image fusion encoder process for learning the counting features. To improve\nthe accuracy, we propose the Extensive Context Extraction Module (ECEM) that is\nbased on a dense connection architecture to encode multi-receptive-fields\ncontextual information and apply the Multi-domain Attention Block (MAB) for\nconcerning the head region in the drone view. Finally, we apply the prediction\nmap to automatically guide the drones to supervise the dense crowd. The\nexperimental results on the DroneRGBT dataset show that, compared with the\nexisting methods, ours has comparable results on objective evaluations and an\neasier training process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Siqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhichao Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class Density and Dataset Quality in High-Dimensional, Unstructured Data. (arXiv:2202.03856v1 [cs.LG])","link":"http://arxiv.org/abs/2202.03856","description":"<p>We provide a definition for class density that can be used to measure the\naggregate similarity of the samples within each of the classes in a\nhigh-dimensional, unstructured dataset. We then put forth several candidate\nmethods for calculating class density and analyze the correlation between the\nvalues each method produces with the corresponding individual class test\naccuracies achieved on a trained model. Additionally, we propose a definition\nfor dataset quality for high-dimensional, unstructured data and show that those\ndatasets that met a certain quality threshold (experimentally demonstrated to\nbe &gt; 10 for the datasets studied) were candidates for eliding redundant data\nbased on the individual class densities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Byerly_A/0/1/0/all/0/1\">Adam Byerly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalganova_T/0/1/0/all/0/1\">Tatiana Kalganova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Optical Flow with Adaptive Graph Reasoning. (arXiv:2202.03857v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03857","description":"<p>Estimating per-pixel motion between video frames, known as optical flow, is a\nlong-standing problem in video understanding and analysis. Most contemporary\noptical flow techniques largely focus on addressing the cross-image matching\nwith feature similarity, with few methods considering how to explicitly reason\nover the given scene for achieving a holistic motion understanding. In this\nwork, taking a fresh perspective, we introduce a novel graph-based approach,\ncalled adaptive graph reasoning for optical flow (AGFlow), to emphasize the\nvalue of scene/context information in optical flow. Our key idea is to decouple\nthe context reasoning from the matching procedure, and exploit scene\ninformation to effectively assist motion estimation by learning to reason over\nthe adaptive graph. The proposed AGFlow can effectively exploit the context\ninformation and incorporate it within the matching procedure, producing more\nrobust and accurate results. On both Sintel clean and final passes, our AGFlow\nachieves the best accuracy with EPE of 1.43 and 2.47 pixels, outperforming\nstate-of-the-art approaches by 11.2% and 13.6%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_A/0/1/0/all/0/1\">Ao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1\">Kunming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping DNN Embedding Manifolds for Network Generalization Prediction. (arXiv:2202.03868v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03868","description":"<p>Understanding Deep Neural Network (DNN) performance in changing conditions is\nessential for deploying DNNs in safety critical applications with unconstrained\nenvironments, e.g., perception for self-driving vehicles or medical image\nanalysis. Recently, the task of Network Generalization Prediction (NGP) has\nbeen proposed to predict how a DNN will generalize in a new operating domain.\nPrevious NGP approaches have relied on labeled metadata and known distributions\nfor the new operating domains. In this study, we propose the first NGP approach\nthat predicts DNN performance based solely on how unlabeled images from an\nexternal operating domain map in the DNN embedding space. We demonstrate this\ntechnique for pedestrian, melanoma, and animal classification tasks and show\nstate of the art NGP in 13 of 15 NGP tasks without requiring domain knowledge.\nAdditionally, we show that our NGP embedding maps can be used to identify\nmisclassified images when the DNN performance is poor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+OBrien_M/0/1/0/all/0/1\">Molly O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukowski_J/0/1/0/all/0/1\">Julia Bukowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pezeshk_A/0/1/0/all/0/1\">Aria Pezeshk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Greg Hager</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BIQ2021: A Large-Scale Blind Image Quality Assessment Database. (arXiv:2202.03879v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03879","description":"<p>The assessment of the perceptual quality of digital images is becoming\nincreasingly important as a result of the widespread use of digital multimedia\ndevices. Smartphones and high-speed internet are just two examples of\ntechnologies that have multiplied the amount of multimedia content available.\nThus, obtaining a representative dataset, which is required for objective\nquality assessment training, is a significant challenge. The Blind Image\nQuality Assessment Database, BIQ2021, is presented in this article. By\nselecting images with naturally occurring distortions and reliable labeling,\nthe dataset addresses the challenge of obtaining representative images for\nno-reference image quality assessment. The dataset consists of three sets of\nimages: those taken without the intention of using them for image quality\nassessment, those taken with intentionally introduced natural distortions, and\nthose taken from an open-source image-sharing platform. It is attempted to\nmaintain a diverse collection of images from various devices, containing a\nvariety of different types of objects and varying degrees of foreground and\nbackground information. To obtain reliable scores, these images are\nsubjectively scored in a laboratory environment using a single stimulus method.\nThe database contains information about subjective scoring, human subject\nstatistics, and the standard deviation of each image. The dataset's Mean\nOpinion Scores (MOS) make it useful for assessing visual quality. Additionally,\nthe proposed database is used to evaluate existing blind image quality\nassessment approaches, and the scores are analyzed using Pearson and Spearman's\ncorrelation coefficients. The image database and MOS are freely available for\nuse and benchmarking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1\">Nisar Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_S/0/1/0/all/0/1\">Shahzad Asif</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLPU: A Geometric Approach For Lidar Pointcloud Upsampling. (arXiv:2202.03901v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03901","description":"<p>In autonomous driving, lidar is inherent for the understanding of the 3D\nenvironment. Lidar sensors vary in vertical resolutions, where a denser\npointcloud depicts a more detailed environment, albeit at a significantly\nhigher cost. Pointcloud upsampling predicts high-resolution pointclouds from\nsparser ones to bridge this performance gap at a lower cost. Although many\nupsampling frameworks have achieved a robust performance, a fair comparison is\ndifficult as they were tested on different datasets and metrics. In this work,\nwe first conduct a consistent comparative study to benchmark the existing\nalgorithms on the KITTI dataset. Then, we observe that there are three common\nfactors that hinder the performance: an inefficient data representation, a\nsmall receptive field, and low-frequency losses. By leveraging the scene\ngeometry, a new self-supervised geometric lidar pointcloud upsampling (GLPU)\nframework is proposed to address the aforementioned limitations. Our\nexperiments demonstrate the effectiveness and superior performance of GLPU\ncompared to other techniques on the KITTI benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eskandar_G/0/1/0/all/0/1\">George Eskandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palaniswamy_J/0/1/0/all/0/1\">Janaranjani Palaniswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guirguis_K/0/1/0/all/0/1\">Karim Guirguis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somashekar_B/0/1/0/all/0/1\">Barath Somashekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge-based fever screening system over private 5G. (arXiv:2202.03917v1 [eess.SP])","link":"http://arxiv.org/abs/2202.03917","description":"<p>Edge computing and 5G have made it possible to perform analytics closer to\nthe source of data and achieve super-low latency response times, which is not\npossible with centralized cloud deployment. In this paper, we present a novel\nfever-screening system, which uses edge machine learning techniques and\nleverages private 5G to accurately identify and screen individuals with fever\nin real-time. Particularly, we present deep-learning based novel techniques for\nfusion and alignment of cross-spectral visual and thermal data streams at the\nedge. Our novel Cross-Spectral Generative Adversarial Network (CS-GAN)\nsynthesizes visual images that have the key, representative object level\nfeatures required to uniquely associate objects across visual and thermal\nspectrum. Two key features of CS-GAN are a novel, feature-preserving loss\nfunction that results in high-quality pairing of corresponding cross-spectral\nobjects, and dual bottleneck residual layers with skip connections (a new,\nnetwork enhancement) to not only accelerate real-time inference, but to also\nspeed up convergence during model training at the edge. To the best of our\nknowledge, this is the first technique that leverages 5G networks and limited\nedge resources to enable real-time feature-level association of objects in\nvisual and thermal streams (30 ms per full HD frame on an Intel Core i7-8650\n4-core, 1.9GHz mobile processor). To the best of our knowledge, this is also\nthe first system to achieve real-time operation, which has enabled fever\nscreening of employees and guests in arenas, theme parks, airports and other\ncritical facilities. By leveraging edge computing and 5G, our fever screening\nsystem is able to achieve 98.5% accuracy and is able to process about 5X more\npeople when compared to a centralized cloud deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sankaradas_M/0/1/0/all/0/1\">Murugan Sankaradas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rao_K/0/1/0/all/0/1\">Kunal Rao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajendran_R/0/1/0/all/0/1\">Ravi Rajendran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Redkar_A/0/1/0/all/0/1\">Amit Redkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakradhar_S/0/1/0/all/0/1\">Srimat Chakradhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"If a Human Can See It, So Should Your System: Reliability Requirements for Machine Vision Components. (arXiv:2202.03930v1 [cs.SE])","link":"http://arxiv.org/abs/2202.03930","description":"<p>Machine Vision Components (MVC) are becoming safety-critical. Assuring their\nquality, including safety, is essential for their successful deployment.\nAssurance relies on the availability of precisely specified and, ideally,\nmachine-verifiable requirements. MVCs with state-of-the-art performance rely on\nmachine learning (ML) and training data but largely lack such requirements.\n</p>\n<p>In this paper, we address the need for defining machine-verifiable\nreliability requirements for MVCs against transformations that simulate the\nfull range of realistic and safety-critical changes in the environment. Using\nhuman performance as a baseline, we define reliability requirements as: 'if the\nchanges in an image do not affect a human's decision, neither should they\naffect the MVC's.' To this end, we provide: (1) a class of safety-related image\ntransformations; (2) reliability requirement classes to specify\ncorrectness-preservation and prediction-preservation for MVCs; (3) a method to\ninstantiate machine-verifiable requirements from these requirements classes\nusing human performance experiment data; (4) human performance experiment data\nfor image recognition involving eight commonly used transformations, from about\n2000 human participants; and (5) a method for automatically checking whether an\nMVC satisfies our requirements. Further, we show that our reliability\nrequirements are feasible and reusable by evaluating our methods on 13\nstate-of-the-art pre-trained image classification models. Finally, we\ndemonstrate that our approach detects reliability gaps in MVCs that other\nexisting methods are unable to detect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Boyue Caroline Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marsso_L/0/1/0/all/0/1\">Lina Marsso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czarnecki_K/0/1/0/all/0/1\">Krzysztof Czarnecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salay_R/0/1/0/all/0/1\">Rick Salay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huakun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_M/0/1/0/all/0/1\">Marsha Chechik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social-DualCVAE: Multimodal Trajectory Forecasting Based on Social Interactions Pattern Aware and Dual Conditional Variational Auto-Encoder. (arXiv:2202.03954v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03954","description":"<p>Pedestrian trajectory forecasting is a fundamental task in multiple utility\nareas, such as self-driving, autonomous robots, and surveillance systems. The\nfuture trajectory forecasting is multi-modal, influenced by physical\ninteraction with scene contexts and intricate social interactions among\npedestrians. The mainly existing literature learns representations of social\ninteractions by deep learning networks, while the explicit interaction patterns\nare not utilized. Different interaction patterns, such as following or\ncollision avoiding, will generate different trends of next movement, thus, the\nawareness of social interaction patterns is important for trajectory\nforecasting. Moreover, the social interaction patterns are privacy concerned or\nlack of labels. To jointly address the above issues, we present a social-dual\nconditional variational auto-encoder (Social-DualCVAE) for multi-modal\ntrajectory forecasting, which is based on a generative model conditioned not\nonly on the past trajectories but also the unsupervised classification of\ninteraction patterns. After generating the category distribution of the\nunlabeled social interaction patterns, DualCVAE, conditioned on the past\ntrajectories and social interaction pattern, is proposed for multi-modal\ntrajectory prediction by latent variables estimating. A variational bound is\nderived as the minimization objective during training. The proposed model is\nevaluated on widely used trajectory benchmarks and outperforms the prior\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiashi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xinming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">James J.Q. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bingham Policy Parameterization for 3D Rotations in Reinforcement Learning. (arXiv:2202.03957v1 [cs.RO])","link":"http://arxiv.org/abs/2202.03957","description":"<p>We propose a new policy parameterization for representing 3D rotations during\nreinforcement learning. Today in the continuous control reinforcement learning\nliterature, many stochastic policy parameterizations are Gaussian. We argue\nthat universally applying a Gaussian policy parameterization is not always\ndesirable for all environments. One such case in particular where this is true\nare tasks that involve predicting a 3D rotation output, either in isolation, or\ncoupled with translation as part of a full 6D pose output. Our proposed Bingham\nPolicy Parameterization (BPP) models the Bingham distribution and allows for\nbetter rotation (quaternion) prediction over a Gaussian policy parameterization\nin a range of reinforcement learning tasks. We evaluate BPP on the rotation\nWahba problem task, as well as a set of vision-based next-best pose robot\nmanipulation tasks from RLBench. We hope that this paper encourages more\nresearch into developing other policy parameterization that are more suited for\nparticular environments, rather than always assuming Gaussian.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Modeling for Out-of-Distribution Generalization. (arXiv:2202.03958v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03958","description":"<p>Though remarkable progress has been achieved in various vision tasks, deep\nneural networks still suffer obvious performance degradation when tested in\nout-of-distribution scenarios. We argue that the feature statistics (mean and\nstandard deviation), which carry the domain characteristics of the training\ndata, can be properly manipulated to improve the generalization ability of deep\nlearning models. Common methods often consider the feature statistics as\ndeterministic values measured from the learned features and do not explicitly\nconsider the uncertain statistics discrepancy caused by potential domain shifts\nduring testing. In this paper, we improve the network generalization ability by\nmodeling the uncertainty of domain shifts with synthesized feature statistics\nduring training. Specifically, we hypothesize that the feature statistic, after\nconsidering the potential uncertainties, follows a multivariate Gaussian\ndistribution. Hence, each feature statistic is no longer a deterministic value,\nbut a probabilistic point with diverse distribution possibilities. With the\nuncertain feature statistics, the models can be trained to alleviate the domain\nperturbations and achieve better robustness against potential domain shifts.\nOur method can be readily integrated into networks without additional\nparameters. Extensive experiments demonstrate that our proposed method\nconsistently improves the network generalization ability on multiple vision\ntasks, including image classification, semantic segmentation, and instance\nretrieval. The code will be released soon at\nhttps://github.com/lixiaotong97/DSU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaotong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yongxing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Contrastive Learning for Cross-domain Hyperspectral Image Representation. (arXiv:2202.03968v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03968","description":"<p>Recently, self-supervised learning has attracted attention due to its\nremarkable ability to acquire meaningful representations for classification\ntasks without using semantic labels. This paper introduces a self-supervised\nlearning framework suitable for hyperspectral images that are inherently\nchallenging to annotate. The proposed framework architecture leverages\ncross-domain CNN, allowing for learning representations from different\nhyperspectral images with varying spectral characteristics and no pixel-level\nannotation. In the framework, cross-domain representations are learned via\ncontrastive learning where neighboring spectral vectors in the same image are\nclustered together in a common representation space encompassing multiple\nhyperspectral images. In contrast, spectral vectors in different hyperspectral\nimages are separated into distinct clusters in the space. To verify that the\nlearned representation through contrastive learning is effectively transferred\ninto a downstream task, we perform a classification task on hyperspectral\nimages. The experimental results demonstrate the advantage of the proposed\nself-supervised representation over models trained from scratch or other\ntransfer learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungtae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Heesung Kwon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation by Test-Time Optimization (TTO) for CBCT-based Adaptive Radiation Therapy. (arXiv:2202.03978v1 [cs.CV])","link":"http://arxiv.org/abs/2202.03978","description":"<p>Online adaptive radiotherapy (ART) requires accurate and efficient\nauto-segmentation of target volumes and organs-at-risk (OARs) in mostly\ncone-beam computed tomography (CBCT) images. Propagating expert-drawn contours\nfrom the pre-treatment planning CT (pCT) through traditional or deep learning\n(DL) based deformable image registration (DIR) can achieve improved results in\nmany situations. Typical DL-based DIR models are population based, that is,\ntrained with a dataset for a population of patients, so they may be affected by\nthe generalizability problem. In this paper, we propose a method called\ntest-time optimization (TTO) to refine a pre-trained DL-based DIR population\nmodel, first for each individual test patient, and then progressively for each\nfraction of online ART treatment. Our proposed method is less susceptible to\nthe generalizability problem, and thus can improve overall performance of\ndifferent DL-based DIR models by improving model accuracy, especially for\noutliers. Our experiments used data from 239 patients with head and neck\nsquamous cell carcinoma to test the proposed method. Firstly, we trained a\npopulation model with 200 patients, and then applied TTO to the remaining 39\ntest patients by refining the trained population model to obtain 39\nindividualized models. We compared each of the individualized models with the\npopulation model in terms of segmentation accuracy. The number of patients with\nat least 0.05 DSC improvement or 2 mm HD95 improvement by TTO averaged over the\n17 selected structures for the state-of-the-art architecture Voxelmorph is 10\nout of 39 test patients. The average time for deriving the individualized model\nusing TTO from the pre-trained population model is approximately four minutes.\nWhen adapting the individualized model to a later fraction of the same patient,\nthe average time is reduced to about one minute and the accuracy is slightly\nimproved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_J/0/1/0/all/0/1\">Jaehee Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_H/0/1/0/all/0/1\">Howard Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Ti Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Justin C. Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Steve Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equivariance versus Augmentation for Spherical Images. (arXiv:2202.03990v1 [cs.LG])","link":"http://arxiv.org/abs/2202.03990","description":"<p>We analyze the role of rotational equivariance in convolutional neural\nnetworks (CNNs) applied to spherical images. We compare the performance of the\ngroup equivariant networks known as S2CNNs and standard non-equivariant CNNs\ntrained with an increasing amount of data augmentation. The chosen\narchitectures can be considered baseline references for the respective design\nparadigms. Our models are trained and evaluated on single or multiple items\nfrom the MNIST or FashionMNIST dataset projected onto the sphere. For the task\nof image classification, which is inherently rotationally invariant, we find\nthat by considerably increasing the amount of data augmentation and the size of\nthe networks, it is possible for the standard CNNs to reach at least the same\nperformance as the equivariant network. In contrast, for the inherently\nequivariant task of semantic segmentation, the non-equivariant networks are\nconsistently outperformed by the equivariant networks with significantly fewer\nparameters. We also analyze and compare the inference latency and training\ntimes of the different networks, enabling detailed tradeoff considerations\nbetween equivariant architectures and data augmentation for practical problems.\nThe equivariant spherical networks used in the experiments will be made\navailable at https://github.com/JanEGerken/sem_seg_s2cnn .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gerken_J/0/1/0/all/0/1\">Jan E. Gerken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlsson_O/0/1/0/all/0/1\">Oscar Carlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linander_H/0/1/0/all/0/1\">Hampus Linander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohlsson_F/0/1/0/all/0/1\">Fredrik Ohlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_C/0/1/0/all/0/1\">Christoffer Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Persson_D/0/1/0/all/0/1\">Daniel Persson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Results and findings of the 2021 Image Similarity Challenge. (arXiv:2202.04007v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04007","description":"<p>The 2021 Image Similarity Challenge introduced a dataset to serve as a new\nbenchmark to evaluate recent image copy detection methods. There were 200\nparticipants to the competition. This paper presents a quantitative and\nqualitative analysis of the top submissions. It appears that the most difficult\nimage transformations involve either severe image crops or hiding into\nunrelated images, combined with local pixel perturbations. The key algorithmic\nelements in the winning submissions are: training on strong augmentations,\nself-supervised learning, score normalization, explicit overlay detection, and\nglobal descriptor matching followed by pairwise image comparison.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papakipos_Z/0/1/0/all/0/1\">Zo&#xeb; Papakipos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1\">Giorgos Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenicek_T/0/1/0/all/0/1\">Tomas Jenicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizzi_E/0/1/0/all/0/1\">Ed Pizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoo_S/0/1/0/all/0/1\">Shuhei Yokoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weipu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Addicam_S/0/1/0/all/0/1\">Sanjay Addicam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_S/0/1/0/all/0/1\">Sergio Manuel Papadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chum_O/0/1/0/all/0/1\">Ondrej Chum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NEWSKVQA: Knowledge-Aware News Video Question Answering. (arXiv:2202.04015v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04015","description":"<p>Answering questions in the context of videos can be helpful in video\nindexing, video retrieval systems, video summarization, learning management\nsystems and surveillance video analysis. Although there exists a large body of\nwork on visual question answering, work on video question answering (1) is\nlimited to domains like movies, TV shows, gameplay, or human activity, and (2)\nis mostly based on common sense reasoning. In this paper, we explore a new\nfrontier in video question answering: answering knowledge-based questions in\nthe context of news videos. To this end, we curate a new dataset of 12K news\nvideos spanning across 156 hours with 1M multiple-choice question-answer pairs\ncovering 8263 unique entities. We make the dataset publicly available. Using\nthis dataset, we propose a novel approach, NEWSKVQA (Knowledge-Aware News Video\nQuestion Answering) which performs multi-modal inferencing over textual\nmultiple-choice questions, videos, their transcripts and knowledge base, and\npresents a strong baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pranay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Contrastive Learning for Volcanic Unrest Detection. (arXiv:2202.04030v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04030","description":"<p>Ground deformation measured from Interferometric Synthetic Aperture Radar\n(InSAR) data is considered a sign of volcanic unrest, statistically linked to a\nvolcanic eruption. Recent studies have shown the potential of using Sentinel-1\nInSAR data and supervised deep learning (DL) methods for the detection of\nvolcanic deformation signals, towards global volcanic hazard mitigation.\nHowever, detection accuracy is compromised from the lack of labelled data and\nclass imbalance. To overcome this, synthetic data are typically used for\nfinetuning DL models pre-trained on the ImageNet dataset. This approach suffers\nfrom poor generalisation on real InSAR data. This letter proposes the use of\nself-supervised contrastive learning to learn quality visual representations\nhidden in unlabeled InSAR data. Our approach, based on the SimCLR framework,\nprovides a solution that does not require a specialized architecture nor a\nlarge labelled or synthetic dataset. We show that our self-supervised pipeline\nachieves higher accuracy with respect to the state-of-the-art methods, and\nshows excellent generalisation even for out-of-distribution test data. Finally,\nwe showcase the effectiveness of our approach for detecting the unrest episodes\npreceding the recent Icelandic Fagradalsfjall volcanic eruption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bountos_N/0/1/0/all/0/1\">Nikolaos Ioannis Bountos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1\">Ioannis Papoutsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michail_D/0/1/0/all/0/1\">Dimitrios Michail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Aligned: Gradient Optimization for Non-Negative Image Synthesis. (arXiv:2202.04036v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04036","description":"<p>In this work, we address an important problem of optical see through (OST)\naugmented reality: non-negative image synthesis. Most of the image generation\nmethods fail under this condition, since they assume full control over each\npixel and cannot create darker pixels by adding light. In order to solve the\nnon-negative image generation problem in AR image synthesis, prior works have\nattempted to utilize optical illusion to simulate human vision but fail to\npreserve lightness constancy well under situations such as high dynamic range.\nIn our paper, we instead propose a method that is able to preserve lightness\nconstancy at a local level, thus capturing high frequency details. Compared\nwith existing work, our method shows strong performance in image-to-image\ntranslation tasks, particularly in scenarios such as large scale images, high\nresolution images, and high dynamic range image transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Flora Yu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1\">Katie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guandao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haraldsson_H/0/1/0/all/0/1\">Harald Haraldsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Conditioned Generative Adversarial Networks for Image Editing. (arXiv:2202.04040v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04040","description":"<p>Generative Adversarial Networks (GANs) are susceptible to bias, learned from\neither the unbalanced data, or through mode collapse. The networks focus on the\ncore of the data distribution, leaving the tails - or the edges of the\ndistribution - behind. We argue that this bias is responsible not only for\nfairness concerns, but that it plays a key role in the collapse of\nlatent-traversal editing methods when deviating away from the distribution's\ncore. Building on this observation, we outline a method for mitigating\ngenerative bias through a self-conditioning process, where distances in the\nlatent-space of a pre-trained generator are used to provide initial labels for\nthe data. By fine-tuning the generator on a re-sampled distribution drawn from\nthese self-labeled data, we force the generator to better contend with rare\nsemantic attributes and enable more realistic generation of these properties.\nWe compare our models to a wide range of latent editing methods, and show that\nby alleviating the bias they achieve finer semantic control and better identity\npreservation through a wider range of transformations. Our code and models will\nbe available at https://github.com/yzliu567/sc-gan\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunzhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1\">Rinon Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit H. Bermano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoquan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decision boundaries and convex hulls in the feature space that deep learning functions learn from images. (arXiv:2202.04052v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04052","description":"<p>The success of deep neural networks in image classification and learning can\nbe partly attributed to the features they extract from images. It is often\nspeculated about the properties of a low-dimensional manifold that models\nextract and learn from images. However, there is not sufficient understanding\nabout this low-dimensional space based on theory or empirical evidence. For\nimage classification models, their last hidden layer is the one where images of\neach class is separated from other classes and it also has the least number of\nfeatures. Here, we develop methods and formulations to study that feature space\nfor any model. We study the partitioning of the domain in feature space,\nidentify regions guaranteed to have certain classifications, and investigate\nits implications for the pixel space. We observe that geometric arrangements of\ndecision boundaries in feature space is significantly different compared to\npixel space, providing insights about adversarial vulnerabilities, image\nmorphing, extrapolation, ambiguity in classification, and the mathematical\nunderstanding of image classification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yousefzadeh_R/0/1/0/all/0/1\">Roozbeh Yousefzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers. (arXiv:2202.04053v1 [cs.CV])","link":"http://arxiv.org/abs/2202.04053","description":"<p>Generating images from textual descriptions has gained a lot of attention.\nRecently, DALL-E, a multimodal transformer language model, and its variants\nhave shown high-quality text-to-image generation capabilities with a simple\narchitecture and training objective, powered by large-scale training data and\ncomputation. However, despite the interesting image generation results, there\nhas not been a detailed analysis on how to evaluate such models. In this work,\nwe investigate the reasoning capabilities and social biases of such\ntext-to-image generative transformers in detail. First, we measure four visual\nreasoning skills: object recognition, object counting, color recognition, and\nspatial relation understanding. For this, we propose PaintSkills, a diagnostic\ndataset and evaluation toolkit that measures these four visual reasoning\nskills. Second, we measure the text alignment and quality of the generated\nimages based on pretrained image captioning, image-text retrieval, and image\nclassification models. Third, we assess social biases in the models. For this,\nwe suggest evaluation of gender and racial biases of text-to-image generation\nmodels based on a pretrained image-text retrieval model and human evaluation.\nIn our experiments, we show that recent text-to-image models perform better in\nrecognizing and counting objects than recognizing colors and understanding\nspatial relations, while there exists a large gap between model performances\nand oracle accuracy on all skills. Next, we demonstrate that recent\ntext-to-image models learn specific gender/racial biases from web image-text\npairs. We also show that our automatic evaluations of visual reasoning skills\nand gender bias are highly correlated with human judgments. We hope our work\nwill help guide future progress in improving text-to-image models on visual\nreasoning skills and social biases. Code and data at:\nhttps://github.com/j-min/DallEval\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zala_A/0/1/0/all/0/1\">Abhay Zala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexibly Regularized Mixture Models and Application to Image Segmentation. (arXiv:1905.10629v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1905.10629","description":"<p>Probabilistic finite mixture models are widely used for unsupervised\nclustering. These models can often be improved by adapting them to the topology\nof the data. For instance, in order to classify spatially adjacent data points\nsimilarly, it is common to introduce a Laplacian constraint on the posterior\nprobability that each data point belongs to a class. Alternatively, the mixing\nprobabilities can be treated as free parameters, while assuming Gauss-Markov or\nmore complex priors to regularize those mixing probabilities. However, these\napproaches are constrained by the shape of the prior and often lead to\ncomplicated or intractable inference. Here, we propose a new parametrization of\nthe Dirichlet distribution to flexibly regularize the mixing probabilities of\nover-parametrized mixture distributions. Using the Expectation-Maximization\nalgorithm, we show that our approach allows us to define any linear update rule\nfor the mixing probabilities, including spatial smoothing regularization as a\nspecial case. We then show that this flexible design can be extended to share\nclass information between multiple mixture models. We apply our algorithm to\nartificial and natural image segmentation tasks, and we provide quantitative\nand qualitative comparison of the performance of Gaussian and Student-t\nmixtures on the Berkeley Segmentation Dataset. We also demonstrate how to\npropagate class information across the layers of deep convolutional neural\nnetworks in a probabilistically optimal way, suggesting a new interpretation\nfor feedback signals in biological visual systems. Our flexible approach can be\neasily generalized to adapt probabilistic mixture models to arbitrary data\ntopologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vacher_J/0/1/0/all/0/1\">Jonathan Vacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_C/0/1/0/all/0/1\">Claire Launay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coen_Cagli_R/0/1/0/all/0/1\">Ruben Coen-Cagli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Out-of-Distribution Detection for Real-World Settings. (arXiv:1911.11132v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.11132","description":"<p>Detecting out-of-distribution examples is important for safety-critical\nmachine learning applications such as detecting novel biological phenomena and\nself-driving cars. However, existing research mainly focuses on simple\nsmall-scale settings. To set the stage for more realistic out-of-distribution\ndetection, we depart from small-scale settings and explore large-scale\nmulticlass and multi-label settings with high-resolution images and thousands\nof classes. To make future work in real-world settings possible, we create new\nbenchmarks for three large-scale settings. To test ImageNet multiclass anomaly\ndetectors, we introduce the Species dataset containing over 700,000 images and\nover a thousand anomalous species. We leverage ImageNet-21K to evaluate PASCAL\nVOC and COCO multilabel anomaly detectors. Third, we introduce a new benchmark\nfor anomaly segmentation by introducing a segmentation benchmark with road\nanomalies. We conduct extensive experiments in these more realistic settings\nfor out-of-distribution detection and find that a surprisingly simple detector\nbased on the maximum logit outperforms prior methods in all the large-scale\nmulti-class, multi-label, and segmentation tasks, establishing a simple new\nbaseline for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1\">Mantas Mazeika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Joe Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mostajabi_M/0/1/0/all/0/1\">Mohammadreza Mostajabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Free Style Projection for Arbitrary Style Transfer. (arXiv:2003.07694v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.07694","description":"<p>Arbitrary image style transfer is a challenging task which aims to stylize a\ncontent image conditioned on arbitrary style images. In this task the\nfeature-level content-style transformation plays a vital role for proper fusion\nof features. Existing feature transformation algorithms often suffer from loss\nof content or style details, non-natural stroke patterns, and unstable\ntraining. To mitigate these issues, this paper proposes a new feature-level\nstyle transformation technique, named Style Projection, for parameter-free,\nfast, and effective content-style transformation. This paper further presents a\nreal-time feed-forward model to leverage Style Projection for arbitrary image\nstyle transfer, which includes a regularization term for matching the semantics\nbetween input contents and stylized outputs. Extensive qualitative analysis,\nquantitative evaluation, and user study have demonstrated the effectiveness and\nefficiency of the proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bihan Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huan_J/0/1/0/all/0/1\">Jun Huan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse-RS: a versatile framework for query-efficient sparse black-box adversarial attacks. (arXiv:2006.12834v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.12834","description":"<p>We propose a versatile framework based on random search, Sparse-RS, for\nscore-based sparse targeted and untargeted attacks in the black-box setting.\nSparse-RS does not rely on substitute models and achieves state-of-the-art\nsuccess rate and query efficiency for multiple sparse attack models:\n$l_0$-bounded perturbations, adversarial patches, and adversarial frames. The\n$l_0$-version of untargeted Sparse-RS outperforms all black-box and even all\nwhite-box attacks for different models on MNIST, CIFAR-10, and ImageNet.\nMoreover, our untargeted Sparse-RS achieves very high success rates even for\nthe challenging settings of $20\\times20$ adversarial patches and $2$-pixel wide\nadversarial frames for $224\\times224$ images. Finally, we show that Sparse-RS\ncan be applied to generate targeted universal adversarial patches where it\nsignificantly outperforms the existing approaches. The code of our framework is\navailable at https://github.com/fra31/sparse-rs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1\">Francesco Croce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andriushchenko_M/0/1/0/all/0/1\">Maksym Andriushchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Naman D. Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1\">Nicolas Flammarion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1\">Matthias Hein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Update Guided Interdependent Networks for Single Image Dehazing. (arXiv:2008.01701v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.01701","description":"<p>Images with haze of different varieties often pose a significant challenge to\ndehazing. Therefore, guidance by estimates of haze parameters related to its\nvariety would be beneficial and they should be progressively updated along with\niterative haze reduction to allow optimal dehazing. To this end, we propose a\nmulti-network dehazing framework containing novel interdependent dehazing and\nhaze parameter updater networks that operate within a unique iterative\nmechanism. The haze parameters, transmission map and atmospheric light, are\nfirst estimated using specific convolutional networks allowing color cast\nhandling. The estimated parameters are then used as priors in our dehazing\nmodule, where the estimates are progressively updated by novel convolutional\nnetworks using the iterative mechanism. The updating takes place jointly with\nprogressive dehazing by a convolutional network that invokes inter-iteration\ndependencies. The joint updating and dehazing within the iterative mechanism\ngradually modify the haze parameter estimates toward achieving optimal\ndehazing. Through ablation studies, our iterative dehazing framework is shown\nto be more effective than the use of conventional LSTM based recurrence,\nimage-to-image mapping and haze model based estimation. Our dehazing framework\nis qualitatively and quantitatively found to outperform the state-of-the-art on\nsynthetic and real-world hazy images of several datasets with varied hazy\nconditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1\">Aupendu Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhara_S/0/1/0/all/0/1\">Sobhan Kanti Dhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_D/0/1/0/all/0/1\">Debashis Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_P/0/1/0/all/0/1\">Prabir Kumar Biswas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose Estimation for Robot Manipulators via Keypoint Optimization and Sim-to-Real Transfer. (arXiv:2010.08054v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2010.08054","description":"<p>Keypoint detection is an essential building block for many robotic\napplications like motion capture and pose estimation. Historically, keypoints\nare detected using uniquely engineered markers such as checkerboards or\nfiducials. More recently, deep learning methods have been explored as they have\nthe ability to detect user-defined keypoints in a marker-less manner. However,\ndifferent manually selected keypoints can have uneven performance when it comes\nto detection and localization. An example of this can be found on symmetric\nrobotic tools where DNN detectors cannot solve the correspondence problem\ncorrectly. In this work, we propose a new and autonomous way to define the\nkeypoint locations that overcomes these challenges. The approach involves\nfinding the optimal set of keypoints on robotic manipulators for robust visual\ndetection and localization. Using a robotic simulator as a medium, our\nalgorithm utilizes synthetic data for DNN training, and the proposed algorithm\nis used to optimize the selection of keypoints through an iterative approach.\nThe results show that when using the optimized keypoints, the detection\nperformance of the DNNs improved significantly. We further use the optimized\nkeypoints for real robotic applications by using domain randomization to bridge\nthe reality gap between the simulator and the physical world. The physical\nworld experiments show how the proposed method can be applied to the\nwide-breadth of robotic applications that require visual feedback, such as\ncamera-to-robot calibration, robotic tool tracking, and end-effector pose\nestimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingpei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_F/0/1/0/all/0/1\">Florian Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yip_M/0/1/0/all/0/1\">Michael Yip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfVoxeLO: Self-supervised LiDAR Odometry with Voxel-based Deep Neural Networks. (arXiv:2010.09343v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.09343","description":"<p>Recent learning-based LiDAR odometry methods have demonstrated their\ncompetitiveness. However, most methods still face two substantial challenges:\n1) the 2D projection representation of LiDAR data cannot effectively encode 3D\nstructures from the point clouds; 2) the needs for a large amount of labeled\ndata for training limit the application scope of these methods. In this paper,\nwe propose a self-supervised LiDAR odometry method, dubbed SelfVoxeLO, to\ntackle these two difficulties. Specifically, we propose a 3D convolution\nnetwork to process the raw LiDAR data directly, which extracts features that\nbetter encode the 3D geometric patterns. To suit our network to self-supervised\nlearning, we design several novel loss functions that utilize the inherent\nproperties of LiDAR point clouds. Moreover, an uncertainty-aware mechanism is\nincorporated in the loss functions to alleviate the interference of moving\nobjects/noises. We evaluate our method's performances on two large-scale\ndatasets, i.e., KITTI and Apollo-SouthBay. Our method outperforms\nstate-of-the-art unsupervised methods by 27%/32% in terms of\ntranslational/rotational errors on the KITTI dataset and also performs well on\nthe Apollo-SouthBay dataset. By including more unlabelled training data, our\nmethod can further improve performance comparable to the supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kwan-Yee Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super-Resolution of Real-World Faces. (arXiv:2011.02427v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2011.02427","description":"<p>Real low-resolution (LR) face images contain degradations which are too\nvaried and complex to be captured by known downsampling kernels and\nsignal-independent noises. So, in order to successfully super-resolve real\nfaces, a method needs to be robust to a wide range of noise, blur, compression\nartifacts etc. Some of the recent works attempt to model these degradations\nfrom a dataset of real images using a Generative Adversarial Network (GAN).\nThey generate synthetically degraded LR images and use them with corresponding\nreal high-resolution(HR) image to train a super-resolution (SR) network using a\ncombination of a pixel-wise loss and an adversarial loss. In this paper, we\npropose a two module super-resolution network where the feature extractor\nmodule extracts robust features from the LR image, and the SR module generates\nan HR estimate using only these robust features. We train a degradation GAN to\nconvert bicubically downsampled clean images to real degraded images, and\ninterpolate between the obtained degraded LR image and its clean LR\ncounterpart. This interpolated LR image is then used along with it's\ncorresponding HR counterpart to train the super-resolution network from end to\nend. Entropy Regularized Wasserstein Divergence is used to force the encoded\nfeatures learnt from the clean and degraded images to closely resemble those\nextracted from the interpolated image to ensure robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Goswami_S/0/1/0/all/0/1\">Saurabh Goswami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aakanksha/0/1/0/all/0/1\">Aakanksha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+N_R/0/1/0/all/0/1\">Rajagopalan A. N</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LCDNet: Deep Loop Closure Detection and Point Cloud Registration for LiDAR SLAM. (arXiv:2103.05056v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.05056","description":"<p>Loop closure detection is an essential component of Simultaneous Localization\nand Mapping (SLAM) systems, which reduces the drift accumulated over time. Over\nthe years, several deep learning approaches have been proposed to address this\ntask, however their performance has been subpar compared to handcrafted\ntechniques, especially while dealing with reverse loops. In this paper, we\nintroduce the novel LCDNet that effectively detects loop closures in LiDAR\npoint clouds by simultaneously identifying previously visited places and\nestimating the 6-DoF relative transformation between the current scan and the\nmap. LCDNet is composed of a shared encoder, a place recognition head that\nextracts global descriptors, and a relative pose head that estimates the\ntransformation between two point clouds. We introduce a novel relative pose\nhead based on the unbalanced optimal transport theory that we implement in a\ndifferentiable manner to allow for end-to-end training. Extensive evaluations\nof LCDNet on multiple real-world autonomous driving datasets show that our\napproach outperforms state-of-the-art loop closure detection and point cloud\nregistration techniques by a large margin, especially while dealing with\nreverse loops. Moreover, we integrate our proposed loop closure detection\napproach into a LiDAR SLAM library to provide a complete mapping system and\ndemonstrate the generalization ability using different sensor setup in an\nunseen city.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1\">Daniele Cattaneo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1\">Matteo Vaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine learning method for light field refocusing. (arXiv:2103.16020v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.16020","description":"<p>Light field imaging introduced the capability to refocus an image after\ncapturing. Currently there are two popular methods for refocusing,\nshift-and-sum and Fourier slice methods. Neither of these two methods can\nrefocus the light field in real-time without any pre-processing. In this paper\nwe introduce a machine learning based refocusing technique that is capable of\nextracting 16 refocused images with refocusing parameters of\n\\alpha=0.125,0.250,0.375,...,2.0 in real-time. We have trained our network,\nwhich is called RefNet, in two experiments. Once using the Fourier slice method\nas the training -- i.e., \"ground truth\" -- data and another using the\nshift-and-sum method as the training data. We showed that in both cases, not\nonly is the RefNet method at least 134x faster than previous approaches, but\nalso the color prediction of RefNet is superior to both Fourier slice and\nshift-and-sum methods while having similar depth of field and focus distance\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hedayati_E/0/1/0/all/0/1\">Eisa Hedayati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Havens_T/0/1/0/all/0/1\">Timothy C. Havens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bos_J/0/1/0/all/0/1\">Jeremy P. Bos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models. (arXiv:2111.07355v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.07355","description":"<p>Hospitals, especially their emergency services, receive a high number of\nwrist fracture cases. For correct diagnosis and proper treatment of these,\nimages obtained from various medical equipment must be viewed by physicians,\nalong with the patients medical records and physical examination. The aim of\nthis study is to perform fracture detection by use of deep learning on wrist\nXray images to support physicians in the diagnosis of these fractures,\nparticularly in the emergency services. Using SABL, RegNet, RetinaNet, PAA,\nLibra R_CNN, FSAF, Faster R_CNN, Dynamic R_CNN and DCN deep learning based\nobject detection models with various backbones, 20 different fracture detection\nprocedures were performed on Gazi University Hospitals dataset of wrist Xray\nimages. To further improve these procedures, five different ensemble models\nwere developed and then used to reform an ensemble model to develop a unique\ndetection model, wrist fracture detection_combo (WFD_C). From 26 different\nmodels for fracture detection, the highest detection result obtained was 0.8639\naverage precision (AP50) in the WFD-C model. Huawei Turkey R&amp;D Center supports\nthis study within the scope of the ongoing cooperation project coded 071813\nbetween Gazi University, Huawei and Medskor. Code is available at\nhttps://github.com/fatihuysal88/wrist-d\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hardalac_F/0/1/0/all/0/1\">F&#x131;rat Hardala&#xe7;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uysal_F/0/1/0/all/0/1\">Fatih Uysal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peker_O/0/1/0/all/0/1\">Ozan Peker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ciceklidag_M/0/1/0/all/0/1\">Murat &#xc7;i&#xe7;eklida&#x11f;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tolunay_T/0/1/0/all/0/1\">Tolga Tolunay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tokgoz_N/0/1/0/all/0/1\">Nil Tokg&#xf6;z</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kutbay_U/0/1/0/all/0/1\">U&#x11f;urhan Kutbay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demirciler_B/0/1/0/all/0/1\">Boran Demirciler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mert_F/0/1/0/all/0/1\">Fatih Mert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low Precision Decentralized Distributed Training over IID and non-IID Data. (arXiv:2111.09389v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.09389","description":"<p>Decentralized distributed learning is the key to enabling large-scale machine\nlearning (training) on the edge devices utilizing private user-generated local\ndata, without relying on the cloud. However, the practical realization of such\non-device training is limited by the communication and compute bottleneck. In\nthis paper, we propose and show the convergence of low precision decentralized\ntraining that aims to reduce the computational complexity and communication\ncost of decentralized training. Many feedback-based compression techniques have\nbeen proposed in the literature to reduce communication costs. To the best of\nour knowledge, there is no work that applies and shows compute efficient\ntraining techniques such quantization, pruning, etc., for peer-to-peer\ndecentralized learning setups. Since real-world applications have a significant\nskew in the data distribution, we design \"Range-EvoNorm\" as the normalization\nactivation layer which is better suited for low precision training over non-IID\ndata. Moreover, we show that the proposed low precision training can be used in\nsynergy with other communication compression methods decreasing the\ncommunication cost further. Our experiments indicate that 8-bit decentralized\ntraining has minimal accuracy loss compared to its full precision counterpart\neven with non-IID data. However, when low precision training is accompanied by\ncommunication compression through sparsification we observe a 1-2% drop in\naccuracy. The proposed low precision decentralized training decreases\ncomputational complexity, memory usage, and communication cost by 4x and\ncompute energy by a factor of ~20x, while trading off less than a $1\\%$\naccuracy for both IID and non-IID data. In particular, with higher skew values,\nwe observe an increase in accuracy (by ~ 0.5%) with low precision training,\nindicating the regularization effect of the quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aketi_S/0/1/0/all/0/1\">Sai Aparna Aketi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodge_S/0/1/0/all/0/1\">Sangamesh Kodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Probability Estimation. (arXiv:2111.10734v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.10734","description":"<p>Reliable probability estimation is of crucial importance in many real-world\napplications where there is inherent uncertainty, such as weather forecasting,\nmedical prognosis, or collision avoidance in autonomous vehicles.\nProbability-estimation models are trained on observed outcomes (e.g. whether it\nhas rained or not, or whether a patient has died or not), because the\nground-truth probabilities of the events of interest are typically unknown. The\nproblem is therefore analogous to binary classification, with the important\ndifference that the objective is to estimate probabilities rather than\npredicting the specific outcome. The goal of this work is to investigate\nprobability estimation from high-dimensional data using deep neural networks.\nThere exist several methods to improve the probabilities generated by these\nmodels but they mostly focus on classification problems where the probabilities\nare related to model uncertainty. In the case of problems with inherent\nuncertainty, it is challenging to evaluate performance without access to\nground-truth probabilities. To address this, we build a synthetic dataset to\nstudy and compare different computable metrics. We evaluate existing methods on\nthe synthetic data as well as on three real-world probability estimation tasks,\nall of which involve inherent uncertainty. We also give a theoretical analysis\nof a model for high-dimensional probability estimation which reproduces several\nof the phenomena evinced in our experiments. Finally, we propose a new method\nfor probability estimation using neural networks, which modifies the training\nprocess to promote output probabilities that are consistent with empirical\nprobabilities computed from the data. The method outperforms existing\napproaches on most metrics on the simulated as well as real-world data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaku_A/0/1/0/all/0/1\">Aakash Kaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Weicheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibovich_M/0/1/0/all/0/1\">Matan Leibovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Sreyas Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Boyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanna_L/0/1/0/all/0/1\">Laure Zanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavian_N/0/1/0/all/0/1\">Narges Razavian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HPRN: Holistic Prior-embedded Relation Network for Spectral Super-Resolution. (arXiv:2112.14608v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.14608","description":"<p>Spectral super-resolution (SSR) refers to the hyperspectral image (HSI)\nrecovery from an RGB counterpart. Due to the one-to-many nature of the SSR\nproblem, a single RGB image can be reprojected to many HSIs. The key to tackle\nthis ill-posed problem is to plug into multi-source prior information such as\nthe natural spatial context-prior of RGB images, deep feature-prior or inherent\nstatistical-prior of HSIs, etc., so as to effectively alleviate the degree of\nill-posedness. However, most current approaches only consider the general and\nlimited priors in their customized convolutional neural networks (CNNs), which\nleads to the inability to guarantee the confidence and fidelity of\nreconstructed spectra. In this paper, we propose a novel holistic\nprior-embedded relation network (HPRN) to integrate comprehensive priors to\nregularize and optimize the solution space of SSR. Basically, the core\nframework is delicately assembled by several multi-residual relation blocks\n(MRBs) that fully facilitate the transmission and utilization of the\nlow-frequency content prior of RGBs. Innovatively, the semantic prior of RGB\ninputs is introduced to mark category attributes, and a semantic-driven spatial\nrelation module (SSRM) is invented to perform the feature aggregation of\nclustered similar range for refining recovered characteristics. Additionally,\nwe develop a transformer-based channel relation module (TCRM), which breaks the\nhabit of employing scalars as the descriptors of channel-wise relations in the\nprevious deep feature-prior, and replaces them with certain vectors to make the\nmapping function more robust and smoother. In order to maintain the\nmathematical correlation and spectral consistency between hyperspectral bands,\nthe second-order prior constraints (SOPC) are incorporated into the loss\nfunction to guide the HSI reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chaoxiong Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiaojiao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yunsong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniFormer: Unified Transformer for Efficient Spatiotemporal Representation Learning. (arXiv:2201.04676v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04676","description":"<p>It is a challenging task to learn rich and multi-scale spatiotemporal\nsemantics from high-dimensional videos, due to large local redundancy and\ncomplex global dependency between video frames. The recent advances in this\nresearch have been mainly driven by 3D convolutional neural networks and vision\ntransformers. Although 3D convolution can efficiently aggregate local context\nto suppress local redundancy from a small 3D neighborhood, it lacks the\ncapability to capture global dependency because of the limited receptive field.\nAlternatively, vision transformers can effectively capture long-range\ndependency by self-attention mechanism, while having the limitation on reducing\nlocal redundancy with blind similarity comparison among all the tokens in each\nlayer. Based on these observations, we propose a novel Unified transFormer\n(UniFormer) which seamlessly integrates merits of 3D convolution and\nspatiotemporal self-attention in a concise transformer format, and achieves a\npreferable balance between computation and accuracy. Different from traditional\ntransformers, our relation aggregator can tackle both spatiotemporal redundancy\nand dependency, by learning local and global token affinity respectively in\nshallow and deep layers. We conduct extensive experiments on the popular video\nbenchmarks, e.g., Kinetics-400, Kinetics-600, and Something-Something V1&amp;V2.\nWith only ImageNet-1K pretraining, our UniFormer achieves 82.9%/84.8% top-1\naccuracy on Kinetics-400/Kinetics-600, while requiring 10x fewer GFLOPs than\nother state-of-the-art methods. For Something-Something V1 and V2, our\nUniFormer achieves new state-of-the-art performances of 60.9% and 71.2% top-1\naccuracy respectively. Code is available at\nhttps://github.com/Sense-X/UniFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guanglu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore the Expression: Facial Expression Generation using Auxiliary Classifier Generative Adversarial Network. (arXiv:2201.09061v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09061","description":"<p>Facial expressions are a form of non-verbal communication that humans perform\nseamlessly for meaningful transfer of information. Most of the literature\naddresses the facial expression recognition aspect however, with the advent of\nGenerative Models, it has become possible to explore the affect space in\naddition to mere classification of a set of expressions. In this article, we\npropose a generative model architecture which robustly generates a set of\nfacial expressions for multiple character identities and explores the\npossibilities of generating complex expressions by combining the simple ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_J/0/1/0/all/0/1\">J. Rafid Siddiqui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. (arXiv:2201.12329v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12329","description":"<p>We present in this paper a novel query formulation using dynamic anchor boxes\nfor DETR (DEtection TRansformer) and offer a deeper understanding of the role\nof queries in DETR. This new formulation directly uses box coordinates as\nqueries in Transformer decoders and dynamically updates them layer-by-layer.\nUsing box coordinates not only helps using explicit positional priors to\nimprove the query-to-feature similarity and eliminate the slow training\nconvergence issue in DETR, but also allows us to modulate the positional\nattention map using the box width and height information. Such a design makes\nit clear that queries in DETR can be implemented as performing soft ROI pooling\nlayer-by-layer in a cascade manner. As a result, it leads to the best\nperformance on MS-COCO benchmark among the DETR-like detection models under the\nsame setting, e.g., AP 45.7\\% using ResNet50-DC5 as backbone trained in 50\nepochs. We also conducted extensive experiments to confirm our analysis and\nverify the effectiveness of our methods. Code is available at\n\\url{https://github.com/SlongLiu/DAB-DETR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xianbiao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-arbitrary Invertible Image Downscaling. (arXiv:2201.12576v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12576","description":"<p>Downscaling is indispensable when distributing high-resolution (HR) images\nover the Internet to fit the displays of various resolutions, while upscaling\nis also necessary when users want to see details of the distributed images.\nRecent invertible image downscaling methods jointly model these two problems\nand achieve significant improvements. However, they only consider fixed integer\nscale factors that cannot meet the requirement of conveniently fitting the\ndisplays of various resolutions in real-world applications. In this paper, we\npropose a scale-Arbitrary Invertible image Downscaling Network (AIDN), to\nnatively downscale HR images with arbitrary scale factors for fitting various\ntarget resolutions. Meanwhile, the HR information is embedded in the downscaled\nlow-resolution (LR) counterparts in a nearly imperceptible form such that our\nAIDN can also restore the original HR images solely from the LR images. The key\nto supporting arbitrary scale factors is our proposed Conditional Resampling\nModule (CRM) that conditions the downscaling/upscaling kernels and sampling\nlocations on both scale factors and image content. Extensive experimental\nresults demonstrate that our AIDN achieves top performance for invertible\ndownscaling with both arbitrary integer and non-integer scale factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Jinbo Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1\">Tien-Tsin Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Novelty Search with a Surrogate Model to Engineer Meta-Diversity in Ensembles of Classifiers. (arXiv:2201.12896v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12896","description":"<p>Using Neuroevolution combined with Novelty Search to promote behavioural\ndiversity is capable of constructing high-performing ensembles for\nclassification. However, using gradient descent to train evolved architectures\nduring the search can be computationally prohibitive. Here we propose a method\nto overcome this limitation by using a surrogate model which estimates the\nbehavioural distance between two neural network architectures required to\ncalculate the sparseness term in Novelty Search. We demonstrate a speedup of 10\ntimes over previous work and significantly improve on previous reported results\non three benchmark datasets from Computer Vision -- CIFAR-10, CIFAR-100, and\nSVHN. This results from the expanded architecture search space facilitated by\nusing a surrogate. Our method represents an improved paradigm for implementing\nhorizontal scaling of learning algorithms by making an explicit search for\ndiversity considerably more tractable for the same bounded resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_R/0/1/0/all/0/1\">Rui P. Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hart_E/0/1/0/all/0/1\">Emma Hart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurka_D/0/1/0/all/0/1\">David Burth Kurka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitt_J/0/1/0/all/0/1\">Jeremy V. Pitt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extension: Adaptive Sampling with Implicit Radiance Field. (arXiv:2202.00855v3 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2202.00855","description":"<p>This manuscript discusses the extension of adaptive light field sampling with\nimplicit radiance fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuchi Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate calibration of multi-perspective cameras from a generalization of the hand-eye constraint. (arXiv:2202.00886v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.00886","description":"<p>Multi-perspective cameras are quickly gaining importance in many applications\nsuch as smart vehicles and virtual or augmented reality. However, a large\nsystem size or absence of overlap in neighbouring fields-of-view often\ncomplicate their calibration. We present a novel solution which relies on the\navailability of an external motion capture system. Our core contribution\nconsists of an extension to the hand-eye calibration problem which jointly\nsolves multi-eye-to-base problems in closed form. We furthermore demonstrate\nits equivalence to the multi-eye-in-hand problem. The practical validity of our\napproach is supported by our experiments, indicating that the method is highly\nefficient and accurate, and outperforms existing closed-form alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1\">Soren Schwertfeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge-Selective Feature Weaving for Point Cloud Matching. (arXiv:2202.02149v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02149","description":"<p>This paper tackles the problem of accurately matching the points of two 3D\npoint clouds. Most conventional methods improve their performance by extracting\nrepresentative features from each point via deep-learning-based algorithms. On\nthe other hand, the correspondence calculation between the extracted features\nhas not been examined in depth, and non-trainable algorithms (e.g. the Sinkhorn\nalgorithm) are frequently applied. As a result, the extracted features may be\nforcibly fitted to a non-trainable algorithm. Furthermore, the extracted\nfeatures frequently contain stochastically unavoidable errors, which degrades\nthe matching accuracy. In this paper, instead of using a non-trainable\nalgorithm, we propose a differentiable matching network that can be jointly\noptimized with the feature extraction procedure. Our network first constructs\ngraphs with edges connecting the points of each point cloud and then extracts\ndiscriminative edge features by using two main components: a shared set-encoder\nand an edge-selective cross-concatenation. These components enable us to\nsymmetrically consider two point clouds and to extract discriminative edge\nfeatures, respectively. By using the extracted discriminative edge features,\nour network can accurately calculate the correspondence between points. Our\nexperimental results show that the proposed network can significantly improve\nthe performance of point cloud matching. Our code is available at\nhttps://github.com/yanarin/ESFW\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yanagi_R/0/1/0/all/0/1\">Rintaro Yanagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1\">Atsushi Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sone_S/0/1/0/all/0/1\">Shusaku Sone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiba_N/0/1/0/all/0/1\">Naoya Chiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1\">Yoshitaka Ushiku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PrivPAS: A real time Privacy-Preserving AI System and applied ethics. (arXiv:2202.02524v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02524","description":"<p>With 3.78 billion social media users worldwide in 2021 (48% of the human\npopulation), almost 3 billion images are shared daily. At the same time, a\nconsistent evolution of smartphone cameras has led to a photography explosion\nwith 85% of all new pictures being captured using smartphones. However, lately,\nthere has been an increased discussion of privacy concerns when a person being\nphotographed is unaware of the picture being taken or has reservations about\nthe same being shared. These privacy violations are amplified for people with\ndisabilities, who may find it challenging to raise dissent even if they are\naware. Such unauthorized image captures may also be misused to gain sympathy by\nthird-party organizations, leading to a privacy breach. Privacy for people with\ndisabilities has so far received comparatively less attention from the AI\ncommunity. This motivates us to work towards a solution to generate\nprivacy-conscious cues for raising awareness in smartphone users of any\nsensitivity in their viewfinder content. To this end, we introduce PrivPAS (A\nreal time Privacy-Preserving AI System) a novel framework to identify sensitive\ncontent. Additionally, we curate and annotate a dataset to identify and\nlocalize accessibility markers and classify whether an image is sensitive to a\nfeatured subject with a disability. We demonstrate that the proposed\nlightweight architecture, with a memory footprint of a mere 8.49MB, achieves a\nhigh mAP of 89.52% on resource-constrained devices. Furthermore, our pipeline,\ntrained on face anonymized data, achieves an F1-score of 73.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_H/0/1/0/all/0/1\">Harichandana B S S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1\">Vibhav Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sourav Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramena_G/0/1/0/all/0/1\">Gopi Ramena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sumit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_B/0/1/0/all/0/1\">Barath Raj Kandur Raja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLPanoDepth: Global-to-Local Panoramic Depth Estimation. (arXiv:2202.02796v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02796","description":"<p>In this paper, we propose a learning-based method for predicting dense depth\nvalues of a scene from a monocular omnidirectional image. An omnidirectional\nimage has a full field-of-view, providing much more complete descriptions of\nthe scene than perspective images. However, fully-convolutional networks that\nmost current solutions rely on fail to capture rich global contexts from the\npanorama. To address this issue and also the distortion of equirectangular\nprojection in the panorama, we propose Cubemap Vision Transformers (CViT), a\nnew transformer-based architecture that can model long-range dependencies and\nextract distortion-free global features from the panorama. We show that cubemap\nvision transformers have a global receptive field at every stage and can\nprovide globally coherent predictions for spherical signals. To preserve\nimportant local features, we further design a convolution-based branch in our\npipeline (dubbed GLPanoDepth) and fuse global features from cubemap vision\ntransformers at multiple scales. This global-to-local strategy allows us to\nfully exploit useful global and local features in the panorama, achieving\nstate-of-the-art performance in panoramic depth estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiayang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Shuichang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Haoyu Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imposing Temporal Consistency on Deep Monocular Body Shape and Pose Estimation. (arXiv:2202.03074v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03074","description":"<p>Accurate and temporally consistent modeling of human bodies is essential for\na wide range of applications, including character animation, understanding\nhuman social behavior and AR/VR interfaces. Capturing human motion accurately\nfrom a monocular image sequence is still challenging and the modeling quality\nis strongly influenced by the temporal consistency of the captured body motion.\nOur work presents an elegant solution for the integration of temporal\nconstraints in the fitting process. This does not only increase temporal\nconsistency but also robustness during the optimization. In detail, we derive\nparameters of a sequence of body models, representing shape and motion of a\nperson, including jaw poses, facial expressions, and finger poses. We optimize\nthese parameters over the complete image sequence, fitting one consistent body\nshape while imposing temporal consistency on the body motion, assuming linear\nbody joint trajectories over a short time. Our approach enables the derivation\nof realistic 3D body models from image sequences, including facial expression\nand articulated hands. In extensive experiments, we show that our approach\nresults in accurately estimated body shape and motion, also for challenging\nmovements and poses. Further, we apply it to the special application of sign\nlanguage analysis, where accurate and temporal consistent motion modelling is\nessential, and show that the approach is well-suited for this kind of\napplication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_A/0/1/0/all/0/1\">Alexandra Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilsmann_A/0/1/0/all/0/1\">Anna Hilsmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgenstern_W/0/1/0/all/0/1\">Wieland Morgenstern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisert_P/0/1/0/all/0/1\">Peter Eisert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Long-Term Person Re-Identification with Clothes Change. (arXiv:2202.03087v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03087","description":"<p>We investigate unsupervised person re-identification (Re-ID) with clothes\nchange, a new challenging problem with more practical usability and scalability\nto real-world deployment. Most existing re-id methods artificially assume the\nclothes of every single person to be stationary across space and time. This\ncondition is mostly valid for short-term re-id scenarios since an average\nperson would often change the clothes even within a single day. To alleviate\nthis assumption, several recent works have introduced the clothes change facet\nto re-id, with a focus on supervised learning person identity discriminative\nrepresentation with invariance to clothes changes. Taking a step further\ntowards this long-term re-id direction, we further eliminate the requirement of\nperson identity labels, as they are significantly more expensive and more\ntedious to annotate in comparison to short-term person re-id datasets. Compared\nto conventional unsupervised short-term re-id, this new problem is drastically\nmore challenging as different people may have similar clothes whilst the same\nperson can wear multiple suites of clothes over different locations and times\nwith very distinct appearance. To overcome such obstacles, we introduce a novel\nCurriculum Person Clustering (CPC) method that can adaptively regulate the\nunsupervised clustering criterion according to the clustering confidence.\nExperiments on three long-term person re-id datasets show that our CPC\noutperforms SOTA unsupervised re-id methods and even closely matches the\nsupervised re-id models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingkun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}