{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-17T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Combinatorics of \\textit{Salva Veritate} Principles. (arXiv:2201.05173v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05173","description":"<p>Various concepts of grammatical compositionality arise in many theories of\nboth natural and artificial languages, and often play a key role in accounts of\nthe syntax-semantics interface. We propose that many instances of\ncompositionality should entail non-trivial combinatorial claims about the\nexpressive power of languages which satisfy these compositional properties. As\nan example, we present a formal analysis demonstrating that a particular class\nof languages which admit salva vertitate substitutions - a property which we\nclaim to be a particularly strong example of compositional principle - must\nalso satisfy a very natural combinatorial constraint identified in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trushaev_N/0/1/0/all/0/1\">Norman E. Trushaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Approaches to Conversational Information Retrieval. (arXiv:2201.05176v1 [cs.IR])","link":"http://arxiv.org/abs/2201.05176","description":"<p>A conversational information retrieval (CIR) system is an information\nretrieval (IR) system with a conversational interface which allows users to\ninteract with the system to seek information via multi-turn conversations of\nnatural language, in spoken or written form. Recent progress in deep learning\nhas brought tremendous improvements in natural language processing (NLP) and\nconversational AI, leading to a plethora of commercial conversational services\nthat allow naturally spoken and typed interaction, increasing the need for more\nhuman-centric interactions in IR. As a result, we have witnessed a resurgent\ninterest in developing modern CIR systems in both research communities and\nindustry. This book surveys recent advances in CIR, focusing on neural\napproaches that have been developed in the last few years. This book is based\non the authors' tutorial at SIGIR'2020 (Gao et al., 2020b), with IR and NLP\ncommunities as the primary target audience. However, audiences with other\nbackground, such as machine learning and human-computer interaction, will also\nfind it an accessible introduction to CIR. We hope that this book will prove a\nvaluable resource for students, researchers, and software developers. This\nmanuscript is a working draft. Comments are welcome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craswell_N/0/1/0/all/0/1\">Nick Craswell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making a (Counterfactual) Difference One Rationale at a Time. (arXiv:2201.05177v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05177","description":"<p>Rationales, snippets of extracted text that explain an inference, have\nemerged as a popular framework for interpretable natural language processing\n(NLP). Rationale models typically consist of two cooperating modules: a\nselector and a classifier with the goal of maximizing the mutual information\n(MMI) between the \"selected\" text and the document label. Despite their\npromises, MMI-based methods often pick up on spurious text patterns and result\nin models with nonsensical behaviors. In this work, we investigate whether\ncounterfactual data augmentation (CDA), without human assistance, can improve\nthe performance of the selector by lowering the mutual information between\nspurious signals and the document label. Our counterfactuals are produced in an\nunsupervised fashion using class-dependent generative models. From an\ninformation theoretic lens, we derive properties of the unaugmented dataset for\nwhich our CDA approach would succeed. The effectiveness of CDA is empirically\nevaluated by comparing against several baselines including an improved\nMMI-based rationale schema on two multi aspect datasets. Our results show that\nCDA produces rationales that better capture the signal of interest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plyler_M/0/1/0/all/0/1\">Mitchell Plyler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1\">Michael Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_M/0/1/0/all/0/1\">Min Chi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP in Human Rights Research -- Extracting Knowledge Graphs About Police and Army Units and Their Commanders. (arXiv:2201.05230v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05230","description":"<p>In this working paper we explore the use of an NLP system to assist the work\nof Security Force Monitor (SFM). SFM creates data about the organizational\nstructure, command personnel and operations of police, army and other security\nforces, which assists human rights researchers, journalists and litigators in\ntheir work to help identify and bring to account specific units and personnel\nalleged to have committed abuses of human rights and international criminal\nlaw. This working paper presents an NLP system that extracts from English\nlanguage news reports the names of security force units and the biographical\ndetails of their personnel, and infers the formal relationship between them.\nPublished alongside this working paper are the system's code and training\ndataset. We find that the experimental NLP system performs the task at a fair\nto good level. Its performance is sufficient to justify further development\ninto a live workflow that will give insight into whether its performance\ntranslates into savings in time and resource that would make it an effective\ntechnical intervention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bauer_D/0/1/0/all/0/1\">Daniel Bauer</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Longley_T/0/1/0/all/0/1\">Tom Longley</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yueen Ma</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_T/0/1/0/all/0/1\">Tony Wilson</a> (2) ((1) Department of Computer Science, Columbia University, (2) Security Force Monitor, Human Rights Institute, Columbia Law School)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal alphabet for single text compression. (arXiv:2201.05234v1 [cs.IT])","link":"http://arxiv.org/abs/2201.05234","description":"<p>A text can be viewed via different representations, i.e. as a sequence of\nletters, n-grams of letters, syllables, words, and phrases. Here we study the\noptimal noiseless compression of texts using the Huffman code, where the\nalphabet of encoding coincides with one of those representations. We show that\nit is necessary to account for the codebook when compressing a single text.\nHence, the total compression comprises of the optimally compressed text --\ncharacterized by the entropy of the alphabet elements -- and the codebook which\nis text-specific and therefore has to be included for noiseless\n(de)compression. For texts of Project Gutenberg the best compression is\nprovided by syllables, i.e. the minimal meaning-expressing element of the\nlanguage. If only sufficiently short texts are retained, the optimal alphabet\nis that of letters or 2-grams of letters depending on the retained length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allahverdyan_A/0/1/0/all/0/1\">Armen E. Allahverdyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khachatryan_A/0/1/0/all/0/1\">Andranik Khachatryan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Pretrained Language Models Based Text Generation. (arXiv:2201.05273v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05273","description":"<p>Text Generation aims to produce plausible and readable text in human language\nfrom input data. The resurgence of deep learning has greatly advanced this\nfield by neural generation models, especially the paradigm of pretrained\nlanguage models (PLMs). Grounding text generation on PLMs is seen as a\npromising direction in both academia and industry. In this survey, we present\nthe recent advances achieved in the topic of PLMs for text generation. In\ndetail, we begin with introducing three key points of applying PLMs to text\ngeneration: 1) how to encode the input data as representations preserving input\nsemantics which can be fused into PLMs; 2) how to design a universal and\nperformant architecture of PLMs served as generation models; and 3) how to\noptimize PLMs given the reference text and ensure the generated text satisfying\nspecial text properties. Then, we figure out several challenges and future\ndirections within each key point. Next, we present a summary of various useful\nresources and typical text generation applications to work with PLMs. Finally,\nwe conclude and summarize the contribution of this survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Narrative Semantic Overlap Task: Evaluation and Benchmark. (arXiv:2201.05294v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05294","description":"<p>In this paper, we introduce an important yet relatively unexplored NLP task\ncalled Multi-Narrative Semantic Overlap (MNSO), which entails generating a\nSemantic Overlap of multiple alternate narratives. As no benchmark dataset is\nreadily available for this task, we created one by crawling 2,925 narrative\npairs from the web and then, went through the tedious process of manually\ncreating 411 different ground-truth semantic overlaps by engaging human\nannotators. As a way to evaluate this novel task, we first conducted a\nsystematic study by borrowing the popular ROUGE metric from text-summarization\nliterature and discovered that ROUGE is not suitable for our task.\nSubsequently, we conducted further human annotations/validations to create 200\ndocument-level and 1,518 sentence-level ground-truth labels which helped us\nformulate a new precision-recall style evaluation metric, called SEM-F1\n(semantic F1). Experimental results show that the proposed SEM-F1 metric yields\nhigher correlation with human judgement as well as higher inter-rater-agreement\ncompared to ROUGE metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_N/0/1/0/all/0/1\">Naman Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akter_M/0/1/0/all/0/1\">Mousumi Akter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santu_S/0/1/0/all/0/1\">Shubhra Kanti Karmaker Santu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering. (arXiv:2201.05299v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05299","description":"<p>Outside-knowledge visual question answering (OK-VQA) requires the agent to\ncomprehend the image, make use of relevant knowledge from the entire web, and\ndigest all the information to answer the question. Most previous works address\nthe problem by first fusing the image and question in the multi-modal space,\nwhich is inflexible for further fusion with a vast amount of external\nknowledge. In this paper, we call for a paradigm shift for the OK-VQA task,\nwhich transforms the image into plain text, so that we can enable knowledge\npassage retrieval, and generative question-answering in the natural language\nspace. This paradigm takes advantage of the sheer volume of gigantic knowledge\nbases and the richness of pre-trained language models. A\nTransform-Retrieve-Generate framework (TRiG) framework is proposed, which can\nbe plug-and-played with alternative image-to-text models and textual knowledge\nbases. Experimental results show that our TRiG framework outperforms all\nstate-of-the-art supervised methods by at least 11.1% absolute margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_Q/0/1/0/all/0/1\">Qing Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reganti_A/0/1/0/all/0/1\">Aishwarya Reganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying a Generic Sequence-to-Sequence Model for Simple and Effective Keyphrase Generation. (arXiv:2201.05302v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05302","description":"<p>In recent years, a number of keyphrase generation (KPG) approaches were\nproposed consisting of complex model architectures, dedicated training\nparadigms and decoding strategies. In this work, we opt for simplicity and show\nhow a commonly used seq2seq language model, BART, can be easily adapted to\ngenerate keyphrases from the text in a single batch computation using a simple\ntraining procedure. Empirical results on five benchmarks show that our approach\nis as good as the existing state-of-the-art KPG systems, but using a much\nsimpler and easy to deploy framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md Faisal Mahbub Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1\">Michael Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExtraPhrase: Efficient Data Augmentation for Abstractive Summarization. (arXiv:2201.05313v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05313","description":"<p>Neural models trained with large amount of parallel data have achieved\nimpressive performance in abstractive summarization tasks. However, large-scale\nparallel corpora are expensive and challenging to construct. In this work, we\nintroduce a low-cost and effective strategy, ExtraPhrase, to augment training\ndata for abstractive summarization tasks. ExtraPhrase constructs pseudo\ntraining data in two steps: extractive summarization and paraphrasing. We\nextract major parts of an input text in the extractive summarization step, and\nobtain its diverse expressions with the paraphrasing step. Through experiments,\nwe show that ExtraPhrase improves the performance of abstractive summarization\ntasks by more than 0.50 points in ROUGE scores compared to the setting without\ndata augmentation. ExtraPhrase also outperforms existing methods such as\nback-translation and self-training. We also show that ExtraPhrase is\nsignificantly effective when the amount of genuine training data is remarkably\nsmall, i.e., a low-resource setting. Moreover, ExtraPhrase is more\ncost-efficient than the existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loem_M/0/1/0/all/0/1\">Mengsay Loem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1\">Sho Takase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1\">Masahiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CommonsenseQA 2.0: Exposing the Limits of AI through Gamification. (arXiv:2201.05320v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05320","description":"<p>Constructing benchmarks that test the abilities of modern natural language\nunderstanding models is difficult - pre-trained language models exploit\nartifacts in benchmarks to achieve human parity, but still fail on adversarial\nexamples and make errors that demonstrate a lack of common sense. In this work,\nwe propose gamification as a framework for data construction. The goal of\nplayers in the game is to compose questions that mislead a rival AI while using\nspecific phrases for extra points. The game environment leads to enhanced user\nengagement and simultaneously gives the game designer control over the\ncollected data, allowing us to collect high-quality data at scale. Using our\nmethod we create CommonsenseQA 2.0, which includes 14,343 yes/no questions, and\ndemonstrate its difficulty for models that are orders-of-magnitude larger than\nthe AI used in the game itself. Our best baseline, the T5-based Unicorn with\n11B parameters achieves an accuracy of 70.2%, substantially higher than GPT-3\n(52.9%) in a few-shot inference setup. Both score well below human performance\nwhich is at 94.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Talmor_A/0/1/0/all/0/1\">Alon Talmor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"This Must Be the Place: Predicting Engagement of Online Communities in a Large-scale Distributed Campaign. (arXiv:2201.05334v1 [cs.SI])","link":"http://arxiv.org/abs/2201.05334","description":"<p>Understanding collective decision making at a large-scale, and elucidating\nhow community organization and community dynamics shape collective behavior are\nat the heart of social science research. In this work we study the behavior of\nthousands of communities with millions of active members. We define a novel\ntask: predicting which community will undertake an unexpected, large-scale,\ndistributed campaign. To this end, we develop a hybrid model, combining textual\ncues, community meta-data, and structural properties. We show how this\nmulti-faceted model can accurately predict large-scale collective\ndecision-making in a distributed environment. We demonstrate the applicability\nof our model through Reddit's r/place a large-scale online experiment in which\nmillions of users, self-organized in thousands of communities, clashed and\ncollaborated in an effort to realize their agenda.\n</p>\n<p>Our hybrid model achieves a high F1 prediction score of 0.826. We find that\ncoarse meta-features are as important for prediction accuracy as fine-grained\ntextual cues, while explicit structural features play a smaller role.\nInterpreting our model, we provide and support various social insights about\nthe unique characteristics of the communities that participated in the r/place\nexperiment.\n</p>\n<p>Our results and analysis shed light on the complex social dynamics that drive\ncollective behavior, and on the factors that propel user coordination. The\nscale and the unique conditions of the r/place experiment suggest that our\nfindings may apply in broader contexts, such as online activism, (countering)\nthe spread of hate speech and reducing political polarization. The broader\napplicability of the model is demonstrated through an extensive analysis of the\nWallStreetBets community, their role in r/place and the GameStop short squeeze\ncampaign of 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Israeli_A/0/1/0/all/0/1\">Abraham Israeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kremiansky_A/0/1/0/all/0/1\">Alexander Kremiansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsur_O/0/1/0/all/0/1\">Oren Tsur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05337","description":"<p>Controllable Text Generation (CTG) is emerging area in the field of natural\nlanguage generation (NLG). It is regarded as crucial for the development of\nadvanced text generation technologies that are more natural and better meet the\nspecific constraints in practical applications. In recent years, methods using\nlarge-scale pre-trained language models (PLMs), in particular the widely used\ntransformer-based PLMs, have become a new paradigm of NLG, allowing generation\nof more diverse and fluent text. However, due to the lower level of\ninterpretability of deep neural networks, the controllability of these methods\nneed to be guaranteed. To this end, controllable text generation using\ntransformer-based PLMs has become a rapidly growing yet challenging new\nresearch hotspot. A diverse range of approaches have emerged in the recent 3-4\nyears, targeting different CTG tasks which may require different types of\ncontrolled constraints. In this paper, we present a systematic critical review\non the common tasks, main approaches and evaluation methods in this area.\nFinally, we discuss the challenges that the field is facing, and put forward\nvarious promising future directions. To the best of our knowledge, this is the\nfirst survey paper to summarize CTG techniques from the perspective of PLMs. We\nhope it can help researchers in related fields to quickly track the academic\nfrontier, providing them with a landscape of the area and a roadmap for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haolin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polarity and Subjectivity Detection with Multitask Learning and BERT Embedding. (arXiv:2201.05363v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05363","description":"<p>Multitask learning often helps improve the performance of related tasks as\nthese often have inter-dependence on each other and perform better when solved\nin a joint framework. In this paper, we present a deep multitask learning\nframework that jointly performs polarity and subjective detection. We propose\nan attention-based multitask model for predicting polarity and subjectivity.\nThe input sentences are transformed into vectors using pre-trained BERT and\nGlove embeddings, and the results depict that BERT embedding based model works\nbetter than the Glove based model. We compare our approach with\nstate-of-the-art models in both subjective and polarity classification\nsingle-task and multitask frameworks. The proposed approach reports baseline\nperformances for both polarity detection and subjectivity detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Satapathy_R/0/1/0/all/0/1\">Ranjan Satapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardeshi_S/0/1/0/all/0/1\">Shweta Pardeshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mental Health Assessment for the Chatbots. (arXiv:2201.05382v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05382","description":"<p>Previous researches on dialogue system assessment usually focus on the\nquality evaluation (e.g. fluency, relevance, etc) of responses generated by the\nchatbots, which are local and technical metrics. For a chatbot which responds\nto millions of online users including minors, we argue that it should have a\nhealthy mental tendency in order to avoid the negative psychological impact on\nthem. In this paper, we establish several mental health assessment dimensions\nfor chatbots (depression, anxiety, alcohol addiction, empathy) and introduce\nthe questionnaire-based mental health assessment methods. We conduct\nassessments on some well-known open-domain chatbots and find that there are\nsevere mental health issues for all these chatbots. We consider that it is due\nto the neglect of the mental health risks during the dataset building and the\nmodel training procedures. We expect to attract researchers' attention to the\nserious mental health problems of chatbots and improve the chatbots' ability in\npositive emotional interaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yong Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressively Optimized Bi-Granular Document Representation for Scalable Embedding Based Retrieval. (arXiv:2201.05409v1 [cs.IR])","link":"http://arxiv.org/abs/2201.05409","description":"<p>Ad-hoc search calls for the selection of appropriate answers from a\nmassive-scale corpus. Nowadays, the embedding-based retrieval (EBR) becomes a\npromising solution, where deep learning based document representation and ANN\nsearch techniques are allied to handle this task. However, a major challenge is\nthat the ANN index can be too large to fit into memory, given the considerable\nsize of answer corpus. In this work, we tackle this problem with Bi-Granular\nDocument Representation, where the lightweight sparse embeddings are indexed\nand standby in memory for coarse-grained candidate search, and the heavyweight\ndense embeddings are hosted in disk for fine-grained post verification. For the\nbest of retrieval accuracy, a Progressive Optimization framework is designed.\nThe sparse embeddings are learned ahead for high-quality search of candidates.\nConditioned on the candidate distribution induced by the sparse embeddings, the\ndense embeddings are continuously learned to optimize the discrimination of\nground-truth from the shortlisted candidates. Besides, two techniques: the\ncontrastive quantization and the locality-centric sampling are introduced for\nthe learning of sparse and dense embeddings, which substantially contribute to\ntheir performances. Thanks to the above features, our method effectively\nhandles massive-scale EBR with strong advantages in accuracy: with up to +4.3%\nrecall gain on million-scale corpus, and up to +17.5% recall gain on\nbillion-scale corpus. Besides, Our method is applied to a major sponsored\nsearch platform with substantial gains on revenue (+1.95%), Recall (+1.01%) and\nCTR (+0.49%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Weihao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianjin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaozhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Denvy Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliciting Knowledge from Pretrained Language Models for Prototypical Prompt Verbalizer. (arXiv:2201.05411v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05411","description":"<p>Recent advances on prompt-tuning cast few-shot classification tasks as a\nmasked language modeling problem. By wrapping input into a template and using a\nverbalizer which constructs a mapping between label space and label word space,\nprompt-tuning can achieve excellent results in zero-shot and few-shot\nscenarios. However, typical prompt-tuning needs a manually designed verbalizer\nwhich requires domain expertise and human efforts. And the insufficient label\nspace may introduce considerable bias into the results. In this paper, we focus\non eliciting knowledge from pretrained language models and propose a\nprototypical prompt verbalizer for prompt-tuning. Labels are represented by\nprototypical embeddings in the feature space rather than by discrete words. The\ndistances between the embedding at the masked position of input and\nprototypical embeddings are used as classification criterion. For zero-shot\nsettings, knowledge is elicited from pretrained language models by a manually\ndesigned template to form initial prototypical embeddings. For few-shot\nsettings, models are tuned to learn meaningful and interpretable prototypical\nembeddings. Our method optimizes models by contrastive learning. Extensive\nexperimental results on several many-class text classification datasets with\nlow-resource settings demonstrate the effectiveness of our approach compared\nwith other verbalizer construction methods. Our implementation is available at\nhttps://github.com/Ydongd/prototypical-prompt-verbalizer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yinyi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_T/0/1/0/all/0/1\">Tong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yongtao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wen Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Stop Sets on Stopping Active Learning for Text Classification. (arXiv:2201.05460v1 [cs.IR])","link":"http://arxiv.org/abs/2201.05460","description":"<p>Active learning is an increasingly important branch of machine learning and a\npowerful technique for natural language processing. The main advantage of\nactive learning is its potential to reduce the amount of labeled data needed to\nlearn high-performing models. A vital aspect of an effective active learning\nalgorithm is the determination of when to stop obtaining additional labeled\ndata. Several leading state-of-the-art stopping methods use a stop set to help\nmake this decision. However, there has been relatively less attention given to\nthe choice of stop set than to the stopping algorithms that are applied on the\nstop set. Different choices of stop sets can lead to significant differences in\nstopping method performance. We investigate the impact of different stop set\nchoices on different stopping methods. This paper shows the choice of the stop\nset can have a significant impact on the performance of stopping methods and\nthe impact is different for stability-based methods from that on\nconfidence-based methods. Furthermore, the unbiased representative stop sets\nsuggested by original authors of methods work better than the systematically\nbiased stop sets used in recently published work, and stopping methods based on\nstabilizing predictions have stronger performance than confidence-based\nstopping methods when unbiased representative stop sets are used. We provide\nthe largest quantity of experimental results on the impact of stop sets to\ndate. The findings are important for helping to illuminate the impact of this\nimportant aspect of stopping methods that has been under-considered in recently\npublished work and that can have a large practical impact on the performance of\nstopping methods for important semantic computing applications such as\ntechnology assisted review and text classification more broadly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurlandski_L/0/1/0/all/0/1\">Luke Kurlandski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloodgood_M/0/1/0/all/0/1\">Michael Bloodgood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings. (arXiv:2201.05575v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05575","description":"<p>Previous knowledge graph embedding approaches usually map entities to\nrepresentations and utilize score functions to predict the target entities, yet\nthey struggle to reason rare or emerging unseen entities. In this paper, we\npropose kNN-KGE, a new knowledge graph embedding approach, by linearly\ninterpolating its entity distribution with k-nearest neighbors. We compute the\nnearest neighbors based on the distance in the entity embedding space from the\nknowledge store. Our approach can allow rare or emerging entities to be\nmemorized explicitly rather than implicitly in model parameters. Experimental\nresults demonstrate that our approach can improve inductive and transductive\nlink prediction results and yield better performance for low-resource settings\nwith only a few triples, which might be easier to reason via explicit memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Czech Grammar Error Correction with a Large and Diverse Corpus. (arXiv:2201.05590v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05590","description":"<p>We introduce a large and diverse Czech corpus annotated for grammatical error\ncorrection (GEC) with the aim to contribute to the still scarce data resources\nin this domain for languages other than English. The Grammar Error Correction\nCorpus for Czech (GECCC) offers a variety of four domains, covering error\ndistributions ranging from high error density essays written by non-native\nspeakers, to website texts, where errors are expected to be much less common.\nWe compare several Czech GEC systems, including several Transformer-based ones,\nsetting a strong baseline to future research. Finally, we meta-evaluate common\nGEC metrics against human judgements on our data. We make the new Czech GEC\ncorpus publicly available under the CC BY-SA 4.0 license at\n<a href=\"http://hdl.handle.net/11234/1-4639\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naplava_J/0/1/0/all/0/1\">Jakub N&#xe1;plava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Straka_M/0/1/0/all/0/1\">Milan Straka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strakova_J/0/1/0/all/0/1\">Jana Strakov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosen_A/0/1/0/all/0/1\">Alexandr Rosen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Warm Start and a Clean Crawled Corpus -- A Recipe for Good Language Models. (arXiv:2201.05601v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05601","description":"<p>We train several language models for Icelandic, including IceBERT, that\nachieve state-of-the-art performance in a variety of downstream tasks,\nincluding part-of-speech tagging, named entity recognition, grammatical error\ndetection and constituency parsing. To train the models we introduce a new\ncorpus of Icelandic text, the Icelandic Common Crawl Corpus (IC3), a collection\nof high quality texts found online by targeting the Icelandic top-level-domain\n(TLD). Several other public data sources are also collected for a total of 16GB\nof Icelandic text. To enhance the evaluation of model performance and to raise\nthe bar in baselines for Icelandic, we translate and adapt the WinoGrande\ndataset for co-reference resolution. Through these efforts we demonstrate that\na properly cleaned crawled corpus is sufficient to achieve state-of-the-art\nresults in NLP applications for low to medium resource languages, by comparison\nwith models trained on a curated corpus. We further show that initializing\nmodels using existing multilingual models can lead to state-of-the-art results\nfor some downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snaebjarnarson_V/0/1/0/all/0/1\">V&#xe9;steinn Sn&#xe6;bjarnarson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonarson_H/0/1/0/all/0/1\">Haukur Barri S&#xed;monarson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragnarsson_P/0/1/0/all/0/1\">P&#xe9;tur Orri Ragnarsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingolfsdottir_S/0/1/0/all/0/1\">Svanhv&#xed;t Ing&#xf3;lfsd&#xf3;ttir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_H/0/1/0/all/0/1\">Haukur P&#xe1;ll J&#xf3;nsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+%7B%5CTH%7Dorsteinsson_V/0/1/0/all/0/1\">Vilhj&#xe1;lmur &#xde;orsteinsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Einarsson_H/0/1/0/all/0/1\">Hafsteinn Einarsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Open Text 1.0: Public Domain News in 44 Languages. (arXiv:2201.05609v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05609","description":"<p>We present a new multilingual corpus containing text in 44 languages, many of\nwhich have relatively few existing resources for natural language processing.\nThe first release of the corpus contains over 2.7 million news articles and 1\nmillion shorter passages published between 2001--2021, collected from Voice of\nAmerica news websites. We describe our process for collecting, filtering, and\nprocessing the data. The source material is in the public domain, our\ncollection is licensed using a creative commons license (CC BY 4.0), and all\nsoftware used to create the corpus is released under the MIT License. The\ncorpus will be regularly updated as additional documents are published.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palen_Michel_C/0/1/0/all/0/1\">Chester Palen-Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">June Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Knowledge-Enhanced Text Generation. (arXiv:2010.04389v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.04389","description":"<p>The goal of text generation is to make machines express in human language. It\nis one of the most important yet challenging tasks in natural language\nprocessing (NLP). Since 2014, various neural encoder-decoder models pioneered\nby Seq2Seq have been proposed to achieve the goal by learning to map input text\nto output text. However, the input text alone often provides limited knowledge\nto generate the desired output, so the performance of text generation is still\nfar from satisfaction in many real-world scenarios. To address this issue,\nresearchers have considered incorporating various forms of knowledge beyond the\ninput text into the generation models. This research direction is known as\nknowledge-enhanced text generation. In this survey, we present a comprehensive\nreview of the research on knowledge enhanced text generation over the past five\nyears. The main content includes two parts: (i) general methods and\narchitectures for integrating knowledge into text generation; (ii) specific\ntechniques and applications according to different forms of knowledge data.\nThis survey can have broad audiences, researchers and practitioners, in\nacademia and industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaitang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AuGPT: Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models. (arXiv:2102.05126v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.05126","description":"<p>Attention-based pre-trained language models such as GPT-2 brought\nconsiderable progress to end-to-end dialogue modelling. However, they also\npresent considerable risks for task-oriented dialogue, such as lack of\nknowledge grounding or diversity. To address these issues, we introduce\nmodified training objectives for language model finetuning, and we employ\nmassive data augmentation via back-translation to increase the diversity of the\ntraining data. We further examine the possibilities of combining data from\nmultiples sources to improve performance on the target dataset. We carefully\nevaluate our contributions with both human and automatic methods. Our model\nsubstantially outperforms the baseline on the MultiWOZ data and shows\ncompetitive performance with state of the art in both automatic and human\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulhanek_J/0/1/0/all/0/1\">Jon&#xe1;&#x161; Kulh&#xe1;nek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudecek_V/0/1/0/all/0/1\">Vojt&#x11b;ch Hude&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nekvinda_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Nekvinda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recommending Metamodel Concepts during Modeling Activities with Pre-Trained Language Models. (arXiv:2104.01642v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2104.01642","description":"<p>The design of conceptually sound metamodels that embody proper semantics in\nrelation to the application domain is particularly tedious in Model-Driven\nEngineering. As metamodels define complex relationships between domain\nconcepts, it is crucial for a modeler to define these concepts thoroughly while\nbeing consistent with respect to the application domain. We propose an approach\nto assist a modeler in the design of a metamodel by recommending relevant\ndomain concepts in several modeling scenarios. Our approach does not require to\nextract knowledge from the domain or to hand-design completion rules. Instead,\nwe design a fully data-driven approach using a deep learning model that is able\nto abstract domain concepts by learning from both structural and lexical\nmetamodel properties in a corpus of thousands of independent metamodels. We\nevaluate our approach on a test set containing 166 metamodels, unseen during\nthe model training, with more than 5000 test samples. Our preliminary results\nshow that the trained model is able to provide accurate top-$5$ lists of\nrelevant recommendations for concept renaming scenarios. Although promising,\nthe results are less compelling for the scenario of the iterative construction\nof the metamodel, in part because of the conservative strategy we use to\nevaluate the recommendations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weyssow_M/0/1/0/all/0/1\">Martin Weyssow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahraoui_H/0/1/0/all/0/1\">Houari Sahraoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syriani_E/0/1/0/all/0/1\">Eugene Syriani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation. (arXiv:2109.06513v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06513","description":"<p>Dialog models can be greatly strengthened through grounding on various\nexternal information, but grounded dialog corpora are usually not naturally\naccessible. In this work, we focus on the few-shot learning for grounded dialog\ngeneration (GDG). We first propose a simple prompting method for GDG tasks,\nwhere different constructs of model input, such as the grounding source and the\nconversation context, are distinguished through continuous or discrete prompts.\nOn three typical GDG tasks, we empirically demonstrate and analyze in-depth the\neffectiveness of our method. We then conduct extensive experiments to\nthoroughly investigate how our prompting method works with different\npre-trained models. We show that prompted language models perform superiorly to\nconversational models, and further analyze various factors that influence the\neffects of prompting. Overall, our work introduces a prompt-based perspective\nto the few-shot learning for GDG tasks, and provides valuable findings and\ninsights for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Logic-Based Framework for Natural Language Inference in Dutch. (arXiv:2110.03323v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03323","description":"<p>We present a framework for deriving inference relations between Dutch\nsentence pairs. The proposed framework relies on logic-based reasoning to\nproduce inspectable proofs leading up to inference labels; its judgements are\ntherefore transparent and formally verifiable. At its core, the system is\npowered by two ${\\lambda}$-calculi, used as syntactic and semantic theories,\nrespectively. Sentences are first converted to syntactic proofs and terms of\nthe linear ${\\lambda}$-calculus using a choice of two parsers: an Alpino-based\npipeline, and Neural Proof Nets. The syntactic terms are then converted to\nsemantic terms of the simply typed ${\\lambda}$-calculus, via a set of hand\ndesigned type- and term-level transformations. Pairs of semantic terms are then\nfed to an automated theorem prover for natural logic which reasons with them\nwhile using the lexical relations found in the Open Dutch WordNet. We evaluate\nthe reasoning pipeline on the recently created Dutch natural language inference\ndataset, and achieve promising results, remaining only within a $1.1-3.2{\\%}$\nperformance margin to strong neural baselines. To the best of our knowledge,\nthe reasoning pipeline is the first logic-based system for Dutch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abzianidze_L/0/1/0/all/0/1\">Lasha Abzianidze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kogkalidis_K/0/1/0/all/0/1\">Konstantinos Kogkalidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emoji-based Co-attention Network for Microblog Sentiment Analysis. (arXiv:2110.14227v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.14227","description":"<p>Emojis are widely used in online social networks to express emotions,\nattitudes, and opinions. As emotional-oriented characters, emojis can be\nmodeled as important features of emotions towards the recipient or subject for\nsentiment analysis. However, existing methods mainly take emojis as heuristic\ninformation that fails to resolve the problem of ambiguity noise. Recent\nresearches have utilized emojis as an independent input to classify text\nsentiment but they ignore the emotional impact of the interaction between text\nand emojis. It results that the emotional semantics of emojis cannot be fully\nexplored. In this paper, we propose an emoji-based co-attention network that\nlearns the mutual emotional semantics between text and emojis on microblogs.\nOur model adopts the co-attention mechanism based on bidirectional long\nshort-term memory incorporating the text and emojis, and integrates a\nsqueeze-and-excitation block in a convolutional neural network classifier to\nincrease its sensitivity to emotional semantic features. Experimental results\nshow that the proposed method can significantly outperform several baselines\nfor sentiment analysis on short texts of social media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaowei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaodan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Honglei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emoji-aware Co-attention Network with EmoGraph2vec Model for Sentiment Anaylsis. (arXiv:2110.14636v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.14636","description":"<p>In social media platforms, emojis have an extremely high occurrence in\ncomputer-mediated communications. Many emojis are used to strengthen the\nemotional expressions and the emojis that co-occurs in a sentence also have a\nstrong sentiment connection. However, when it comes to emoji representation\nlearning, most studies have only utilized the fixed descriptions provided by\nthe Unicode Consortium, without consideration of actual usage scenario. As for\nthe sentiment analysis task, many researchers ignore the emotional impact of\nthe interaction between text and emojis. It results that the emotional\nsemantics of emojis cannot be fully explored. In this work, we propose a method\nto learn emoji representations called EmoGraph2vec and design an emoji-aware\nco-attention network that learns the mutual emotional semantics between text\nand emojis on short texts of social media. In EmoGraph2vec, we form an emoji\nco-occurrence network on real social data and enrich the semantic information\nbased on an external knowledge base EmojiNet to obtain emoji node embeddings.\nOur model designs a co-attention mechanism to incorporate the text and emojis,\nand integrates a squeeze-and-excitation (SE) block into a convolutional neural\nnetwork as a classifier. Finally, we use the transfer learning method to\nincrease converge speed and achieve higher accuracy. Experimental results show\nthat the proposed model can outperform several baselines for sentiment analysis\non benchmark datasets. Additionally, we conduct a series of ablation and\ncomparison experiments to investigate the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaowei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaodan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Honglei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy attacks for automatic speech recognition acoustic models in a federated learning framework. (arXiv:2111.03777v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.03777","description":"<p>This paper investigates methods to effectively retrieve speaker information\nfrom the personalized speaker adapted neural network acoustic models (AMs) in\nautomatic speech recognition (ASR). This problem is especially important in the\ncontext of federated learning of ASR acoustic models where a global model is\nlearnt on the server based on the updates received from multiple clients. We\npropose an approach to analyze information in neural network AMs based on a\nneural network footprint on the so-called Indicator dataset. Using this method,\nwe develop two attack models that aim to infer speaker identity from the\nupdated personalized models without access to the actual users' speech data.\nExperiments on the TED-LIUM 3 corpus demonstrate that the proposed approaches\nare very effective and can provide equal error rate (EER) of 1-2%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomashenko_N/0/1/0/all/0/1\">Natalia Tomashenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mdhaffar_S/0/1/0/all/0/1\">Salima Mdhaffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tommasi_M/0/1/0/all/0/1\">Marc Tommasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonastre_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Bonastre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making sense of electrical vehicle discussions using sentiment analysis on closely related news and user comments. (arXiv:2112.12327v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.12327","description":"<p>We used a token-wise and document-wise sentiment analysis using both\nunsupervised and supervised models applied to both news and user reviews\ndataset. And our token-wise sentiment analysis found a statistically\nsignificant difference in sentiment between the two groups (both of which were\nvery large N), our document-wise supervised sentiment analysis found no\nsignificant difference in sentiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Everts_J/0/1/0/all/0/1\">Josh Everts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xuan Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Attention Network for Stock Movements Prediction. (arXiv:2112.13593v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.13593","description":"<p>Stock prices move as piece-wise trending fluctuation rather than a purely\nrandom walk. Traditionally, the prediction of future stock movements is based\non the historical trading record. Nowadays, with the development of social\nmedia, many active participants in the market choose to publicize their\nstrategies, which provides a window to glimpse over the whole market's attitude\ntowards future movements by extracting the semantics behind social media.\nHowever, social media contains conflicting information and cannot replace\nhistorical records completely. In this work, we propose a multi-modality\nattention network to reduce conflicts and integrate semantic and numeric\nfeatures to predict future stock movements comprehensively. Specifically, we\nfirst extract semantic information from social media and estimate their\ncredibility based on posters' identity and public reputation. Then we\nincorporate the semantic from online posts and numeric features from historical\nrecords to make the trading strategy. Experimental results show that our\napproach outperforms previous methods by a significant margin in both\nprediction accuracy (61.20\\%) and trading profits (9.13\\%). It demonstrates\nthat our method improves the performance of stock movements prediction and\ninforms future research on multi-modality fusion towards stock prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shwai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shi Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TiltedBERT: Resource Adjustable Version of BERT. (arXiv:2201.03327v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03327","description":"<p>In this paper, we proposed a novel adjustable finetuning method that improves\nthe training and inference time of the BERT model on downstream tasks. In the\nproposed method, we first detect more important word vectors in each layer by\nour proposed redundancy metric and then eliminate the less important word\nvectors with our proposed strategy. In our method, the word vector elimination\nrate in each layer is controlled by the Tilt-Rate hyper-parameter, and the\nmodel learns to work with a considerably lower number of Floating Point\nOperations (FLOPs) than the original BERTbase model. Our proposed method does\nnot need any extra training steps, and also it can be generalized to other\ntransformer-based models. We perform extensive experiments that show the word\nvectors in higher layers have an impressive amount of redundancy that can be\neliminated and decrease the training and inference time. Experimental results\non extensive sentiment analysis, classification and regression datasets, and\nbenchmarks like IMDB and GLUE showed that our proposed method is effective in\nvarious datasets. By applying our method on the BERTbase model, we decrease the\ninference time up to 5.3 times with less than 0.85% accuracy degradation on\naverage. After the fine-tuning stage, the inference time of our model can be\nadjusted with our method offline-tuning property for a wide range of the\nTilt-Rate value selections. Also, we propose a mathematical speedup analysis\nthat can estimate the speedup of our method accurately. With the help of this\nanalysis, the proper Tilt-Rate value can be selected before fine-tuning or\nwhile offline-tuning stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kachuee_S/0/1/0/all/0/1\">Sajjad Kachuee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifkhani_M/0/1/0/all/0/1\">Mohammad Sharifkhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis with Deep Learning Models: A Comparative Study on a Decade of Sinhala Language Facebook Data. (arXiv:2201.03941v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03941","description":"<p>The relationship between Facebook posts and the corresponding reaction\nfeature is an interesting subject to explore and understand. To achieve this\nend, we test state-of-the-art Sinhala sentiment analysis models against a data\nset containing a decade worth of Sinhala posts with millions of reactions. For\nthe purpose of establishing benchmarks and with the goal of identifying the\nbest model for Sinhala sentiment analysis, we also test, on the same data set\nconfiguration, other deep learning models catered for sentiment analysis. In\nthis study we report that the 3 layer Bidirectional LSTM model achieves an F1\nscore of 84.58% for Sinhala sentiment analysis, surpassing the current\nstate-of-the-art model; Capsule B, which only manages to get an F1 score of\n82.04%. Further, since all the deep learning models show F1 scores above 75% we\nconclude that it is safe to claim that Facebook reactions are suitable to\npredict the sentiment of a text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weeraprameshwara_G/0/1/0/all/0/1\">Gihan Weeraprameshwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayawickrama_V/0/1/0/all/0/1\">Vihanga Jayawickrama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijeratne_Y/0/1/0/all/0/1\">Yudhanjaya Wijeratne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Automated Error Analysis: Learning to Characterize Errors. (arXiv:2201.05017v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05017","description":"<p>Characterizing the patterns of errors that a system makes helps researchers\nfocus future development on increasing its accuracy and robustness. We propose\na novel form of \"meta learning\" that automatically learns interpretable rules\nthat characterize the types of errors that a system makes, and demonstrate\nthese rules' ability to help understand and improve two NLP systems. Our\napproach works by collecting error cases on validation data, extracting\nmeta-features describing these samples, and finally learning rules that\ncharacterize errors using these features. We apply our approach to VilBERT, for\nVisual Question Answering, and RoBERTa, for Common Sense Question Answering.\nOur system learns interpretable rules that provide insights into systemic\nerrors these systems make on the given tasks. Using these insights, we are also\nable to \"close the loop\" and modestly improve performance of these systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shivang Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond J. Mooney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Resources in the Tamasheq Language. (arXiv:2201.05051v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05051","description":"<p>In this paper we present two datasets for Tamasheq, a developing language\nmainly spoken in Mali and Niger. These two datasets were made available for the\nIWSLT 2022 low-resource speech translation track, and they consist of\ncollections of radio recordings from the Studio Kalangou (Niger) and Studio\nTamani (Mali) daily broadcast news. We share (i) a massive amount of unlabeled\naudio data (671 hours) in five languages: French from Niger, Fulfulde, Hausa,\nTamasheq and Zarma, and (ii) a smaller parallel corpus of audio recordings (17\nhours) in Tamasheq, with utterance-level translations in the French language.\nAll this data is shared under the Creative Commons BY-NC-ND 3.0 license. We\nhope these resources will inspire the speech community to develop and benchmark\nmodels using the Tamasheq language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1\">Marcely Zanon Boito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bougares_F/0/1/0/all/0/1\">Fethi Bougares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbier_F/0/1/0/all/0/1\">Florentin Barbier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gahbiche_S/0/1/0/all/0/1\">Souhir Gahbiche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouvier_M/0/1/0/all/0/1\">Mickael Rouvier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grow-and-Clip: Informative-yet-Concise Evidence Distillation for Answer Explanation. (arXiv:2201.05088v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05088","description":"<p>Interpreting the predictions of existing Question Answering (QA) models is\ncritical to many real-world intelligent applications, such as QA systems for\nhealthcare, education, and finance. However, existing QA models lack\ninterpretability and provide no feedback or explanation for end-users to help\nthem understand why a specific prediction is the answer to a question. In this\nresearch, we argue that the evidences of an answer is critical to enhancing the\ninterpretability of QA models. Unlike previous research that simply extracts\nseveral sentence(s) in the context as evidence, we are the first to explicitly\ndefine the concept of evidence as the supporting facts in a context which are\ninformative, concise, and readable. Besides, we provide effective strategies to\nquantitatively measure the informativeness, conciseness and readability of\nevidence. Furthermore, we propose Grow-and-Clip Evidence Distillation (GCED)\nalgorithm to extract evidences from the contexts by trade-off informativeness,\nconciseness, and readability. We conduct extensive experiments on the SQuAD and\nTriviaQA datasets with several baseline models to evaluate the effect of GCED\non interpreting answers to questions. Human evaluation are also carried out to\ncheck the quality of distilled evidences. Experimental results show that\nautomatic distilled evidences have human-like informativeness, conciseness and\nreadability, which can enhance the interpretability of the answers to\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning Enhancement of CNNs via Separation Index Maximizing at the First Convolutional Layer. (arXiv:2201.05217v1 [cs.LG])","link":"http://arxiv.org/abs/2201.05217","description":"<p>In this paper, a straightforward enhancement learning algorithm based on\nSeparation Index (SI) concept is proposed for Convolutional Neural Networks\n(CNNs). At first, the SI as a supervised complexity measure is explained its\nusage in better learning of CNNs for classification problems illustrate. Then,\na learning strategy proposes through which the first layer of a CNN is\noptimized by maximizing the SI, and the further layers are trained through the\nbackpropagation algorithm to learn further layers. In order to maximize the SI\nat the first layer, A variant of ranking loss is optimized by using the quasi\nleast square error technique. Applying such a learning strategy to some known\nCNNs and datasets, its enhancement impact in almost all cases is demonstrated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karimi_A/0/1/0/all/0/1\">Ali Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalhor_A/0/1/0/all/0/1\">Ahmad Kalhor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Density Estimation from Schlieren Images through Machine Learning. (arXiv:2201.05233v1 [physics.flu-dyn])","link":"http://arxiv.org/abs/2201.05233","description":"<p>This study proposes a radically alternate approach for extracting\nquantitative information from schlieren images. The method uses a scaled,\nderivative enhanced Gaussian process model to obtain true density estimates\nfrom two corresponding schlieren images with the knife-edge at horizontal and\nvertical orientations. We illustrate our approach on schlieren images taken\nfrom a wind tunnel sting model, and a supersonic aircraft in flight.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Ubald_B/0/1/0/all/0/1\">Bryn Noel Ubald</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Seshadri_P/0/1/0/all/0/1\">Pranay Seshadri</a> (1 and 2), <a href=\"http://arxiv.org/find/physics/1/au:+Duncan_A/0/1/0/all/0/1\">Andrew Duncan</a> (1 and 2) ((1) The Alan Turing Institute, (2) Imperial College London)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Leaning-Based Ultra-Fast Stair Detection. (arXiv:2201.05275v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05275","description":"<p>Staircases are some of the most common building structures in urban\nenvironments. Stair detection is an important task for various applications,\nincluding the environmental perception of exoskeleton robots, humanoid robots,\nand rescue robots and the navigation of visually impaired people. Most existing\nstair detection algorithms have difficulty dealing with the diversity of stair\nstructure materials, extreme light and serious occlusion. Inspired by human\nperception, we propose an end-to-end method based on deep learning.\nSpecifically, we treat the process of stair line detection as a multitask\ninvolving coarse-grained semantic segmentation and object detection. The input\nimages are divided into cells, and a simple neural network is used to judge\nwhether each cell contains stair lines. For cells containing stair lines, the\nlocations of the stair lines relative to each cell are regressed. Extensive\nexperiments on our dataset show that our method can achieve high performance in\nterms of both speed and accuracy. A lightweight version can even achieve 300+\nframes per second with the same resolution. Our code is available at GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_Z/0/1/0/all/0/1\">Zhongcai Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shuang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiyong Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boundary-aware Self-supervised Learning for Video Scene Segmentation. (arXiv:2201.05277v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05277","description":"<p>Self-supervised learning has drawn attention through its effectiveness in\nlearning in-domain representations with no ground-truth annotations; in\nparticular, it is shown that properly designed pretext tasks (e.g., contrastive\nprediction task) bring significant performance gains for downstream tasks\n(e.g., classification task). Inspired from this, we tackle video scene\nsegmentation, which is a task of temporally localizing scene boundaries in a\nvideo, with a self-supervised learning framework where we mainly focus on\ndesigning effective pretext tasks. In our framework, we discover a\npseudo-boundary from a sequence of shots by splitting it into two continuous,\nnon-overlapping sub-sequences and leverage the pseudo-boundary to facilitate\nthe pre-training. Based on this, we introduce three novel boundary-aware\npretext tasks: 1) Shot-Scene Matching (SSM), 2) Contextual Group Matching (CGM)\nand 3) Pseudo-boundary Prediction (PP); SSM and CGM guide the model to maximize\nintra-scene similarity and inter-scene discrimination while PP encourages the\nmodel to identify transitional moments. Through comprehensive analysis, we\nempirically show that pre-training and transferring contextual representation\nare both critical to improving the video scene segmentation performance.\nLastly, we achieve the new state-of-the-art on the MovieNet-SSeg benchmark. The\ncode is available at https://github.com/kakaobrain/bassl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mun_J/0/1/0/all/0/1\">Jonghwan Mun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Minchul Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Gunsoo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Seongsu Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joonseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eun-Sol Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manifoldron: Direct Space Partition via Manifold Discovery. (arXiv:2201.05279v1 [cs.LG])","link":"http://arxiv.org/abs/2201.05279","description":"<p>A neural network with the widely-used ReLU activation has been shown to\npartition the sample space into many convex polytopes for prediction. However,\nthe parameterized way a neural network and other machine learning models use to\npartition the space has imperfections, e.g., the compromised interpretability\nfor complex models, the inflexibility in decision boundary construction due to\nthe generic character of the model, and the risk of being trapped into shortcut\nsolutions. In contrast, although the non-parameterized models can adorably\navoid or downplay these issues, they are usually insufficiently powerful either\ndue to over-simplification or the failure to accommodate the manifold\nstructures of data. In this context, we first propose a new type of machine\nlearning models referred to as Manifoldron that directly derives decision\nboundaries from data and partitions the space via manifold structure discovery.\nThen, we systematically analyze the key characteristics of the Manifoldron\nincluding interpretability, manifold characterization capability, and its link\nto neural networks. The experimental results on 9 small and 11 large datasets\ndemonstrate that the proposed Manifoldron performs competitively compared to\nthe mainstream machine learning models. We have shared our code\nhttps://github.com/wdayang/Manifoldron for free download and evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dayang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1\">Feng-Lei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Bo-Jian Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1\">Rongjie Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hengyong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Argus++: Robust Real-time Activity Detection for Unconstrained Video Streams with Overlapping Cube Proposals. (arXiv:2201.05290v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05290","description":"<p>Activity detection is one of the attractive computer vision tasks to exploit\nthe video streams captured by widely installed cameras. Although achieving\nimpressive performance, conventional activity detection algorithms are usually\ndesigned under certain constraints, such as using trimmed and/or\nobject-centered video clips as inputs. Therefore, they failed to deal with the\nmulti-scale multi-instance cases in real-world unconstrained video streams,\nwhich are untrimmed and have large field-of-views. Real-time requirements for\nstreaming analysis also mark brute force expansion of them unfeasible.\n</p>\n<p>To overcome these issues, we propose Argus++, a robust real-time activity\ndetection system for analyzing unconstrained video streams. The design of\nArgus++ introduces overlapping spatio-temporal cubes as an intermediate concept\nof activity proposals to ensure coverage and completeness of activity detection\nthrough over-sampling. The overall system is optimized for real-time processing\non standalone consumer-level hardware. Extensive experiments on different\nsurveillance and driving scenarios demonstrated its superior performance in a\nseries of activity detection benchmarks, including CVPR ActivityNet ActEV 2021,\nNIST ActEV SDL UF/KF, TRECVID ActEV 2020/2021, and ICCV ROAD 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lijun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yijun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alexander G. Hauptmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMNet: Muscle motion-guided network for micro-expression recognition. (arXiv:2201.05297v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05297","description":"<p>Facial micro-expressions (MEs) are involuntary facial motions revealing\npeoples real feelings and play an important role in the early intervention of\nmental illness, the national security, and many human-computer interaction\nsystems. However, existing micro-expression datasets are limited and usually\npose some challenges for training good classifiers. To model the subtle facial\nmuscle motions, we propose a robust micro-expression recognition (MER)\nframework, namely muscle motion-guided network (MMNet). Specifically, a\ncontinuous attention (CA) block is introduced to focus on modeling local subtle\nmuscle motion patterns with little identity information, which is different\nfrom most previous methods that directly extract features from complete video\nframes with much identity information. Besides, we design a position\ncalibration (PC) module based on the vision transformer. By adding the position\nembeddings of the face generated by PC module at the end of the two branches,\nthe PC module can help to add position information to facial muscle motion\npattern features for the MER. Extensive experiments on three public\nmicro-expression datasets demonstrate that our approach outperforms\nstate-of-the-art methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_M/0/1/0/all/0/1\">Mingzhe Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhaoqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering. (arXiv:2201.05299v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05299","description":"<p>Outside-knowledge visual question answering (OK-VQA) requires the agent to\ncomprehend the image, make use of relevant knowledge from the entire web, and\ndigest all the information to answer the question. Most previous works address\nthe problem by first fusing the image and question in the multi-modal space,\nwhich is inflexible for further fusion with a vast amount of external\nknowledge. In this paper, we call for a paradigm shift for the OK-VQA task,\nwhich transforms the image into plain text, so that we can enable knowledge\npassage retrieval, and generative question-answering in the natural language\nspace. This paradigm takes advantage of the sheer volume of gigantic knowledge\nbases and the richness of pre-trained language models. A\nTransform-Retrieve-Generate framework (TRiG) framework is proposed, which can\nbe plug-and-played with alternative image-to-text models and textual knowledge\nbases. Experimental results show that our TRiG framework outperforms all\nstate-of-the-art supervised methods by at least 11.1% absolute margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_Q/0/1/0/all/0/1\">Qing Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reganti_A/0/1/0/all/0/1\">Aishwarya Reganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Temporal Video Grounding with Deep Semantic Clustering. (arXiv:2201.05307v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05307","description":"<p>Temporal video grounding (TVG) aims to localize a target segment in a video\naccording to a given sentence query. Though respectable works have made decent\nachievements in this task, they severely rely on abundant video-query paired\ndata, which is expensive and time-consuming to collect in real-world scenarios.\nIn this paper, we explore whether a video grounding model can be learned\nwithout any paired annotations. To the best of our knowledge, this paper is the\nfirst work trying to address TVG in an unsupervised setting. Considering there\nis no paired supervision, we propose a novel Deep Semantic Clustering Network\n(DSCNet) to leverage all semantic information from the whole query set to\ncompose the possible activity in each video for grounding. Specifically, we\nfirst develop a language semantic mining module, which extracts implicit\nsemantic features from the whole query set. Then, these language semantic\nfeatures serve as the guidance to compose the activity in video via a\nvideo-based semantic aggregation module. Finally, we utilize a foreground\nattention branch to filter out the redundant background activities and refine\nthe grounding results. To validate the effectiveness of our DSCNet, we conduct\nexperiments on both ActivityNet Captions and Charades-STA datasets. The results\ndemonstrate that DSCNet achieves competitive performance, and even outperforms\nmost weakly-supervised approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yinzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1\">Xing Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_K/0/1/0/all/0/1\">Kai Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zichuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Skeleton-Based Human Activity Discovery Technique Using Particle Swarm Optimization with Gaussian Mutation. (arXiv:2201.05314v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05314","description":"<p>Human activity discovery aims to distinguish the activities performed by\nhumans, without any prior information of what defines each activity. Most\nmethods presented in human activity recognition are supervised, where there are\nlabeled inputs to train the system. In reality, it is difficult to label data\nbecause of its huge volume and the variety of activities performed by humans.\nIn this paper, a novel unsupervised approach is proposed to perform human\nactivity discovery in 3D skeleton sequences. First, important frames are\nselected based on kinetic energy. Next, the displacement of joints, set of\nstatistical, angles, and orientation features are extracted to represent the\nactivities information. Since not all extracted features have useful\ninformation, the dimension of features is reduced using PCA. Most human\nactivity discovery proposed are not fully unsupervised. They use pre-segmented\nvideos before categorizing activities. To deal with this, we used the\nfragmented sliding time window method to segment the time series of activities\nwith some overlapping. Then, activities are discovered by a novel hybrid\nparticle swarm optimization with a Gaussian mutation algorithm to avoid getting\nstuck in the local optimum. Finally, k-means is applied to the outcome\ncentroids to overcome the slow rate of PSO. Experiments on three datasets have\nbeen presented and the results show the proposed method has superior\nperformance in discovering activities in all evaluation parameters compared to\nthe other state-of-the-art methods and has increased accuracy of at least 4 %\non average. The code is available here:\nhttps://github.com/parhamhadikhani/Human-Activity-Discovery-HPGMK\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hadikhani_P/0/1/0/all/0/1\">Parham Hadikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_D/0/1/0/all/0/1\">Daphne Teck Ching Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_W/0/1/0/all/0/1\">Wee-Hong Ong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-automated Virtual Unfolded View Generation Method of Stomach from CT Volumes. (arXiv:2201.05331v1 [eess.IV])","link":"http://arxiv.org/abs/2201.05331","description":"<p>CT image-based diagnosis of the stomach is developed as a new way of\ndiagnostic method. A virtual unfolded (VU) view is suitable for displaying its\nwall. In this paper, we propose a semi-automated method for generating VU views\nof the stomach. Our method requires minimum manual operations. The\ndetermination of the unfolding forces and the termination of the unfolding\nprocess are automated. The unfolded shape of the stomach is estimated based on\nits radius. The unfolding forces are determined so that the stomach wall is\ndeformed to the expected shape. The iterative deformation process is terminated\nif the difference of the shapes between the deformed shape and expected shape\nis small. Our experiments using 67 CT volumes showed that our proposed method\ncan generate good VU views for 76.1% cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Oda_M/0/1/0/all/0/1\">Masahiro Oda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suito_T/0/1/0/all/0/1\">Tomoaki Suito</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hayashi_Y/0/1/0/all/0/1\">Yuichiro Hayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kitasaka_T/0/1/0/all/0/1\">Takayuki Kitasaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Furukawa_K/0/1/0/all/0/1\">Kazuhiro Furukawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miyahara_R/0/1/0/all/0/1\">Ryoji Miyahara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hirooka_Y/0/1/0/all/0/1\">Yoshiki Hirooka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goto_H/0/1/0/all/0/1\">Hidemi Goto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iinuma_G/0/1/0/all/0/1\">Gen Iinuma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Misawa_K/0/1/0/all/0/1\">Kazunari Misawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nawano_S/0/1/0/all/0/1\">Shigeru Nawano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mori_K/0/1/0/all/0/1\">Kensaku Mori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AWSnet: An Auto-weighted Supervision Attention Network for Myocardial Scar and Edema Segmentation in Multi-sequence Cardiac Magnetic Resonance Images. (arXiv:2201.05344v1 [eess.IV])","link":"http://arxiv.org/abs/2201.05344","description":"<p>Multi-sequence cardiac magnetic resonance (CMR) provides essential pathology\ninformation (scar and edema) to diagnose myocardial infarction. However,\nautomatic pathology segmentation can be challenging due to the difficulty of\neffectively exploring the underlying information from the multi-sequence CMR\ndata. This paper aims to tackle the scar and edema segmentation from\nmulti-sequence CMR with a novel auto-weighted supervision framework, where the\ninteractions among different supervised layers are explored under a\ntask-specific objective using reinforcement learning. Furthermore, we design a\ncoarse-to-fine framework to boost the small myocardial pathology region\nsegmentation with shape prior knowledge. The coarse segmentation model\nidentifies the left ventricle myocardial structure as a shape prior, while the\nfine segmentation model integrates a pixel-wise attention strategy with an\nauto-weighted supervision model to learn and extract salient pathological\nstructures from the multi-sequence CMR data. Extensive experimental results on\na publicly available dataset from Myocardial pathology segmentation combining\nmulti-sequence CMR (MyoPS 2020) demonstrate our method can achieve promising\nperformance compared with other state-of-the-art methods. Our method is\npromising in advancing the myocardial pathology assessment on multi-sequence\nCMR data. To motivate the community, we have made our code publicly available\nvia https://github.com/soleilssss/AWSnet/tree/master.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Kai-Ni Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miao_J/0/1/0/all/0/1\">Juzheng Miao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1\">Jing Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_P/0/1/0/all/0/1\">Ping Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_W/0/1/0/all/0/1\">Wufeng Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_G/0/1/0/all/0/1\">Guang-Quan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency Constrained Arbitrary Image Style Transfer using SIFT and DCNN. (arXiv:2201.05346v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05346","description":"<p>This paper develops a new image synthesis approach to transfer an example\nimage (style image) to other images (content images) by using Deep\nConvolutional Neural Networks (DCNN) model. When common neural style transfer\nmethods are used, the textures and colors in the style image are usually\ntransferred imperfectly to the content image, or some visible errors are\ngenerated. This paper proposes a novel saliency constrained method to reduce or\navoid such effects. It first evaluates some existing saliency detection methods\nto select the most suitable one for use in our method. The selected saliency\ndetection method is used to detect the object in the style image, corresponding\nto the object of the content image with the same saliency. In addition, aim to\nsolve the problem that the size or resolution is different in the style image\nand content, the scale-invariant feature transform is used to generate a series\nof style images and content images which can be used to generate more feature\nmaps for patches matching. It then proposes a new loss function combining the\nsaliency loss, style loss and content loss, adding gradient of saliency\nconstraint into style transfer in iterations. Finally the source images and\nsaliency detection results are utilized as multichannel input to an improved\ndeep CNN framework for style transfer. The experiments show that the saliency\nmaps of source images can help find the correct matching and avoid artifacts.\nExperimental results on different kind of images demonstrate that our method\noutperforms nine representative methods from recent publications and has good\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">HuiHuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Deep Hybrid Boosted and Ensemble Learning-based Brain Tumor Analysis using MRI. (arXiv:2201.05373v1 [eess.IV])","link":"http://arxiv.org/abs/2201.05373","description":"<p>Brain tumors analysis is important in timely diagnosis and effective\ntreatment to cure patients. Tumor analysis is challenging because of tumor\nmorphology like size, location, texture, and heteromorphic appearance in the\nmedical images. In this regard, a novel two-phase deep learning-based framework\nis proposed to detect and categorize brain tumors in magnetic resonance images\n(MRIs). In the first phase, a novel deep boosted features and ensemble\nclassifiers (DBF-EC) scheme is proposed to detect tumor MRI images from healthy\nindividuals effectively. The deep boosted feature space is achieved through the\ncustomized and well-performing deep convolutional neural networks (CNNs), and\nconsequently, fed into the ensemble of machine learning (ML) classifiers. While\nin the second phase, a new hybrid features fusion-based brain tumor\nclassification approach is proposed, comprised of dynamic-static feature and ML\nclassifier to categorize different tumor types. The dynamic features are\nextracted from the proposed BRAIN-RENet CNN, which carefully learns\nheteromorphic and inconsistent behavior of various tumors, while the static\nfeatures are extracted using HOG. The effectiveness of the proposed two-phase\nbrain tumor analysis framework is validated on two standard benchmark datasets;\ncollected from Kaggle and Figshare containing different types of tumor,\nincluding glioma, meningioma, pituitary, and normal images. Experimental\nresults proved that the proposed DBF-EC detection scheme outperforms and\nachieved accuracy (99.56%), precision (0.9991), recall (0.9899), F1-Score\n(0.9945), MCC (0.9892), and AUC-PR (0.9990). While the classification scheme,\nthe joint employment of the deep features fusion of proposed BRAIN-RENet and\nHOG features improves performance significantly in terms of recall (0.9913),\nprecision (0.9906), F1-Score (0.9909), and accuracy (99.20%) on diverse\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zahoor_M/0/1/0/all/0/1\">Mirza Mumtaz Zahoor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qureshi_S/0/1/0/all/0/1\">Shahzad Ahmad Qureshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1\">Saddam Hussain Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1\">Asifullah Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SRVIO: Super Robust Visual Inertial Odometry for dynamic environments and challenging Loop-closure conditions. (arXiv:2201.05386v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05386","description":"<p>The visual localization or odometry problem is a well-known challenge in the\nfield of autonomous robots and cars. Traditionally, this problem can ba tackled\nwith the help of expensive sensors such as lidars. Nowadays, the leading\nresearch is on robust localization using economic sensors, such as cameras and\nIMUs. The geometric methods based on these sensors are pretty good in normal\nconditions withstable lighting and no dynamic objects. These methods suffer\nfrom significant loss and divergence in such challenging environments. The\nscientists came to use deep neural networks (DNNs) as the savior to mitigate\nthis problem. The main idea behind using DNNs was to better understand the\nproblem inside the data and overcome complex conditions (such as a dynamic\nobject in front of the camera, extreme lighting conditions, keeping the track\nat high speeds, etc.) The prior endto-end DNN methods are able to overcome some\nof the mentioned challenges. However, no general and robust framework for all\nof these scenarios is available. In this paper, we have combined geometric and\nDNN based methods to have the pros of geometric SLAM frameworks and overcome\nthe remaining challenges with the DNNs help. To do this, we have modified the\nVins-Mono framework (the most robust and accurate framework till now) and we\nwere able to achieve state-of-the-art results on TUM-Dynamic, TUM-VI, ADVIO and\nEuRoC datasets compared to geometric and end-to-end DNN based SLAMs. Our\nproposed framework was also able to achieve acceptable results on extreme\nsimulated cases resembling the challenges mentioned earlier easy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samadzadeh_A/0/1/0/all/0/1\">Ali Samadzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickabadi_A/0/1/0/all/0/1\">Ahmad Nickabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HardBoost: Boosting Zero-Shot Learning with Hard Classes. (arXiv:2201.05479v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05479","description":"<p>This work is a systematical analysis on the so-called hard class problem in\nzero-shot learning (ZSL), that is, some unseen classes disproportionally affect\nthe ZSL performances than others, as well as how to remedy the problem by\ndetecting and exploiting hard classes. At first, we report our empirical\nfinding that the hard class problem is a ubiquitous phenomenon and persists\nregardless of used specific methods in ZSL. Then, we find that high semantic\naffinity among unseen classes is a plausible underlying cause of hardness and\ndesign two metrics to detect hard classes. Finally, two frameworks are proposed\nto remedy the problem by detecting and exploiting hard classes, one under\ninductive setting, the other under transductive setting. The proposed\nframeworks could accommodate most existing ZSL methods to further significantly\nboost their performances with little efforts. Extensive experiments on three\npopular benchmarks demonstrate the benefits by identifying and exploiting the\nhard classes in ZSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lihua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiulei Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergence of Machine Language: Towards Symbolic Intelligence with Neural Networks. (arXiv:2201.05489v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05489","description":"<p>Representation is a core issue in artificial intelligence. Humans use\ndiscrete language to communicate and learn from each other, while machines use\ncontinuous features (like vector, matrix, or tensor in deep neural networks) to\nrepresent cognitive patterns. Discrete symbols are low-dimensional, decoupled,\nand have strong reasoning ability, while continuous features are\nhigh-dimensional, coupled, and have incredible abstracting capabilities. In\nrecent years, deep learning has developed the idea of continuous representation\nto the extreme, using millions of parameters to achieve high accuracies.\nAlthough this is reasonable from the statistical perspective, it has other\nmajor problems like lacking interpretability, poor generalization, and is easy\nto be attacked. Since both paradigms have strengths and weaknesses, a better\nchoice is to seek reconciliation. In this paper, we make an initial attempt\ntowards this direction. Specifically, we propose to combine symbolism and\nconnectionism principles by using neural networks to derive a discrete\nrepresentation. This process is highly similar to human language, which is a\nnatural combination of discrete symbols and neural systems, where the brain\nprocesses continuous signals and represents intelligence via discrete language.\nTo mimic this functionality, we denote our approach as machine language. By\ndesigning an interactive environment and task, we demonstrated that machines\ncould generate a spontaneous, flexible, and semantic language through\ncooperation. Moreover, through experiments we show that discrete language\nrepresentation has several advantages compared with continuous feature\nrepresentation, from the aspects of interpretability, generalization, and\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xu-Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng-Lin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Determination of building flood risk maps from LiDAR mobile mapping data. (arXiv:2201.05514v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05514","description":"<p>With increasing urbanization, flooding is a major challenge for many cities\ntoday. Based on forecast precipitation, topography, and pipe networks, flood\nsimulations can provide early warnings for areas and buildings at risk of\nflooding. Basement windows, doors, and underground garage entrances are common\nplaces where floodwater can flow into a building. Some buildings have been\nprepared or designed considering the threat of flooding, but others have not.\nTherefore, knowing the heights of these facade openings helps to identify\nplaces that are more susceptible to water ingress. However, such data is not\nyet readily available in most cities. Traditional surveying of the desired\ntargets may be used, but this is a very time-consuming and laborious process.\nThis research presents a new process for the extraction of windows and doors\nfrom LiDAR mobile mapping data. Deep learning object detection models are\ntrained to identify these objects. Usually, this requires to provide large\namounts of manual annotations. In this paper, we mitigate this problem by\nleveraging a rule-based method. In a first step, the rule-based method is used\nto generate pseudo-labels. A semi-supervised learning strategy is then applied\nwith three different levels of supervision. The results show that using only\nautomatically generated pseudo-labels, the learning-based model outperforms the\nrule-based approach by 14.6% in terms of F1-score. After five hours of human\nsupervision, it is possible to improve the model by another 6.2%. By comparing\nthe detected facade openings' heights with the predicted water levels from a\nflood simulation model, a map can be produced which assigns per-building flood\nrisk levels. This information can be combined with flood forecasting to provide\na more targeted disaster prevention guide for the city's infrastructure and\nresidential buildings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1\">Qing Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_C/0/1/0/all/0/1\">Claus Brenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peche_A/0/1/0/all/0/1\">Aaron Peche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Juntao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feuerhake_U/0/1/0/all/0/1\">Udo Feuerhake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sester_M/0/1/0/all/0/1\">Monika Sester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViT2Hash: Unsupervised Information-Preserving Hashing. (arXiv:2201.05541v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05541","description":"<p>Unsupervised image hashing, which maps images into binary codes without\nsupervision, is a compressor with a high compression rate. Hence, how to\npreserving meaningful information of the original data is a critical problem.\nInspired by the large-scale vision pre-training model, known as ViT, which has\nshown significant progress for learning visual representations, in this paper,\nwe propose a simple information-preserving compressor to finetune the ViT model\nfor the target unsupervised hashing task. Specifically, from pixels to\ncontinuous features, we first propose a feature-preserving module, using the\ncorrupted image as input to reconstruct the original feature from the\npre-trained ViT model and the complete image, so that the feature extractor can\nfocus on preserving the meaningful information of original data. Secondly, from\ncontinuous features to hash codes, we propose a hashing-preserving module,\nwhich aims to keep the semantic information from the pre-trained ViT model by\nusing the proposed Kullback-Leibler divergence loss. Besides, the quantization\nloss and the similarity loss are added to minimize the quantization error. Our\nmethod is very simple and achieves a significantly higher degree of MAP on\nthree benchmark image datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Q/0/1/0/all/0/1\">Qinkang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liangdao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Hanjiang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal registration of FISH and nanoSIMS images using convolutional neural network models. (arXiv:2201.05545v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05545","description":"<p>Nanoscale secondary ion mass spectrometry (nanoSIMS) and fluorescence in situ\nhybridization (FISH) microscopy provide high-resolution, multimodal image\nrepresentations of the identity and cell activity respectively of targeted\nmicrobial communities in microbiological research. Despite its importance to\nmicrobiologists, multimodal registration of FISH and nanoSIMS images is\nchallenging given the morphological distortion and background noise in both\nimages. In this study, we use convolutional neural networks (CNNs) for\nmultiscale feature extraction, shape context for computation of the minimum\ntransformation cost feature matching and the thin-plate spline (TPS) model for\nmultimodal registration of the FISH and nanoSIMS images. All the six tested CNN\nmodels, VGG16, VGG19, GoogLeNet and ShuffleNet, ResNet18 and ResNet101\nperformed well, demonstrating the utility of CNNs in the registration of\nmultimodal images with significant background noise and morphology distortion.\nWe also show aggregate shape preserved by binarization to be a robust feature\nfor registering multimodal microbiology-related images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaojia He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meile_C/0/1/0/all/0/1\">Christof Meile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandarkar_S/0/1/0/all/0/1\">Suchendra M. Bhandarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HYLDA: End-to-end Hybrid Learning Domain Adaptation for LiDAR Semantic Segmentation. (arXiv:2201.05585v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05585","description":"<p>In this paper we address the problem of training a LiDAR semantic\nsegmentation network using a fully-labeled source dataset and a target dataset\nthat only has a small number of labels. To this end, we develop a novel\nimage-to-image translation engine, and couple it with a LiDAR semantic\nsegmentation network, resulting in an integrated domain adaptation architecture\nwe call HYLDA. To train the system end-to-end, we adopt a diverse set of\nlearning paradigms, including 1) self-supervision on a simple auxiliary\nreconstruction task, 2) semi-supervised training using a few available labeled\ntarget domain frames, and 3) unsupervised training on the fake translated\nimages generated by the image-to-image translation stage, together with the\nlabeled frames from the source domain. In the latter case, the semantic\nsegmentation network participates in the updating of the image-to-image\ntranslation engine. We demonstrate experimentally that HYLDA effectively\naddresses the challenging problem of improving generalization on validation\ndata from the target domain when only a few target labeled frames are available\nfor training. We perform an extensive evaluation where we compare HYLDA against\nstrong baseline methods using two publicly available LiDAR semantic\nsegmentation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corral_Soto_E/0/1/0/all/0/1\">Eduardo R. Corral-Soto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rochan_M/0/1/0/all/0/1\">Mrigank Rochan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yannis Y. He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aich_S/0/1/0/all/0/1\">Shubhra Aich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingbing_L/0/1/0/all/0/1\">Liu Bingbing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When less is more: Simplifying inputs aids neural network understanding. (arXiv:2201.05610v1 [cs.LG])","link":"http://arxiv.org/abs/2201.05610","description":"<p>How do neural network image classifiers respond to simpler and simpler\ninputs? And what do such responses reveal about the learning process? To answer\nthese questions, we need a clear measure of input simplicity (or inversely,\ncomplexity), an optimization objective that correlates with simplification, and\na framework to incorporate such objective into training and inference. Lastly\nwe need a variety of testbeds to experiment and evaluate the impact of such\nsimplification on learning. In this work, we measure simplicity with the\nencoding bit size given by a pretrained generative model, and minimize the bit\nsize to simplify inputs in training and inference. We investigate the effect of\nsuch simplification in several scenarios: conventional training, dataset\ncondensation and post-hoc explanations. In all settings, inputs are simplified\nalong with the original classification task, and we investigate the trade-off\nbetween input simplicity and task performance. For images with injected\ndistractors, such simplification naturally removes superfluous information. For\ndataset condensation, we find that inputs can be simplified with almost no\naccuracy degradation. When used in post-hoc explanation, our learning-based\nsimplification approach offers a valuable new tool to explore the basis of\nnetwork decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1\">Robin Tibor Schirrmeister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rosanne Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1\">Tonio Ball</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Networks with pixels embedding: a method to improve noise resistance in images classification. (arXiv:2005.11679v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.11679","description":"<p>In the task of image classification, usually, the network is sensitive to\nnoises. For example, an image of cat with noises might be misclassified as an\nostrich. Conventionally, to overcome the problem of noises, one uses the\ntechnique of data augmentation, that is, to teach the network to distinguish\nnoises by adding more images with noises in the training dataset. In this work,\nwe provide a noise-resistance network in images classification by introducing a\ntechnique of pixel embedding. We test the network with pixel embedding, which\nis abbreviated as the network with PE, on the mnist database of handwritten\ndigits. It shows that the network with PE outperforms the conventional network\non images with noises. The technique of pixel embedding can be used in many\ntasks of image classification to improve noise resistance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_H/0/1/0/all/0/1\">Hai-Long Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chi-Chun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fu-Lin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"THIN: THrowable Information Networks and Application for Facial Expression Recognition In The Wild. (arXiv:2010.07614v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.07614","description":"<p>For a number of machine learning problems, an exogenous variable can be\nidentified such that it heavily influences the appearance of the different\nclasses, and an ideal classifier should be invariant to this variable. An\nexample of such exogenous variable is identity if facial expression recognition\n(FER) is considered. In this paper, we propose a dual exogenous/endogenous\nrepresentation. The former captures the exogenous variable whereas the second\none models the task at hand (e.g. facial expression). We design a prediction\nlayer that uses a tree-gated deep ensemble conditioned by the exogenous\nrepresentation. We also propose an exogenous dispelling loss to remove the\nexogenous information from the endogenous representation. Thus, the exogenous\ninformation is used two times in a throwable fashion, first as a conditioning\nvariable for the target task, and second to create invariance within the\nendogenous representation. We call this method THIN, standing for THrowable\nInformation Networks. We experimentally validate THIN in several contexts where\nan exogenous information can be identified, such as digit recognition under\nlarge rotations and shape recognition at multiple scales. We also apply it to\nFER with identity as the exogenous variable. We demonstrate that THIN\nsignificantly outperforms state-of-the-art approaches on several challenging\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnaud_E/0/1/0/all/0/1\">Estephe Arnaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1\">Arnaud Dapogny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailly_K/0/1/0/all/0/1\">Kevin Bailly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Bird's-Eye View of Road Semantics using an Onboard Camera. (arXiv:2012.03040v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.03040","description":"<p>Autonomous navigation requires scene understanding of the action-space to\nmove or anticipate events. For planner agents moving on the ground plane, such\nas autonomous vehicles, this translates to scene understanding in the\nbird's-eye view (BEV). However, the onboard cameras of autonomous cars are\ncustomarily mounted horizontally for a better view of the surrounding. In this\nwork, we study scene understanding in the form of online estimation of semantic\nBEV maps using the video input from a single onboard camera. We study three key\naspects of this task, image-level understanding, BEV level understanding, and\nthe aggregation of temporal information. Based on these three pillars we\npropose a novel architecture that combines these three aspects. In our\nextensive experiments, we demonstrate that the considered aspects are\ncomplementary to each other for BEV understanding. Furthermore, the proposed\narchitecture significantly surpasses the current state-of-the-art. Code:\nhttps://github.com/ybarancan/BEV_feat_stitch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Can_Y/0/1/0/all/0/1\">Yigit Baran Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_O/0/1/0/all/0/1\">Ozan Unal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPICE: Semantic Pseudo-labeling for Image Clustering. (arXiv:2103.09382v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09382","description":"<p>The similarity among samples and the discrepancy between clusters are two\ncrucial aspects of image clustering. However, current deep clustering methods\nsuffer from the inaccurate estimation of either feature similarity or semantic\ndiscrepancy. In this paper, we present a Semantic Pseudo-labeling-based Image\nClustEring (SPICE) framework, which divides the clustering network into a\nfeature model for measuring the instance-level similarity and a clustering head\nfor identifying the cluster-level discrepancy. We design two semantics-aware\npseudo-labeling algorithms, prototype pseudo-labeling, and reliable\npseudo-labeling, which enable accurate and reliable self-supervision over\nclustering. Without using any ground-truth label, we optimize the clustering\nnetwork in three stages: 1) train the feature model through contrastive\nlearning to measure the instance similarity, 2) train the clustering head with\nthe prototype pseudo-labeling algorithm to identify cluster semantics, and 3)\njointly train the feature model and clustering head with the reliable\npseudo-labeling algorithm to improve the clustering performance. Extensive\nexperimental results demonstrate that SPICE achieves significant improvements\n(~10%) over existing methods and establishes the new state-of-the-art\nclustering results on six image benchmark datasets in terms of three popular\nmetrics. Importantly, SPICE significantly reduces the gap between unsupervised\nand fully-supervised classification; e.g., there is only a 2% (91.8% vs 93.8%)\naccuracy difference on CIFAR-10. Our code has been made publically available at\nhttps://github.com/niuchuangnn/SPICE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Hongming Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly supervised segmentation with cross-modality equivariant constraints. (arXiv:2104.02488v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02488","description":"<p>Weakly supervised learning has emerged as an appealing alternative to\nalleviate the need for large labeled datasets in semantic segmentation. Most\ncurrent approaches exploit class activation maps (CAMs), which can be generated\nfrom image-level annotations. Nevertheless, resulting maps have been\ndemonstrated to be highly discriminant, failing to serve as optimal proxy\npixel-level labels. We present a novel learning strategy that leverages\nself-supervision in a multi-modal image scenario to significantly enhance\noriginal CAMs. In particular, the proposed method is based on two observations.\nFirst, the learning of fully-supervised segmentation networks implicitly\nimposes equivariance by means of data augmentation, whereas this implicit\nconstraint disappears on CAMs generated with image tags. And second, the\ncommonalities between image modalities can be employed as an efficient\nself-supervisory signal, correcting the inconsistency shown by CAMs obtained\nacross multiple modalities. To effectively train our model, we integrate a\nnovel loss function that includes a within-modality and a cross-modality\nequivariant term to explicitly impose these constraints during training. In\naddition, we add a KL-divergence on the class prediction distributions to\nfacilitate the information exchange between modalities, which, combined with\nthe equivariant regularizers further improves the performance of our model.\nExhaustive experiments on the popular multi-modal BRATS dataset demonstrate\nthat our approach outperforms relevant recent literature under the same\nlearning conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_G/0/1/0/all/0/1\">Gaurav Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PDO-eS2CNNs: Partial Differential Operator Based Equivariant Spherical CNNs. (arXiv:2104.03584v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03584","description":"<p>Spherical signals exist in many applications, e.g., planetary data, LiDAR\nscans and digitalization of 3D objects, calling for models that can process\nspherical data effectively. It does not perform well when simply projecting\nspherical data into the 2D plane and then using planar convolution neural\nnetworks (CNNs), because of the distortion from projection and ineffective\ntranslation equivariance. Actually, good principles of designing spherical CNNs\nare avoiding distortions and converting the shift equivariance property in\nplanar CNNs to rotation equivariance in the spherical domain. In this work, we\nuse partial differential operators (PDOs) to design a spherical equivariant\nCNN, PDO-eS2CNN, which is exactly rotation equivariant in the continuous\ndomain. We then discretize PDO-eS2CNNs, and analyze the equivariance error\nresulted from discretization. This is the first time that the equivariance\nerror is theoretically analyzed in the spherical domain. In experiments,\nPDO-eS2CNNs show greater parameter efficiency and outperform other spherical\nCNNs significantly on several tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhengyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tiancheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jinwen Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransVG: End-to-End Visual Grounding with Transformers. (arXiv:2104.08541v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08541","description":"<p>In this paper, we present a neat yet effective transformer-based framework\nfor visual grounding, namely TransVG, to address the task of grounding a\nlanguage query to the corresponding region onto an image. The state-of-the-art\nmethods, including two-stage or one-stage ones, rely on a complex module with\nmanually-designed mechanisms to perform the query reasoning and multi-modal\nfusion. However, the involvement of certain mechanisms in fusion module design,\nsuch as query decomposition and image scene graph, makes the models easily\noverfit to datasets with specific scenarios, and limits the plenitudinous\ninteraction between the visual-linguistic context. To avoid this caveat, we\npropose to establish the multi-modal correspondence by leveraging transformers,\nand empirically show that the complex fusion modules e.g., modular attention\nnetwork, dynamic graph, and multi-modal tree) can be replaced by a simple stack\nof transformer encoder layers with higher performance. Moreover, we\nre-formulate the visual grounding as a direct coordinates regression problem\nand avoid making predictions out of a set of candidates i.e., region proposals\nor anchor boxes). Extensive experiments are conducted on five widely used\ndatasets, and a series of state-of-the-art records are set by our TransVG. We\nbuild the benchmark of transformer-based visual grounding framework and make\nthe code available at \\url{https://github.com/djiajunustc/TransVG}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAGAN: Text-To-Image Generation with Combined Attention GANs. (arXiv:2104.12663v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12663","description":"<p>Generating images according to natural language descriptions is a challenging\ntask. Prior research has mainly focused to enhance the quality of generation by\ninvestigating the use of spatial attention and/or textual attention thereby\nneglecting the relationship between channels. In this work, we propose the\nCombined Attention Generative Adversarial Network (CAGAN) to generate\nphoto-realistic images according to textual descriptions. The proposed CAGAN\nutilises two attention models: word attention to draw different sub-regions\nconditioned on related words; and squeeze-and-excitation attention to capture\nnon-linear interaction among channels. With spectral normalisation to stabilise\ntraining, our proposed CAGAN improves the state of the art on the IS and FID on\nthe CUB dataset and the FID on the more challenging COCO dataset. Furthermore,\nwe demonstrate that judging a model by a single evaluation metric can be\nmisleading by developing an additional model adding local self-attention which\nscores a higher IS, outperforming the state of the art on the CUB dataset, but\ngenerates unrealistic images through feature repetition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schulze_H/0/1/0/all/0/1\">Henning Schulze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1\">Dogucan Yaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Brain Tumours in MR Images using Deep Spatiospatial Models. (arXiv:2105.14071v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.14071","description":"<p>A brain tumour is a mass or cluster of abnormal cells in the brain, which has\nthe possibility of becoming life-threatening because of its ability to invade\nneighbouring tissues and also form metastases. An accurate diagnosis is\nessential for successful treatment planning and magnetic resonance imaging is\nthe principal imaging modality for diagnostic of brain tumours and their\nextent. Deep Learning methods in computer vision applications have shown\nsignificant improvement in recent years, most of which can be credited to the\nfact that a sizeable amount of data is available to train models on, and the\nimprovements in the model architectures yielding better approximations in a\nsupervised setting. Classifying tumours using such deep learning methods has\nmade significant progress with the availability of open datasets with reliable\nannotations. Typically those methods are either 3D models, which use 3D\nvolumetric MRIs or even 2D models considering each slice separately. However,\nby treating the slice spatial dimension separately, spatiotemporal models can\nbe employed as spatiospatial models for this task. These models have the\ncapabilities of learning specific spatial and temporal relationship, while\nreducing computational costs. This paper uses two spatiotemporal models, ResNet\n(2+1)D and ResNet Mixed Convolution, to classify different types of brain\ntumours. It was observed that both these models performed superior to the pure\n3D convolutional model, ResNet18. Furthermore, it was also observed that\npre-training the models on a different, even unrelated dataset before training\nthem for the task of tumour classification improves the performance. Finally,\nPre-trained ResNet Mixed Convolution was observed to be the best model in these\nexperiments, achieving a macro F1-score of 0.93 and a test accuracy of 96.98\\%,\nwhile at the same time being the model with the least computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumick Chatterjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nizamani_F/0/1/0/all/0/1\">Faraz Ahmed Nizamani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Speck_O/0/1/0/all/0/1\">Oliver Speck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style. (arXiv:2106.04619v4 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2106.04619","description":"<p>Self-supervised representation learning has shown remarkable success in a\nnumber of domains. A common practice is to perform data augmentation via\nhand-crafted transformations intended to leave the semantics of the data\ninvariant. We seek to understand the empirical success of this approach from a\ntheoretical perspective. We formulate the augmentation process as a latent\nvariable model by postulating a partition of the latent representation into a\ncontent component, which is assumed invariant to augmentation, and a style\ncomponent, which is allowed to change. Unlike prior work on disentanglement and\nindependent component analysis, we allow for both nontrivial statistical and\ncausal dependencies in the latent space. We study the identifiability of the\nlatent representation based on pairs of views of the observations and prove\nsufficient conditions that allow us to identify the invariant content partition\nup to an invertible mapping in both generative and discriminative settings. We\nfind numerical simulations with dependent latent variables are consistent with\nour theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional,\nvisually complex images with rich causal dependencies, which we use to study\nthe effect of data augmentations performed in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gresele_L/0/1/0/all/0/1\">Luigi Gresele</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Besserve_M/0/1/0/all/0/1\">Michel Besserve</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of cell-level anomalies in electroluminescence images of photovoltaic modules. (arXiv:2106.10962v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10962","description":"<p>In the operation &amp; maintenance (O&amp;M) of photovoltaic (PV) plants, the early\nidentification of failures has become crucial to maintain productivity and\nprolong components' life. Of all defects, cell-level anomalies can lead to\nserious failures and may affect surrounding PV modules in the long run. These\nfine defects are usually captured with high spatial resolution\nelectroluminescence (EL) imaging. The difficulty of acquiring such images has\nlimited the availability of data. For this work, multiple data resources and\naugmentation techniques have been used to surpass this limitation. Current\nstate-of-the-art detection methods extract barely low-level information from\nindividual PV cell images, and their performance is conditioned by the\navailable training data. In this article, we propose an end-to-end deep\nlearning pipeline that detects, locates and segments cell-level anomalies from\nentire photovoltaic modules via EL images. The proposed modular pipeline\ncombines three deep learning techniques: 1. object detection (modified\nFaster-RNN), 2. image classification (EfficientNet) and 3. weakly supervised\nsegmentation (autoencoder). The modular nature of the pipeline allows to\nupgrade the deep learning models to the further improvements in the\nstate-of-the-art and also extend the pipeline towards new functionalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Otamendi_U/0/1/0/all/0/1\">Urtzi Otamendi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_I/0/1/0/all/0/1\">I&#xf1;igo Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quartulli_M/0/1/0/all/0/1\">Marco Quartulli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olaizola_I/0/1/0/all/0/1\">Igor G. Olaizola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viles_E/0/1/0/all/0/1\">Elisabeth Viles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambarau_W/0/1/0/all/0/1\">Werther Cambarau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Explainability: A Tutorial on Gradient-Based Attribution Methods for Deep Neural Networks. (arXiv:2107.11400v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.11400","description":"<p>With the rise of deep neural networks, the challenge of explaining the\npredictions of these networks has become increasingly recognized. While many\nmethods for explaining the decisions of deep neural networks exist, there is\ncurrently no consensus on how to evaluate them. On the other hand, robustness\nis a popular topic for deep learning research; however, it is hardly talked\nabout in explainability until very recently. In this tutorial paper, we start\nby presenting gradient-based interpretability methods. These techniques use\ngradient signals to assign the burden of the decision on the input features.\nLater, we discuss how gradient-based methods can be evaluated for their\nrobustness and the role that adversarial robustness plays in having meaningful\nexplanations. We also discuss the limitations of gradient-based methods.\nFinally, we present the best practices and attributes that should be examined\nbefore choosing an explainability method. We conclude with the future\ndirections for research in the area at the convergence of robustness and\nexplainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_I/0/1/0/all/0/1\">Ian E. Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dera_D/0/1/0/all/0/1\">Dimah Dera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1\">Ghulam Rasool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1\">Nidhal Bouaynaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_R/0/1/0/all/0/1\">Ravi P. Ramachandran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects. (arXiv:2108.07368v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.07368","description":"<p>Segmenting medical images accurately and reliably is important for disease\ndiagnosis and treatment. It is a challenging task because of the wide variety\nof objects' sizes, shapes, and scanning modalities. Recently, many\nconvolutional neural networks (CNN) have been designed for segmentation tasks\nand achieved great success. Few studies, however, have fully considered the\nsizes of objects, and thus most demonstrate poor performance for small objects\nsegmentation. This can have a significant impact on the early detection of\ndiseases. This paper proposes a Context Axial Reserve Attention Network\n(CaraNet) to improve the segmentation performance on small objects compared\nwith several recent state-of-the-art models. We test our CaraNet on brain tumor\n(BraTS 2018) and polyp (Kvasir-SEG, CVC-ColonDB, CVC-ClinicDB, CVC-300, and\nETIS-LaribPolypDB) segmentation datasets. Our CaraNet achieves the top-rank\nmean Dice segmentation accuracy, and results show a distinct advantage of\nCaraNet in the segmentation of small medical objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lou_A/0/1/0/all/0/1\">Ange Lou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_S/0/1/0/all/0/1\">Shuyue Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ko_H/0/1/0/all/0/1\">Hanseok Ko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loew_M/0/1/0/all/0/1\">Murray Loew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning 3D Mineral Prospectivity from 3D Geological Models Using Convolutional Neural Networks: Application to a Structure-controlled Hydrothermal Gold Deposit. (arXiv:2109.00756v2 [physics.geo-ph] UPDATED)","link":"http://arxiv.org/abs/2109.00756","description":"<p>The three-dimensional (3D) geological models are the typical and key data\nsource in the 3D mineral prospecitivity modeling. Identifying\nprospectivity-informative predictor variables from the 3D geological models is\na challenging and tedious task. Motivated by the ability of convolutional\nneural networks (CNNs) to learn the intrinsic features, in this paper, we\npresent a novel method that leverages CNNs to learn 3D mineral prospectivity\nfrom the 3D geological models. By exploiting the learning ability of CNNs, the\npresented method allows for disentangling complex correlation to the\nmineralization and thus opens a door to circumvent the tedious work for\ndesigning the predictor variables. Specifically, to explore the unstructured 3D\ngeological models with the CNNs whose input should be structured, we develop a\n2D CNN framework in which the geometry of geological boundary is compiled and\nreorganized into multi-channel images and fed into the CNN. This ensures an\neffective and efficient training of CNNs while allowing the prospective model\nto approximate the ore-forming process. The presented method is applied to a\ntypical structure-controlled hydrothermal deposit, the Dayingezhuang gold\ndeposit, eastern China, in which the presented method was compared with the\nprospectivity modeling methods using hand-designed predictor variables. The\nresults demonstrate the presented method capacitates a performance boost of the\n3D prospectivity modeling and empowers us to decrease work-load and prospecting\nrisk in prediction of deep-seated orebodies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Deng_H/0/1/0/all/0/1\">Hao Deng</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Zheng_Y/0/1/0/all/0/1\">Yang Zheng</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Chen_J/0/1/0/all/0/1\">Jin Chen</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Yu_S/0/1/0/all/0/1\">Shuyan Yu</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Xiao_K/0/1/0/all/0/1\">Keyan Xiao</a> (2), <a href=\"http://arxiv.org/find/physics/1/au:+Mao_X/0/1/0/all/0/1\">Xiancheng Mao</a> (1) ((1) Centural South University, (2) Chinese Academy of Geological Sciences)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-token Modeling with Conditional Computation. (arXiv:2109.02008v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.02008","description":"<p>Mixture-of-Experts (MoE), a conditional computation architecture, achieved\npromising performance by scaling local module (i.e. feed-forward network) of\ntransformer. However, scaling the cross-token module (i.e. self-attention) is\nchallenging due to the unstable training. This work proposes Sparse-MLP, an\nall-MLP model which applies sparsely-activated MLPs to cross-token modeling.\nSpecifically, in each Sparse block of our all-MLP model, we apply two stages of\nMoE layers: one with MLP experts mixing information within channels along image\npatch dimension, the other with MLP experts mixing information within patches\nalong the channel dimension. In addition, by proposing importance-score routing\nstrategy for MoE and redesigning the image representation shape, we further\nimprove our model's computational efficiency. Experimentally, we are more\ncomputation-efficient than Vision Transformers with comparable accuracy. Also,\nour models can outperform MLP-Mixer by 2.5\\% on ImageNet Top-1 accuracy with\nfewer parameters and computational cost. On downstream tasks, i.e. Cifar10 and\nCifar100, our models can still achieve better performance than baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RobustART: Benchmarking Robustness on Architecture Design and Training Techniques. (arXiv:2109.05211v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05211","description":"<p>Deep neural networks (DNNs) are vulnerable to adversarial noises, which\nmotivates the benchmark of model robustness. Existing benchmarks mainly focus\non evaluating defenses, but there are no comprehensive studies of how\narchitecture design and training techniques affect robustness. Comprehensively\nbenchmarking their relationships is beneficial for better understanding and\ndeveloping robust DNNs. Thus, we propose RobustART, the first comprehensive\nRobustness investigation benchmark on ImageNet regarding ARchitecture design\n(49 human-designed off-the-shelf architectures and 1200+ networks from neural\narchitecture search) and Training techniques (10+ techniques, e.g., data\naugmentation) towards diverse noises (adversarial, natural, and system noises).\nExtensive experiments substantiated several insights for the first time, e.g.,\n(1) adversarial training is effective for the robustness against all noises\ntypes for Transformers and MLP-Mixers; (2) given comparable model sizes and\naligned training settings, CNNs &gt; Transformers &gt; MLP-Mixers on robustness\nagainst natural and system noises; Transformers &gt; MLP-Mixers &gt; CNNs on\nadversarial robustness; (3) for some light-weight architectures, increasing\nmodel sizes or using extra data cannot improve robustness. Our benchmark\npresents: (1) an open-source platform for comprehensive robustness evaluation;\n(2) a variety of pre-trained models to facilitate robustness evaluation; and\n(3) a new view to better understand the mechanism towards designing robust\nDNNs. We will continuously develop to this ecosystem for the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shiyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aishan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiakai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Competence-Aware Path Planning via Introspective Perception. (arXiv:2109.13974v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.13974","description":"<p>Robots deployed in the real world over extended periods of time need to\nreason about unexpected failures, learn to predict them, and to proactively\ntake actions to avoid future failures. Existing approaches for competence-aware\nplanning are either model-based, requiring explicit enumeration of known\nfailure modes, or purely statistical, using state- and location-specific\nfailure statistics to infer competence. We instead propose a structured\nmodel-free approach to competence-aware planning by reasoning about plan\nexecution failures due to errors in perception, without requiring a priori\nenumeration of failure sources or requiring location-specific failure\nstatistics. We introduce competence-aware path planning via introspective\nperception (CPIP), a Bayesian framework to iteratively learn and exploit\ntask-level competence in novel deployment environments. CPIP factorizes the\ncompetence-aware planning problem into two components. First, perception errors\nare learned in a model-free and location-agnostic setting via introspective\nperception prior to deployment in novel environments. Second, during actual\ndeployments, the prediction of task-level failures is learned in a\ncontext-aware setting. Experiments in a simulation show that the proposed CPIP\napproach outperforms the frequentist baseline in multiple mobile robot tasks,\nand is further validated via real robot experiments in an environment with\nperceptually challenging obstacles and terrain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabiee_S/0/1/0/all/0/1\">Sadegh Rabiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basich_C/0/1/0/all/0/1\">Connor Basich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wray_K/0/1/0/all/0/1\">Kyle Hollins Wray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zilberstein_S/0/1/0/all/0/1\">Shlomo Zilberstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_J/0/1/0/all/0/1\">Joydeep Biswas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Network Compression through Generalized Kronecker Product Decomposition. (arXiv:2109.14710v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.14710","description":"<p>Modern Convolutional Neural Network (CNN) architectures, despite their\nsuperiority in solving various problems, are generally too large to be deployed\non resource constrained edge devices. In this paper, we reduce memory usage and\nfloating-point operations required by convolutional layers in CNNs. We compress\nthese layers by generalizing the Kronecker Product Decomposition to apply to\nmultidimensional tensors, leading to the Generalized Kronecker Product\nDecomposition (GKPD). Our approach yields a plug-and-play module that can be\nused as a drop-in replacement for any convolutional layer. Experimental results\nfor image classification on CIFAR-10 and ImageNet datasets using ResNet,\nMobileNetv2 and SeNet architectures substantiate the effectiveness of our\nproposed approach. We find that GKPD outperforms state-of-the-art decomposition\nmethods including Tensor-Train and Tensor-Ring as well as other relevant\ncompression methods such as pruning and knowledge distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hameed_M/0/1/0/all/0/1\">Marawan Gamal Abdel Hameed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahaei_M/0/1/0/all/0/1\">Marzieh S. Tahaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosleh_A/0/1/0/all/0/1\">Ali Mosleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Online Visual Invariances for Novel Objects via Supervised and Self-Supervised Training. (arXiv:2110.01476v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01476","description":"<p>Humans can identify objects following various spatial transformations such as\nscale and viewpoint. This extends to novel objects, after a single presentation\nat a single pose, sometimes referred to as online invariance. CNNs have been\nproposed as a compelling model of human vision, but their ability to identify\nobjects across transformations is typically tested on held-out samples of\ntrained categories after extensive data augmentation. This paper assesses\nwhether standard CNNs can support human-like online invariance by training\nmodels to recognize images of synthetic 3D objects that undergo several\ntransformations: rotation, scaling, translation, brightness, contrast, and\nviewpoint. Through the analysis of models' internal representations, we show\nthat standard supervised CNNs trained on transformed objects can acquire strong\ninvariances on novel classes even when trained with as few as 50 objects taken\nfrom 10 classes. This extended to a different dataset of photographs of real\nobjects. We also show that these invariances can be acquired in a\nself-supervised way, through solving the same/different task. We suggest that\nthis latter approach may be similar to how humans acquire invariances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biscione_V/0/1/0/all/0/1\">Valerio Biscione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowers_J/0/1/0/all/0/1\">Jeffrey S. Bowers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-Based Detection, Classification and Prediction/Prognosis in Medical Imaging: Towards Radiophenomics. (arXiv:2110.10332v4 [physics.med-ph] UPDATED)","link":"http://arxiv.org/abs/2110.10332","description":"<p>Artificial intelligence (AI) techniques have significant potential to enable\neffective, robust and automated image phenotyping including identification of\nsubtle patterns. AI-based detection searches the image space to find the\nregions of interest based on patterns and features. There is a spectrum of\ntumor histologies from benign to malignant that can be identified by AI-based\nclassification approaches using image features. The extraction of minable\ninformation from images gives way to the field of radiomics and can be explored\nvia explicit (handcrafted/engineered) and deep radiomics frameworks. Radiomics\nanalysis has the potential to be utilized as a noninvasive technique for the\naccurate characterization of tumors to improve diagnosis and treatment\nmonitoring. This work reviews AI-based techniques, with a special focus on\noncological PET and PET/CT imaging, for different detection, classification,\nand prediction/prognosis tasks. We also discuss needed efforts to enable the\ntranslation of AI techniques to routine clinical workflows, and potential\nimprovements and complementary techniques such as the use of natural language\nprocessing on electronic health records and neuro-symbolic AI techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yousefirizi_F/0/1/0/all/0/1\">Fereshteh Yousefirizi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Decazes_P/0/1/0/all/0/1\">Pierre Decazes</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Amyar_A/0/1/0/all/0/1\">Amine Amyar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Saboury_B/0/1/0/all/0/1\">Babak Saboury</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rahmim_A/0/1/0/all/0/1\">Arman Rahmim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lebanon Solar Rooftop Potential Assessment using Buildings Segmentation from Aerial Images. (arXiv:2111.11397v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11397","description":"<p>Estimating the solar rooftop potential of buildings' rooftops at a large\nscale is a fundamental step for every country to utilize its solar power\nefficiently. However, such estimation becomes time-consuming and costly if done\nthrough on-site measurements. This paper uses deep learning-based multi-class\ninstance segmentation to extract buildings' footprints from satellite images.\nHence, we introduce Lebanon's first complete and comprehensive buildings'\nfootprints map. Furthermore, we propose a photovoltaic panels placement\nalgorithm to estimate the solar potential of every rooftop, which results in\nLebanon's first buildings' solar rooftop potential map too. Finally, we report\ntotal solar rooftop potential per district and localize regions corresponding\nto the highest solar rooftop potential yield.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nasrallah_H/0/1/0/all/0/1\">Hasan Nasrallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samhat_A/0/1/0/all/0/1\">Abed Ellatif Samhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faour_G/0/1/0/all/0/1\">Ghaleb Faour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1\">Ali J. Ghandour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Principled Disentanglement for Domain Generalization. (arXiv:2111.13839v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.13839","description":"<p>A fundamental challenge for machine learning models is generalizing to\nout-of-distribution (OOD) data, in part due to spurious correlations. To tackle\nthis challenge, we first formalize the OOD generalization problem as\nconstrained optimization, called Disentanglement-constrained Domain\nGeneralization (DDG). We relax this non-trivial constrained optimization to a\ntractable form with finite-dimensional parameterization and empirical\napproximation. Then a theoretical analysis of the extent to which the above\ntransformations deviates from the original problem is provided. Based on the\ntransformation, we propose a primal-dual algorithm for joint representation\ndisentanglement and domain generalization. In contrast to traditional\napproaches based on domain adversarial training and domain labels, DDG jointly\nlearns semantic and variation encoders for disentanglement, enabling flexible\nmanipulation and augmentation on training data. DDG aims to learn intrinsic\nrepresentations of semantic concepts that are invariant to nuisance factors and\ngeneralizable across different domains. Comprehensive experiments on popular\nbenchmarks show that DDG can achieve competitive OOD performance and uncover\ninterpretable salient structures within data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi-Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boundary Aware Learning for Out-of-distribution Detection. (arXiv:2112.11648v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11648","description":"<p>This paper focuses on the problem of detecting out-of-distribution (ood)\nsamples with neural nets. In image recognition tasks, the trained classifier\noften gives high confidence score for input images which are remote from the\nin-distribution (id) data, and this has greatly limited its application in real\nworld. For alleviating this problem, we propose a GAN based boundary aware\nclassifier (GBAC) for generating a closed hyperspace which only contains most\nid data. Our method is based on the fact that the traditional neural net\nseperates the feature space as several unclosed regions which are not suitable\nfor ood detection. With GBAC as an auxiliary module, the ood data distributed\noutside the closed hyperspace will be assigned with much lower score, allowing\nmore effective ood detection while maintaining the classification performance.\nMoreover, we present a fast sampling method for generating hard ood\nrepresentations which lie on the boundary of pre-mentioned closed hyperspace.\nExperiments taken on several datasets and neural net architectures promise the\neffectiveness of GBAC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Sen Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Richard YiDa Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Gaofeng Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Culture-to-Culture Image Translation with Generative Adversarial Networks. (arXiv:2201.01565v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01565","description":"<p>This article introduces the concept of image \"culturization\", i.e., defined\nas the process of altering the \"brushstroke of cultural features\" that make\nobjects perceived as belonging to a given culture while preserving their\nfunctionalities. First, we propose a pipeline for translating objects' images\nfrom a source to a target cultural domain based on Generative Adversarial\nNetworks (GAN). Then, we gather data through an online questionnaire to test\nfour hypotheses concerning the preferences of Italian participants towards\nobjects and environments belonging to different cultures. As expected, results\ndepend on individual tastes and preference: however, they are in line with our\nconjecture that some people, during the interaction with a robot or another\nintelligent system, might prefer to be shown images whose cultural domain has\nbeen modified to match their cultural background.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaino_G/0/1/0/all/0/1\">Giulia Zaino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recchiuto_C/0/1/0/all/0/1\">Carmine Tommaso Recchiuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sgorbissa_A/0/1/0/all/0/1\">Antonio Sgorbissa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Incremental Learning Approach to Automatically Recognize Pulmonary Diseases from the Multi-vendor Chest Radiographs. (arXiv:2201.02574v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02574","description":"<p>Pulmonary diseases can cause severe respiratory problems, leading to sudden\ndeath if not treated timely. Many researchers have utilized deep learning\nsystems to diagnose pulmonary disorders using chest X-rays (CXRs). However,\nsuch systems require exhaustive training efforts on large-scale data to\neffectively diagnose chest abnormalities. Furthermore, procuring such\nlarge-scale data is often infeasible and impractical, especially for rare\ndiseases. With the recent advances in incremental learning, researchers have\nperiodically tuned deep neural networks to learn different classification tasks\nwith few training examples. Although, such systems can resist catastrophic\nforgetting, they treat the knowledge representations independently of each\nother, and this limits their classification performance. Also, to the best of\nour knowledge, there is no incremental learning-driven image diagnostic\nframework that is specifically designed to screen pulmonary disorders from the\nCXRs. To address this, we present a novel framework that can learn to screen\ndifferent chest abnormalities incrementally. In addition to this, the proposed\nframework is penalized through an incremental learning loss function that\ninfers Bayesian theory to recognize structural and semantic inter-dependencies\nbetween incrementally learned knowledge representations to diagnose the\npulmonary diseases effectively, regardless of the scanner specifications. We\ntested the proposed framework on five public CXR datasets containing different\nchest abnormalities, where it outperformed various state-of-the-art system\nthrough various metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sirshar_M/0/1/0/all/0/1\">Mehreen Sirshar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akram_M/0/1/0/all/0/1\">Muhammad Usman Akram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1\">Shoab Ahmed Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers. (arXiv:2201.05047v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05047","description":"<p>Detection Transformer (DETR) and Deformable DETR have been proposed to\neliminate the need for many hand-designed components in object detection while\ndemonstrating good performance as previous complex hand-crafted detectors.\nHowever, their performance on Video Object Detection (VOD) has not been well\nexplored. In this paper, we present TransVOD, the first end-to-end video object\ndetection system based on spatial-temporal Transformer architectures. The first\ngoal of this paper is to streamline the pipeline of VOD, effectively removing\nthe need for many hand-crafted components for feature aggregation, e.g.,\noptical flow model, relation networks. Besides, benefited from the object query\ndesign in DETR, our method does not need complicated post-processing methods\nsuch as Seq-NMS. In particular, we present a temporal Transformer to aggregate\nboth the spatial object queries and the feature memories of each frame. Our\ntemporal transformer consists of two components: Temporal Query Encoder (TQE)\nto fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to\nobtain current frame detection results. These designs boost the strong baseline\ndeformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID\ndataset. Then, we present two improved versions of TransVOD including\nTransVOD++ and TransVOD Lite. The former fuses object-level information into\nobject query via dynamic convolution while the latter models the entire video\nclips as the output to speed up the inference time. We give detailed analysis\nof all three models in the experiment part. In particular, our proposed\nTransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet\nVID with 90.0% mAP. Our proposed TransVOD Lite also achieves the best speed and\naccuracy trade-off with 83.7% mAP while running at around 30 FPS on a single\nV100 GPU device. Code and models will be available for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}