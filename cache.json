{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems. (arXiv:2204.03021v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03021","description":"<p>Conversational agents have come increasingly closer to human competence in\nopen-domain dialogue settings; however, such models can reflect insensitive,\nhurtful, or entirely incoherent viewpoints that erode a user's trust in the\nmoral integrity of the system. Moral deviations are difficult to mitigate\nbecause moral judgments are not universal, and there may be multiple competing\njudgments that apply to a situation simultaneously. In this work, we introduce\na new resource, not to authoritatively resolve moral ambiguities, but instead\nto facilitate systematic understanding of the intuitions, values and moral\njudgments reflected in the utterances of dialogue systems. The Moral Integrity\nCorpus, MIC, is such a resource, which captures the moral assumptions of 38k\nprompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects\na particular moral conviction that can explain why a chatbot's reply may appear\nacceptable or problematic. We further organize RoTs with a set of 9 moral and\nsocial attributes and benchmark performance for attribute classification. Most\nimportantly, we show that current neural language models can automatically\ngenerate new RoTs that reasonably describe previously unseen interactions, but\nthey still struggle with certain scenarios. Our findings suggest that MIC will\nbe a useful resource for understanding and language models' implicit moral\nassumptions and flexibly benchmarking the integrity of conversational agents.\nTo download the data, see https://github.com/GT-SALT/mic\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jane A. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi-Chia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Halevy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment. (arXiv:2204.03025v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03025","description":"<p>Most research on question answering focuses on the pre-deployment stage;\ni.e., building an accurate model for deployment. In this paper, we ask the\nquestion: Can we improve QA systems further \\emph{post-}deployment based on\nuser interactions? We focus on two kinds of improvements: 1) improving the QA\nsystem's performance itself, and 2) providing the model with the ability to\nexplain the correctness or incorrectness of an answer. We collect a\nretrieval-based QA dataset, FeedbackQA, which contains interactive feedback\nfrom users. We collect this dataset by deploying a base QA system to\ncrowdworkers who then engage with the system and provide feedback on the\nquality of its answers. The feedback contains both structured ratings and\nunstructured natural language explanations. We train a neural model with this\nfeedback data that can generate explanations and re-score answer candidates. We\nshow that feedback data not only improves the accuracy of the deployed QA\nsystem but also other stronger non-deployed systems. The generated explanations\nalso help users make informed decisions about the correctness of answers.\nProject page: https://mcgill-nlp.github.io/feedbackqa/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Prakhar Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xing Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie C.K. Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VALUE: Understanding Dialect Disparity in NLU. (arXiv:2204.03031v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03031","description":"<p>English Natural Language Understanding (NLU) systems have achieved great\nperformances and even outperformed humans on benchmarks like GLUE and\nSuperGLUE. However, these benchmarks contain only textbook Standard American\nEnglish (SAE). Other dialects have been largely overlooked in the NLP\ncommunity. This leads to biased and inequitable NLU systems that serve only a\nsub-population of speakers. To understand disparities in current models and to\nfacilitate more dialect-competent NLU systems, we introduce the VernAcular\nLanguage Understanding Evaluation (VALUE) benchmark, a challenging variant of\nGLUE that we created with a set of lexical and morphosyntactic transformation\nrules. In this initial release (V.1), we construct rules for 11 features of\nAfrican American Vernacular English (AAVE), and we recruit fluent AAVE speakers\nto validate each feature transformation via linguistic acceptability judgments\nin a participatory design manner. Experiments show that these new dialectal\nfeatures can lead to a drop in model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_C/0/1/0/all/0/1\">Camille Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_J/0/1/0/all/0/1\">Jessica Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Annotation for Building A Suite of Clinical Natural Language Processing Tasks: Progress Note Understanding. (arXiv:2204.03035v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03035","description":"<p>Applying methods in natural language processing on electronic health records\n(EHR) data is a growing field. Existing corpus and annotation focus on modeling\ntextual features and relation prediction. However, there is a paucity of\nannotated corpus built to model clinical diagnostic thinking, a process\ninvolving text understanding, domain knowledge abstraction and reasoning. This\nwork introduces a hierarchical annotation schema with three stages to address\nclinical text understanding, clinical reasoning, and summarization. We created\nan annotated corpus based on an extensive collection of publicly available\ndaily progress notes, a type of EHR documentation that is collected in time\nseries in a problem-oriented format. The conventional format for a progress\nnote follows a Subjective, Objective, Assessment and Plan heading (SOAP). We\nalso define a new suite of tasks, Progress Note Understanding, with three tasks\nutilizing the three annotation stages. The novel suite of tasks was designed to\ntrain and evaluate future NLP models for clinical text understanding, clinical\nknowledge representation, inference, and summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dligach_D/0/1/0/all/0/1\">Dmitriy Dligach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1\">Timothy Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tesch_S/0/1/0/all/0/1\">Samuel Tesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laffin_R/0/1/0/all/0/1\">Ryan Laffin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Churpek_M/0/1/0/all/0/1\">Matthew M. Churpek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afshar_M/0/1/0/all/0/1\">Majid Afshar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis. (arXiv:2204.03040v1 [cs.SD])","link":"http://arxiv.org/abs/2204.03040","description":"<p>In this work, we present the SOMOS dataset, the first large-scale mean\nopinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS)\nsamples. It can be employed to train automatic MOS prediction systems focused\non the assessment of modern synthesizers, and can stimulate advancements in\nacoustic model evaluation. It consists of 20K synthetic utterances of the LJ\nSpeech voice, a public domain speech dataset which is a common benchmark for\nbuilding neural acoustic models and vocoders. Utterances are generated from 200\nTTS systems including vanilla neural acoustic models as well as models which\nallow prosodic variations. An LPCNet vocoder is used for all systems, so that\nthe samples' variation depends only on the acoustic models. The synthesized\nutterances provide balanced and adequate domain and length coverage. We collect\nMOS naturalness evaluations on 3 English Amazon Mechanical Turk locales and\nshare practices leading to reliable crowdsourced annotations for this task.\nBaseline results of state-of-the-art MOS prediction models on the SOMOS dataset\nare presented, while we show the challenges that such models face when assigned\nto evaluate synthetic utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maniati_G/0/1/0/all/0/1\">Georgia Maniati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vioni_A/0/1/0/all/0/1\">Alexandra Vioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellinas_N/0/1/0/all/0/1\">Nikolaos Ellinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikitaras_K/0/1/0/all/0/1\">Karolos Nikitaras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klapsas_K/0/1/0/all/0/1\">Konstantinos Klapsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_J/0/1/0/all/0/1\">June Sig Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jho_G/0/1/0/all/0/1\">Gunu Jho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalamandaris_A/0/1/0/all/0/1\">Aimilios Chalamandaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiakoulis_P/0/1/0/all/0/1\">Pirros Tsiakoulis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing finetuned models for better pretraining. (arXiv:2204.03044v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03044","description":"<p>Pretrained models are the standard starting point for training. This approach\nconsistently outperforms the use of a random initialization. However,\npretraining is a costly endeavour that few can undertake.\n</p>\n<p>In this paper, we create better base models at hardly any cost, by fusing\nmultiple existing fine tuned models into one. Specifically, we fuse by\naveraging the weights of these models. We show that the fused model results\nsurpass the pretrained model ones. We also show that fusing is often better\nthan intertraining.\n</p>\n<p>We find that fusing is less dependent on the target task. Furthermore, weight\ndecay nullifies intertraining effects but not those of fusing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venezian_E/0/1/0/all/0/1\">Elad Venezian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abusive and Threatening Language Detection in Urdu using Supervised Machine Learning and Feature Combinations. (arXiv:2204.03062v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03062","description":"<p>This paper presents the system descriptions submitted at the FIRE Shared Task\n2021 on Urdu's Abusive and Threatening Language Detection Task. This challenge\naims at automatically identifying abusive and threatening tweets written in\nUrdu. Our submitted results were selected for the third recognition at the\ncompetition. This paper reports a non-exhaustive list of experiments that\nallowed us to reach the submitted results. Moreover, after the result\ndeclaration of the competition, we managed to attain even better results than\nthe submitted results. Our models achieved 0.8318 F1 score on Task A (Abusive\nLanguage Detection for Urdu Tweets) and 0.4931 F1 score on Task B (Threatening\nLanguage Detection for Urdu Tweets). Results show that Support Vector Machines\nwith stopwords removed, lemmatization applied, and features vector created by\nthe combinations of word n-grams for n=1,2,3 produced the best results for Task\nA. For Task B, Support Vector Machines with stopwords removed, lemmatization\nnot applied, feature vector created from a pre-trained Urdu Word2Vec (on word\nunigrams and bigrams), and making the dataset balanced using oversampling\ntechnique produced the best results. The code is made available for\nreproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humayoun_M/0/1/0/all/0/1\">Muhammad Humayoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The 2021 Urdu Fake News Detection Task using Supervised Machine Learning and Feature Combinations. (arXiv:2204.03064v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03064","description":"<p>This paper presents the system description submitted at the FIRE Shared Task:\n\"The 2021 Fake News Detection in the Urdu Language\". This challenge aims at\nautomatically identifying Fake news written in Urdu. Our submitted results\nranked fifth in the competition. However, after the result declaration of the\ncompetition, we managed to attain even better results than the submitted\nresults. The best F1 Macro score achieved by one of our models is 0.6674,\nhigher than the second-best score in the competition. The result is achieved on\nSupport Vector Machines (polynomial kernel degree 1) with stopwords removed,\nlemmatization applied, and selecting the 20K best features out of 1.557 million\nfeatures in total (which were produced by Word n-grams n=1,2,3,4 and Char\nn-grams n=2,3,4,5,6). The code is made available for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humayoun_M/0/1/0/all/0/1\">Muhammad Humayoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ByT5 model for massively multilingual grapheme-to-phoneme conversion. (arXiv:2204.03067v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03067","description":"<p>In this study, we tackle massively multilingual grapheme-to-phoneme\nconversion through implementing G2P models based on ByT5. We have curated a G2P\ndataset from various sources that covers around 100 languages and trained\nlarge-scale multilingual G2P models based on ByT5. We found that ByT5 operating\non byte-level inputs significantly outperformed the token-based mT5 model in\nterms of multilingual G2P. Pairwise comparison with monolingual models in these\nlanguages suggests that multilingual ByT5 models generally lower the phone\nerror rate by jointly learning from a variety of languages. The pretrained\nmodel can further benefit low resource G2P through zero-shot prediction on\nunseen languages or provides pretrained weights for finetuning, which helps the\nmodel converge to a lower phone error rate than randomly initialized weights.\nTo facilitate future research on multilingual G2P, we make available our code\nand pretrained multilingual G2P models at:\nhttps://github.com/lingjzhu/CharsiuG2P.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Urdu Morphology, Orthography and Lexicon Extraction. (arXiv:2204.03071v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03071","description":"<p>Urdu is a challenging language because of, first, its Perso-Arabic script and\nsecond, its morphological system having inherent grammatical forms and\nvocabulary of Arabic, Persian and the native languages of South Asia. This\npaper describes an implementation of the Urdu language as a software API, and\nwe deal with orthography, morphology and the extraction of the lexicon. The\nmorphology is implemented in a toolkit called Functional Morphology (Forsberg &amp;\nRanta, 2004), which is based on the idea of dealing grammars as software\nlibraries. Therefore this implementation could be reused in applications such\nas intelligent search of keywords, language training and infrastructure for\nsyntax. We also present an implementation of a small part of Urdu syntax to\ndemonstrate this reusability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humayoun_M/0/1/0/all/0/1\">Muhammad Humayoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammarstrom_H/0/1/0/all/0/1\">Harald Hammarstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranta_A/0/1/0/all/0/1\">Aarne Ranta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Infused Decoding. (arXiv:2204.03084v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03084","description":"<p>Pre-trained language models (LMs) have been shown to memorize a substantial\namount of knowledge from the pre-training corpora; however, they are still\nlimited in recalling factually correct knowledge given a certain context.\nHence, they tend to suffer from counterfactual or hallucinatory generation when\nused in knowledge-intensive natural language generation (NLG) tasks. Recent\nremedies to this problem focus on modifying either the pre-training or task\nfine-tuning objectives to incorporate knowledge, which normally require\nadditional costly training or architecture modification of LMs for practical\napplications. We present Knowledge Infused Decoding (KID) -- a novel decoding\nalgorithm for generative LMs, which dynamically infuses external knowledge into\neach step of the LM decoding. Specifically, we maintain a local knowledge\nmemory based on the current context, interacting with a dynamically created\nexternal knowledge trie, and continuously update the local memory as a\nknowledge-aware constraint to guide decoding via reinforcement learning. On six\ndiverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART)\narmed with KID outperform many task-optimized state-of-the-art models, and show\nparticularly strong performance in few-shot scenarios over seven related\nknowledge-infusion techniques. Human evaluation confirms KID's ability to\ngenerate more relevant and factual language for the input context when compared\nwith multiple baselines. Finally, KID also alleviates exposure bias and\nprovides stable generation quality when generating longer sequences. Code for\nKID is available at https://github.com/microsoft/KID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shashank Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaonkar_R/0/1/0/all/0/1\">Radhika Gaonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chongyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1\">Milad Shokouhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis. (arXiv:2204.03117v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03117","description":"<p>Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to align aspects and corresponding sentiments for\naspect-specific sentiment polarity inference. It is challenging because a\nsentence may contain multiple aspects or complicated (e.g., conditional,\ncoordinating, or adversative) relations. Recently, exploiting dependency syntax\ninformation with graph neural networks has been the most popular trend. Despite\nits success, methods that heavily rely on the dependency tree pose challenges\nin accurately modeling the alignment of the aspects and their words indicative\nof sentiment, since the dependency tree may provide noisy signals of unrelated\nassociations (e.g., the \"conj\" relation between \"great\" and \"dreadful\" in\nFigure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax\naware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully\nexploits the syntax information (e.g., phrase segmentation and hierarchical\nstructure) of the constituent tree of a sentence to model the sentiment-aware\ncontext of every single aspect (called intra-context) and the sentiment\nrelations across aspects (called inter-context) for learning. Experiments on\nfour benchmark datasets demonstrate that BiSyn-GAT+ outperforms the\nstate-of-the-art methods consistently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shuo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiyong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. (arXiv:2204.03162v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03162","description":"<p>We present a novel task and dataset for evaluating the ability of vision and\nlanguage models to conduct visio-linguistic compositional reasoning, which we\ncall Winoground. Given two images and two captions, the goal is to match them\ncorrectly - but crucially, both captions contain a completely identical set of\nwords, only in a different order. The dataset was carefully hand-curated by\nexpert annotators and is labeled with a rich set of fine-grained tags to assist\nin analyzing model performance. We probe a diverse range of state-of-the-art\nvision and language models and find that, surprisingly, none of them do much\nbetter than chance. Evidently, these models are not as skilled at\nvisio-linguistic compositional reasoning as we might have hoped. We perform an\nextensive analysis to obtain insights into how future work might try to\nmitigate these models' shortcomings. We aim for Winoground to serve as a useful\nevaluation set for advancing the state of the art and driving further progress\nin the field. The dataset is available at\nhttps://huggingface.co/datasets/facebook/winoground.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Ryan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1\">Max Bartolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_C/0/1/0/all/0/1\">Candace Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3M: Multi-loss, Multi-path and Multi-level Neural Networks for speech recognition. (arXiv:2204.03178v1 [cs.SD])","link":"http://arxiv.org/abs/2204.03178","description":"<p>Recently, Conformer based CTC/AED model has become a mainstream architecture\nfor ASR. In this paper, based on our prior work, we identify and integrate\nseveral approaches to achieve further improvements for ASR tasks, which we\ndenote as multi-loss, multi-path and multi-level, summarized as \"3M\" model.\nSpecifically, multi-loss refers to the joint CTC/AED loss and multi-path\ndenotes the Mixture-of-Experts(MoE) architecture which can effectively increase\nthe model capacity without remarkably increasing computation cost. Multi-level\nmeans that we introduce auxiliary loss at multiple level of a deep model to\nhelp training. We evaluate our proposed method on the public WenetSpeech\ndataset and experimental results show that the proposed method provides\n12.2%-17.6% relative CER improvement over the baseline model trained by Wenet\ntoolkit. On our large scale dataset of 150k hours corpus, the 3M model has also\nshown obvious superiority over the baseline Conformer model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhao You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shulin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Joint Learning Approach for Semi-supervised Neural Topic Modeling. (arXiv:2204.03208v1 [cs.IR])","link":"http://arxiv.org/abs/2204.03208","description":"<p>Topic models are some of the most popular ways to represent textual data in\nan interpret-able manner. Recently, advances in deep generative models,\nspecifically auto-encoding variational Bayes (AEVB), have led to the\nintroduction of unsupervised neural topic models, which leverage deep\ngenerative models as opposed to traditional statistics-based topic models. We\nextend upon these neural topic models by introducing the Label-Indexed Neural\nTopic Model (LI-NTM), which is, to the extent of our knowledge, the first\neffective upstream semi-supervised neural topic model. We find that LI-NTM\noutperforms existing neural topic models in document reconstruction benchmarks,\nwith the most notable results in low labeled data regimes and for data-sets\nwith informative labels; furthermore, our jointly learned classifier\noutperforms baseline classifiers in ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1\">Jeffrey Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_R/0/1/0/all/0/1\">Rajat Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tumma_N/0/1/0/all/0/1\">Neehal Tumma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abhishek Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1\">Finale Doshi-Velez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Attention through Gradient-Based Learned Runtime Pruning. (arXiv:2204.03227v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03227","description":"<p>Self-attention is a key enabler of state-of-art accuracy for various\ntransformer-based Natural Language Processing models. This attention mechanism\ncalculates a correlation score for each word with respect to the other words in\na sentence. Commonly, only a small subset of words highly correlates with the\nword under attention, which is only determined at runtime. As such, a\nsignificant amount of computation is inconsequential due to low attention\nscores and can potentially be pruned. The main challenge is finding the\nthreshold for the scores below which subsequent computation will be\ninconsequential. Although such a threshold is discrete, this paper formulates\nits search through a soft differentiable regularizer integrated into the loss\nfunction of the training. This formulation piggy backs on the back-propagation\ntraining to analytically co-optimize the threshold and the weights\nsimultaneously, striking a formally optimal balance between accuracy and\ncomputation pruning. To best utilize this mathematical innovation, we devise a\nbit-serial architecture, dubbed LeOPArd, for transformer language models with\nbit-level early termination microarchitectural mechanism. We evaluate our\ndesign across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision\ntransformer models. Post-layout results show that, on average, LeOPArd yields\n1.9x and 3.9x speedup and energy reduction, respectively, while keeping the\naverage accuracy virtually intact (&lt;0.2% degradation)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodrati_S/0/1/0/all/0/1\">Soroush Ghodrati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdanbakhsh_A/0/1/0/all/0/1\">Amir Yazdanbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilzadeh_H/0/1/0/all/0/1\">Hadi Esmaeilzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Mingu Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators. (arXiv:2204.03243v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03243","description":"<p>We present a new framework AMOS that pretrains text encoders with an\nAdversarial learning curriculum via a Mixture Of Signals from multiple\nauxiliary generators. Following ELECTRA-style pretraining, the main encoder is\ntrained as a discriminator to detect replaced tokens generated by auxiliary\nmasked language models (MLMs). Different from ELECTRA which trains one MLM as\nthe generator, we jointly train multiple MLMs of different sizes to provide\ntraining signals at various levels of difficulty. To push the discriminator to\nlearn better with challenging replaced tokens, we learn mixture weights over\nthe auxiliary MLMs' outputs to maximize the discriminator loss by\nbackpropagating the gradient from the discriminator via Gumbel-Softmax. For\nbetter pretraining efficiency, we propose a way to assemble multiple MLMs into\none unified auxiliary model. AMOS outperforms ELECTRA and recent\nstate-of-the-art pretrained models by about 1 point on the GLUE benchmark for\nBERT base-sized models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1\">Payal Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwary_S/0/1/0/all/0/1\">Saurabh Tiwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic WordNet Construction using Word Sense Induction through Sentence Embeddings. (arXiv:2204.03251v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03251","description":"<p>Language resources such as wordnets remain indispensable tools for different\nnatural language tasks and applications. However, for low-resource languages\nsuch as Filipino, existing wordnets are old and outdated, and producing new\nones may be slow and costly in terms of time and resources. In this paper, we\npropose an automatic method for constructing a wordnet from scratch using only\nan unlabeled corpus and a sentence embeddings-based language model. Using this,\nwe produce FilWordNet, a new wordnet that supplants and improves the outdated\nFilipino WordNet. We evaluate our automatically-induced senses and synsets by\nmatching them with senses from the Princeton WordNet, as well as comparing the\nsynsets to the old Filipino WordNet. We empirically show that our method can\ninduce existing, as well as potentially new, senses and synsets automatically\nwithout the need for human supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Velasco_D/0/1/0/all/0/1\">Dan John Velasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alba_A/0/1/0/all/0/1\">Axel Alba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelagio_T/0/1/0/all/0/1\">Trisha Gail Pelagio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_B/0/1/0/all/0/1\">Bryce Anthony Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arabic Text-To-Speech (TTS) Data Preparation. (arXiv:2204.03255v1 [cs.SD])","link":"http://arxiv.org/abs/2204.03255","description":"<p>People may be puzzled by the fact that voice over recordings data sets exist\nin addition to Text-to-Speech (TTS), Synthesis system advancements, albeit this\nis not the case. The goal of this study is to explain the relevance of TTS as\nwell as the data preparation procedures. TTS relies heavily on recorded data\nsince it can have a substantial influence on the outcomes of TTS modules.\nFurthermore, whether the domain is specialized or general, appropriate data\nshould be developed to address all predicted language variants and domains.\nDifferent recording methodologies, taking into account quality and behavior,\nmay also be advantageous in the development of the module. In light of the lack\nof Arabic language in present synthesizing systems, numerous variables that\nimpact the flow of recorded utterances are being considered in order to\nmanipulate an Arabic TTS module. In this study, two viewpoints will be\ndiscussed: linguistics and the creation of high-quality recordings for TTS. The\npurpose of this work is to offer light on how ground-truth utterances may\ninfluence the evolution of speech systems in terms of naturalness,\nintelligibility, and understanding. Well provide voice actor specs as well as\ndata specs that will assist both voice actors and voice coaches in the studio\nas well as the annotators who will be evaluating the audios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Masri_H/0/1/0/all/0/1\">Hala Al Masri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zater_M/0/1/0/all/0/1\">Muhy Eddin Za&#x27;ter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Korean Online Hate Speech Dataset for Multilabel Classification: How Can Social Science Aid Developing Better Hate Speech Dataset?. (arXiv:2204.03262v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03262","description":"<p>We suggest a multilabel Korean online hate speech dataset that covers seven\ncategories of hate speech: (1) Race and Nationality, (2) Religion, (3)\nRegionalism, (4) Ageism, (5) Misogyny, (6) Sexual Minorities, and (7) Male. Our\n35K dataset consists of 24K online comments with Krippendorff's Alpha label\naccordance of .713, 2.2K neutral sentences from Wikipedia, 1.7K additionally\nlabeled sentences generated by the Human-in-the-Loop procedure and\nrule-generated 7.1K neutral sentences. The base model with 24K initial dataset\nachieved the accuracy of LRAP .892, but improved to .919 after being combined\nwith 11K additional data. Unlike the conventional binary hate and non-hate\ndichotomy approach, we designed a dataset considering both the cultural and\nlinguistic context to overcome the limitations of western culture-based English\ntexts. Thus, this paper is not only limited to presenting a local hate speech\ndataset but extends as a manual for building a more generalized hate speech\ndataset with diverse cultural backgrounds based on social science perspectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1\">TaeYoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_E/0/1/0/all/0/1\">Eunrang Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junbum Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_Y/0/1/0/all/0/1\">Youngeun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Junmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_J/0/1/0/all/0/1\">JeongKyu Suh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PALBERT: Teaching ALBERT to Ponder. (arXiv:2204.03276v1 [cs.LG])","link":"http://arxiv.org/abs/2204.03276","description":"<p>Currently, pre-trained models can be considered the default choice for a wide\nrange of NLP tasks. Despite their SoTA results, there is practical evidence\nthat these models may require a different number of computing layers for\ndifferent input sequences, since evaluating all layers leads to overconfidence\non wrong predictions (namely overthinking). This problem can potentially be\nsolved by implementing adaptive computation time approaches, which were first\ndesigned to improve inference speed. Recently proposed PonderNet may be a\npromising solution for performing an early exit by treating the exit layers\nindex as a latent variable. However, the originally proposed exit criterion,\nrelying on sampling from trained posterior distribution on the probability of\nexiting from i-th layer, introduces major variance in model outputs,\nsignificantly reducing the resulting models performance. In this paper, we\npropose Ponder ALBERT (PALBERT): an improvement to PonderNet with a novel\ndeterministic Q-exit criterion and a revisited model architecture. We compared\nPALBERT with recent methods for performing an early exit. We observed that the\nproposed changes can be considered significant improvements on the original\nPonderNet architecture and outperform PABEE on a wide range of GLUE tasks. In\naddition, we also performed an in-depth ablation study of the proposed\narchitecture to further understand Lambda layers and their performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balagansky_N/0/1/0/all/0/1\">Nikita Balagansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavrilov_D/0/1/0/all/0/1\">Daniil Gavrilov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entailment Graph Learning with Textual Entailment and Soft Transitivity. (arXiv:2204.03286v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03286","description":"<p>Typed entailment graphs try to learn the entailment relations between\npredicates from text and model them as edges between predicate nodes. The\nconstruction of entailment graphs usually suffers from severe sparsity and\nunreliability of distributional similarity. We propose a two-stage method,\nEntailment Graph with Textual Entailment and Transitivity (EGT2). EGT2 learns\nlocal entailment relations by recognizing possible textual entailment between\ntemplate sentences formed by typed CCG-parsed predicates. Based on the\ngenerated local graph, EGT2 then uses three novel soft transitivity constraints\nto consider the logical transitivity in entailment structures. Experiments on\nbenchmark datasets show that EGT2 can well model the transitivity in entailment\ngraph to alleviate the sparsity issue, and lead to significant improvement over\ncurrent state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-Based Extractive Summarisation for Scientific Articles. (arXiv:2204.03301v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03301","description":"<p>This paper presents the results of research on supervised extractive text\nsummarisation for scientific articles. We show that a simple sequential tagging\nmodel based only on the text within a document achieves high results against a\nsimple classification model. Improvements can be achieved through additional\nsentence-level features, though these were minimal. Through further analysis,\nwe show the potential of the sequential model relying on the structure of the\ndocument depending on the academic discipline which the document is from.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kershaw_D/0/1/0/all/0/1\">Daniel Kershaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koeling_R/0/1/0/all/0/1\">Rob Koeling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Three-Module Modeling For End-to-End Spoken Language Understanding Using Pre-trained DNN-HMM-Based Acoustic-Phonetic Model. (arXiv:2204.03315v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03315","description":"<p>In spoken language understanding (SLU), what the user says is converted to\nhis/her intent. Recent work on end-to-end SLU has shown that accuracy can be\nimproved via pre-training approaches. We revisit ideas presented by Lugosch et\nal. using speech pre-training and three-module modeling; however, to ease\nconstruction of the end-to-end SLU model, we use as our phoneme module an\nopen-source acoustic-phonetic model from a DNN-HMM hybrid automatic speech\nrecognition (ASR) system instead of training one from scratch. Hence we\nfine-tune on speech only for the word module, and we apply multi-target\nlearning (MTL) on the word and intent modules to jointly optimize SLU\nperformance. MTL yields a relative reduction of 40% in intent-classification\nerror rates (from 1.0% to 0.6%). Note that our three-module model is a\nstreaming method. The final outcome of the proposed three-module modeling\napproach yields an intent accuracy of 99.4% on FluentSpeech, an intent error\nrate reduction of 50% compared to that of Lugosch et al. Although we focus on\nreal-time streaming methods, we also list non-streaming methods for comparison.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nick J.C. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yandan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Haimei Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoencoding Language Model Based Ensemble Learning for Commonsense Validation and Explanation. (arXiv:2204.03324v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03324","description":"<p>An ultimate goal of artificial intelligence is to build computer systems that\ncan understand human languages. Understanding commonsense knowledge about the\nworld expressed in text is one of the foundational and challenging problems to\ncreate such intelligent systems. As a step towards this goal, we present in\nthis paper ALMEn, an Autoencoding Language Model based Ensemble learning method\nfor commonsense validation and explanation. By ensembling several advanced\npre-trained language models including RoBERTa, DeBERTa, and ELECTRA with\nSiamese neural networks, our method can distinguish natural language statements\nthat are against commonsense (validation subtask) and correctly identify the\nreason for making against commonsense (explanation selection subtask).\nExperimental results on the benchmark dataset of SemEval-2020 Task 4 show that\nour method outperforms state-of-the-art models, reaching 97.9% and 95.4%\naccuracies on the validation and explanation selection subtasks, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huy_N/0/1/0/all/0/1\">Ngo Quang Huy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phuong_T/0/1/0/all/0/1\">Tu Minh Phuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1\">Ngo Xuan Bach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Abstractive Question Answering over Tables or Text. (arXiv:2204.03357v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03357","description":"<p>A long-term ambition of information seeking QA systems is to reason over\nmulti-modal contexts and generate natural answers to user queries. Today,\nmemory intensive pre-trained language models are adapted to downstream tasks\nsuch as QA by fine-tuning the model on QA data in a specific modality like\nunstructured text or structured tables. To avoid training such memory-hungry\nmodels while utilizing a uniform architecture for each modality,\nparameter-efficient adapters add and train small task-specific bottle-neck\nlayers between transformer layers. In this work, we study parameter-efficient\nabstractive QA in encoder-decoder models over structured tabular data and\nunstructured textual data using only 1.5% additional parameters for each\nmodality. We also ablate over adapter layers in both encoder and decoder\nmodules to study the efficiency-performance trade-off and demonstrate that\nreducing additional trainable parameters down to 0.7%-1.0% leads to comparable\nresults. Our models out-perform current state-of-the-art models on tabular QA\ndatasets such as Tablesum and FeTaQA, and achieve comparable performance on a\ntextual QA dataset such as NarrativeQA using significantly less trainable\nparameters than fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pal_V/0/1/0/all/0/1\">Vaishali Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1\">Evangelos Kanoulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances. (arXiv:2204.03375v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03375","description":"<p>Dialogue State Tracking (DST) is primarily evaluated using Joint Goal\nAccuracy (JGA) defined as the fraction of turns where the ground-truth dialogue\nstate exactly matches the prediction. Generally in DST, the dialogue state or\nbelief state for a given turn contains all the intents shown by the user till\nthat turn. Due to this cumulative nature of the belief state, it is difficult\nto get a correct prediction once a misprediction has occurred. Thus, although\nbeing a useful metric, it can be harsh at times and underestimate the true\npotential of a DST model. Moreover, an improvement in JGA can sometimes\ndecrease the performance of turn-level or non-cumulative belief state\nprediction due to inconsistency in annotations. So, using JGA as the only\nmetric for model selection may not be ideal for all scenarios. In this work, we\ndiscuss various evaluation metrics used for DST along with their shortcomings.\nTo address the existing issues, we propose a new evaluation metric named\nFlexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike\nJGA, it tries to give penalized rewards to mispredictions that are locally\ncorrect i.e. the root cause of the error is an earlier turn. By doing so, FGA\nconsiders the performance of both cumulative and turn-level prediction flexibly\nand provides a better insight than the existing metrics. We also show that FGA\nis a better discriminator of DST model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Suvodip Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummara_R/0/1/0/all/0/1\">Ramamohan Kummara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1\">Maunendra Sankar Desarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAESTRO: Matched Speech Text Representations through Modality Matching. (arXiv:2204.03409v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03409","description":"<p>We present Maestro, a self-supervised training method to unify\nrepresentations learnt from speech and text modalities. Self-supervised\nlearning from speech signals aims to learn the latent structure inherent in the\nsignal, while self-supervised learning from text attempts to capture lexical\ninformation. Learning aligned representations from unpaired speech and text\nsequences is a challenging task. Previous work either implicitly enforced the\nrepresentations learnt from these two modalities to be aligned in the latent\nspace through multitasking and parameter sharing or explicitly through\nconversion of modalities via speech synthesis. While the former suffers from\ninterference between the two modalities, the latter introduces additional\ncomplexity. In this paper, we propose Maestro, a novel algorithm to learn\nunified representations from both these modalities simultaneously that can\ntransfer to diverse downstream tasks such as Automated Speech Recognition (ASR)\nand Speech Translation (ST). Maestro learns unified representations through\nsequence alignment, duration prediction and matching embeddings in the learned\nspace through an aligned masked-language model loss. We establish a new\nstate-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 11% relative\nreduction in Word Error Rate (WER), multidomain SpeechStew ASR (3.7% relative)\nand 21 languages to English multilingual ST on CoVoST 2 with an improvement of\n2.8 BLEU averaged over 21 languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zen_H/0/1/0/all/0/1\">Heiga Zen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0. (arXiv:2204.03417v1 [eess.AS])","link":"http://arxiv.org/abs/2204.03417","description":"<p>Stuttering is a varied speech disorder that harms an individual's\ncommunication ability. Persons who stutter (PWS) often use speech therapy to\ncope with their condition. Improving speech recognition systems for people with\nsuch non-typical speech or tracking the effectiveness of speech therapy would\nrequire systems that can detect dysfluencies while at the same time being able\nto detect speech techniques acquired in therapy.\n</p>\n<p>This paper shows that fine-tuning wav2vec 2.0 for the classification of\nstuttering on a sizeable English corpus containing stuttered speech, in\nconjunction with multi-task learning, boosts the effectiveness of the\ngeneral-purpose wav2vec 2.0 features for detecting stuttering in speech; both\nwithin and across languages. We evaluate our method on Fluencybank and the\nGerman therapy-centric Kassel State of Fluency (KSoF) dataset by training\nSupport Vector Machine classifiers using features extracted from the fine-tuned\nmodels for six different stuttering-related events types: blocks,\nprolongations, sound repetitions, word repetitions, interjections, and -\nspecific to therapy - speech modifications. Using embeddings from the\nfine-tuned models leads to relative classification performance gains up to 27\\%\nw.r.t. F1-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian P. Bayerl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_D/0/1/0/all/0/1\">Dominik Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noth_E/0/1/0/all/0/1\">Elmar N&#xf6;th</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Vocal Fatigue with Neural Embeddings. (arXiv:2204.03428v1 [eess.AS])","link":"http://arxiv.org/abs/2204.03428","description":"<p>Vocal fatigue refers to the feeling of tiredness and weakness of voice due to\nextended utilization. This paper investigates the effectiveness of neural\nembeddings for the detection of vocal fatigue. We compare x-vectors,\nECAPA-TDNN, and wav2vec 2.0 embeddings on a corpus of academic spoken English.\nLow-dimensional mappings of the data reveal that neural embeddings capture\ninformation about the change in vocal characteristics of a speaker during\nprolonged voice usage. We show that vocal fatigue can be reliably predicted\nusing all three kinds of neural embeddings after only 50 minutes of continuous\nspeaking when temporal smoothing and normalization are applied to the extracted\nembeddings. We employ support vector machines for classification and achieve\naccuracy scores of 81% using x-vectors, 85% using ECAPA-TDNN embeddings, and\n82% using wav2vec 2.0 embeddings as input features. We obtain an accuracy score\nof 76%, when the trained system is applied to a different speaker and recording\nenvironment without any adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian P. Bayerl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_D/0/1/0/all/0/1\">Dominik Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumann_I/0/1/0/all/0/1\">Ilja Baumann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bocklet_T/0/1/0/all/0/1\">Tobias Bocklet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTuit: Understanding Spanish language in Twitter through a native transformer. (arXiv:2204.03465v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03465","description":"<p>The appearance of complex attention-based language models such as BERT,\nRoberta or GPT-3 has allowed to address highly complex tasks in a plethora of\nscenarios. However, when applied to specific domains, these models encounter\nconsiderable difficulties. This is the case of Social Networks such as Twitter,\nan ever-changing stream of information written with informal and complex\nlanguage, where each message requires careful evaluation to be understood even\nby humans given the important role that context plays. Addressing tasks in this\ndomain through Natural Language Processing involves severe challenges. When\npowerful state-of-the-art multilingual language models are applied to this\nscenario, language specific nuances use to get lost in translation. To face\nthese challenges we present \\textbf{BERTuit}, the larger transformer proposed\nso far for Spanish language, pre-trained on a massive dataset of 230M Spanish\ntweets using RoBERTa optimization. Our motivation is to provide a powerful\nresource to better understand Spanish Twitter and to be used on applications\nfocused on this social network, with special emphasis on solutions devoted to\ntackle the spreading of misinformation in this platform. BERTuit is evaluated\non several tasks and compared against M-BERT, XLM-RoBERTa and XLM-T, very\ncompetitive multilingual transformers. The utility of our approach is shown\nwith applications, in this case: a zero-shot methodology to visualize groups of\nhoaxes and profiling authors spreading disinformation.\n</p>\n<p>Misinformation spreads wildly on platforms such as Twitter in languages other\nthan English, meaning performance of transformers may suffer when transferred\noutside English speaking communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huertas_Tato_J/0/1/0/all/0/1\">Javier Huertas-Tato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Alejandro Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_D/0/1/0/all/0/1\">David Camacho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delta Keyword Transformer: Bringing Transformers to the Edge through Dynamically Pruned Multi-Head Self-Attention. (arXiv:2204.03479v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03479","description":"<p>Multi-head self-attention forms the core of Transformer networks. However,\ntheir quadratically growing complexity with respect to the input sequence\nlength impedes their deployment on resource-constrained edge devices. We\naddress this challenge by proposing a dynamic pruning method, which exploits\nthe temporal stability of data across tokens to reduce inference cost. The\nthreshold-based method only retains significant differences between the\nsubsequent tokens, effectively reducing the number of multiply-accumulates, as\nwell as the internal tensor data sizes. The approach is evaluated on the Google\nSpeech Commands Dataset for keyword spotting, and the performance is compared\nagainst the baseline Keyword Transformer. Our experiments show that we can\nreduce ~80% of operations while maintaining the original 98.4% accuracy.\nMoreover, a reduction of ~87-94% operations can be achieved when only degrading\nthe accuracy by 1-4%, speeding up the multi-head self-attention inference by a\nfactor of ~7.5-16.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jelcicova_Z/0/1/0/all/0/1\">Zuzana Jel&#x10d;icov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verhelst_M/0/1/0/all/0/1\">Marian Verhelst</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Position-based Prompting for Health Outcome Generation. (arXiv:2204.03489v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03489","description":"<p>Probing Pre-trained Language Models (PLMs) using prompts has indirectly\nimplied that language models (LMs) can be treated as knowledge bases. To this\nend, this phenomena has been effective especially when these LMs are fine-tuned\ntowards not just data of a specific domain, but also to the style or linguistic\npattern of the prompts themselves. We observe that, satisfying a particular\nlinguistic pattern in prompts is an unsustainable constraint that unnecessarily\nlengthens the probing task, especially because, they are often manually\ndesigned and the range of possible prompt template patterns can vary depending\non the prompting objective and domain. We therefore explore an idea of using a\nposition-attention mechanism to capture positional information of each word in\na prompt relative to the mask to be filled, hence avoiding the need to\nre-construct prompts when the prompts linguistic pattern changes. Using our\napproach, we demonstrate the ability of eliciting answers to rare prompt\ntemplates (in a case study on health outcome generation) such as Postfix and\nMixed patterns whose missing information is respectively at the start and in\nmultiple random places of the prompt. More so, using various biomedical PLMs,\nour approach consistently outperforms a baseline in which the default mask\nlanguage model (MLM) representation is used to predict masked tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaho_M/0/1/0/all/0/1\">M. Abaho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">D. Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_P/0/1/0/all/0/1\">P. Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodd_S/0/1/0/all/0/1\">S. Dodd</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Understanding based Multi-Document Machine Reading Comprehension. (arXiv:2204.03494v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03494","description":"<p>Most existing multi-document machine reading comprehension models mainly\nfocus on understanding the interactions between the input question and\ndocuments, but ignore following two kinds of understandings. First, to\nunderstand the semantic meaning of words in the input question and documents\nfrom the perspective of each other. Second, to understand the supporting cues\nfor a correct answer from the perspective of intra-document and\ninter-documents. Ignoring these two kinds of important understandings would\nmake the models oversee some important information that may be helpful for\ninding correct answers. To overcome this deiciency, we propose a deep\nunderstanding based model for multi-document machine reading comprehension. It\nhas three cascaded deep understanding modules which are designed to understand\nthe accurate semantic meaning of words, the interactions between the input\nquestion and documents, and the supporting cues for the correct answer. We\nevaluate our model on two large scale benchmark datasets, namely TriviaQA Web\nand DuReader. Extensive experiments show that our model achieves\nstate-of-the-art results on both datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Feiliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bochao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huimin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingchao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings to Transformers. (arXiv:2204.03503v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03503","description":"<p>Automated short answer grading (ASAG) has gained attention in education as a\nmeans to scale educational tasks to the growing number of students. Recent\nprogress in Natural Language Processing and Machine Learning has largely\ninfluenced the field of ASAG, of which we survey the recent research\nadvancements. We complement previous surveys by providing a comprehensive\nanalysis of recently published methods that deploy deep learning approaches. In\nparticular, we focus our analysis on the transition from hand engineered\nfeatures to representation learning approaches, which learn representative\nfeatures for the task at hand automatically from large corpora of data. We\nstructure our analysis of deep learning methods along three categories: word\nembeddings, sequential models, and attention-based methods. Deep learning\nimpacted ASAG differently than other fields of NLP, as we noticed that the\nlearned representations alone do not contribute to achieve the best results,\nbut they rather show to work in a complementary way with hand-engineered\nfeatures. The best performance are indeed achieved by methods that combine the\ncarefully hand-engineered features with the power of the semantic descriptions\nprovided by the latest models, like transformers architectures. We identify\nchallenges and provide an outlook on research direction that can be addressed\nin the future\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haller_S/0/1/0/all/0/1\">Stefan Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldea_A/0/1/0/all/0/1\">Adina Aldea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seifert_C/0/1/0/all/0/1\">Christin Seifert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strisciuglio_N/0/1/0/all/0/1\">Nicola Strisciuglio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Rankings into Quantized Scores in Peer Review. (arXiv:2204.03505v1 [cs.IR])","link":"http://arxiv.org/abs/2204.03505","description":"<p>In peer review, reviewers are usually asked to provide scores for the papers.\nThe scores are then used by Area Chairs or Program Chairs in various ways in\nthe decision-making process. The scores are usually elicited in a quantized\nform to accommodate the limited cognitive ability of humans to describe their\nopinions in numerical values. It has been found that the quantized scores\nsuffer from a large number of ties, thereby leading to a significant loss of\ninformation. To mitigate this issue, conferences have started to ask reviewers\nto additionally provide a ranking of the papers they have reviewed. There are\nhowever two key challenges. First, there is no standard procedure for using\nthis ranking information and Area Chairs may use it in different ways\n(including simply ignoring them), thereby leading to arbitrariness in the\npeer-review process. Second, there are no suitable interfaces for judicious use\nof this data nor methods to incorporate it in existing workflows, thereby\nleading to inefficiencies. We take a principled approach to integrate the\nranking information into the scores. The output of our method is an updated\nscore pertaining to each review that also incorporates the rankings. Our\napproach addresses the two aforementioned challenges by: (i) ensuring that\nrankings are incorporated into the updates scores in the same manner for all\npapers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use\nexisting interfaces and workflows designed for scores. We empirically evaluate\nour method on synthetic datasets as well as on peer reviews from the ICLR 2017\nconference, and find that it reduces the error by approximately 30% as compared\nto the best performing baseline on the ICLR 2017 data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yusha Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nihar B. Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aarti Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QCRI's COVID-19 Disinformation Detector: A System to Fight the COVID-19 Infodemic in Social Media. (arXiv:2204.03506v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03506","description":"<p>Fighting the ongoing COVID-19 infodemic has been declared as one of the most\nimportant focus areas by the World Health Organization since the onset of the\nCOVID-19 pandemic. While the information that is consumed and disseminated\nconsists of promoting fake cures, rumors, and conspiracy theories to spreading\nxenophobia and panic, at the same time there is information (e.g., containing\nadvice, promoting cure) that can help different stakeholders such as\npolicy-makers. Social media platforms enable the infodemic and there has been\nan effort to curate the content on such platforms, analyze and debunk them.\nWhile a majority of the research efforts consider one or two aspects (e.g.,\ndetecting factuality) of such information, in this study we focus on a\nmultifaceted approach, including an\nAPI,\\url{https://app.swaggerhub.com/apis/yifan2019/Tanbih/0.8.0/} and a demo\nsystem,\\url{https://covid19.tanbih.org}, which we made freely and publicly\navailable. We believe that this will facilitate researchers and different\nstakeholders. A screencast of the API services and demo is\navailable.\\url{https://youtu.be/zhbcSvxEKMk}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Animesh Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Multi-task Learning in Natural Language Processing: Regarding Task Relatedness and Training Methods. (arXiv:2204.03508v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03508","description":"<p>Multi-task learning (MTL) has become increasingly popular in natural language\nprocessing (NLP) because it improves the performance of related tasks by\nexploiting their commonalities and differences. Nevertheless, it is still not\nunderstood very well how multi-task learning can be implemented based on the\nrelatedness of training tasks. In this survey, we review recent advances of\nmulti-task learning methods in NLP, with the aim of summarizing them into two\ngeneral multi-task training methods based on their task relatedness: (i) joint\ntraining and (ii) multi-step training. We present examples in various NLP\ndownstream applications, summarize the task relationships and discuss future\ndirections of this promising topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mengxia Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhichun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging pre-trained language models for conversational information seeking from text. (arXiv:2204.03542v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03542","description":"<p>Recent advances in Natural Language Processing, and in particular on the\nconstruction of very large pre-trained language representation models, is\nopening up new perspectives on the construction of conversational information\nseeking (CIS) systems. In this paper we investigate the usage of in-context\nlearning and pre-trained language representation models to address the problem\nof information extraction from process description documents, in an incremental\nquestion and answering oriented fashion. In particular we investigate the usage\nof the native GPT-3 (Generative Pre-trained Transformer 3) model, together with\ntwo in-context learning customizations that inject conceptual definitions and a\nlimited number of samples in a few shot-learning fashion. The results highlight\nthe potential of the approach and the usefulness of the in-context learning\ncustomizations, which can substantially contribute to address the \"training\ndata challenge\" of deep learning based NLP techniques the BPM field. It also\nhighlight the challenge posed by control flow relations for which further\ntraining needs to be devised.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bellan_P/0/1/0/all/0/1\">Patrizio Bellan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragoni_M/0/1/0/all/0/1\">Mauro Dragoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidini_C/0/1/0/all/0/1\">Chiara Ghidini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic. (arXiv:2204.03558v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03558","description":"<p>As natural language processing systems become more widespread, it is\nnecessary to address fairness issues in their implementation and deployment to\nensure that their negative impacts on society are understood and minimized.\nHowever, there is limited work that studies fairness using a multilingual and\nintersectional framework or on downstream tasks. In this paper, we introduce\nfour multilingual Equity Evaluation Corpora, supplementary test sets designed\nto measure social biases, and a novel statistical framework for studying\nunisectional and intersectional social biases in natural language processing.\nWe use these tools to measure gender, racial, ethnic, and intersectional social\nbiases across five models trained on emotion regression tasks in English,\nSpanish, and Arabic. We find that many systems demonstrate statistically\nsignificant unisectional and intersectional social biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camara_A/0/1/0/all/0/1\">Ant&#xf3;nio C&#xe2;mara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taneja_N/0/1/0/all/0/1\">Nina Taneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azad_T/0/1/0/all/0/1\">Tamjeed Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allaway_E/0/1/0/all/0/1\">Emily Allaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotional Speech Recognition with Pre-trained Deep Visual Models. (arXiv:2204.03561v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03561","description":"<p>In this paper, we propose a new methodology for emotional speech recognition\nusing visual deep neural network models. We employ the transfer learning\ncapabilities of the pre-trained computer vision deep models to have a mandate\nfor the emotion recognition in speech task. In order to achieve that, we\npropose to use a composite set of acoustic features and a procedure to convert\nthem into images. Besides, we present a training paradigm for these models\ntaking into consideration the different characteristics between acoustic-based\nimages and regular ones. In our experiments, we use the pre-trained VGG-16\nmodel and test the overall methodology on the Berlin EMO-DB dataset for\nspeaker-independent emotion recognition. We evaluate the proposed model on the\nfull list of the seven emotions and the results set a new state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ragheb_W/0/1/0/all/0/1\">Waleed Ragheb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzapour_M/0/1/0/all/0/1\">Mehdi Mirzapour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delfardi_A/0/1/0/all/0/1\">Ali Delfardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacquenet_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Jacquenet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbon_L/0/1/0/all/0/1\">Lawrence Carbon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v1 [cs.LG])","link":"http://arxiv.org/abs/2204.03574","description":"<p>We introduce compositional soft prompting (CSP), a parameter-efficient\nlearning technique to improve the zero-shot compositionality of large-scale\npretrained vision-language models (VLMs) without the overhead of fine-tuning\nthe entire model. VLMs can represent arbitrary classes as natural language\nprompts in their flexible text encoders but they underperform state-of-the-art\nmethods on compositional zero-shot benchmark tasks. To improve VLMs, we propose\na novel form of soft prompting. We treat the attributes and objects that are\ncomposed to define classes as learnable tokens of vocabulary and tune them on\nmultiple prompt compositions. During inference, we recompose the learned\nattribute-object vocabulary in new combinations and show that CSP outperforms\nthe original VLM on benchmark datasets by an average of 14.7 percentage points\nof accuracy. CSP also achieves new state-of-the-art accuracies on two out of\nthree benchmark datasets, while only fine-tuning a small number of parameters.\nFurther, we show that CSP improves generalization to higher-order\nattribute-attribute-object compositions and combinations of pretrained\nattributes and fine-tuned objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nihal V. Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peilin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing the limits of natural language models for predicting human language judgments. (arXiv:2204.03592v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03592","description":"<p>Neural network language models can serve as computational hypotheses about\nhow humans process language. We compared the model-human consistency of diverse\nlanguage models using a novel experimental approach: controversial sentence\npairs. For each controversial sentence pair, two language models disagree about\nwhich sentence is more likely to occur in natural text. Considering nine\nlanguage models (including n-gram, recurrent neural networks, and transformer\nmodels), we created hundreds of such controversial sentence pairs by either\nselecting sentences from a corpus or synthetically optimizing sentence pairs to\nbe highly controversial. Human subjects then provided judgments indicating for\neach pair which of the two sentences is more likely. Controversial sentence\npairs proved highly effective at revealing model failures and identifying\nmodels that aligned most closely with human judgments. The most\nhuman-consistent model tested was GPT-2, although experiments also revealed\nsignificant shortcomings of its alignment with human perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golan_T/0/1/0/all/0/1\">Tal Golan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegelman_M/0/1/0/all/0/1\">Matthew Siegelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kriegeskorte_N/0/1/0/all/0/1\">Nikolaus Kriegeskorte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldassano_C/0/1/0/all/0/1\">Christopher Baldassano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Label Correlations for Second-Order Semantic Dependency Parsing with Mean-Field Inference. (arXiv:2204.03619v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03619","description":"<p>Second-order semantic parsing with end-to-end mean-field inference has been\nshown good performance. In this work we aim to improve this method by modeling\nlabel correlations between adjacent arcs. However, direct modeling leads to\nmemory explosion because second-order score tensors have sizes of $O(n^3L^2)$\n($n$ is the sentence length and $L$ is the number of labels), which is not\naffordable. To tackle this computational challenge, we leverage tensor\ndecomposition techniques, and interestingly, we show that the large\nsecond-order score tensors have no need to be materialized during mean-field\ninference, thereby reducing the computational complexity from cubic to\nquadratic. We conduct experiments on SemEval 2015 Task 18 English datasets,\nshowing the effectiveness of modeling label correlations. Our code is publicly\navailable at https://github.com/sustcsonglin/mean-field-dep-parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"tmVar 3.0: an improved variant concept recognition and normalization tool. (arXiv:2204.03637v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03637","description":"<p>Previous studies have shown that automated text-mining tools are becoming\nincreasingly important for successfully unlocking variant information in\nscientific literature at large scale. Despite multiple attempts in the past,\nexisting tools are still of limited recognition scope and precision. We propose\ntmVar 3.0: an improved variant recognition and normalization tool. Compared to\nits predecessors, tmVar 3.0 is able to recognize a wide spectrum of variant\nrelated entities (e.g., allele and copy number variants), and to group\ndifferent variant mentions belonging to the same concept in an article for\nimproved accuracy. Moreover, tmVar3 provides additional variant normalization\noptions such as allele-specific identifiers from the ClinGen Allele Registry.\ntmVar3 exhibits a state-of-the-art performance with over 90% accuracy in\nF-measure in variant recognition and normalization, when evaluated on three\nindependent benchmarking datasets. tmVar3 is freely available for download. We\nhave also processed the entire PubMed and PMC with tmVar3 and released its\nannotations on our FTP. Availability: <a href=\"ftp://ftp.ncbi.nlm.nih.gov/pub/lu/tmVar3\">this ftp URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chih-Hsuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allot_A/0/1/0/all/0/1\">Alexis Allot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riehle_K/0/1/0/all/0/1\">Kevin Riehle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milosavljevic_A/0/1/0/all/0/1\">Aleksandar Milosavljevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Entity Linking: A Survey of Models Based on Deep Learning. (arXiv:2006.00575v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.00575","description":"<p>This survey presents a comprehensive description of recent neural entity\nlinking (EL) systems developed since 2015 as a result of the \"deep learning\nrevolution\" in natural language processing. Its goal is to systemize design\nfeatures of neural entity linking systems and compare their performance to the\nremarkable classic methods on common benchmarks. This work distills a generic\narchitecture of a neural EL system and discusses its components, such as\ncandidate generation, mention-context encoding, and entity ranking, summarizing\nprominent methods for each of them. The vast variety of modifications of this\ngeneral architecture are grouped by several common themes: joint entity mention\ndetection and disambiguation, models for global linking, domain-independent\ntechniques including zero-shot and distant supervision methods, and\ncross-lingual approaches. Since many neural models take advantage of entity and\nmention/context embeddings to represent their meaning, this work also overviews\nprominent entity embedding techniques. Finally, the survey touches on\napplications of entity linking, focusing on the recently emerged use-case of\nenhancing deep pre-trained masked language models based on the Transformer\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sevgili_O/0/1/0/all/0/1\">Ozge Sevgili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelmanov_A/0/1/0/all/0/1\">Artem Shelmanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arkhipov_M/0/1/0/all/0/1\">Mikhail Arkhipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Improving Selective Prediction Ability of NLP Systems. (arXiv:2008.09371v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.09371","description":"<p>It's better to say \"I can't answer\" than to answer incorrectly. This\nselective prediction ability is crucial for NLP systems to be reliably deployed\nin real-world applications. Prior work has shown that existing selective\nprediction techniques fail to perform well, especially in the out-of-domain\nsetting. In this work, we propose a method that improves probability estimates\nof models by calibrating them using prediction confidence and difficulty score\nof instances. Using these two signals, we first annotate held-out instances and\nthen train a calibrator to predict the likelihood of correctness of the model's\nprediction. We instantiate our method with Natural Language Inference (NLI) and\nDuplicate Detection (DD) tasks and evaluate it in both In-Domain (IID) and\nOut-of-Domain (OOD) settings. In (IID, OOD) settings, we show that the\nrepresentations learned by our calibrator result in an improvement of (15.81%,\n5.64%) and (6.19%, 13.9%) over 'MaxProb' -- a selective prediction baseline --\non NLI and DD tasks respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributed NLI: Learning to Predict Human Opinion Distributions for Language Reasoning. (arXiv:2104.08676v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08676","description":"<p>We introduce distributed NLI, a new NLU task with a goal to predict the\ndistribution of human judgements for natural language inference. We show that\nby applying additional distribution estimation methods, namely, Monte Carlo\n(MC) Dropout, Deep Ensemble, Re-Calibration, and Distribution Distillation,\nmodels can capture human judgement distribution more effectively than the\nsoftmax baseline. We show that MC Dropout is able to achieve decent performance\nwithout any distribution annotations while Re-Calibration can give further\nimprovements with extra distribution annotations, suggesting the value of\nmultiple annotations for one example in modeling the distribution of human\njudgements. Despite these improvements, the best results are still far below\nthe estimated human upper-bound, indicating that predicting the distribution of\nhuman judgements is still an open, challenging problem with a large room for\nimprovements. We showcase the common errors for MC Dropout and Re-Calibration.\nFinally, we give guidelines on the usage of these methods with different levels\nof data availability and encourage future work on modeling the human opinion\ndistribution for language reasoning. Our code and data are publicly available\nat https://github.com/easonnie/ChaosNLI\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yixin Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lambek pregroups are Frobenius spiders in preorders. (arXiv:2105.03038v4 [math.CT] UPDATED)","link":"http://arxiv.org/abs/2105.03038","description":"<p>\"Spider\" is a nickname of special Frobenius algebras, a fundamental structure\nfrom mathematics, physics, and computer science. Pregroups are a fundamental\nstructure from linguistics. Pregroups and spiders have been used together in\nnatural language processing: one for syntax, the other for semantics. It turns\nout that pregroups themselves can be characterized as pointed spiders in the\ncategory of preordered relations, where they naturally arise from grammars. The\nother way around, preordered spider algebras in general can be characterized as\nunions of pregroups. This extends the characterization of relational spider\nalgebras as disjoint unions of groups. The compositional framework that emerged\nwith the results suggests new ways to understand and apply the basis structures\nin machine learning and data analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Pavlovic_D/0/1/0/all/0/1\">Dusko Pavlovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Pretrained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection. (arXiv:2106.04564v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.04564","description":"<p>Pre-trained Transformer-based models were reported to be robust in intent\nclassification. In this work, we first point out the importance of in-domain\nout-of-scope detection in few-shot intent recognition tasks and then illustrate\nthe vulnerability of pre-trained Transformer-based models against samples that\nare in-domain but out-of-scope (ID-OOS). We construct two new datasets, and\nempirically show that pre-trained models do not perform well on both ID-OOS\nexamples and general out-of-scope examples, especially on fine-grained few-shot\nintent detection tasks. To figure out how the models mistakenly classify ID-OOS\nintents as in-scope intents, we further conduct analysis on confidence scores\nand the overlapping keywords, as well as point out several prospective\ndirections for future work. Resources are available on\nhttps://github.com/jianguoz/Few-Shot-Intent-Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Controlled Generation with Encoder-Decoder Transformers. (arXiv:2106.06411v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06411","description":"<p>Controlling neural network-based models for natural language generation (NLG)\nhas broad applications in numerous areas such as machine translation, document\nsummarization, and dialog systems. Approaches that enable such control in a\nzero-shot manner would be of great importance as, among other reasons, they\nremove the need for additional annotated data and training. In this work, we\npropose novel approaches for controlling encoder-decoder transformer-based NLG\nmodels in zero-shot. This is done by introducing three control knobs, namely,\nattention biasing, decoder mixing, and context augmentation, that are applied\nto these models at generation time. These knobs control the generation process\nby directly manipulating trained NLG models (e.g., biasing cross-attention\nlayers) to realize the desired attributes in the generated outputs. We show\nthat not only are these NLG models robust to such manipulations, but also their\nbehavior could be controlled without an impact on their generation performance.\nThese results, to the best of our knowledge, are the first of their kind.\nThrough these control knobs, we also investigate the role of transformer\ndecoder's self-attention module and show strong evidence that its primary role\nis maintaining fluency of sentences generated by these models. Based on this\nhypothesis, we show that alternative architectures for transformer decoders\ncould be viable options. We also study how this hypothesis could lead to more\nefficient ways for training encoder-decoder transformer models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namazifar_M/0/1/0/all/0/1\">Mahdi Namazifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-T&#xfc;r</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KazNERD: Kazakh Named Entity Recognition Dataset. (arXiv:2111.13419v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.13419","description":"<p>We present the development of a dataset for Kazakh named entity recognition.\nThe dataset was built as there is a clear need for publicly available annotated\ncorpora in Kazakh, as well as annotation guidelines containing\nstraightforward--but rigorous--rules and examples. The dataset annotation,\nbased on the IOB2 scheme, was carried out on television news text by two native\nKazakh speakers under the supervision of the first author. The resulting\ndataset contains 112,702 sentences and 136,333 annotations for 25 entity\nclasses. State-of-the-art machine learning models to automatise Kazakh named\nentity recognition were also built, with the best-performing model achieving an\nexact match F1-score of 97.22% on the test set. The annotated dataset,\nguidelines, and codes used to train the models are freely available for\ndownload under the CC BY 4.0 licence from https://github.com/IS2AI/KazNERD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeshpanov_R/0/1/0/all/0/1\">Rustem Yeshpanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khassanov_Y/0/1/0/all/0/1\">Yerbolat Khassanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Data-based Curricula Work?. (arXiv:2112.06510v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06510","description":"<p>Current state-of-the-art NLP systems use large neural networks that require\nlots of computational resources for training. Inspired by human knowledge\nacquisition, researchers have proposed curriculum learning, - sequencing of\ntasks (task-based curricula) or ordering and sampling of the datasets\n(data-based curricula) that facilitate training. This work investigates the\nbenefits of data-based curriculum learning for large modern language models\nsuch as BERT and T5. We experiment with various curricula based on a range of\ncomplexity measures and different sampling strategies. Extensive experiments on\ndifferent NLP tasks show that curricula based on various complexity measures\nrarely has any benefits while random sampling performs either as well or better\nthan curricula.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Surkov_M/0/1/0/all/0/1\">Maxim K. Surkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosin_V/0/1/0/all/0/1\">Vladislav D. Mosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer. (arXiv:2202.07305v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07305","description":"<p>Image narrative generation is a task to create a story from an image with a\nsubjective viewpoint. Given the importance of the subjective feelings of\nwriters, readers, and characters in storytelling, an image narrative generation\nmethod should consider human emotion. In this study, we propose a novel method\nof image narrative generation called ViNTER (Visual Narrative Transformer with\nEmotion arc Representation), which takes \"emotion arc\" as input to capture a\nsequence of emotional changes. Since emotion arcs represent the trajectory of\nemotional change, it is expected that we can include detailed information about\nthe emotional changes in the story to the model. We present experimental\nresults of both automatic and manual evaluations on the Image Narrative dataset\nand demonstrate the effectiveness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1\">Kohei Uehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_Y/0/1/0/all/0/1\">Yusuke Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1\">Yusuke Mukuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining On Alzheimer's Diseases Related Knowledge Graph to Identity Potential AD-related Semantic Triples for Drug Repurposing. (arXiv:2202.08712v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.08712","description":"<p>To date, there are no effective treatments for most neurodegenerative\ndiseases. Knowledge graphs can provide comprehensive and semantic\nrepresentation for heterogeneous data, and have been successfully leveraged in\nmany biomedical applications including drug repurposing. Our objective is to\nconstruct a knowledge graph from literature to study relations between\nAlzheimer's disease (AD) and chemicals, drugs and dietary supplements in order\nto identify opportunities to prevent or delay neurodegenerative progression. We\ncollected biomedical annotations and extracted their relations using SemRep via\nSemMedDB. We used both a BERT-based classifier and rule-based methods during\ndata preprocessing to exclude noise while preserving most AD-related semantic\ntriples. The 1,672,110 filtered triples were used to train with knowledge graph\ncompletion algorithms (i.e., TransE, DistMult, and ComplEx) to predict\ncandidates that might be helpful for AD treatment or prevention. Among three\nknowledge graph completion models, TransE outperformed the other two (MR =\n13.45, Hits@1 = 0.306). We leveraged the time-slicing technique to further\nevaluate the prediction results. We found supporting evidence for most highly\nranked candidates predicted by our model which indicates that our approach can\ninform reliable new knowledge. This paper shows that our graph mining model can\npredict reliable new relationships between AD and other entities (i.e., dietary\nsupplements, chemicals, and drugs). The knowledge graph constructed can\nfacilitate data-driven knowledge discoveries and the generation of novel\nhypotheses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nian_Y/0/1/0/all/0/1\">Yi Nian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jingna Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingcheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Cui Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2203.13366","description":"<p>For a long period, different recommendation tasks typically require designing\ntask-specific architectures and training objectives. As a result, it is hard to\ntransfer the learned knowledge and representations from one task to another,\nthus restricting the generalization ability of existing recommendation\napproaches, e.g., a sequential recommendation model can hardly be applied or\ntransferred to a review generation method. To deal with such issues,\nconsidering that language grounding is a powerful medium to describe and\nrepresent various problems or tasks, we present a flexible and unified\ntext-to-text paradigm called \"Pretrain, Personalized Prompt, and Predict\nParadigm\" (P5) for recommendation, which unifies various recommendation tasks\nin a shared framework. In P5, all data such as user-item interactions, item\nmetadata, and user reviews are converted to a common format -- natural language\nsequences. The rich information from natural language assist P5 to capture\ndeeper semantics for recommendation. P5 learns different tasks with the same\nlanguage modeling objective during pretraining. Thus, it possesses the\npotential to serve as the foundation model for downstream recommendation tasks,\nallows easy integration with other modalities, and enables instruction-based\nrecommendation, which will revolutionize the technical form of recommender\nsystem towards universal recommendation engine. With adaptive personalized\nprompt for different users, P5 is able to make predictions in a zero-shot or\nfew-shot manner and largely reduces the necessity for extensive fine-tuning. On\nseveral recommendation benchmarks, we conduct experiments to show the\neffectiveness of our generative approach. We will release our prompts and\npretrained P5 language model to help advance future research on Recommendation\nas Language Processing (RLP) and Personalized Foundation Models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zuohui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues. (arXiv:2203.13926v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13926","description":"<p>This paper addresses the problem of dialogue reasoning with contextualized\ncommonsense inference. We curate CICERO, a dataset of dyadic conversations with\nfive types of utterance-level reasoning-based inferences: cause, subsequent\nevent, prerequisite, motivation, and emotional reaction. The dataset contains\n53,105 of such inferences from 5,672 dialogues. We use this dataset to solve\nrelevant generative and discriminative tasks: generation of cause and\nsubsequent event; generation of prerequisite, motivation, and listener's\nemotional reaction; and selection of plausible alternatives. Our results\nascertain the value of such dialogue-centric commonsense knowledge datasets. It\nis our hope that CICERO will open new research avenues into commonsense-based\ndialogue reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment. (arXiv:2203.15937v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.15937","description":"<p>Current leading mispronunciation detection and diagnosis (MDD) systems\nachieve promising performance via end-to-end phoneme recognition. One challenge\nof such end-to-end solutions is the scarcity of human-annotated phonemes on\nnatural L2 speech. In this work, we leverage unlabeled L2 speech via a\npseudo-labeling (PL) procedure and extend the fine-tuning approach based on\npre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec\n2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples\nplus the created pseudo-labeled L2 speech samples. Our pseudo labels are\ndynamic and are produced by an ensemble of the online model on-the-fly, which\nensures that our model is robust to pseudo label noise. We show that\nfine-tuning with pseudo labels gains a 5.35% phoneme error rate reduction and\n2.48% MDD F1 score improvement over a labeled-samples-only fine-tuning\nbaseline. The proposed PL method is also shown to outperform conventional\noffline PL methods. Compared to the state-of-the-art MDD systems, our MDD\nsolution achieves a more accurate and consistent phonetic error diagnosis. In\naddition, we conduct an open test on a separate UTD-4Accents dataset, where our\nsystem recognition outputs show a strong correlation with human perception,\nbased on accentedness and intelligibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Mu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hirschi_K/0/1/0/all/0/1\">Kevin Hirschi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Looney_S/0/1/0/all/0/1\">Stephen D. Looney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_O/0/1/0/all/0/1\">Okim Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_J/0/1/0/all/0/1\">John H. L. Hansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PanGu-Bot: Efficient Generative Dialogue Pre-training from Pre-trained Language Model. (arXiv:2203.17090v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.17090","description":"<p>In this paper, we introduce PanGu-Bot, a Chinese pre-trained open-domain\ndialogue generation model based on a large pre-trained language model (PLM)\nPANGU-alpha (Zeng et al.,2021). Different from other pre-trained dialogue\nmodels trained over a massive amount of dialogue data from scratch, we aim to\nbuild a powerful dialogue model with relatively fewer data and computation\ncosts by inheriting valuable language capabilities and knowledge from PLMs. To\nthis end, we train PanGu-Bot from the large PLM PANGU-alpha, which has been\nproven well-performed on a variety of Chinese natural language tasks. We\ninvestigate different aspects of responses generated by PanGu-Bot, including\nresponse quality, knowledge, and safety. We show that PanGu-Bot outperforms\nstate-of-the-art Chinese dialogue systems (CDIALGPT (Wang et al., 2020), EVA\n(Zhou et al., 2021)) w.r.t. the above three aspects. We also demonstrate that\nPanGu-Bot can be easily deployed to generate emotional responses without\nfurther training. Throughout our empirical analysis, we also point out that the\nPanGu-Bot response quality, knowledge correctness, and safety are still far\nfrom perfect, and further explorations are indispensable to building reliable\nand smart dialogue systems. Our model and code will be available at\nhttps://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-Bot\nsoon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yulong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chuanfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shiqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PaLM: Scaling Language Modeling with Pathways. (arXiv:2204.02311v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02311","description":"<p>Large language models have been shown to achieve remarkable performance\nacross a variety of natural language tasks using few-shot learning, which\ndrastically reduces the number of task-specific training examples needed to\nadapt the model to a particular application. To further our understanding of\nthe impact of scale on few-shot learning, we trained a 540-billion parameter,\ndensely activated, Transformer language model, which we call Pathways Language\nModel PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML\nsystem which enables highly efficient training across multiple TPU Pods. We\ndemonstrate continued benefits of scaling by achieving state-of-the-art\nfew-shot learning results on hundreds of language understanding and generation\nbenchmarks. On a number of these tasks, PaLM 540B achieves breakthrough\nperformance, outperforming the finetuned state-of-the-art on a suite of\nmulti-step reasoning tasks, and outperforming average human performance on the\nrecently released BIG-bench benchmark. A significant number of BIG-bench tasks\nshowed discontinuous improvements from model scale, meaning that performance\nsteeply increased as we scaled to our largest model. PaLM also has strong\ncapabilities in multilingual tasks and source code generation, which we\ndemonstrate on a wide array of benchmarks. We additionally provide a\ncomprehensive analysis on bias and toxicity, and study the extent of training\ndata memorization with respect to model scale. Finally, we discuss the ethical\nconsiderations related to large language models and discuss potential\nmitigation strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1\">Jacob Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1\">Gaurav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barham_P/0/1/0/all/0/1\">Paul Barham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuh_P/0/1/0/all/0/1\">Parker Schuh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kensen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvyashchenko_S/0/1/0/all/0/1\">Sasha Tsvyashchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Abhishek Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_P/0/1/0/all/0/1\">Parker Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_B/0/1/0/all/0/1\">Ben Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pope_R/0/1/0/all/0/1\">Reiner Pope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradbury_J/0/1/0/all/0/1\">James Bradbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isard_M/0/1/0/all/0/1\">Michael Isard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gur_Ari_G/0/1/0/all/0/1\">Guy Gur-Ari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duke_T/0/1/0/all/0/1\">Toju Duke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levskaya_A/0/1/0/all/0/1\">Anselm Levskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghemawat_S/0/1/0/all/0/1\">Sanjay Ghemawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1\">Vedant Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1\">Kevin Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_L/0/1/0/all/0/1\">Liam Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_D/0/1/0/all/0/1\">David Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyeontaek Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiridonov_A/0/1/0/all/0/1\">Alexander Spiridonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sepassi_R/0/1/0/all/0/1\">Ryan Sepassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Shivani Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omernick_M/0/1/0/all/0/1\">Mark Omernick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillai_T/0/1/0/all/0/1\">Thanumalayan Sankaranarayana Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellat_M/0/1/0/all/0/1\">Marie Pellat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewkowycz_A/0/1/0/all/0/1\">Aitor Lewkowycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_E/0/1/0/all/0/1\">Erica Moreira</a>, et al. (15 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple and Effective Unsupervised Speech Synthesis. (arXiv:2204.02524v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2204.02524","description":"<p>We introduce the first unsupervised speech synthesis system based on a\nsimple, yet effective recipe. The framework leverages recent work in\nunsupervised speech recognition as well as existing neural-based speech\nsynthesis. Using only unlabeled speech audio and unlabeled text as well as a\nlexicon, our method enables speech synthesis without the need for a\nhuman-labeled corpus. Experiments demonstrate the unsupervised system can\nsynthesize speech similar to a supervised counterpart in terms of naturalness\nand intelligibility measured by human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alexander H. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Jeff Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevskiv_A/0/1/0/all/0/1\">Alexei Baevskiv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"drsphelps at SemEval-2022 Task 2: Learning idiom representations using BERTRAM. (arXiv:2204.02821v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02821","description":"<p>This paper describes our system for SemEval-2022 Task 2 Multilingual\nIdiomaticity Detection and Sentence Embedding sub-task B. We modify a standard\nBERT sentence transformer by adding embeddings for each idioms, which are\ncreated using BERTRAM and a small number of contexts. We show that this\ntechnique increases the quality of idiom representations and leads to better\nperformance on the task. We also perform analysis on our final results and show\nthat the quality of the produced idiom embeddings is highly sensitive to the\nquality of the input contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phelps_D/0/1/0/all/0/1\">Dylan Phelps</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Follow My Eye: Using Gaze to Supervise Computer-Aided Diagnosis. (arXiv:2204.02976v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02976","description":"<p>When deep neural network (DNN) was first introduced to the medical image\nanalysis community, researchers were impressed by its performance. However, it\nis evident now that a large number of manually labeled data is often a must to\ntrain a properly functioning DNN. This demand for supervision data and labels\nis a major bottleneck in current medical image analysis, since collecting a\nlarge number of annotations from experienced experts can be time-consuming and\nexpensive. In this paper, we demonstrate that the eye movement of radiologists\nreading medical images can be a new form of supervision to train the DNN-based\ncomputer-aided diagnosis (CAD) system. Particularly, we record the tracks of\nthe radiologists' gaze when they are reading images. The gaze information is\nprocessed and then used to supervise the DNN's attention via an Attention\nConsistency module. To the best of our knowledge, the above pipeline is among\nthe earliest efforts to leverage expert eye movement for deep-learning-based\nCAD. We have conducted extensive experiments on knee X-ray images for\nosteoarthritis assessment. The results show that our method can achieve\nconsiderable improvement in diagnosis performance, with the help of gaze\nsupervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_X/0/1/0/all/0/1\">Xi Ouyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scale Memory-Based Video Deblurring. (arXiv:2204.02977v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02977","description":"<p>Video deblurring has achieved remarkable progress thanks to the success of\ndeep neural networks. Most methods solve for the deblurring end-to-end with\nlimited information propagation from the video sequence. However, different\nframe regions exhibit different characteristics and should be provided with\ncorresponding relevant information. To achieve fine-grained deblurring, we\ndesigned a memory branch to memorize the blurry-sharp feature pairs in the\nmemory bank, thus providing useful information for the blurry query input. To\nenrich the memory of our memory bank, we further designed a bidirectional\nrecurrency and multi-scale strategy based on the memory bank. Experimental\nresults demonstrate that our model outperforms other state-of-the-art methods\nwhile keeping the model complexity and inference time low. The code is\navailable at https://github.com/jibo27/MemDeblur.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ji_B/0/1/0/all/0/1\">Bo Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Different Losses for Deep Learning Image Colorization. (arXiv:2204.02980v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02980","description":"<p>Image colorization aims to add color information to a grayscale image in a\nrealistic way. Recent methods mostly rely on deep learning strategies. While\nlearning to automatically colorize an image, one can define well-suited\nobjective functions related to the desired color output. Some of them are based\non a specific type of error between the predicted image and ground truth one,\nwhile other losses rely on the comparison of perceptual properties. But, is the\nchoice of the objective function that crucial, i.e., does it play an important\nrole in the results? In this chapter, we aim to answer this question by\nanalyzing the impact of the loss function on the estimated colorization\nresults. To that goal, we review the different losses and evaluation metrics\nthat are used in the literature. We then train a baseline network with several\nof the reviewed objective functions: classic L1 and L2 losses, as well as more\ncomplex combinations such as Wasserstein GAN and VGG-based LPIPS loss.\nQuantitative results show that the models trained with VGG-based LPIPS provide\noverall slightly better results for most evaluation metrics. Qualitative\nresults exhibit more vivid colors when with Wasserstein GAN plus the L2 loss or\nagain with the VGG-based LPIPS. Finally, the convenience of quantitative user\nstudies is also discussed to overcome the difficulty of properly assessing on\ncolorized images, notably for the case of old archive photographs where no\nground truth is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ballester_C/0/1/0/all/0/1\">Coloma Ballester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugeau_A/0/1/0/all/0/1\">Aur&#xe9;lie Bugeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrillo_H/0/1/0/all/0/1\">Hernan Carrillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clement_M/0/1/0/all/0/1\">Micha&#xeb;l Cl&#xe9;ment</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giraud_R/0/1/0/all/0/1\">R&#xe9;mi Giraud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raad_L/0/1/0/all/0/1\">Lara Raad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitoria_P/0/1/0/all/0/1\">Patricia Vitoria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientCellSeg: Efficient Volumetric Cell Segmentation Using Context Aware Pseudocoloring. (arXiv:2204.03014v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03014","description":"<p>Volumetric cell segmentation in fluorescence microscopy images is important\nto study a wide variety of cellular processes. Applications range from the\nanalysis of cancer cells to behavioral studies of cells in the embryonic stage.\nLike in other computer vision fields, most recent methods use either large\nconvolutional neural networks (CNNs) or vision transformer models (ViTs). Since\nthe number of available 3D microscopy images is typically limited in\napplications, we take a different approach and introduce a small CNN for\nvolumetric cell segmentation. Compared to previous CNN models for cell\nsegmentation, our model is efficient and has an asymmetric encoder-decoder\nstructure with very few parameters in the decoder. Training efficiency is\nfurther improved via transfer learning. In addition, we introduce Context Aware\nPseudocoloring to exploit spatial context in z-direction of 3D images while\nperforming volumetric cell segmentation slice-wise. We evaluated our method\nusing different 3D datasets from the Cell Segmentation Benchmark of the Cell\nTracking Challenge. Our segmentation method achieves top-ranking results, while\nour CNN model has an up to 25x lower number of parameters than other\ntop-ranking methods. Code and pretrained models are available at:\nhttps://github.com/roydenwa/efficient-cell-seg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wagner_R/0/1/0/all/0/1\">Royden Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rohr_K/0/1/0/all/0/1\">Karl Rohr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Untrimmed Videos: Self-Supervised Video Representation Learning with Hierarchical Consistency. (arXiv:2204.03017v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03017","description":"<p>Natural videos provide rich visual contents for self-supervised learning. Yet\nmost existing approaches for learning spatio-temporal representations rely on\nmanually trimmed videos, leading to limited diversity in visual patterns and\nlimited performance gain. In this work, we aim to learn representations by\nleveraging more abundant information in untrimmed videos. To this end, we\npropose to learn a hierarchy of consistencies in videos, i.e., visual\nconsistency and topical consistency, corresponding respectively to clip pairs\nthat tend to be visually similar when separated by a short time span and share\nsimilar topics when separated by a long time span. Specifically, a hierarchical\nconsistency learning framework HiCo is presented, where the visually consistent\npairs are encouraged to have the same representation through contrastive\nlearning, while the topically consistent pairs are coupled through a topical\nclassifier that distinguishes whether they are topic related. Further, we\nimpose a gradual sampling algorithm for proposed hierarchical consistency\nlearning, and demonstrate its theoretical superiority. Empirically, we show\nthat not only HiCo can generate stronger representations on untrimmed videos,\nit also improves the representation quality when applied to trimmed videos.\nThis is in contrast to standard contrastive learning that fails to learn\nappropriate representations from untrimmed videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical Model Criticism of Variational Auto-Encoders. (arXiv:2204.03030v1 [cs.LG])","link":"http://arxiv.org/abs/2204.03030","description":"<p>We propose a framework for the statistical evaluation of variational\nauto-encoders (VAEs) and test two instances of this framework in the context of\nmodelling images of handwritten digits and a corpus of English text. Our take\non evaluation is based on the idea of statistical model criticism, popular in\nBayesian data analysis, whereby a statistical model is evaluated in terms of\nits ability to reproduce statistics of an unknown data generating process from\nwhich we can obtain samples. A VAE learns not one, but two joint distributions\nover a shared sample space, each exploiting a choice of factorisation that\nmakes sampling tractable in one of two directions (latent-to-data,\ndata-to-latent). We evaluate samples from these distributions, assessing their\n(marginal) fit to the observed data and our choice of prior, and we also\nevaluate samples through a pipeline that connects the two distributions\nstarting from a data sample, assessing whether together they exploit and reveal\nlatent factors of variation that are useful to a practitioner. We show that\nthis methodology offers possibilities for model selection qualitatively beyond\nintrinsic evaluation metrics and at a finer granularity than commonly used\nstatistics can offer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barkhof_C/0/1/0/all/0/1\">Claartje Barkhof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSGN++: Exploiting Visual-Spatial Relation forStereo-based 3D Detectors. (arXiv:2204.03039v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03039","description":"<p>Camera-based 3D object detectors are welcome due to their wider deployment\nand lower price than LiDAR sensors. We revisit the prior stereo modeling DSGN\nabout the stereo volume constructions for representing both 3D geometry and\nsemantics. We polish the stereo modeling and propose our approach, DSGN++,\naiming for improving information flow throughout the 2D-to-3D pipeline in the\nfollowing three main aspects. First, to effectively lift the 2D information to\nstereo volume, we propose depth-wise plane sweeping (DPS) that allows denser\nconnections and extracts depth-guided features. Second, for better grasping\ndifferently spaced features, we present a novel stereo volume -- Dual-view\nStereo Volume (DSV) that integrates front-view and top-view features and\nreconstructs sub-voxel depth in the camera frustum. Third, as the foreground\nregion becomes less dominant in 3D space, we firstly propose a multi-modal data\nediting strategy -- Stereo-LiDAR Copy-Paste, which ensures cross-modal\nalignment and improves data efficiency. Without bells and whistles, extensive\nexperiments in various modality setups on the popular KITTI benchmark show that\nour method consistently outperforms other camera-based 3D detectors for all\ncategories. Code will be released at https://github.com/chenyilun95/DSGN2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shijia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing finetuned models for better pretraining. (arXiv:2204.03044v1 [cs.CL])","link":"http://arxiv.org/abs/2204.03044","description":"<p>Pretrained models are the standard starting point for training. This approach\nconsistently outperforms the use of a random initialization. However,\npretraining is a costly endeavour that few can undertake.\n</p>\n<p>In this paper, we create better base models at hardly any cost, by fusing\nmultiple existing fine tuned models into one. Specifically, we fuse by\naveraging the weights of these models. We show that the fused model results\nsurpass the pretrained model ones. We also show that fusing is often better\nthan intertraining.\n</p>\n<p>We find that fusing is less dependent on the target task. Furthermore, weight\ndecay nullifies intertraining effects but not those of fusing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venezian_E/0/1/0/all/0/1\">Elad Venezian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thermal to Visible Image Synthesis under Atmospheric Turbulence. (arXiv:2204.03057v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03057","description":"<p>In many practical applications of long-range imaging such as biometrics and\nsurveillance, thermal imagining modalities are often used to capture images in\nlow-light and nighttime conditions. However, such imaging systems often suffer\nfrom atmospheric turbulence, which introduces severe blur and deformation\nartifacts to the captured images. Such an issue is unavoidable in long-range\nimaging and significantly decreases the face verification accuracy. In this\npaper, we first investigate the problem with a turbulence simulation method on\nreal-world thermal images. An end-to-end reconstruction method is then proposed\nwhich can directly transform thermal images into visible-spectrum images by\nutilizing natural image priors based on a pre-trained StyleGAN2 network.\nCompared with the existing two-steps methods of consecutive turbulence\nmitigation and thermal to visible image translation, our method is demonstrated\nto be effective in terms of both the visual quality of the reconstructed\nresults and face verification accuracy. Moreover, to the best of our knowledge,\nthis is the first work that studies the problem of thermal to visible image\ntranslation under atmospheric turbulence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1\">Kangfu Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Y/0/1/0/all/0/1\">Yiqun Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Late multimodal fusion for image and audio music transcription. (arXiv:2204.03063v1 [cs.MM])","link":"http://arxiv.org/abs/2204.03063","description":"<p>Music transcription, which deals with the conversion of music sources into a\nstructured digital format, is a key problem for Music Information Retrieval\n(MIR). When addressing this challenge in computational terms, the MIR community\nfollows two lines of research: music documents, which is the case of Optical\nMusic Recognition (OMR), or audio recordings, which is the case of Automatic\nMusic Transcription (AMT). The different nature of the aforementioned input\ndata has conditioned these fields to develop modality-specific frameworks.\nHowever, their recent definition in terms of sequence labeling tasks leads to a\ncommon output representation, which enables research on a combined paradigm. In\nthis respect, multimodal image and audio music transcription comprises the\nchallenge of effectively combining the information conveyed by image and audio\nmodalities. In this work, we explore this question at a late-fusion level: we\nstudy four combination approaches in order to merge, for the first time, the\nhypotheses regarding end-to-end OMR and AMT systems in a lattice-based search\nspace. The results obtained for a series of performance scenarios -- in which\nthe corresponding single-modality models yield different error rates -- showed\ninteresting benefits of these approaches. In addition, two of the four\nstrategies considered significantly improve the corresponding unimodal standard\nrecognition frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alfaro_Contreras_M/0/1/0/all/0/1\">Mar&#xed;a Alfaro-Contreras</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Valero_Mas_J/0/1/0/all/0/1\">Jose J. Valero-Mas</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Inesta_J/0/1/0/all/0/1\">Jos&#xe9; M. I&#xf1;esta</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Calvo_Zaragoza_J/0/1/0/all/0/1\">Jorge Calvo-Zaragoza</a> (1) ((1) Instituto Universitario de Investigaci&#xf3;n Inform&#xe1;tica, University of Alicante, Alicante, Spain)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Self-Optimal-Transport Feature Transform. (arXiv:2204.03065v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03065","description":"<p>The Self-Optimal-Transport (SOT) feature transform is designed to upgrade the\nset of features of a data instance to facilitate downstream matching or\ngrouping related tasks. The transformed set encodes a rich representation of\nhigh order relations between the instance features. Distances between\ntransformed features capture their direct original similarity and their third\nparty agreement regarding similarity to other features in the set. A particular\nmin-cost-max-flow fractional matching problem, whose entropy regularized\nversion can be approximated by an optimal transport (OT) optimization, results\nin our transductive transform which is efficient, differentiable, equivariant,\nparameterless and probabilistically interpretable. Empirically, the transform\nis highly effective and flexible in its use, consistently improving networks it\nis inserted into, in a variety of tasks and training schemes. We demonstrate\nits merits through the problem of unsupervised clustering and its efficiency\nand wide applicability for few-shot-classification, with state-of-the-art\nresults, and large-scale person re-identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shalam_D/0/1/0/all/0/1\">Daniel Shalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korman_S/0/1/0/all/0/1\">Simon Korman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSCARS: An Outlier-Sensitive Content-Based Radiography Retrieval System. (arXiv:2204.03074v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03074","description":"<p>Improving the retrieval relevance on noisy datasets is an emerging need for\nthe curation of a large-scale clean dataset in the medical domain. While\nexisting methods can be applied for class-wise retrieval (aka. inter-class),\nthey cannot distinguish the granularity of likeness within the same class (aka.\nintra-class). The problem is exacerbated on medical external datasets, where\nnoisy samples of the same class are treated equally during training. Our goal\nis to identify both intra/inter-class similarities for fine-grained retrieval.\nTo achieve this, we propose an Outlier-Sensitive Content-based rAdiologhy\nRetrieval System (OSCARS), consisting of two steps. First, we train an outlier\ndetector on a clean internal dataset in an unsupervised manner. Then we use the\ntrained detector to generate the anomaly scores on the external dataset, whose\ndistribution will be used to bin intra-class variations. Second, we propose a\nquadruplet (a, p, nintra, ninter) sampling strategy, where intra-class\nnegatives nintra are sampled from bins of the same class other than the bin\nanchor a belongs to, while niner are randomly sampled from inter-classes. We\nsuggest a weighted metric learning objective to balance the intra and\ninter-class feature learning. We experimented on two representative public\nradiography datasets. Experiments show the effectiveness of our approach. The\ntraining and evaluation code can be found in\nhttps://github.com/XiaoyuanGuo/oscars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiali Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1\">Saptarshi Purkayastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Hari Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1\">Judy Wawira Gichoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1\">Imon Banerjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance Segmentation of Unlabeled Modalities via Cyclic Segmentation GAN. (arXiv:2204.03082v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03082","description":"<p>Instance segmentation for unlabeled imaging modalities is a challenging but\nessential task as collecting expert annotation can be expensive and\ntime-consuming. Existing works segment a new modality by either deploying a\npre-trained model optimized on diverse training data or conducting domain\ntranslation and image segmentation as two independent steps. In this work, we\npropose a novel Cyclic Segmentation Generative Adversarial Network (CySGAN)\nthat conducts image translation and instance segmentation jointly using a\nunified framework. Besides the CycleGAN losses for image translation and\nsupervised losses for the annotated source domain, we introduce additional\nself-supervised and segmentation-based adversarial objectives to improve the\nmodel performance by leveraging unlabeled target domain images. We benchmark\nour approach on the task of 3D neuronal nuclei segmentation with annotated\nelectron microscopy (EM) images and unlabeled expansion microscopy (ExM) data.\nOur CySGAN outperforms both pretrained generalist models and the baselines that\nsequentially conduct image translation and segmentation. Our implementation and\nthe newly collected, densely annotated ExM nuclei dataset, named NucExM, are\navailable at https://connectomics-bazaar.github.io/proj/CySGAN/index.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lauenburg_L/0/1/0/all/0/1\">Leander Lauenburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zudi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1\">M&#xe1;rcia dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arganda_Carreras_I/0/1/0/all/0/1\">Ignacio Arganda-Carreras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyden_E/0/1/0/all/0/1\">Edward S. Boyden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Donglai Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Visual Person-of-Interest DeepFake Detection. (arXiv:2204.03083v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03083","description":"<p>Face manipulation technology is advancing very rapidly, and new methods are\nbeing proposed day by day. The aim of this work is to propose a deepfake\ndetector that can cope with the wide variety of manipulation methods and\nscenarios encountered in the real world. Our key insight is that each person\nhas specific biometric characteristics that a synthetic generator cannot likely\nreproduce. Accordingly, we extract high-level audio-visual biometric features\nwhich characterize the identity of a person, and use them to create a\nperson-of-interest (POI) deepfake detector. We leverage a contrastive learning\nparadigm to learn the moving-face and audio segments embeddings that are most\ndiscriminative for each identity. As a result, when the video and/or audio of a\nperson is manipulated, its representation in the embedding space becomes\ninconsistent with the real identity, allowing reliable detection. Training is\ncarried out exclusively on real talking-face videos, thus the detector does not\ndepend on any specific manipulation method and yields the highest\ngeneralization ability. In addition, our method can detect both single-modality\n(audio-only, video-only) and multi-modality (audio-video) attacks, and is\nrobust to low-quality or corrupted videos by building only on high-level\nsemantic features. Experiments on a wide variety of datasets confirm that our\nmethod ensures a SOTA performance, with an average improvement in terms of AUC\nof around 3%, 10%, and 7% for high-quality, low quality and attacked videos,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cozzolino_D/0/1/0/all/0/1\">Davide Cozzolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verdoliva_L/0/1/0/all/0/1\">Luisa Verdoliva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Self-supervised Representation Learning for Movie Understanding. (arXiv:2204.03101v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03101","description":"<p>Most self-supervised video representation learning approaches focus on action\nrecognition. In contrast, in this paper we focus on self-supervised video\nlearning for movie understanding and propose a novel hierarchical\nself-supervised pretraining strategy that separately pretrains each level of\nour hierarchical movie understanding model (based on [37]). Specifically, we\npropose to pretrain the low-level video backbone using a contrastive learning\nobjective, while pretrain the higher-level video contextualizer using an event\nmask prediction task, which enables the usage of different data sources for\npretraining different levels of the hierarchy. We first show that our\nself-supervised pretraining strategies are effective and lead to improved\nperformance on all tasks and metrics on VidSitu benchmark [37] (e.g., improving\non semantic role prediction from 47% to 61% CIDEr scores). We further\ndemonstrate the effectiveness of our contextualized event features on LVU tasks\n[54], both when used alone and when combined with instance features, showing\ntheir complementarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1\">Fanyi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_K/0/1/0/all/0/1\">Kaustav Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis. (arXiv:2204.03105v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03105","description":"<p>In this paper, we address the problem of texture representation for 3D shapes\nfor the challenging and underexplored tasks of texture transfer and synthesis.\nPrevious works either apply spherical texture maps which may lead to large\ndistortions, or use continuous texture fields that yield smooth outputs lacking\ndetails. We argue that the traditional way of representing textures with images\nand linking them to a 3D mesh via UV mapping is more desirable, since\nsynthesizing 2D images is a well-studied problem. We propose AUV-Net which\nlearns to embed 3D surfaces into a 2D aligned UV space, by mapping the\ncorresponding semantic parts of different 3D shapes to the same location in the\nUV space. As a result, textures are aligned across objects, and can thus be\neasily synthesized by generative models of images. Texture alignment is learned\nin an unsupervised manner by a simple yet effective texture alignment module,\ntaking inspiration from traditional works on linear subspace learning. The\nlearned UV mapping and aligned texture representations enable a variety of\napplications including texture transfer, texture synthesis, and textured single\nview 3D reconstruction. We conduct experiments on multiple datasets to\ndemonstrate the effectiveness of our method. Project page:\nhttps://nv-tlabs.github.io/AUV-NET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kangxue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UIGR: Unified Interactive Garment Retrieval. (arXiv:2204.03111v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03111","description":"<p>Interactive garment retrieval (IGR) aims to retrieve a target garment image\nbased on a reference garment image along with user feedback on what to change\non the reference garment. Two IGR tasks have been studied extensively:\ntext-guided garment retrieval (TGR) and visually compatible garment retrieval\n(VCR). The user feedback for the former indicates what semantic attributes to\nchange with the garment category preserved, while the category is the only\nthing to be changed explicitly for the latter, with an implicit requirement on\nstyle preservation. Despite the similarity between these two tasks and the\npractical need for an efficient system tackling both, they have never been\nunified and modeled jointly. In this paper, we propose a Unified Interactive\nGarment Retrieval (UIGR) framework to unify TGR and VCR. To this end, we first\ncontribute a large-scale benchmark suited for both problems. We further propose\na strong baseline architecture to integrate TGR and VCR in one model. Extensive\nexperiments suggest that unifying two tasks in one framework is not only more\nefficient by requiring a single model only, it also leads to better\nperformance. Code and datasets are available at\nhttps://github.com/BrandonHanx/CompFashion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoCOR: Autonomous Condylar Offset Ratio Calculator on TKA-Postoperative Lateral Knee X-ray. (arXiv:2204.03120v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03120","description":"<p>The postoperative range of motion is one of the crucial factors indicating\nthe outcome of Total Knee Arthroplasty (TKA). Although the correlation between\nrange of knee flexion and posterior condylar offset (PCO) is controversial in\nthe literature, PCO maintains its importance on evaluation of TKA. Due to\nlimitations on PCO measurement, two novel parameters, posterior condylar offset\nratio (PCOR) and anterior condylar offset ratio (ACOR), were introduced.\nNowadays, the calculation of PCOR and ACOR on plain lateral radiographs is done\nmanually by orthopedic surgeons. In this regard, we developed a software,\nAutoCOR, to calculate PCOR and ACOR autonomously, utilizing unsupervised\nmachine learning algorithm (k-means clustering) and digital image processing\ntechniques. The software AutoCOR is capable of detecting the anterior/posterior\nedge points and anterior/posterior cortex of the femoral shaft on true\npostoperative lateral conventional radiographs. To test the algorithm, 50\npostoperative true lateral radiographs from Istanbul Kosuyolu Medipol Hospital\nDatabase were used (32 patients). The mean PCOR was 0.984 (SD 0.235) in\nsoftware results and 0.972 (SD 0.164) in ground truth values. It shows strong\nand significant correlation between software and ground truth values (Pearson\nr=0.845 p&lt;0.0001). The mean ACOR was 0.107 (SD 0.092) in software results and\n0.107 (SD 0.070) in ground truth values. It shows moderate and significant\ncorrelation between software and ground truth values (Spearman's rs=0.519\np=0.0001412). We suggest that AutoCOR is a useful tool that can be used in\nclinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cakmak_G/0/1/0/all/0/1\">Gulsade Rabia Cakmak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamamci_I/0/1/0/all/0/1\">Ibrahim Ethem Hamamci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_M/0/1/0/all/0/1\">Mehmet Kursat Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhajj_R/0/1/0/all/0/1\">Reda Alhajj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azboy_I/0/1/0/all/0/1\">Ibrahim Azboy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozdemir_M/0/1/0/all/0/1\">Mehmet Kemal Ozdemir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffCloud: Real-to-Sim from Point Clouds with Differentiable Simulation and Rendering of Deformable Objects. (arXiv:2204.03139v1 [cs.RO])","link":"http://arxiv.org/abs/2204.03139","description":"<p>Research in manipulation of deformable objects is typically conducted on a\nlimited range of scenarios, because handling each scenario on hardware takes\nsignificant effort. Realistic simulators with support for various types of\ndeformations and interactions have the potential to speed up experimentation\nwith novel tasks and algorithms. However, for highly deformable objects it is\nchallenging to align the output of a simulator with the behavior of real\nobjects. Manual tuning is not intuitive, hence automated methods are needed. We\nview this alignment problem as a joint perception-inference challenge and\ndemonstrate how to use recent neural network architectures to successfully\nperform simulation parameter inference from real point clouds. We analyze the\nperformance of various architectures, comparing their data and training\nrequirements. Furthermore, we propose to leverage differentiable point cloud\nsampling and differentiable simulation to significantly reduce the time to\nachieve the alignment. We employ an efficient way to propagate gradients from\npoint clouds to simulated meshes and further through to the physical simulation\nparameters, such as mass and stiffness. Experiments with highly deformable\nobjects show that our method can achieve comparable or better alignment with\nreal object behavior, while reducing the time needed to achieve this by more\nthan an order of magnitude. Videos and supplementary material are available at\nhttps://tinyurl.com/diffcloud.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_P/0/1/0/all/0/1\">Priya Sundaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonova_R/0/1/0/all/0/1\">Rika Antonova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1\">Jeannette Bohg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Machine Learning Attacks Against Video Anomaly Detection Systems. (arXiv:2204.03141v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03141","description":"<p>Anomaly detection in videos is an important computer vision problem with\nvarious applications including automated video surveillance. Although\nadversarial attacks on image understanding models have been heavily\ninvestigated, there is not much work on adversarial machine learning targeting\nvideo understanding models and no previous work which focuses on video anomaly\ndetection. To this end, we investigate an adversarial machine learning attack\nagainst video anomaly detection systems, that can be implemented via an\neasy-to-perform cyber-attack. Since surveillance cameras are usually connected\nto the server running the anomaly detection model through a wireless network,\nthey are prone to cyber-attacks targeting the wireless connection. We\ndemonstrate how Wi-Fi deauthentication attack, a notoriously easy-to-perform\nand effective denial-of-service (DoS) attack, can be utilized to generate\nadversarial data for video anomaly detection systems. Specifically, we apply\nseveral effects caused by the Wi-Fi deauthentication attack on video quality\n(e.g., slow down, freeze, fast forward, low resolution) to the popular\nbenchmark datasets for video anomaly detection. Our experiments with several\nstate-of-the-art anomaly detection models show that the attackers can\nsignificantly undermine the reliability of video anomaly detection systems by\ncausing frequent false alarms and hiding physical anomalies from the\nsurveillance system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mumcu_F/0/1/0/all/0/1\">Furkan Mumcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_K/0/1/0/all/0/1\">Keval Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_Y/0/1/0/all/0/1\">Yasin Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Cross-Domain Pretrained Model for Hyperspectral Image Classification. (arXiv:2204.03144v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03144","description":"<p>A pretrain-finetune strategy is widely used to reduce the overfitting that\ncan occur when data is insufficient for CNN training. First few layers of a CNN\npretrained on a large-scale RGB dataset are capable of acquiring general image\ncharacteristics which are remarkably effective in tasks targeted for different\nRGB datasets. However, when it comes down to hyperspectral domain where each\ndomain has its unique spectral properties, the pretrain-finetune strategy no\nlonger can be deployed in a conventional way while presenting three major\nissues: 1) inconsistent spectral characteristics among the domains (e.g.,\nfrequency range), 2) inconsistent number of data channels among the domains,\nand 3) absence of large-scale hyperspectral dataset.\n</p>\n<p>We seek to train a universal cross-domain model which can later be deployed\nfor various spectral domains. To achieve, we physically furnish multiple inlets\nto the model while having a universal portion which is designed to handle the\ninconsistent spectral characteristics among different domains. Note that only\nthe universal portion is used in the finetune process. This approach naturally\nenables the learning of our model on multiple domains simultaneously which acts\nas an effective workaround for the issue of the absence of large-scale dataset.\n</p>\n<p>We have carried out a study to extensively compare models that were trained\nusing cross-domain approach with ones trained from scratch. Our approach was\nfound to be superior both in accuracy and in training efficiency. In addition,\nwe have verified that our approach effectively reduces the overfitting issue,\nenabling us to deepen the model up to 13 layers (from 9) without compromising\nthe accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungtae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eum_S/0/1/0/all/0/1\">Sungmin Eum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Heesung Kwon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just-Noticeable-Difference Based Edge Map Quality Measure. (arXiv:2204.03155v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03155","description":"<p>The performance of an edge detector can be improved when assisted with an\neffective edge map quality measure. Several evaluation methods have been\nproposed resulting in different performance score for the same candidate edge\nmap. However, an effective measure is the one that can be automated and which\ncorrelates with human judgement perceived quality of the edge map.\nDistance-based edge map measures are widely used for assessment of edge map\nquality. These methods consider distance and statistical properties of edge\npixels to estimate a performance score. The existing methods can be automated;\nhowever, they lack perceptual features. This paper presents edge map quality\nmeasure based on Just-Noticeable-Difference (JND) feature of human visual\nsystem, to compensate the shortcomings of distance-based edge measures. For\nthis purpose, we have designed constant stimulus experiment to measure the JND\nvalue for two spatial alternative. Experimental results show that JND based\ndistance calculation outperforms existing distance-based measures according to\nsubjective evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Ijaz Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seokjoo Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible Sampling for Long-tailed Skin Lesion Classification. (arXiv:2204.03161v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03161","description":"<p>Most of the medical tasks naturally exhibit a long-tailed distribution due to\nthe complex patient-level conditions and the existence of rare diseases.\nExisting long-tailed learning methods usually treat each class equally to\nre-balance the long-tailed distribution. However, considering that some\nchallenging classes may present diverse intra-class distributions, re-balancing\nall classes equally may lead to a significant performance drop. To address\nthis, in this paper, we propose a curriculum learning-based framework called\nFlexible Sampling for the long-tailed skin lesion classification task.\nSpecifically, we initially sample a subset of training data as anchor points\nbased on the individual class prototypes. Then, these anchor points are used to\npre-train an inference model to evaluate the per-class learning difficulty.\nFinally, we use a curriculum sampling module to dynamically query new samples\nfrom the rest training samples with the learning difficulty-aware sampling\nprobability. We evaluated our model against several state-of-the-art methods on\nthe ISIC dataset. The results with two long-tailed settings have demonstrated\nthe superiority of our proposed training strategy, which achieves a new\nbenchmark for long-tailed skin lesion classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1\">Lie Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yicheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonnington_P/0/1/0/all/0/1\">Paul Bonnington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. (arXiv:2204.03162v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03162","description":"<p>We present a novel task and dataset for evaluating the ability of vision and\nlanguage models to conduct visio-linguistic compositional reasoning, which we\ncall Winoground. Given two images and two captions, the goal is to match them\ncorrectly - but crucially, both captions contain a completely identical set of\nwords, only in a different order. The dataset was carefully hand-curated by\nexpert annotators and is labeled with a rich set of fine-grained tags to assist\nin analyzing model performance. We probe a diverse range of state-of-the-art\nvision and language models and find that, surprisingly, none of them do much\nbetter than chance. Evidently, these models are not as skilled at\nvisio-linguistic compositional reasoning as we might have hoped. We perform an\nextensive analysis to obtain insights into how future work might try to\nmitigate these models' shortcomings. We aim for Winoground to serve as a useful\nevaluation set for advancing the state of the art and driving further progress\nin the field. The dataset is available at\nhttps://huggingface.co/datasets/facebook/winoground.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Ryan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1\">Max Bartolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_C/0/1/0/all/0/1\">Candace Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Dose CT Denoising via Sinogram Inner-Structure Transformer. (arXiv:2204.03163v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03163","description":"<p>Low-Dose Computed Tomography (LDCT) technique, which reduces the radiation\nharm to human bodies, is now attracting increasing interest in the medical\nimaging field. As the image quality is degraded by low dose radiation, LDCT\nexams require specialized reconstruction methods or denoising algorithms.\nHowever, most of the recent effective methods overlook the inner-structure of\nthe original projection data (sinogram) which limits their denoising ability.\nThe inner-structure of the sinogram represents special characteristics of the\ndata in the sinogram domain. By maintaining this structure while denoising, the\nnoise can be obviously restrained. Therefore, we propose an LDCT denoising\nnetwork namely Sinogram Inner-Structure Transformer (SIST) to reduce the noise\nby utilizing the inner-structure in the sinogram domain. Specifically, we study\nthe CT imaging mechanism and statistical characteristics of sinogram to design\nthe sinogram inner-structure loss including the global and local\ninner-structure for restoring high-quality CT images. Besides, we propose a\nsinogram transformer module to better extract sinogram features. The\ntransformer architecture using a self-attention mechanism can exploit\ninterrelations between projections of different view angles, which achieves an\noutstanding performance in sinogram denoising. Furthermore, in order to improve\nthe performance in the image domain, we propose the image reconstruction module\nto complementarily denoise both in the sinogram and image domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Liutao Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhongnian/0/1/0/all/0/1\">Zhongnian</a>, Li, <a href=\"http://arxiv.org/find/eess/1/au:+Rongjun/0/1/0/all/0/1\">Rongjun</a>, Ge, <a href=\"http://arxiv.org/find/eess/1/au:+Junyong/0/1/0/all/0/1\">Junyong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao/0/1/0/all/0/1\">Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haipeng/0/1/0/all/0/1\">Haipeng</a>, Si, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">Daoqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDA GAN: Adversarial-Learning-based 3-D Seismic Data Interpolation and Reconstruction for Complex Missing. (arXiv:2204.03197v1 [physics.geo-ph])","link":"http://arxiv.org/abs/2204.03197","description":"<p>The interpolation and reconstruction of missing traces is a crucial step in\nseismic data processing, moreover it is also a highly ill-posed problem,\nespecially for complex cases such as high-ratio random discrete missing,\ncontinuous missing and missing in rich fault or salt body surveys. These\ncomplex cases are rarely mentioned in current sparse or low-rank priorbased and\ndeep learning-based approaches. To cope with complex missing cases, we propose\nMulti-Dimensional Adversarial GAN (MDA GAN), a novel 3-D GAN framework. It\nemploys three discriminators to ensure the consistency of the reconstructed\ndata with the original data distribution in each dimension. The feature\nsplicing module (FSM) is designed and embedded into the generator of this\nframework, which automatically splices the features of the unmissing part with\nthose of the reconstructed part (missing part), thus fully preserving the\ninformation of the unmissing part. To prevent pixel distortion in the seismic\ndata caused by the adversarial learning process, we propose a new\nreconstruction loss Tanh Cross Entropy (TCE) loss to provide smoother\ngradients. We experimentally verified the effectiveness of the individual\ncomponents of the study and then tested the method on multiple publicly\navailable data. The method achieves reasonable reconstructions for up to 95% of\nrandom discrete missing, 100 traces of continuous missing and more complex\nhybrid missing. In surveys of fault-rich and salt bodies, the method can\nachieve promising reconstructions with up to 75% missing in each of the three\ndirections (98.2% in total).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Dou_Y/0/1/0/all/0/1\">Yimin Dou</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhu_J/0/1/0/all/0/1\">Jianbing Zhu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_T/0/1/0/all/0/1\">Timing Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tan_S/0/1/0/all/0/1\">Shaoquan Tan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Huang_Z/0/1/0/all/0/1\">Zongchao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Network for Early Pulmonary Embolism Detection via Computed Tomography Pulmonary Angiography. (arXiv:2204.03204v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03204","description":"<p>This study was conducted to develop a computer-aided detection (CAD) system\nfor triaging patients with pulmonary embolism (PE). The purpose of the system\nwas to reduce the death rate during the waiting period. Computed tomography\npulmonary angiography (CTPA) is used for PE diagnosis. Because CTPA reports\nrequire a radiologist to review the case and suggest further management, this\ncreates a waiting period during which patients may die. Our proposed CAD method\nwas thus designed to triage patients with PE from those without PE. In contrast\nto related studies involving CAD systems that identify key PE lesion images to\nexpedite PE diagnosis, our system comprises a novel classification-model\nensemble for PE detection and a segmentation model for PE lesion labeling. The\nmodels were trained using data from National Cheng Kung University Hospital and\nopen resources. The classification model yielded 0.73 for receiver operating\ncharacteristic curve (accuracy = 0.85), while the mean intersection over union\nwas 0.689 for the segmentation model. The proposed CAD system can distinguish\nbetween patients with and without PE and automatically label PE lesions to\nexpedite PE diagnosis\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1\">Ching-Yuan Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Che Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun-Chien Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">Chin Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L2G: A Simple Local-to-Global Knowledge Transfer Framework for Weakly Supervised Semantic Segmentation. (arXiv:2204.03206v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03206","description":"<p>Mining precise class-aware attention maps, a.k.a, class activation maps, is\nessential for weakly supervised semantic segmentation. In this paper, we\npresent L2G, a simple online local-to-global knowledge transfer framework for\nhigh-quality object attention mining. We observe that classification models can\ndiscover object regions with more details when replacing the input image with\nits local patches. Taking this into account, we first leverage a local\nclassification network to extract attentions from multiple local patches\nrandomly cropped from the input image. Then, we utilize a global network to\nlearn complementary attention knowledge across multiple local attention maps\nonline. Our framework conducts the global network to learn the captured rich\nobject detail knowledge from a global view and thereby produces high-quality\nattention maps that can be directly used as pseudo annotations for semantic\nsegmentation networks. Experiments show that our method attains 72.1% and 44.2%\nmIoU scores on the validation set of PASCAL VOC 2012 and MS COCO 2014,\nrespectively, setting new state-of-the-art records. Code is available at\nhttps://github.com/PengtaoJiang/L2G.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng-Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MC-UNet Multi-module Concatenation based on U-shape Network for Retinal Blood Vessels Segmentation. (arXiv:2204.03213v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03213","description":"<p>Accurate segmentation of the blood vessels of the retina is an important step\nin clinical diagnosis of ophthalmic diseases. Many deep learning frameworks\nhave come up for retinal blood vessels segmentation tasks. However, the complex\nvascular structure and uncertain pathological features make the blood vessel\nsegmentation still very challenging. A novel U-shaped network named\nMulti-module Concatenation which is based on Atrous convolution and\nmulti-kernel pooling is put forward to retinal vessels segmentation in this\npaper. The proposed network structure retains three layers the essential\nstructure of U-Net, in which the atrous convolution combining the multi-kernel\npooling blocks are designed to obtain more contextual information. The spatial\nattention module is concatenated with dense atrous convolution module and\nmulti-kernel pooling module to form a multi-module concatenation. And different\ndilation rates are selected by cascading to acquire a larger receptive field in\natrous convolution. Adequate comparative experiments are conducted on these\npublic retinal datasets: DRIVE, STARE and CHASE_DB1. The results show that the\nproposed method is effective, especially for microvessels. The code will be put\nout at https://github.com/Rebeccala/MC-UNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yi Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_N/0/1/0/all/0/1\">Nan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1\">Hongtao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_Z/0/1/0/all/0/1\">Zihao Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Changcai Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_L/0/1/0/all/0/1\">Lanyan Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1\">Riqing Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_L/0/1/0/all/0/1\">Lifang Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What You See is What You Get: Distributional Generalization for Algorithm Design in Deep Learning. (arXiv:2204.03230v1 [cs.LG])","link":"http://arxiv.org/abs/2204.03230","description":"<p>We investigate and leverage a connection between Differential Privacy (DP)\nand the recently proposed notion of Distributional Generalization (DG).\nApplying this connection, we introduce new conceptual tools for designing\ndeep-learning methods that bypass \"pathologies\" of standard stochastic gradient\ndescent (SGD). First, we prove that differentially private methods satisfy a\n\"What You See Is What You Get (WYSIWYG)\" generalization guarantee: whatever a\nmodel does on its train data is almost exactly what it will do at test time.\nThis guarantee is formally captured by distributional generalization. WYSIWYG\nenables principled algorithm design in deep learning by reducing\n$\\textit{generalization}$ concerns to $\\textit{optimization}$ ones: in order to\nmitigate unwanted behavior at test time, it is provably sufficient to mitigate\nthis behavior on the train data. This is notably false for standard (non-DP)\nmethods, hence this observation has applications even when privacy is not\nrequired. For example, importance sampling is known to fail for standard SGD,\nbut we show that it has exactly the intended effect for DP-trained models.\nThus, with DP-SGD, unlike with SGD, we can influence test-time behavior by\nmaking principled train-time interventions. We use these insights to construct\nsimple algorithms which match or outperform SOTA in several distributional\nrobustness applications, and to significantly improve the privacy vs. disparate\nimpact trade-off of DP-SGD. Finally, we also improve on known theoretical\nbounds relating differential privacy, stability, and distributional\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulynych_B/0/1/0/all/0/1\">Bogdan Kulynych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yao-Yuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blasiok_J/0/1/0/all/0/1\">Jaros&#x142;aw B&#x142;asiok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1\">Preetum Nakkiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIT-UAV: A High-altitude Infrared Thermal Dataset for Unmanned Aerial Vehicles. (arXiv:2204.03245v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03245","description":"<p>This paper presents a High-altitude infrared thermal dataset, HIT-UAV, for\nobject detection applications on Unmanned Aerial Vehicles (UAVs). HIT-UAV\ncontains 2898 infrared thermal images extracted from 43470 frames. These images\nare collected by UAV from schools, parking lots, roads, playgrounds, etc.\nHIT-UAV provides different flight data for each place, including flight\naltitude (from 60 to 130 meters), camera perspective (from 30 to 90 degrees),\ndate, and daylight intensity. For each image, the HIT-UAV manual annotates\nobject instances with two types of the bounding box (oriented and standard) to\naddress the challenge that object instances have a significant overlap in\naerial images. To the best of our knowledge, HIT-UAV is the first publicly\navailable high-altitude infrared thermal UAV dataset for persons and vehicles\ndetection. Moreover, we trained and evaluated the benchmark detection\nalgorithms (YOLOv4 and YOLOv4-tiny) on HIT-UAV. Compared to the visual light\ndataset, the detection algorithms have excellent performance on HIT-UAV because\nthe infrared thermal images do not contain a significant quantity of irrelevant\ninformation with detection objects. This indicates that infrared thermal\ndatasets can significantly promote the development of object detection\napplications. We hope HIT-UAV contributes to UAV applications such as traffic\nsurveillance and city monitoring at night. The dataset is available at\nhttps://github.com/suojiashun/HIT-UAV-Infrared-Thermal-Dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suo_J/0/1/0/all/0/1\">Jiashun Suo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingzhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weisong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pan-cancer computational histopathology reveals tumor mutational burden status through weakly-supervised deep learning. (arXiv:2204.03257v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03257","description":"<p>Tumor mutational burden (TMB) is a potential genomic biomarker that can help\nidentify patients who will benefit from immunotherapy across a variety of\ncancers. We included whole slide images (WSIs) of 3228 diagnostic slides from\nthe Cancer Genome Atlas and 531 WSIs from the Clinical Proteomic Tumor Analysis\nConsortium for the development and verification of a pan-cancer TMB prediction\nmodel (PC-TMB). We proposed a multiscale weakly-supervised deep learning\nframework for predicting TMB of seven types of tumors based only on routinely\nused hematoxylin-eosin (H&amp;E)-stained WSIs. PC-TMB achieved a mean area under\ncurve (AUC) of 0.818 (0.804-0.831) in the cross-validation cohort, which was\nsuperior to the best single-scale model. In comparison with the\nstate-of-the-art TMB prediction model from previous publications, our\nmultiscale model achieved better performance over previously reported models.\nIn addition, the improvements of PC-TMB over the single-tumor models were also\nconfirmed by the ablation tests on 10x magnification. The PC-TMB algorithm also\nexhibited good generalization on external validation cohort with AUC of 0.732\n(0.683-0.761). PC-TMB possessed a comparable survival-risk stratification\nperformance to the TMB measured by whole exome sequencing, but with low cost\nand being time-efficient for providing a prognostic biomarker of multiple solid\ntumors. Moreover, spatial heterogeneity of TMB within tumors was also\nidentified through our PC-TMB, which might enable image-based screening for\nmolecular biomarkers with spatial variation and potential exploring for\ngenotype-spatial heterogeneity relationships.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siteng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jinxi Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Junhua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiao Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Sensitive Temporal Feature Learning for Gait Recognition. (arXiv:2204.03270v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03270","description":"<p>Although gait recognition has drawn increasing research attention recently,\nit remains challenging to learn discriminative temporal representation, since\nthe silhouette differences are quite subtle in spatial domain. Inspired by the\nobservation that human can distinguish gaits of different subjects by\nadaptively focusing on temporal clips with different time scales, we propose a\ncontext-sensitive temporal feature learning (CSTL) network for gait\nrecognition. CSTL produces temporal features in three scales, and adaptively\naggregates them according to the contextual information from local and global\nperspectives. Specifically, CSTL contains an adaptive temporal aggregation\nmodule that subsequently performs local relation modeling and global relation\nmodeling to fuse the multi-scale features. Besides, in order to remedy the\nspatial feature corruption caused by temporal operations, CSTL incorporates a\nsalient spatial feature learning (SSFL) module to select groups of\ndiscriminative spatial features. Particularly, we utilize transformers to\nimplement the global relation modeling and the SSFL module. To the best of our\nknowledge, this is the first work that adopts transformer in gait recognition.\nExtensive experiments conducted on three datasets demonstrate the\nstate-of-the-art performance. Concretely, we achieve rank-1 accuracies of\n98.7%, 96.2% and 88.7% under normal-walking, bag-carrying and coat-wearing\nconditions on CASIA-B, 97.5% on OU-MVLP and 50.6% on GREW.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaohu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Duowang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Botao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bin Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Real Time Satellite Pose Estimation on Low Power Edge TPU. (arXiv:2204.03296v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03296","description":"<p>Pose estimation of an uncooperative space resident object is a key asset\ntowards autonomy in close proximity operations. In this context monocular\ncameras are a valuable solution because of their low system requirements.\nHowever, the associated image processing algorithms are either too\ncomputationally expensive for real time on-board implementation, or not enough\naccurate. In this paper we propose a pose estimation software exploiting neural\nnetwork architectures which can be scaled to different accuracy-latency\ntrade-offs. We designed our pipeline to be compatible with Edge Tensor\nProcessing Units to show how low power machine learning accelerators could\nenable Artificial Intelligence exploitation in space. The neural networks were\ntested both on the benchmark Spacecraft Pose Estimation Dataset, and on the\npurposely developed Cosmo Photorealistic Dataset, which depicts a COSMO-SkyMed\nsatellite in a variety of random poses and steerable solar panels orientations.\nThe lightest version of our architecture achieves state-of-the-art accuracy on\nboth datasets but at a fraction of networks complexity, running at 7.7 frames\nper second on a Coral Dev Board Mini consuming just 2.2W.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lotti_A/0/1/0/all/0/1\">Alessandro Lotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modenini_D/0/1/0/all/0/1\">Dario Modenini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tortora_P/0/1/0/all/0/1\">Paolo Tortora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saponara_M/0/1/0/all/0/1\">Massimiliano Saponara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perino_M/0/1/0/all/0/1\">Maria A. Perino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swarm behavior tracking based on a deep vision algorithm. (arXiv:2204.03319v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03319","description":"<p>The intelligent swarm behavior of social insects (such as ants) springs up in\ndifferent environments, promising to provide insights for the study of embodied\nintelligence. Researching swarm behavior requires that researchers could\naccurately track each individual over time. Obviously, manually labeling\nindividual insects in a video is labor-intensive. Automatic tracking methods,\nhowever, also poses serious challenges: (1) individuals are small and similar\nin appearance; (2) frequent interactions with each other cause severe and\nlong-term occlusion. With the advances of artificial intelligence and computing\nvision technologies, we are hopeful to provide a tool to automate monitor\nmultiple insects to address the above challenges. In this paper, we propose a\ndetection and tracking framework for multi-ant tracking in the videos by: (1)\nadopting a two-stage object detection framework using ResNet-50 as backbone and\ncoding the position of regions of interest to locate ants accurately; (2) using\nthe ResNet model to develop the appearance descriptors of ants; (3)\nconstructing long-term appearance sequences and combining them with motion\ninformation to achieve online tracking. To validate our method, we construct an\nant database including 10 videos of ants from different indoor and outdoor\nscenes. We achieve a state-of-the-art performance of 95.7\\% mMOTA and 81.1\\%\nmMOTP in indoor videos, 81.8\\% mMOTA and 81.9\\% mMOTP in outdoor videos.\nAdditionally, Our method runs 6-10 times faster than existing methods for\ninsect tracking. Experimental results demonstrate that our method provides a\npowerful tool for accelerating the unraveling of the mechanisms underlying the\nswarm behavior of social insects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Meihong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaoyan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shihui Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Sample $\\zeta$-mixup: Richer, More Realistic Synthetic Samples from a $p$-Series Interpolant. (arXiv:2204.03323v1 [cs.LG])","link":"http://arxiv.org/abs/2204.03323","description":"<p>Modern deep learning training procedures rely on model regularization\ntechniques such as data augmentation methods, which generate training samples\nthat increase the diversity of data and richness of label information. A\npopular recent method, mixup, uses convex combinations of pairs of original\nsamples to generate new samples. However, as we show in our experiments, mixup\ncan produce undesirable synthetic samples, where the data is sampled off the\nmanifold and can contain incorrect labels. We propose $\\zeta$-mixup, a\ngeneralization of mixup with provably and demonstrably desirable properties\nthat allows convex combinations of $N \\geq 2$ samples, leading to more\nrealistic and diverse outputs that incorporate information from $N$ original\nsamples by using a $p$-series interpolant. We show that, compared to mixup,\n$\\zeta$-mixup better preserves the intrinsic dimensionality of the original\ndatasets, which is a desirable property for training generalizable models.\nFurthermore, we show that our implementation of $\\zeta$-mixup is faster than\nmixup, and extensive evaluation on controlled synthetic and 24 real-world\nnatural and medical image classification datasets shows that $\\zeta$-mixup\noutperforms mixup and traditional data augmentation techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abhishek_K/0/1/0/all/0/1\">Kumar Abhishek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_C/0/1/0/all/0/1\">Colin J. Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Review of Sign Language Recognition: Different Types, Modalities, and Datasets. (arXiv:2204.03328v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03328","description":"<p>A machine can understand human activities, and the meaning of signs can help\novercome the communication barriers between the inaudible and ordinary people.\nSign Language Recognition (SLR) is a fascinating research area and a crucial\ntask concerning computer vision and pattern recognition. Recently, SLR usage\nhas increased in many applications, but the environment, background image\nresolution, modalities, and datasets affect the performance a lot. Many\nresearchers have been striving to carry out generic real-time SLR models. This\nreview paper facilitates a comprehensive overview of SLR and discusses the\nneeds, challenges, and problems associated with SLR. We study related works\nabout manual and non-manual, various modalities, and datasets. Research\nprogress and existing state-of-the-art SLR models over the past decade have\nbeen reviewed. Finally, we find the research gap and limitations in this domain\nand suggest future directions. This review paper will be helpful for readers\nand researchers to get complete guidance about SLR and the progressive design\nof the state-of-the-art SLR model\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhiarasan_D/0/1/0/all/0/1\">Dr. M. Madhiarasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1\">Prof. Partha Pratim Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Feature Mining for Video Semantic Segmentation. (arXiv:2204.03330v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03330","description":"<p>The contextual information plays a core role in semantic segmentation. As for\nvideo semantic segmentation, the contexts include static contexts and motional\ncontexts, corresponding to static content and moving content in a video clip,\nrespectively. The static contexts are well exploited in image semantic\nsegmentation by learning multi-scale and global/long-range features. The\nmotional contexts are studied in previous video semantic segmentation. However,\nthere is no research about how to simultaneously learn static and motional\ncontexts which are highly correlated and complementary to each other. To\naddress this problem, we propose a Coarse-to-Fine Feature Mining (CFFM)\ntechnique to learn a unified presentation of static contexts and motional\ncontexts. This technique consists of two parts: coarse-to-fine feature\nassembling and cross-frame feature mining. The former operation prepares data\nfor further processing, enabling the subsequent joint learning of static and\nmotional contexts. The latter operation mines useful information/contexts from\nthe sequential frames to enhance the video contexts of the features of the\ntarget frame. The enhanced features can be directly applied for the final\nprediction. Experimental results on popular benchmarks demonstrate that the\nproposed CFFM performs favorably against state-of-the-art methods for video\nsemantic segmentation. Our implementation is available at\nhttps://github.com/GuoleiSun/VSS-CFFM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guolei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1\">Thomas Probst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Optical Flow-Based Line Feature Tracking. (arXiv:2204.03331v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03331","description":"<p>In this paper we propose a novel sparse optical flow (SOF)-based line feature\ntracking method for the camera pose estimation problem. This method is inspired\nby the point-based SOF algorithm and developed based on an observation that two\nadjacent images in time-varying image sequences satisfy brightness invariant.\nBased on this observation, we re-define the goal of line feature tracking:\ntrack two endpoints of a line feature instead of the entire line based on gray\nvalue matching instead of descriptor matching. To achieve this goal, an\nefficient two endpoint tracking (TET) method is presented: first, describe a\ngiven line feature with its two endpoints; next, track the two endpoints based\non SOF to obtain two new tracked endpoints by minimizing a pixel-level\ngrayscale residual function; finally, connect the two tracked endpoints to\ngenerate a new line feature. The correspondence is established between the\ngiven and the new line feature. Compared with current descriptor-based methods,\nour TET method needs not to compute descriptors and detect line features\nrepeatedly. Naturally, it has an obvious advantage over computation.\nExperiments in several public benchmark datasets show our method yields highly\ncompetitive accuracy with an obvious advantage over speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongshan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_I/0/1/0/all/0/1\">Islam Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Sieve: Prediction of Grading Curves from Images of Concrete Aggregate. (arXiv:2204.03333v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03333","description":"<p>A large component of the building material concrete consists of aggregate\nwith varying particle sizes between 0.125 and 32 mm. Its actual size\ndistribution significantly affects the quality characteristics of the final\nconcrete in both, the fresh and hardened states. The usually unknown variations\nin the size distribution of the aggregate particles, which can be large\nespecially when using recycled aggregate materials, are typically compensated\nby an increased usage of cement which, however, has severe negative impacts on\neconomical and ecological aspects of the concrete production. In order to allow\na precise control of the target properties of the concrete, unknown variations\nin the size distribution have to be quantified to enable a proper adaptation of\nthe concrete's mixture design in real time. To this end, this paper proposes a\ndeep learning based method for the determination of concrete aggregate grading\ncurves. In this context, we propose a network architecture applying multi-scale\nfeature extraction modules in order to handle the strongly diverse object sizes\nof the particles. Furthermore, we propose and publish a novel dataset of\nconcrete aggregate used for the quantitative evaluation of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coenen_M/0/1/0/all/0/1\">Max Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_D/0/1/0/all/0/1\">Dries Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heipke_C/0/1/0/all/0/1\">Christian Heipke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haist_M/0/1/0/all/0/1\">Michael Haist</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSTR: End-to-End One-Step Person Search With Transformers. (arXiv:2204.03340v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03340","description":"<p>We propose a novel one-step transformer-based person search framework, PSTR,\nthat jointly performs person detection and re-identification (re-id) in a\nsingle architecture. PSTR comprises a person search-specialized (PSS) module\nthat contains a detection encoder-decoder for person detection along with a\ndiscriminative re-id decoder for person re-id. The discriminative re-id decoder\nutilizes a multi-level supervision scheme with a shared decoder for\ndiscriminative re-id feature learning and also comprises a part attention block\nto encode relationship between different parts of a person. We further\nintroduce a simple multi-scale scheme to support re-id across person instances\nat different scales. PSTR jointly achieves the diverse objectives of\nobject-level recognition (detection) and instance-level matching (re-id). To\nthe best of our knowledge, we are the first to propose an end-to-end one-step\ntransformer-based person search framework. Experiments are performed on two\npopular benchmarks: CUHK-SYSU and PRW. Our extensive ablations reveal the\nmerits of the proposed contributions. Further, the proposed PSTR sets a new\nstate-of-the-art on both benchmarks. On the challenging PRW benchmark, PSTR\nachieves a mean average precision (mAP) score of 56.5%. The source code is\navailable at \\url{https://github.com/JialeCao001/PSTR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiale Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yanwei Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1\">Rao Muhammad Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1\">Hisham Cholakkal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implementing a Real-Time, YOLOv5 based Social Distancing Measuring System for Covid-19. (arXiv:2204.03350v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03350","description":"<p>The purpose of this work is, to provide a YOLOv5 deep learning-based social\ndistance monitoring framework using an overhead view perspective. In addition,\nwe have developed a custom defined model YOLOv5 modified CSP (Cross Stage\nPartial Network) and assessed the performance on COCO and Visdrone dataset with\nand without transfer learning. Our findings show that the developed model\nsuccessfully identifies the individual who violates the social distances. The\naccuracy of 81.7% for the modified bottleneck CSP without transfer learning is\nobserved on COCO dataset after training the model for 300 epochs whereas for\nthe same epochs, the default YOLOv5 model is attaining 80.1% accuracy with\ntransfer learning. This shows an improvement in accuracy by our modified\nbottleneck CSP model. For the Visdrone dataset, we are able to achieve an\naccuracy of upto 56.5% for certain classes and especially an accuracy of 40%\nfor people and pedestrians with transfer learning using the default YOLOv5s\nmodel for 30 epochs. While the modified bottleneck CSP is able to perform\nslightly better than the default model with an accuracy score of upto 58.1% for\ncertain classes and an accuracy of ~40.4% for people and pedestrians.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darapaneni_N/0/1/0/all/0/1\">Narayana Darapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shrawan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1\">Selvarangan Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_H/0/1/0/all/0/1\">Hemalatha K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_A/0/1/0/all/0/1\">Arunkumar Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagendra/0/1/0/all/0/1\">Nagendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paduri_A/0/1/0/all/0/1\">Anwesh Reddy Paduri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Online Multi-Sensor Depth Fusion. (arXiv:2204.03353v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03353","description":"<p>Many hand-held or mixed reality devices are used with a single sensor for 3D\nreconstruction, although they often comprise multiple sensors. Multi-sensor\ndepth fusion is able to substantially improve the robustness and accuracy of 3D\nreconstruction methods, but existing techniques are not robust enough to handle\nsensors which operate with diverse value ranges as well as noise and outlier\nstatistics. To this end, we introduce SenFuNet, a depth fusion approach that\nlearns sensor-specific noise and outlier statistics and combines the data\nstreams of depth frames from different sensors in an online fashion. Our method\nfuses multi-sensor depth streams regardless of time synchronization and\ncalibration and generalizes well with little training data. We conduct\nexperiments with various sensor combinations on the real-world CoRBS and\nScene3D datasets, as well as the Replica dataset. Experiments demonstrate that\nour fusion strategy outperforms traditional and recent online depth fusion\napproaches. In addition, the combination of multiple sensors yields more robust\noutlier handling and precise surface reconstruction than the use of a single\nsensor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sandstrom_E/0/1/0/all/0/1\">Erik Sandstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weder_S/0/1/0/all/0/1\">Silvan Weder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Transformer. A sparse-aware solution for efficient event data processing. (arXiv:2204.03355v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03355","description":"<p>Event cameras are sensors of great interest for many applications that run in\nlow-resource and challenging environments. They log sparse illumination changes\nwith high temporal resolution and high dynamic range, while they present\nminimal power consumption. However, top-performing methods often ignore\nspecific event-data properties, leading to the development of generic but\ncomputationally expensive algorithms. Efforts toward efficient solutions\nusually do not achieve top-accuracy results for complex tasks. This work\nproposes a novel framework, Event Transformer (EvT), that effectively takes\nadvantage of event-data properties to be highly efficient and accurate. We\nintroduce a new patch-based event representation and a compact transformer-like\narchitecture to process it. EvT is evaluated on different event-based\nbenchmarks for action and gesture recognition. Evaluation results show better\nor comparable accuracy to the state-of-the-art while requiring significantly\nless computation resources, which makes EvT able to work with minimal latency\nboth on GPU and CPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO. (arXiv:2204.03359v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03359","description":"<p>Image-Test matching (ITM) is a common task for evaluating the quality of\nVision and Language (VL) models. However, existing ITM benchmarks have a\nsignificant limitation. They have many missing correspondences, originating\nfrom the data construction process itself. For example, a caption is only\nmatched with one image although the caption can be matched with other similar\nimages, and vice versa. To correct the massive false negatives, we construct\nthe Extended COCO Validation (ECCV) Caption dataset by supplying the missing\nassociations with machine and human annotators. We employ five state-of-the-art\nITM models with diverse properties for our annotation process. Our dataset\nprovides x3.6 positive image-to-caption associations and x8.5 caption-to-image\nassociations compared to the original MS-COCO. We also propose to use an\ninformative ranking-based metric, rather than the popular Recall@K(R@K). We\nre-evaluate the existing 25 VL models on existing and proposed benchmarks. Our\nfindings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K R@K,\nCxC R@1 are highly correlated with each other, while the rankings change when\nwe shift to the ECCV mAP. Lastly, we delve into the effect of the bias\nintroduced by the choice of machine annotator. Source code and dataset are\navailable at https://github.com/naver-ai/eccv-caption\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wonjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Song Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Minsuk Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seong Joon Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Distracted Driver using Convolution Neural Network. (arXiv:2204.03371v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03371","description":"<p>With over 50 million car sales annually and over 1.3 million deaths every\nyear due to motor accidents we have chosen this space. India accounts for 11\nper cent of global death in road accidents. Drivers are held responsible for\n78% of accidents. Road safety problems in developing countries is a major\nconcern and human behavior is ascribed as one of the main causes and\naccelerators of road safety problems. Driver distraction has been identified as\nthe main reason for accidents. Distractions can be caused due to reasons such\nas mobile usage, drinking, operating instruments, facial makeup, social\ninteraction. For the scope of this project, we will focus on building a highly\nefficient ML model to classify different driver distractions at runtime using\ncomputer vision. We would also analyze the overall speed and scalability of the\nmodel in order to be able to set it up on an edge device. We use CNN, VGG-16,\nRestNet50 and ensemble of CNN to predict the classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darapaneni_N/0/1/0/all/0/1\">Narayana Darapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_J/0/1/0/all/0/1\">Jai Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazra_M/0/1/0/all/0/1\">MoniShankar Hazra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_N/0/1/0/all/0/1\">Naman Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_S/0/1/0/all/0/1\">Simrandeep Singh Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Saurabh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paduri_A/0/1/0/all/0/1\">Anwesh Reddy Paduri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HunYuan_tvr for Text-Video Retrivial. (arXiv:2204.03382v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03382","description":"<p>Text-Video Retrieval plays an important role in multi-modal understanding and\nhas attracted increasing attention in recent years. Most existing methods focus\non constructing contrastive pairs between whole videos and complete caption\nsentences, while ignoring fine-grained cross-modal relationships, e.g., short\nclips and phrases or single frame and word. In this paper, we propose a novel\nmethod, named HunYuan\\_tvr, to explore hierarchical cross-modal interactions by\nsimultaneously exploring video-sentence, clip-phrase, and frame-word\nrelationships. Considering intrinsic semantic relations between frames,\nHunYuan\\_tvr first performs self-attention to explore frame-wise correlations\nand adaptively clusters correlated frames into clip-level representations.\nThen, the clip-wise correlation is explored to aggregate clip representations\ninto a compact one to describe the video globally. In this way, we can\nconstruct hierarchical video representations for frame-clip-video\ngranularities, and also explore word-wise correlations to form\nword-phrase-sentence embeddings for the text modality. Finally, hierarchical\ncontrastive learning is designed to explore cross-modal\nrelationships,~\\emph{i.e.,} frame-word, clip-phrase, and video-sentence, which\nenables HunYuan\\_tvr to achieve a comprehensive multi-modal understanding.\nFurther boosted by adaptive label denosing and marginal sample enhancement,\nHunYuan\\_tvr obtains new state-of-the-art results on various benchmarks, e.g.,\nRank@1 of 55.0%, 57.8%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC,\nDiDemo, and ActivityNet respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Shaobo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Weijie Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Rong-Cheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dihong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sixiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Vision Transformers: Flexible Attention-Based Modelling of Biomedical Surfaces. (arXiv:2204.03408v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03408","description":"<p>Recent state-of-the-art performances of Vision Transformers (ViT) in computer\nvision tasks demonstrate that a general-purpose architecture, which implements\nlong-range self-attention, could replace the local feature learning operations\nof convolutional neural networks. In this paper, we extend ViTs to surfaces by\nreformulating the task of surface learning as a sequence-to-sequence learning\nproblem, by proposing patching mechanisms for general surface meshes. Sequences\nof patches are then processed by a transformer encoder and used for\nclassification or regression. We validate our method on a range of different\nbiomedical surface domains and tasks: brain age prediction in the developing\nHuman Connectome Project (dHCP), fluid intelligence prediction in the Human\nConnectome Project (HCP), and coronary artery calcium score classification\nusing surfaces from the Scottish Computed Tomography of the Heart (SCOT-HEART)\ndataset, and investigate the impact of pretraining and data augmentation on\nmodel performance. Results suggest that Surface Vision Transformers (SiT)\ndemonstrate consistent improvement over geometric deep learning methods for\nbrain age and fluid intelligence prediction and achieve comparable performance\non calcium score classification to standard metrics used in clinical practice.\nFurthermore, analysis of transformer attention maps offers clear and\nindividualised predictions of the features driving each task. Code is available\non Github: https://github.com/metrics-lab/surface-vision-transformers\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dahan_S/0/1/0/all/0/1\">Simon Dahan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williams_L/0/1/0/all/0/1\">Logan Z. J. Williams</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fawaz_A/0/1/0/all/0/1\">Abdulah Fawaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Chunhui Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coalson_T/0/1/0/all/0/1\">Timothy S. Coalson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williams_M/0/1/0/all/0/1\">Michelle C. Williams</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Newby_D/0/1/0/all/0/1\">David E. Newby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Edwards_A/0/1/0/all/0/1\">A. David Edwards</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glasser_M/0/1/0/all/0/1\">Matthew F. Glasser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Young_A/0/1/0/all/0/1\">Alistair A. Young</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robinson_E/0/1/0/all/0/1\">Emma C. Robinson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Prototype Prompt-tuning with Pre-trained Representation for Class Incremental Learning. (arXiv:2204.03410v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03410","description":"<p>Class incremental learning has attracted much attention, but most existing\nworks still continually fine-tune the representation model, resulting in much\ncatastrophic forgetting. Instead of struggling to fight against such forgetting\nby replaying or distillation like most of the existing methods, we take the\npre-train-and-prompt-tuning paradigm to sequentially learn new visual concepts\nbased on a fixed semantic rich pre-trained representation model by incremental\nprototype prompt-tuning (IPP), which substantially reduces the catastrophic\nforgetting. In addition, an example prototype classification is proposed to\ncompensate for semantic drift, the problem caused by learning bias at different\nphases. Extensive experiments conducted on the three incremental learning\nbenchmarks demonstrate that our method consistently outperforms other\nstate-of-the-art methods with a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jieren Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jianhua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haojian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunkuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Aware Active Learning for Endoscopic Image Analysis. (arXiv:2204.03440v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03440","description":"<p>Semantic segmentation of polyps and depth estimation are two important\nresearch problems in endoscopic image analysis. One of the main obstacles to\nconduct research on these research problems is lack of annotated data.\nEndoscopic annotations necessitate the specialist knowledge of expert\nendoscopists and due to this, it can be difficult to organise, expensive and\ntime consuming. To address this problem, we investigate an active learning\nparadigm to reduce the number of training examples by selecting the most\ndiscriminative and diverse unlabelled examples for the task taken into\nconsideration. Most of the existing active learning pipelines are task-agnostic\nin nature and are often sub-optimal to the end task. In this paper, we propose\na novel task-aware active learning pipeline and applied for two important tasks\nin endoscopic image analysis: semantic segmentation and depth estimation. We\ncompared our method with the competitive baselines. From the experimental\nresults, we observe a substantial improvement over the compared baselines.\nCodes are available at https://github.com/thetna/endo-active-learn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thapa_S/0/1/0/all/0/1\">Shrawan Kumar Thapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poudel_P/0/1/0/all/0/1\">Pranav Poudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1\">Binod Bhattarai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Visual Geo-localization Benchmark. (arXiv:2204.03444v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03444","description":"<p>In this paper, we propose a new open-source benchmarking framework for Visual\nGeo-localization (VG) that allows to build, train, and test a wide range of\ncommonly used architectures, with the flexibility to change individual\ncomponents of a geo-localization pipeline. The purpose of this framework is\ntwofold: i) gaining insights into how different components and design choices\nin a VG pipeline impact the final results, both in terms of performance\n(recall@N metric) and system requirements (such as execution time and memory\nconsumption); ii) establish a systematic evaluation protocol for comparing\ndifferent methods. Using the proposed framework, we perform a large suite of\nexperiments which provide criteria for choosing backbone, aggregation and\nnegative mining depending on the use-case and requirements. We also assess the\nimpact of engineering techniques like pre/post-processing, data augmentation\nand image resizing, showing that better performance can be obtained through\nsomewhat simple procedures: for example, downscaling the images' resolution to\n80% can lead to similar results with a 36% savings in extraction time and\ndataset storage requirement. Code and trained models are available at\nhttps://deep-vg-bench.herokuapp.com/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berton_G/0/1/0/all/0/1\">Gabriele Berton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mereu_R/0/1/0/all/0/1\">Riccardo Mereu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivigno_G/0/1/0/all/0/1\">Gabriele Trivigno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masone_C/0/1/0/all/0/1\">Carlo Masone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csurka_G/0/1/0/all/0/1\">Gabriela Csurka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1\">Torsten Sattler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Diffusion Models. (arXiv:2204.03458v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03458","description":"<p>Generating temporally coherent high fidelity video is an important milestone\nin generative modeling research. We make progress towards this milestone by\nproposing a diffusion model for video generation that shows very promising\ninitial results. Our model is a natural extension of the standard image\ndiffusion architecture, and it enables jointly training from image and video\ndata, which we find to reduce the variance of minibatch gradients and speed up\noptimization. To generate long and higher resolution videos we introduce a new\nconditional sampling technique for spatial and temporal video extension that\nperforms better than previously proposed methods. We present the first results\non a large text-conditioned video generation task, as well as state-of-the-art\nresults on an established unconditional video generation benchmark.\nSupplementary material is available at https://video-diffusion.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results. (arXiv:2204.03475v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03475","description":"<p>ImageNet serves as the primary dataset for evaluating the quality of\ncomputer-vision models. The common practice today is training each architecture\nwith a tailor-made scheme, designed and tuned by an expert. In this paper, we\npresent a unified scheme for training any backbone on ImageNet. The scheme,\nnamed USI (Unified Scheme for ImageNet), is based on knowledge distillation and\nmodern tricks. It requires no adjustments or hyper-parameters tuning between\ndifferent models, and is efficient in terms of training times. We test USI on a\nwide variety of architectures, including CNNs, Transformers, Mobile-oriented\nand MLP-only. On all models tested, USI outperforms previous state-of-the-art\nresults. Hence, we are able to transform training on ImageNet from an\nexpert-oriented task to an automatic seamless routine. Since USI accepts any\nbackbone and trains it to top results, it also enables to perform methodical\ncomparisons, and identify the most efficient backbones along the speed-accuracy\nPareto curve. Implementation is available\nat:https://github.com/Alibaba-MIIL/Solving_ImageNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawen_H/0/1/0/all/0/1\">Hussam Lawen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProbNVS: Fast Novel View Synthesis with Learned Probability-Guided Sampling. (arXiv:2204.03476v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03476","description":"<p>Existing state-of-the-art novel view synthesis methods rely on either fairly\naccurate 3D geometry estimation or sampling of the entire space for neural\nvolumetric rendering, which limit the overall efficiency. In order to improve\nthe rendering efficiency by reducing sampling points without sacrificing\nrendering quality, we propose to build a novel view synthesis framework based\non learned MVS priors that enables general, fast and photo-realistic view\nsynthesis simultaneously. Specifically, fewer but important points are sampled\nunder the guidance of depth probability distributions extracted from the\nlearned MVS architecture. Based on the learned probability-guided sampling, a\nneural volume rendering module is elaborately devised to fully aggregate source\nview information as well as the learned scene structures to synthesize\nphotorealistic target view images. Finally, the rendering results in uncertain,\noccluded and unreferenced regions can be further improved by incorporating a\nconfidence-aware refinement module. Experiments show that our method achieves\n15 to 40 times faster rendering compared to state-of-the-art baselines, with\nstrong generalization capacity and comparable high-quality novel view synthesis\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuemei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zerong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing the Long-Term Behaviour of Deep Reinforcement Learning for Pushing and Grasping. (arXiv:2204.03487v1 [cs.LG])","link":"http://arxiv.org/abs/2204.03487","description":"<p>We investigate the \"Visual Pushing for Grasping\" (VPG) system by Zeng et al.\nand the \"Hourglass\" system by Ewerton et al., an evolution of the former. The\nfocus of our work is the investigation of the capabilities of both systems to\nlearn long-term rewards and policies. Zeng et al. original task only needs a\nlimited amount of foresight. Ewerton et al. attain their best performance using\nan agent which only takes the most immediate action under consideration. We are\ninterested in the ability of their models and training algorithms to accurately\npredict long-term Q-Values. To evaluate this ability, we design a new bin\nsorting task and reward function. Our task requires agents to accurately\nestimate future rewards and therefore use high discount factors in their\nQ-Value calculation. We investigate the behaviour of an adaptation of the VPG\ntraining algorithm on our task. We show that this adaptation can not accurately\npredict the required long-term action sequences. In addition to the limitations\nidentified by Ewerton et al., it suffers from the known Deep Q-Learning problem\nof overestimated Q-Values. In an effort to solve our task, we turn to the\nHourglass models and combine them with the Double Q-Learning approach. We show\nthat this approach enables the models to accurately predict long-term action\nsequences when trained with large discount factors. Our results show that the\nDouble Q-Learning technique is essential for training with very high discount\nfactors, as the models Q-Value predictions diverge otherwise. We also\nexperiment with different approaches for discount factor scheduling, loss\ncalculation and exploration procedures. Our results show that the latter\nfactors do not visibly influence the model's performance for our task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chau_R/0/1/0/all/0/1\">Rodrigo Chau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Distributed Learning using Vision Transformer with Random Patch Permutation. (arXiv:2204.03500v1 [cs.LG])","link":"http://arxiv.org/abs/2204.03500","description":"<p>The widespread application of artificial intelligence in health research is\ncurrently hampered by limitations in data availability. Distributed learning\nmethods such as federated learning (FL) and shared learning (SL) are introduced\nto solve this problem as well as data management and ownership issues with\ntheir different strengths and weaknesses. The recent proposal of federated\nsplit task-agnostic (FeSTA) learning tries to reconcile the distinct merits of\nFL and SL by enabling the multi-task collaboration between participants through\nVision Transformer (ViT) architecture, but they suffer from higher\ncommunication overhead. To address this, here we present a multi-task\ndistributed learning using ViT with random patch permutation. Instead of using\na CNN based head as in FeSTA, p-FeSTA adopts a randomly permuting simple patch\nembedder, improving the multi-task learning performance without sacrificing\nprivacy. Experimental results confirm that the proposed method significantly\nenhances the benefit of multi-task collaboration, communication efficiency, and\nprivacy preservation, shedding light on practical multi-task distributed\nlearning in the field of medical imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sangjoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Many-to-many Splatting for Efficient Video Frame Interpolation. (arXiv:2204.03513v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03513","description":"<p>Motion-based video frame interpolation commonly relies on optical flow to\nwarp pixels from the inputs to the desired interpolation instant. Yet due to\nthe inherent challenges of motion estimation (e.g. occlusions and\ndiscontinuities), most state-of-the-art interpolation approaches require\nsubsequent refinement of the warped result to generate satisfying outputs,\nwhich drastically decreases the efficiency for multi-frame interpolation. In\nthis work, we propose a fully differentiable Many-to-Many (M2M) splatting\nframework to interpolate frames efficiently. Specifically, given a frame pair,\nwe estimate multiple bidirectional flows to directly forward warp the pixels to\nthe desired time step, and then fuse any overlapping pixels. In doing so, each\nsource pixel renders multiple target pixels and each target pixel can be\nsynthesized from a larger area of visual context. This establishes a\nmany-to-many splatting scheme with robustness to artifacts like holes.\nMoreover, for each input frame pair, M2M only performs motion estimation once\nand has a minuscule computational overhead when interpolating an arbitrary\nnumber of in-between frames, hence achieving fast multi-frame interpolation. We\nconducted extensive experiments to analyze M2M, and found that it significantly\nimproves efficiency while maintaining high effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Ping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_S/0/1/0/all/0/1\">Simon Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale. (arXiv:2204.03514v1 [cs.AI])","link":"http://arxiv.org/abs/2204.03514","description":"<p>We present a large-scale study of imitating human demonstrations on tasks\nthat require a virtual robot to search for objects in new environments -- (1)\nObjectGoal Navigation (e.g. 'find &amp; go to a chair') and (2) Pick&amp;Place (e.g.\n'find mug, pick mug, find counter, place mug on counter'). First, we develop a\nvirtual teleoperation data-collection infrastructure -- connecting Habitat\nsimulator running in a web browser to Amazon Mechanical Turk, allowing remote\nusers to teleoperate virtual robots, safely and at scale. We collect 80k\ndemonstrations for ObjectNav and 12k demonstrations for Pick&amp;Place, which is an\norder of magnitude larger than existing human demonstration datasets in\nsimulation or on real robots.\n</p>\n<p>Second, we attempt to answer the question -- how does large-scale imitation\nlearning (IL) (which hasn't been hitherto possible) compare to reinforcement\nlearning (RL) (which is the status quo)? On ObjectNav, we find that IL (with no\nbells or whistles) using 70k human demonstrations outperforms RL using 240k\nagent-gathered trajectories. The IL-trained agent demonstrates efficient\nobject-search behavior -- it peeks into rooms, checks corners for small\nobjects, turns in place to get a panoramic view -- none of these are exhibited\nas prominently by the RL agent, and to induce these behaviors via RL would\nrequire tedious reward engineering. Finally, accuracy vs. training data size\nplots show promising scaling behavior, suggesting that simply collecting more\ndemonstrations is likely to advance the state of art further. On Pick&amp;Place,\nthe comparison is starker -- IL agents achieve ${\\sim}$18% success on episodes\nwith new object-receptacle locations when trained with 9.5k human\ndemonstrations, while RL agents fail to get beyond 0%. Overall, our work\nprovides compelling evidence for investing in large-scale imitation learning.\n</p>\n<p>Project page: https://ram81.github.io/projects/habitat-web.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramrakhya_R/0/1/0/all/0/1\">Ram Ramrakhya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Undersander_E/0/1/0/all/0/1\">Eric Undersander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Abhishek Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualizing Deep Neural Networks with Topographic Activation Maps. (arXiv:2204.03528v1 [cs.LG])","link":"http://arxiv.org/abs/2204.03528","description":"<p>Machine Learning with Deep Neural Networks (DNNs) has become a successful\ntool in solving tasks across various fields of application. The success of DNNs\nis strongly connected to their high complexity in terms of the number of\nnetwork layers or of neurons in each layer, which severely complicates to\nunderstand how DNNs solve their learned task. To improve the explainability of\nDNNs, we adapt methods from neuroscience because this field has a rich\nexperience in analyzing complex and opaque systems. In this work, we draw\ninspiration from how neuroscience uses topographic maps to visualize the\nactivity of the brain when it performs certain tasks. Transferring this\napproach to DNNs can help to visualize and understand their internal processes\nmore intuitively, too. However, the inner structures of brains and DNNs differ\nsubstantially. Therefore, to be able to visualize activations of neurons in\nDNNs as topographic maps, we research techniques to layout the neurons in a\ntwo-dimensional space in which neurons of similar activity are in the vicinity\nof each other. In this work, we introduce and compare different methods to\nobtain a topographic layout of the neurons in a network layer. Moreover, we\ndemonstrate how to use the resulting topographic activation maps to identify\nerrors or encoded biases in DNNs or data sets. Our novel visualization\ntechnique improves the transparency of DNN-based algorithmic decision-making\nsystems and is accessible to a broad audience because topographic maps are\nintuitive to interpret without expert-knowledge in Machine Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krug_A/0/1/0/all/0/1\">Andreas Krug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratul_R/0/1/0/all/0/1\">Raihan Kabir Ratul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stober_S/0/1/0/all/0/1\">Sebastian Stober</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Multiscale Object-based Superpixel Framework. (arXiv:2204.03533v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03533","description":"<p>Superpixel segmentation can be used as an intermediary step in many\napplications, often to improve object delineation and reduce computer workload.\nHowever, classical methods do not incorporate information about the desired\nobject. Deep-learning-based approaches consider object information, but their\ndelineation performance depends on data annotation. Additionally, the\ncomputational time of object-based methods is usually much higher than desired.\nIn this work, we propose a novel superpixel framework, named Superpixels\nthrough Iterative CLEarcutting (SICLE), which exploits object information being\nable to generate a multiscale segmentation on-the-fly. SICLE starts off from\nseed oversampling and repeats optimal connectivity-based superpixel delineation\nand object-based seed removal until a desired number of superpixels is reached.\nIt generalizes recent superpixel methods, surpassing them and other\nstate-of-the-art approaches in efficiency and effectiveness according to\nmultiple delineation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belem_F/0/1/0/all/0/1\">Felipe Bel&#xe9;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perret_B/0/1/0/all/0/1\">Benjamin Perret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cousty_J/0/1/0/all/0/1\">Jean Cousty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guimaraes_S/0/1/0/all/0/1\">Silvio J. F. Guimar&#xe3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcao_A/0/1/0/all/0/1\">Alexandre Falc&#xe3;o</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Zero-Shot HOI Detection via Vision and Language Knowledge Distillation. (arXiv:2204.03541v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03541","description":"<p>Most existing Human-Object Interaction~(HOI) Detection methods rely heavily\non full annotations with predefined HOI categories, which is limited in\ndiversity and costly to scale further. We aim at advancing zero-shot HOI\ndetection to detect both seen and unseen HOIs simultaneously. The fundamental\nchallenges are to discover potential human-object pairs and identify novel HOI\ncategories. To overcome the above challenges, we propose a novel end-to-end\nzero-shot HOI Detection (EoID) framework via vision-language knowledge\ndistillation. We first design an Interactive Score module combined with a\nTwo-stage Bipartite Matching algorithm to achieve interaction distinguishment\nfor human-object pairs in an action-agnostic manner. Then we transfer the\ndistribution of action probability from the pretrained vision-language teacher\nas well as the seen ground truth to the HOI model to attain zero-shot HOI\nclassification. Extensive experiments on HICO-Det dataset demonstrate that our\nmodel discovers potential interactive pairs and enables the recognition of\nunseen HOIs. Finally, our method outperforms the previous SOTA by 8.92% on\nunseen mAP and 10.18% on overall mAP under UA setting, by 6.02% on unseen mAP\nand 9.1% on overall mAP under UC setting. Moreover, our method is generalizable\nto large-scale object detection data to further scale up the action sets. The\nsource code will be available at: https://github.com/mrwu-mac/EoID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mingrui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiaxin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Procedures for Establishing Generative Adversarial Network-based Stochastic Image Models in Medical Imaging. (arXiv:2204.03547v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03547","description":"<p>Modern generative models, such as generative adversarial networks (GANs),\nhold tremendous promise for several areas of medical imaging, such as\nunconditional medical image synthesis, image restoration, reconstruction and\ntranslation, and optimization of imaging systems. However, procedures for\nestablishing stochastic image models (SIMs) using GANs remain generic and do\nnot address specific issues relevant to medical imaging. In this work,\ncanonical SIMs that simulate realistic vessels in angiography images are\nemployed to evaluate procedures for establishing SIMs using GANs. The GAN-based\nSIM is compared to the canonical SIM based on its ability to reproduce those\nstatistics that are meaningful to the particular medically realistic SIM\nconsidered. It is shown that evaluating GANs using classical metrics and\nmedically relevant metrics may lead to different conclusions about the fidelity\nof the trained GANs. This work highlights the need for the development of\nobjective metrics for evaluating GANs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kelkar_V/0/1/0/all/0/1\">Varun A. Kelkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gotsis_D/0/1/0/all/0/1\">Dimitrios S. Gotsis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brooks_F/0/1/0/all/0/1\">Frank J. Brooks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myers_K/0/1/0/all/0/1\">Kyle J. Myers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+KC_P/0/1/0/all/0/1\">Prabhat KC</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_R/0/1/0/all/0/1\">Rongping Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Digital Disguises: Leveraging Face Swaps to Protect Patient Privacy. (arXiv:2204.03559v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03559","description":"<p>With rapid advancements in image generation technology, face swapping for\nprivacy protection has emerged as an active area of research. The ultimate\nbenefit is improved access to video datasets, e.g. in healthcare settings.\nRecent literature has proposed deep network-based architectures to perform\nfacial swaps and reported the associated reduction in facial recognition\naccuracy. However, there is not much reporting on how well these methods\npreserve the types of semantic information needed for the privatized videos to\nremain useful for their intended application. Our main contribution is a novel\nend-to-end face swapping pipeline for recorded videos of standardized\nassessments of autism symptoms in children. Through this design, we are the\nfirst to provide a methodology for assessing the privacy-utility trade-offs for\nthe face swapping approach to patient privacy protection. Our methodology can\nshow, for example, that current deep network based face swapping is\nbottle-necked by face detection in real world videos, and the extent to which\ngaze and expression information is preserved by face swaps relative to baseline\nprivatization methods such as blurring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_E/0/1/0/all/0/1\">Ethan Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shic_F/0/1/0/all/0/1\">Frederick Shic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skytta_J/0/1/0/all/0/1\">Jenny Skytta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_E/0/1/0/all/0/1\">Eakta Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotional Speech Recognition with Pre-trained Deep Visual Models. (arXiv:2204.03561v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03561","description":"<p>In this paper, we propose a new methodology for emotional speech recognition\nusing visual deep neural network models. We employ the transfer learning\ncapabilities of the pre-trained computer vision deep models to have a mandate\nfor the emotion recognition in speech task. In order to achieve that, we\npropose to use a composite set of acoustic features and a procedure to convert\nthem into images. Besides, we present a training paradigm for these models\ntaking into consideration the different characteristics between acoustic-based\nimages and regular ones. In our experiments, we use the pre-trained VGG-16\nmodel and test the overall methodology on the Berlin EMO-DB dataset for\nspeaker-independent emotion recognition. We evaluate the proposed model on the\nfull list of the seven emotions and the results set a new state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ragheb_W/0/1/0/all/0/1\">Waleed Ragheb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzapour_M/0/1/0/all/0/1\">Mehdi Mirzapour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delfardi_A/0/1/0/all/0/1\">Ali Delfardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacquenet_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Jacquenet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbon_L/0/1/0/all/0/1\">Lawrence Carbon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explicit and Implicit Pattern Relation Analysis for Discovering Actionable Negative Sequences. (arXiv:2204.03571v1 [cs.AI])","link":"http://arxiv.org/abs/2204.03571","description":"<p>Real-life events, behaviors and interactions produce sequential data. An\nimportant but rarely explored problem is to analyze those nonoccurring (also\ncalled negative) yet important sequences, forming negative sequence analysis\n(NSA). A typical NSA area is to discover negative sequential patterns (NSPs)\nconsisting of important non-occurring and occurring elements and patterns. The\nlimited existing work on NSP mining relies on frequentist and downward closure\nproperty-based pattern selection, producing large and highly redundant NSPs,\nnonactionable for business decision-making. This work makes the first attempt\nfor actionable NSP discovery. It builds an NSP graph representation, quantify\nboth explicit occurrence and implicit non-occurrence-based element and pattern\nrelations, and then discover significant, diverse and informative NSPs in the\nNSP graph to represent the entire NSP set for discovering actionable NSPs. A\nDPP-based NSP representation and actionable NSP discovery method EINSP\nintroduces novel and significant contributions for NSA and sequence analysis:\n(1) it represents NSPs by a determinantal point process (DPP) based graph; (2)\nit quantifies actionable NSPs in terms of their statistical significance,\ndiversity, and strength of explicit/implicit element/pattern relations; and (3)\nit models and measures both explicit and implicit element/pattern relations in\nthe DPP-based NSP graph to represent direct and indirect couplings between NSP\nitems, elements and patterns. We substantially analyze the effectiveness of\nEINSP in terms of various theoretical and empirical aspects including\ncomplexity, item/pattern coverage, pattern size and diversity, implicit pattern\nrelation strength, and data factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Longbing Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pathology-Based Machine Learning Method to Assist in Epithelial Dysplasia Diagnosis. (arXiv:2204.03572v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03572","description":"<p>The Epithelial Dysplasia (ED) is a tissue alteration commonly present in\nlesions preceding oral cancer, being its presence one of the most important\nfactors in the progression toward carcinoma. This study proposes a method to\ndesign a low computational cost classification system to support the detection\nof dysplastic epithelia, contributing to reduce the variability of pathologist\nassessments. We employ a multilayer artificial neural network (MLP-ANN) and\ndefining the regions of the epithelium to be assessed based on the knowledge of\nthe pathologist. The performance of the proposed solution was statistically\nevaluated. The implemented MLP-ANN presented an average accuracy of 87%, with a\nvariability much inferior to that obtained from three trained evaluators.\nMoreover, the proposed solution led to results which are very close to those\nobtained using a convolutional neural network (CNN) implemented by transfer\nlearning, with 100 times less computational complexity. In conclusion, our\nresults show that a simple neural network structure can lead to a performance\nequivalent to that of much more complex structures, which are routinely used in\nthe literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rocha_K/0/1/0/all/0/1\">Karoline da Rocha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bermudez_J/0/1/0/all/0/1\">Jos&#xe9; C. M. Bermudez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivero_E/0/1/0/all/0/1\">Elena R. C. Rivero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Costa_M/0/1/0/all/0/1\">M&#xe1;rcio H. Costa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v1 [cs.LG])","link":"http://arxiv.org/abs/2204.03574","description":"<p>We introduce compositional soft prompting (CSP), a parameter-efficient\nlearning technique to improve the zero-shot compositionality of large-scale\npretrained vision-language models (VLMs) without the overhead of fine-tuning\nthe entire model. VLMs can represent arbitrary classes as natural language\nprompts in their flexible text encoders but they underperform state-of-the-art\nmethods on compositional zero-shot benchmark tasks. To improve VLMs, we propose\na novel form of soft prompting. We treat the attributes and objects that are\ncomposed to define classes as learnable tokens of vocabulary and tune them on\nmultiple prompt compositions. During inference, we recompose the learned\nattribute-object vocabulary in new combinations and show that CSP outperforms\nthe original VLM on benchmark datasets by an average of 14.7 percentage points\nof accuracy. CSP also achieves new state-of-the-art accuracies on two out of\nthree benchmark datasets, while only fine-tuning a small number of parameters.\nFurther, we show that CSP improves generalization to higher-order\nattribute-attribute-object compositions and combinations of pretrained\nattributes and fine-tuned objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nihal V. Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peilin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoRF: Learning 3D Object Radiance Fields from Single View Observations. (arXiv:2204.03593v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03593","description":"<p>We introduce AutoRF - a new approach for learning neural 3D object\nrepresentations where each object in the training set is observed by only a\nsingle view. This setting is in stark contrast to the majority of existing\nworks that leverage multiple views of the same object, employ explicit priors\nduring training, or require pixel-perfect annotations. To address this\nchallenging setting, we propose to learn a normalized, object-centric\nrepresentation whose embedding describes and disentangles shape, appearance,\nand pose. Each encoding provides well-generalizable, compact information about\nthe object of interest, which is decoded in a single-shot into a new target\nview, thus enabling novel view synthesis. We further improve the reconstruction\nquality by optimizing shape and appearance codes at test time by fitting the\nrepresentation tightly to the input image. In a series of experiments, we show\nthat our method generalizes well to unseen objects, even across different\ndatasets of challenging real-world street scenes such as nuScenes, KITTI, and\nMapillary Metropolis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_N/0/1/0/all/0/1\">Norman M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonelli_A/0/1/0/all/0/1\">Andrea Simonelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porzi_L/0/1/0/all/0/1\">Lorenzo Porzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulo_S/0/1/0/all/0/1\">Samuel Rota Bul&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontschieder_P/0/1/0/all/0/1\">Peter Kontschieder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pin the Memory: Learning to Generalize Semantic Segmentation. (arXiv:2204.03609v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03609","description":"<p>The rise of deep neural networks has led to several breakthroughs for\nsemantic segmentation. In spite of this, a model trained on source domain often\nfails to work properly in new challenging domains, that is directly concerned\nwith the generalization capability of the model. In this paper, we present a\nnovel memory-guided domain generalization method for semantic segmentation\nbased on meta-learning framework. Especially, our method abstracts the\nconceptual knowledge of semantic classes into categorical memory which is\nconstant beyond the domains. Upon the meta-learning concept, we repeatedly\ntrain memory-guided networks and simulate virtual test to 1) learn how to\nmemorize a domain-agnostic and distinct information of classes and 2) offer an\nexternally settled memory as a class-guidance to reduce the ambiguity of\nrepresentation in the test data of arbitrary unseen domain. To this end, we\nalso propose memory divergence and feature cohesion losses, which encourage to\nlearn memory reading and update processes for category-aware domain\ngeneralization. Extensive experiments for semantic segmentation demonstrate the\nsuperior generalization capability of our method over state-of-the-art works on\nvarious benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1\">Dongbo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Contrastive Learning in Image-Text-Label Space. (arXiv:2204.03610v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03610","description":"<p>Visual recognition is recently learned via either supervised learning on\nhuman-annotated image-label data or language-image contrastive learning with\nwebly-crawled image-text pairs. While supervised learning may result in a more\ndiscriminative representation, language-image pretraining shows unprecedented\nzero-shot recognition capability, largely due to the different properties of\ndata sources and learning objectives. In this work, we introduce a new\nformulation by combining the two data sources into a common image-text-label\nspace. In this space, we propose a new learning paradigm, called Unified\nContrastive Learning (UniCL) with a single learning objective to seamlessly\nprompt the synergy of two data types. Extensive experiments show that our UniCL\nis an effective way of learning semantically rich yet discriminative\nrepresentations, universally for image recognition in zero-shot, linear-probe,\nfully finetuning and transfer learning scenarios. Particularly, it attains\ngains up to 9.2% and 14.5% in average on zero-shot recognition benchmarks over\nthe language-image contrastive learning and supervised learning methods,\nrespectively. In linear probe setting, it also boosts the performance over the\ntwo methods by 7.3% and 3.4%, respectively. Our study also indicates that UniCL\nstand-alone is a good learner on pure image-label data, rivaling the supervised\nlearning methods across three image classification datasets and two types of\nvision backbones, ResNet and Swin Transformer. Code is available at\nhttps://github.com/microsoft/UniCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pneumonia Detection in Chest X-Rays using Neural Networks. (arXiv:2204.03618v1 [eess.IV])","link":"http://arxiv.org/abs/2204.03618","description":"<p>With the advancement in AI, deep learning techniques are widely used to\ndesign robust classification models in several areas such as medical diagnosis\ntasks in which it achieves good performance. In this paper, we have proposed\nthe CNN model (Convolutional Neural Network) for the classification of Chest\nX-ray images for Radiological Society of North America Pneumonia (RSNA)\ndatasets. The study also tries to achieve the same RSNA benchmark results using\nthe limited computational resources by trying out various approaches to the\nmethodologies that have been implemented in recent years. The proposed method\nis based on a non-complex CNN and the use of transfer learning algorithms like\nXception, InceptionV3/V4, EfficientNetB7. Along with this, the study also tries\nto achieve the same RSNA benchmark results using the limited computational\nresources by trying out various approaches to the methodologies that have been\nimplemented in recent years. The RSNA benchmark MAP score is 0.25, but using\nthe Mask RCNN model on a stratified sample of 3017 along with image\naugmentation gave a MAP score of 0.15. Meanwhile, the YoloV3 without any\nhyperparameter tuning gave the MAP score of 0.32 but still, the loss keeps\ndecreasing. Running the model for a greater number of iterations can give\nbetter results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Darapaneni_N/0/1/0/all/0/1\">Narayana Darapaneni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ranjan_A/0/1/0/all/0/1\">Ashish Ranjan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bright_D/0/1/0/all/0/1\">Dany Bright</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trivedi_D/0/1/0/all/0/1\">Devendra Trivedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_K/0/1/0/all/0/1\">Ketul Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_V/0/1/0/all/0/1\">Vivek Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paduri_A/0/1/0/all/0/1\">Anwesh Reddy Paduri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effects of Regularization and Data Augmentation are Class Dependent. (arXiv:2204.03632v1 [cs.LG])","link":"http://arxiv.org/abs/2204.03632","description":"<p>Regularization is a fundamental technique to prevent over-fitting and to\nimprove generalization performances by constraining a model's complexity.\nCurrent Deep Networks heavily rely on regularizers such as Data-Augmentation\n(DA) or weight-decay, and employ structural risk minimization, i.e.\ncross-validation, to select the optimal regularization hyper-parameters. In\nthis study, we demonstrate that techniques such as DA or weight decay produce a\nmodel with a reduced complexity that is unfair across classes. The optimal\namount of DA or weight decay found from cross-validation leads to disastrous\nmodel performances on some classes e.g. on Imagenet with a resnet50, the \"barn\nspider\" classification test accuracy falls from $68\\%$ to $46\\%$ only by\nintroducing random crop DA during training. Even more surprising, such\nperformance drop also appears when introducing uninformative regularization\ntechniques such as weight decay. Those results demonstrate that our search for\never increasing generalization performance -- averaged over all classes and\nsamples -- has left us with models and regularizers that silently sacrifice\nperformances on some classes. This scenario can become dangerous when deploying\na model on downstream tasks e.g. an Imagenet pre-trained resnet50 deployed on\nINaturalist sees its performances fall from $70\\%$ to $30\\%$ on class \\#8889\nwhen introducing random crop DA during the Imagenet pre-training phase. Those\nresults demonstrate that designing novel regularizers without class-dependent\nbias remains an open research question.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bottou_L/0/1/0/all/0/1\">Leon Bottou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Incremental Learning with Strong Pre-trained Models. (arXiv:2204.03634v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03634","description":"<p>Class-incremental learning (CIL) has been widely studied under the setting of\nstarting from a small number of classes (base classes). Instead, we explore an\nunderstudied real-world setting of CIL that starts with a strong model\npre-trained on a large number of base classes. We hypothesize that a strong\nbase model can provide a good representation for novel classes and incremental\nlearning can be done with small adaptations. We propose a 2-stage training\nscheme, i) feature augmentation -- cloning part of the backbone and fine-tuning\nit on the novel data, and ii) fusion -- combining the base and novel\nclassifiers into a unified classifier. Experiments show that the proposed\nmethod significantly outperforms state-of-the-art CIL methods on the\nlarge-scale ImageNet dataset (e.g. +10% overall accuracy than the best). We\nalso propose and analyze understudied practical CIL scenarios, such as\nbase-novel overlap with distribution shift. Our proposed method is robust and\ngeneralizes to all analyzed CIL settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tz-Ying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_G/0/1/0/all/0/1\">Gurumurthy Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhizhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1\">Nuno Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1\">Rahul Bhotika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Category-Level Object Pose Estimation. (arXiv:2204.03635v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03635","description":"<p>Object pose estimation is an important component of most vision pipelines for\nembodied agents, as well as in 3D vision more generally. In this paper we\ntackle the problem of estimating the pose of novel object categories in a\nzero-shot manner. This extends much of the existing literature by removing the\nneed for pose-labelled datasets or category-specific CAD models for training or\ninference. Specifically, we make the following contributions. First, we\nformalise the zero-shot, category-level pose estimation problem and frame it in\na way that is most applicable to real-world embodied agents. Secondly, we\npropose a novel method based on semantic correspondences from a self-supervised\nvision transformer to solve the pose estimation problem. We further re-purpose\nthe recent CO3D dataset to present a controlled and realistic test setting.\nFinally, we demonstrate that all baselines for our proposed task perform\npoorly, and show that our method provides a six-fold improvement in average\nrotation accuracy at 30 degrees. Our code is available at\nhttps://github.com/applied-ai-lab/zero-shot-pose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goodwin_W/0/1/0/all/0/1\">Walter Goodwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaze_S/0/1/0/all/0/1\">Sagar Vaze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havoutis_I/0/1/0/all/0/1\">Ioannis Havoutis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1\">Ingmar Posner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation. (arXiv:2204.03636v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03636","description":"<p>Depth estimation from images serves as the fundamental step of 3D perception\nfor autonomous driving and is an economical alternative to expensive depth\nsensors like LiDAR. The temporal photometric consistency enables\nself-supervised depth estimation without labels, further facilitating its\napplication. However, most existing methods predict the depth solely based on\neach monocular image and ignore the correlations among multiple surrounding\ncameras, which are typically available for modern self-driving vehicles. In\nthis paper, we propose a SurroundDepth method to incorporate the information\nfrom multiple surrounding views to predict depth maps across cameras.\nSpecifically, we employ a joint network to process all the surrounding views\nand propose a cross-view transformer to effectively fuse the information from\nmultiple views. We apply cross-view self-attention to efficiently enable the\nglobal interactions between multi-camera feature maps. Different from\nself-supervised monocular depth estimation, we are able to predict real-world\nscales given multi-camera extrinsic matrices. To achieve this goal, we adopt\nstructure-from-motion to extract scale-aware pseudo depths to pretrain the\nmodels. Further, instead of predicting the ego-motion of each individual\ncamera, we estimate a universal ego-motion of the vehicle and transfer it to\neach view to achieve multi-view consistency. In experiments, our method\nachieves the state-of-the-art performance on the challenging multi-camera depth\nestimation datasets DDAD and nuScenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Linqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenzhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer. (arXiv:2204.03638v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03638","description":"<p>Videos are created to express emotion, exchange information, and share\nexperiences. Video synthesis has intrigued researchers for a long time. Despite\nthe rapid progress driven by advances in visual synthesis, most existing\nstudies focus on improving the frames' quality and the transitions between\nthem, while little progress has been made in generating longer videos. In this\npaper, we present a method that builds on 3D-VQGAN and transformers to generate\nvideos with thousands of frames. Our evaluation shows that our model trained on\n16-frame video clips from standard benchmarks such as UCF-101, Sky Time-lapse,\nand Taichi-HD datasets can generate diverse, coherent, and high-quality long\nvideos. We also showcase conditional extensions of our approach for generating\nmeaningful long videos by incorporating temporal information with text and\naudio. Videos and code can be found at\nhttps://songweige.github.io/projects/tats/index.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Thomas Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Harry Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guan Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">David Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equivariance Discovery by Learned Parameter-Sharing. (arXiv:2204.03640v1 [cs.LG])","link":"http://arxiv.org/abs/2204.03640","description":"<p>Designing equivariance as an inductive bias into deep-nets has been a\nprominent approach to build effective models, e.g., a convolutional neural\nnetwork incorporates translation equivariance. However, incorporating these\ninductive biases requires knowledge about the equivariance properties of the\ndata, which may not be available, e.g., when encountering a new domain. To\naddress this, we study how to discover interpretable equivariances from data.\nSpecifically, we formulate this discovery process as an optimization problem\nover a model's parameter-sharing schemes. We propose to use the partition\ndistance to empirically quantify the accuracy of the recovered equivariance.\nAlso, we theoretically analyze the method for Gaussian data and provide a bound\non the mean squared gap between the studied discovery scheme and the oracle\nscheme. Empirically, we show that the approach recovers known equivariances,\nsuch as permutations and shifts, on sum of numbers and spatially-invariant\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_R/0/1/0/all/0/1\">Raymond A. Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuan-Ting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander G. Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image-to-Image Translation with Generative Prior. (arXiv:2204.03641v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03641","description":"<p>Unsupervised image-to-image translation aims to learn the translation between\ntwo visual domains without paired data. Despite the recent progress in image\ntranslation models, it remains challenging to build mappings between complex\ndomains with drastic visual discrepancies. In this work, we present a novel\nframework, Generative Prior-guided UNsupervised Image-to-image Translation\n(GP-UNIT), to improve the overall quality and applicability of the translation\nalgorithm. Our key insight is to leverage the generative prior from pre-trained\nclass-conditional GANs (e.g., BigGAN) to learn rich content correspondences\nacross various domains. We propose a novel coarse-to-fine scheme: we first\ndistill the generative prior to capture a robust coarse-level content\nrepresentation that can link objects at an abstract semantic level, based on\nwhich fine-level content features are adaptively learned for more accurate\nmulti-level content correspondences. Extensive experiments demonstrate the\nsuperiority of our versatile framework over state-of-the-art methods in robust,\nhigh-quality and diversified translations, even for challenging and distant\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-train, Self-train, Distill: A simple recipe for Supersizing 3D Reconstruction. (arXiv:2204.03642v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03642","description":"<p>Our work learns a unified model for single-view 3D reconstruction of objects\nfrom hundreds of semantic categories. As a scalable alternative to direct 3D\nsupervision, our work relies on segmented image collections for learning 3D of\ngeneric categories. Unlike prior works that use similar supervision but learn\nindependent category-specific models from scratch, our approach of learning a\nunified model simplifies the training process while also allowing the model to\nbenefit from the common structure across categories. Using image collections\nfrom standard recognition datasets, we show that our approach allows learning\n3D inference for over 150 object categories. We evaluate using two datasets and\nqualitatively and quantitatively show that our unified reconstruction approach\nimproves over prior category-specific reconstruction baselines. Our final 3D\nreconstruction model is also capable of zero-shot inference on images from\nunseen object categories and we empirically show that increasing the number of\ntraining categories improves the reconstruction quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alwala_K/0/1/0/all/0/1\">Kalyan Vasudev Alwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulsiani_S/0/1/0/all/0/1\">Shubham Tulsiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Total Variation Optimization Layers for Computer Vision. (arXiv:2204.03643v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03643","description":"<p>Optimization within a layer of a deep-net has emerged as a new direction for\ndeep-net layer design. However, there are two main challenges when applying\nthese layers to computer vision tasks: (a) which optimization problem within a\nlayer is useful?; (b) how to ensure that computation within a layer remains\nefficient? To study question (a), in this work, we propose total variation (TV)\nminimization as a layer for computer vision. Motivated by the success of total\nvariation in image processing, we hypothesize that TV as a layer provides\nuseful inductive bias for deep-nets too. We study this hypothesis on five\ncomputer vision tasks: image classification, weakly supervised object\nlocalization, edge-preserving smoothing, edge detection, and image denoising,\nimproving over existing baselines. To achieve these results we had to address\nquestion (b): we developed a GPU-based projected-Newton method which is\n$37\\times$ faster than existing solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_R/0/1/0/all/0/1\">Raymond A. Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuan-Ting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhongzheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander G. Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DaViT: Dual Attention Vision Transformers. (arXiv:2204.03645v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03645","description":"<p>In this work, we introduce Dual Attention Vision Transformers (DaViT), a\nsimple yet effective vision transformer architecture that is able to capture\nglobal context while maintaining computational efficiency. We propose\napproaching the problem from an orthogonal angle: exploiting self-attention\nmechanisms with both \"spatial tokens\" and \"channel tokens\". With spatial\ntokens, the spatial dimension defines the token scope, and the channel\ndimension defines the token feature dimension. With channel tokens, we have the\ninverse: the channel dimension defines the token scope, and the spatial\ndimension defines the token feature dimension. We further group tokens along\nthe sequence direction for both spatial and channel tokens to maintain the\nlinear complexity of the entire model. We show that these two self-attentions\ncomplement each other: (i) since each channel token contains an abstract\nrepresentation of the entire image, the channel attention naturally captures\nglobal interactions and representations by taking all spatial positions into\naccount when computing attention scores between channels; (ii) the spatial\nattention refines the local representations by performing fine-grained\ninteractions across spatial locations, which in turn helps the global\ninformation modeling in channel attention. Extensive experiments show our DaViT\nachieves state-of-the-art performance on four different tasks with efficient\ncomputations. Without extra data, DaViT-Tiny, DaViT-Small, and DaViT-Base\nachieve 82.8%, 84.2%, and 84.6% top-1 accuracy on ImageNet-1K with 28.3M,\n49.7M, and 87.9M parameters, respectively. When we further scale up DaViT with\n1.5B weakly supervised image and text pairs, DaViT-Gaint reaches 90.4% top-1\naccuracy on ImageNet-1K. Code is available at https://github.com/dingmyu/davit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment. (arXiv:2204.03646v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03646","description":"<p>Most existing action quality assessment methods rely on the deep features of\nan entire video to predict the score, which is less reliable due to the\nnon-transparent inference process and poor interpretability. We argue that\nunderstanding both high-level semantics and internal temporal structures of\nactions in competitive sports videos is the key to making predictions accurate\nand interpretable. Towards this goal, we construct a new fine-grained dataset,\ncalled FineDiving, developed on diverse diving events with detailed annotations\non action procedures. We also propose a procedure-aware approach for action\nquality assessment, learned by a new Temporal Segmentation Attention module.\nSpecifically, we propose to parse pairwise query and exemplar action instances\ninto consecutive steps with diverse semantic and temporal correspondences. The\nprocedure-aware cross-attention is proposed to learn embeddings between query\nand exemplar steps to discover their semantic, spatial, and temporal\ncorrespondences, and further serve for fine-grained contrastive regression to\nderive a reliable scoring mechanism. Extensive experiments demonstrate that our\napproach achieves substantial improvements over state-of-the-art methods with\nbetter interpretability. The dataset and code are available at\n\\url{https://github.com/xujinglin/FineDiving}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xumin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting CLIP For Phrase Localization Without Further Training. (arXiv:2204.03647v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03647","description":"<p>Supervised or weakly supervised methods for phrase localization (textual\ngrounding) either rely on human annotations or some other supervised models,\ne.g., object detectors. Obtaining these annotations is labor-intensive and may\nbe difficult to scale in practice. We propose to leverage recent advances in\ncontrastive language-vision models, CLIP, pre-trained on image and caption\npairs collected from the internet. In its original form, CLIP only outputs an\nimage-level embedding without any spatial resolution. We adapt CLIP to generate\nhigh-resolution spatial feature maps. Importantly, we can extract feature maps\nfrom both ViT and ResNet CLIP model while maintaining the semantic properties\nof an image embedding. This provides a natural framework for phrase\nlocalization. Our method for phrase localization requires no human annotations\nor additional training. Extensive experiments show that our method outperforms\nexisting no-training methods in zero-shot phrase localization, and in some\ncases, it even outperforms supervised methods. Code is available at\nhttps://github.com/pals-ttic/adapting-CLIP .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1\">Greg Shakhnarovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_R/0/1/0/all/0/1\">Raymond A. Yeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SunStage: Portrait Reconstruction and Relighting using the Sun as a Light Stage. (arXiv:2204.03648v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03648","description":"<p>Outdoor portrait photographs are often marred by the harsh shadows cast under\ndirect sunlight. To resolve this, one can use post-capture lighting\nmanipulation techniques, but these methods either require complex hardware\n(e.g., a light stage) to capture each individual, or rely on image-based priors\nand thus fail to reconstruct many of the subtle facial details that vary from\nperson to person. In this paper, we present SunStage, a system for accurate,\nindividually-tailored, and lightweight reconstruction of facial geometry and\nreflectance that can be used for general portrait relighting with cast shadows.\nOur method only requires the user to capture a selfie video outdoors, rotating\nin place, and uses the varying angles between the sun and the face as\nconstraints in the joint reconstruction of facial geometry, reflectance\nproperties, and lighting parameters. Aside from relighting, we show that our\nreconstruction can be used for applications like reflectance editing and view\nsynthesis. Results and interactive demos are available at\nhttps://grail.cs.washington.edu/projects/sunstage/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holynski_A/0/1/0/all/0/1\">Aleksander Holynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiuming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuaner Cecilia Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Prompt Learning for Vision-Language Models. (arXiv:2204.03649v1 [cs.CV])","link":"http://arxiv.org/abs/2204.03649","description":"<p>Contrastive vision-language models like CLIP have shown great progress in\nzero-shot transfer learning. This new paradigm uses large-scale image-text\npairs for training and aligns images and texts in a common embedding space. In\nthe inference stage, the proper text description, known as prompt, needs to be\ncarefully designed for zero-shot transfer. To avoid laborious prompt\nengineering and simultaneously improve transfer performance, recent works such\nas CoOp, CLIP-Adapter and Tip-Adapter propose to adapt vision-language models\nfor downstream image recognition tasks by either optimizing the continuous\nprompt representations or training an additional adapter network on top of the\npre-trained vision-language models on a small set of labeled data. Though\npromising improvements are achieved, using labeled images from target datasets\nmay violate the intention of zero-shot transfer of pre-trained vision-language\nmodels. In this paper, we propose an unsupervised prompt learning (UPL)\nframework, which does not require any annotations of the target dataset, to\nimprove the zero-shot transfer of CLIP-like vision-language models.\nExperimentally, for zero-shot transfer, our UPL outperforms original CLIP with\nprompt engineering and on ImageNet as well as other 10 datasets. An enhanced\nversion of UPL is even on par with the 8-shot CoOp and the 8-shot TIP-Adapter\non most datasets while our method does not need any labeled images for\ntraining. Code and models are available at\nhttps://github.com/tonyhuang2022/UPL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tony Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_J/0/1/0/all/0/1\">Jack Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Hard Examples for Pixel-wise Classification. (arXiv:1812.05447v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1812.05447","description":"<p>Pixel-wise classification in remote sensing identifies entities in\nlarge-scale satellite-based images at the pixel level. Few fully annotated\nlarge-scale datasets for pixel-wise classification exist due to the challenges\nof annotating individual pixels. Training data scarcity inevitably ensues from\nthe annotation challenge, leading to overfitting classifiers and degraded\nclassification performance. The lack of annotated pixels also necessarily\nresults in few hard examples of various entities critical for generating a\nrobust classification hyperplane. To overcome the problem of the data scarcity\nand lack of hard examples in training, we introduce a two-step hard example\ngeneration (HEG) approach that first generates hard example candidates and then\nmines actual hard examples. In the first step, a generator that creates hard\nexample candidates is learned via the adversarial learning framework by fooling\na discriminator and a pixel-wise classification model at the same time. In the\nsecond step, mining is performed to build a fixed number of hard examples from\na large pool of real and artificially generated examples. To evaluate the\neffectiveness of the proposed HEG approach, we design a 9-layer fully\nconvolutional network suitable for pixel-wise classification. Experiments show\nthat using generated hard examples from the proposed HEG approach improves the\npixel-wise classification model's accuracy on red tide detection and\nhyperspectral image classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungtae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Heesung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wonkook Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Driven Multi-Camera Pedestrian Detection. (arXiv:1812.10779v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1812.10779","description":"<p>In the current worldwide situation, pedestrian detection has reemerged as a\npivotal tool for intelligent video-based systems aiming to solve tasks such as\npedestrian tracking, social distancing monitoring or pedestrian mass counting.\nPedestrian detection methods, even the top performing ones, are highly\nsensitive to occlusions among pedestrians, which dramatically degrades their\nperformance in crowded scenarios. The generalization of multi-camera set-ups\npermits to better confront occlusions by combining information from different\nviewpoints. In this paper, we present a multi-camera approach to globally\ncombine pedestrian detections leveraging automatically extracted scene context.\nContrarily to the majority of the methods of the state-of-the-art, the proposed\napproach is scene-agnostic, not requiring a tailored adaptation to the target\nscenario\\textemdash e.g., via fine-tunning. This noteworthy attribute does not\nrequire \\textit{ad hoc} training with labelled data, expediting the deployment\nof the proposed method in real-world situations. Context information, obtained\nvia semantic segmentation, is used 1) to automatically generate a common Area\nof Interest for the scene and all the cameras, avoiding the usual need of\nmanually defining it; and 2) to obtain detections for each camera by solving a\nglobal optimization problem that maximizes coherence of detections both in each\n2D image and in the 3D scene. This process yields tightly-fitted bounding boxes\nthat circumvent occlusions or miss-detections. Experimental results on five\npublicly available datasets show that the proposed approach outperforms\nstate-of-the-art multi-camera pedestrian detectors, even some specifically\ntrained on the target scenario, signifying the versatility and robustness of\nthe proposed method without requiring ad-hoc annotations nor human-guided\nconfiguration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Cifuentes_A/0/1/0/all/0/1\">Alejandro L&#xf3;pez-Cifuentes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escudero_Vinolo_M/0/1/0/all/0/1\">Marcos Escudero-Vi&#xf1;olo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bescos_J/0/1/0/all/0/1\">Jes&#xfa;s Besc&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carballeira_P/0/1/0/all/0/1\">Pablo Carballeira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GGNN: Graph-based GPU Nearest Neighbor Search. (arXiv:1912.01059v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.01059","description":"<p>Approximate nearest neighbor (ANN) search in high dimensions is an integral\npart of several computer vision systems and gains importance in deep learning\nwith explicit memory representations. Since PQT, FAISS, and SONG started to\nleverage the massive parallelism offered by GPUs, GPU-based implementations are\na crucial resource for today's state-of-the-art ANN methods. While most of\nthese methods allow for faster queries, less emphasis is devoted to\naccelerating the construction of the underlying index structures. In this\npaper, we propose a novel GPU-friendly search structure based on nearest\nneighbor graphs and information propagation on graphs. Our method is designed\nto take advantage of GPU architectures to accelerate the hierarchical\nconstruction of the index structure and for performing the query. Empirical\nevaluation shows that GGNN significantly surpasses the state-of-the-art CPU-\nand GPU-based systems in terms of build-time, accuracy and search speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Groh_F/0/1/0/all/0/1\">Fabian Groh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruppert_L/0/1/0/all/0/1\">Lukas Ruppert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieschollek_P/0/1/0/all/0/1\">Patrick Wieschollek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P.A. Lensch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAIS: Automatic Channel Pruning via Differentiable Annealing Indicator Search. (arXiv:2011.02166v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.02166","description":"<p>The convolutional neural network has achieved great success in fulfilling\ncomputer vision tasks despite large computation overhead against efficient\ndeployment. Structured (channel) pruning is usually applied to reduce the model\nredundancy while preserving the network structure, such that the pruned network\ncan be easily deployed in practice. However, existing structured pruning\nmethods require hand-crafted rules which may lead to tremendous pruning space.\nIn this paper, we introduce Differentiable Annealing Indicator Search (DAIS)\nthat leverages the strength of neural architecture search in the channel\npruning and automatically searches for the effective pruned model with given\nconstraints on computation overhead. Specifically, DAIS relaxes the binarized\nchannel indicators to be continuous and then jointly learns both indicators and\nmodel parameters via bi-level optimization. To bridge the non-negligible\ndiscrepancy between the continuous model and the target binarized model, DAIS\nproposes an annealing-based procedure to steer the indicator convergence\ntowards binarized states. Moreover, DAIS designs various regularizations based\non a priori structural knowledge to control the pruning sparsity and to improve\nmodel performance. Experimental results show that DAIS outperforms\nstate-of-the-art pruning methods on CIFAR-10, CIFAR-100, and ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yushuo Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pengyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_K/0/1/0/all/0/1\">Kaigui Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics-informed neural networks for myocardial perfusion MRI quantification. (arXiv:2011.12844v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2011.12844","description":"<p>Tracer-kinetic models allow for the quantification of kinetic parameters such\nas blood flow from dynamic contrast-enhanced magnetic resonance (MR) images.\nFitting the observed data with multi-compartment exchange models is desirable,\nas they are physiologically plausible and resolve directly for blood flow and\nmicrovascular function. However, the reliability of model fitting is limited by\nthe low signal-to-noise ratio, temporal resolution, and acquisition length.\nThis may result in inaccurate parameter estimates.\n</p>\n<p>This study introduces physics-informed neural networks (PINNs) as a means to\nperform myocardial perfusion MR quantification, which provides a versatile\nscheme for the inference of kinetic parameters. These neural networks can be\ntrained to fit the observed perfusion MR data while respecting the underlying\nphysical conservation laws described by a multi-compartment exchange model.\nHere, we provide a framework for the implementation of PINNs in myocardial\nperfusion MR.\n</p>\n<p>The approach is validated both in silico and in vivo. In the in silico study,\nan overall reduction in mean-squared error with the ground-truth parameters was\nobserved compared to a standard non-linear least squares fitting approach. The\nin vivo study demonstrates that the method produces parameter values comparable\nto those previously found in literature, as well as providing parameter maps\nwhich match the clinical diagnosis of patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Herten_R/0/1/0/all/0/1\">Rudolf L.M. van Herten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiribiri_A/0/1/0/all/0/1\">Amedeo Chiribiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breeuwer_M/0/1/0/all/0/1\">Marcel Breeuwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veta_M/0/1/0/all/0/1\">Mitko Veta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scannell_C/0/1/0/all/0/1\">Cian M. Scannell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning Inverts the Data Generating Process. (arXiv:2102.08850v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.08850","description":"<p>Contrastive learning has recently seen tremendous success in self-supervised\nlearning. So far, however, it is largely unclear why the learned\nrepresentations generalize so effectively to a large variety of downstream\ntasks. We here prove that feedforward models trained with objectives belonging\nto the commonly used InfoNCE family learn to implicitly invert the underlying\ngenerative model of the observed data. While the proofs make certain\nstatistical assumptions about the generative model, we observe empirically that\nour findings hold even if these assumptions are severely violated. Our theory\nhighlights a fundamental connection between contrastive learning, generative\nmodeling, and nonlinear independent component analysis, thereby furthering our\nunderstanding of the learned representations as well as providing a theoretical\nfoundation to derive more effective contrastive losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roland S. Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1\">Steffen Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Federated Peer Learning for Skin Lesion Classification. (arXiv:2103.03703v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.03703","description":"<p>Globally, Skin carcinoma is among the most lethal diseases. Millions of\npeople are diagnosed with this cancer every year. Sill, early detection can\ndecrease the medication cost and mortality rate substantially. The recent\nimprovement in automated cancer classification using deep learning methods has\nreached a human-level performance requiring a large amount of annotated data\nassembled in one location, yet, finding such conditions usually is not\nfeasible. Recently, federated learning (FL) has been proposed to train\ndecentralized models in a privacy-preserved fashion depending on labeled data\nat the client-side, which is usually not available and costly. To address this,\nwe propose \\verb!FedPerl!, a semi-supervised federated learning method. Our\nmethod is inspired by peer learning from educational psychology and ensemble\naveraging from committee machines. FedPerl builds communities based on clients'\nsimilarities. Then it encourages communities members to learn from each other\nto generate more accurate pseudo labels for the unlabeled data. We also\nproposed the peer anonymization (PA) technique to anonymize clients. As a core\ncomponent of our method, PA is orthogonal to other methods without additional\ncomplexity and reduces the communication cost while enhancing performance.\nFinally, we propose a dynamic peer-learning policy that controls the learning\nstream to avoid any degradation in the performance, especially for individual\nclients. Our experimental setup consists of 71,000 skin lesion images collected\nfrom 5 publicly available datasets. We test our method in four different\nscenarios in SSFL. With few annotated data, FedPerl is on par with a\nstate-of-the-art method in skin lesion classification in the standard setup\nwhile outperforming SSFLs and the baselines by 1.8% and 15.8%, respectively.\nAlso, it generalizes better to unseen clients while being less sensitive to\nnoisy ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bdair_T/0/1/0/all/0/1\">Tariq Bdair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albarqouni_S/0/1/0/all/0/1\">Shadi Albarqouni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiscale Clustering of Hyperspectral Images Through Spectral-Spatial Diffusion Geometry. (arXiv:2103.15783v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.15783","description":"<p>Clustering algorithms partition a dataset into groups of similar points. The\nprimary contribution of this article is the Multiscale Spatially-Regularized\nDiffusion Learning (M-SRDL) clustering algorithm, which uses\nspatially-regularized diffusion distances to efficiently and accurately learn\nmultiple scales of latent structure in hyperspectral images. The M-SRDL\nclustering algorithm extracts clusterings at many scales from a hyperspectral\nimage and outputs these clusterings' variation of information-barycenter as an\nexemplar for all underlying cluster structure. We show that incorporating\nspatial regularization into a multiscale clustering framework results in\nsmoother and more coherent clusters when applied to hyperspectral data,\nyielding more accurate clustering labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polk_S/0/1/0/all/0/1\">Sam L. Polk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_J/0/1/0/all/0/1\">James M. Murphy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A tutorial on $\\mathbf{SE}(3)$ transformation parameterizations and on-manifold optimization. (arXiv:2103.15980v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.15980","description":"<p>An arbitrary rigid transformation in $\\mathbf{SE}(3)$ can be separated into\ntwo parts, namely, a translation and a rigid rotation. This technical report\nreviews, under a unifying viewpoint, three common alternatives to representing\nthe rotation part: sets of three (yaw-pitch-roll) Euler angles, orthogonal\nrotation matrices from $\\mathbf{SO}(3)$ and quaternions. It will be described:\n(i) the equivalence between these representations and the formulas for\ntransforming one to each other (in all cases considering the translational and\nrotational parts as a whole), (ii) how to compose poses with poses and poses\nwith points in each representation and (iii) how the uncertainty of the poses\n(when modeled as Gaussian distributions) is affected by these transformations\nand compositions. Some brief notes are also given about the Jacobians required\nto implement least-squares optimization on manifolds, an very promising\napproach in recent engineering literature. The text reflects which MRPT C++\nlibrary functions implement each of the described algorithms. All formulas and\ntheir implementation have been thoroughly validated by means of unit testing\nand numerical estimation of the Jacobians\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blanco_Claraco_J/0/1/0/all/0/1\">Jos&#xe9; Luis Blanco-Claraco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Dynamics of Nonlinear Representation Learning and Its Application. (arXiv:2106.14836v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.14836","description":"<p>Representations of the world environment play a crucial role in artificial\nintelligence. It is often inefficient to conduct reasoning and inference\ndirectly in the space of raw sensory representations, such as pixel values of\nimages. Representation learning allows us to automatically discover suitable\nrepresentations from raw sensory data. For example, given raw sensory data, a\ndeep neural network learns nonlinear representations at its hidden layers,\nwhich are subsequently used for classification at its output layer. This\nhappens implicitly during training through minimizing a supervised or\nunsupervised loss. In this paper, we study the dynamics of such implicit\nnonlinear representation learning. We identify a pair of a new assumption and a\nnovel condition, called the common model structure assumption and the\ndata-architecture alignment condition. Under the common model structure\nassumption, the data-architecture alignment condition is shown to be sufficient\nfor the global convergence and necessary for the global optimality. Moreover,\nour theory explains how and when increasing the network size does and does not\nimprove the training behaviors in the practical regime. Our results provide\npractical guidance for designing a model structure: e.g., the common model\nstructure assumption can be used as a justification for using a particular\nmodel structure instead of others. We also derive a new training framework,\nwhich satisfies the data-architecture alignment condition by automatically\nmodifying any given training algorithm. Given a standard training algorithm,\nthe framework running its modified version is empirically shown to maintain\ncompetitive test performances while providing global convergence guarantees for\ndeep residual neural networks with convolutions, skip connections, and batch\nnormalization with datasets, including MNIST, CIFAR-10, CIFAR-100, Semeion,\nKMNIST and SVHN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhun Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scarce Data Driven Deep Learning of Drones via Generalized Data Distribution Space. (arXiv:2108.08244v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08244","description":"<p>Increased drone proliferation in civilian and professional settings has\ncreated new threat vectors for airports and national infrastructures. The\neconomic damage for a single major airport from drone incursions is estimated\nto be millions per day. Due to the lack of diverse drone training data,\naccurate training of deep learning detection algorithms under scarce data is an\nopen challenge. Existing methods largely rely on collecting diverse and\ncomprehensive experimental drone footage data, artificially induced data\naugmentation, transfer and meta-learning, as well as physics-informed learning.\nHowever, these methods cannot guarantee capturing diverse drone designs and\nfully understanding the deep feature space of drones. Here, we show how\nunderstanding the general distribution of the drone data via a Generative\nAdversarial Network (GAN) and explaining the missing features using Topological\nData Analysis (TDA) - can allow us to acquire missing data to achieve rapid and\nmore accurate learning. We demonstrate our results on a drone image dataset,\nwhich contains both real drone images as well as simulated images from\ncomputer-aided design. When compared to random data collection (usual practice\n- discriminator accuracy of 94.67\\% after 200 epochs), our proposed GAN-TDA\ninformed data collection method offers a significant 4\\% improvement (99.42\\%\nafter 200 epochs). We believe that this approach of exploiting general data\ndistribution knowledge form neural networks can be applied to a wide range of\nscarce data open challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Schyler C. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhuangkun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsourdos_A/0/1/0/all/0/1\">Antonios Tsourdos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weisi Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Cycle-consistent Generative Adversarial Networks for Pan-sharpening. (arXiv:2109.09395v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09395","description":"<p>Deep learning based pan-sharpening has received significant research interest\nin recent years. Most of existing methods fall into the supervised learning\nframework in which they down-sample the multi-spectral (MS) and panchromatic\n(PAN) images and regard the original MS images as ground truths to form\ntraining samples. Although impressive performance could be achieved, they have\ndifficulties generalizing to the original full-scale images due to the scale\ngap, which makes them lack of practicability. In this paper, we propose an\nunsupervised generative adversarial framework that learns from the full-scale\nimages without the ground truths to alleviate this problem. We extract the\nmodality-specific features from the PAN and MS images with a two-stream\ngenerator, perform fusion in the feature domain, and then reconstruct the\npan-sharpened images. Furthermore, we introduce a novel hybrid loss based on\nthe cycle-consistency and adversarial scheme to improve the performance.\nComparison experiments with the state-of-the-art methods are conducted on\nGaoFen-2 and WorldView-3 satellites. Results demonstrate that the proposed\nmethod can greatly improve the pan-sharpening performance on the full-scale\nimages, which clearly show its practical value. Codes are available at\nhttps://github.com/zhysora/UCGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huanyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_D/0/1/0/all/0/1\">Dawei Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning based Medical Image Deepfake Detection: A Comparative Study. (arXiv:2109.12800v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12800","description":"<p>Deep generative networks in recent years have reinforced the need for caution\nwhile consuming various modalities of digital information. One avenue of\ndeepfake creation is aligned with injection and removal of tumors from medical\nscans. Failure to detect medical deepfakes can lead to large setbacks on\nhospital resources or even loss of life. This paper attempts to address the\ndetection of such attacks with a structured case study. Specifically, we\nevaluate eight different machine learning algorithms, which including three\nconventional machine learning methods, support vector machine, random forest,\ndecision tree, and five deep learning models, DenseNet121, DenseNet201,\nResNet50, ResNet101, VGG19, on distinguishing between tampered and untampered\nimages.For deep learning models, the five models are used for feature\nextraction, then fine-tune for each pre-trained model is performed. The\nfindings of this work show near perfect accuracy in detecting instances of\ntumor injections and removals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Solaiyappan_S/0/1/0/all/0/1\">Siddharth Solaiyappan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Training of 3D Seismic Image Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05319","description":"<p>Data-driven fault detection has been regarded as a 3D image segmentation\ntask. The models trained from synthetic data are difficult to generalize in\nsome surveys. Recently, training 3D fault segmentation using sparse manual 2D\nslices is thought to yield promising results, but manual labeling has many\nfalse negative labels (abnormal annotations), which is detrimental to training\nand consequently to detection performance. Motivated to train 3D fault\nsegmentation networks under sparse 2D labels while suppressing false negative\nlabels, we analyze the training process gradient and propose the Mask Dice (MD)\nloss. Moreover, the fault is an edge feature, and current encoder-decoder\narchitectures widely used for fault detection (e.g., U-shape network) are not\nconducive to edge representation. Consequently, Fault-Net is proposed, which is\ndesigned for the characteristics of faults, employs high-resolution propagation\nfeatures, and embeds MultiScale Compression Fusion block to fuse multi-scale\ninformation, which allows the edge information to be fully preserved during\npropagation and fusion, thus enabling advanced performance via few\ncomputational resources. Experimental demonstrates that MD loss supports the\ninclusion of human experience in training and suppresses false negative labels\ntherein, enabling baseline models to improve performance and generalize to more\nsurveys. Fault-Net is capable to provide a more stable and reliable\ninterpretation of faults, it uses extremely low computational resources and\ninference is significantly faster than other models. Our method indicates\noptimal performance in comparison with several mainstream methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yimin Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianbing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Timing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shaoquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zongchao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ByteTrack: Multi-Object Tracking by Associating Every Detection Box. (arXiv:2110.06864v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06864","description":"<p>Multi-object tracking (MOT) aims at estimating bounding boxes and identities\nof objects in videos. Most methods obtain identities by associating detection\nboxes whose scores are higher than a threshold. The objects with low detection\nscores, e.g. occluded objects, are simply thrown away, which brings\nnon-negligible true object missing and fragmented trajectories. To solve this\nproblem, we present a simple, effective and generic association method,\ntracking by associating almost every detection box instead of only the high\nscore ones. For the low score detection boxes, we utilize their similarities\nwith tracklets to recover true objects and filter out the background\ndetections. When applied to 9 different state-of-the-art trackers, our method\nachieves consistent improvement on IDF1 score ranging from 1 to 10 points. To\nput forwards the state-of-the-art performance of MOT, we design a simple and\nstrong tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3\nIDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a\nsingle V100 GPU. ByteTrack also achieves state-of-the-art performance on MOT20,\nHiEve and BDD100K tracking benchmarks. The source code, pre-trained models with\ndeploy versions and tutorials of applying to other trackers are released at\nhttps://github.com/ifzhang/ByteTrack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dongdong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_F/0/1/0/all/0/1\">Fucheng Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Lightweight Single Object Tracking with UHP-SOT++. (arXiv:2111.07548v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07548","description":"<p>An unsupervised, lightweight and high-performance single object tracker,\ncalled UHP-SOT, was proposed by Zhou et al. recently. As an extension, we\npresent an enhanced version and name it UHP-SOT++ in this work. Built upon the\nfoundation of the discriminative-correlation-filters-based (DCF-based) tracker,\ntwo new ingredients are introduced in UHP-SOT and UHP-SOT++: 1) background\nmotion modeling and 2) object box trajectory modeling. The main difference\nbetween UHP-SOT and UHP-SOT++ is the fusion strategy of proposals from three\nmodels (i.e., DCF, background motion and object box trajectory models). An\nimproved fusion strategy is adopted by UHP-SOT++ for more robust tracking\nperformance against large-scale tracking datasets. Our second contribution lies\nin an extensive evaluation of the performance of state-of-the-art supervised\nand unsupervised methods by testing them on four SOT benchmark datasets -\nOTB2015, TC128, UAV123 and LaSOT. Experiments show that UHP-SOT++ outperforms\nall previous unsupervised methods and several deep-learning (DL) methods in\ntracking accuracy. Since UHP-SOT++ has extremely small model size, high\ntracking performance, and low computational complexity (operating at a rate of\n20 FPS on an i5 CPU even without code optimization), it is an ideal solution in\nreal-time object tracking on resource-limited platforms. Based on the\nexperimental results, we compare pros and cons of supervised and unsupervised\ntrackers and provide a new perspective to understand the performance gap\nbetween supervised and unsupervised methods, which is the third contribution of\nthis work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Suya You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StylePart: Image-based Shape Part Manipulation. (arXiv:2111.10520v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10520","description":"<p>Due to a lack of image-based \"part controllers\", shape manipulation of\nman-made shape images, such as resizing the backrest of a chair or replacing a\ncup handle is not intuitive. To tackle this problem, we present StylePart, a\nframework that enables direct shape manipulation of an image by leveraging\ngenerative models of both images and 3D shapes. Our key contribution is a\nshape-consistent latent mapping function that connects the image generative\nlatent space and the 3D man-made shape attribute latent space. Our method\n\"forwardly maps\" the image content to its corresponding 3D shape attributes,\nwhere the shape part can be easily manipulated. The attribute codes of the\nmanipulated 3D shape are then \"backwardly mapped\" to the image latent code to\nobtain the final manipulated image. We demonstrate our approach through various\nmanipulation tasks, including part replacement, part resizing, and viewpoint\nmanipulation, and evaluate its effectiveness through extensive ablation\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_I/0/1/0/all/0/1\">I-Chao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Li-Wen Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu-Ting Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bing-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion-from-Blur: 3D Shape and Motion Estimation of Motion-blurred Objects in Videos. (arXiv:2111.14465v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14465","description":"<p>We propose a method for jointly estimating the 3D motion, 3D shape, and\nappearance of highly motion-blurred objects from a video. To this end, we model\nthe blurred appearance of a fast moving object in a generative fashion by\nparametrizing its 3D position, rotation, velocity, acceleration, bounces,\nshape, and texture over the duration of a predefined time window spanning\nmultiple frames. Using differentiable rendering, we are able to estimate all\nparameters by minimizing the pixel-wise reprojection error to the input video\nvia backpropagating through a rendering pipeline that accounts for motion blur\nby averaging the graphics output over short time intervals. For that purpose,\nwe also estimate the camera exposure gap time within the same optimization. To\naccount for abrupt motion changes like bounces, we model the motion trajectory\nas a piece-wise polynomial, and we are able to estimate the specific time of\nthe bounce at sub-frame accuracy. Experiments on established benchmark datasets\ndemonstrate that our method outperforms previous methods for fast moving object\ndeblurring and 3D reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rozumnyi_D/0/1/0/all/0/1\">Denys Rozumnyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPIN: Simplifying Polar Invariance for Neural networks Application to vision-based irradiance forecasting. (arXiv:2111.14507v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14507","description":"<p>Translational invariance induced by pooling operations is an inherent\nproperty of convolutional neural networks, which facilitates numerous computer\nvision tasks such as classification. Yet to leverage rotational invariant\ntasks, convolutional architectures require specific rotational invariant layers\nor extensive data augmentation to learn from diverse rotated versions of a\ngiven spatial configuration. Unwrapping the image into its polar coordinates\nprovides a more explicit representation to train a convolutional architecture\nas the rotational invariance becomes translational, hence the visually distinct\nbut otherwise equivalent rotated versions of a given scene can be learnt from a\nsingle image. We show with two common vision-based solar irradiance forecasting\nchallenges (i.e. using ground-taken sky images or satellite images), that this\npreprocessing step significantly improves prediction results by standardising\nthe scene representation, while decreasing training time by a factor of 4\ncompared to augmenting data with rotations. In addition, this transformation\nmagnifies the area surrounding the centre of the rotation, leading to more\naccurate short-term irradiance predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1\">Quentin Paletta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anthony Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbod_G/0/1/0/all/0/1\">Guillaume Arbod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanc_P/0/1/0/all/0/1\">Philippe Blanc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation. (arXiv:2111.14826v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14826","description":"<p>The nonuniform quantization strategy for compressing neural networks usually\nachieves better performance than its counterpart, i.e., uniform strategy, due\nto its superior representational capacity. However, many nonuniform\nquantization methods overlook the complicated projection process in\nimplementing the nonuniformly quantized weights/activations, which incurs\nnon-negligible time and space overhead in hardware deployment. In this study,\nwe propose Nonuniform-to-Uniform Quantization (N2UQ), a method that can\nmaintain the strong representation ability of nonuniform methods while being\nhardware-friendly and efficient as the uniform quantization for model\ninference. We achieve this through learning the flexible in-equidistant input\nthresholds to better fit the underlying distribution while quantizing these\nreal-valued inputs into equidistant output levels. To train the quantized\nnetwork with learnable input thresholds, we introduce a generalized\nstraight-through estimator (G-STE) for intractable backward derivative\ncalculation w.r.t. threshold parameters. Additionally, we consider entropy\npreserving regularization to further reduce information loss in weight\nquantization. Even under this adverse constraint of imposing uniformly\nquantized weights and activations, our N2UQ outperforms state-of-the-art\nnonuniform quantization methods by 0.5~1.7 on ImageNet, demonstrating the\ncontribution of N2UQ design. Code and models are available at:\nhttps://github.com/liuzechun/Nonuniform-to-Uniform-Quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Depth Priors for Neural Radiance Fields from Sparse Input Views. (arXiv:2112.03288v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03288","description":"<p>Neural radiance fields (NeRF) encode a scene into a neural representation\nthat enables photo-realistic rendering of novel views. However, a successful\nreconstruction from RGB images requires a large number of input views taken\nunder static conditions - typically up to a few hundred images for room-size\nscenes. Our method aims to synthesize novel views of whole rooms from an order\nof magnitude fewer images. To this end, we leverage dense depth priors in order\nto constrain the NeRF optimization. First, we take advantage of the sparse\ndepth data that is freely available from the structure from motion (SfM)\npreprocessing step used to estimate camera poses. Second, we use depth\ncompletion to convert these sparse points into dense depth maps and uncertainty\nestimates, which are used to guide NeRF optimization. Our method enables\ndata-efficient novel view synthesis on challenging indoor scenes, using as few\nas 18 images for an entire scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roessle_B/0/1/0/all/0/1\">Barbara Roessle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mildenhall_B/0/1/0/all/0/1\">Ben Mildenhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Pratul P. Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07225","description":"<p>The long-tailed class distribution in visual recognition tasks poses great\nchallenges for neural networks on how to handle the biased predictions between\nhead and tail classes, i.e., the model tends to classify tail classes as head\nclasses. While existing research focused on data resampling and loss function\nengineering, in this paper, we take a different perspective: the classification\nmargins. We study the relationship between the margins and logits\n(classification scores) and empirically observe the biased margins and the\nbiased logits are positively correlated. We propose MARC, a simple yet\neffective MARgin Calibration function to dynamically calibrate the biased\nmargins for unbiased logits. We validate MARC through extensive experiments on\ncommon long-tailed benchmarks including CIFAR-LT, ImageNet-LT, Places-LT, and\niNaturalist-LT. Experimental results demonstrate that our MARC achieves\nfavorable results on these benchmarks. In addition, MARC is extremely easy to\nimplement with just three lines of code. We hope this simple method will\nmotivate people to rethink the biased margins and biased logits in long-tailed\nvisual recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1\">Takahiro Shinozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An effective coaxiality measurement for twist drill based on line structured light sensor. (arXiv:2112.09873v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09873","description":"<p>Aiming at the accurate and effective coaxiality measurement for twist drill\nwith irregular surface, an optical measurement mechanism is proposed in this\npaper. First, A high-precision rotation instrument based on four core units is\ndesigned, which can obtain the 3-D point cloud data of full angle for the twist\ndrill. Second, in the data processing stage, an improved robust Gaussian\nmixture model is established for accurate and rapid blade back segmentation. To\nimprove measurement efficiency, a rapid reconstruction method of the twist\ndrill axis based on orthogonal synthesis is provided to locate the axial\nposition of the maximum deviation from the benchmark by utilizing the extracted\nblade back data. Finally, by calculating the maximum radial Euclidean distance\nfrom the benchmark, the coaxiality error of the twist drill is obtained.\nComparing with other measurement methods, experimental results show that our\nproposed method is effective with high precision of 3 um and high efficiency of\nless than 3 s/pc. The result demonstrate that the proposed method is effective,\nrobust and automatic, it can be applied in many actual industrial scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1\">Ailing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiaojiao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shufang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fei Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Raw High-Definition Radar for Multi-Task Learning. (arXiv:2112.10646v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10646","description":"<p>With their robustness to adverse weather conditions and ability to measure\nspeeds, radar sensors have been part of the automotive landscape for more than\ntwo decades. Recent progress toward High Definition (HD) Imaging radar has\ndriven the angular resolution below the degree, thus approaching laser scanning\nperformance. However, the amount of data a HD radar delivers and the\ncomputational cost to estimate the angular positions remain a challenge. In\nthis paper, we propose a novel HD radar sensing model, FFT-RadNet, that\neliminates the overhead of computing the range-azimuth-Doppler 3D tensor,\nlearning instead to recover angles from a range-Doppler spectrum. FFT-RadNet is\ntrained both to detect vehicles and to segment free driving space. On both\ntasks, it competes with the most recent radar-based models while requiring less\ncompute and memory. Also, we collected and annotated 2-hour worth of raw data\nfrom synchronized automotive-grade sensors (camera, laser, HD radar) in various\nenvironments (city street, highway, countryside road). This unique dataset,\nnick-named RADIal for \"Radar, Lidar et al.\", is available at\nhttps://github.com/valeoai/RADIal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rebut_J/0/1/0/all/0/1\">Julien Rebut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouaknine_A/0/1/0/all/0/1\">Arthur Ouaknine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_W/0/1/0/all/0/1\">Waqas Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metrics for saliency map evaluation of deep learning explanation methods. (arXiv:2201.13291v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.13291","description":"<p>Due to the black-box nature of deep learning models, there is a recent\ndevelopment of solutions for visual explanations of CNNs. Given the high cost\nof user studies, metrics are necessary to compare and evaluate these different\nmethods. In this paper, we critically analyze the Deletion Area Under Curve\n(DAUC) and Insertion Area Under Curve (IAUC) metrics proposed by Petsiuk et al.\n(2018). These metrics were designed to evaluate the faithfulness of saliency\nmaps generated by generic methods such as Grad-CAM or RISE. First, we show that\nthe actual saliency score values given by the saliency map are ignored as only\nthe ranking of the scores is taken into account. This shows that these metrics\nare insufficient by themselves, as the visual appearance of a saliency map can\nchange significantly without the ranking of the scores being modified.\nSecondly, we argue that during the computation of DAUC and IAUC, the model is\npresented with images that are out of the training distribution which might\nlead to an unreliable behavior of the model being explained. To complement\nDAUC/IAUC, we propose new metrics that quantify the sparsity and the\ncalibration of explanation methods, two previously unstudied properties.\nFinally, we give general remarks about the metrics studied in this paper and\ndiscuss how to evaluate them in a user study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1\">Tristan Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freour_T/0/1/0/all/0/1\">Thomas Fr&#xe9;our</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchere_H/0/1/0/all/0/1\">Harold Mouch&#xe8;re</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning of Generative Image Priors for MRI Reconstruction. (arXiv:2202.04175v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.04175","description":"<p>Multi-institutional efforts can facilitate training of deep MRI\nreconstruction models, albeit privacy risks arise during cross-site sharing of\nimaging data. Federated learning (FL) has recently been introduced to address\nprivacy concerns by enabling distributed training without transfer of imaging\ndata. Existing FL methods for MRI reconstruction employ conditional models to\nmap from undersampled to fully-sampled acquisitions via explicit knowledge of\nthe imaging operator. Since conditional models generalize poorly across\ndifferent acceleration rates or sampling densities, imaging operators must be\nfixed between training and testing, and they are typically matched across\nsites. To improve generalization and flexibility in multi-institutional\ncollaborations, here we introduce a novel method for MRI reconstruction based\non Federated learning of Generative IMage Priors (FedGIMP). FedGIMP leverages a\ntwo-stage approach: cross-site learning of a generative MRI prior, and\nsubject-specific injection of the imaging operator. The global MRI prior is\nlearned via an unconditional adversarial model that synthesizes high-quality MR\nimages based on latent variables. Specificity in the prior is preserved via a\nmapper subnetwork that produces site-specific latents. During inference, the\nprior is combined with subject-specific imaging operators to enable\nreconstruction, and further adapted to individual test samples by minimizing\ndata-consistency loss. Comprehensive experiments on multi-institutional\ndatasets clearly demonstrate enhanced generalization performance of FedGIMP\nagainst site-specific and federated methods based on conditional models, as\nwell as traditional reconstruction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Elmas_G/0/1/0/all/0/1\">Gokberk Elmas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1\">Salman UH Dar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korkmaz_Y/0/1/0/all/0/1\">Yilmaz Korkmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ceyani_E/0/1/0/all/0/1\">Emir Ceyani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Susam_B/0/1/0/all/0/1\">Burak Susam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1\">Muzaffer &#xd6;zbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection. (arXiv:2202.06934v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06934","description":"<p>Detection of small objects and objects far away in the scene is a major\nchallenge in surveillance applications. Such objects are represented by small\nnumber of pixels in the image and lack sufficient details, making them\ndifficult to detect using conventional detectors. In this work, an open-source\nframework called Slicing Aided Hyper Inference (SAHI) is proposed that provides\na generic slicing aided inference and fine-tuning pipeline for small object\ndetection. The proposed technique is generic in the sense that it can be\napplied on top of any available object detector without any fine-tuning.\nExperimental evaluations, using object detection baselines on the Visdrone and\nxView aerial object detection datasets show that the proposed inference method\ncan increase object detection AP by 6.8%, 5.1% and 5.3% for FCOS, VFNet and\nTOOD detectors, respectively. Moreover, the detection accuracy can be further\nincreased with a slicing aided fine-tuning, resulting in a cumulative increase\nof 12.7%, 13.4% and 14.5% AP in the same order. Proposed technique has been\nintegrated with Detectron2, MMDetection and YOLOv5 models and it is publicly\navailable at https://github.com/obss/sahi.git .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyon_F/0/1/0/all/0/1\">Fatih Cagatay Akyon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altinuc_S/0/1/0/all/0/1\">Sinan Onur Altinuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1\">Alptekin Temizel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminability-enforcing loss to improve representation learning. (arXiv:2202.07073v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07073","description":"<p>During the training process, deep neural networks implicitly learn to\nrepresent the input data samples through a hierarchy of features, where the\nsize of the hierarchy is determined by the number of layers. In this paper, we\nfocus on enforcing the discriminative power of the high-level representations,\nthat are typically learned by the deeper layers (closer to the output). To this\nend, we introduce a new loss term inspired by the Gini impurity, which is aimed\nat minimizing the entropy (increasing the discriminative power) of individual\nhigh-level features with respect to the class labels. Although our Gini loss\ninduces highly-discriminative features, it does not ensure that the\ndistribution of the high-level features matches the distribution of the\nclasses. As such, we introduce another loss term to minimize the\nKullback-Leibler divergence between the two distributions. We conduct\nexperiments on two image classification data sets (CIFAR-100 and Caltech 101),\nconsidering multiple neural architectures ranging from convolutional networks\n(ResNet-17, ResNet-18, ResNet-50) to transformers (CvT). Our empirical results\nshow that integrating our novel loss terms into the training objective\nconsistently outperforms the models trained with cross-entropy alone, without\nincreasing the inference time at all.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Croitoru_F/0/1/0/all/0/1\">Florinel-Alin Croitoru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigore_D/0/1/0/all/0/1\">Diana-Nicoleta Grigore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer. (arXiv:2202.07305v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07305","description":"<p>Image narrative generation is a task to create a story from an image with a\nsubjective viewpoint. Given the importance of the subjective feelings of\nwriters, readers, and characters in storytelling, an image narrative generation\nmethod should consider human emotion. In this study, we propose a novel method\nof image narrative generation called ViNTER (Visual Narrative Transformer with\nEmotion arc Representation), which takes \"emotion arc\" as input to capture a\nsequence of emotional changes. Since emotion arcs represent the trajectory of\nemotional change, it is expected that we can include detailed information about\nthe emotional changes in the story to the model. We present experimental\nresults of both automatic and manual evaluations on the Image Narrative dataset\nand demonstrate the effectiveness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1\">Kohei Uehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_Y/0/1/0/all/0/1\">Yusuke Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1\">Yusuke Mukuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realistic Blur Synthesis for Learning Image Deblurring. (arXiv:2202.08771v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08771","description":"<p>Training learning-based deblurring methods demands a tremendous amount of\nblurred and sharp image pairs. Unfortunately, existing synthetic datasets are\nnot realistic enough, and deblurring models trained on them cannot handle real\nblurred images effectively. While real datasets have recently been proposed,\nthey provide limited diversity of scenes and camera settings, and capturing\nreal datasets for diverse settings is still challenging. This paper analyzes\nvarious factors that introduce differences between real and synthetic blurred\nimages, and presents a novel blur synthesis pipeline to synthesize more\nrealistic blur. We also present RSBlur, a novel dataset with real blurred\nimages and the corresponding sharp image sequences to enable detailed analysis\non the differences between real and synthetic blur. With our blur synthesis\npipeline and RSBlur dataset, we reveal the effects of different factors in blur\nsynthesis. We also show that our synthesis method can improve the deblurring\nperformance on real blurred images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rim_J/0/1/0/all/0/1\">Jaesung Rim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geonung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jungeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. (arXiv:2203.03605v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03605","description":"<p>We present DINO (\\textbf{D}ETR with \\textbf{I}mproved de\\textbf{N}oising\nanch\\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in\nthis paper. DINO improves over previous DETR-like models in performance and\nefficiency by using a contrastive way for denoising training, a mixed query\nselection method for anchor initialization, and a look forward twice scheme for\nbox prediction. DINO achieves $48.3$AP in $12$ epochs and $51.0$AP in $36$\nepochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a\nsignificant improvement of $\\textbf{+4.9}$\\textbf{AP} and\n$\\textbf{+2.4}$\\textbf{AP}, respectively, compared to DN-DETR, the previous\nbest DETR-like model. DINO scales well in both model size and data size.\nWithout bells and whistles, after pre-training on the Objects365 dataset with a\nSwinL backbone, DINO obtains the best results on both COCO \\texttt{val2017}\n($\\textbf{63.2}$\\textbf{AP}) and \\texttt{test-dev}\n(\\textbf{$\\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO\nsignificantly reduces its model size and pre-training data size while achieving\nbetter results. Our code will be available at\n\\url{https://github.com/IDEACVR/DINO}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1\">Lionel M. Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Heung-Yeung Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible Amortized Variational Inference in qBOLD MRI. (arXiv:2203.05845v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.05845","description":"<p>Streamlined qBOLD acquisitions enable experimentally straightforward\nobservations of brain oxygen metabolism. $R_2^\\prime$ maps are easily inferred;\nhowever, the Oxygen extraction fraction (OEF) and deoxygenated blood volume\n(DBV) are more ambiguously determined from the data. As such, existing\ninference methods tend to yield very noisy and underestimated OEF maps, while\noverestimating DBV.\n</p>\n<p>This work describes a novel probabilistic machine learning approach that can\ninfer plausible distributions of OEF and DBV. Initially, we create a model that\nproduces informative voxelwise prior distribution based on synthetic training\ndata. Contrary to prior work, we model the joint distribution of OEF and DBV\nthrough a scaled multivariate logit-Normal distribution, which enables the\nvalues to be constrained within a plausible range. The prior distribution model\nis used to train an efficient amortized variational Bayesian inference model.\nThis model learns to infer OEF and DBV by predicting real image data, with few\ntraining data required, using the signal equations as a forward model.\n</p>\n<p>We demonstrate that our approach enables the inference of smooth OEF and DBV\nmaps, with a physiologically plausible distribution that can be adapted through\nspecification of an informative prior distribution. Other benefits include\nmodel comparison (via the evidence lower bound) and uncertainty quantification\nfor identifying image artefacts. Results are demonstrated on a small study\ncomparing subjects undergoing hyperventilation and at rest. We illustrate that\nthe proposed approach allows measurement of gray matter differences in OEF and\nDBV and enables voxelwise comparison between conditions, where we observe\nsignificant increases in OEF and $R_2^\\prime$ during hyperventilation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Simpson_I/0/1/0/all/0/1\">Ivor J.A. Simpson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McManamon_A/0/1/0/all/0/1\">Ashley McManamon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orzsik_B/0/1/0/all/0/1\">Bal&#xe1;zs &#xd6;rzsik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stone_A/0/1/0/all/0/1\">Alan J. Stone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blockley_N/0/1/0/all/0/1\">Nicholas P. Blockley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asllani_I/0/1/0/all/0/1\">Iris Asllani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colasanti_A/0/1/0/all/0/1\">Alessandro Colasanti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cercignani_M/0/1/0/all/0/1\">Mara Cercignani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaRTS: Causality-driven Robot Tool Segmentation from Vision and Kinematics Data. (arXiv:2203.09475v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2203.09475","description":"<p>Vision-based segmentation of the robotic tool during robot-assisted surgery\nenables downstream applications, such as augmented reality feedback, while\nallowing for inaccuracies in robot kinematics. With the introduction of deep\nlearning, many methods were presented to solve instrument segmentation directly\nand solely from images. While these approaches made remarkable progress on\nbenchmark datasets, fundamental challenges pertaining to their robustness\nremain. We present CaRTS, a causality-driven robot tool segmentation algorithm,\nthat is designed based on a complementary causal model of the robot tool\nsegmentation task. Rather than directly inferring segmentation masks from\nobserved images, CaRTS iteratively aligns tool models with image observations\nby updating the initially incorrect robot kinematic parameters through forward\nkinematics and differentiable rendering to optimize image feature similarity\nend-to-end. We benchmark CaRTS with competing techniques on both synthetic as\nwell as real data from the dVRK, generated in precisely controlled scenarios to\nallow for counterfactual synthesis. On training-domain test data, CaRTS\nachieves a Dice score of 93.4 that is preserved well (Dice score of 91.8) when\ntested on counterfactual altered test data, exhibiting low brightness, smoke,\nblood, and altered background patterns. This compares favorably to Dice scores\nof 95.0 and 62.8, respectively, of a purely image-based method trained and\ntested on the same data. Future work will involve accelerating CaRTS to achieve\nvideo framerate and estimating the impact occlusion has in practice. Despite\nthese limitations, our results are promising: In addition to achieving high\nsegmentation accuracy, CaRTS provides estimates of the true robot kinematics,\nwhich may benefit applications such as force estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jintan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazanzides_P/0/1/0/all/0/1\">Peter Kazanzides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Ying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Few-Shot Learning via Implanting and Compressing. (arXiv:2203.10297v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10297","description":"<p>This work focuses on tackling the challenging but realistic visual task of\nIncremental Few-Shot Learning (IFSL), which requires a model to continually\nlearn novel classes from only a few examples while not forgetting the base\nclasses on which it was pre-trained. Our study reveals that the challenges of\nIFSL lie in both inter-class separation and novel-class representation. Dur to\nintra-class variation, a novel class may implicitly leverage the knowledge from\nmultiple base classes to construct its feature representation. Hence, simply\nreusing the pre-trained embedding space could lead to a scattered feature\ndistribution and result in category confusion. To address such issues, we\npropose a two-step learning strategy referred to as \\textbf{Im}planting and\n\\textbf{Co}mpressing (\\textbf{IMCO}), which optimizes both feature space\npartition and novel class reconstruction in a systematic manner. Specifically,\nin the \\textbf{Implanting} step, we propose to mimic the data distribution of\nnovel classes with the assistance of data-abundant base set, so that a model\ncould learn semantically-rich features that are beneficial for discriminating\nbetween the base and other unseen classes. In the \\textbf{Compressing} step, we\nadapt the feature extractor to precisely represent each novel class for\nenhancing intra-class compactness, together with a regularized parameter\nupdating rule for preventing aggressive model updating. Finally, we demonstrate\nthat IMCO outperforms competing baselines with a significant margin, both in\nimage classification task and more challenging object detection task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haiyue Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xijia Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zilong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Cheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vadakkepat_P/0/1/0/all/0/1\">Prahlad Vadakkepat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tong Heng Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAN: a Segmentation-free Document Attention Network for Handwritten Document Recognition. (arXiv:2203.12273v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12273","description":"<p>Unconstrained handwritten text recognition is a challenging computer vision\ntask. It is traditionally handled by a two-step approach combining line\nsegmentation followed by text line recognition. For the first time, we propose\nan end-to-end segmentation-free architecture for the task of handwritten\ndocument recognition: the Document Attention Network. In addition to the text\nrecognition, the model is trained to label text parts using begin and end tags\nin an XML-like fashion. This model is made up of an FCN encoder for feature\nextraction and a stack of transformer decoder layers for a recurrent\ntoken-by-token prediction process. It takes whole text documents as input and\nsequentially outputs characters, as well as logical layout tokens. Contrary to\nthe existing segmentation-based approaches, the model is trained without using\nany segmentation label. We achieve competitive results on the READ 2016 dataset\nat page level, as well as double-page level with a CER of 3.53% and 3.69%,\nrespectively. We also provide results for the RIMES 2009 dataset at page level,\nreaching 4.54% of CER.\n</p>\n<p>We provide all source code and pre-trained model weights at\nhttps://github.com/FactoDeepLearning/DAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coquenet_D/0/1/0/all/0/1\">Denis Coquenet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatelain_C/0/1/0/all/0/1\">Cl&#xe9;ment Chatelain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paquet_T/0/1/0/all/0/1\">Thierry Paquet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-augmented histopathologic review using image analysis to optimize DNA yield and tumor purity from FFPE slides. (arXiv:2203.13948v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13948","description":"<p>To achieve minimum DNA input and tumor purity requirements for\nnext-generation sequencing (NGS), pathologists visually estimate\nmacrodissection and slide count decisions. Misestimation may cause tissue waste\nand increased laboratory costs. We developed an AI-augmented smart pathology\nreview system (SmartPath) to empower pathologists with quantitative metrics for\ndetermining tissue extraction parameters. Using digitized H&amp;E-stained FFPE\nslides as inputs, SmartPath segments tumors, extracts cell-based features, and\nsuggests macrodissection areas. To predict DNA yield per slide, the extracted\nfeatures are correlated with known DNA yields. Then, a pathologist-defined\ntarget yield divided by the predicted DNA yield/slide gives the number of\nslides to scrape. Following model development, an internal validation trial was\nconducted within the Tempus Labs molecular sequencing laboratory. We evaluated\nour system on 501 clinical colorectal cancer slides, where half received\nSmartPath-augmented review and half traditional pathologist review. The\nSmartPath cohort had 25% more DNA yields within a desired target range of\n100-2000ng. The SmartPath system recommended fewer slides to scrape for large\ntissue sections, saving tissue in these cases. Conversely, SmartPath\nrecommended more slides to scrape for samples with scant tissue sections,\nhelping prevent costly re-extraction due to insufficient extraction yield. A\nstatistical analysis was performed to measure the impact of covariates on the\nresults, offering insights on how to improve future applications of SmartPath.\nOverall, the study demonstrated that AI-augmented histopathologic review using\nSmartPath could decrease tissue waste, sequencing time, and laboratory costs by\noptimizing DNA yields and tumor purity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1\">Boles&#x142;aw L. Osinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+BenTaieb_A/0/1/0/all/0/1\">A&#xef;cha BenTaieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_I/0/1/0/all/0/1\">Irvin Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">Ryan D. Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Rohan P. Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westley_A/0/1/0/all/0/1\">Andrew Westley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlson_M/0/1/0/all/0/1\">Michael Carlson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willis_C/0/1/0/all/0/1\">Caleb Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schleicher_L/0/1/0/all/0/1\">Luke Schleicher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahon_B/0/1/0/all/0/1\">Brett M. Mahon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumpe_M/0/1/0/all/0/1\">Martin C. Stumpe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimT: Handling Open-set Noise for Domain Adaptive Semantic Segmentation. (arXiv:2203.15202v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15202","description":"<p>This paper studies a practical domain adaptive (DA) semantic segmentation\nproblem where only pseudo-labeled target data is accessible through a black-box\nmodel. Due to the domain gap and label shift between two domains,\npseudo-labeled target data contains mixed closed-set and open-set label noises.\nIn this paper, we propose a simplex noise transition matrix (SimT) to model the\nmixed noise distributions in DA semantic segmentation and formulate the problem\nas estimation of SimT. By exploiting computational geometry analysis and\nproperties of segmentation, we design three complementary regularizers, i.e.\nvolume regularization, anchor guidance, convex guarantee, to approximate the\ntrue SimT. Specifically, volume regularization minimizes the volume of simplex\nformed by rows of the non-square SimT, which ensures outputs of segmentation\nmodel to fit into the ground truth label distribution. To compensate for the\nlack of open-set knowledge, anchor guidance and convex guarantee are devised to\nfacilitate the modeling of open-set noise distribution and enhance the\ndiscriminative feature learning among closed-set and open-set classes. The\nestimated SimT is further utilized to correct noise issues in pseudo labels and\npromote the generalization ability of segmentation model on target domain data.\nExtensive experimental results demonstrate that the proposed SimT can be\nflexibly plugged into existing DA methods to boost the performance. The source\ncode is available at https://github.com/CityU-AIM-Group/SimT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoqing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pay Attention to Hidden States for Video Deblurring: Ping-Pong Recurrent Neural Networks and Selective Non-Local Attention. (arXiv:2203.16063v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16063","description":"<p>Video deblurring models exploit information in the neighboring frames to\nremove blur caused by the motion of the camera and the objects. Recurrent\nNeural Networks~(RNNs) are often adopted to model the temporal dependency\nbetween frames via hidden states. When motion blur is strong, however, hidden\nstates are hard to deliver proper information due to the displacement between\ndifferent frames. While there have been attempts to update the hidden states,\nit is difficult to handle misaligned features beyond the receptive field of\nsimple modules. Thus, we propose 2 modules to supplement the RNN architecture\nfor video deblurring. First, we design Ping-Pong RNN~(PPRNN) that acts on\nupdating the hidden states by referring to the features from the current and\nthe previous time steps alternately. PPRNN gathers relevant information from\nthe both features in an iterative and balanced manner by utilizing its\nrecurrent architecture. Second, we use a Selective Non-Local Attention~(SNLA)\nmodule to additionally refine the hidden state by aligning it with the\npositional information from the input frame feature. The attention score is\nscaled by the relevance to the input feature to focus on the necessary\ninformation. By paying attention to hidden states with both modules, which have\nstrong synergy, our PAHS framework improves the representation powers of RNN\nstructures and achieves state-of-the-art deblurring performance on standard\nbenchmarks and real-world videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">JoonKyu Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nah_S/0/1/0/all/0/1\">Seungjun Nah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CADG: A Model Based on Cross Attention for Domain Generalization. (arXiv:2203.17067v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.17067","description":"<p>In Domain Generalization (DG) tasks, models are trained by using only\ntraining data from the source domains to achieve generalization on an unseen\ntarget domain, this will suffer from the distribution shift problem. So it's\nimportant to learn a classifier to focus on the common representation which can\nbe used to classify on multi-domains, so that this classifier can achieve a\nhigh performance on an unseen target domain as well. With the success of cross\nattention in various cross-modal tasks, we find that cross attention is a\npowerful mechanism to align the features come from different distributions. So\nwe design a model named CADG (cross attention for domain generalization),\nwherein cross attention plays a important role, to address distribution shift\nproblem. Such design makes the classifier can be adopted on multi-domains, so\nthe classifier will generalize well on an unseen domain. Experiments show that\nour proposed method achieves state-of-the-art performance on a variety of\ndomain generalization benchmarks compared with other single model and can even\nachieve a better performance than some ensemble-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1\">Cheng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Donglin Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do learned representations respect causal relationships?. (arXiv:2204.00762v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00762","description":"<p>Data often has many semantic attributes that are causally associated with\neach other. But do attribute-specific learned representations of data also\nrespect the same causal relations? We answer this question in three steps.\nFirst, we introduce NCINet, an approach for observational causal discovery from\nhigh-dimensional data. It is trained purely on synthetically generated\nrepresentations and can be applied to real representations, and is specifically\ndesigned to mitigate the domain gap between the two. Second, we apply NCINet to\nidentify the causal relations between image representations of different pairs\nof attributes with known and unknown causal relations between the labels. For\nthis purpose, we consider image representations learned for predicting\nattributes on the 3D Shapes, CelebA, and the CASIA-WebFace datasets, which we\nannotate with multiple multi-class attributes. Third, we analyze the effect on\nthe underlying causal relation between learned representations induced by\nvarious design choices in representation learning. Our experiments indicate\nthat (1) NCINet significantly outperforms existing observational causal\ndiscovery approaches for estimating the causal relation between pairs of random\nsamples, both in the presence and absence of an unobserved confounder, (2)\nunder controlled scenarios, learned representations can indeed satisfy the\nunderlying causal relations between their respective labels, and (3) the causal\nrelations are positively correlated with the predictive capability of the\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1\">Vishnu Naresh Boddeti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAD: A Large-scale Dataset towards Airport Detection in Synthetic Aperture Radar Images. (arXiv:2204.00790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00790","description":"<p>Airports have an important role in both military and civilian domains. The\nsynthetic aperture radar (SAR) based airport detection has received increasing\nattention in recent years. However, due to the high cost of SAR imaging and\nannotation process, there is no publicly available SAR dataset for airport\ndetection. As a result, deep learning methods have not been fully used in\nairport detection tasks. To provide a benchmark for airport detection research\nin SAR images, this paper introduces a large-scale SAR Airport Dataset (SAD).\nIn order to adequately reflect the demands of real world applications, it\ncontains 624 SAR images from Sentinel 1B and covers 104 airfield instances with\ndifferent scales, orientations and shapes. The experiments of multiple deep\nlearning approach on this dataset proves its effectiveness. It developing\nstate-of-the-art airport area detection algorithms or other relevant tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daochang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongsheng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rotated Object Detection via Scale-invariant Mahalanobis Distance in Aerial Images. (arXiv:2204.00840v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00840","description":"<p>Rotated object detection in aerial images is a meaningful yet challenging\ntask as objects are densely arranged and have arbitrary orientations. The\neight-parameter (coordinates of box vectors) methods in rotated object\ndetection usually use ln-norm losses (L1 loss, L2 loss, and smooth L1 loss) as\nloss functions. As ln-norm losses are mainly based on non-scale-invariant\nMinkowski distance, using ln-norm losses will lead to inconsistency with the\ndetection metric rotational Intersection-over-Union (IoU) and training\ninstability. To address the problems, we use Mahalanobis distance to calculate\nloss between the predicted and the target box vertices' vectors, proposing a\nnew loss function called Mahalanobis Distance Loss (MDL) for eight-parameter\nrotated object detection. As Mahalanobis distance is scale-invariant, MDL is\nmore consistent with detection metric and more stable during training than\nln-norm losses. To alleviate the problem of boundary discontinuity like all\nother eight-parameter methods, we further take the minimum loss value to make\nMDL continuous at boundary cases. We achieve state-of-art performance on\nDOTA-v1.0 with the proposed method MDL. Furthermore, compared to the experiment\nthat uses smooth L1 loss, we find that MDL performs better in rotated object\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Siyang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruijie Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploration of Active Learning for Affective Digital Phenotyping. (arXiv:2204.01915v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.01915","description":"<p>Some of the most severe bottlenecks preventing widespread development of\nmachine learning models for human behavior include a dearth of labeled training\ndata and difficulty of acquiring high quality labels. Active learning is a\nparadigm for using algorithms to computationally select a useful subset of data\npoints to label using metrics for model uncertainty and data similarity. We\nexplore active learning for naturalistic computer vision emotion data, a\nparticularly heterogeneous and complex data space due to inherently subjective\nlabels. Using frames collected from gameplay acquired from a therapeutic\nsmartphone game for children with autism, we run a simulation of active\nlearning using gameplay prompts as metadata to aid in the active learning\nprocess. We find that active learning using information generated during\ngameplay slightly outperforms random selection of the same number of labeled\nframes. We next investigate a method to conduct active learning with subjective\ndata, such as in affective computing, and where multiple crowdsourced labels\ncan be acquired for each image. Using the Child Affective Facial Expression\n(CAFE) dataset, we simulate an active learning process for crowdsourcing many\nlabels and find that prioritizing frames using the entropy of the crowdsourced\nlabel distribution results in lower categorical cross-entropy loss compared to\nrandom frame selection. Collectively, these results demonstrate pilot\nevaluations of two novel active learning approaches for subjective affective\ndata collected in noisy settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1\">Peter Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutlu_C/0/1/0/all/0/1\">Cezmi Mutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_A/0/1/0/all/0/1\">Aaron Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1\">Cathy Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunlap_K/0/1/0/all/0/1\">Kaitlyn Dunlap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kent_J/0/1/0/all/0/1\">Jack Kent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Husic_A/0/1/0/all/0/1\">Arman Husic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockham_N/0/1/0/all/0/1\">Nate Stockham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrisman_B/0/1/0/all/0/1\">Brianna Chrisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paskov_K/0/1/0/all/0/1\">Kelley Paskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jae-Yoon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1\">Dennis P. Wall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSDoodle: Searching for App Screens via Interactive Sketching. (arXiv:2204.01968v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01968","description":"<p>Keyword-based mobile screen search does not account for screen content and\nfails to operate as a universal tool for all levels of users. Visual searching\n(e.g., image, sketch) is structured and easy to adopt. Current visual search\napproaches count on a complete screen and are therefore slow and tedious.\nPSDoodle employs a deep neural network to recognize partial screen element\ndrawings instantly on a digital drawing interface and shows results in\nreal-time. PSDoodle is the first tool that utilizes partial sketches and\nsearches for screens in an interactive iterative way. PSDoodle supports\ndifferent drawing styles and retrieves search results that are relevant to the\nuser's sketch query. A short video demonstration is available online at:\nhttps://youtu.be/3cVLHFm5pY4\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohian_S/0/1/0/all/0/1\">Soumik Mohian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csallner_C/0/1/0/all/0/1\">Christoph Csallner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Visual Geo-localization for Large-Scale Applications. (arXiv:2204.02287v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02287","description":"<p>Visual Geo-localization (VG) is the task of estimating the position where a\ngiven photo was taken by comparing it with a large database of images of known\nlocations. To investigate how existing techniques would perform on a real-world\ncity-wide VG application, we build San Francisco eXtra Large, a new dataset\ncovering a whole city and providing a wide range of challenging cases, with a\nsize 30x bigger than the previous largest dataset for visual geo-localization.\nWe find that current methods fail to scale to such large datasets, therefore we\ndesign a new highly scalable training technique, called CosPlace, which casts\nthe training as a classification problem avoiding the expensive mining needed\nby the commonly used contrastive learning. We achieve state-of-the-art\nperformance on a wide range of datasets and find that CosPlace is robust to\nheavy domain changes. Moreover, we show that, compared to the previous\nstate-of-the-art, CosPlace requires roughly 80% less GPU memory at train time,\nand it achieves better results with 8x smaller descriptors, paving the way for\ncity-wide real-world visual geo-localization. Dataset, code and trained models\nare available for research purposes at https://github.com/gmberton/CosPlace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berton_G/0/1/0/all/0/1\">Gabriele Berton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masone_C/0/1/0/all/0/1\">Carlo Masone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SqueezeNeRF: Further factorized FastNeRF for memory-efficient inference. (arXiv:2204.02585v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02585","description":"<p>Neural Radiance Fields (NeRF) has emerged as the state-of-the-art method for\nnovel view generation of complex scenes, but is very slow during inference.\nRecently, there have been multiple works on speeding up NeRF inference, but the\nstate of the art methods for real-time NeRF inference rely on caching the\nneural network output, which occupies several giga-bytes of disk space that\nlimits their real-world applicability. As caching the neural network of\noriginal NeRF network is not feasible, Garbin et al. proposed \"FastNeRF\" which\nfactorizes the problem into 2 sub-networks - one which depends only on the 3D\ncoordinate of a sample point and one which depends only on the 2D camera\nviewing direction. Although this factorization enables them to reduce the cache\nsize and perform inference at over 200 frames per second, the memory overhead\nis still substantial. In this work, we propose SqueezeNeRF, which is more than\n60 times memory-efficient than the sparse cache of FastNeRF and is still able\nto render at more than 190 frames per second on a high spec GPU during\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadhwani_K/0/1/0/all/0/1\">Krishna Wadhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kojima_T/0/1/0/all/0/1\">Tamaki Kojima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification. (arXiv:2204.02611v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02611","description":"<p>Recently, large-scale synthetic datasets are shown to be very useful for\ngeneralizable person re-identification. However, synthesized persons in\nexisting datasets are mostly cartoon-like and in random dress collocation,\nwhich limits their performance. To address this, in this work, an automatic\napproach is proposed to directly clone the whole outfits from real-world person\nimages to virtual 3D characters, such that any virtual person thus created will\nappear very similar to its real-world counterpart. Specifically, based on UV\ntexture mapping, two cloning methods are designed, namely registered clothes\nmapping and homogeneous cloth expansion. Given clothes keypoints detected on\nperson images and labeled on regular UV maps with clear clothes structures,\nregistered mapping applies perspective homography to warp real-world clothes to\nthe counterparts on the UV map. As for invisible clothes parts and irregular UV\nmaps, homogeneous expansion segments a homogeneous area on clothes as a\nrealistic cloth pattern or cell, and expand the cell to fill the UV map.\nFurthermore, a similarity-diversity expansion strategy is proposed, by\nclustering person images, sampling images per cluster, and cloning outfits for\n3D character generation. This way, virtual persons can be scaled up densely in\nvisual similarity to challenge model learning, and diversely in population to\nenrich sample distribution. Finally, by rendering the cloned characters in\nUnity3D scenes, a more realistic virtual dataset called ClonedPerson is\ncreated, with 5,621 identities and 887,766 images. Experimental results show\nthat the model trained on ClonedPerson has a better generalization performance,\nsuperior to that trained on other popular real-world and synthetic person\nre-identification datasets. The ClonedPerson project is available at\nhttps://github.com/Yanan-Wang-cs/ClonedPerson.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xuezhi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards An End-to-End Framework for Flow-Guided Video Inpainting. (arXiv:2204.02663v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.02663","description":"<p>Optical flow, which captures motion information across frames, is exploited\nin recent video inpainting methods through propagating pixels along its\ntrajectories. However, the hand-crafted flow-based processes in these methods\nare applied separately to form the whole inpainting pipeline. Thus, these\nmethods are less efficient and rely heavily on the intermediate results from\nearlier stages. In this paper, we propose an End-to-End framework for\nFlow-Guided Video Inpainting (E$^2$FGVI) through elaborately designed three\ntrainable modules, namely, flow completion, feature propagation, and content\nhallucination modules. The three modules correspond with the three stages of\nprevious flow-based methods but can be jointly optimized, leading to a more\nefficient and effective inpainting process. Experimental results demonstrate\nthat the proposed method outperforms state-of-the-art methods both\nqualitatively and quantitatively and shows promising efficiency. The code is\navailable at https://github.com/MCG-NKU/E2FGVI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_C/0/1/0/all/0/1\">Cheng-Ze Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">Jianhua Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_C/0/1/0/all/0/1\">Chun-Le Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expression-preserving face frontalization improves visually assisted speech processing. (arXiv:2204.02810v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02810","description":"<p>Face frontalization consists of synthesizing a frontally-viewed face from an\narbitrarily-viewed one. The main contribution of this paper is a frontalization\nmethodology that preserves non-rigid facial deformations in order to boost the\nperformance of visually assisted speech communication. The method alternates\nbetween the estimation of (i)~the rigid transformation (scale, rotation, and\ntranslation) and (ii)~the non-rigid deformation between an arbitrarily-viewed\nface and a face model. The method has two important merits: it can deal with\nnon-Gaussian errors in the data and it incorporates a dynamical face\ndeformation model. For that purpose, we use the generalized Student\nt-distribution in combination with a linear dynamic system in order to account\nfor both rigid head motions and time-varying facial deformations caused by\nspeech production. We propose to use the zero-mean normalized cross-correlation\n(ZNCC) score to evaluate the ability of the method to preserve facial\nexpressions. The method is thoroughly evaluated and compared with several state\nof the art methods, either based on traditional geometric models or on deep\nlearning. Moreover, we show that the method, when incorporated into deep\nlearning pipelines, namely lip reading and speech enhancement, improves word\nrecognition and speech intelligibilty scores by a considerable margin.\nSupplemental material is accessible at\nhttps://team.inria.fr/robotlearn/research/facefrontalization-benchmark/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhiqi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1\">Mostafa Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1\">Radu Horaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}