{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DPUV3INT8: A Compiler View to programmable FPGA Inference Engines. (arXiv:2110.04327v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04327","description":"<p>We have a FPGA design, we make it fast, efficient, and tested for a few\nimportant examples. Now we must infer a general solution to deploy in the data\ncenter. Here, we describe the FPGA DPUV3INT8 design and our compiler effort.\nThe hand-tuned SW-HW solution for Resnet50\\_v1 has (close to) 2 times better\nimages per second (throughput) than our best FPGA implementation; the compiler\ngeneralizes the hand written techniques achieving about 1.5 times better\nperformance for the same example, the compiler generalizes the optimizations to\na model zoo of networks, and it achieves 80+\\% HW efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DAlberto_P/0/1/0/all/0/1\">Paolo D&#x27;Alberto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiangsha Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollavaram_M/0/1/0/all/0/1\">Manasa Bollavaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shaoxia Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering. (arXiv:2110.04330v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04330","description":"<p>Current Open-Domain Question Answering (ODQA) model paradigm often contains a\nretrieving module and a reading module. Given an input question, the reading\nmodule predicts the answer from the relevant passages which are retrieved by\nthe retriever. The recent proposed Fusion-in-Decoder (FiD), which is built on\ntop of the pretrained generative model T5, achieves the state-of-the-art\nperformance in the reading module. Although being effective, it remains\nconstrained by inefficient attention on all retrieved passages which contain a\nlot of noise. In this work, we propose a novel method KG-FiD, which filters\nnoisy passages by leveraging the structural relationship among the retrieved\npassages with a knowledge graph. We initiate the passage node embedding from\nthe FiD encoder and then use graph neural network (GNN) to update the\nrepresentation for reranking. To improve the efficiency, we build the GNN on\ntop of the intermediate layer output of the FiD encoder and only pass a few top\nreranked passages into the higher layers of encoder and decoder for answer\ngeneration. We also apply the proposed GNN based reranking method to enhance\nthe passage retrieval results in the retrieving module. Extensive experiments\non common ODQA benchmark datasets (Natural Question and TriviaQA) demonstrate\nthat KG-FiD can improve vanilla FiD by up to 1.5% on answer exact match score\nand achieve comparable performance with FiD with only 40% of computation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Donghan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Describe Solutions for Bug Reports Based on Developer Discussions. (arXiv:2110.04353v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04353","description":"<p>When a software bug is reported, developers engage in a discussion to\ncollaboratively resolve it. While the solution is likely formulated within the\ndiscussion, it is often buried in a large amount of text, making it difficult\nto comprehend, which delays its implementation. To expedite bug resolution, we\npropose generating a concise natural language description of the solution by\nsynthesizing relevant content within the discussion, which encompasses both\nnatural language and source code. Furthermore, to support generating an\ninformative description during an ongoing discussion, we propose a secondary\ntask of determining when sufficient context about the solution emerges in\nreal-time. We construct a dataset for these tasks with a novel technique for\nobtaining noisy supervision from repository changes linked to bug reports. We\nestablish baselines for generating solution descriptions, and develop a\nclassifier which makes a prediction following each new utterance on whether or\nnot the necessary context for performing generation is available. Through\nautomated and human evaluation, we find these tasks to form an ideal testbed\nfor complex reasoning in long, bimodal dialogue context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panthaplackel_S/0/1/0/all/0/1\">Sheena Panthaplackel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gligoric_M/0/1/0/all/0/1\">Milos Gligoric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond J. Mooney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Unified View of Parameter-Efficient Transfer Learning. (arXiv:2110.04366v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04366","description":"<p>Fine-tuning large pre-trained language models on downstream tasks has become\nthe de-facto learning paradigm in NLP. However, conventional approaches\nfine-tune all the parameters of the pre-trained model, which becomes\nprohibitive as the model size and the number of tasks grow. Recent work has\nproposed a variety of parameter-efficient transfer learning methods that only\nfine-tune a small number of (extra) parameters to attain strong performance.\nWhile effective, the critical ingredients for success and the connections among\nthe various methods are poorly understood. In this paper, we break down the\ndesign of state-of-the-art parameter-efficient transfer learning methods and\npresent a unified framework that establishes connections between them.\nSpecifically, we re-frame them as modifications to specific hidden states in\npre-trained models, and define a set of design dimensions along which different\nmethods vary, such as the function to compute the modification and the position\nto apply the modification. Through comprehensive empirical studies across\nmachine translation, text summarization, language understanding, and text\nclassification benchmarks, we utilize the unified view to identify important\ndesign choices in previous methods. Furthermore, our unified framework enables\nthe transfer of design elements across different approaches, and as a result we\nare able to instantiate new parameter-efficient fine-tuning methods that tune\nless parameters than previous methods while being more effective, achieving\ncomparable results to fine-tuning all parameters on all four tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Few More Examples May Be Worth Billions of Parameters. (arXiv:2110.04374v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04374","description":"<p>We investigate the dynamics of increasing the number of model parameters\nversus the number of labeled examples across a wide variety of tasks. Our\nexploration reveals that while scaling parameters consistently yields\nperformance improvements, the contribution of additional examples highly\ndepends on the task's format. Specifically, in open question answering tasks,\nenlarging the training set does not improve performance. In contrast,\nclassification, extractive question answering, and multiple choice tasks\nbenefit so much from additional examples that collecting a few hundred examples\nis often \"worth\" billions of parameters. We hypothesize that unlike open\nquestion answering, which involves recalling specific information, solving\nstrategies for tasks with a more restricted output space transfer across\nexamples, and can therefore be learned with small amounts of labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirstain_Y/0/1/0/all/0/1\">Yuval Kirstain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Summarization Systems across Gender, Age, and Race. (arXiv:2110.04384v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04384","description":"<p>Summarization systems are ultimately evaluated by human annotators and\nraters. Usually, annotators and raters do not reflect the demographics of end\nusers, but are recruited through student populations or crowdsourcing platforms\nwith skewed demographics. For two different evaluation scenarios -- evaluation\nagainst gold summaries and system output ratings -- we show that summary\nevaluation is sensitive to protected attributes. This can severely bias system\ndevelopment and evaluation, leading us to build models that cater for some\ngroups rather than others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jorgensen_A/0/1/0/all/0/1\">Anna J&#xf8;rgensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Eval4NLP Shared Task on Explainable Quality Estimation: Overview and Results. (arXiv:2110.04392v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04392","description":"<p>In this paper, we introduce the Eval4NLP-2021shared task on explainable\nquality estimation. Given a source-translation pair, this shared task requires\nnot only to provide a sentence-level score indicating the overall quality of\nthe translation, but also to explain this score by identifying the words that\nnegatively impact translation quality. We present the data, annotation\nguidelines and evaluation setup of the shared task, describe the six\nparticipating systems, and analyze the results. To the best of our knowledge,\nthis is the first shared task on explainable NLP evaluation metrics. Datasets\nand results are available at https://github.com/eval4nlp/SharedTask2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lertvittayakumjorn_P/0/1/0/all/0/1\">Piyawat Lertvittayakumjorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors. (arXiv:2110.04399v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04399","description":"<p>Evaluation metrics are a key ingredient for progress of text generation\nsystems. In recent years, several BERT-based evaluation metrics have been\nproposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much\nbetter with human assessment of text generation quality than BLEU or ROUGE,\ninvented two decades ago. However, little is known what these metrics, which\nare based on black-box language model representations, actually capture (it is\ntypically assumed they model semantic similarity). In this work, we \\wei{use a\nsimple regression based global explainability technique to} disentangle metric\nscores along linguistic factors, including semantics, syntax, morphology, and\nlexical overlap. We show that the different metrics capture all aspects to some\ndegree, but that they are all substantially sensitive to lexical overlap, just\nlike BLEU and ROUGE. This exposes limitations of these novelly proposed\nmetrics, which we also highlight in an adversarial test scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaster_M/0/1/0/all/0/1\">Marvin Kaster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HydraSum -- Disentangling Stylistic Features in Text Summarization using Multi-Decoder Models. (arXiv:2110.04400v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04400","description":"<p>Existing abstractive summarization models lack explicit control mechanisms\nthat would allow users to influence the stylistic features of the model\noutputs. This results in generating generic summaries that do not cater to the\nusers needs or preferences. To address this issue we introduce HydraSum, a new\nsummarization architecture that extends the single decoder framework of current\nmodels, e.g. BART, to a mixture-of-experts version consisting of multiple\ndecoders. Our proposed model encourages each expert, i.e. decoder, to learn and\ngenerate stylistically-distinct summaries along dimensions such as\nabstractiveness, length, specificity, and others. At each time step, HydraSum\nemploys a gating mechanism that decides the contribution of each individual\ndecoder to the next token's output probability distribution. Through\nexperiments on three summarization datasets (CNN, Newsroom, XSum), we\ndemonstrate that this gating mechanism automatically learns to assign\ncontrasting summary styles to different HydraSum decoders under the standard\ntraining objective without the need for additional supervision. We further show\nthat a guided version of the training process can explicitly govern which\nsummary style is partitioned between decoders, e.g. high abstractiveness vs.\nlow abstractiveness or high specificity vs. low specificity, and also increase\nthe stylistic-difference between individual decoders. Finally, our experiments\ndemonstrate that our decoder framework is highly flexible: during inference, we\ncan sample from individual decoders or mixtures of different subsets of the\ndecoders to yield a diverse set of summaries and enforce single- and\nmulti-style control over summary generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content. (arXiv:2110.04406v1 [cs.HC])","link":"http://arxiv.org/abs/2110.04406","description":"<p>Natural language descriptions sometimes accompany visualizations to better\ncommunicate and contextualize their insights, and to improve their\naccessibility for readers with disabilities. However, it is difficult to\nevaluate the usefulness of these descriptions, and how effectively they improve\naccess to meaningful information, because we have little understanding of the\nsemantic content they convey, and how different readers receive this content.\nIn response, we introduce a conceptual model for the semantic content conveyed\nby natural language descriptions of visualizations. Developed through a\ngrounded theory analysis of 2,147 sentences, our model spans four levels of\nsemantic content: enumerating visualization construction properties (e.g.,\nmarks and encodings); reporting statistical concepts and relations (e.g.,\nextrema and correlations); identifying perceptual and cognitive phenomena\n(e.g., complex trends and patterns); and elucidating domain-specific insights\n(e.g., social and political context). To demonstrate how our model can be\napplied to evaluate the effectiveness of visualization descriptions, we conduct\na mixed-methods evaluation with 30 blind and 90 sighted readers, and find that\nthese reader groups differ significantly on which semantic content they rank as\nmost useful. Together, our model and findings suggest that access to meaningful\ninformation is strongly reader-specific, and that research in automatic\nvisualization captioning should orient toward descriptions that more richly\ncommunicate overall trends and statistics, sensitive to reader preferences. Our\nwork further opens a space of research on natural language as a data interface\ncoequal with visualization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lundgard_A/0/1/0/all/0/1\">Alan Lundgard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1\">Arvind Satyanarayan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Community Sensitive Norm Violations in Online Conversations. (arXiv:2110.04419v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04419","description":"<p>Online platforms and communities establish their own norms that govern what\nbehavior is acceptable within the community. Substantial effort in NLP has\nfocused on identifying unacceptable behaviors and, recently, on forecasting\nthem before they occur. However, these efforts have largely focused on toxicity\nas the sole form of community norm violation. Such focus has overlooked the\nmuch larger set of rules that moderators enforce. Here, we introduce a new\ndataset focusing on a more complete spectrum of community norms and their\nviolations in the local conversational and global community contexts. We\nintroduce a series of models that use this data to develop context- and\ncommunity-sensitive norm violation detection, showing that these changes give\nhigh performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Young Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendelsohn_J/0/1/0/all/0/1\">Julia Mendelsohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radhakrishnan_K/0/1/0/all/0/1\">Karthik Radhakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_K/0/1/0/all/0/1\">Kinjal Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanakagiri_T/0/1/0/all/0/1\">Tushar Kanakagiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning. (arXiv:2110.04429v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04429","description":"<p>Distantly supervised named entity recognition (DS-NER) efficiently reduces\nlabor costs but meanwhile intrinsically suffers from the label noise due to the\nstrong assumption of distant supervision. Typically, the wrongly labeled\ninstances comprise numbers of incomplete and inaccurate annotation noise, while\nmost prior denoising works are only concerned with one kind of noise and fail\nto fully explore useful information in the whole training set. To address this\nissue, we propose a robust learning paradigm named Self-Collaborative Denoising\nLearning (SCDL), which jointly trains two teacher-student networks in a\nmutually-beneficial manner to iteratively perform noisy label refinery. Each\nnetwork is designed to exploit reliable labels via self denoising, and two\nnetworks communicate with each other to explore unreliable annotations by\ncollaborative denoising. Extensive experimental results on five real-world\ndatasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mengge Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-stage Visual Cues Enhancement Network for Referring Image Segmentation. (arXiv:2110.04435v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04435","description":"<p>Referring Image Segmentation (RIS) aims at segmenting the target object from\nan image referred by one given natural language expression. The diverse and\nflexible expressions as well as complex visual contents in the images raise the\nRIS model with higher demands for investigating fine-grained matching behaviors\nbetween words in expressions and objects presented in images. However, such\nmatching behaviors are hard to be learned and captured when the visual cues of\nreferents (i.e. referred objects) are insufficient, as the referents with weak\nvisual cues tend to be easily confused by cluttered background at boundary or\neven overwhelmed by salient objects in the image. And the insufficient visual\ncues issue can not be handled by the cross-modal fusion mechanisms as done in\nprevious work. In this paper, we tackle this problem from a novel perspective\nof enhancing the visual information for the referents by devising a Two-stage\nVisual cues enhancement Network (TV-Net), where a novel Retrieval and\nEnrichment Scheme (RES) and an Adaptive Multi-resolution feature Fusion (AMF)\nmodule are proposed. Through the two-stage enhancement, our proposed TV-Net\nenjoys better performances in learning fine-grained matching behaviors between\nthe natural language expression and image, especially when the visual\ninformation of the referent is inadequate, thus produces better segmentation\nresults. Extensive experiments are conducted to validate the effectiveness of\nthe proposed method on the RIS task, with our proposed TV-Net surpassing the\nstate-of-the-art approaches on four benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zequn Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weixin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaolin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language for Human-Robot Collaboration: Problems Beyond Language Grounding. (arXiv:2110.04441v1 [cs.AI])","link":"http://arxiv.org/abs/2110.04441","description":"<p>To enable robots to instruct humans in collaborations, we identify several\naspects of language processing that are not commonly studied in this context.\nThese include location, planning, and generation. We suggest evaluations for\neach task, offer baselines for simple methods, and close by discussing\nchallenges and opportunities in studying language for collaboration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pate_S/0/1/0/all/0/1\">Seth Pate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Love_M/0/1/0/all/0/1\">Maxwell Love</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguri_S/0/1/0/all/0/1\">Siddarth Ganguri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1\">Lawson L.S. Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging recent advances in Pre-Trained Language Models forEye-Tracking Prediction. (arXiv:2110.04475v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04475","description":"<p>Cognitively inspired Natural Language Pro-cessing uses human-derived\nbehavioral datalike eye-tracking data, which reflect the seman-tic\nrepresentations of language in the humanbrain to augment the neural nets to\nsolve arange of tasks spanning syntax and semanticswith the aim of teaching\nmachines about lan-guage processing mechanisms. In this paper,we use the ZuCo\n1.0 and ZuCo 2.0 dataset con-taining the eye-gaze features to explore\ndiffer-ent linguistic models to directly predict thesegaze features for each\nword with respect to itssentence. We tried different neural networkmodels with\nthe words as inputs to predict thetargets. And after lots of experimentation\nandfeature engineering finally devised a novel ar-chitecture consisting of\nRoBERTa Token Clas-sifier with a dense layer on top for languagemodeling and a\nstand-alone model consistingof dense layers followed by a transformer layerfor\nthe extra features we engineered. Finally,we took the mean of the outputs of\nboth thesemodels to make the final predictions. We eval-uated the models using\nmean absolute error(MAE) and the R2 score for each target.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhavan_V/0/1/0/all/0/1\">Varun Madhavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawate_A/0/1/0/all/0/1\">Aditya Girish Pawate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Shraman Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1\">Abhranil Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Active Summarization. (arXiv:2110.04480v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04480","description":"<p>Bayesian Active Learning has had significant impact to various NLP problems,\nbut nevertheless it's application to text summarization has been explored very\nlittle. We introduce Bayesian Active Summarization (BAS), as a method of\ncombining active learning methods with state-of-the-art summarization models.\nOur findings suggest that BAS achieves better and more robust performance,\ncompared to random selection, particularly for small and very small data\nannotation budgets. Using BAS we showcase it is possible to leverage large\nsummarization models to effectively solve real-world problems with very limited\nannotated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gidiotis_A/0/1/0/all/0/1\">Alexios Gidiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Lifelong Learning of Multilingual Text-To-Speech Synthesis. (arXiv:2110.04482v1 [eess.AS])","link":"http://arxiv.org/abs/2110.04482","description":"<p>This work presents a lifelong learning approach to train a multilingual\nText-To-Speech (TTS) system, where each language was seen as an individual task\nand was learned sequentially and continually. It does not require pooled data\nfrom all languages altogether, and thus alleviates the storage and computation\nburden. One of the challenges of lifelong learning methods is \"catastrophic\nforgetting\": in TTS scenario it means that model performance quickly degrades\non previous languages when adapted to a new language. We approach this problem\nvia a data-replay-based lifelong learning method. We formulate the replay\nprocess as a supervised learning problem, and propose a simple yet effective\ndual-sampler framework to tackle the heavily language-imbalanced training\nsamples. Through objective and subjective evaluations, we show that this\nsupervised learning formulation outperforms other gradient-based and\nregularization-based lifelong learning methods, achieving 43% Mel-Cepstral\nDistortion reduction compared to a fine-tuning baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Mu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_S/0/1/0/all/0/1\">Shaojin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wav2vec-S: Semi-Supervised Pre-Training for Speech Recognition. (arXiv:2110.04484v1 [eess.AS])","link":"http://arxiv.org/abs/2110.04484","description":"<p>Self-supervised pre-training has dramatically improved the performance of\nautomatic speech recognition (ASR). However, most existing self-supervised\npre-training approaches are task-agnostic, i.e., could be applied to various\ndownstream tasks. And there is a gap between the task-agnostic pre-training and\nthe task-specific downstream fine-tuning, which may degrade the downstream\nperformance. In this work, we propose a novel pre-training paradigm called\nwav2vec-S, where we use task-specific semi-supervised pre-training to bridge\nthis gap. Specifically, the semi-supervised pre-training is conducted on the\nbasis of self-supervised pre-training such as wav2vec 2.0. Experiments on ASR\nshow that compared to wav2vec 2.0, wav2vec-S only requires marginal increment\nof pre-training time but could significantly improve ASR performance on\nin-domain, cross-domain and cross-lingual datasets. The average relative WER\nreductions are 26.3% and 6.3% for 1h and 10h fine-tuning, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Han Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1\">Ying Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_G/0/1/0/all/0/1\">Gaofeng Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyuan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yonghong Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS With Accurate Phoneme Duration Control. (arXiv:2110.04486v1 [cs.SD])","link":"http://arxiv.org/abs/2110.04486","description":"<p>Sequence expansion between encoder and decoder is a critical challenge in\nsequence-to-sequence TTS. Attention-based methods achieve great naturalness but\nsuffer from unstable issues like missing and repeating phonemes, not to mention\naccurate duration control. Duration-informed methods, on the contrary, seem to\neasily adjust phoneme duration but show obvious degradation in speech\nnaturalness. This paper proposes PAMA-TTS to address the problem. It takes the\nadvantage of both flexible attention and explicit duration models. Based on the\nmonotonic attention mechanism, PAMA-TTS also leverages token duration and\nrelative position of a frame, especially countdown information, i.e. in how\nmany future frames the present phoneme will end. They help the attention to\nmove forward along the token sequence in a soft but reliable control.\nExperimental results prove that PAMA-TTS achieves the highest naturalness,\nwhile has on-par or even better duration controllability than the\nduration-informed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yunchao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_J/0/1/0/all/0/1\">Jian Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Isotropy Analysis in the Multilingual BERT Embedding Space. (arXiv:2110.04504v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04504","description":"<p>Several studies have explored various advantages of multilingual pre-trained\nmodels (e.g., multilingual BERT) in capturing shared linguistic knowledge.\nHowever, their limitations have not been paid enough attention. In this paper,\nwe investigate the representation degeneration problem in multilingual\ncontextual word representations (CWRs) of BERT and show that the embedding\nspaces of the selected languages suffer from anisotropy problem. Our\nexperimental results demonstrate that, similarly to their monolingual\ncounterparts, increasing the isotropy of multilingual embedding space can\nsignificantly improve its representation power and performance. Our analysis\nindicates that although the degenerated directions vary in different languages,\nthey encode similar linguistic knowledge, suggesting a shared linguistic space\namong languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajaee_S/0/1/0/all/0/1\">Sara Rajaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending Multi-Text Sentence Fusion Resources via Pyramid Annotations. (arXiv:2110.04517v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04517","description":"<p>NLP models that compare or consolidate information across multiple documents\noften struggle when challenged with recognizing substantial information\nredundancies across the texts. For example, in multi-document summarization it\nis crucial to identify salient information across texts and then generate a\nnon-redundant summary, while facing repeated and usually differently-phrased\nsalient content. To facilitate researching such challenges, the sentence-level\ntask of \\textit{sentence fusion} was proposed, yet previous datasets for this\ntask were very limited in their size and scope. In this paper, we revisit and\nsubstantially extend previous dataset creation efforts. With careful\nmodifications, relabeling and employing complementing data sources, we were\nable to triple the size of a notable earlier dataset. Moreover, we show that\nour extended version uses more representative texts for multi-document tasks\nand provides a larger and more diverse training set, which substantially\nimproves model training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weiss_D/0/1/0/all/0/1\">Daniela Brook Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roit_P/0/1/0/all/0/1\">Paul Roit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_O/0/1/0/all/0/1\">Ori Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DMRST: A Joint Framework for Document-Level Multilingual RST Discourse Segmentation and Parsing. (arXiv:2110.04518v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04518","description":"<p>Text discourse parsing weighs importantly in understanding information flow\nand argumentative structure in natural language, making it beneficial for\ndownstream tasks. While previous work significantly improves the performance of\nRST discourse parsing, they are not readily applicable to practical use cases:\n(1) EDU segmentation is not integrated into most existing tree parsing\nframeworks, thus it is not straightforward to apply such models on newly-coming\ndata. (2) Most parsers cannot be used in multilingual scenarios, because they\nare developed only in English. (3) Parsers trained from single-domain treebanks\ndo not generalize well on out-of-domain inputs. In this work, we propose a\ndocument-level multilingual RST discourse parsing framework, which conducts EDU\nsegmentation and discourse tree parsing jointly. Moreover, we propose a\ncross-translation augmentation strategy to enable the framework to support\nmultilingual parsing and improve its domain generality. Experimental results\nshow that our model achieves state-of-the-art performance on document-level\nmultilingual RST parsing in all sub-tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Ke Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rumor Detection on Twitter with Claim-Guided Hierarchical Graph Attention Networks. (arXiv:2110.04522v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04522","description":"<p>Rumors are rampant in the era of social media. Conversation structures\nprovide valuable clues to differentiate between real and fake claims. However,\nexisting rumor detection methods are either limited to the strict relation of\nuser responses or oversimplify the conversation structure. In this study, to\nsubstantially reinforces the interaction of user opinions while alleviating the\nnegative impact imposed by irrelevant posts, we first represent the\nconversation thread as an undirected interaction graph. We then present a\nClaim-guided Hierarchical Graph Attention Network for rumor classification,\nwhich enhances the representation learning for responsive posts considering the\nentire social contexts and attends over the posts that can semantically infer\nthe target claim. Extensive experiments on three Twitter datasets demonstrate\nthat our rumor detection method achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for detecting rumors\nat early stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Mingfei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Disentangled Arguments with Prompts: A Simple Event Extraction Framework that Works. (arXiv:2110.04525v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04525","description":"<p>Event Extraction bridges the gap between text and event signals. Based on the\nassumption of trigger-argument dependency, existing approaches have achieved\nstate-of-the-art performance with expert-designed templates or complicated\ndecoding constraints. In this paper, for the first time we introduce the\nprompt-based learning strategy to the domain of Event Extraction, which\nempowers the automatic exploitation of label semantics on both input and output\nsides. To validate the effectiveness of the proposed generative method, we\nconduct extensive experiments with 11 diverse baselines. Empirical results show\nthat, in terms of F1 score on Argument Extraction, our simple architecture is\nstronger than any other generative counterpart and even competitive with\nalgorithms that require template engineering. Regarding the measure of recall,\nit sets new overall records for both Argument and Trigger Extractions. We\nhereby recommend this framework to the community, with the code publicly\navailable at https://git.io/GDAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_J/0/1/0/all/0/1\">Jinghui Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xutan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haotian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multi-Party Dialogue Discourse Parsing via Domain Integration. (arXiv:2110.04526v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04526","description":"<p>While multi-party conversations are often less structured than monologues and\ndocuments, they are implicitly organized by semantic level correlations across\nthe interactive turns, and dialogue discourse analysis can be applied to\npredict the dependency structure and relations between the elementary discourse\nunits, and provide feature-rich structural information for downstream tasks.\nHowever, the existing corpora with dialogue discourse annotation are collected\nfrom specific domains with limited sample sizes, rendering the performance of\ndata-driven approaches poor on incoming dialogues without any domain\nadaptation. In this paper, we first introduce a Transformer-based parser, and\nassess its cross-domain performance. We next adopt three methods to gain domain\nintegration from both data and language modeling perspectives to improve the\ngeneralization capability. Empirical results show that the neural parser can\nbenefit from our proposed methods, and performs better on cross-domain dialogue\nsamples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design. (arXiv:2110.04541v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04541","description":"<p>Pretraining Neural Language Models (NLMs) over a large corpus involves\nchunking the text into training examples, which are contiguous text segments of\nsizes processable by the neural architecture. We highlight a bias introduced by\nthis common practice: we prove that the pretrained NLM can model much stronger\ndependencies between text segments that appeared in the same training example,\nthan it can between text segments that appeared in different training examples.\nThis intuitive result has a twofold role. First, it formalizes the motivation\nbehind a broad line of recent successful NLM training heuristics, proposed for\nthe pretraining and fine-tuning stages, which do not necessarily appear related\nat first glance. Second, our result clearly indicates further improvements to\nbe made in NLM pretraining for the benefit of Natural Language Understanding\ntasks. As an example, we propose \"kNN-Pretraining\": we show that including\nsemantically related non-neighboring sentences in the same pretraining example\nyields improved sentence representations and open domain question answering\nabilities. This theoretically motivated degree of freedom for \"pretraining\nexample design\" indicates new training schemes for self-improving\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1\">Noam Wies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannai_D/0/1/0/all/0/1\">Daniel Jannai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navon_D/0/1/0/all/0/1\">Dan Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Adapter: Better Vision-Language Models with Feature Adapters. (arXiv:2110.04544v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04544","description":"<p>Large-scale contrastive vision-language pre-training has shown significant\nprogress in visual representation learning. Unlike traditional visual systems\ntrained by a fixed set of discrete labels, a new paradigm was introduced in\n\\cite{radford2021learning} to directly learn to align images with raw texts in\nan open-vocabulary setting. On downstream tasks, a carefully chosen text prompt\nis employed to make zero-shot predictions.~To avoid non-trivial prompt\nengineering, context optimization \\cite{zhou2021coop} has been proposed to\nlearn continuous vectors as task-specific prompts with few-shot training\nexamples.~In this paper, we show that there is an alternative path to achieve\nbetter vision-language models other than prompt tuning.~While prompt tuning is\nfor the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with\nfeature adapters on either visual or language branch. Specifically,\nCLIP-Adapter adopts an additional bottleneck layer to learn new features and\nperforms residual-style feature blending with the original pre-trained\nfeatures.~As a consequence, CLIP-Adapter is able to outperform context\noptimization while maintains a simple design. Experiments and extensive\nablation studies on various visual classification tasks demonstrate the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Rongyao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploration of Self-Supervised Pretrained Representations for End-to-End Speech Recognition. (arXiv:2110.04590v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04590","description":"<p>Self-supervised pretraining on speech data has achieved a lot of progress.\nHigh-fidelity representation of the speech signal is learned from a lot of\nuntranscribed data and shows promising performance. Recently, there are several\nworks focusing on evaluating the quality of self-supervised pretrained\nrepresentations on various tasks without domain restriction, e.g. SUPERB.\nHowever, such evaluations do not provide a comprehensive comparison among many\nASR benchmark corpora. In this paper, we focus on the general applications of\npretrained speech representations, on advanced end-to-end automatic speech\nrecognition (E2E-ASR) models. We select several pretrained speech\nrepresentations and present the experimental results on various open-source and\npublicly available corpora for E2E-ASR. Without any modification of the\nback-end model architectures or training strategy, some of the experiments with\npretrained representations, e.g., WSJ, WSJ0-2mix with HuBERT, reach or\noutperform current state-of-the-art (SOTA) recognition performance. Moreover,\nwe further explore more scenarios for whether the pretraining representations\nare effective, such as the cross-language or overlapped speech. The scripts,\nconfiguratons and the trained models have been released in ESPnet to let the\ncommunity reproduce our experiments and improve them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maekaku_T/0/1/0/all/0/1\">Takashi Maekaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yen-Ju Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1\">Aswin Shanmugam Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianzi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Automatic Speech Recognition Trained on Small Disordered Speech Datasets. (arXiv:2110.04612v1 [eess.AS])","link":"http://arxiv.org/abs/2110.04612","description":"<p>This study investigates the performance of personalized automatic speech\nrecognition (ASR) for recognizing disordered speech using small amounts of\nper-speaker adaptation data. We trained personalized models for 195 individuals\nwith different types and severities of speech impairment with training sets\nranging in size from &lt;1 minute to 18-20 minutes of speech data. Word error rate\n(WER) thresholds were selected to determine Success Percentage (the percentage\nof personalized models reaching the target WER) in different application\nscenarios. For the home automation scenario, 79% of speakers reached the target\nWER with 18-20 minutes of speech; but even with only 3-4 minutes of speech, 63%\nof speakers reached the target WER. Further evaluation found similar\nimprovement on test sets with conversational and out-of-domain, unprompted\nphrases. Our results demonstrate that with only a few minutes of recordings,\nindividuals with disordered speech could benefit from personalized ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tobin_J/0/1/0/all/0/1\">Jimmy Tobin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathetic Response Generation through Graph-based Multi-hop Reasoning on Emotional Causality. (arXiv:2110.04614v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04614","description":"<p>Empathetic response generation aims to comprehend the user emotion and then\nrespond to it appropriately. Most existing works merely focus on what the\nemotion is and ignore how the emotion is evoked, thus weakening the capacity of\nthe model to understand the emotional experience of the user for generating\nempathetic responses. To tackle this problem, we consider the emotional\ncausality, namely, what feelings the user expresses (i.e., emotion) and why the\nuser has such feelings (i.e., cause). Then, we propose a novel graph-based\nmodel with multi-hop reasoning to model the emotional causality of the\nempathetic conversation. Finally, we demonstrate the effectiveness of our model\non EMPATHETICDIALOGUES in comparison with several competitive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiashuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LI_W/0/1/0/all/0/1\">Wenjie LI</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Peiqin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_F/0/1/0/all/0/1\">Feiteng Mu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Rationale Extraction for Deep QA models. (arXiv:2110.04620v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04620","description":"<p>As neural-network-based QA models become deeper and more complex, there is a\ndemand for robust frameworks which can access a model's rationale for its\nprediction. Current techniques that provide insights on a model's working are\neither dependent on adversarial datasets or are proposing models with explicit\nexplanation generation components. These techniques are time-consuming and\nchallenging to extend to existing models and new datasets. In this work, we use\n`Integrated Gradients' to extract rationale for existing state-of-the-art\nmodels in the task of Reading Comprehension based Question Answering (RCQA). On\ndetailed analysis and comparison with collected human rationales, we find that\nthough ~40-80% words of extracted rationale coincide with the human rationale\n(precision), only 6-19% of human rationale is present in the extracted\nrationale (recall).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramnath_S/0/1/0/all/0/1\">Sahana Ramnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nema_P/0/1/0/all/0/1\">Preksha Nema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahni_D/0/1/0/all/0/1\">Deep Sahni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Relation between Syntactic Divergence and Zero-Shot Performance. (arXiv:2110.04644v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04644","description":"<p>We explore the link between the extent to which syntactic relations are\npreserved in translation and the ease of correctly constructing a parse tree in\na zero-shot setting. While previous work suggests such a relation, it tends to\nfocus on the macro level and not on the level of individual edges-a gap we aim\nto address. As a test case, we take the transfer of Universal Dependencies (UD)\nparsing from English to a diverse set of languages and conduct two sets of\nexperiments. In one, we analyze zero-shot performance based on the extent to\nwhich English source edges are preserved in translation. In another, we apply\nthree linguistically motivated transformations to UD, creating more\ncross-lingually stable versions of it, and assess their zero-shot parsability.\nIn order to compare parsing performance across different schemes, we perform\nextrinsic evaluation on the downstream task of cross-lingual relation\nextraction (RE) using a subset of a popular English RE benchmark translated to\nRussian and Korean. In both sets of experiments, our results suggest a strong\nrelation between cross-lingual stability and zero-shot parsing performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arviv_O/0/1/0/all/0/1\">Ofir Arviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1\">Dmitry Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karidi_T/0/1/0/all/0/1\">Taelin Karidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Follow Language Instructions with Compositional Policies. (arXiv:2110.04647v1 [cs.LG])","link":"http://arxiv.org/abs/2110.04647","description":"<p>We propose a framework that learns to execute natural language instructions\nin an environment consisting of goal-reaching tasks that share components of\ntheir task descriptions. Our approach leverages the compositionality of both\nvalue functions and language, with the aim of reducing the sample complexity of\nlearning novel tasks. First, we train a reinforcement learning agent to learn\nvalue functions that can be subsequently composed through a Boolean algebra to\nsolve novel tasks. Second, we fine-tune a seq2seq model pretrained on web-scale\ncorpora to map language to logical expressions that specify the required value\nfunction compositions. Evaluating our agent in the BabyAI domain, we observe a\ndecrease of 86% in the number of training steps needed to learn a second task\nafter mastering a single task. Results from ablation studies further indicate\nthat it is the combination of compositional value functions and language\nrepresentations that allows the agent to quickly generalize to new tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_V/0/1/0/all/0/1\">Vanya Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasse_G/0/1/0/all/0/1\">Geraud Nangue Tasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalan_N/0/1/0/all/0/1\">Nakul Gopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Steven James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gombolay_M/0/1/0/all/0/1\">Matthew Gombolay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosman_B/0/1/0/all/0/1\">Benjamin Rosman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Sequence to Sequence Learning for Compositional Generalization. (arXiv:2110.04655v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04655","description":"<p>There is mounting evidence that existing neural network models, in particular\nthe very popular sequence-to-sequence architecture, struggle with compositional\ngeneralization, i.e., the ability to systematically generalize to unseen\ncompositions of seen components. In this paper we demonstrate that one of the\nreasons hindering compositional generalization relates to the representations\nbeing entangled. We propose an extension to sequence-to-sequence models which\nallows us to learn disentangled representations by adaptively re-encoding (at\neach time step) the source input. Specifically, we condition the source\nrepresentations on the newly decoded target context which makes it easier for\nthe encoder to exploit specialized information for each prediction rather than\ncapturing all source information in a single forward pass. Experimental results\non semantic parsing and machine translation empirically show that our proposal\nyields more disentangled representations and better generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Audio Captions Be Evaluated with Image Caption Metrics?. (arXiv:2110.04684v1 [cs.SD])","link":"http://arxiv.org/abs/2110.04684","description":"<p>Automated audio captioning aims at generating textual descriptions for an\naudio clip. To evaluate the quality of generated audio captions, previous works\ndirectly adopt image captioning metrics like SPICE and CIDEr, without\njustifying their suitability in this new domain, which may mislead the\ndevelopment of advanced models. This problem is still unstudied due to the lack\nof human judgment datasets on caption quality. Therefore, we firstly construct\ntwo evaluation benchmarks, AudioCaps-Eval and Clotho-Eval. They are established\nwith pairwise comparison instead of absolute rating to achieve better\ninter-annotator agreement. Current metrics are found in poor correlation with\nhuman annotations on these datasets. To overcome their limitations, we propose\na metric named FENSE, where we combine the strength of Sentence-BERT in\ncapturing similarity, and a novel Error Detector to penalize erroneous\nsentences for robustness. On the newly established benchmarks, FENSE\noutperforms current metrics by 14-25% accuracy. Code, data and web demo\navailable at: https://github.com/blmoistawinde/fense\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zelin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiling Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuenan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zeyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Channel End-to-End Neural Diarization with Distributed Microphones. (arXiv:2110.04694v1 [eess.AS])","link":"http://arxiv.org/abs/2110.04694","description":"<p>Recent progress on end-to-end neural diarization (EEND) has enabled\noverlap-aware speaker diarization with a single neural network. This paper\nproposes to enhance EEND by using multi-channel signals from distributed\nmicrophones. We replace Transformer encoders in EEND with two types of encoders\nthat process a multi-channel input: spatio-temporal and co-attention encoders.\nBoth are independent of the number and geometry of microphones and suitable for\ndistributed microphone settings. We also propose a model adaptation method\nusing only single-channel recordings. With simulated and real-recorded\ndatasets, we demonstrated that the proposed method outperformed conventional\nEEND when a multi-channel input was given while maintaining comparable\nperformance with a single-channel input. We also showed that the proposed\nmethod performed well even when spatial information is inoperative given\nmulti-channel inputs, such as in hybrid meetings in which the utterances of\nmultiple remote participants are played back from the same loudspeaker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Horiguchi_S/0/1/0/all/0/1\">Shota Horiguchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takashima_Y/0/1/0/all/0/1\">Yuki Takashima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_P/0/1/0/all/0/1\">Paola Garcia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawaguchi_Y/0/1/0/all/0/1\">Yohei Kawaguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention in Natural Language Processing. (arXiv:1902.02181v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1902.02181","description":"<p>Attention is an increasingly popular mechanism used in a wide range of neural\narchitectures. The mechanism itself has been realized in a variety of formats.\nHowever, because of the fast-paced advances in this domain, a systematic\noverview of attention is still missing. In this article, we define a unified\nmodel for attention architectures in natural language processing, with a focus\non those designed to work with vector representations of the textual data. We\npropose a taxonomy of attention models according to four dimensions: the\nrepresentation of the input, the compatibility function, the distribution\nfunction, and the multiplicity of the input and/or output. We present the\nexamples of how prior information can be exploited in attention models and\ndiscuss ongoing research efforts and open challenges in the area, providing the\nfirst extensive categorization of the vast body of literature in this exciting\ndomain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippi_M/0/1/0/all/0/1\">Marco Lippi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Relational Image Captioning via Multi-task Triple-Stream Networks. (arXiv:2010.03855v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.03855","description":"<p>We introduce dense relational captioning, a novel image captioning task which\naims to generate multiple captions with respect to relational information\nbetween objects in a visual scene. Relational captioning provides explicit\ndescriptions for each relationship between object combinations. This framework\nis advantageous in both diversity and amount of information, leading to a\ncomprehensive image understanding based on relationships, e.g., relational\nproposal generation. For relational understanding between objects, the\npart-of-speech (POS; i.e., subject-object-predicate categories) can be a\nvaluable prior information to guide the causal sequence of words in a caption.\nWe enforce our framework to learn not only to generate captions but also to\nunderstand the POS of each word. To this end, we propose the multi-task\ntriple-stream network (MTTSNet) which consists of three recurrent units\nresponsible for each POS which is trained by jointly predicting the correct\ncaptions and POS for each word. In addition, we found that the performance of\nMTTSNet can be improved by modulating the object embeddings with an explicit\nrelational module. We demonstrate that our proposed model can generate more\ndiverse and richer captions, via extensive experimental analysis on large scale\ndatasets and several metrics. Then, we present applications of our framework to\nholistic image captioning, scene graph generation, and retrieval tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinsoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLQE-PE: A Multilingual Quality Estimation and Post-Editing Dataset. (arXiv:2010.04480v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.04480","description":"<p>We present MLQE-PE, a new dataset for Machine Translation (MT) Quality\nEstimation (QE) and Automatic Post-Editing (APE). The dataset contains eleven\nlanguage pairs, with human labels for up to 10,000 translations per language\npair in the following formats: sentence-level direct assessments and\npost-editing effort, and word-level good/bad labels. It also contains the\npost-edited sentences, as well as titles of the articles where the sentences\nwere extracted from, and the neural MT models used to translate the text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1\">Erick Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerva_C/0/1/0/all/0/1\">Chrysoula Zerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blain_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Blain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopatina_N/0/1/0/all/0/1\">Nina Lopatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemMT: A Semantic-based Testing Approach for Machine Translation Systems. (arXiv:2012.01815v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2012.01815","description":"<p>Machine translation has wide applications in daily life. In mission-critical\napplications such as translating official documents, incorrect translation can\nhave unpleasant or sometimes catastrophic consequences. This motivates recent\nresearch on testing methodologies for machine translation systems. Existing\nmethodologies mostly rely on metamorphic relations designed at the textual\nlevel (e.g., Levenshtein distance) or syntactic level (e.g., the distance\nbetween grammar structures) to determine the correctness of translation\nresults. However, these metamorphic relations do not consider whether the\noriginal and translated sentences have the same meaning (i.e., Semantic\nsimilarity). Therefore, in this paper, we propose SemMT, an automatic testing\napproach for machine translation systems based on semantic similarity checking.\nSemMT applies round-trip translation and measures the semantic similarity\nbetween the original and translated sentences. Our insight is that the\nsemantics expressed by the logic and numeric constraint in sentences can be\ncaptured using regular expressions (or deterministic finite automata) where\nefficient equivalence/similarity checking algorithms are available. Leveraging\nthe insight, we propose three semantic similarity metrics and implement them in\nSemMT. The experiment result reveals SemMT can achieve higher effectiveness\ncompared with state-of-the-art works, achieving an increase of 21% and 23% on\naccuracy and F-Score, respectively. We also explore potential improvements that\ncan be achieved when proper combinations of metrics are adopted. Finally, we\ndiscuss a solution to locate the suspicious trip in round-trip translation,\nwhich may shed lights on further exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jialun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meiziniu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yeting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1\">Ming Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_S/0/1/0/all/0/1\">Shing-Chi Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximating How Single Head Attention Learns. (arXiv:2103.07601v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07601","description":"<p>Why do models often attend to salient words, and how does this evolve\nthroughout training? We approximate model training as a two stage process:\nearly on in training when the attention weights are uniform, the model learns\nto translate individual input word `i` to `o` if they co-occur frequently.\nLater, the model learns to attend to `i` while the correct output is $o$\nbecause it knows `i` translates to `o`. To formalize, we define a model\nproperty, Knowledge to Translate Individual Words (KTIW) (e.g. knowing that `i`\ntranslates to `o`), and claim that it drives the learning of the attention.\nThis claim is supported by the fact that before the attention mechanism is\nlearned, KTIW can be learned from word co-occurrence statistics, but not the\nother way around. Particularly, we can construct a training distribution that\nmakes KTIW hard to learn, the learning of the attention fails, and the model\ncannot even learn the simple task of copying the input words to the output. Our\napproximation explains why models sometimes attend to salient words, and\ninspires a toy example where a multi-head attention model can overcome the\nabove hard training distribution by improving learning dynamics rather than\nexpressiveness. We end by discussing the limitation of our approximation\nframework and suggest future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snell_C/0/1/0/all/0/1\">Charlie Snell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LSTM Based Sentiment Analysis for Cryptocurrency Prediction. (arXiv:2103.14804v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.14804","description":"<p>Recent studies in big data analytics and natural language processing develop\nautomatic techniques in analyzing sentiment in the social media information. In\naddition, the growing user base of social media and the high volume of posts\nalso provide valuable sentiment information to predict the price fluctuation of\nthe cryptocurrency. This research is directed to predicting the volatile price\nmovement of cryptocurrency by analyzing the sentiment in social media and\nfinding the correlation between them. While previous work has been developed to\nanalyze sentiment in English social media posts, we propose a method to\nidentify the sentiment of the Chinese social media posts from the most popular\nChinese social media platform Sina-Weibo. We develop the pipeline to capture\nWeibo posts, describe the creation of the crypto-specific sentiment dictionary,\nand propose a long short-term memory (LSTM) based recurrent neural network\nalong with the historical cryptocurrency price movement to predict the price\ntrend for future time frames. The conducted experiments demonstrate the\nproposed approach outperforms the state of the art auto regressive based model\nby 18.5% in precision and 15.4% in recall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surbiryala_J/0/1/0/all/0/1\">Jayachander Surbiryala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_V/0/1/0/all/0/1\">Vasileios Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. (arXiv:2104.06378v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06378","description":"<p>The problem of answering questions using knowledge from pre-trained language\nmodels (LMs) and knowledge graphs (KGs) presents two challenges: given a QA\ncontext (question and answer choice), methods need to (i) identify relevant\nknowledge from large KGs, and (ii) perform joint reasoning over the QA context\nand KG. In this work, we propose a new model, QA-GNN, which addresses the above\nchallenges through two key innovations: (i) relevance scoring, where we use LMs\nto estimate the importance of KG nodes relative to the given QA context, and\n(ii) joint reasoning, where we connect the QA context and KG to form a joint\ngraph, and mutually update their representations through graph neural networks.\nWe evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its\nimprovement over existing LM and LM+KG models, as well as its capability to\nperform interpretable and structured reasoning, e.g., correctly handling\nnegation in questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph. (arXiv:2104.07302v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07302","description":"<p>Multi-hop Question Answering (QA) is a challenging task because it requires\nprecise reasoning with entity relations at every step towards the answer. The\nrelations can be represented in terms of labels in knowledge graph (e.g.,\n\\textit{spouse}) or text in text corpus (e.g., \\textit{they have been married\nfor 26 years}). Existing models usually infer the answer by predicting the\nsequential relation path or aggregating the hidden graph features. The former\nis hard to optimize, and the latter lacks interpretability. In this paper, we\npropose TransferNet, an effective and transparent model for multi-hop QA, which\nsupports both label and text relations in a unified framework. TransferNet\njumps across entities at multiple steps. At each step, it attends to different\nparts of the question, computes activated scores for relations, and then\ntransfer the previous entity scores along activated relations in a\ndifferentiable way. We carry out extensive experiments on three datasets and\ndemonstrate that TransferNet surpasses the state-of-the-art models by a large\nmargin. In particular, on MetaQA, it achieves 100\\% accuracy in 2-hop and 3-hop\nquestions. By qualitative analysis, we show that TransferNet has transparent\nand interpretable intermediate results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation. (arXiv:2104.08200v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08200","description":"<p>Natural language generation (NLG) benchmarks provide an important avenue to\nmeasure progress and develop better NLG systems. Unfortunately, the lack of\npublicly available NLG benchmarks for low-resource languages poses a\nchallenging barrier for building NLG systems that work well for languages with\nlimited amounts of data. Here we introduce IndoNLG, the first benchmark to\nmeasure natural language generation (NLG) progress in three low-resource -- yet\nwidely spoken -- languages of Indonesia: Indonesian, Javanese, and Sundanese.\nAltogether, these languages are spoken by more than 100 million native\nspeakers, and hence constitute an important use case of NLG systems today.\nConcretely, IndoNLG covers six tasks: summarization, question answering,\nchit-chat, and three different pairs of machine translation (MT) tasks. We\ncollate a clean pretraining corpus of Indonesian, Sundanese, and Javanese\ndatasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and\nIndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on\nall tasks -- despite using only one-fifth the parameters of a larger\nmultilingual model, mBART-LARGE (Liu et al., 2020). This finding emphasizes the\nimportance of pretraining on closely related, local languages to achieve more\nefficient learning and faster inference for very low-resource languages like\nJavanese and Sundanese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincentio_K/0/1/0/all/0/1\">Karissa Vincentio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaohong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhiguna Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_Z/0/1/0/all/0/1\">Zhi Yuan Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahar_S/0/1/0/all/0/1\">Syafri Bahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodra_M/0/1/0/all/0/1\">Masayu Leylia Khodra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1\">Ayu Purwarianti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning. (arXiv:2104.08808v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08808","description":"<p>The ability to continuously expand knowledge over time and utilize it to\nrapidly generalize to new tasks is a key feature of human linguistic\nintelligence. Existing models that pursue rapid generalization to new tasks\n(e.g., few-shot learning methods), however, are mostly trained in a single shot\non fixed datasets, unable to dynamically expand their knowledge; while\ncontinual learning algorithms are not specifically designed for rapid\ngeneralization. We present a new learning setup, Continual Learning of Few-Shot\nLearners (CLIF), to address the challenges of both learning settings in a\nunified setup. CLIF assumes a model learns from a sequence of diverse NLP tasks\narriving sequentially, accumulating knowledge for improved generalization to\nnew tasks, while also retaining performance on the tasks learned earlier. We\nexamine how the generalization ability is affected in the continual learning\nsetup, evaluate a number of continual learning algorithms, and propose a novel\nregularized adapter generation approach. We find that catastrophic forgetting\naffects generalization ability to a less degree than performance on seen tasks;\nwhile continual learning algorithms can still bring considerable benefit to the\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoFormer: Enhanced Transformer with Rotary Position Embedding. (arXiv:2104.09864v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09864","description":"<p>Position encoding in transformer architecture provides supervision for\ndependency modeling between elements at different positions in the sequence. We\ninvestigate various methods to encode positional information in\ntransformer-based language models and propose a novel implementation named\nRotary Position Embedding(RoPE). The proposed RoPE encodes absolute positional\ninformation with rotation matrix and naturally incorporates explicit relative\nposition dependency in self-attention formulation. Notably, RoPE comes with\nvaluable properties such as flexibility of being expand to any sequence\nlengths, decaying inter-token dependency with increasing relative distances,\nand capability of equipping the linear self-attention with relative position\nencoding. As a result, the enhanced transformer with rotary position embedding,\nor RoFormer, achieves superior performance in tasks with long texts. We release\nthe theoretical analysis along with some preliminary experiment results on\nChinese data. The undergoing experiment for English benchmark will soon be\nupdated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianlin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shengfeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bo Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfeng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Offensive Expressions of Opinion in Context. (arXiv:2104.12227v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.12227","description":"<p>Classic information extraction techniques consist in building questions and\nanswers about the facts. Indeed, it is still a challenge to subjective\ninformation extraction systems to identify opinions and feelings in context. In\nsentiment-based NLP tasks, there are few resources to information extraction,\nabove all offensive or hateful opinions in context. To fill this important gap,\nthis short paper provides a new cross-lingual and contextual offensive lexicon,\nwhich consists of explicit and implicit offensive and swearing expressions of\nopinion, which were annotated in two different classes: context dependent and\ncontext-independent offensive. In addition, we provide markers to identify hate\nspeech. Annotation approach was evaluated at the expression-level and achieves\nhigh human inter-annotator agreement. The provided offensive lexicon is\navailable in Portuguese and English languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Alves Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_I/0/1/0/all/0/1\">Isabelle Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabiana Rodrigues de G&#xf3;es</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Lexicon-Based Approach for Hate Speech and Offensive Language Detection. (arXiv:2104.12265v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.12265","description":"<p>This paper provides a new approach for offensive language and hate speech\ndetection on social media. Our approach incorporates an offensive lexicon\ncomposed of implicit and explicit offensive and swearing expressions annotated\nwith binary classes: context-dependent and context-independent offensive. Due\nto the severity of the hate speech and offensive comments in Brazil, and the\nlack of research in Portuguese, Brazilian Portuguese is the language used to\nvalidate the proposed method. Nevertheless, our proposal may be applied to any\nother language or domain. Based on the obtained results, the proposed approach\nshowed high-performance overcoming the current baselines for European and\nBrazilian Portuguese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Alves Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabiana Rodrigues de G&#xf3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_I/0/1/0/all/0/1\">Isabelle Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benevenuto_F/0/1/0/all/0/1\">Fabr&#xed;cio Benevenuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_T/0/1/0/all/0/1\">Thiago Alexandre Salgueiro Pardo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WhatTheWikiFact: Fact-Checking Claims Against Wikipedia. (arXiv:2105.00826v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00826","description":"<p>The rise of Internet has made it a major source of information.\nUnfortunately, not all information online is true, and thus a number of\nfact-checking initiatives have been launched, both manual and automatic, to\ndeal with the problem. Here, we present our contribution in this regard:\n\\emph{WhatTheWikiFact}, a system for automatic claim verification using\nWikipedia. The system can predict the veracity of an input claim, and it\nfurther shows the evidence it has retrieved as part of the verification\nprocess. It shows confidence scores and a list of relevant Wikipedia articles,\ntogether with detailed information about each article, including the phrase\nused to retrieve it, the most relevant sentences extracted from it and their\nstance with respect to the input claim, as well as the associated\nprobabilities. The system supports several languages: Bulgarian, English, and\nRussian.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1\">Anton Chernyavskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilvovsky_D/0/1/0/all/0/1\">Dmitry Ilvovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Black or White but never neutral: How readers perceive identity from yellow or skin-toned emoji. (arXiv:2105.05887v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.05887","description":"<p>Research in sociology and linguistics shows that people use language not only\nto express their own identity but to understand the identity of others. Recent\nwork established a connection between expression of identity and emoji usage on\nsocial media, through use of emoji skin tone modifiers. Motivated by that\nfinding, this work asks if, as with language, readers are sensitive to such\nacts of self-expression and use them to understand the identity of authors. In\nbehavioral experiments (n=488), where text and emoji content of social media\nposts were carefully controlled before being presented to participants, we find\nin the affirmative -- emoji are a salient signal of author identity. That\nsignal is distinct from, and complementary to, the one encoded in language.\nParticipant groups (based on self-identified ethnicity) showed no differences\nin how they perceive this signal, except in the case of the default yellow\nemoji. While both groups associate this with a White identity, the effect was\nstronger in White participants. Our finding that emoji can index social\nvariables will have experimental applications for researchers but also\nimplications for designers: supposedly ``neutral`` defaults may be more\nrepresentative of some users than others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robertson_A/0/1/0/all/0/1\">Alexander Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magdy_W/0/1/0/all/0/1\">Walid Magdy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1\">Sharon Goldwater</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doc2Dict: Information Extraction as Text Generation. (arXiv:2105.07510v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.07510","description":"<p>Typically, information extraction (IE) requires a pipeline approach: first, a\nsequence labeling model is trained on manually annotated documents to extract\nrelevant spans; then, when a new document arrives, a model predicts spans which\nare then post-processed and standardized to convert the information into a\ndatabase entry. We replace this labor-intensive workflow with a transformer\nlanguage model trained on existing database records to directly generate\nstructured JSON. Our solution removes the workload associated with producing\ntoken-level annotations and takes advantage of a data source which is generally\nquite plentiful (e.g. database records). As long documents are common in\ninformation extraction tasks, we use gradient checkpointing and chunked\nencoding to apply our method to sequences of up to 32,000 tokens on a single\nGPU. Our Doc2Dict approach is competitive with more complex, hand-engineered\npipelines and offers a simple but effective baseline for document-level\ninformation extraction. We release our Doc2Dict model and code to reproduce our\nexperiments and facilitate future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Townsend_B/0/1/0/all/0/1\">Benjamin Townsend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ito_Fisher_E/0/1/0/all/0/1\">Eamon Ito-Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lily Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_M/0/1/0/all/0/1\">Madison May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable agent communication from scratch (with a generic visual processor emerging on the side). (arXiv:2106.04258v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.04258","description":"<p>As deep networks begin to be deployed as autonomous agents, the issue of how\nthey can communicate with each other becomes important. Here, we train two deep\nnets from scratch to perform realistic referent identification through\nunsupervised emergent communication. We show that the largely interpretable\nemergent protocol allows the nets to successfully communicate even about object\ntypes they did not see at training time. The visual representations induced as\na by-product of our training regime, moreover, show comparable quality, when\nre-used as generic visual features, to a recent self-supervised learning model.\nOur results provide concrete evidence of the viability of (interpretable)\nemergent deep net communication in a more realistic scenario than previously\nconsidered, as well as establishing an intriguing link between this field and\nself-supervised visual learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1\">Marco Baroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Spatio-Temporal Language with Transformers. (arXiv:2106.08858v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2106.08858","description":"<p>Language is an interface to the outside world. In order for embodied agents\nto use it, language must be grounded in other, sensorimotor modalities. While\nthere is an extended literature studying how machines can learn grounded\nlanguage, the topic of how to learn spatio-temporal linguistic concepts is\nstill largely uncharted. To make progress in this direction, we here introduce\na novel spatio-temporal language grounding task where the goal is to learn the\nmeaning of spatio-temporal descriptions of behavioral traces of an embodied\nagent. This is achieved by training a truth function that predicts if a\ndescription matches a given history of observations. The descriptions involve\ntime-extended predicates in past and present tense as well as spatio-temporal\nreferences to objects in the scene. To study the role of architectural biases\nin this task, we train several models including multimodal Transformer\narchitectures; the latter implement different attention computations between\nwords and objects across space and time. We test models on two classes of\ngeneralization: 1) generalization to randomly held-out sentences; 2)\ngeneralization to grammar primitives. We observe that maintaining object\nidentity in the attention computation of our Transformers is instrumental to\nachieving good performance on generalization overall, and that summarizing\nobject traces in a single token has little influence on performance. We then\ndiscuss how this opens new perspectives for language-guided autonomous embodied\nagents. We also release our code under open-source license as well as\npretrained models and datasets to encourage the wider community to build upon\nand extend our work in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1\">Tristan Karch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1\">Laetitia Teodorescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1\">Cl&#xe9;ment Moulin-Frier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-wise Analysis of a Self-supervised Speech Representation Model. (arXiv:2107.04734v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.04734","description":"<p>Recently proposed self-supervised learning approaches have been successful\nfor pre-training speech representation models. The utility of these learned\nrepresentations has been observed empirically, but not much has been studied\nabout the type or extent of information encoded in the pre-trained\nrepresentations themselves. Developing such insights can help understand the\ncapabilities and limits of these models and enable the research community to\nmore efficiently develop their usage for downstream applications. In this work,\nwe begin to fill this gap by examining one recent and successful pre-trained\nmodel (wav2vec 2.0), via its intermediate representation vectors, using a suite\nof analysis tools. We use the metrics of canonical correlation, mutual\ninformation, and performance on simple downstream tasks with non-parametric\nprobes, in order to (i) query for acoustic and linguistic information content,\n(ii) characterize the evolution of information across model layers, and (iii)\nunderstand how fine-tuning the model for automatic speech recognition (ASR)\naffects these observations. Our findings motivate modifying the fine-tuning\nprotocol for ASR, which produces improved word error rates in a low-resource\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1\">Ju-Chieh Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Model with Self-Adaptive Sliding Window for Efficient Spoken Document Segmentation. (arXiv:2107.09278v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.09278","description":"<p>Transcripts generated by automatic speech recognition (ASR) systems for\nspoken documents lack structural annotations such as paragraphs, significantly\nreducing their readability. Automatically predicting paragraph segmentation for\nspoken documents may both improve readability and downstream NLP performance\nsuch as summarization and machine reading comprehension. We propose a sequence\nmodel with self-adaptive sliding window for accurate and efficient paragraph\nsegmentation. We also propose an approach to exploit phonetic information,\nwhich significantly improves robustness of spoken document segmentation to ASR\nerrors. Evaluations are conducted on the English Wiki-727K document\nsegmentation benchmark, a Chinese Wikipedia-based document segmentation dataset\nwe created, and an in-house Chinese spoken document dataset. Our proposed model\noutperforms the state-of-the-art (SOTA) model based on the same BERT-Base,\nincreasing segmentation F1 on the English benchmark by 4.2 points and on\nChinese datasets by 4.3-10.1 points, while reducing inference time to less than\n1/6 of inference time of the current SOTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yali Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Learning with Latent Neural Grammars. (arXiv:2109.01135v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01135","description":"<p>Sequence-to-sequence learning with neural networks has become the de facto\nstandard for sequence prediction tasks. This approach typically models the\nlocal distribution over the next word with a powerful neural network that can\ncondition on arbitrary context. While flexible and performant, these models\noften require large datasets for training and can fail spectacularly on\nbenchmarks designed to test for compositional generalization. This work\nexplores an alternative, hierarchical approach to sequence-to-sequence learning\nwith quasi-synchronous grammars, where each node in the target tree is\ntransduced by a node in the source tree. Both the source and target trees are\ntreated as latent and induced during training. We develop a neural\nparameterization of the grammar which enables parameter sharing over the\ncombinatorial space of derivation rules without the need for manual feature\nengineering. We apply this latent neural grammar to various domains -- a\ndiagnostic language navigation task designed to test for compositional\ngeneralization (SCAN), style transfer, and small-scale machine translation --\nand find that it performs respectably compared to standard baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization. (arXiv:2109.02401v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02401","description":"<p>Multimodal abstractive summarization (MAS) models that summarize videos\n(vision modality) and their corresponding transcripts (text modality) are able\nto extract the essential information from massive multimodal data on the\nInternet. Recently, large-scale generative pre-trained language models (GPLMs)\nhave been shown to be effective in text generation tasks. However, existing MAS\nmodels cannot leverage GPLMs' powerful generation ability. To fill this\nresearch gap, we aim to study two research questions: 1) how to inject visual\ninformation into GPLMs without hurting their generation ability; and 2) where\nis the optimal place in GPLMs to inject the visual information? In this paper,\nwe present a simple yet effective method to construct vision guided (VG) GPLMs\nfor the MAS task using attention-based add-on layers to incorporate visual\ninformation while maintaining their original text generation ability. Results\nshow that our best model significantly surpasses the prior state-of-the-art\nmodel by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,\nand our visual guidance method contributes 83.6% of the overall improvement.\nFurthermore, we conduct thorough ablation studies to analyze the effectiveness\nof various modality fusion methods and fusion locations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding. (arXiv:2109.04947v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04947","description":"<p>Large-scale, pre-trained language models (LMs) have achieved human-level\nperformance on a breadth of language understanding tasks. However, evaluations\nonly based on end task performance shed little light on machines' true ability\nin language understanding and reasoning. In this paper, we highlight the\nimportance of evaluating the underlying reasoning process in addition to end\nperformance. Toward this goal, we introduce Tiered Reasoning for Intuitive\nPhysics (TRIP), a novel commonsense reasoning dataset with dense annotations\nthat enable multi-tiered evaluation of machines' reasoning process. Our\nempirical results show that while large LMs can achieve high end performance,\nthey struggle to support their predictions with valid supporting evidence. The\nTRIP dataset and our baseline results will motivate verifiable evaluation of\ncommonsense reasoning and facilitate future research toward developing better\nlanguage understanding and reasoning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiaozi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pack Together: Entity and Relation Extraction with Levitated Marker. (arXiv:2109.06067v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06067","description":"<p>Named Entity Recognition (NER) and Relation Extraction (RE) are the core\nsub-tasks for information extraction. Many recent works formulate these two\ntasks as the span (pair) classification problem, and thus focus on\ninvestigating how to obtain a better span representation from the pre-trained\nencoder. However, a major limitation of existing works is that they ignore the\ndependencies between spans (pairs). In this work, we propose a novel span\nrepresentation approach, named Packed Levitated Markers, to consider the\ndependencies between the spans (pairs) by strategically packing the markers in\nthe encoder. In particular, we propose a group packing strategy to enable our\nmodel to process massive spans together to consider their dependencies with\nlimited resources. Furthermore, for those more complicated span pair\nclassification tasks, we design a subject-oriented packing strategy, which\npacks each subject and all its objects into an instance to model the\ndependencies between the same-subject span pairs. Our experiments show that our\nmodel with packed levitated markers outperforms the sequence labeling model by\n0.4%-1.9% F1 on three flat NER tasks, beats the token concat model on six NER\nbenchmarks, and obtains a 3.5%-3.6% strict relation F1 improvement with higher\nspeed over previous SOTA models on ACE04 and ACE05. Code and models are\npublicly available at https://github.com/thunlp/PL-Marker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition. (arXiv:2109.09161v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09161","description":"<p>Unifying acoustic and linguistic representation learning has become\nincreasingly crucial to transfer the knowledge learned on the abundance of\nhigh-resource language data for low-resource speech recognition. Existing\napproaches simply cascade pre-trained acoustic and language models to learn the\ntransfer from speech to text. However, how to solve the representation\ndiscrepancy of speech and text is unexplored, which hinders the utilization of\nacoustic and linguistic information. Moreover, previous works simply replace\nthe embedding layer of the pre-trained language model with the acoustic\nfeatures, which may cause the catastrophic forgetting problem. In this work, we\nintroduce Wav-BERT, a cooperative acoustic and linguistic representation\nlearning method to fuse and utilize the contextual information of speech and\ntext. Specifically, we unify a pre-trained acoustic model (wav2vec 2.0) and a\nlanguage model (BERT) into an end-to-end trainable framework. A Representation\nAggregation Module is designed to aggregate acoustic and linguistic\nrepresentation, and an Embedding Attention Module is introduced to incorporate\nacoustic information into BERT, which can effectively facilitate the\ncooperation of two pre-trained models and thus boost the representation\nlearning. Extensive experiments show that our Wav-BERT significantly\noutperforms the existing approaches and achieves state-of-the-art performance\non low-resource speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guolin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yubei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1\">Ke Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Bias in NLP -- Application to Hate Speech Classification using transfer learning techniques. (arXiv:2109.09725v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09725","description":"<p>In this paper, a BERT based neural network model is applied to the JIGSAW\ndata set in order to create a model identifying hateful and toxic comments\n(strictly seperated from offensive language) in online social platforms\n(English language), in this case Twitter. Three other neural network\narchitectures and a GPT-2 model are also applied on the provided data set in\norder to compare these different models. The trained BERT model is then applied\non two different data sets to evaluate its generalisation power, namely on\nanother Twitter data set and the data set HASOC 2019 which includes Twitter and\nalso Facebook comments; we focus on the English HASOC 2019 data. In addition,\nit can be shown that by fine-tuning the trained BERT model on these two data\nsets by applying different transfer learning scenarios via retraining partial\nor all layers the predictive scores improve compared to simply applying the\nmodel pre-trained on the JIGSAW data set. With our results, we get precisions\nfrom 64% to around 90% while still achieving acceptable recall values of at\nleast lower 60s%, proving that BERT is suitable for real use cases in social\nplatforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zagidullina_A/0/1/0/all/0/1\">Aygul Zagidullina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patoulidis_G/0/1/0/all/0/1\">Georgios Patoulidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bokstaller_J/0/1/0/all/0/1\">Jonas Bokstaller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WRENCH: A Comprehensive Benchmark for Weak Supervision. (arXiv:2109.11377v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.11377","description":"<p>Recent Weak Supervision (WS) approaches have had widespread success in easing\nthe bottleneck of labeling training data for machine learning by synthesizing\nlabels from multiple potentially noisy supervision sources. However, proper\nmeasurement and analysis of these approaches remain a challenge. First,\ndatasets used in existing works are often private and/or custom, limiting\nstandardization. Second, WS datasets with the same name and base data often\nvary in terms of the labels and weak supervision sources used, a significant\n\"hidden\" source of evaluation variance. Finally, WS studies often diverge in\nterms of the evaluation protocol and ablations used. To address these problems,\nwe introduce a benchmark platform, WRENCH, for thorough and standardized\nevaluation of WS approaches. It consists of 22 varied real-world datasets for\nclassification and sequence tagging; a range of real, synthetic, and\nprocedurally-generated weak supervision sources; and a modular, extensible\nframework for WS evaluation, including implementations for popular WS methods.\nWe use WRENCH to conduct extensive comparisons over more than 120 method\nvariants to demonstrate its efficacy as a benchmark platform. The code is\navailable at https://github.com/JieyuZ2/wrench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alexander Ratner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12584","description":"<p>In recent times, there has been definitive progress in the field of NLP, with\nits applications growing as the utility of our language models increases with\nadvances in their performance. However, these models require a large amount of\ncomputational power and data to train, consequently leading to large carbon\nfootprints. Therefore, it is imperative that we study the carbon efficiency and\nlook for alternatives to reduce the overall environmental impact of training\nmodels, in particular large language models. In our work, we assess the\nperformance of models for machine translation, across multiple language pairs\nto assess the difference in computational power required to train these models\nfor each of these language pairs and examine the various components of these\nmodels to analyze aspects of our pipeline that can be optimized to reduce these\ncarbon emissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yusuf_M/0/1/0/all/0/1\">Mirza Yusuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surana_P/0/1/0/all/0/1\">Praatibh Surana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gauri Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications. (arXiv:2109.14776v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14776","description":"<p>Certainty and uncertainty are fundamental to science communication. Hedges\nhave widely been used as proxies for uncertainty. However, certainty is a\ncomplex construct, with authors expressing not only the degree but the type and\naspects of uncertainty in order to give the reader a certain impression of what\nis known. Here, we introduce a new study of certainty that models both the\nlevel and the aspects of certainty in scientific findings. Using a new dataset\nof 2167 annotated scientific findings, we demonstrate that hedges alone account\nfor only a partial explanation of certainty. We show that both the overall\ncertainty and individual aspects can be predicted with pre-trained language\nmodels, providing a more complete picture of the author's intended\ncommunication. Downstream analyses on 431K scientific findings from news and\nscientific abstracts demonstrate that modeling sentence-level and aspect-level\ncertainty is meaningful for areas like science communication. Both the model\nand datasets used in this paper are released at\nhttps://blablablab.si.umich.edu/projects/certainty/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiaxin Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"#ContextMatters: Advantages and Limitations of Using Machine Learning to Support Women in Politics. (arXiv:2110.00116v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2110.00116","description":"<p>The United Nations identified gender equality as a Sustainable Development\nGoal in 2015, recognizing the underrepresentation of women in politics as a\nspecific barrier to achieving gender equality. Political systems around the\nworld experience gender inequality across all levels of elected government as\nfewer women run for office than men. This is due in part to online abuse,\nparticularly on social media platforms like Twitter, where women seeking or in\npower tend to be targeted with more toxic maltreatment than their male\ncounterparts. In this paper, we present reflections on ParityBOT - the first\nnatural language processing-based intervention designed to affect online\ndiscourse for women in politics for the better, at scale. Deployed across\nelections in Canada, the United States and New Zealand, ParityBOT was used to\nanalyse and classify more than 12 million tweets directed at women candidates\nand counter toxic tweets with supportive ones. From these elections we present\nthree case studies highlighting the current limitations of, and future research\nand application opportunities for, using a natural language processing-based\nsystem to detect online toxicity, specifically with regards to contextually\nimportant microaggressions. We examine the rate of false negatives, where\nParityBOT failed to pick up on insults directed at specific high profile women,\nwhich would be obvious to human users. We examine the unaddressed harms of\nmicroaggressions and the potential of yet unseen damage they cause for women in\nthese communities, and for progress towards gender equality overall, in light\nof these technological blindspots. This work concludes with a discussion on the\nbenefits of partnerships between nonprofit social groups and technology experts\nto develop responsible, socially impactful approaches to addressing online\nhate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Comer_J/0/1/0/all/0/1\">Jacqueline Comer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Work_S/0/1/0/all/0/1\">Sam Work</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathewson_K/0/1/0/all/0/1\">Kory W Mathewson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuthbertson_L/0/1/0/all/0/1\">Lana Cuthbertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machin_K/0/1/0/all/0/1\">Kasey Machin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow Articles. (arXiv:2110.03179v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03179","description":"<p>We present HowSumm, a novel large-scale dataset for the task of query-focused\nmulti-document summarization (qMDS), which targets the use-case of generating\nactionable instructions from a set of sources. This use-case is different from\nthe use-cases covered in existing multi-document summarization (MDS) datasets\nand is applicable to educational and industrial scenarios. We employed\nautomatic methods, and leveraged statistics from existing human-crafted qMDS\ndatasets, to create HowSumm from wikiHow website articles and the sources they\ncite. We describe the creation of the dataset and discuss the unique features\nthat distinguish it from other summarization corpora. Automatic and human\nevaluations of both extractive and abstractive summarization models on the\ndataset reveal that there is room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boni_O/0/1/0/all/0/1\">Odellia Boni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feigenblat_G/0/1/0/all/0/1\">Guy Feigenblat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lev_G/0/1/0/all/0/1\">Guy Lev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmueli_Scheuer_M/0/1/0/all/0/1\">Michal Shmueli-Scheuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznajder_B/0/1/0/all/0/1\">Benjamin Sznajder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopnicki_D/0/1/0/all/0/1\">David Konopnicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisualTTS: TTS with Accurate Lip-Speech Synchronization for Automatic Voice Over. (arXiv:2110.03342v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.03342","description":"<p>In this paper, we formulate a novel task to synthesize speech in sync with a\nsilent pre-recorded video, denoted as automatic voice over (AVO). Unlike\ntraditional speech synthesis, AVO seeks to generate not only human-sounding\nspeech, but also perfect lip-speech synchronization. A natural solution to AVO\nis to condition the speech rendering on the temporal progression of lip\nsequence in the video. We propose a novel text-to-speech model that is\nconditioned on visual input, named VisualTTS, for accurate lip-speech\nsynchronization. The proposed VisualTTS adopts two novel mechanisms that are 1)\ntextual-visual attention, and 2) visual fusion strategy during acoustic\ndecoding, which both contribute to forming accurate alignment between the input\ntext content and lip motion in input lip sequence. Experimental results show\nthat VisualTTS achieves accurate lip-speech synchronization and outperforms all\nbaseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_J/0/1/0/all/0/1\">Junchen Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sisman_B/0/1/0/all/0/1\">Berrak Sisman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying Phonological Features in Multilingual Text-To-Speech. (arXiv:2110.03609v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03609","description":"<p>This study investigates whether phonological features can be applied in\ntext-to-speech systems to generate native and non-native speech in English and\nMandarin. We present a mapping of ARPABET/pinyin to SAMPA/SAMPA-SC and then to\nphonological features. We tested whether this mapping could lead to the\nsuccessful generation of native, non-native, and code-switched speech in the\ntwo languages. We ran two experiments, one with a small dataset and one with a\nlarger dataset. The results proved that phonological features could be used as\na feasible input system, although further investigation is needed to improve\nmodel performance. The accented output generated by the TTS models also helps\nwith understanding human second language acquisition processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huinan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiewen Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation of professions in entertainment media: Insights into frequency and sentiment trends through computational text analysis. (arXiv:2110.03873v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03873","description":"<p>Societal ideas and trends dictate media narratives and cinematic depictions\nwhich in turn influences people's beliefs and perceptions of the real world.\nMedia portrayal of culture, education, government, religion, and family affect\ntheir function and evolution over time as people interpret and perceive these\nrepresentations and incorporate them into their beliefs and actions. It is\nimportant to study media depictions of these social structures so that they do\nnot propagate or reinforce negative stereotypes, or discriminate against any\ndemographic section. In this work, we examine media representation of\nprofessions and provide computational insights into their incidence, and\nsentiment expressed, in entertainment media content. We create a searchable\ntaxonomy of professional groups and titles to facilitate their retrieval from\nspeaker-agnostic text passages like movie and television (TV) show subtitles.\nWe leverage this taxonomy and relevant natural language processing (NLP) models\nto create a corpus of professional mentions in media content, spanning more\nthan 136,000 IMDb titles over seven decades (1950-2017). We analyze the\nfrequency and sentiment trends of different occupations, study the effect of\nmedia attributes like genre, country of production, and title type on these\ntrends, and investigate if the incidence of professions in media subtitles\ncorrelate with their real-world employment statistics. We observe increased\nmedia mentions of STEM, arts, sports, and entertainment occupations in the\nanalyzed subtitles, and a decreased frequency of manual labor jobs and military\noccupations. The sentiment expressed toward lawyers, police, and doctors is\nbecoming negative over time, whereas astronauts, musicians, singers, and\nengineers are mentioned favorably. Professions that employ more people have\nincreased media frequency, supporting our hypothesis that media acts as a\nmirror to society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baruah_S/0/1/0/all/0/1\">Sabyasachee Baruah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somandepalli_K/0/1/0/all/0/1\">Krishna Somandepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"3D Meta-Segmentation Neural Network. (arXiv:2110.04297v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04297","description":"<p>Though deep learning methods have shown great success in 3D point cloud part\nsegmentation, they generally rely on a large volume of labeled training data,\nwhich makes the model suffer from unsatisfied generalization abilities to\nunseen classes with limited data. To address this problem, we present a novel\nmeta-learning strategy that regards the 3D shape segmentation function as a\ntask. By training over a number of 3D part segmentation tasks, our method is\ncapable to learn the prior over the respective 3D segmentation function space\nwhich leads to an optimal model that is rapidly adapting to new part\nsegmentation tasks. To implement our meta-learning strategy, we propose two\nnovel modules: meta part segmentation learner and part segmentation learner.\nDuring the training process, the part segmentation learner is trained to\ncomplete a specific part segmentation task in the few-shot scenario. In the\nmeantime, the meta part segmentation learner is trained to capture the prior\nfrom multiple similar part segmentation tasks. Based on the learned information\nof task distribution, our meta part segmentation learner is able to dynamically\nupdate the part segmentation learner with optimal parameters which enable our\npart segmentation learner to rapidly adapt and have great generalization\nability on new part segmentation tasks. We demonstrate that our model achieves\nsuperior part segmentation performance with the few-shot setting on the widely\nused dataset: ShapeNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yu Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal ImageNet: How to discover spurious features in Deep Learning?. (arXiv:2110.04301v1 [cs.LG])","link":"http://arxiv.org/abs/2110.04301","description":"<p>A key reason for the lack of reliability of deep neural networks in the real\nworld is their heavy reliance on {\\it spurious} input features that are\ncausally unrelated to the true label. Focusing on image classifications, we\ndefine causal attributes as the set of visual features that are always a part\nof the object while spurious attributes are the ones that are likely to {\\it\nco-occur} with the object but not a part of it (e.g., attribute ``fingers\" for\nclass ``band aid\"). Traditional methods for discovering spurious features\neither require extensive human annotations (thus, not scalable), or are useful\non specific models. In this work, we introduce a {\\it scalable} framework to\ndiscover a subset of spurious and causal visual attributes used in inferences\nof a general model and localize them on a large number of images with minimal\nhuman supervision. Our methodology is based on this key idea: to identify\nspurious or causal \\textit{visual attributes} used in model predictions, we\nidentify spurious or causal \\textit{neural features} (penultimate layer neurons\nof a robust model) via limited human supervision (e.g., using top 5 activating\nimages per feature). We then show that these neural feature annotations {\\it\ngeneralize} extremely well to many more images {\\it without} any human\nsupervision. We use the activation maps for these neural features as the soft\nmasks to highlight spurious or causal visual attributes. Using this\nmethodology, we introduce the {\\it Causal Imagenet} dataset containing causal\nand spurious masks for a large set of samples from Imagenet. We assess the\nperformance of several popular Imagenet models and show that they rely heavily\non various spurious features in their predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sahil Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Face Mask Recognition with Advanced Face Cut Algorithm for Human Safety Measures. (arXiv:2110.04316v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04316","description":"<p>In the last year, the outbreak of COVID-19 has deployed computer vision and\nmachine learning algorithms in various fields to enhance human life\ninteractions. COVID-19 is a highly contaminated disease that affects mainly the\nrespiratory organs of the human body. We must wear a mask in this situation as\nthe virus can be contaminated through the air and a non-masked person can be\naffected. Our proposal deploys a computer vision and deep learning framework to\nrecognize face masks from images or videos. We have implemented a Boundary\ndependent face cut recognition algorithm that can cut the face from the image\nusing 27 landmarks and then the preprocessed image can further be sent to the\ndeep learning ResNet50 model. The experimental result shows a significant\nadvancement of 3.4 percent compared to the YOLOV3 mask recognition architecture\nin just 10 epochs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Arkaprabha Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Md Firoj Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning a Self-Expressive Network for Subspace Clustering. (arXiv:2110.04318v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04318","description":"<p>State-of-the-art subspace clustering methods are based on self-expressive\nmodel, which represents each data point as a linear combination of other data\npoints. However, such methods are designed for a finite sample dataset and lack\nthe ability to generalize to out-of-sample data. Moreover, since the number of\nself-expressive coefficients grows quadratically with the number of data\npoints, their ability to handle large-scale datasets is often limited. In this\npaper, we propose a novel framework for subspace clustering, termed\nSelf-Expressive Network (SENet), which employs a properly designed neural\nnetwork to learn a self-expressive representation of the data. We show that our\nSENet can not only learn the self-expressive coefficients with desired\nproperties on the training data, but also handle out-of-sample data. Besides,\nwe show that SENet can also be leveraged to perform subspace clustering on\nlarge-scale datasets. Extensive experiments conducted on synthetic data and\nreal world benchmark data validate the effectiveness of the proposed method. In\nparticular, SENet yields highly competitive performance on MNIST, Fashion MNIST\nand Extended MNIST and state-of-the-art performance on CIFAR-10. The code is\navailable at https://github.com/zhangsz1998/Self-Expressive-Network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shangzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chong You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Ren&#xe9; Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Guang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Token Attacks on Vision Transformers. (arXiv:2110.04337v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04337","description":"<p>Vision transformers rely on a patch token based self attention mechanism, in\ncontrast to convolutional networks. We investigate fundamental differences\nbetween these two families of models, by designing a block sparsity based\nadversarial token attack. We probe and analyze transformer as well as\nconvolutional models with token attacks of varying patch sizes. We infer that\ntransformer models are more sensitive to token attacks than convolutional\nmodels, with ResNets outperforming Transformer models by up to $\\sim30\\%$ in\nrobust accuracy for single token attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Ameya Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagatap_G/0/1/0/all/0/1\">Gauri Jagatap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1\">Chinmay Hegde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum pixel representations and compression for $N$-dimensional images. (arXiv:2110.04405v1 [quant-ph])","link":"http://arxiv.org/abs/2110.04405","description":"<p>We introduce a novel and uniform framework for quantum pixel representations\nthat overarches many of the most popular representations proposed in the recent\nliterature, such as (I)FRQI, (I)NEQR, MCRQI, and (I)NCQI. The proposed QPIXL\nframework results in more efficient circuit implementations and significantly\nreduces the gate complexity for all considered quantum pixel representations.\nOur method only requires a linear number of gates in terms of the number of\npixels and does not use ancilla qubits. Furthermore, the circuits only consist\nof Ry gates and CNOT gates making them practical in the NISQ era. Additionally,\nwe propose a circuit and image compression algorithm that is shown to be highly\neffective, being able to reduce the necessary gates to prepare an FRQI state\nfor example scientific images by up to 90% without sacrificing image quality.\nOur algorithms are made publicly available as part of QPIXL++, a Quantum Image\nPixel Library.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Amankwah_M/0/1/0/all/0/1\">Mercy G. Amankwah</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Camps_D/0/1/0/all/0/1\">Daan Camps</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bethel_E/0/1/0/all/0/1\">E. Wes Bethel</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Beeumen_R/0/1/0/all/0/1\">Roel Van Beeumen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Perciano_T/0/1/0/all/0/1\">Talita Perciano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Pose-Aware Part Decomposition for 3D Articulated Objects. (arXiv:2110.04411v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04411","description":"<p>Articulated objects exist widely in the real world. However, previous 3D\ngenerative methods for unsupervised part decomposition are unsuitable for such\nobjects, because they assume a spatially fixed part location, resulting in\ninconsistent part parsing. In this paper, we propose PPD (unsupervised\nPose-aware Part Decomposition) to address a novel setting that explicitly\ntargets man-made articulated objects with mechanical joints, considering the\npart poses. We show that category-common prior learning for both part shapes\nand poses facilitates the unsupervised learning of (1) part decomposition with\nnon-primitive-based implicit representation, and (2) part pose as joint\nparameters under single-frame shape supervision. We evaluate our method on\nsynthetic and real datasets, and we show that it outperforms previous works in\nconsistent part parsing of the articulated objects based on comparable part\npose estimation performance to the supervised baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kawana_Y/0/1/0/all/0/1\">Yuki Kawana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1\">Yusuke Mukuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness Evaluation of Transformer-based Form Field Extractors via Form Attacks. (arXiv:2110.04413v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04413","description":"<p>We propose a novel framework to evaluate the robustness of transformer-based\nform field extraction methods via form attacks. We introduce 14 novel form\ntransformations to evaluate the vulnerability of the state-of-the-art field\nextractors against form attacks from both OCR level and form level, including\nOCR location/order rearrangement, form background manipulation and form\nfield-value augmentation. We conduct robustness evaluation using real invoices\nand receipts, and perform comprehensive research analysis. Experimental results\nsuggest that the evaluated models are very susceptible to form perturbations\nsuch as the variation of field-values (~15% drop in F1 score), the\ndisarrangement of input text order(~15% drop in F1 score) and the disruption of\nthe neighboring words of field-values(~10% drop in F1 score). Guided by the\nanalysis, we make recommendations to improve the design of field extractors and\nthe process of data collection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Le Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arabic Speech Emotion Recognition Employing Wav2vec2.0 and HuBERT Based on BAVED Dataset. (arXiv:2110.04425v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04425","description":"<p>Recently, there have been tremendous research outcomes in the fields of\nspeech recognition and natural language processing. This is due to the\nwell-developed multi-layers deep learning paradigms such as wav2vec2.0,\nWav2vecU, WavBERT, and HuBERT that provide better representation learning and\nhigh information capturing. Such paradigms run on hundreds of unlabeled data,\nthen fine-tuned on a small dataset for specific tasks. This paper introduces a\ndeep learning constructed emotional recognition model for Arabic speech\ndialogues. The developed model employs the state of the art audio\nrepresentations include wav2vec2.0 and HuBERT. The experiment and performance\nresults of our model overcome the previous known outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_O/0/1/0/all/0/1\">Omar Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aly_S/0/1/0/all/0/1\">Salah A. Aly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing Unlabeled Data to Improve Generalization of Biometric Gender and Age Classifiers. (arXiv:2110.04427v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04427","description":"<p>With significant advances in deep learning, many computer vision applications\nhave reached the inflection point. However, these deep learning models need\nlarge amount of labeled data for model training and optimum parameter\nestimation. Limited labeled data for model training results in over-fitting and\nimpacts their generalization performance. However, the collection and\nannotation of large amount of data is a very time consuming and expensive\noperation. Further, due to privacy and security concerns, the large amount of\nlabeled data could not be collected for certain applications such as those\ninvolving medical field. Self-training, Co-training, and Self-ensemble methods\nare three types of semi-supervised learning methods that can be used to exploit\nunlabeled data. In this paper, we propose self-ensemble based deep learning\nmodel that along with limited labeled data, harness unlabeled data for\nimproving the generalization performance. We evaluated the proposed\nself-ensemble based deep-learning model for soft-biometric gender and age\nclassification. Experimental evaluation on CelebA and VISOB datasets suggest\ngender classification accuracy of 94.46% and 81.00%, respectively, using only\n1000 labeled samples and remaining 199k samples as unlabeled samples for CelebA\ndataset and similarly,1000 labeled samples with remaining 107k samples as\nunlabeled samples for VISOB dataset. Comparative evaluation suggest that there\nis $5.74\\%$ and $8.47\\%$ improvement in the accuracy of the self-ensemble model\nwhen compared with supervised model trained on the entire CelebA and VISOB\ndataset, respectively. We also evaluated the proposed learning method for\nage-group prediction on Adience dataset and it outperformed the baseline\nsupervised deep-learning learning model with a better exact accuracy of 55.55\n$\\pm$ 4.28 which is 3.92% more than the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nadimpalli_A/0/1/0/all/0/1\">Aakash Varma Nadimpalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_N/0/1/0/all/0/1\">Narsi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1\">Sreeraj Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rattani_A/0/1/0/all/0/1\">Ajita Rattani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankingMatch: Delving into Semi-Supervised Learning with Consistency Regularization and Ranking Loss. (arXiv:2110.04430v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04430","description":"<p>Semi-supervised learning (SSL) has played an important role in leveraging\nunlabeled data when labeled data is limited. One of the most successful SSL\napproaches is based on consistency regularization, which encourages the model\nto produce unchanged with perturbed input. However, there has been less\nattention spent on inputs that have the same label. Motivated by the\nobservation that the inputs having the same label should have the similar model\noutputs, we propose a novel method, RankingMatch, that considers not only the\nperturbed inputs but also the similarity among the inputs having the same\nlabel. We especially introduce a new objective function, dubbed BatchMean\nTriplet loss, which has the advantage of computational efficiency while taking\ninto account all input samples. Our RankingMatch achieves state-of-the-art\nperformance across many standard SSL benchmarks with a variety of labeled data\namounts, including 95.13% accuracy on CIFAR-10 with 250 labels, 77.65% accuracy\non CIFAR-100 with 10000 labels, 97.76% accuracy on SVHN with 250 labels, and\n97.77% accuracy on SVHN with 1000 labels. We also perform an ablation study to\nprove the efficacy of the proposed BatchMean Triplet loss against existing\nversions of Triplet loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trung Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Mingu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOMA: Solving Optical Marker-Based MoCap Automatically. (arXiv:2110.04431v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04431","description":"<p>Marker-based optical motion capture (mocap) is the \"gold standard\" method for\nacquiring accurate 3D human motion in computer vision, medicine, and graphics.\nThe raw output of these systems are noisy and incomplete 3D points or short\ntracklets of points. To be useful, one must associate these points with\ncorresponding markers on the captured subject; i.e. \"labelling\". Given these\nlabels, one can then \"solve\" for the 3D skeleton or body surface mesh.\nCommercial auto-labeling tools require a specific calibration procedure at\ncapture time, which is not possible for archival data. Here we train a novel\nneural network called SOMA, which takes raw mocap point clouds with varying\nnumbers of points, labels them at scale without any calibration data,\nindependent of the capture technology, and requiring only minimal human\nintervention. Our key insight is that, while labeling point clouds is highly\nambiguous, the 3D body provides strong constraints on the solution that can be\nexploited by a learning-based method. To enable learning, we generate massive\ntraining sets of simulated noisy and ground truth mocap markers animated by 3D\nbodies from AMASS. SOMA exploits an architecture with stacked self-attention\nelements to learn the spatial structure of the 3D body and an optimal transport\nlayer to constrain the assignment (labeling) problem while rejecting outliers.\nWe extensively evaluate SOMA both quantitatively and qualitatively. SOMA is\nmore accurate and robust than existing state of the art research methods and\ncan be applied where commercial systems cannot. We automatically label over 8\nhours of archival mocap data across 4 different datasets captured using various\ntechnologies and output SMPL-X body models. The model and data is released for\nresearch purposes at https://soma.is.tue.mpg.de/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_N/0/1/0/all/0/1\">Nima Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-stage Visual Cues Enhancement Network for Referring Image Segmentation. (arXiv:2110.04435v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04435","description":"<p>Referring Image Segmentation (RIS) aims at segmenting the target object from\nan image referred by one given natural language expression. The diverse and\nflexible expressions as well as complex visual contents in the images raise the\nRIS model with higher demands for investigating fine-grained matching behaviors\nbetween words in expressions and objects presented in images. However, such\nmatching behaviors are hard to be learned and captured when the visual cues of\nreferents (i.e. referred objects) are insufficient, as the referents with weak\nvisual cues tend to be easily confused by cluttered background at boundary or\neven overwhelmed by salient objects in the image. And the insufficient visual\ncues issue can not be handled by the cross-modal fusion mechanisms as done in\nprevious work. In this paper, we tackle this problem from a novel perspective\nof enhancing the visual information for the referents by devising a Two-stage\nVisual cues enhancement Network (TV-Net), where a novel Retrieval and\nEnrichment Scheme (RES) and an Adaptive Multi-resolution feature Fusion (AMF)\nmodule are proposed. Through the two-stage enhancement, our proposed TV-Net\nenjoys better performances in learning fine-grained matching behaviors between\nthe natural language expression and image, especially when the visual\ninformation of the referent is inadequate, thus produces better segmentation\nresults. Extensive experiments are conducted to validate the effectiveness of\nthe proposed method on the RIS task, with our proposed TV-Net surpassing the\nstate-of-the-art approaches on four benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zequn Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weixin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaolin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientPhys: Enabling Simple, Fast and Accurate Camera-Based Vitals Measurement. (arXiv:2110.04447v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04447","description":"<p>Camera-based physiological measurement is a growing field with neural models\nproviding state-the-art-performance. Prior research have explored various\n``end-to-end'' models; however these methods still require several\npreprocessing steps. These additional operations are often non-trivial to\nimplement making replication and deployment difficult and can even have a\nhigher computational budget than the ``core'' network itself. In this paper, we\npropose two novel and efficient neural models for camera-based physiological\nmeasurement called EfficientPhys that remove the need for face detection,\nsegmentation, normalization, color space transformation or any other\npreprocessing steps. Using an input of raw video frames, our models achieve\nstate-of-the-art accuracy on three public datasets. We show that this is the\ncase whether using a transformer or convolutional backbone. We further evaluate\nthe latency of the proposed networks and show that our most light weight\nnetwork also achieves a 33% improvement in efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_B/0/1/0/all/0/1\">Brian L. Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Ziheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shwetak Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Editing as Teleoperation: A Case Study in 6DoF Kit Assembly. (arXiv:2110.04450v1 [cs.RO])","link":"http://arxiv.org/abs/2110.04450","description":"<p>Studies in robot teleoperation have been centered around action\nspecifications -- from continuous joint control to discrete end-effector pose\ncontrol. However, these robot-centric interfaces often require skilled\noperators with extensive robotics expertise. To make teleoperation accessible\nto non-expert users, we propose the framework \"Scene Editing as Teleoperation\"\n(SEaT), where the key idea is to transform the traditional \"robot-centric\"\ninterface into a \"scene-centric\" interface -- instead of controlling the robot,\nusers focus on specifying the task's goal by manipulating digital twins of the\nreal-world objects. As a result, a user can perform teleoperation without any\nexpert knowledge of the robot hardware. To achieve this goal, we utilize a\ncategory-agnostic scene-completion algorithm that translates the real-world\nworkspace (with unknown objects) into a manipulable virtual scene\nrepresentation and an action-snapping algorithm that refines the user input\nbefore generating the robot's action plan. To train the algorithms, we\nprocedurally generated a large-scale, diverse kit-assembly dataset that\ncontains object-kit pairs that mimic real-world object-kitting tasks. Our\nexperiments in simulation and on a real-world system demonstrate that our\nframework improves both the efficiency and success rate for 6DoF kit-assembly\ntasks. A user study demonstrates that SEaT framework participants achieve a\nhigher task success rate and report a lower subjective workload compared to an\nalternative robot-centric interface. Video can be found at\nhttps://www.youtube.com/watch?v=-NdR3mkPbQQ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Shubham Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jen-Shuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feiner_S/0/1/0/all/0/1\">Steven K. Feiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer based COVID-19 Detection using Chest X-rays. (arXiv:2110.04458v1 [eess.IV])","link":"http://arxiv.org/abs/2110.04458","description":"<p>COVID-19 is a global pandemic, and detecting them is a momentous task for\nmedical professionals today due to its rapid mutations. Current methods of\nexamining chest X-rays and CT scan requires profound knowledge and are time\nconsuming, which suggests that it shrinks the precious time of medical\npractitioners when people's lives are at stake. This study tries to assist this\nprocess by achieving state-of-the-art performance in classifying chest X-rays\nby fine-tuning Vision Transformer(ViT). The proposed approach uses pretrained\nmodels, fine-tuned for detecting the presence of COVID-19 disease on chest\nX-rays. This approach achieves an accuracy score of 97.61%, precision score of\n95.34%, recall score of 93.84% and, f1-score of 94.58%. This result signifies\nthe performance of transformer-based models on chest X-ray.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Krishnan_K/0/1/0/all/0/1\">Koushik Sivarama Krishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krishnan_K/0/1/0/all/0/1\">Karthik Sivarama Krishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Training for Face Recognition Systems using Contrastive Adversarial Learning and Triplet Loss Fine-tuning. (arXiv:2110.04459v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04459","description":"<p>Though much work has been done in the domain of improving the adversarial\nrobustness of facial recognition systems, a surprisingly small percentage of it\nhas focused on self-supervised approaches. In this work, we present an approach\nthat combines Ad-versarial Pre-Training with Triplet Loss\nAdversarialFine-Tuning. We compare our methods with the pre-trained ResNet50\nmodel that forms the backbone of FaceNet, finetuned on our CelebA dataset.\nThrough comparing adversarial robustness achieved without adversarial training,\nwith triplet loss adversarial training, and our contrastive pre-training\ncombined with triplet loss adversarial fine-tuning, we find that our method\nachieves comparable results with far fewer epochs re-quired during fine-tuning.\nThis seems promising, increasing the training time for fine-tuning should yield\neven better results. In addition to this, a modified semi-supervised experiment\nwas conducted, which demonstrated the improvement of contrastive adversarial\ntraining with the introduction of small amounts of labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_U/0/1/0/all/0/1\">Umar Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meeker_N/0/1/0/all/0/1\">Nick Meeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samarasinghe_S/0/1/0/all/0/1\">Sarinda Samarasinghe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting decision-making in the future: Human versus Machine. (arXiv:2110.04465v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04465","description":"<p>Deep neural networks (DNNs) have become remarkably successful in data\nprediction, and have even been used to predict future actions based on limited\ninput. This raises the question: do these systems actually \"understand\" the\nevent similar to humans? Here, we address this issue using videos taken from an\naccident situation in a driving simulation. In this situation, drivers had to\nchoose between crashing into a suddenly-appeared obstacle or steering their car\noff a previously indicated cliff. We compared how well humans and a DNN\npredicted this decision as a function of time before the event. The DNN\noutperformed humans for early time-points, but had an equal performance for\nlater time-points. Interestingly, spatio-temporal image manipulations and\nGrad-CAM visualizations uncovered some expected behavior, but also highlighted\npotential differences in temporal processing for the DNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1\">Hoe Sung Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_U/0/1/0/all/0/1\">Uijong Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallraven_C/0/1/0/all/0/1\">Christian Wallraven</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label quality in AffectNet: results of crowd-based re-annotation. (arXiv:2110.04476v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04476","description":"<p>AffectNet is one of the most popular resources for facial expression\nrecognition (FER) on relatively unconstrained in-the-wild images. Given that\nimages were annotated by only one annotator with limited consistency checks on\nthe data, however, label quality and consistency may be limited. Here, we take\na similar approach to a study that re-labeled another, smaller dataset\n(FER2013) with crowd-based annotations, and report results from a re-labeling\nand re-annotation of a subset of difficult AffectNet faces with 13 people on\nboth expression label, and valence and arousal ratings. Our results show that\nhuman labels overall have medium to good consistency, whereas human ratings\nespecially for valence are in excellent agreement. Importantly, however,\ncrowd-based labels are significantly shifting towards neutral and happy\ncategories and crowd-based affective ratings form a consistent pattern\ndifferent from the original ratings. ResNets fully trained on the original\nAffectNet dataset do not predict human voting patterns, but when weakly-trained\ndo so much better, particularly for valence. Our results have important\nramifications for label quality in affective computing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doo Yon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallraven_C/0/1/0/all/0/1\">Christian Wallraven</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Feature Consistency Driven Attention Erasing Network for Fine-Grained Image Retrieval. (arXiv:2110.04479v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04479","description":"<p>Large-scale fine-grained image retrieval has two main problems. First, low\ndimensional feature embedding can fasten the retrieval process but bring\naccuracy reduce due to overlooking the feature of significant attention regions\nof images in fine-grained datasets. Second, fine-grained images lead to the\nsame category query hash codes mapping into the different cluster in database\nhash latent space. To handle these two issues, we propose a feature consistency\ndriven attention erasing network (FCAENet) for fine-grained image retrieval.\nFor the first issue, we propose an adaptive augmentation module in FCAENet,\nwhich is selective region erasing module (SREM). SREM makes the network more\nrobust on subtle differences of fine-grained task by adaptively covering some\nregions of raw images. The feature extractor and hash layer can learn more\nrepresentative hash code for fine-grained images by SREM. With regard to the\nsecond issue, we fully exploit the pair-wise similarity information and add the\nenhancing space relation loss (ESRL) in FCAENet to make the vulnerable relation\nstabler between the query hash code and database hash code. We conduct\nextensive experiments on five fine-grained benchmark datasets (CUB2011,\nAircraft, NABirds, VegFru, Food101) for 12bits, 24bits, 32bits, 48bits hash\ncode. The results show that FCAENet achieves the state-of-the-art (SOTA)\nfine-grained retrieval performance compared with other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuchang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Binghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Facial Expression Recognition in Humans and Machines: Using CAM, GradCAM, and Extremal Perturbation. (arXiv:2110.04481v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04481","description":"<p>Facial expression recognition (FER) is a topic attracting significant\nresearch in both psychology and machine learning with a wide range of\napplications. Despite a wealth of research on human FER and considerable\nprogress in computational FER made possible by deep neural networks (DNNs),\ncomparatively less work has been done on comparing the degree to which DNNs may\nbe comparable to human performance. In this work, we compared the recognition\nperformance and attention patterns of humans and machines during a\ntwo-alternative forced-choice FER task. Human attention was here gathered\nthrough click data that progressively uncovered a face, whereas model attention\nwas obtained using three different popular techniques from explainable AI: CAM,\nGradCAM and Extremal Perturbation. In both cases, performance was gathered as\npercent correct. For this task, we found that humans outperformed machines\nquite significantly. In terms of attention patterns, we found that Extremal\nPerturbation had the best overall fit with the human attention map during the\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Serin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallraven_C/0/1/0/all/0/1\">Christian Wallraven</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualizing the embedding space to explain the effect of knowledge distillation. (arXiv:2110.04483v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04483","description":"<p>Recent research has found that knowledge distillation can be effective in\nreducing the size of a network and in increasing generalization. A pre-trained,\nlarge teacher network, for example, was shown to be able to bootstrap a student\nmodel that eventually outperforms the teacher in a limited label environment.\nDespite these advances, it still is relatively unclear \\emph{why} this method\nworks, that is, what the resulting student model does 'better'. To address this\nissue, here, we utilize two non-linear, low-dimensional embedding methods\n(t-SNE and IVIS) to visualize representation spaces of different layers in a\nnetwork. We perform a set of extensive experiments with different architecture\nparameters and distillation methods. The resulting visualizations and metrics\nclearly show that distillation guides the network to find a more compact\nrepresentation space for higher accuracy already in earlier layers compared to\nits non-distilled version.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyun Seung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallraven_C/0/1/0/all/0/1\">Christian Wallraven</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colour augmentation for improved semi-supervised semantic segmentation. (arXiv:2110.04487v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04487","description":"<p>Consistency regularization describes a class of approaches that have yielded\nstate-of-the-art results for semi-supervised classification. While\nsemi-supervised semantic segmentation proved to be more challenging, a number\nof successful approaches have been recently proposed. Recent work explored the\nchallenges involved in using consistency regularization for segmentation\nproblems. In their self-supervised work Chen et al. found that colour\naugmentation prevents a classification network from using image colour\nstatistics as a short-cut for self-supervised learning via instance\ndiscrimination. Drawing inspiration from this we find that a similar problem\nimpedes semi-supervised semantic segmentation and offer colour augmentation as\na solution, improving semi-supervised semantic segmentation performance on\nchallenging photographic imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+French_G/0/1/0/all/0/1\">Geoff French</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackiewicz_M/0/1/0/all/0/1\">Michal Mackiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demystifying the Transferability of Adversarial Attacks in Computer Networks. (arXiv:2110.04488v1 [cs.CR])","link":"http://arxiv.org/abs/2110.04488","description":"<p>Deep Convolutional Neural Networks (CNN) models are one of the most popular\nnetworks in deep learning. With their large fields of application in different\nareas, they are extensively used in both academia and industry. CNN-based\nmodels include several exciting implementations such as early breast cancer\ndetection or detecting developmental delays in children (e.g., autism, speech\ndisorders, etc.). However, previous studies demonstrate that these models are\nsubject to various adversarial attacks. Interestingly, some adversarial\nexamples could potentially still be effective against different unknown models.\nThis particular property is known as adversarial transferability, and prior\nworks slightly analyzed this characteristic in a very limited application\ndomain. In this paper, we aim to demystify the transferability threats in\ncomputer networks by studying the possibility of transferring adversarial\nexamples. In particular, we provide the first comprehensive study which\nassesses the robustness of CNN-based models for computer networks against\nadversarial transferability. In our experiments, we consider five different\nattacks: (1) the Iterative Fast Gradient Method (I-FGSM), (2) the\nJacobian-based Saliency Map attack (JSMA), (3) the L-BFGS attack, (4) the\nProjected Gradient Descent attack (PGD), and (5) the DeepFool attack. These\nattacks are performed against two well-known datasets: the N-BaIoT dataset and\nthe Domain Generating Algorithms (DGA) dataset. Our results show that the\ntransferability happens in specific use cases where the adversary can easily\ncompromise the victim's network with very few knowledge of the targeted model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1\">Ehsan Nowroozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekdad_Y/0/1/0/all/0/1\">Yassine Mekdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berenjestanaki_M/0/1/0/all/0/1\">Mohammad Hajian Berenjestanaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergougui_A/0/1/0/all/0/1\">Abdeslam EL Fergougui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invertible Tone Mapping with Selectable Styles. (arXiv:2110.04491v1 [eess.IV])","link":"http://arxiv.org/abs/2110.04491","description":"<p>Although digital cameras can acquire high-dynamic range (HDR) images, the\ncaptured HDR information are mostly quantized to low-dynamic range (LDR) images\nfor display compatibility and compact storage. In this paper, we propose an\ninvertible tone mapping method that converts the multi-exposure HDR to a true\nLDR (8-bit per color channel) and reserves the capability to accurately restore\nthe original HDR from this {\\em invertible LDR}. Our invertible LDR can mimic\nthe appearance of a user-selected tone mapping style. It can be shared over any\nexisting social network platforms that may re-encode or format-convert the\nuploaded images, without much hurting the accuracy of the restored HDR\ncounterpart. To achieve this, we regard the tone mapping and the restoration as\ncoupled processes, and formulate them as an encoding-and-decoding problem\nthrough convolutional neural networks. Particularly, our model supports\npluggable style modulators, each of which bakes a specific tone mapping style,\nand thus favors the application flexibility. Our method is evaluated with a\nrich variety of HDR images and multiple tone mapping operators, which shows the\nsuperiority over relevant state-of-the-art methods. Also, we conduct ablation\nstudy to justify our design and discuss the robustness and generality toward\nreal applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuming Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_M/0/1/0/all/0/1\">Menghan Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xueting Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chengze Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_T/0/1/0/all/0/1\">Tien-Tsin Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weight Evolution: Improving Deep Neural Networks Training through Evolving Inferior Weight Values. (arXiv:2110.04492v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04492","description":"<p>To obtain good performance, convolutional neural networks are usually\nover-parameterized. This phenomenon has stimulated two interesting topics:\npruning the unimportant weights for compression and reactivating the\nunimportant weights to make full use of network capability. However, current\nweight reactivation methods usually reactivate the entire filters, which may\nnot be precise enough. Looking back in history, the prosperity of filter\npruning is mainly due to its friendliness to hardware implementation, but\npruning at a finer structure level, i.e., weight elements, usually leads to\nbetter network performance. We study the problem of weight element reactivation\nin this paper. Motivated by evolution, we select the unimportant filters and\nupdate their unimportant elements by combining them with the important elements\nof important filters, just like gene crossover to produce better offspring, and\nthe proposed method is called weight evolution (WE). WE is mainly composed of\nfour strategies. We propose a global selection strategy and a local selection\nstrategy and combine them to locate the unimportant filters. A forward matching\nstrategy is proposed to find the matched important filters and a crossover\nstrategy is proposed to utilize the important elements of the important filters\nfor updating unimportant filters. WE is plug-in to existing network\narchitectures. Comprehensive experiments show that WE outperforms the other\nreactivation methods and plug-in training methods with typical convolutional\nneural networks, especially lightweight networks. Our code is available at\nhttps://github.com/BZQLin/Weight-evolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenquan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kailing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1\">Xiaofen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangmin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGMNet: Scene Graph Matching Network for Few-Shot Remote Sensing Scene Classification. (arXiv:2110.04494v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04494","description":"<p>Few-Shot Remote Sensing Scene Classification (FSRSSC) is an important task,\nwhich aims to recognize novel scene classes with few examples. Recently,\nseveral studies attempt to address the FSRSSC problem by following few-shot\nnatural image classification methods. These existing methods have made\npromising progress and achieved superior performance. However, they all\noverlook two unique characteristics of remote sensing images: (i) object\nco-occurrence that multiple objects tend to appear together in a scene image\nand (ii) object spatial correlation that these co-occurrence objects are\ndistributed in the scene image following some spatial structure patterns. Such\nunique characteristics are very beneficial for FSRSSC, which can effectively\nalleviate the scarcity issue of labeled remote sensing images since they can\nprovide more refined descriptions for each scene class. To fully exploit these\ncharacteristics, we propose a novel scene graph matching-based meta-learning\nframework for FSRSSC, called SGMNet. In this framework, a scene graph\nconstruction module is carefully designed to represent each test remote sensing\nimage or each scene class as a scene graph, where the nodes reflect these\nco-occurrence objects meanwhile the edges capture the spatial correlations\nbetween these co-occurrence objects. Then, a scene graph matching module is\nfurther developed to evaluate the similarity score between each test remote\nsensing image and each scene class. Finally, based on the similarity scores, we\nperform the scene class prediction via a nearest neighbor classifier. We\nconduct extensive experiments on UCMerced LandUse, WHU19, AID, and\nNWPU-RESISC45 datasets. The experimental results show that our method obtains\nsuperior performance over the previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baoquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shanshan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xutao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yunming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rui Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZSpeedL -- Evaluating the Performance of Zero-Shot Learning Methods using Low-Power Devices. (arXiv:2110.04535v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04535","description":"<p>The recognition of unseen objects from a semantic representation or textual\ndescription, usually denoted as zero-shot learning, is more prone to be used in\nreal-world scenarios when compared to traditional object recognition.\nNevertheless, no work has evaluated the feasibility of deploying zero-shot\nlearning approaches in these scenarios, particularly when using low-power\ndevices. In this paper, we provide the first benchmark on the inference time of\nzero-shot learning, comprising an evaluation of state-of-the-art approaches\nregarding their speed/accuracy trade-off. An analysis to the processing time of\nthe different phases of the ZSL inference stage reveals that visual feature\nextraction is the major bottleneck in this paradigm, but, we show that\nlightweight networks can dramatically reduce the overall inference time without\nreducing the accuracy obtained by the de facto ResNet101 architecture. Also,\nthis benchmark evaluates how different ZSL approaches perform in low-power\ndevices, and how the visual feature extraction phase could be optimized in this\nhardware. To foster the research and deployment of ZSL systems capable of\noperating in real-world scenarios, we release the evaluation framework used in\nthis benchmark (https://github.com/CristianoPatricio/zsl-methods).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patricio_C/0/1/0/all/0/1\">Cristiano Patr&#xed;cio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_J/0/1/0/all/0/1\">Jo&#xe3;o Neves</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus Your Distribution: Coarse-to-Fine Non-Contrastive Learning for Anomaly Detection and Localization. (arXiv:2110.04538v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04538","description":"<p>The essence of unsupervised anomaly detection is to learn the compact\ndistribution of normal samples and detect outliers as anomalies in testing.\nMeanwhile, the anomalies in real-world are usually subtle and fine-grained in a\nhigh-resolution image especially for industrial applications. Towards this end,\nwe propose a novel framework for unsupervised anomaly detection and\nlocalization. Our method aims at learning dense and compact distribution from\nnormal images with a coarse-to-fine alignment process. The coarse alignment\nstage standardizes the pixel-wise position of objects in both image and feature\nlevels. The fine alignment stage then densely maximizes the similarity of\nfeatures among all corresponding locations in a batch. To facilitate the\nlearning with only normal images, we propose a new pretext task called\nnon-contrastive learning for the fine alignment stage. Non-contrastive learning\nextracts robust and discriminating normal image representations without making\nassumptions on abnormal samples, and it thus empowers our model to generalize\nto various anomalous scenarios. Extensive experiments on two typical industrial\ndatasets of MVTec AD and BenTech AD demonstrate that our framework is effective\nin detecting various real-world defects and achieves a new state-of-the-art in\nindustrial unsupervised anomaly detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Ye Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Rui Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_T/0/1/0/all/0/1\">Tianpeng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Balanced Active Learning for Image Classification. (arXiv:2110.04543v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04543","description":"<p>Active learning aims to reduce the labeling effort that is required to train\nalgorithms by learning an acquisition function selecting the most relevant data\nfor which a label should be requested from a large unlabeled data pool. Active\nlearning is generally studied on balanced datasets where an equal amount of\nimages per class is available. However, real-world datasets suffer from severe\nimbalanced classes, the so called long-tail distribution. We argue that this\nfurther complicates the active learning process, since the imbalanced data pool\ncan result in suboptimal classifiers. To address this problem in the context of\nactive learning, we proposed a general optimization framework that explicitly\ntakes class-balancing into account. Results on three datasets showed that the\nmethod is general (it can be combined with most existing active learning\nalgorithms) and can be effectively applied to boost the performance of both\ninformative and representative-based active learning methods. In addition, we\nshowed that also on balanced datasets our method generally results in a\nperformance gain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bengar_J/0/1/0/all/0/1\">Javad Zolfaghari Bengar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuentes_L/0/1/0/all/0/1\">Laura Lopez Fuentes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1\">Bogdan Raducanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Adapter: Better Vision-Language Models with Feature Adapters. (arXiv:2110.04544v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04544","description":"<p>Large-scale contrastive vision-language pre-training has shown significant\nprogress in visual representation learning. Unlike traditional visual systems\ntrained by a fixed set of discrete labels, a new paradigm was introduced in\n\\cite{radford2021learning} to directly learn to align images with raw texts in\nan open-vocabulary setting. On downstream tasks, a carefully chosen text prompt\nis employed to make zero-shot predictions.~To avoid non-trivial prompt\nengineering, context optimization \\cite{zhou2021coop} has been proposed to\nlearn continuous vectors as task-specific prompts with few-shot training\nexamples.~In this paper, we show that there is an alternative path to achieve\nbetter vision-language models other than prompt tuning.~While prompt tuning is\nfor the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with\nfeature adapters on either visual or language branch. Specifically,\nCLIP-Adapter adopts an additional bottleneck layer to learn new features and\nperforms residual-style feature blending with the original pre-trained\nfeatures.~As a consequence, CLIP-Adapter is able to outperform context\noptimization while maintains a simple design. Experiments and extensive\nablation studies on various visual classification tasks demonstrate the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Rongyao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Data-Free Domain Generalization. (arXiv:2110.04545v1 [cs.LG])","link":"http://arxiv.org/abs/2110.04545","description":"<p>In this work, we investigate the unexplored intersection of domain\ngeneralization and data-free learning. In particular, we address the question:\nHow can knowledge contained in models trained on different source data domains\ncan be merged into a single model that generalizes well to unseen target\ndomains, in the absence of source and target domain data? Machine learning\nmodels that can cope with domain shift are essential for for real-world\nscenarios with often changing data distributions. Prior domain generalization\nmethods typically rely on using source domain data, making them unsuitable for\nprivate decentralized data. We define the novel problem of Data-Free Domain\nGeneralization (DFDG), a practical setting where models trained on the source\ndomains separately are available instead of the original datasets, and\ninvestigate how to effectively solve the domain generalization problem in that\ncase. We propose DEKAN, an approach that extracts and fuses domain-specific\nknowledge from the available teacher models into a student model robust to\ndomain shift. Our empirical evaluation demonstrates the effectiveness of our\nmethod which achieves first state-of-the-art results in DFDG by significantly\noutperforming ensemble and data-free knowledge distillation baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frikha_A/0/1/0/all/0/1\">Ahmed Frikha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haokun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krompass_D/0/1/0/all/0/1\">Denis Krompa&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Runkler_T/0/1/0/all/0/1\">Thomas Runkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Representation Learning Meets Pseudo-Label Supervised Self-Distillation: A New Approach to Rare Disease Classification. (arXiv:2110.04558v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04558","description":"<p>Rare diseases are characterized by low prevalence and are often chronically\ndebilitating or life-threatening. Imaging-based classification of rare diseases\nis challenging due to the severe shortage in training examples. Few-shot\nlearning (FSL) methods tackle this challenge by extracting generalizable prior\nknowledge from a large base dataset of common diseases and normal controls, and\ntransferring the knowledge to rare diseases. Yet, most existing methods require\nthe base dataset to be labeled and do not make full use of the precious\nexamples of the rare diseases. To this end, we propose in this work a novel\nhybrid approach to rare disease classification, featuring two key novelties\ntargeted at the above drawbacks. First, we adopt the unsupervised\nrepresentation learning (URL) based on self-supervising contrastive loss,\nwhereby to eliminate the overhead in labeling the base dataset. Second, we\nintegrate the URL with pseudo-label supervised classification for effective\nself-distillation of the knowledge about the rare diseases, composing a hybrid\napproach taking advantages of both unsupervised and (pseudo-) supervised\nlearning on the base dataset. Experimental results on classification of rare\nskin lesions show that our hybrid approach substantially outperforms existing\nFSL methods (including those using fully supervised base dataset) for rare\ndisease classification via effective integration of the URL and pseudo-label\ndriven self-distillation, thus establishing a new state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jinghan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporally Consistent Video Colorization with Deep Feature Propagation and Self-regularization Learning. (arXiv:2110.04562v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04562","description":"<p>Video colorization is a challenging and highly ill-posed problem. Although\nrecent years have witnessed remarkable progress in single image colorization,\nthere is relatively less research effort on video colorization and existing\nmethods always suffer from severe flickering artifacts (temporal inconsistency)\nor unsatisfying colorization performance. We address this problem from a new\nperspective, by jointly considering colorization and temporal consistency in a\nunified framework. Specifically, we propose a novel temporally consistent video\ncolorization framework (TCVC). TCVC effectively propagates frame-level deep\nfeatures in a bidirectional way to enhance the temporal consistency of\ncolorization. Furthermore, TCVC introduces a self-regularization learning (SRL)\nscheme to minimize the prediction difference obtained with different time\nsteps. SRL does not require any ground-truth color videos for training and can\nfurther improve temporal consistency. Experiments demonstrate that our method\ncan not only obtain visually pleasing colorized video, but also achieve clearly\nbetter temporal consistency than state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1\">Kelvin C.K. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Recognition of Abdominal Organs in Ultrasound Images based on Deep Neural Networks and K-Nearest-Neighbor Classification. (arXiv:2110.04563v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04563","description":"<p>Abdominal ultrasound imaging has been widely used to assist in the diagnosis\nand treatment of various abdominal organs. In order to shorten the examination\ntime and reduce the cognitive burden on the sonographers, we present a\nclassification method that combines the deep learning techniques and\nk-Nearest-Neighbor (k-NN) classification to automatically recognize various\nabdominal organs in the ultrasound images in real time. Fine-tuned deep neural\nnetworks are used in combination with PCA dimension reduction to extract\nhigh-level features from raw ultrasound images, and a k-NN classifier is\nemployed to predict the abdominal organ in the image. We demonstrate the\neffectiveness of our method in the task of ultrasound image classification to\nautomatically recognize six abdominal organs. A comprehensive comparison of\ndifferent configurations is conducted to study the influence of different\nfeature extractors and classifiers on the classification accuracy. Both\nquantitative and qualitative results show that with minimal training effort,\nour method can \"lazily\" recognize the abdominal organs in the ultrasound images\nin real time with an accuracy of 96.67%. Our implementation code is publicly\navailable at: https://github.com/LeeKeyu/abdominal_ultrasound_classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Keyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Max Q.-H. Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Space-Time-Separable Graph Convolutional Network for Pose Forecasting. (arXiv:2110.04573v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04573","description":"<p>Human pose forecasting is a complex structured-data sequence-modelling task,\nwhich has received increasing attention, also due to numerous potential\napplications. Research has mainly addressed the temporal dimension as time\nseries and the interaction of human body joints with a kinematic tree or by a\ngraph. This has decoupled the two aspects and leveraged progress from the\nrelevant fields, but it has also limited the understanding of the complex\nstructural joint spatio-temporal dynamics of the human pose. Here we propose a\nnovel Space-Time-Separable Graph Convolutional Network (STS-GCN) for pose\nforecasting. For the first time, STS-GCN models the human pose dynamics only\nwith a graph convolutional network (GCN), including the temporal evolution and\nthe spatial joint interaction within a single-graph framework, which allows the\ncross-talk of motion and spatial correlations. Concurrently, STS-GCN is the\nfirst space-time-separable GCN: the space-time graph connectivity is factored\ninto space and time affinity matrices, which bottlenecks the space-time\ncross-talk, while enabling full joint-joint and time-time correlations. Both\naffinity matrices are learnt end-to-end, which results in connections\nsubstantially deviating from the standard kinematic tree and the linear-time\ntime series. In experimental evaluation on three complex, recent and\nlarge-scale benchmarks, Human3.6M [Ionescu et al. TPAMI'14], AMASS [Mahmood et\nal. ICCV'19] and 3DPW [Von Marcard et al. ECCV'18], STS-GCN outperforms the\nstate-of-the-art, surpassing the current best technique [Mao et al. ECCV'20] by\nover 32% in average at the most difficult long-term predictions, while only\nrequiring 1.7% of its parameters. We explain the results qualitatively and\nillustrate the graph interactions by the factored joint-joint and time-time\nlearnt graph connections.\n</p>\n<p>Our source code is available at: https://github.com/FraLuca/STSGCN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sofianos_T/0/1/0/all/0/1\">Theodoros Sofianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampieri_A/0/1/0/all/0/1\">Alessio Sampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_L/0/1/0/all/0/1\">Luca Franco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galasso_F/0/1/0/all/0/1\">Fabio Galasso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Long-Tailed Learning: A Survey. (arXiv:2110.04596v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04596","description":"<p>Deep long-tailed learning, one of the most challenging problems in visual\nrecognition, aims to train well-performing deep models from a large number of\nimages that follow a long-tailed class distribution. In the last decade, deep\nlearning has emerged as a powerful recognition model for learning high-quality\nimage representations and has led to remarkable breakthroughs in generic visual\nrecognition. However, long-tailed class imbalance, a common problem in\npractical visual recognition tasks, often limits the practicality of deep\nnetwork based recognition models in real-world applications, since they can be\neasily biased towards dominant classes and perform poorly on tail classes. To\naddress this problem, a large number of studies have been conducted in recent\nyears, making promising progress in the field of deep long-tailed learning.\nConsidering the rapid evolution of this field, this paper aims to provide a\ncomprehensive survey on recent advances in deep long-tailed learning. To be\nspecific, we group existing deep long-tailed learning studies into three main\ncategories (i.e., class re-balancing, information augmentation and module\nimprovement), and review these methods following this taxonomy in detail.\nAfterward, we empirically analyze several state-of-the-art methods by\nevaluating to what extent they address the issue of class imbalance via a newly\nproposed evaluation metric, i.e., relative accuracy. We conclude the survey by\nhighlighting important applications of deep long-tailed learning and\nidentifying several promising directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Bingyi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Single/Multi-Attribute of Object with Symmetry and Group. (arXiv:2110.04603v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04603","description":"<p>Attributes and objects can compose diverse compositions. To model the\ncompositional nature of these concepts, it is a good choice to learn them as\ntransformations, e.g., coupling and decoupling. However, complex\ntransformations need to satisfy specific principles to guarantee rationality.\nHere, we first propose a previously ignored principle of attribute-object\ntransformation: Symmetry. For example, coupling peeled-apple with attribute\npeeled should result in peeled-apple, and decoupling peeled from apple should\nstill output apple. Incorporating the symmetry, we propose a transformation\nframework inspired by group theory, i.e., SymNet. It consists of two modules:\nCoupling Network and Decoupling Network. We adopt deep neural networks to\nimplement SymNet and train it in an end-to-end paradigm with the group axioms\nand symmetry as objectives. Then, we propose a Relative Moving Distance (RMD)\nbased method to utilize the attribute change instead of the attribute pattern\nitself to classify attributes. Besides the compositions of single-attribute and\nobject, our RMD is also suitable for complex compositions of multiple\nattributes and objects when incorporating attribute correlations. SymNet can be\nutilized for attribute learning, compositional zero-shot learning and\noutperforms the state-of-the-art on four widely-used benchmarks. Code is at\nhttps://github.com/DirtyHarryLYL/SymNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong-Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaohan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning MRI Artifact Removal With Unpaired Data. (arXiv:2110.04604v1 [eess.IV])","link":"http://arxiv.org/abs/2110.04604","description":"<p>Retrospective artifact correction (RAC) improves image quality post\nacquisition and enhances image usability. Recent machine learning driven\ntechniques for RAC are predominantly based on supervised learning and therefore\npractical utility can be limited as data with paired artifact-free and\nartifact-corrupted images are typically insufficient or even non-existent. Here\nwe show that unwanted image artifacts can be disentangled and removed from an\nimage via an RAC neural network learned with unpaired data. This implies that\nour method does not require matching artifact-corrupted data to be either\ncollected via acquisition or generated via simulation. Experimental results\ndemonstrate that our method is remarkably effective in removing artifacts and\nretaining anatomical details in images with different contrasts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Siyuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thung_K/0/1/0/all/0/1\">Kim-Han Thung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_W/0/1/0/all/0/1\">Weili Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Google Landmark Retrieval 2021 Competition Third Place Solution. (arXiv:2110.04619v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04619","description":"<p>We present our solutions to the Google Landmark Challenges 2021, for both the\nretrieval and the recognition tracks. Both solutions are ensembles of\ntransformers and ConvNet models based on Sub-center ArcFace with dynamic\nmargins. Since the two tracks share the same training data, we used the same\npipeline and training approach, but with different model selections for the\nensemble and different post-processing. The key improvement over last year is\nnewer state-of-the-art vision architectures, especially transformers which\nsignificantly outperform ConvNets for the retrieval task. We finished third and\nfourth places for the retrieval and recognition tracks respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ha_Q/0/1/0/all/0/1\">Qishen Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vector-quantized Image Modeling with Improved VQGAN. (arXiv:2110.04627v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04627","description":"<p>Pretraining language models with next-token prediction on massive text\ncorpora has delivered phenomenal zero-shot, few-shot, transfer learning and\nmulti-tasking capabilities on both generative and discriminative language\ntasks. Motivated by this success, we explore a Vector-quantized Image Modeling\n(VIM) approach that involves pretraining a Transformer to predict rasterized\nimage tokens autoregressively. The discrete image tokens are encoded from a\nlearned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple\nimprovements over vanilla VQGAN from architecture to codebook learning,\nyielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN\nfurther improves vector-quantized image modeling tasks, including\nunconditional, class-conditioned image generation and unsupervised\nrepresentation learning. When trained on ImageNet at 256x256 resolution, we\nachieve Inception Score (IS) of 175.1 and Fr'echet Inception Distance (FID) of\n4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and\n17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised\npretraining, we further evaluate the pretrained Transformer by averaging\nintermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained\nVIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 72.2%\nfor a similar model size. ViM-L also outperforms iGPT-XL which is trained with\nextra web image data and larger model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_A/0/1/0/all/0/1\">Alexander Ku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DenseNet approach to segmentation and classification of dermatoscopic skin lesions images. (arXiv:2110.04632v1 [eess.IV])","link":"http://arxiv.org/abs/2110.04632","description":"<p>At present, cancer is one of the most important health issues in the world.\nBecause early detection and appropriate treatment in cancer are very effective\nin the recovery and survival of patients, image processing as a diagnostic tool\ncan help doctors to diagnose in the first recognition of cancer. One of the\nmost important steps in diagnosing a skin lesion is to automatically detect the\nborder of the skin image because the accuracy of the next steps depends on it.\nIf these subtleties are identified, they can have a great impact on the\ndiagnosis of the disease. Therefore, there is a good opportunity to develop\nmore accurate algorithms to analyze such images. This paper proposes an\nimproved method for segmentation and classification for skin lesions using two\narchitectures, the U-Net for image segmentation and the DenseNet121 for image\nclassification which have excellent accuracy. We tested the segmentation\narchitecture of our model on the ISIC-2018 dataset and the classification on\nthe HAM10000 dataset. Our results show that the combination of U-Net and\nDenseNet121 architectures provides acceptable results in dermatoscopic image\nanalysis compared to previous research. Another classification examined in this\nstudy is cancerous and non-cancerous samples. In this classification, cancerous\nand non-cancerous samples were detected in DenseNet121 network with 79.49% and\n93.11% accuracy respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zare_R/0/1/0/all/0/1\">Reza Zare</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pourkazemi_A/0/1/0/all/0/1\">Arash Pourkazemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex Network-Based Approach for Feature Extraction and Classification of Musical Genres. (arXiv:2110.04654v1 [eess.AS])","link":"http://arxiv.org/abs/2110.04654","description":"<p>Musical genre's classification has been a relevant research topic. The\nassociation between music and genres is fundamental for the media industry,\nwhich manages musical recommendation systems, and for music streaming services,\nwhich may appear classified by genres. In this context, this work presents a\nfeature extraction method for the automatic classification of musical genres,\nbased on complex networks and their topological measurements. The proposed\nmethod initially converts the musics into sequences of musical notes and then\nmaps the sequences as complex networks. Topological measurements are extracted\nto characterize the network topology, which composes a feature vector that\napplies to the classification of musical genres. The method was evaluated in\nthe classification of 10 musical genres by adopting the GTZAN dataset and 8\nmusical genres by adopting the FMA dataset. The results were compared with\nmethods in the literature. The proposed method outperformed all compared\nmethods by presenting high accuracy and low standard deviation, showing its\nsuitability for the musical genre's classification, which contributes to the\nmedia industry in the automatic classification with assertiveness and\nrobustness. The proposed method is implemented in an open source in the Python\nlanguage and freely available at https://github.com/omatheuspimenta/examinner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pimenta_Zanon_M/0/1/0/all/0/1\">Matheus Henrique Pimenta-Zanon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bressan_G/0/1/0/all/0/1\">Glaucia Maria Bressan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lopes_F/0/1/0/all/0/1\">Fabr&#xed;cio Martins Lopes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-appearance-aided Differential Evolution for Motion Transfer. (arXiv:2110.04658v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04658","description":"<p>Image animation transfers the motion of a driving video to a static object in\na source image, while keeping the source identity unchanged. Great progress has\nbeen made in unsupervised motion transfer recently, where no labelled data or\nground truth domain priors are needed. However, current unsupervised approaches\nstill struggle when there are large motion or viewpoint discrepancies between\nthe source and driving images. In this paper, we introduce three measures that\nwe found to be effective for overcoming such large viewpoint changes. Firstly,\nto achieve more fine-grained motion deformation fields, we propose to apply\nNeural-ODEs for parametrizing the evolution dynamics of the motion transfer\nfrom source to driving. Secondly, to handle occlusions caused by large\nviewpoint and motion changes, we take advantage of the appearance flow obtained\nfrom the source image itself (\"self-appearance\"), which essentially \"borrows\"\nsimilar structures from other regions of an image to inpaint missing regions.\nFinally, our framework is also able to leverage the information from additional\nreference views which help to drive the source identity in spite of varying\nmotion state. Extensive experiments demonstrate that our approach outperforms\nthe state-of-the-arts by a significant margin (~40%), across six benchmarks\nvarying from human faces, human bodies to robots and cartoon characters. Model\ngenerality analysis indicates that our approach generalises the best across\ndifferent object categories as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peirong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xuefei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yipin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ashish Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oquab_M/0/1/0/all/0/1\">Maxime Oquab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couprie_C/0/1/0/all/0/1\">Camille Couprie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-Splits: Improved K-Means Clustering Algorithm to Automatically Detect the Number of Clusters. (arXiv:2110.04660v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04660","description":"<p>This paper introduces k-splits, an improved hierarchical algorithm based on\nk-means to cluster data without prior knowledge of the number of clusters.\nK-splits starts from a small number of clusters and uses the most significant\ndata distribution axis to split these clusters incrementally into better fits\nif needed. Accuracy and speed are two main advantages of the proposed method.\nWe experiment on six synthetic benchmark datasets plus two real-world datasets\nMNIST and Fashion-MNIST, to prove that our algorithm has excellent accuracy in\nfinding the correct number of clusters under different conditions. We also show\nthat k-splits is faster than similar methods and can even be faster than the\nstandard k-means in lower dimensions. Finally, we suggest using k-splits to\nuncover the exact position of centroids and then input them as initial points\nto the k-means algorithm to fine-tune the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1\">Seyed Omid Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalhor_A/0/1/0/all/0/1\">Ahmad Kalhor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodaghi_H/0/1/0/all/0/1\">Hossein Bodaghi</a> (University of Tehran, College of Engineering, School of Electrical and Computer Engineering, Tehran, Iran)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Road Extraction: A Dataset for Map Update using Aerial Images. (arXiv:2110.04690v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04690","description":"<p>The increasing availability of satellite and aerial imagery has sparked\nsubstantial interest in automatically updating street maps by processing aerial\nimages. Until now, the community has largely focused on road extraction, where\nroad networks are inferred from scratch from an aerial image. However, given\nthat relatively high-quality maps exist in most parts of the world, in\npractice, inference approaches must be applied to update existing maps rather\nthan infer new ones. With recent road extraction methods showing high accuracy,\nwe argue that it is time to transition to the more practical map update task,\nwhere an existing map is updated by adding, removing, and shifting roads,\nwithout introducing errors in parts of the existing map that remain up-to-date.\nIn this paper, we develop a new dataset called MUNO21 for the map update task,\nand show that it poses several new and interesting research challenges. We\nevaluate several state-of-the-art road extraction methods on MUNO21, and find\nthat substantial further improvements in accuracy will be needed to realize\nautomatic map update.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastani_F/0/1/0/all/0/1\">Favyen Bastani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madden_S/0/1/0/all/0/1\">Sam Madden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unauthorized AI cannot Recognize Me: Reversible Adversarial Example. (arXiv:1811.00189v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1811.00189","description":"<p>In this study, we propose a new methodology to control how user's data is\nrecognized and used by AI via exploiting the properties of adversarial\nexamples. For this purpose, we propose reversible adversarial example (RAE), a\nnew type of adversarial example. A remarkable feature of RAE is that the image\ncan be correctly recognized and used by the AI model specified by the user\nbecause the authorized AI can recover the original image from the RAE exactly\nby eliminating adversarial perturbation. On the other hand, other unauthorized\nAI models cannot recognize it correctly because it functions as an adversarial\nexample. Moreover, RAE can be considered as one type of encryption to computer\nvision since reversibility guarantees the decryption. To realize RAE, we\ncombine three technologies, adversarial example, reversible data hiding for\nexact recovery of adversarial perturbation, and encryption for selective\ncontrol of AIs who can remove adversarial perturbation. Experimental results\nshow that the proposed method can achieve comparable attack ability with the\ncorresponding adversarial attack method and similar visual quality with the\noriginal image, including white-box attacks and black-box attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukuchi_K/0/1/0/all/0/1\">Kazuto Fukuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akimoto_Y/0/1/0/all/0/1\">Youhei Akimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakuma_J/0/1/0/all/0/1\">Jun Sakuma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extreme Low Resolution Activity Recognition with Confident Spatial-Temporal Attention Transfer. (arXiv:1909.03580v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1909.03580","description":"<p>Activity recognition on extreme low-resolution videos, e.g., a resolution of\n12*16 pixels, plays a vital role in far-view surveillance and\nprivacy-preserving multimedia analysis. Low-resolution videos only contain\nlimited information. Given the fact that one same activity may be represented\nby videos in both high resolution (HR) and extreme low resolution (eLR), it is\nworth studying to utilize the relevant HR data to improve the eLR activity\nrecognition. In this work, we propose a novel Confident Spatial-Temporal\nAttention Transfer (CSTAT) for eLR activity recognition. CSTAT can acquire\ninformation from HR data by reducing the attention differences with a\ntransfer-learning strategy. Besides, the credibility of the supervisory signal\nis also taken into consideration for a more confident transferring process.\nExperimental results on two well-known datasets, i.e., UCF101 and HMDB51,\ndemonstrate that, the proposed method can effectively improve the accuracy of\neLR activity recognition and achieve an accuracy of 59.23% on 12*16 videos in\nHMDB51, a state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yucai Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xieyuanli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lingxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhengming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MILA: Multi-Task Learning from Videos via Efficient Inter-Frame Attention. (arXiv:2002.07362v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.07362","description":"<p>Prior work in multi-task learning has mainly focused on predictions on a\nsingle image. In this work, we present a new approach for multi-task learning\nfrom videos via efficient inter-frame local attention (MILA). Our approach\ncontains a novel inter-frame attention module which allows learning of\ntask-specific attention across frames. We embed the attention module in a\n``slow-fast'' architecture, where the slower network runs on sparsely sampled\nkeyframes and the light-weight shallow network runs on non-keyframes at a high\nframe rate. We also propose an effective adversarial learning strategy to\nencourage the slow and fast network to learn similar features. Our approach\nensures low-latency multi-task learning while maintaining high quality\npredictions. Experiments show competitive accuracy compared to state-of-the-art\non two multi-task learning benchmarks while reducing the number of floating\npoint operations (FLOPs) by up to 70\\%. In addition, our attention based\nfeature propagation method (ILA) outperforms prior work in terms of task\naccuracy while also reducing up to 90\\% of FLOPs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_C/0/1/0/all/0/1\">Chuhang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eledath_J/0/1/0/all/0/1\">Jayan Eledath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medioni_G/0/1/0/all/0/1\">Gerard Medioni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Structure Preserving Projection for Heterogeneous Domain Adaptation. (arXiv:2004.12427v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2004.12427","description":"<p>Heterogeneous Domain Adaptation (HDA) addresses the transfer learning\nproblems where data from the source and target domains are of different\nmodalities (e.g., texts and images) or feature dimensions (e.g., features\nextracted with different methods). It is useful for multi-modal data analysis.\nTraditional domain adaptation algorithms assume that the representations of\nsource and target samples reside in the same feature space, hence are likely to\nfail in solving the heterogeneous domain adaptation problem. Contemporary\nstate-of-the-art HDA approaches are usually composed of complex optimization\nobjectives for favourable performance and are therefore computationally\nexpensive and less generalizable. To address these issues, we propose a novel\nCross-Domain Structure Preserving Projection (CDSPP) algorithm for HDA. As an\nextension of the classic LPP to heterogeneous domains, CDSPP aims to learn\ndomain-specific projections to map sample features from source and target\ndomains into a common subspace such that the class consistency is preserved and\ndata distributions are sufficiently aligned. CDSPP is simple and has\ndeterministic solutions by solving a generalized eigenvalue problem. It is\nnaturally suitable for supervised HDA but has also been extended for\nsemi-supervised HDA where the unlabelled target domain samples are available.\nExtensive experiments have been conducted on commonly used benchmark datasets\n(i.e. Office-Caltech, Multilingual Reuters Collection, NUS-WIDE-ImageNet) for\nHDA as well as the Office-Home dataset firstly introduced for HDA by ourselves\ndue to its significantly larger number of classes than the existing ones (65 vs\n10, 6 and 8). The experimental results of both supervised and semi-supervised\nHDA demonstrate the superior performance of our proposed method against\ncontemporary state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1\">Toby P. Breckon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WaveFuse: A Unified Deep Framework for Image Fusion with Discrete Wavelet Transform. (arXiv:2007.14110v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.14110","description":"<p>We propose an unsupervised image fusion architecture for multiple application\nscenarios based on the combination of multi-scale discrete wavelet transform\nthrough regional energy and deep learning. To our best knowledge, this is the\nfirst time the conventional image fusion method has been combined with deep\nlearning. The useful information of feature maps can be utilized adequately\nthrough multi-scale discrete wavelet transform in our proposed method.Compared\nwith other state-of-the-art fusion method, the proposed algorithm exhibits\nbetter fusion performance in both subjective and objective evaluation.\nMoreover, it's worth mentioning that comparable fusion performance trained in\nCOCO dataset can be obtained by training with a much smaller dataset with only\nhundreds of images chosen randomly from COCO. Hence, the training time is\nshortened substantially, leading to the improvement of the model's performance\nboth in practicality and training efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaolei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Manning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhijian Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultAV: Multiplicative Adversarial Videos. (arXiv:2009.08058v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.08058","description":"<p>The majority of adversarial machine learning research focuses on additive\nattacks, which add adversarial perturbation to input data. On the other hand,\nunlike image recognition problems, only a handful of attack approaches have\nbeen explored in the video domain. In this paper, we propose a novel attack\nmethod against video recognition models, Multiplicative Adversarial Videos\n(MultAV), which imposes perturbation on video data by multiplication. MultAV\nhas different noise distributions to the additive counterparts and thus\nchallenges the defense methods tailored to resisting additive adversarial\nattacks. Moreover, it can be generalized to not only Lp-norm attacks with a new\nadversary constraint called ratio bound, but also different types of physically\nrealizable attacks. Experimental results show that the model adversarially\ntrained against additive attack is less robust to MultAV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Relational Image Captioning via Multi-task Triple-Stream Networks. (arXiv:2010.03855v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.03855","description":"<p>We introduce dense relational captioning, a novel image captioning task which\naims to generate multiple captions with respect to relational information\nbetween objects in a visual scene. Relational captioning provides explicit\ndescriptions for each relationship between object combinations. This framework\nis advantageous in both diversity and amount of information, leading to a\ncomprehensive image understanding based on relationships, e.g., relational\nproposal generation. For relational understanding between objects, the\npart-of-speech (POS; i.e., subject-object-predicate categories) can be a\nvaluable prior information to guide the causal sequence of words in a caption.\nWe enforce our framework to learn not only to generate captions but also to\nunderstand the POS of each word. To this end, we propose the multi-task\ntriple-stream network (MTTSNet) which consists of three recurrent units\nresponsible for each POS which is trained by jointly predicting the correct\ncaptions and POS for each word. In addition, we found that the performance of\nMTTSNet can be improved by modulating the object embeddings with an explicit\nrelational module. We demonstrate that our proposed model can generate more\ndiverse and richer captions, via extensive experimental analysis on large scale\ndatasets and several metrics. Then, we present applications of our framework to\nholistic image captioning, scene graph generation, and retrieval tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinsoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings. (arXiv:2010.08666v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.08666","description":"<p>Generalizing deep neural networks to new target domains is critical to their\nreal-world utility. In practice, it may be feasible to get some target data\nlabeled, but to be cost-effective it is desirable to select a\nmaximally-informative subset via active learning (AL). We study the problem of\nAL under a domain shift, called Active Domain Adaptation (Active DA). We\ndemonstrate how existing AL approaches based solely on model uncertainty or\ndiversity sampling are less effective for Active DA. We propose Clustering\nUncertainty-weighted Embeddings (CLUE), a novel label acquisition strategy for\nActive DA that performs uncertainty-weighted clustering to identify target\ninstances for labeling that are both uncertain under the model and diverse in\nfeature space. CLUE consistently outperforms competing label acquisition\nstrategies for Active DA and AL across learning settings on 6 diverse domain\nshifts for image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1\">Viraj Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1\">Arjun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Unsupervised Domain Adaptation for Semantic Segmentation. (arXiv:2010.09236v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.09236","description":"<p>Unsupervised Domain Adaptation (UDA) for semantic segmentation has been\nfavorably applied to real-world scenarios in which pixel-level labels are hard\nto be obtained. In most of the existing UDA methods, all target data are\nassumed to be introduced simultaneously. Yet, the data are usually presented\nsequentially in the real world. Moreover, Continual UDA, which deals with more\npractical scenarios with multiple target domains in the continual learning\nsetting, has not been actively explored. In this light, we propose Continual\nUDA for semantic segmentation based on a newly designed Expanding\nTarget-specific Memory (ETM) framework. Our novel ETM framework contains\nTarget-specific Memory (TM) for each target domain to alleviate catastrophic\nforgetting. Furthermore, a proposed Double Hinge Adversarial (DHA) loss leads\nthe network to produce better UDA performance overall. Our design of the TM and\ntraining objectives let the semantic segmentation network adapt to the current\ntarget domain while preserving the knowledge learned on previous target\ndomains. The model with the proposed framework outperforms other\nstate-of-the-art models in continual learning settings on standard benchmarks\nsuch as GTA5, SYNTHIA, CityScapes, IDD, and Cross-City datasets. The source\ncode is available at https://github.com/joonh-kim/ETM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joonhyuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Sahng-Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Gyeong-Moon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Hwan Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fair Knowledge Transfer for Imbalanced Domain Adaptation. (arXiv:2010.12184v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.12184","description":"<p>Domain adaptation (DA) becomes an up-and-coming technique to address the\ninsufficient or no annotation issue by exploiting external source knowledge.\nExisting DA algorithms mainly focus on practical knowledge transfer through\ndomain alignment. Unfortunately, they ignore the fairness issue when the\nauxiliary source is extremely imbalanced across different categories, which\nresults in severe under-presented knowledge adaptation of minority source set.\nTo this end, we propose a Towards Fair Knowledge Transfer (TFKT) framework to\nhandle the fairness challenge in imbalanced cross-domain learning.\nSpecifically, a novel cross-domain mixup generation is exploited to augment the\nminority source set with target information to enhance fairness. Moreover, dual\ndistinct classifiers and cross-domain prototype alignment are developed to seek\na more robust classifier boundary and mitigate the domain shift. Such three\nstrategies are formulated into a unified framework to address the fairness\nissue and domain shift challenge. Extensive experiments over two popular\nbenchmarks have verified the effectiveness of our proposed model by comparing\nto existing state-of-the-art DA models, and especially our model significantly\nimproves over 20% on two benchmarks in terms of the overall accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_T/0/1/0/all/0/1\">Taotao Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bingrong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhengming Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness May Be at Odds with Fairness: An Empirical Study on Class-wise Accuracy. (arXiv:2010.13365v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.13365","description":"<p>Convolutional neural networks (CNNs) have made significant advancement,\nhowever, they are widely known to be vulnerable to adversarial attacks.\nAdversarial training is the most widely used technique for improving\nadversarial robustness to strong white-box attacks. Prior works have been\nevaluating and improving the model average robustness without class-wise\nevaluation. The average evaluation alone might provide a false sense of\nrobustness. For example, the attacker can focus on attacking the vulnerable\nclass, which can be dangerous, especially, when the vulnerable class is a\ncritical one, such as \"human\" in autonomous driving. We propose an empirical\nstudy on the class-wise accuracy and robustness of adversarially trained\nmodels. We find that there exists inter-class discrepancy for accuracy and\nrobustness even when the training dataset has an equal number of samples for\neach class. For example, in CIFAR10, \"cat\" is much more vulnerable than other\nclasses. Moreover, this inter-class discrepancy also exists for normally\ntrained models, while adversarial training tends to further increase the\ndiscrepancy. Our work aims to investigate the following questions: (a) is the\nphenomenon of inter-class discrepancy universal regardless of datasets, model\narchitectures and optimization hyper-parameters? (b) If so, what can be\npossible explanations for the inter-class discrepancy? (c) Can the techniques\nproposed in the long tail classification be readily extended to adversarial\ntraining for addressing the inter-class discrepancy?\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benz_P/0/1/0/all/0/1\">Philipp Benz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karjauv_A/0/1/0/all/0/1\">Adil Karjauv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep-Dup: An Adversarial Weight Duplication Attack Framework to Crush Deep Neural Network in Multi-Tenant FPGA. (arXiv:2011.03006v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2011.03006","description":"<p>The wide deployment of Deep Neural Networks (DNN) in high-performance cloud\ncomputing platforms brought to light multi-tenant cloud field-programmable gate\narrays (FPGA) as a popular choice of accelerator to boost performance due to\nits hardware reprogramming flexibility. Such a multi-tenant FPGA setup for DNN\nacceleration potentially exposes DNN interference tasks under severe threat\nfrom malicious users. This work, to the best of our knowledge, is the first to\nexplore DNN model vulnerabilities in multi-tenant FPGAs. We propose a novel\nadversarial attack framework: Deep-Dup, in which the adversarial tenant can\ninject adversarial faults to the DNN model in the victim tenant of FPGA.\nSpecifically, she can aggressively overload the shared power distribution\nsystem of FPGA with malicious power-plundering circuits, achieving adversarial\nweight duplication (AWD) hardware attack that duplicates certain DNN weight\npackages during data transmission between off-chip memory and on-chip buffer,\nto hijack the DNN function of the victim tenant. Further, to identify the most\nvulnerable DNN weight packages for a given malicious objective, we propose a\ngeneric vulnerable weight package searching algorithm, called Progressive\nDifferential Evolution Search (P-DES), which is, for the first time, adaptive\nto both deep learning white-box and black-box attack models. The proposed\nDeep-Dup is experimentally validated in a developed multi-tenant FPGA\nprototype, for two popular deep learning applications, i.e., Object Detection\nand Image Classification. Successful attacks are demonstrated in six popular\nDNN architectures (e.g., YOLOv2, ResNet-50, MobileNet, etc.)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rakin_A/0/1/0/all/0/1\">Adnan Siraj Rakin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yukui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaolin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deliang Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Interpretable Classification and Weakly-Supervised Segmentation of Histology Images via Max-Min Uncertainty. (arXiv:2011.07221v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.07221","description":"<p>Weakly-supervised learning (WSL) has recently triggered substantial interest\nas it mitigates the lack of pixel-wise annotations. Given global image labels,\nWSL methods yield pixel-level predictions (segmentations), which enable to\ninterpret class predictions. Despite their recent success, mostly with natural\nimages, such methods can face important challenges when the foreground and\nbackground regions have similar visual cues, yielding high false-positive rates\nin segmentations, as is the case in challenging histology images. WSL training\nis commonly driven by standard classification losses, which implicitly maximize\nmodel confidence, and locate the discriminative regions linked to\nclassification decisions. Therefore, they lack mechanisms for modeling\nexplicitly non-discriminative regions and reducing false-positive rates. We\npropose novel regularization terms, which enable the model to seek both\nnon-discriminative and discriminative regions, while discouraging unbalanced\nsegmentations. We introduce high uncertainty as a criterion to localize\nnon-discriminative regions that do not affect classifier decision, and describe\nit with original Kullback-Leibler (KL) divergence losses evaluating the\ndeviation of posterior predictions from the uniform distribution. Our KL terms\nencourage high uncertainty of the model when the latter inputs the latent\nnon-discriminative regions. Our loss integrates: (i) a cross-entropy seeking a\nforeground, where model confidence about class prediction is high; (ii) a KL\nregularizer seeking a background, where model uncertainty is high; and (iii)\nlog-barrier terms discouraging unbalanced segmentations. Comprehensive\nexperiments and ablation studies over the public GlaS colon cancer data and a\nCamelyon16 patch-based benchmark for breast cancer show substantial\nimprovements over state-of-the-art WSL methods, and confirm the effect of our\nnew regularizers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Drone LAMS: A Drone-based Face Detection Dataset with Large Angles and Many Scenarios. (arXiv:2011.07689v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.07689","description":"<p>This work presented a new drone-based face detection dataset Drone LAMS in\norder to solve issues of low performance of drone-based face detection in\nscenarios such as large angles which was a predominant working condition when a\ndrone flies high. The proposed dataset captured images from 261 videos with\nover 43k annotations and 4.0k images with pitch or yaw angle in the range of\n-90{\\deg} to 90{\\deg}. Drone LAMS showed significant improvement over currently\navailable drone-based face detection datasets in terms of detection\nperformance, especially with large pitch and yaw angle. Detailed analysis of\nhow key factors, such as duplication rate, annotation method, etc., impact\ndataset performance was also provided to facilitate further usage of a drone on\nface detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yi Luo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyi Chen</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">X.-G. Ma</a> (2) ((1) School of Energy and Environment, Southeast University, Nanjing, China (2) International Institute for Urban Systems Engineering, Southeast University, Nanjing, China)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency-based segmentation of dermoscopic images using color information. (arXiv:2011.13179v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2011.13179","description":"<p>Skin lesion segmentation is one of the crucial steps for an efficient\nnon-invasive computer-aided early diagnosis of melanoma. This paper\ninvestigates how color information, besides saliency, can be used to determine\nthe pigmented lesion region automatically. Unlike most existing segmentation\nmethods using only the saliency in order to discriminate against the skin\nlesion from the surrounding regions, we propose a novel method employing a\nbinarization process coupled with new perceptual criteria, inspired by the\nhuman visual perception, related to the properties of saliency and color of the\ninput image data distribution. As a means of refining the accuracy of the\nproposed method, the segmentation step is preceded by a pre-processing aimed at\nreducing the computation burden, removing artifacts, and improving contrast. We\nhave assessed the method on two public databases, including 1497 dermoscopic\nimages. We have also compared its performance with classical and recent\nsaliency-based methods designed explicitly for dermoscopic images. The\nqualitative and quantitative evaluation indicates that the proposed method is\npromising since it produces an accurate skin lesion segmentation and performs\nsatisfactorily compared to other existing saliency-based segmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ramella_G/0/1/0/all/0/1\">Giuliana Ramella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S2FGAN: Semantically Aware Interactive Sketch-to-Face Translation. (arXiv:2011.14785v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14785","description":"<p>Interactive facial image manipulation attempts to edit single and multiple\nface attributes using a photo-realistic face and/or semantic mask as input. In\nthe absence of the photo-realistic image (only sketch/mask available), previous\nmethods only retrieve the original face but ignore the potential of aiding\nmodel controllability and diversity in the translation process. This paper\nproposes a sketch-to-image generation framework called S2FGAN, aiming to\nimprove users' ability to interpret and flexibility of face attribute editing\nfrom a simple sketch. The proposed framework modifies the constrained latent\nspace semantics trained on Generative Adversarial Networks (GANs). We employ\ntwo latent spaces to control the face appearance and adjust the desired\nattributes of the generated face. Instead of constraining the translation\nprocess by using a reference image, the users can command the model to retouch\nthe generated images by involving the semantic information in the generation\nprocess. In this way, our method can manipulate single or multiple face\nattributes by only specifying attributes to be changed. Extensive experimental\nresults on CelebAMask-HQ dataset empirically shows our superior performance and\neffectiveness on this task. Our method successfully outperforms\nstate-of-the-art methods on attribute manipulation by exploiting greater\ncontrol of attribute intensity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md Zakir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1\">Shafin Rahman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Adversarial Attacks and Defenses on 3D Point Clouds. (arXiv:2012.05657v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05657","description":"<p>Deep neural networks are prone to adversarial examples that maliciously alter\nthe network's outcome. Due to the increasing popularity of 3D sensors in\nsafety-critical systems and the vast deployment of deep learning models for 3D\npoint sets, there is a growing interest in adversarial attacks and defenses for\nsuch models. So far, the research has focused on the semantic level, namely,\ndeep point cloud classifiers. However, point clouds are also widely used in a\ngeometric-related form that includes encoding and reconstructing the geometry.\nIn this work, we are the first to consider the problem of adversarial examples\nat a geometric level. In this setting, the question is how to craft a small\nchange to a clean source point cloud that leads, after passing through an\nautoencoder model, to the reconstruction of a different target shape. Our\nattack is in sharp contrast to existing semantic attacks on 3D point clouds.\nWhile such works aim to modify the predicted label by a classifier, we alter\nthe entire reconstructed geometry. Additionally, we demonstrate the robustness\nof our attack in the case of defense, where we show that remnant\ncharacteristics of the target shape are still present at the output after\napplying the defense to the adversarial input. Our code is publicly available\nat https://github.com/itailang/geometric_adv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_I/0/1/0/all/0/1\">Itai Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotlicki_U/0/1/0/all/0/1\">Uriel Kotlicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1\">Shai Avidan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EventHands: Real-Time Neural 3D Hand Pose Estimation from an Event Stream. (arXiv:2012.06475v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.06475","description":"<p>3D hand pose estimation from monocular videos is a long-standing and\nchallenging problem, which is now seeing a strong upturn. In this work, we\naddress it for the first time using a single event camera, i.e., an\nasynchronous vision sensor reacting on brightness changes. Our EventHands\napproach has characteristics previously not demonstrated with a single RGB or\ndepth camera such as high temporal resolution at low data throughputs and\nreal-time performance at 1000 Hz. Due to the different data modality of event\ncameras compared to classical cameras, existing methods cannot be directly\napplied to and re-trained for event streams. We thus design a new neural\napproach which accepts a new event stream representation suitable for learning,\nwhich is trained on newly-generated synthetic event streams and can generalise\nto real data. Experiments show that EventHands outperforms recent monocular\nmethods using a colour (or depth) camera in terms of accuracy and its ability\nto capture hand motions of unprecedented speed. Our method, the event stream\nsimulator and the dataset are publicly available; see\nhttps://4dqv.mpi-inf.mpg.de/EventHands/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rudnev_V/0/1/0/all/0/1\">Viktor Rudnev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidel_H/0/1/0/all/0/1\">Hans-Peter Seidel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1\">Franziska Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1\">Mohamed Elgharib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentional Biased Stochastic Gradient for Imbalanced Classification. (arXiv:2012.06951v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.06951","description":"<p>In this paper, we present a simple yet effective method (ABSGD) for\naddressing the data imbalance issue in deep learning. Our method is a simple\nmodification to momentum SGD where we leverage an attentional mechanism to\nassign an individual importance weight to each gradient in the mini-batch.\nUnlike many existing heuristic-driven methods for tackling data imbalance, our\nmethod is grounded in {\\it theoretically justified distributionally robust\noptimization (DRO)}, which is guaranteed to converge to a stationary point of\nan information-regularized DRO problem. The individual-level weight of a\nsampled data is systematically proportional to the exponential of a scaled loss\nvalue of the data, where the scaling factor is interpreted as the\nregularization parameter in the framework of information-regularized DRO.\nCompared with existing class-level weighting schemes, our method can capture\nthe diversity between individual examples within each class. Compared with\nexisting individual-level weighting methods using meta-learning that require\nthree backward propagations for computing mini-batch stochastic gradients, our\nmethod is more efficient with only one backward propagation at each iteration\nas in standard deep learning methods. To balance between the learning of\nfeature extraction layers and the learning of the classifier layer, we employ a\ntwo-stage method that uses SGD for pretraining followed by ABSGD for learning a\nrobust classifier and finetuning lower layers. Our empirical studies on several\nbenchmark datasets demonstrate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qi Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wotao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Block-based Hybrid Image Compression. (arXiv:2012.09550v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.09550","description":"<p>Recent works on learned image compression perform encoding and decoding\nprocesses in a full-resolution manner, resulting in two problems when deployed\nfor practical applications. First, parallel acceleration of the autoregressive\nentropy model cannot be achieved due to serial decoding. Second,\nfull-resolution inference often causes the out-of-memory(OOM) problem with\nlimited GPU resources, especially for high-resolution images. Block partition\nis a good design choice to handle the above issues, but it brings about new\nchallenges in reducing the redundancy between blocks and eliminating block\neffects. To tackle the above challenges, this paper provides a learned\nblock-based hybrid image compression (LBHIC) framework. Specifically, we\nintroduce explicit intra prediction into a learned image compression framework\nto utilize the relation among adjacent blocks. Superior to context modeling by\nlinear weighting of neighbor pixels in traditional codecs, we propose a\ncontextual prediction module (CPM) to better capture long-range correlations by\nutilizing the strip pooling to extract the most relevant information in\nneighboring latent space, thus achieving effective information prediction.\nMoreover, to alleviate blocking artifacts, we further propose a boundary-aware\npostprocessing module (BPM) with the edge importance taken into account.\nExtensive experiments demonstrate that the proposed LBHIC codec outperforms the\nVVC, with a bit-rate conservation of 4.1%, and reduces the decoding time by\napproximately 86.7% compared with that of state-of-the-art learned image\ncompression methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yaojun Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SENTRY: Selective Entropy Optimization via Committee Consistency for Unsupervised Domain Adaptation. (arXiv:2012.11460v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.11460","description":"<p>Many existing approaches for unsupervised domain adaptation (UDA) focus on\nadapting under only data distribution shift and offer limited success under\nadditional cross-domain label distribution shift. Recent work based on\nself-training using target pseudo-labels has shown promise, but on challenging\nshifts pseudo-labels may be highly unreliable, and using them for self-training\nmay cause error accumulation and domain misalignment. We propose Selective\nEntropy Optimization via Committee Consistency (SENTRY), a UDA algorithm that\njudges the reliability of a target instance based on its predictive consistency\nunder a committee of random image transformations. Our algorithm then\nselectively minimizes predictive entropy to increase confidence on highly\nconsistent target instances, while maximizing predictive entropy to reduce\nconfidence on highly inconsistent ones. In combination with pseudo-label based\napproximate target class balancing, our approach leads to significant\nimprovements over the state-of-the-art on 27/31 domain shifts from standard UDA\nbenchmarks as well as benchmarks designed to stress-test adaptation under label\ndistribution shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1\">Viraj Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khare_S/0/1/0/all/0/1\">Shivam Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kartik_D/0/1/0/all/0/1\">Deeksha Kartik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Efficient Hierarchical Neural Architecture Search for Image Restoration. (arXiv:2012.13212v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.13212","description":"<p>Recently, much attention has been spent on neural architecture search (NAS),\naiming to outperform those manually-designed neural architectures on high-level\nvision recognition tasks. Inspired by the success, here we attempt to leverage\nNAS techniques to automatically design efficient network architectures for\nlow-level image restoration tasks. In particular, we propose a memory-efficient\nhierarchical NAS (termed HiNAS) and apply it to two such tasks: image denoising\nand image super-resolution. HiNAS adopts gradient based search strategies and\nbuilds a flexible hierarchical search space, including the inner search space\nand outer search space. They are in charge of designing cell architectures and\ndeciding cell widths, respectively. For the inner search space, we propose a\nlayer-wise architecture sharing strategy (LWAS), resulting in more flexible\narchitectures and better performance. For the outer search space, we design a\ncell-sharing strategy to save memory, and considerably accelerate the search\nspeed. The proposed HiNAS method is both memory and computation efficient. With\na single GTX1080Ti GPU, it takes only about 1 hour for searching for denoising\nnetwork on the BSD-500 dataset and 3.5 hours for searching for the\nsuper-resolution structure on the DIV2K dataset. Experiments show that the\narchitectures found by HiNAS have fewer parameters and enjoy a faster inference\nspeed, while achieving highly competitive performance compared with\nstate-of-the-art methods. Code is available at:\nhttps://github.com/hkzhang91/HiNAS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Ying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chengrong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zongwen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic 3D Multi-Modal, Multi-Object Tracking for Autonomous Driving. (arXiv:2012.13755v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.13755","description":"<p>Multi-object tracking is an important ability for an autonomous vehicle to\nsafely navigate a traffic scene. Current state-of-the-art follows the\ntracking-by-detection paradigm where existing tracks are associated with\ndetected objects through some distance metric. The key challenges to increase\ntracking accuracy lie in data association and track life cycle management. We\npropose a probabilistic, multi-modal, multi-object tracking system consisting\nof different trainable modules to provide robust and data-driven tracking\nresults. First, we learn how to fuse features from 2D images and 3D LiDAR point\nclouds to capture the appearance and geometric information of an object.\nSecond, we propose to learn a metric that combines the Mahalanobis and feature\ndistances when comparing a track and a new detection in data association. And\nthird, we propose to learn when to initialize a track from an unmatched object\ndetection. Through extensive quantitative and qualitative results, we show that\nwhen using the same object detectors our method outperforms state-of-the-art\napproaches on the NuScenes and KITTI datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Hsu-kuang Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1\">Jeannette Bohg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spending Your Winning Lottery Better After Drawing It. (arXiv:2101.03255v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.03255","description":"<p>Lottery Ticket Hypothesis (LTH) suggests that a dense neural network contains\na sparse sub-network that can match the performance of the original dense\nnetwork when trained in isolation from scratch. Most works retrain the sparse\nsub-network with the same training protocols as its dense network, such as\ninitialization, architecture blocks, and training recipes. However, till now it\nis unclear that whether these training protocols are optimal for sparse\nnetworks.\n</p>\n<p>In this paper, we demonstrate that it is unnecessary for spare retraining to\nstrictly inherit those properties from the dense network. Instead, by plugging\nin purposeful \"tweaks\" of the sparse subnetwork architecture or its training\nrecipe, its retraining can be significantly improved than the default,\nespecially at high sparsity levels. Combining all our proposed \"tweaks\" can\nyield the new state-of-the-art performance of LTH, and these modifications can\nbe easily adapted to other sparse training algorithms in general. Specifically,\nwe have achieved a significant and consistent performance gain of1.05% - 4.93%\nfor ResNet18 on CIFAR-100 over vanilla-LTH. Moreover, our methods are shown to\ngeneralize across datasets (CIFAR10, CIFAR100, TinyImageNet) and architectures\n(Vgg16, ResNet-18/ResNet-34, MobileNet). All codes will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Ajay Kumar Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Target Detection and Segmentation in Circular-Scan Synthetic-Aperture-Sonar Images using Semi-Supervised Convolutional Encoder-Decoders. (arXiv:2101.03603v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.03603","description":"<p>We propose a framework for saliency-based, multi-target detection and\nsegmentation of circular-scan, synthetic-aperture-sonar (CSAS) imagery. Our\nframework relies on a multi-branch, convolutional encoder-decoder network ({\\sc\nMB-CEDN}). The encoder portion of the {\\sc MB-CEDN} extracts visual contrast\nfeatures from CSAS images. These features are fed into dual decoders that\nperform pixel-level segmentation to mask targets. Each decoder provides\ndifferent perspectives as to what constitutes a salient target. These opinions\nare aggregated and cascaded into a deep-parsing network to refine the\nsegmentation.\n</p>\n<p>We evaluate our framework using real-world CSAS imagery consisting of five\nbroad target classes. We compare against existing approaches from the\ncomputer-vision literature. We show that our framework outperforms supervised,\ndeep-saliency networks designed for natural imagery. It greatly outperforms\nunsupervised saliency approaches developed for natural imagery. This\nillustrates that natural-image-based models may need to be altered to be\neffective for this imaging-sonar modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1\">Isaac J. Sledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emigh_M/0/1/0/all/0/1\">Matthew S. Emigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_J/0/1/0/all/0/1\">Jonathan L. King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woods_D/0/1/0/all/0/1\">Denton L. Woods</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobb_J/0/1/0/all/0/1\">J. Tory Cobb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1\">Jose C. Principe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classic versus deep learning approaches to address computer vision challenges. (arXiv:2101.09744v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.09744","description":"<p>Computer vision and image processing address many challenging applications.\nWhile the last decade has seen deep neural network architectures\nrevolutionizing those fields, early methods relied on 'classic', i.e.,\nnon-learned approaches. In this study, we explore the differences between\nclassic and deep learning (DL) algorithms to gain new insight regarding which\nis more suitable for a given application. The focus is on two challenging\nill-posed problems, namely faint edge detection and multispectral image\nregistration, studying recent state-of-the-art DL and classic solutions. While\nthose DL algorithms outperform classic methods in terms of accuracy and\ndevelopment time, they tend to have higher resource requirements and are unable\nto perform outside their training space. Moreover, classic algorithms are more\ntransparent, which facilitates their adoption for real-life applications. As\nboth classes of approaches have unique strengths and limitations, the choice of\na solution is clearly application dependent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ofir_N/0/1/0/all/0/1\">Nati Ofir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nebel_J/0/1/0/all/0/1\">Jean-Christophe Nebel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-Hairstyle: A Large-scale Korean Hairstyle Dataset for Virtual Hair Editing and Hairstyle Classification. (arXiv:2102.06288v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.06288","description":"<p>The hair and beauty industry is a fast-growing industry. This led to the\ndevelopment of various applications, such as virtual hair dyeing or hairstyle\ntransfer, to satisfy the customer's needs. Although several hairstyle datasets\nare available for these applications, they often consist of a relatively small\nnumber of images with low resolution, thus limiting their performance on\nhigh-quality hair editing. In response, we introduce a novel large-scale Korean\nhairstyle dataset, K-hairstyle, containing 500,000 high-resolution images. In\naddition, K-hairstyle includes various hair attributes annotated by Korean\nexpert hairstylists as well as hair segmentation masks. We validate the\neffectiveness of our dataset via several applications, such as hair dyeing,\nhairstyle transfer, and hairstyle classification. K-hairstyle is publicly\navailable at https://psh01087.github.io/K-Hairstyle/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_C/0/1/0/all/0/1\">Chaeyeon Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1\">Gyojung Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_K/0/1/0/all/0/1\">Keonmin Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_W/0/1/0/all/0/1\">Wonzo Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaesung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A General Descent Aggregation Framework for Gradient-based Bi-level Optimization. (arXiv:2102.07976v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.07976","description":"<p>In recent years, a variety of gradient-based methods have been developed to\nsolve Bi-Level Optimization (BLO) problems in machine learning and computer\nvision areas. However, the theoretical correctness and practical effectiveness\nof these existing approaches always rely on some restrictive conditions (e.g.,\nLower-Level Singleton, LLS), which could hardly be satisfied in real-world\napplications. Moreover, previous literature only proves theoretical results\nbased on their specific iteration strategies, thus lack a general recipe to\nuniformly analyze the convergence behaviors of different gradient-based BLOs.\nIn this work, we formulate BLOs from an optimistic bi-level viewpoint and\nestablish a new gradient-based algorithmic framework, named Bi-level Descent\nAggregation (BDA), to partially address the above issues. Specifically, BDA\nprovides a modularized structure to hierarchically aggregate both the upper-\nand lower-level subproblems to generate our bi-level iterative dynamics.\nTheoretically, we establish a general convergence analysis template and derive\na new proof recipe to investigate the essential theoretical properties of\ngradient-based BLO methods. Furthermore, this work systematically explores the\nconvergence behavior of BDA in different optimization scenarios, i.e.,\nconsidering various solution qualities (i.e., global/local/stationary solution)\nreturned from solving approximation subproblems. Extensive experiments justify\nour theoretical results and demonstrate the superiority of the proposed\nalgorithm for hyper-parameter optimization and meta-learning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_P/0/1/0/all/0/1\">Pan Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoming Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shangzhi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persistent Homology and Graphs Representation Learning. (arXiv:2102.12926v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.12926","description":"<p>This article aims to study the topological invariant properties encoded in\nnode graph representational embeddings by utilizing tools available in\npersistent homology. Specifically, given a node embedding representation\nalgorithm, we consider the case when these embeddings are real-valued. By\nviewing these embeddings as scalar functions on a domain of interest, we can\nutilize the tools available in persistent homology to study the topological\ninformation encoded in these representations. Our construction effectively\ndefines a unique persistence-based graph descriptor, on both the graph and node\nlevels, for every node representation algorithm. To demonstrate the\neffectiveness of the proposed method, we study the topological descriptors\ninduced by DeepWalk, Node2Vec and Diff2Vec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1\">Mustafa Hajij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xuanting Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Application of Image-to-Image Translation: Chromosome Straightening Framework by Learning from a Single Image. (arXiv:2103.02835v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.02835","description":"<p>In medical imaging, chromosome straightening plays a significant role in the\npathological study of chromosomes and in the development of cytogenetic maps.\nWhereas different approaches exist for the straightening task, typically\ngeometric algorithms are used whose outputs are characterized by jagged edges\nor fragments with discontinued banding patterns. To address the flaws in the\ngeometric algorithms, we propose a novel framework based on image-to-image\ntranslation to learn a pertinent mapping dependence for synthesizing\nstraightened chromosomes with uninterrupted banding patterns and preserved\ndetails. In addition, to avoid the pitfall of deficient input chromosomes, we\nconstruct an augmented dataset using only one single curved chromosome image\nfor training models. Based on this framework, we apply two popular\nimage-to-image translation architectures, U-shape networks and conditional\ngenerative adversarial networks, to assess its efficacy. Experiments on a\ndataset comprised of 642 real-world chromosomes demonstrate the superiority of\nour framework, as compared to the geometric method in straightening\nperformance, by rendering realistic and continued chromosome details.\nFurthermore, our straightened results improve the chromosome classification by\n0.98%-1.39% mean accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Sifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Daiyun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yalun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chunxiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_J/0/1/0/all/0/1\">Jia Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coenen_F/0/1/0/all/0/1\">Frans Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jionglong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High Perceptual Quality Image Denoising with a Posterior Sampling CGAN. (arXiv:2103.04192v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04192","description":"<p>The vast work in Deep Learning (DL) has led to a leap in image denoising\nresearch. Most DL solutions for this task have chosen to put their efforts on\nthe denoiser's architecture while maximizing distortion performance. However,\ndistortion driven solutions lead to blurry results with sub-optimal perceptual\nquality, especially in immoderate noise levels. In this paper we propose a\ndifferent perspective, aiming to produce sharp and visually pleasing denoised\nimages that are still faithful to their clean sources. Formally, our goal is to\nachieve high perceptual quality with acceptable distortion. This is attained by\na stochastic denoiser that samples from the posterior distribution, trained as\na generator in the framework of conditional generative adversarial networks\n(CGAN). Contrary to distortion-based regularization terms that conflict with\nperceptual quality, we introduce to the CGAN objective a theoretically founded\npenalty term that does not force a distortion requirement on individual\nsamples, but rather on their mean. We showcase our proposed method with a novel\ndenoiser architecture that achieves the reformed denoising goal and produces\nvivid and diverse outcomes in immoderate noise levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohayon_G/0/1/0/all/0/1\">Guy Ohayon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adrai_T/0/1/0/all/0/1\">Theo Adrai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaksman_G/0/1/0/all/0/1\">Gregory Vaksman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrated and Partially Calibrated Semi-Generalized Homographies. (arXiv:2103.06535v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.06535","description":"<p>In this paper, we propose the first minimal solutions for estimating the\nsemi-generalized homography given a perspective and a generalized camera. The\nproposed solvers use five 2D-2D image point correspondences induced by a scene\nplane. One of them assumes the perspective camera to be fully calibrated, while\nthe other solver estimates the unknown focal length together with the absolute\npose parameters. This setup is particularly important in structure-from-motion\nand image-based localization pipelines, where a new camera is localized in each\nstep with respect to a set of known cameras and 2D-3D correspondences might not\nbe available. As a consequence of a clever parametrization and the elimination\nideal method, our approach only needs to solve a univariate polynomial of\ndegree five or three. The proposed solvers are stable and efficient as\ndemonstrated by a number of synthetic and real-world experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhayani_S/0/1/0/all/0/1\">Snehal Bhayani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1\">Torsten Sattler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1\">Daniel Barath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beliansky_P/0/1/0/all/0/1\">Patrik Beliansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukelova_Z/0/1/0/all/0/1\">Zuzana Kukelova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Amend Facial Expression Representation via De-albino and Affinity. (arXiv:2103.10189v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.10189","description":"<p>Facial Expression Recognition (FER) is a classification task that points to\nface variants. Hence, there are certain affinity features between facial\nexpressions, receiving little attention in the FER literature. Convolution\npadding, despite helping capture the edge information, causes erosion of the\nfeature map simultaneously. After multi-layer filling convolution, the output\nfeature map named albino feature definitely weakens the representation of the\nexpression. To tackle these challenges, we propose a novel architecture named\nAmending Representation Module (ARM). ARM is a substitute for the pooling\nlayer. Theoretically, it can be embedded in the back end of any network to deal\nwith the Padding Erosion. ARM efficiently enhances facial expression\nrepresentation from two different directions: 1) reducing the weight of eroded\nfeatures to offset the side effect of padding, and 2) decomposing facial\nfeatures to simplify representation learning. Experiments on public benchmarks\nprove that our ARM boosts the performance of FER remarkably. The validation\naccuracies are respectively 90.42% on RAF-DB, 65.2% on Affect-Net, and 58.71%\non SFEW, exceeding current state-of-the-art methods. Our implementation and\ntrained models are available at\nhttps://github.com/JiaweiShiCV/Amend-Representation-Module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiawei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Songhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhiwei Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNETR: Transformers for 3D Medical Image Segmentation. (arXiv:2103.10504v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.10504","description":"<p>Fully Convolutional Neural Networks (FCNNs) with contracting and expanding\npaths have shown prominence for the majority of medical image segmentation\napplications since the past decade. In FCNNs, the encoder plays an integral\nrole by learning both global and local features and contextual representations\nwhich can be utilized for semantic output prediction by the decoder. Despite\ntheir success, the locality of convolutional layers in FCNNs, limits the\ncapability of learning long-range spatial dependencies. Inspired by the recent\nsuccess of transformers for Natural Language Processing (NLP) in long-range\nsequence learning, we reformulate the task of volumetric (3D) medical image\nsegmentation as a sequence-to-sequence prediction problem. We introduce a novel\narchitecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer\nas the encoder to learn sequence representations of the input volume and\neffectively capture the global multi-scale information, while also following\nthe successful \"U-shaped\" network design for the encoder and decoder. The\ntransformer encoder is directly connected to a decoder via skip connections at\ndifferent resolutions to compute the final semantic segmentation output. We\nhave validated the performance of our method on the Multi Atlas Labeling Beyond\nThe Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical\nSegmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation\ntasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV\nleaderboard. Code: https://monai.io/research/unetr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nath_V/0/1/0/all/0/1\">Vishwesh Nath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myronenko_A/0/1/0/all/0/1\">Andriy Myronenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett Landman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthesis of Compositional Animations from Textual Descriptions. (arXiv:2103.14675v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14675","description":"<p>\"How can we animate 3D-characters from a movie script or move robots by\nsimply telling them what we would like them to do?\" \"How unstructured and\ncomplex can we make a sentence and still generate plausible movements from it?\"\nThese are questions that need to be answered in the long-run, as the field is\nstill in its infancy. Inspired by these problems, we present a new technique\nfor generating compositional actions, which handles complex input sentences.\nOur output is a 3D pose sequence depicting the actions in the input sentence.\nWe propose a hierarchical two-stream sequential model to explore a finer\njoint-level mapping between natural language sentences and 3D pose sequences\ncorresponding to the given motion. We learn two manifold representations of the\nmotion -- one each for the upper body and the lower body movements. Our model\ncan generate plausible pose sequences for short sentences describing single\nactions as well as long compositional sentences describing multiple sequential\nand superimposed actions. We evaluate our proposed model on the publicly\navailable KIT Motion-Language Dataset containing 3D pose data with\nhuman-annotated sentences. Experimental results show that our model advances\nthe state-of-the-art on text-based motion synthesis in objective evaluations by\na margin of 50%. Qualitative evaluations based on a user study indicate that\nour synthesized motions are perceived to be the closest to the ground-truth\nmotion captures for both short and compositional sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Anindita Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheema_N/0/1/0/all/0/1\">Noshaba Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_C/0/1/0/all/0/1\">Cennet Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slusallek_P/0/1/0/all/0/1\">Philipp Slusallek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards High Fidelity Monocular Face Reconstruction with Rich Reflectance using Self-supervised Learning and Ray Tracing. (arXiv:2103.15432v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15432","description":"<p>Robust face reconstruction from monocular image in general lighting\nconditions is challenging. Methods combining deep neural network encoders with\ndifferentiable rendering have opened up the path for very fast monocular\nreconstruction of geometry, lighting and reflectance. They can also be trained\nin self-supervised manner for increased robustness and better generalization.\nHowever, their differentiable rasterization based image formation models, as\nwell as underlying scene parameterization, limit them to Lambertian face\nreflectance and to poor shape details. More recently, ray tracing was\nintroduced for monocular face reconstruction within a classic\noptimization-based framework and enables state-of-the art results. However\noptimization-based approaches are inherently slow and lack robustness. In this\npaper, we build our work on the aforementioned approaches and propose a new\nmethod that greatly improves reconstruction quality and robustness in general\nscenes. We achieve this by combining a CNN encoder with a differentiable ray\ntracer, which enables us to base the reconstruction on much more advanced\npersonalized diffuse and specular albedos, a more sophisticated illumination\nmodel and a plausible representation of self-shadows. This enables to take a\nbig leap forward in reconstruction quality of shape, appearance and lighting\neven in scenes with difficult illumination. With consistent face attributes\nreconstruction, our method leads to practical applications such as relighting\nand self-shadows removal. Compared to state-of-the-art methods, our results\nshow improved accuracy and validity of the approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dib_A/0/1/0/all/0/1\">Abdallah Dib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thebault_C/0/1/0/all/0/1\">Cedric Thebault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">Junghyun Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosselin_P/0/1/0/all/0/1\">Philippe-Henri Gosselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chevallier_L/0/1/0/all/0/1\">Louis Chevallier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Procrustean Training for Imbalanced Deep Learning. (arXiv:2104.01769v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.01769","description":"<p>Neural networks trained with class-imbalanced data are known to perform\npoorly on minor classes of scarce training data. Several recent works attribute\nthis to over-fitting to minor classes. In this paper, we provide a novel\nexplanation of this issue. We found that a neural network tends to first\nunder-fit the minor classes by classifying most of their data into the major\nclasses in early training epochs. To correct these wrong predictions, the\nneural network then must focus on pushing features of minor class data across\nthe decision boundaries between major and minor classes, leading to much larger\ngradients for features of minor classes. We argue that such an under-fitting\nphase over-emphasizes the competition between major and minor classes, hinders\nthe neural network from learning the discriminative knowledge that can be\ngeneralized to test data, and eventually results in over-fitting. To address\nthis issue, we propose a novel learning strategy to equalize the training\nprogress across classes. We mix features of the major class data with those of\nother data in a mini-batch, intentionally weakening their features to prevent a\nneural network from fitting them first. We show that this strategy can largely\nbalance the training accuracy and feature gradients across classes, effectively\nmitigating the under-fitting then over-fitting problem for minor class data. On\nseveral benchmark datasets, our approach achieves the state-of-the-art\naccuracy, especially for the challenging step-imbalanced cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Fashion Similarity Prediction by Attribute-Specific Embedding Learning. (arXiv:2104.02429v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02429","description":"<p>This paper strives to predict fine-grained fashion similarity. In this\nsimilarity paradigm, one should pay more attention to the similarity in terms\nof a specific design/attribute between fashion items. For example, whether the\ncollar designs of the two clothes are similar. It has potential value in many\nfashion related applications, such as fashion copyright protection. To this\nend, we propose an Attribute-Specific Embedding Network (ASEN) to jointly learn\nmultiple attribute-specific embeddings, thus measure the fine-grained\nsimilarity in the corresponding space. The proposed ASEN is comprised of a\nglobal branch and a local branch. The global branch takes the whole image as\ninput to extract features from a global perspective, while the local branch\ntakes as input the zoomed-in region-of-interest (RoI) w.r.t. the specified\nattribute thus able to extract more fine-grained features. As the global branch\nand the local branch extract the features from different perspectives, they are\ncomplementary to each other. Additionally, in each branch, two attention\nmodules, i.e., Attribute-aware Spatial Attention and Attribute-aware Channel\nAttention, are integrated to make ASEN be able to locate the related regions\nand capture the essential patterns under the guidance of the specified\nattribute, thus make the learned attribute-specific embeddings better reflect\nthe fine-grained similarity. Extensive experiments on three fashion-related\ndatasets, i.e., FashionAI, DARN, and DeepFashion, show the effectiveness of\nASEN for fine-grained fashion similarity prediction and its potential for\nfashion reranking. Code and data are available at\nhttps://github.com/maryeon/asenpp .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jianfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaofeng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Richang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes. (arXiv:2104.03953v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03953","description":"<p>Neural implicit surface representations have emerged as a promising paradigm\nto capture 3D shapes in a continuous and resolution-independent manner.\nHowever, adapting them to articulated shapes is non-trivial. Existing\napproaches learn a backward warp field that maps deformed to canonical points.\nHowever, this is problematic since the backward warp field is pose dependent\nand thus requires large amounts of data to learn. To address this, we introduce\nSNARF, which combines the advantages of linear blend skinning (LBS) for\npolygonal meshes with those of neural implicit surfaces by learning a forward\ndeformation field without direct supervision. This deformation field is defined\nin canonical, pose-independent space, allowing for generalization to unseen\nposes. Learning the deformation field from posed meshes alone is challenging\nsince the correspondences of deformed points are defined implicitly and may not\nbe unique under changes of topology. We propose a forward skinning model that\nfinds all canonical correspondences of any deformed point using iterative root\nfinding. We derive analytical gradients via implicit differentiation, enabling\nend-to-end training from 3D meshes with bone transformations. Compared to\nstate-of-the-art neural implicit representations, our approach generalizes\nbetter to unseen poses while preserving accuracy. We demonstrate our method in\nchallenging scenarios on (clothed) 3D humans in diverse and unseen poses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yufeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Reconstruct 3D Non-Cuboid Room Layout from a Single RGB Image. (arXiv:2104.07986v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07986","description":"<p>Single-image room layout reconstruction aims to reconstruct the enclosed 3D\nstructure of a room from a single image. Most previous work relies on the\ncuboid-shape prior. This paper considers a more general indoor assumption,\ni.e., the room layout consists of a single ceiling, a single floor, and several\nvertical walls. To this end, we first employ Convolutional Neural Networks to\ndetect planes and vertical lines between adjacent walls. Meanwhile, estimating\nthe 3D parameters for each plane. Then, a simple yet effective geometric\nreasoning method is adopted to achieve room layout reconstruction. Furthermore,\nwe optimize the 3D plane parameters to reconstruct a geometrically consistent\nroom layout between planes and lines. The experimental results on public\ndatasets validate the effectiveness and efficiency of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xili Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Rui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojun Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransVG: End-to-End Visual Grounding with Transformers. (arXiv:2104.08541v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08541","description":"<p>In this paper, we present a neat yet effective transformer-based framework\nfor visual grounding, namely TransVG, to address the task of grounding a\nlanguage query to the corresponding region onto an image. The state-of-the-art\nmethods, including two-stage or one-stage ones, rely on a complex module with\nmanually-designed mechanisms to perform the query reasoning and multi-modal\nfusion. However, the involvement of certain mechanisms in fusion module design,\nsuch as query decomposition and image scene graph, makes the models easily\noverfit to datasets with specific scenarios, and limits the plenitudinous\ninteraction between the visual-linguistic context. To avoid this caveat, we\npropose to establish the multi-modal correspondence by leveraging transformers,\nand empirically show that the complex fusion modules (\\eg, modular attention\nnetwork, dynamic graph, and multi-modal tree) can be replaced by a simple stack\nof transformer encoder layers with higher performance. Moreover, we\nre-formulate the visual grounding as a direct coordinates regression problem\nand avoid making predictions out of a set of candidates (\\emph{i.e.}, region\nproposals or anchor boxes). Extensive experiments are conducted on five widely\nused datasets, and a series of state-of-the-art records are set by our TransVG.\nWe build the benchmark of transformer-based visual grounding framework and make\nthe code available at \\url{https://github.com/djiajunustc/TransVG}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MemX: An Attention-Aware Smart Eyewear System for Personalized Moment Auto-capture. (arXiv:2105.00916v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00916","description":"<p>This work presents MemX: a biologically-inspired attention-aware eyewear\nsystem developed with the goal of pursuing the long-awaited vision of a\npersonalized visual Memex. MemX captures human visual attention on the fly,\nanalyzes the salient visual content, and records moments of personal interest\nin the form of compact video snippets. Accurate attentive scene detection and\nanalysis on resource-constrained platforms is challenging because these tasks\nare computation and energy intensive. We propose a new temporal visual\nattention network that unifies human visual attention tracking and salient\nvisual content analysis. Attention tracking focuses computation-intensive video\nanalysis on salient regions, while video analysis makes human attention\ndetection and tracking more accurate. Using the YouTube-VIS dataset and 30\nparticipants, we experimentally show that MemX significantly improves the\nattention tracking accuracy over the eye-tracking-alone method, while\nmaintaining high system energy efficiency. We have also conducted 11 in-field\npilot studies across a range of daily usage scenarios, which demonstrate the\nfeasibility and potential benefits of MemX.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yuhu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingying Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingzhi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Ning Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Li Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Strong Baseline for Universal Targeted Attacks on Siamese Visual Tracking. (arXiv:2105.02480v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02480","description":"<p>Siamese trackers are shown to be vulnerable to adversarial attacks recently.\nHowever, the existing attack methods craft the perturbations for each video\nindependently, which comes at a non-negligible computational cost. In this\npaper, we show the existence of universal perturbations that can enable the\ntargeted attack, e.g., forcing a tracker to follow the ground-truth trajectory\nwith specified offsets, to be video-agnostic and free from inference in a\nnetwork. Specifically, we attack a tracker by adding a universal imperceptible\nperturbation to the template image and adding a fake target, i.e., a small\nuniversal adversarial patch, into the search images adhering to the predefined\ntrajectory, so that the tracker outputs the location and size of the fake\ntarget instead of the real target. Our approach allows perturbing a novel video\nto come at no additional cost except the mere addition operations -- and not\nrequire gradient optimization or network inference. Experimental results on\nseveral datasets demonstrate that our approach can effectively fool the Siamese\ntrackers in a targeted attack manner. We show that the proposed perturbations\nare not only universal across videos, but also generalize well across different\ntrackers. Such perturbations are therefore doubly universal, both with respect\nto the data and the network architectures. We will make our code publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenbang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yaya Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaoru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Pengpeng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity Measure for B-Reps. (arXiv:2105.02961v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02961","description":"<p>Boundary Representations (B-Reps) are the industry standard in 3D Computer\nAided Design/Manufacturing (CAD/CAM) and industrial design due to their\nfidelity in representing stylistic details. However, they have been ignored in\nthe 3D style research. Existing 3D style metrics typically operate on meshes or\npointclouds, and fail to account for end-user subjectivity by adopting fixed\ndefinitions of style, either through crowd-sourcing for style labels or\nhand-crafted features. We propose UVStyle-Net, a style similarity measure for\nB-Reps that leverages the style signals in the second order statistics of the\nactivations in a pre-trained (unsupervised) 3D encoder, and learns their\nrelative importance to a subjective end-user through few-shot learning. Our\napproach differs from all existing data-driven 3D style methods since it may be\nused in completely unsupervised settings, which is desirable given the lack of\npublicly available labelled B-Rep datasets. More importantly, the few-shot\nlearning accounts for the inherent subjectivity associated with style. We show\nquantitatively that our proposed method with B-Reps is able to capture stronger\nstyle signals than alternative methods on meshes and pointclouds despite its\nsignificantly greater computational efficiency. We also show it is able to\ngenerate meaningful style gradients with respect to the input shape, and that\nfew-shot learning with as few as two positive examples selected by an end-user\nis sufficient to significantly improve the style measure. Finally, we\ndemonstrate its efficacy on a large unlabeled public dataset of CAD models.\nSource code and data will be released in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meltzer_P/0/1/0/all/0/1\">Peter Meltzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayani_H/0/1/0/all/0/1\">Hooman Shayani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1\">Amir Khasahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_P/0/1/0/all/0/1\">Pradeep Kumar Jayaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1\">Aditya Sanghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1\">Joseph Lambourne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised identification of surgical robotic actions from small non homogeneous datasets. (arXiv:2105.08488v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.08488","description":"<p>Robot-assisted surgery is an established clinical practice. The automatic\nidentification of surgical actions is needed for a range of applications,\nincluding performance assessment of trainees and surgical process modeling for\nautonomous execution and monitoring. However, supervised action identification\nis not feasible, due to the burden of manually annotating recordings of\npotentially complex and long surgical executions. Moreover, often few example\nexecutions of a surgical procedure can be recorded. This paper proposes a novel\nfast algorithm for unsupervised identification of surgical actions in a\nstandard surgical training task, the ring transfer, executed with da Vinci\nResearch Kit. Exploiting kinematic and semantic visual features automatically\nextracted from a very limited dataset of executions, we are able to\nsignificantly outperform state-of-the-art results on a dataset of non-expert\nexecutions (58\\% vs. 24\\% F1-score), and improve performance in the presence of\nnoise, short actions and non-homogeneous workflows, i.e. non repetitive action\nsequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meli_D/0/1/0/all/0/1\">Daniele Meli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiorini_P/0/1/0/all/0/1\">Paolo Fiorini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Aggressive Adversarial Attacks on 3D Point Cloud. (arXiv:2105.09090v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09090","description":"<p>Deep neural networks are found to be prone to adversarial examples which\ncould deliberately fool the model to make mistakes. Recently, a few of works\nexpand this task from 2D image to 3D point cloud by using global point cloud\noptimization. However, the perturbations of global point are not effective for\nmisleading the victim model. First, not all points are important in\noptimization toward misleading. Abundant points account considerable distortion\nbudget but contribute trivially to attack. Second, the multi-label optimization\nis suboptimal for adversarial attack, since it consumes extra energy in finding\nmulti-label victim model collapse and causes instance transformation to be\ndissimilar to any particular instance. Third, the independent adversarial and\nperceptibility losses, caring misclassification and dissimilarity separately,\ntreat the updating of each point equally without a focus. Therefore, once\nperceptibility loss approaches its budget threshold, all points would be stock\nin the surface of hypersphere and attack would be locked in local optimality.\nTherefore, we propose a local aggressive adversarial attacks (L3A) to solve\nabove issues. Technically, we select a bunch of salient points, the high-score\nsubset of point cloud according to gradient, to perturb. Then a flow of\naggressive optimization strategies are developed to reinforce the unperceptive\ngeneration of adversarial examples toward misleading victim models. Extensive\nexperiments on PointNet, PointNet++ and DGCNN demonstrate the state-of-the-art\nperformance of our method against existing adversarial attack methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingjie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CFA-Net: Controllable Face Anonymization Network with Identity Representation Manipulation. (arXiv:2105.11137v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11137","description":"<p>De-identification of face data has drawn increasing attention in recent\nyears. It is important to protect people's identities meanwhile keeping the\nutility of the data in many computer vision tasks. We propose a Controllable\nFace Anonymization Network (CFA-Net), a novel approach that can anonymize the\nidentity of given faces in images and videos, based on a generator that can\ndisentangle face identity from other image contents. We reach the goal of\ncontrollable face anonymization through manipulating identity vectors in the\ngenerator's identity representation space. Various anonymized faces deriving\nfrom an original face can be generated through our method and maintain high\nsimilarity to the original image contents. Quantitative and qualitative results\ndemonstrate our method's superiority over literature models on visual quality\nand anonymization validity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tianxiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Greedy Bayesian Posterior Approximation with Deep Ensembles. (arXiv:2105.14275v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14275","description":"<p>Ensembles of independently trained neural networks are a state-of-the-art\napproach to estimate predictive uncertainty in Deep Learning, and can be\ninterpreted as an approximation of the posterior distribution via a mixture of\ndelta functions. The training of ensembles relies on non-convexity of the loss\nlandscape and random initialization of their individual members, making the\nresulting posterior approximation uncontrolled. This paper proposes a novel and\nprincipled method to tackle this limitation, minimizing an $f$-divergence\nbetween the true posterior and a kernel density estimator in a function space.\nWe analyze this objective from a combinatorial point of view, and show that it\nis submodular with respect to mixture components for any $f$. Subsequently, we\nconsider the problem of ensemble construction, and from the marginal gain of\nthe total objective, we derive a novel diversity term for training ensembles\ngreedily. The performance of our approach is demonstrated on computer vision\nout-of-distribution detection benchmarks in a range of architectures trained on\nmultiple datasets. The source code of our method is publicly available at\nhttps://github.com/MIPT-Oulu/greedy_ensembles_training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiulpin_A/0/1/0/all/0/1\">Aleksei Tiulpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1\">Matthew B. Blaschko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Two-Flow Network for Tele-Registration of Point Clouds. (arXiv:2106.00329v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00329","description":"<p>Rigid registration of partial observations is a fundamental problem in\nvarious applied fields. In computer graphics, special attention has been given\nto the registration between two partial point clouds generated by scanning\ndevices. State-of-the-art registration techniques still struggle when the\noverlap region between the two point clouds is small, and completely fail if\nthere is no overlap between the scan pairs. In this paper, we present a\nlearning-based technique that alleviates this problem, and allows registration\nbetween point clouds, presented in arbitrary poses, and having little or even\nno overlap, a setting that has been referred to as tele-registration. Our\ntechnique is based on a novel neural network design that learns a prior of a\nclass of shapes and can complete a partial shape. The key idea is combining the\nregistration and completion tasks in a way that reinforces each other. In\nparticular, we simultaneously train the registration network and completion\nnetwork using two coupled flows, one that register-and-complete, and one that\ncomplete-and-register, and encourage the two flows to produce a consistent\nresult. We show that, compared with each separate flow, this two-flow training\nleads to robust and reliable tele-registration, and hence to a better point\ncloud prediction that completes the registered scans. It is also worth\nmentioning that each of the components in our neural network outperforms\nstate-of-the-art methods in both completion and registration. We further\nanalyze our network with several ablation studies and demonstrate its\nperformance on a large number of partial point clouds, both synthetic and\nreal-world, that have only small or no overlap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zihao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1\">Zimu Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ruizhen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning High-Precision Bounding Box for Rotated Object Detection via Kullback-Leibler Divergence. (arXiv:2106.01883v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01883","description":"<p>Existing rotated object detectors are mostly inherited from the horizontal\ndetection paradigm, as the latter has evolved into a well-developed area.\nHowever, these detectors are difficult to perform prominently in high-precision\ndetection due to the limitation of current regression loss design, especially\nfor objects with large aspect ratios. Taking the perspective that horizontal\ndetection is a special case for rotated object detection, in this paper, we are\nmotivated to change the design of rotation regression loss from induction\nparadigm to deduction methodology, in terms of the relation between rotation\nand horizontal detection. We show that one essential challenge is how to\nmodulate the coupled parameters in the rotation regression loss, as such the\nestimated parameters can influence to each other during the dynamic joint\noptimization, in an adaptive and synergetic way. Specifically, we first convert\nthe rotated bounding box into a 2-D Gaussian distribution, and then calculate\nthe Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the\nregression loss. By analyzing the gradient of each parameter, we show that KLD\n(and its derivatives) can dynamically adjust the parameter gradients according\nto the characteristics of the object. It will adjust the importance (gradient\nweight) of the angle parameter according to the aspect ratio. This mechanism\ncan be vital for high-precision detection as a slight angle error would cause a\nserious accuracy drop for large aspect ratios objects. More importantly, we\nhave proved that KLD is scale invariant. We further show that the KLD loss can\nbe degenerated into the popular $l_{n}$-norm loss for horizontal detection.\nExperimental results on seven datasets using different detectors show its\nconsistent superiority, and codes are available at\nhttps://github.com/yangxue0827/RotationDetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaojiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jirui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1\">Qi Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wentao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combinatorial Optimization for Panoptic Segmentation: A Fully Differentiable Approach. (arXiv:2106.03188v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03188","description":"<p>We propose a fully differentiable architecture for simultaneous semantic and\ninstance segmentation (a.k.a. panoptic segmentation) consisting of a\nconvolutional neural network and an asymmetric multiway cut problem solver. The\nlatter solves a combinatorial optimization problem that elegantly incorporates\nsemantic and boundary predictions to produce a panoptic labeling. Our\nformulation allows to directly maximize a smooth surrogate of the panoptic\nquality metric by backpropagating the gradient through the optimization\nproblem. Experimental evaluation shows improvement by backpropagating through\nthe optimization problem w.r.t. comparable approaches on Cityscapes and COCO\ndatasets. Overall, our approach shows the utility of using combinatorial\noptimization in tandem with deep learning in a challenging large scale\nreal-world problem and showcases benefits and insights into training such an\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1\">Ahmed Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifting Transformation Learning for Out-of-Distribution Detection. (arXiv:2106.03899v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03899","description":"<p>Detecting out-of-distribution (OOD) samples plays a key role in open-world\nand safety-critical applications such as autonomous systems and healthcare.\nRecently, self-supervised representation learning techniques (via contrastive\nlearning and pretext learning) have shown effective in improving OOD detection.\nHowever, one major issue with such approaches is the choice of shifting\ntransformations and pretext tasks which depends on the in-domain distribution.\nIn this paper, we propose a simple framework that leverages a shifting\ntransformation learning setting for learning multiple shifted representations\nof the training set for improved OOD detection. To address the problem of\nselecting optimal shifting transformation and pretext tasks, we propose a\nsimple mechanism for automatically selecting the transformations and modulating\ntheir effect on representation learning without requiring any OOD training\nsamples. In extensive experiments, we show that our simple framework\noutperforms state-of-the-art OOD detection models on several image datasets. We\nalso characterize the criteria for a desirable OOD detector for real-world\napplications and demonstrate the efficacy of our proposed technique against\nstate-of-the-art OOD detection techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohseni_S/0/1/0/all/0/1\">Sina Mohseni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1\">Arash Vahdat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadawa_J/0/1/0/all/0/1\">Jay Yadawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Affiliate: Mutual Centralized Learning for Few-shot Classification. (arXiv:2106.05517v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05517","description":"<p>Few-shot learning (FSL) aims to learn a classifier that can be easily adapted\nto accommodate new tasks not seen during training, given only a few examples.\nTo handle the limited-data problem in few-shot regimes, recent methods tend to\ncollectively use a set of local features to densely represent an image instead\nof using a mixed global feature. They generally explore a unidirectional\nquery-to-support paradigm in FSL, e.g., find the nearest/optimal support\nfeature for each query feature and aggregate these local matches for a joint\nclassification. In this paper, we propose a new method Mutual Centralized\nLearning (MCL) to fully affiliate the two disjoint sets of dense features in a\nbidirectional paradigm. We associate each local feature with a particle that\ncan bidirectionally random walk in a discrete feature space by the\naffiliations. To estimate the class probability, we propose the features'\naccessibility that measures the expected number of visits to the support\nfeatures of that class in a Markov process. We relate our method to learning a\ncentrality on an affiliation network and demonstrate its capability to be\nplugged in existing methods by highlighting centralized local features.\nExperiments show that our method achieves the state-of-the-art on both\nminiImageNet and tieredImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weifeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Chao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Network Modeling of Probabilities for Coding the Octree Representation of Point Clouds. (arXiv:2106.06482v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06482","description":"<p>This paper describes a novel lossless point cloud compression algorithm that\nuses a neural network for estimating the coding probabilities for the occupancy\nstatus of voxels, depending on wide three dimensional contexts around the voxel\nto be encoded. The point cloud is represented as an octree, with each\nresolution layer being sequentially encoded and decoded using arithmetic\ncoding, starting from the lowest resolution, until the final resolution is\nreached. The occupancy probability of each voxel of the splitting pattern at\neach node of the octree is modeled by a neural network, having at its input the\nalready encoded occupancy status of several octree nodes (belonging to the past\nand current resolutions), corresponding to a 3D context surrounding the node to\nbe encoded. The algorithm has a fast and a slow version, the fast version\nselecting differently several voxels of the context, which allows an increased\nparallelization by sending larger batches of templates to be estimated by the\nneural network, at both encoder and decoder. The proposed algorithms yield\nstate-of-the-art results on benchmark datasets. The implementation will be made\navailable at https://github.com/marmus12/nnctx\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaya_E/0/1/0/all/0/1\">Emre Can Kaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabus_I/0/1/0/all/0/1\">Ioan Tabus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LE-NAS: Learning-based Ensemble with NAS for Dose Prediction. (arXiv:2106.06733v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.06733","description":"<p>Radiation therapy treatment planning is a complex process, as the target dose\nprescription and normal tissue sparing are conflicting objectives. Automated\nand accurate dose prediction for radiation therapy planning is in high demand.\nIn this study, we propose a novel learning-based ensemble approach, named\nLE-NAS, which integrates neural architecture search (NAS) with knowledge\ndistillation for 3D radiotherapy dose prediction. Specifically, the prediction\nnetwork first exhaustively searches each block from enormous architecture\nspace. Then, multiple architectures are selected with promising performance and\ndiversity. To reduce the inference time, we adopt the teacher-student paradigm\nby treating the combination of diverse outputs from multiple searched networks\nas supervisions to guide the student network training. In addition, we apply\nadversarial learning to optimize the student network to recover the knowledge\nin teacher networks. To the best of our knowledge, we are the first to\ninvestigate the combination of NAS and knowledge distillation. The proposed\nmethod has been evaluated on the public OpenKBP dataset, and experimental\nresults demonstrate the effectiveness of our method and its superior\nperformance to the state-of-the-art method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Yi Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yanfei Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jingguang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_G/0/1/0/all/0/1\">Guocai Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Steerable Partial Differential Operators for Equivariant Neural Networks. (arXiv:2106.10163v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.10163","description":"<p>Recent work in equivariant deep learning bears strong similarities to\nphysics. Fields over a base space are fundamental entities in both subjects, as\nare equivariant maps between these fields. In deep learning, however, these\nmaps are usually defined by convolutions with a kernel, whereas they are\npartial differential operators (PDOs) in physics. Developing the theory of\nequivariant PDOs in the context of deep learning could bring these subjects\neven closer together and lead to a stronger flow of ideas. In this work, we\nderive a $G$-steerability constraint that completely characterizes when a PDO\nbetween feature vector fields is equivariant, for arbitrary symmetry groups\n$G$. We then fully solve this constraint for several important groups. We use\nour solutions as equivariant drop-in replacements for convolutional layers and\nbenchmark them in that role. Finally, we develop a framework for equivariant\nmaps based on Schwartz distributions that unifies classical convolutions and\ndifferential operators and gives insight about the relation between the two.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jenner_E/0/1/0/all/0/1\">Erik Jenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1\">Maurice Weiler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Train Your MAML to Excel in Few-Shot Classification. (arXiv:2106.16245v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.16245","description":"<p>Model-agnostic meta-learning (MAML) is arguably one of the most popular\nmeta-learning algorithms nowadays. Nevertheless, its performance on few-shot\nclassification is far behind many recent algorithms dedicated to the problem.\nIn this paper, we point out several key facets of how to train MAML to excel in\nfew-shot classification. First, we find that MAML needs a large number of\ngradient steps in its inner loop update, which contradicts its common usage in\nfew-shot classification. Second, we find that MAML is sensitive to the class\nlabel assignments during meta-testing. Concretely, MAML meta-trains the\ninitialization of an $N$-way classifier. These $N$ ways, during meta-testing,\nthen have $N!$ different permutations to be paired with a few-shot task of $N$\nnovel classes. We find that these permutations lead to a huge variance of\naccuracy, making MAML unstable in few-shot classification. Third, we\ninvestigate several approaches to make MAML permutation-invariant, among which\nmeta-training a single vector to initialize all the $N$ weight vectors in the\nclassification head performs the best. On benchmark datasets like MiniImageNet\nand TieredImageNet, our approach, which we name UNICORN-MAML, performs on a par\nwith or even outperforms state-of-the-art few-shot classification algorithms,\nwithout sacrificing MAML's simplicity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is attention to bounding boxes all you need for pedestrian action prediction?. (arXiv:2107.08031v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08031","description":"<p>The human driver is no longer the only one concerned with the complexity of\nthe driving scenarios. Autonomous vehicles (AV) are similarly becoming involved\nin the process. Nowadays, the development of AV in urban places underpins\nessential safety concerns for vulnerable road users (VRUs) such as pedestrians.\nTherefore, to make the roads safer, it is critical to classify and predict\ntheir future behavior. In this paper, we present a framework based on multiple\nvariations of the Transformer models to reason attentively about the dynamic\nevolution of the pedestrians' past trajectory and predict its future actions of\ncrossing or not crossing the street. We proved that using only bounding boxes\nas input to our model can outperform the previous state-of-the-art models and\nreach a prediction accuracy of 91% and an F1-score of 0.83 on the PIE dataset\nup to two seconds ahead in the future. In addition, we introduced a large-size\nsimulated dataset (CP2A) using CARLA for action prediction. Our model has\nsimilarly reached high accuracy (91%) and F1-score (0.91) on this dataset.\nInterestingly, we showed that pre-training our Transformer model on the\nsimulated dataset and then fine-tuning it on the real dataset can be very\neffective for the action prediction task. Finally, we created the \"human\nattention to bounding boxes\" experiment that equally proved the ability of\nhumans to predict the future sufficiently by only giving attention to the\nbounding boxes without the need for environmental context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Achaji_L/0/1/0/all/0/1\">Lina Achaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1\">Julien Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouqueray_T/0/1/0/all/0/1\">Thibault Fouqueray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aioun_F/0/1/0/all/0/1\">Francois Aioun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpillet_F/0/1/0/all/0/1\">Francois Charpillet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face.evoLVe: A High-Performance Face Recognition Library. (arXiv:2107.08621v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08621","description":"<p>In this paper, we develop face.evoLVe -- a comprehensive library that\ncollects and implements a wide range of popular deep learning-based methods for\nface recognition. First of all, face.evoLVe is composed of key components that\ncover the full process of face analytics, including face alignment, data\nprocessing, various backbones, losses, and alternatives with bags of tricks for\nimproving performance. Later, face.evoLVe supports multi-GPU training on top of\ndifferent deep learning platforms, such as PyTorch and PaddlePaddle, which\nfacilitates researchers to work on both large-scale datasets with millions of\nimages and low-shot counterparts with limited well-annotated data. More\nimportantly, along with face.evoLVe, images before &amp; after alignment in the\ncommon benchmark datasets are released with source codes and trained models\nprovided. All these efforts lower the technical burdens in reproducing the\nexisting methods for comparison, while users of our library could focus on\ndeveloping advanced approaches more efficiently. Last but not least,\nface.evoLVe is well designed and vibrantly evolving, so that new face\nrecognition approaches can be easily plugged into our framework. Note that we\nhave used face.evoLVe to participate in a number of face recognition\ncompetitions and secured the first place. The version that supports PyTorch is\npublicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the\nPaddlePaddle version is available at\nhttps://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.\nFace.evoLVe has been widely used for face analytics, receiving 2.4K stars and\n622 forks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuperCaustics: Real-time, open-source simulation of transparent objects for deep learning applications. (arXiv:2107.11008v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2107.11008","description":"<p>Transparent objects are a very challenging problem in computer vision. They\nare hard to segment or classify due to their lack of precise boundaries, and\nthere is limited data available for training deep neural networks. As such,\ncurrent solutions for this problem employ rigid synthetic datasets, which lack\nflexibility and lead to severe performance degradation when deployed on\nreal-world scenarios. In particular, these synthetic datasets omit features\nsuch as refraction, dispersion and caustics due to limitations in the rendering\npipeline. To address this issue, we present SuperCaustics, a real-time,\nopen-source simulation of transparent objects designed for deep learning\napplications. SuperCaustics features extensive modules for stochastic\nenvironment creation; uses hardware ray-tracing to support caustics,\ndispersion, and refraction; and enables generating massive datasets with\nmulti-modal, pixel-perfect ground truth annotations. To validate our proposed\nsystem, we trained a deep neural network from scratch to segment transparent\nobjects in difficult lighting scenarios. Our neural network achieved\nperformance comparable to the state-of-the-art on a real-world dataset using\nonly 10% of the training data and in a fraction of the training time. Further\nexperiments show that a model trained with SuperCaustics can segment different\ntypes of caustics, even in images with multiple overlapping transparent\nobjects. To the best of our knowledge, this is the first such result for a\nmodel trained on synthetic data. Both our open-source code and experimental\ndata are freely available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mousavi_M/0/1/0/all/0/1\">Mehdi Mousavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estrada_R/0/1/0/all/0/1\">Rolando Estrada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation. (arXiv:2107.11264v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11264","description":"<p>Identifying unexpected objects on roads in semantic segmentation (e.g.,\nidentifying dogs on roads) is crucial in safety-critical applications. Existing\napproaches use images of unexpected objects from external datasets or require\nadditional training (e.g., retraining segmentation networks or training an\nextra network), which necessitate a non-trivial amount of labor intensity or\nlengthy inference time. One possible alternative is to use prediction scores of\na pre-trained network such as the max logits (i.e., maximum values among\nclasses before the final softmax layer) for detecting such objects. However,\nthe distribution of max logits of each predicted class is significantly\ndifferent from each other, which degrades the performance of identifying\nunexpected objects in urban-scene segmentation. To address this issue, we\npropose a simple yet effective approach that standardizes the max logits in\norder to align the different distributions and reflect the relative meanings of\nmax logits within each predicted class. Moreover, we consider the local regions\nfrom two different perspectives based on the intuition that neighboring pixels\nshare similar semantic information. In contrast to previous approaches, our\nmethod does not utilize any external datasets or require additional training,\nwhich makes our method widely applicable to existing pre-trained segmentation\nmodels. Such a straightforward approach achieves a new state-of-the-art\nperformance on the publicly available Fishyscapes Lost &amp; Found leaderboard with\na large margin. Our code is publicly available at this\n$\\href{https://github.com/shjung13/Standardized-max-logits}{link}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sanghun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwak_D/0/1/0/all/0/1\">Daehoon Gwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sungha Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Embodied Vision Navigation: A Survey. (arXiv:2108.04097v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.04097","description":"<p>\"Embodied visual navigation\" problem requires an agent to navigate in a 3D\nenvironment mainly rely on its first-person observation. This problem has\nattracted rising attention in recent years due to its wide application in\nautonomous driving, vacuum cleaner, and rescue robot. A navigation agent is\nsupposed to have various intelligent skills, such as visual perceiving,\nmapping, planning, exploring and reasoning, etc. Building such an agent that\nobserves, thinks, and acts is a key to real intelligence. The remarkable\nlearning ability of deep learning methods empowered the agents to accomplish\nembodied visual navigation tasks. Despite this, embodied visual navigation is\nstill in its infancy since a lot of advanced skills are required, including\nperceiving partially observed visual input, exploring unseen areas, memorizing\nand modeling seen scenarios, understanding cross-modal instructions, and\nadapting to a new environment, etc. Recently, embodied visual navigation has\nattracted rising attention of the community, and numerous works has been\nproposed to learn these skills. This paper attempts to establish an outline of\nthe current works in the field of embodied visual navigation by providing a\ncomprehensive literature survey. We summarize the benchmarks and metrics,\nreview different methods, analysis the challenges, and highlight the\nstate-of-the-art methods. Finally, we discuss unresolved challenges in the\nfield of embodied visual navigation and give promising directions in pursuing\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengda Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_V/0/1/0/all/0/1\">Vincent CS Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Day-Night Domain Adaptation with a Physics Prior. (arXiv:2108.05137v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05137","description":"<p>We explore the zero-shot setting for day-night domain adaptation. The\ntraditional domain adaptation setting is to train on one domain and adapt to\nthe target domain by exploiting unlabeled data samples from the test set. As\ngathering relevant test data is expensive and sometimes even impossible, we\nremove any reliance on test data imagery and instead exploit a visual inductive\nprior derived from physics-based reflection models for domain adaptation. We\ncast a number of color invariant edge detectors as trainable layers in a\nconvolutional neural network and evaluate their robustness to illumination\nchanges. We show that the color invariant layer reduces the day-night\ndistribution shift in feature map activations throughout the network. We\ndemonstrate improved performance for zero-shot day to night domain adaptation\non both synthetic as well as natural datasets in various tasks, including\nclassification, segmentation and place recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lengyel_A/0/1/0/all/0/1\">Attila Lengyel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan C. van Gemert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Depth Completion with Calibrated Backprojection Layers. (arXiv:2108.10531v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10531","description":"<p>We propose a deep neural network architecture to infer dense depth from an\nimage and a sparse point cloud. It is trained using a video stream and\ncorresponding synchronized sparse point cloud, as obtained from a LIDAR or\nother range sensor, along with the intrinsic calibration parameters of the\ncamera. At inference time, the calibration of the camera, which can be\ndifferent than the one used for training, is fed as an input to the network\nalong with the sparse point cloud and a single image. A Calibrated\nBackprojection Layer backprojects each pixel in the image to three-dimensional\nspace using the calibration matrix and a depth feature descriptor. The\nresulting 3D positional encoding is concatenated with the image descriptor and\nthe previous layer output to yield the input to the next layer of the encoder.\nA decoder, exploiting skip-connections, produces a dense depth map. The\nresulting Calibrated Backprojection Network, or KBNet, is trained without\nsupervision by minimizing the photometric reprojection error. KBNet imputes\nmissing depth value based on the training set, rather than on generic\nregularization. We test KBNet on public depth completion benchmarks, where it\noutperforms the state of the art by 30.5% indoor and 8.8% outdoor when the same\ncamera is used for training and testing. When the test camera is different, the\nimprovement reaches 62%. Code available at:\nhttps://github.com/alexklwong/calibrated-backprojection-network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Scaling Law for Synthetic-to-Real Transfer: How Much Is Your Pre-training Effective?. (arXiv:2108.11018v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.11018","description":"<p>Synthetic-to-real transfer learning is a framework in which a synthetically\ngenerated dataset is used to pre-train a model to improve its performance on\nreal vision tasks. The most significant advantage of using synthetic images is\nthat the ground-truth labels are automatically available, enabling unlimited\nexpansion of the data size without human cost. However, synthetic data may have\na huge domain gap, in which case increasing the data size does not improve the\nperformance. How can we know that? In this study, we derive a simple scaling\nlaw that predicts the performance from the amount of pre-training data. By\nestimating the parameters of the law, we can judge whether we should increase\nthe data or change the setting of image synthesis. Further, we analyze the\ntheory of transfer learning by considering learning dynamics and confirm that\nthe derived generalization bound is consistent with our empirical findings. We\nempirically validated our scaling law on various experimental settings of\nbenchmark tasks, model sizes, and complexities of synthetic images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mikami_H/0/1/0/all/0/1\">Hiroaki Mikami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukumizu_K/0/1/0/all/0/1\">Kenji Fukumizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murai_S/0/1/0/all/0/1\">Shogo Murai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_S/0/1/0/all/0/1\">Shuji Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikuchi_Y/0/1/0/all/0/1\">Yuta Kikuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1\">Taiji Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maeda_S/0/1/0/all/0/1\">Shin-ichi Maeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1\">Kohei Hayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOVEA: Foveated Image Magnification for Autonomous Navigation. (arXiv:2108.12102v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12102","description":"<p>Efficient processing of high-res video streams is safety-critical for many\nrobotics applications such as autonomous driving. To maintain real-time\nperformance, many practical systems downsample the video stream. But this can\nhurt downstream tasks such as (small) object detection. Instead, we take\ninspiration from biological vision systems that allocate more foveal \"pixels\"\nto salient parts of the scene. We introduce FOVEA, an approach for intelligent\ndownsampling that ensures salient image regions remain \"magnified\" in the\ndownsampled output. Given a high-res image, FOVEA applies a differentiable\nresampling layer that outputs a small fixed-size image canvas, which is then\nprocessed with a differentiable vision module (e.g., object detection network),\nwhose output is then differentiably backward mapped onto the original image\nsize. The key idea is to resample such that background pixels can make room for\nsalient pixels of interest. In order to ensure the overall pipeline remains\nefficient, FOVEA makes use of cheap and readily available cues for saliency,\nincluding dataset-specific spatial priors or temporal priors computed from\nobject predictions in the recent past. On the autonomous driving datasets\nArgoverse-HD and BDD100K, our proposed method boosts the detection AP over\nstandard Faster R-CNN, both with and without finetuning. Without any noticeable\nincrease in compute, we improve accuracy on small objects by over 2x without\ndegrading performance on large objects. Finally, FOVEA sets a new record for\nstreaming AP (from 17.8 to 23.0 on a GTX 1080 Ti GPU), a metric designed to\ncapture both accuracy and latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thavamani_C/0/1/0/all/0/1\">Chittesh Thavamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengtian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cebron_N/0/1/0/all/0/1\">Nicolas Cebron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OARnet: Automated organs-at-risk delineation in Head and Neck CT images. (arXiv:2108.13987v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.13987","description":"<p>A 3D deep learning model (OARnet) is developed and used to delineate 28 H&amp;N\nOARs on CT images. OARnet utilizes a densely connected network to detect the\nOAR bounding-box, then delineates the OAR within the box. It reuses information\nfrom any layer to subsequent layers and uses skip connections to combine\ninformation from different dense block levels to progressively improve\ndelineation accuracy. Training uses up to 28 expert manual delineated (MD) OARs\nfrom 165 CTs. Dice similarity coefficient (DSC) and the 95th percentile\nHausdorff distance (HD95) with respect to MD is assessed for 70 other CTs.\nMean, maximum, and root-mean-square dose differences with respect to MD are\nassessed for 56 of the 70 CTs. OARnet is compared with UaNet, AnatomyNet, and\nMulti-Atlas Segmentation (MAS). Wilcoxon signed-rank tests using 95% confidence\nintervals are used to assess significance. Wilcoxon signed ranked tests show\nthat, compared with UaNet, OARnet improves (p&lt;0.05) the DSC (23/28 OARs) and\nHD95 (17/28). OARnet outperforms both AnatomyNet and MAS for DSC (28/28) and\nHD95 (27/28). Compared with UaNet, OARnet improves median DSC up to 0.05 and\nHD95 up to 1.5mm. Compared with AnatomyNet and MAS, OARnet improves median\n(DSC, HD95) by up to (0.08, 2.7mm) and (0.17, 6.3mm). Dosimetrically, OARnet\noutperforms UaNet (Dmax 7/28; Dmean 10/28), AnatomyNet (Dmax 21/28; Dmean\n24/28), and MAS (Dmax 22/28; Dmean 21/28). The DenseNet architecture is\noptimized using a hybrid approach that performs OAR-specific bounding box\ndetection followed by feature recognition. Compared with other auto-delineation\nmethods, OARnet is better than or equal to UaNet for all but one geometric\n(Temporal Lobe L, HD95) and one dosimetric (Eye L, mean dose) endpoint for the\n28 H&amp;N OARs, and is better than or equal to both AnatomyNet and MAS for all\nOARs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Soomro_M/0/1/0/all/0/1\">Mumtaz Hussain Soomro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nourzadeh_H/0/1/0/all/0/1\">Hamidreza Nourzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alves_V/0/1/0/all/0/1\">Victor Gabriel Leandro Alves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_W/0/1/0/all/0/1\">Wookjin Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siebers_J/0/1/0/all/0/1\">Jeffrey V. Siebers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatiotemporal Inconsistency Learning for DeepFake Video Detection. (arXiv:2109.01860v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01860","description":"<p>The rapid development of facial manipulation techniques has aroused public\nconcerns in recent years. Following the success of deep learning, existing\nmethods always formulate DeepFake video detection as a binary classification\nproblem and develop frame-based and video-based solutions. However, little\nattention has been paid to capturing the spatial-temporal inconsistency in\nforged videos. To address this issue, we term this task as a Spatial-Temporal\nInconsistency Learning (STIL) process and instantiate it into a novel STIL\nblock, which consists of a Spatial Inconsistency Module (SIM), a Temporal\nInconsistency Module (TIM), and an Information Supplement Module (ISM).\nSpecifically, we present a novel temporal modeling paradigm in TIM by\nexploiting the temporal difference over adjacent frames along with both\nhorizontal and vertical directions. And the ISM simultaneously utilizes the\nspatial information from SIM and temporal information from TIM to establish a\nmore comprehensive spatial-temporal representation. Moreover, our STIL block is\nflexible and could be plugged into existing 2D CNNs. Extensive experiments and\nvisualizations are presented to demonstrate the effectiveness of our method\nagainst the state-of-the-art competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhihao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Timbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks. (arXiv:2109.02096v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2109.02096","description":"<p>This research project investigates the application of deep learning to timbre\ntransfer, where the timbre of a source audio can be converted to the timbre of\na target audio with minimal loss in quality. The adopted approach combines\nVariational Autoencoders with Generative Adversarial Networks to construct\nmeaningful representations of the source audio and produce realistic\ngenerations of the target audio and is applied to the Flickr 8k Audio dataset\nfor transferring the vocal timbre between speakers and the URMP dataset for\ntransferring the musical timbre between instruments. Furthermore, variations of\nthe adopted approach are trained, and generalised performance is compared using\nthe metrics SSIM (Structural Similarity Index) and FAD (Frech\\'et Audio\nDistance). It was found that a many-to-many approach supersedes a one-to-one\napproach in terms of reconstructive capabilities, and that the adoption of a\nbasic over a bottleneck residual block design is more suitable for enriching\ncontent information about a latent space. It was also found that the decision\non whether cyclic loss takes on a variational autoencoder or vanilla\nautoencoder approach does not have a significant impact on reconstructive and\nadversarial translation aspects of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonnici_R/0/1/0/all/0/1\">Russell Sammut Bonnici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1\">Charalampos Saitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benning_M/0/1/0/all/0/1\">Martin Benning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TADA: Taxonomy Adaptive Domain Adaptation. (arXiv:2109.04813v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04813","description":"<p>Traditional domain adaptation addresses the task of adapting a model to a\nnovel target domain under limited or no additional supervision. While tackling\nthe input domain gap, the standard domain adaptation settings assume no domain\nchange in the output space. In semantic prediction tasks, different datasets\nare often labeled according to different semantic taxonomies. In many\nreal-world settings, the target domain task requires a different taxonomy than\nthe one imposed by the source domain. We therefore introduce the more general\ntaxonomy adaptive domain adaptation (TADA) problem, allowing for inconsistent\ntaxonomies between the two domains. We further propose an approach that jointly\naddresses the image-level and label-level domain adaptation. On the\nlabel-level, we employ a bilateral mixed sampling strategy to augment the\ntarget domain, and a relabelling method to unify and align the label spaces. We\naddress the image-level domain gap by proposing an uncertainty-rectified\ncontrastive learning method, leading to more domain-invariant and class\ndiscriminative features. We extensively evaluate the effectiveness of our\nframework under different TADA settings: open taxonomy, coarse-to-fine\ntaxonomy, and partially-overlapping taxonomy. Our framework outperforms\nprevious state-of-the-art by a large margin, while capable of adapting to\ntarget taxonomies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Rui Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1\">Ajad Chhatkuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation. (arXiv:2109.06163v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06163","description":"<p>Scene depth estimation from stereo and monocular imagery is critical for\nextracting 3D information for downstream tasks such as scene understanding.\nRecently, learning-based methods for depth estimation have received much\nattention due to their high performance and flexibility in hardware choice.\nHowever, collecting ground truth data for supervised training of these\nalgorithms is costly or outright impossible. This circumstance suggests a need\nfor alternative learning approaches that do not require corresponding depth\nmeasurements. Indeed, self-supervised learning of depth estimation provides an\nincreasingly popular alternative. It is based on the idea that observed frames\ncan be synthesized from neighboring frames if accurate depth of the scene is\nknown - or in this case, estimated. We show empirically that - contrary to\ncommon belief - improvements in image synthesis do not necessitate improvement\nin depth estimation. Rather, optimizing for image synthesis can result in\ndiverging performance with respect to the main prediction objective - depth. We\nattribute this diverging phenomenon to aleatoric uncertainties, which originate\nfrom data. Based on our experiments on four datasets (spanning street, indoor,\nand medical) and five architectures (monocular and stereo), we conclude that\nthis diverging phenomenon is independent of the dataset domain and not\nmitigated by commonly used regularization techniques. To underscore the\nimportance of this finding, we include a survey of methods which use image\nsynthesis, totaling 127 papers over the last six years. This observed\ndivergence has not been previously reported or studied in depth, suggesting\nroom for future improvement of self-supervised approaches which might be\nimpacted the finding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoshuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drenkow_N/0/1/0/all/0/1\">Nathan Drenkow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_A/0/1/0/all/0/1\">Andy S. Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1\">Alexander Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creighton_F/0/1/0/all/0/1\">Francis X. Creighton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Russell H. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHFC: Multi-Head Feature Collaboration for Few-Shot Learning. (arXiv:2109.07785v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07785","description":"<p>Few-shot learning (FSL) aims to address the data-scarce problem. A standard\nFSL framework is composed of two components: (1) Pre-train. Employ the base\ndata to generate a CNN-based feature extraction model (FEM). (2) Meta-test.\nApply the trained FEM to acquire the novel data's features and recognize them.\nFSL relies heavily on the design of the FEM. However, various FEMs have\ndistinct emphases. For example, several may focus more attention on the contour\ninformation, whereas others may lay particular emphasis on the texture\ninformation. The single-head feature is only a one-sided representation of the\nsample. Besides the negative influence of cross-domain (e.g., the trained FEM\ncan not adapt to the novel class flawlessly), the distribution of novel data\nmay have a certain degree of deviation compared with the ground truth\ndistribution, which is dubbed as distribution-shift-problem (DSP). To address\nthe DSP, we propose Multi-Head Feature Collaboration (MHFC) algorithm, which\nattempts to project the multi-head features (e.g., multiple features extracted\nfrom a variety of FEMs) to a unified space and fuse them to capture more\ndiscriminative information. Typically, first, we introduce a subspace learning\nmethod to transform the multi-head features to aligned low-dimensional\nrepresentations. It corrects the DSP via learning the feature with more\npowerful discrimination and overcomes the problem of inconsistent measurement\nscales from different head features. Then, we design an attention block to\nupdate combination weights for each head feature automatically. It\ncomprehensively considers the contribution of various perspectives and further\nimproves the discrimination of features. We evaluate the proposed method on\nfive benchmark datasets (including cross-domain experiments) and achieve\nsignificant improvements of 2.1%-7.8% compared with state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shuai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chunyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan-Jiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bao-Di Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifting 2D Object Locations to 3D by Discounting LiDAR Outliers across Objects and Views. (arXiv:2109.07945v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07945","description":"<p>We present a system for automatic converting of 2D mask object predictions\nand raw LiDAR point clouds into full 3D bounding boxes of objects. Because the\nLiDAR point clouds are partial, directly fitting bounding boxes to the point\nclouds is meaningless. Instead, we suggest that obtaining good results requires\nsharing information between \\emph{all} objects in the dataset jointly, over\nmultiple frames. We then make three improvements to the baseline. First, we\naddress ambiguities in predicting the object rotations via direct optimization\nin this space while still backpropagating rotation prediction through the\nmodel. Second, we explicitly model outliers and task the network with learning\ntheir typical patterns, thus better discounting them. Third, we enforce\ntemporal consistency when video data is available. With these contributions,\nour method significantly outperforms previous work despite the fact that those\nmethods use significantly more complex pipelines, 3D models and additional\nhuman-annotated external sources of prior information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McCraith_R/0/1/0/all/0/1\">Robert McCraith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Insafutdinov_E/0/1/0/all/0/1\">Eldar Insafutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_L/0/1/0/all/0/1\">Lukas Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the rogue wave pattern triggered from Gaussian perturbations by deep learning. (arXiv:2109.08909v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08909","description":"<p>Weak Gaussian perturbations on a plane wave background could trigger lots of\nrogue waves, due to modulational instability. Numerical simulations showed that\nthese rogue waves seemed to have similar unit structure. However, to the best\nof our knowledge, there is no relative result to prove that these rogue waves\nhave the similar patterns for different perturbations, partly due to that it is\nhard to measure the rogue wave pattern automatically. In this work, we address\nthese problems from the perspective of computer vision via using deep neural\nnetworks. We propose a Rogue Wave Detection Network (RWD-Net) model to\nautomatically and accurately detect RWs on the images, which directly indicates\nthey have the similar computer vision patterns. For this purpose, we herein\nmeanwhile have designed the related dataset, termed as Rogue Wave Dataset-$10$K\n(RWD-$10$K), which has $10,191$ RW images with bounding box annotations for\neach RW unit. In our detection experiments, we get $99.29\\%$ average precision\non the test splits of the RWD-$10$K dataset. Finally, we derive our novel\nmetric, the density of RW units (DRW), to characterize the evolution of\nGaussian perturbations and obtain the statistical results on them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Liwen Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">XinHang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Delu Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_L/0/1/0/all/0/1\">Liming Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Li-Chen Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iWave3D: End-to-end Brain Image Compression with Trainable 3-D Wavelet Transform. (arXiv:2109.08942v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.08942","description":"<p>With the rapid development of whole brain imaging technology, a large number\nof brain images have been produced, which puts forward a great demand for\nefficient brain image compression methods. At present, the most commonly used\ncompression methods are all based on 3-D wavelet transform, such as JP3D.\nHowever, traditional 3-D wavelet transforms are designed manually with certain\nassumptions on the signal, but brain images are not as ideal as assumed. What's\nmore, they are not directly optimized for compression task. In order to solve\nthese problems, we propose a trainable 3-D wavelet transform based on the\nlifting scheme, in which the predict and update steps are replaced by 3-D\nconvolutional neural networks. Then the proposed transform is embedded into an\nend-to-end compression scheme called iWave3D, which is trained with a large\namount of brain images to directly minimize the rate-distortion loss.\nExperimental results demonstrate that our method outperforms JP3D significantly\nby 2.012 dB in terms of average BD-PSNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xue_D/0/1/0/all/0/1\">Dongmei Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_H/0/1/0/all/0/1\">Haichuan Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Context-Aware Network for Abdominal Multi-organ Segmentation. (arXiv:2109.10601v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.10601","description":"<p>The contextual information, presented in abdominal CT scan, is relative\nconsistent. In order to make full use of the overall 3D context, we develop a\nwhole-volume-based coarse-to-fine framework for efficient and effective\nabdominal multi-organ segmentation. We propose a new efficientSegNet network,\nwhich is composed of encoder, decoder and context block. For the decoder\nmodule, anisotropic convolution with a k*k*1 intra-slice convolution and a\n1*1*k inter-slice convolution, is designed to reduce the computation burden.\nFor the context block, we propose strip pooling module to capture anisotropic\nand long-range contextual information, which exists in abdominal scene.\nQuantitative evaluation on the FLARE2021 validation cases, this method achieves\nthe average dice similarity coefficient (DSC) of 0.895 and average normalized\nsurface distance (NSD) of 0.775. The average running time is 9.8 s per case in\ninference phase, and maximum used GPU memory is 1017 MB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Hua Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two Souls in an Adversarial Image: Towards Universal Adversarial Example Detection using Multi-view Inconsistency. (arXiv:2109.12459v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12459","description":"<p>In the evasion attacks against deep neural networks (DNN), the attacker\ngenerates adversarial instances that are visually indistinguishable from benign\nsamples and sends them to the target DNN to trigger misclassifications. In this\npaper, we propose a novel multi-view adversarial image detector, namely Argos,\nbased on a novel observation. That is, there exist two \"souls\" in an\nadversarial instance, i.e., the visually unchanged content, which corresponds\nto the true label, and the added invisible perturbation, which corresponds to\nthe misclassified label. Such inconsistencies could be further amplified\nthrough an autoregressive generative approach that generates images with seed\npixels selected from the original image, a selected label, and pixel\ndistributions learned from the training data. The generated images (i.e., the\n\"views\") will deviate significantly from the original one if the label is\nadversarial, demonstrating inconsistencies that Argos expects to detect. To\nthis end, Argos first amplifies the discrepancies between the visual content of\nan image and its misclassified label induced by the attack using a set of\nregeneration mechanisms and then identifies an image as adversarial if the\nreproduced views deviate to a preset degree. Our experimental results show that\nArgos significantly outperforms two representative adversarial detectors in\nboth detection accuracy and robustness against six well-known adversarial\nattacks. Code is available at:\nhttps://github.com/sohaib730/Argos-Adversarial_Detection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiani_S/0/1/0/all/0/1\">Sohaib Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awan_S/0/1/0/all/0/1\">Sana Awan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Chao Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferability Estimation for Semantic Segmentation Task. (arXiv:2109.15242v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15242","description":"<p>Transferability estimation is a fundamental problem in transfer learning to\npredict how good the performance is when transferring a source model (or source\ntask) to a target task. With the guidance of transferability score, we can\nefficiently select the highly transferable source models without performing the\nreal transfer in practice. Recent analytical transferability metrics are mainly\ndesigned for image classification problem, and currently there is no specific\ninvestigation for the transferability estimation of semantic segmentation task,\nwhich is an essential problem in autonomous driving, medical image analysis,\netc. Consequently, we further extend the recent analytical transferability\nmetric OTCE (Optimal Transport based Conditional Entropy) score to the semantic\nsegmentation task. The challenge in applying the OTCE score is the high\ndimensional segmentation output, which is difficult to find the optimal\ncoupling between so many pixels under an acceptable computation cost. Thus we\npropose to randomly sample N pixels for computing OTCE score and take the\nexpectation over K repetitions as the final transferability score. Experimental\nevaluation on Cityscapes, BDD100K and GTA5 datasets demonstrates that the OTCE\nscore highly correlates with the transfer performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Few-Shot Action Recognition via Action-Appearance Aligned Meta-Adaptation. (arXiv:2109.15317v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15317","description":"<p>We present MetaUVFS as the first Unsupervised Meta-learning algorithm for\nVideo Few-Shot action recognition. MetaUVFS leverages over 550K unlabeled\nvideos to train a two-stream 2D and 3D CNN architecture via contrastive\nlearning to capture the appearance-specific spatial and action-specific\nspatio-temporal video features respectively. MetaUVFS comprises a novel\nAction-Appearance Aligned Meta-adaptation (A3M) module that learns to focus on\nthe action-oriented video features in relation to the appearance features via\nexplicit few-shot episodic meta-learning over unsupervised hard-mined episodes.\nOur action-appearance alignment and explicit few-shot learner conditions the\nunsupervised training to mimic the downstream few-shot task, enabling MetaUVFS\nto significantly outperform all unsupervised methods on few-shot benchmarks.\nMoreover, unlike previous few-shot action recognition methods that are\nsupervised, MetaUVFS needs neither base-class labels nor a supervised\npretrained backbone. Thus, we need to train MetaUVFS just once to perform\ncompetitively or sometimes even outperform state-of-the-art supervised methods\non popular HMDB51, UCF101, and Kinetics100 few-shot datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patravali_J/0/1/0/all/0/1\">Jay Patravali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1\">Gaurav Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Ye Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fuxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Event Recognition. (arXiv:2110.00755v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00755","description":"<p>The literature shows outstanding capabilities for CNNs in event recognition\nin images. However, fewer attempts are made to analyze the potential causes\nbehind the decisions of the models and exploring whether the predictions are\nbased on event-salient objects or regions? To explore this important aspect of\nevent recognition, in this work, we propose an explainable event recognition\nframework relying on Grad-CAM and an Xception architecture-based CNN model.\nExperiments are conducted on three large-scale datasets covering a diversified\nset of natural disasters, social, and sports events. Overall, the model showed\noutstanding generalization capabilities obtaining overall F1-scores of 0.91,\n0.94, and 0.97 on natural disasters, social, and sports events, respectively.\nMoreover, for subjective analysis of activation maps generated through Grad-CAM\nfor the predicted samples of the model, a crowdsourcing study is conducted to\nanalyze whether the model's predictions are based on event-related\nobjects/regions or not? The results of the study indicate that 78%, 84%, and\n78% of the model decisions on natural disasters, sports, and social events\ndatasets, respectively, are based onevent-related objects or regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_I/0/1/0/all/0/1\">Imran Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Kashif Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gul_N/0/1/0/all/0/1\">Namra Gul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1\">Talhat Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_N/0/1/0/all/0/1\">Nasir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1\">Ala Al-Fuqaha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does deep learning model calibration improve performance in class-imbalanced medical image classification?. (arXiv:2110.00918v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.00918","description":"<p>In medical image classification tasks, it is common to find that the number\nof normal samples far exceeds the number of abnormal samples. In such\nclass-imbalanced situations, reliable training of deep neural networks\ncontinues to be a major challenge. Under these circumstances, the predicted\nclass probabilities may be biased toward the majority class. Calibration has\nbeen suggested to alleviate some of these effects. However, there is\ninsufficient analysis explaining when and whether calibrating a model would be\nbeneficial in improving performance. In this study, we perform a systematic\nanalysis of the effect of model calibration on its performance on two medical\nimage modalities, namely, chest X-rays and fundus images, using various deep\nlearning classifier backbones. For this, we study the following variations: (i)\nthe degree of imbalances in the dataset used for training; (ii) calibration\nmethods; and (iii) two classification thresholds, namely, default decision\nthreshold of 0.5, and optimal threshold from precision-recall curves. Our\nresults indicate that at the default operating threshold of 0.5, the\nperformance achieved through calibration is significantly superior (p &lt; 0.05)\nto using uncalibrated probabilities. However, at the PR-guided threshold, these\ngains are not significantly different (p &gt; 0.05). This finding holds for both\nimage modalities and at varying degrees of imbalance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajaraman_S/0/1/0/all/0/1\">Sivaramakrishnan Rajaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_P/0/1/0/all/0/1\">Prasanth Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antani_S/0/1/0/all/0/1\">Sameer Antani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Representation Learning for Spatial Image Steganalysis. (arXiv:2110.00957v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2110.00957","description":"<p>In this paper, we introduce a graph representation learning architecture for\nspatial image steganalysis, which is motivated by the assumption that\nsteganographic modifications unavoidably distort the statistical\ncharacteristics of the hidden graph features derived from cover images. In the\ndetailed architecture, we translate each image to a graph, where nodes\nrepresent the patches of the image and edges indicate the local associations\nbetween the patches. Each node is associated with a feature vector determined\nfrom the corresponding patch by a shallow convolutional neural network (CNN)\nstructure. By feeding the graph to an attention network, the discriminative\nfeatures can be learned for efficient steganalysis. Experiments indicate that\nthe reported architecture achieves a competitive performance compared to the\nbenchmark CNN model, which has shown the potential of graph learning for\nsteganalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiyun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A free lunch from ViT:Adaptive Attention Multi-scale Fusion Transformer for Fine-grained Visual Recognition. (arXiv:2110.01240v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01240","description":"<p>Learning subtle representation about object parts plays a vital role in\nfine-grained visual recognition (FGVR) field. The vision transformer (ViT)\nachieves promising results on computer vision due to its attention mechanism.\nNonetheless, with the fixed size of patches in ViT, the class token in deep\nlayer focuses on the global receptive field and cannot generate\nmulti-granularity features for FGVR. To capture region attention without box\nannotations and compensate for ViT shortcomings in FGVR, we propose a novel\nmethod named Adaptive attention multi-scale Fusion Transformer (AFTrans). The\nSelective Attention Collection Module (SACM) in our approach leverages\nattention weights in ViT and filters them adaptively to correspond with the\nrelative importance of input patches. The multiple scales (global and local)\npipeline is supervised by our weights sharing encoder and can be easily trained\nend-to-end. Comprehensive experiments demonstrate that AFTrans can achieve SOTA\nperformance on three published fine-grained benchmarks: CUB-200-2011, Stanford\nDogs and iNat2017.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jian Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Ling Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangcheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_F/0/1/0/all/0/1\">Feng Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weiqian Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Procedure Planning in Instructional Videos via Contextual Modeling and Model-based Policy Learning. (arXiv:2110.01770v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01770","description":"<p>Learning new skills by observing humans' behaviors is an essential capability\nof AI. In this work, we leverage instructional videos to study humans'\ndecision-making processes, focusing on learning a model to plan goal-directed\nactions in real-life videos. In contrast to conventional action recognition,\ngoal-directed actions are based on expectations of their outcomes requiring\ncausal knowledge of potential consequences of actions. Thus, integrating the\nenvironment structure with goals is critical for solving this task. Previous\nworks learn a single world model will fail to distinguish various tasks,\nresulting in an ambiguous latent space; planning through it will gradually\nneglect the desired outcomes since the global information of the future goal\ndegrades quickly as the procedure evolves. We address these limitations with a\nnew formulation of procedure planning and propose novel algorithms to model\nhuman behaviors through Bayesian Inference and model-based Imitation Learning.\nExperiments conducted on real-world instructional videos show that our method\ncan achieve state-of-the-art performance in reaching the indicated goals.\nFurthermore, the learned contextual information presents interesting features\nfor planning in a latent space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jing Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning of Perceptually Optimized Block Motion Estimates for Video Compression. (arXiv:2110.01805v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.01805","description":"<p>Block based motion estimation is integral to inter prediction processes\nperformed in hybrid video codecs. Prevalent block matching based methods that\nare used to compute block motion vectors (MVs) rely on computationally\nintensive search procedures. They also suffer from the aperture problem, which\ncan worsen as the block size is reduced. Moreover, the block matching criteria\nused in typical codecs do not account for the resulting levels of perceptual\nquality of the motion compensated pictures that are created upon decoding.\nTowards achieving the elusive goal of perceptually optimized motion estimation,\nwe propose a search-free block motion estimation framework using a multi-stage\nconvolutional neural network, which is able to conduct motion estimation on\nmultiple block sizes simultaneously, using a triplet of frames as input. This\ncomposite block translation network (CBT-Net) is trained in a self-supervised\nmanner on a large database that we created from publicly available uncompressed\nvideo content. We deploy the multi-scale structural similarity (MS-SSIM) loss\nfunction to optimize the perceptual quality of the motion compensated predicted\nframes. Our experimental results highlight the computational efficiency of our\nproposed model relative to conventional block matching based motion estimation\nalgorithms, for comparable prediction errors. Further, when used to perform\ninter prediction in AV1, the MV predictions of the perceptually optimized model\nresult in average Bjontegaard-delta rate (BD-rate) improvements of -1.70% and\n-1.52% with respect to the MS-SSIM and Video Multi-Method Assessment Fusion\n(VMAF) quality metrics, respectively as compared to the block matching based\nmotion estimation system employed in the SVT-AV1 encoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Paul_S/0/1/0/all/0/1\">Somdyuti Paul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norkin_A/0/1/0/all/0/1\">Andrey Norkin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Computer Vision Technologies for Fish Tracking. (arXiv:2110.02551v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02551","description":"<p>Fish tracking based on computer vision is a complex and challenging task in\nfishery production and ecological studies. Most of the applications of fish\ntracking use classic filtering algorithms, which lack in accuracy and\nefficiency. To solve this issue, deep learning methods utilized deep neural\nnetworks to extract the features, which achieve a good performance in the fish\ntracking. Some one-stage detection algorithms have gradually been adopted in\nthis area for the real-time applications. The transfer learning to fish target\nis the current development direction. At present, fish tracking technology is\nnot enough to cover actual application requirements. According to the\nliterature data collected by us, there has not been any extensive review about\nvision-based fish tracking in the community. In this paper, we introduced the\ndevelopment and application prospects of fish tracking technology in last ten\nyears. Firstly, we introduced the open source datasets of fish, and summarized\nthe preprocessing technologies of underwater images. Secondly, we analyzed the\ndetection and tracking algorithms for fish, and sorted out some transferable\nfrontier tracking model. Thirdly, we listed the actual applications, metrics\nand bottlenecks of the fish tracking such as occlusion and multi-scale.\nFinally, we give the discussion for fish tracking datasets, solutions of the\nbottlenecks, and improvements. We expect that our work can help the fish\ntracking models to achieve higher accuracy and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Meng Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Robustness Comparison of Vision Transformer and MLP-Mixer to CNNs. (arXiv:2110.02797v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02797","description":"<p>Convolutional Neural Networks (CNNs) have become the de facto gold standard\nin computer vision applications in the past years. Recently, however, new model\narchitectures have been proposed challenging the status quo. The Vision\nTransformer (ViT) relies solely on attention modules, while the MLP-Mixer\narchitecture substitutes the self-attention modules with Multi-Layer\nPerceptrons (MLPs). Despite their great success, CNNs have been widely known to\nbe vulnerable to adversarial attacks, causing serious concerns for\nsecurity-sensitive applications. Thus, it is critical for the community to know\nwhether the newly proposed ViT and MLP-Mixer are also vulnerable to adversarial\nattacks. To this end, we empirically evaluate their adversarial robustness\nunder several adversarial attack setups and benchmark them against the widely\nused CNNs. Overall, we find that the two architectures, especially ViT, are\nmore robust than their CNN models. Using a toy example, we also provide\nempirical evidence that the lower adversarial robustness of CNNs can be\npartially attributed to their shift-invariant property. Our frequency analysis\nsuggests that the most robust ViT architectures tend to rely more on\nlow-frequency features compared with CNNs. Additionally, we have an intriguing\nfinding that MLP-Mixer is extremely vulnerable to universal adversarial\nperturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benz_P/0/1/0/all/0/1\">Philipp Benz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_S/0/1/0/all/0/1\">Soomin Ham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karjauv_A/0/1/0/all/0/1\">Adil Karjauv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TreeGCN-ED: Encoding Point Cloud using a Tree-Structured Graph Network. (arXiv:2110.03170v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03170","description":"<p>Point cloud is an efficient way of representing and storing 3D geometric\ndata. Deep learning algorithms on point clouds are time and memory efficient.\nSeveral methods such as PointNet and FoldingNet have been proposed for\nprocessing point clouds. This work proposes an autoencoder based framework to\ngenerate robust embeddings for point clouds by utilizing hierarchical\ninformation using graph convolution. We perform multiple experiments to assess\nthe quality of embeddings generated by the proposed encoder architecture and\nvisualize the t-SNE map to highlight its ability to distinguish between\ndifferent object classes. We further demonstrate the applicability of the\nproposed framework in applications like: 3D point cloud completion and Single\nimage based 3D reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prajwal Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadekar_K/0/1/0/all/0/1\">Kaustubh Sadekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token Pooling in Vision Transformers. (arXiv:2110.03860v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03860","description":"<p>Despite the recent success in many applications, the high computational\nrequirements of vision transformers limit their use in resource-constrained\nsettings. While many existing methods improve the quadratic complexity of\nattention, in most vision transformers, self-attention is not the major\ncomputation bottleneck, e.g., more than 80% of the computation is spent on\nfully-connected layers. To improve the computational complexity of all layers,\nwe propose a novel token downsampling method, called Token Pooling, efficiently\nexploiting redundancies in the images and intermediate token representations.\nWe show that, under mild assumptions, softmax-attention acts as a\nhigh-dimensional low-pass (smoothing) filter. Thus, its output contains\nredundancy that can be pruned to achieve a better trade-off between the\ncomputational cost and accuracy. Our new technique accurately approximates a\nset of tokens by minimizing the reconstruction error caused by downsampling. We\nsolve this optimization problem via cost-efficient clustering. We rigorously\nanalyze and compare to prior downsampling methods. Our experiments show that\nToken Pooling significantly improves the cost-accuracy trade-off over the\nstate-of-the-art downsampling. Token Pooling is a simple and effective operator\nthat can benefit many architectures. Applied to DeiT, it achieves the same\nImageNet top-1 accuracy using 42% fewer computations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marin_D/0/1/0/all/0/1\">Dmitrii Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jen-Hao Rick Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1\">Anurag Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_A/0/1/0/all/0/1\">Anish Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1\">Oncel Tuzel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi Proxy Anchor Loss and Effectiveness of Deep Metric Learning Performance Metrics. (arXiv:2110.03997v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03997","description":"<p>Deep metric learning (DML) learns the mapping, which maps into embedding\nspace in which similar data is near and dissimilar data is far. In this paper,\nwe propose the new proxy-based loss and the new DML performance metric. This\nstudy contributes two following: (1) we propose multi-proxies anchor (MPA)\nloss, and we show the effectiveness of the multi-proxies approach on\nproxy-based loss. (2) we establish the good stability and flexible normalized\ndiscounted cumulative gain (nDCG@k) metric as the effective DML performance\nmetric. Finally, we demonstrate MPA loss's effectiveness, and MPA loss achieves\nnew state-of-the-art performance on two datasets for fine-grained images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeki_S/0/1/0/all/0/1\">Shozo Saeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_M/0/1/0/all/0/1\">Minoru Kawahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aman_H/0/1/0/all/0/1\">Hirohisa Aman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Spatial Nonstationarity via Deformable Convolutions for Deep Traffic Flow Prediction. (arXiv:2101.12010v2 [physics.soc-ph] CROSS LISTED)","link":"http://arxiv.org/abs/2101.12010","description":"<p>Deep neural networks are being increasingly used for short-term traffic flow\nprediction, which can be generally categorized as convolutional (CNNs) or graph\nneural networks (GNNs). CNNs are preferable for region-wise traffic prediction\nby taking advantage of localized spatial correlations, whilst GNNs achieves\nbetter performance for graph-structured traffic data. When applied to\nregion-wise traffic prediction, CNNs typically partition an underlying\nterritory into grid-like spatial units, and employ standard convolutions to\nlearn spatial dependence among the units. However, standard convolutions with\nfixed geometric structures cannot fully model the nonstationary characteristics\nof local traffic flows. To overcome the deficiency, we introduce deformable\nconvolution that augments the spatial sampling locations with additional\noffsets, to enhance the modeling capability of spatial nonstationarity. On this\nbasis, we design a deep deformable convolutional residual network, namely\nDeFlow-Net, that can effectively model global spatial dependence, local spatial\nnonstationarity, and temporal periodicity of traffic flows. Furthermore, to\nbetter fit with convolutions, we suggest to first aggregate traffic flows\naccording to pre-conceived regions or self-organized regions based on traffic\nflows, then dispose to sequentially organized raster images for network input.\nExtensive experiments on real-world traffic flows demonstrate that DeFlow-Net\noutperforms GNNs and existing CNNs using standard convolutions, and spatial\npartition by pre-conceived regions or self-organized regions further enhances\nthe performance. We also demonstrate the advantage of DeFlow-Net in maintaining\nspatial autocorrelation, and reveal the impacts of partition shapes and scales\non deep traffic flow prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Zeng_W/0/1/0/all/0/1\">Wei Zeng</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lin_C/0/1/0/all/0/1\">Chengqiao Lin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lin_J/0/1/0/all/0/1\">Juncong Lin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tung_A/0/1/0/all/0/1\">Anthony K. H. Tung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image scaling by de la Vall\\'ee-Poussin filtered interpolation. (arXiv:2109.13897v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2109.13897","description":"<p>We present a new image scaling method both for downscaling and upscaling,\nrunning with any scale factor or desired size. It is based on the sampling of\nan approximating bivariate polynomial, which globally interpolates the data and\nis defined by a filter of de la Vall\\'ee Poussin type whose action ray is\nsuitable regulated to improve the approximation. The method has been tested on\na significant number of different image datasets. The results are evaluated in\nqualitative and quantitative terms and compared with other available\ncompetitive methods. The perceived quality of the resulting scaled images is\nsuch that important details are preserved, and the appearance of artifacts is\nlow. Very high-quality measure values in downscaling and the competitive ones\nin upscaling evidence the effectiveness of the method. Good visual quality,\nlimited computational effort, and moderate memory demanding make the method\nsuitable for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Occorsio_D/0/1/0/all/0/1\">Donatella Occorsio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramella_G/0/1/0/all/0/1\">Giuliana Ramella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Themistoclakis_W/0/1/0/all/0/1\">Woula Themistoclakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}