{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Decision-Focused Summarization. (arXiv:2109.06896v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06896","description":"<p>Relevance in summarization is typically defined based on textual information\nalone, without incorporating insights about a particular decision. As a result,\nto support risk analysis of pancreatic cancer, summaries of medical notes may\ninclude irrelevant information such as a knee injury. We propose a novel\nproblem, decision-focused summarization, where the goal is to summarize\nrelevant information for a decision. We leverage a predictive model that makes\nthe decision based on the full text to provide valuable insights on how a\ndecision can be inferred from text. To build a summary, we then select\nrepresentative sentences that lead to similar model decisions as using the full\ntext while accounting for textual non-redundancy. To evaluate our method\n(DecSum), we build a testbed where the task is to summarize the first ten\nreviews of a restaurant in support of predicting its future rating on Yelp.\nDecSum substantially outperforms text-only summarization methods and\nmodel-based explanation methods in decision faithfulness and\nrepresentativeness. We further demonstrate that DecSum is the only method that\nenables humans to outperform random chance in predicting which restaurant will\nbe better rated in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chao-Chun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"fairseq S^2: A Scalable and Integrable Speech Synthesis Toolkit. (arXiv:2109.06912v1 [eess.AS])","link":"http://arxiv.org/abs/2109.06912","description":"<p>This paper presents fairseq S^2, a fairseq extension for speech synthesis. We\nimplement a number of autoregressive (AR) and non-AR text-to-speech models, and\ntheir multi-speaker variants. To enable training speech synthesis models with\nless curated data, a number of preprocessing tools are built and their\nimportance is shown empirically. To facilitate faster iteration of development\nand analysis, a suite of automatic metrics is included. Apart from the features\nadded specifically for this extension, fairseq S^2 also benefits from the\nscalability offered by fairseq and can be easily integrated with other\nstate-of-the-art systems provided in this framework. The code, documentation,\nand pre-trained models are available at\nhttps://github.com/pytorch/fairseq/tree/master/examples/speech_synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Language-specificity of Multilingual BERT and the Impact of Fine-tuning. (arXiv:2109.06935v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06935","description":"<p>Recent work has shown evidence that the knowledge acquired by multilingual\nBERT (mBERT) has two components: a language-specific and a language-neutral\none. This paper analyses the relationship between them, in the context of\nfine-tuning on two tasks -- POS tagging and natural language inference -- which\nrequire the model to bring to bear different degrees of language-specific\nknowledge. Visualisations reveal that mBERT loses the ability to cluster\nrepresentations by language after fine-tuning, a result that is supported by\nevidence from language identification experiments. However, further experiments\non 'unlearning' language-specific representations using gradient reversal and\niterative adversarial learning are shown not to add further improvement to the\nlanguage-independent component over and above the effect of fine-tuning. The\nresults presented here suggest that the process of fine-tuning causes a\nreorganisation of the model's limited representational capacity, enhancing\nlanguage-independent representations at the expense of language-specific ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanti_M/0/1/0/all/0/1\">Marc Tanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plas_L/0/1/0/all/0/1\">Lonneke van der Plas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borg_C/0/1/0/all/0/1\">Claudia Borg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders. (arXiv:2109.06939v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06939","description":"<p>Multi-task learning with transformer encoders (MTL) has emerged as a powerful\ntechnique to improve performance on closely-related tasks for both accuracy and\nefficiency while a question still remains whether or not it would perform as\nwell on tasks that are distinct in nature. We first present MTL results on five\nNLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over\nsingle-task learning. We then conduct an extensive pruning analysis to show\nthat a certain set of attention heads get claimed by most tasks during MTL, who\ninterfere with one another to fine-tune those heads for their own objectives.\nBased on this finding, we propose the Stem Cell Hypothesis to reveal the\nexistence of attention heads naturally talented for many tasks that cannot be\njointly trained to create adequate embeddings for all of those tasks. Finally,\nwe design novel parameter-free probes to justify our hypothesis and demonstrate\nhow attention heads are transformed across the five tasks during MTL through\nlabel analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Han He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatically Exposing Problems with Neural Dialog Models. (arXiv:2109.06950v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06950","description":"<p>Neural dialog models are known to suffer from problems such as generating\nunsafe and inconsistent responses. Even though these problems are crucial and\nprevalent, they are mostly manually identified by model designers through\ninteractions. Recently, some research instructs crowdworkers to goad the bots\ninto triggering such problems. However, humans leverage superficial clues such\nas hate speech, while leaving systematic problems undercover. In this paper, we\npropose two methods including reinforcement learning to automatically trigger a\ndialog model into generating problematic responses. We show the effect of our\nmethods in exposing safety and contradiction issues with state-of-the-art\ndialog models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagae_K/0/1/0/all/0/1\">Kenji Sagae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Adapters for Parameter-Efficient ASR Adaptation to Atypical and Accented Speech. (arXiv:2109.06952v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06952","description":"<p>Automatic Speech Recognition (ASR) systems are often optimized to work best\nfor speakers with canonical speech patterns. Unfortunately, these systems\nperform poorly when tested on atypical speech and heavily accented speech. It\nhas previously been shown that personalization through model fine-tuning\nsubstantially improves performance. However, maintaining such large models per\nspeaker is costly and difficult to scale. We show that by adding a relatively\nsmall number of extra parameters to the encoder layers via so-called residual\nadapter, we can achieve similar adaptation gains compared to model fine-tuning,\nwhile only updating a tiny fraction (less than 0.5%) of the model parameters.\nWe demonstrate this on two speech adaptation tasks (atypical and accented\nspeech) and for two state-of-the-art ASR architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zayats_V/0/1/0/all/0/1\">Vicky Zayats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padfield_D/0/1/0/all/0/1\">Dirk Padfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaillancourt_K/0/1/0/all/0/1\">Kara Vaillancourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biadsy_F/0/1/0/all/0/1\">Fadi Biadsy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for More Efficient Dynamic Programs. (arXiv:2109.06966v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06966","description":"<p>Computational models of human language often involve combinatorial problems.\nFor instance, a probabilistic parser may marginalize over exponentially many\ntrees to make predictions. Algorithms for such problems often employ dynamic\nprogramming and are not always unique. Finding one with optimal asymptotic\nruntime can be unintuitive, time-consuming, and error-prone. Our work aims to\nautomate this laborious process. Given an initial correct declarative program,\nwe search for a sequence of semantics-preserving transformations to improve its\nrunning time as much as possible. To this end, we describe a set of program\ntransformations, a simple metric for assessing the efficiency of a transformed\nprogram, and a heuristic search procedure to improve this metric. We show that\nin practice, automated search -- like the mental search performed by human\nprogrammers -- can find substantial improvements to the initial program.\nEmpirically, we show that many common speed-ups described in the NLP literature\ncould have been discovered automatically by our system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Identification of Dementia from Transcripts using Transformer Networks. (arXiv:2109.06980v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06980","description":"<p>Alzheimer's disease (AD) is the main cause of dementia which is accompanied\nby loss of memory and may lead to severe consequences in peoples' everyday life\nif not diagnosed on time. Very few works have exploited transformer-based\nnetworks and despite the high accuracy achieved, little work has been done in\nterms of model interpretability. In addition, although Mini-Mental State Exam\n(MMSE) scores are inextricably linked with the identification of dementia,\nresearch works face the task of dementia identification and the task of the\nprediction of MMSE scores as two separate tasks. In order to address these\nlimitations, we employ several transformer-based models, with BERT achieving\nthe highest accuracy accounting for 85.56%. Concurrently, we propose an\ninterpretable method to detect AD patients based on siamese networks reaching\naccuracy up to 81.18%. Next, we introduce two multi-task learning models, where\nthe main task refers to the identification of dementia (binary classification),\nwhile the auxiliary one corresponds to the identification of the severity of\ndementia (multiclass classification). Our model obtains accuracy equal to\n84.99% on the detection of AD patients in the multi-task learning setting.\nFinally, we present some new methods to identify the linguistic patterns used\nby AD patients and non-AD ones, including text statistics, vocabulary\nuniqueness, word usage, correlations via a detailed linguistic analysis, and\nexplainability techniques (LIME). Findings indicate significant differences in\nlanguage between AD and non-AD patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NOPE: A Corpus of Naturally-Occurring Presuppositions in English. (arXiv:2109.06987v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06987","description":"<p>Understanding language requires grasping not only the overtly stated content,\nbut also making inferences about things that were left unsaid. These inferences\ninclude presuppositions, a phenomenon by which a listener learns about new\ninformation through reasoning about what a speaker takes as given.\nPresuppositions require complex understanding of the lexical and syntactic\nproperties that trigger them as well as the broader conversational context. In\nthis work, we introduce the Naturally-Occurring Presuppositions in English\n(NOPE) Corpus to investigate the context-sensitivity of 10 different types of\npresupposition triggers and to evaluate machine learning models' ability to\npredict human inferences. We find that most of the triggers we investigate\nexhibit moderate variability. We further find that transformer-based models\ndraw correct inferences in simple cases involving presuppositions, but they\nfail to capture the minority of exceptional cases in which human judgments\nreveal complex interactions between context and triggers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1\">Alicia Parrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_S/0/1/0/all/0/1\">Sebastian Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1\">Alex Warstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agha_O/0/1/0/all/0/1\">Omar Agha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soo-Hwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhuoye Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Three Step Training Approach with Data Augmentation for Morphological Inflection. (arXiv:2109.07006v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07006","description":"<p>We present the BME submission for the SIGMORPHON 2021 Task 0 Part 1,\nGeneralization Across Typologically Diverse Languages shared task. We use an\nLSTM encoder-decoder model with three step training that is first trained on\nall languages, then fine-tuned on each language families and finally finetuned\non individual languages. We use a different type of data augmentation technique\nin the first two steps. Our system outperformed the only other submission.\nAlthough it remains worse than the Transformer baseline released by the\norganizers, our model is simpler and our data augmentation techniques are\neasily applicable to new languages. We perform ablation studies and show that\nthe augmentation techniques and the three training steps often help but\nsometimes have a negative effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szolnok_G/0/1/0/all/0/1\">Gabor Szolnok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barta_B/0/1/0/all/0/1\">Botond Barta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakatos_D/0/1/0/all/0/1\">Dorina Lakatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acs_J/0/1/0/all/0/1\">Judit Acs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering. (arXiv:2109.07009v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07009","description":"<p>In this paper we propose a novel approach towards improving the efficiency of\nQuestion Answering (QA) systems by filtering out questions that will not be\nanswered by them. This is based on an interesting new finding: the answer\nconfidence scores of state-of-the-art QA systems can be approximated well by\nmodels solely using the input question text. This enables preemptive filtering\nof questions that are not answered by the system due to their answer confidence\nscores being lower than the system threshold. Specifically, we learn\nTransformer-based question models by distilling Transformer-based answering\nmodels. Our experiments on three popular QA datasets and one industrial QA\nbenchmark demonstrate the ability of our question models to approximate the\nPrecision/Recall curves of the target QA system well. These question models,\nwhen used as filters, can effectively trade off lower computation cost of QA\nsystems for lower Recall, e.g., reducing computation by ~60%, while only losing\n~3-4% of Recall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddhant Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Written Justifications are Key to Aggregate Crowdsourced Forecasts. (arXiv:2109.07017v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07017","description":"<p>This paper demonstrates that aggregating crowdsourced forecasts benefits from\nmodeling the written justifications provided by forecasters. Our experiments\nshow that the majority and weighted vote baselines are competitive, and that\nthe written justifications are beneficial to call a question throughout its\nlife except in the last quarter. We also conduct an error analysis shedding\nlight into the characteristics that make a justification unreliable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotamraju_S/0/1/0/all/0/1\">Saketh Kotamraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_E/0/1/0/all/0/1\">Eduardo Blanco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency Effects on Syntactic Rule Learning in Transformers. (arXiv:2109.07020v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07020","description":"<p>Pre-trained language models perform well on a variety of linguistic tasks\nthat require symbolic reasoning, raising the question of whether such models\nimplicitly represent abstract symbols and rules. We investigate this question\nusing the case study of BERT's performance on English subject-verb agreement.\nUnlike prior work, we train multiple instances of BERT from scratch, allowing\nus to perform a series of controlled interventions at pre-training time. We\nshow that BERT often generalizes well to subject-verb pairs that never occurred\nin training, suggesting a degree of rule-governed behavior. We also find,\nhowever, that performance is heavily influenced by word frequency, with\nexperiments showing that both the absolute frequency of a verb form, as well as\nthe frequency relative to the alternate inflection, are causally implicated in\nthe predictions BERT makes at inference time. Closer analysis of these\nfrequency effects reveals that BERT's behavior is consistent with a system that\ncorrectly applies the SVA rule in general but struggles to overcome strong\ntraining priors and to estimate agreement features (singular vs. plural) on\ninfrequent lexical items.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1\">Dan Garrette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Is Indeed All You Need: Semantically Attention-Guided Decoding for Data-to-Text NLG. (arXiv:2109.07043v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07043","description":"<p>Ever since neural models were adopted in data-to-text language generation,\nthey have invariably been reliant on extrinsic components to improve their\nsemantic accuracy, because the models normally do not exhibit the ability to\ngenerate text that reliably mentions all of the information provided in the\ninput. In this paper, we propose a novel decoding method that extracts\ninterpretable information from encoder-decoder models' cross-attention, and\nuses it to infer which attributes are mentioned in the generated text, which is\nsubsequently used to rescore beam hypotheses. Using this decoding method with\nT5 and BART, we show on three datasets its ability to dramatically reduce\nsemantic errors in the generated outputs, while maintaining their\nstate-of-the-art quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Juraska_J/0/1/0/all/0/1\">Juraj Juraska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_M/0/1/0/all/0/1\">Marilyn Walker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Conditional Generative Matching Model for Multi-lingual Reply Suggestion. (arXiv:2109.07046v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07046","description":"<p>We study the problem of multilingual automated reply suggestions (RS) model\nserving many languages simultaneously. Multilingual models are often challenged\nby model capacity and severe data distribution skew across languages. While\nprior works largely focus on monolingual models, we propose Conditional\nGenerative Matching models (CGM), optimized within a Variational Autoencoder\nframework to address challenges arising from multi-lingual RS. CGM does so with\nexpressive message conditional priors, mixture densities to enhance\nmulti-lingual data representation, latent alignment for language\ndiscrimination, and effective variational optimization techniques for training\nmulti-lingual RS. The enhancements result in performance that exceed\ncompetitive baselines in relevance (ROUGE score) by more than 10\\% on average,\nand 16\\% for low resource languages. CGM also shows remarkable improvements in\ndiversity (80\\%) illustrating its expressiveness in representation of\nmulti-lingual data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1\">Budhaditya Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1\">Milad Shokouhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARCH: Efficient Adversarial Regularized Training with Caching. (arXiv:2109.07048v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07048","description":"<p>Adversarial regularization can improve model generalization in many natural\nlanguage processing tasks. However, conventional approaches are computationally\nexpensive since they need to generate a perturbation for each sample in each\nepoch. We propose a new adversarial regularization method ARCH (adversarial\nregularization with caching), where perturbations are generated and cached once\nevery several epochs. As caching all the perturbations imposes memory usage\nconcerns, we adopt a K-nearest neighbors-based strategy to tackle this issue.\nThe strategy only requires caching a small amount of perturbations, without\nintroducing additional training time. We evaluate our proposed method on a set\nof neural machine translation and natural language understanding tasks. We\nobserve that ARCH significantly eases the computational burden (saves up to\n70\\% of computational time in comparison with conventional approaches). More\nsurprisingly, by reducing the variance of stochastic gradients, ARCH produces a\nnotably better (in most of the tasks) or comparable model generalization. Our\ncode is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Training with Differentiable Teacher. (arXiv:2109.07049v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07049","description":"<p>Self-training achieves enormous success in various semi-supervised and\nweakly-supervised learning tasks. The method can be interpreted as a\nteacher-student framework, where the teacher generates pseudo-labels, and the\nstudent makes predictions. The two models are updated alternatingly. However,\nsuch a straightforward alternating update rule leads to training instability.\nThis is because a small change in the teacher may result in a significant\nchange in the student. To address this issue, we propose {\\ours}, short for\ndifferentiable self-training, that treats teacher-student as a Stackelberg\ngame. In this game, a leader is always in a more advantageous position than a\nfollower. In self-training, the student contributes to the prediction\nperformance, and the teacher controls the training process by generating\npseudo-labels. Therefore, we treat the student as the leader and the teacher as\nthe follower. The leader procures its advantage by acknowledging the follower's\nstrategy, which involves differentiable pseudo-labels and differentiable sample\nweights. Consequently, the leader-follower interaction can be effectively\ncaptured via Stackelberg gradient, obtained by differentiating the follower's\nstrategy. Experimental results on semi- and weakly-supervised classification\nand named entity recognition tasks show that our model outperforms existing\napproaches by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Er_S/0/1/0/all/0/1\">Siawpeng Er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1\">Hongyuan Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Text Auto-Completion with Next Phrase Prediction. (arXiv:2109.07067v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07067","description":"<p>Language models such as GPT-2 have performed well on constructing\nsyntactically sound sentences for text auto-completion task. However, such\nmodels often require considerable training effort to adapt to specific writing\ndomains (e.g., medical). In this paper, we propose an intermediate training\nstrategy to enhance pre-trained language models' performance in the text\nauto-completion task and fastly adapt them to specific domains. Our strategy\nincludes a novel self-supervised training objective called Next Phrase\nPrediction (NPP), which encourages a language model to complete the partial\nquery with enriched phrases and eventually improve the model's text\nauto-completion performance. Preliminary experiments have shown that our\napproach is able to outperform the baselines in auto-completion for email and\nacademic writing domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Lexically Constrained Headline Generation. (arXiv:2109.07080v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07080","description":"<p>This paper explores a variant of automatic headline generation methods, where\na generated headline is required to include a given phrase such as a company or\na product name. Previous methods using Transformer-based models generate a\nheadline including a given phrase by providing the encoder with additional\ninformation corresponding to the given phrase. However, these methods cannot\nalways include the phrase in the generated headline. Inspired by previous\nRNN-based methods generating token sequences in backward and forward directions\nfrom the given phrase, we propose a simple Transformer-based method that\nguarantees to include the given phrase in the high-quality generated headline.\nWe also consider a new headline generation strategy that takes advantage of the\ncontrollable generation order of Transformer. Our experiments with the Japanese\nNews Corpus demonstrate that our methods, which are guaranteed to include the\nphrase in the generated headline, achieve ROUGE scores comparable to previous\nTransformer-based methods. We also show that our generation strategy performs\nbetter than previous strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamada_K/0/1/0/all/0/1\">Kosuke Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hitomi_Y/0/1/0/all/0/1\">Yuta Hitomi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamori_H/0/1/0/all/0/1\">Hideaki Tamori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasano_R/0/1/0/all/0/1\">Ryohei Sasano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Koichi Takeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Extraction of Word Embedding from Q-contexts. (arXiv:2109.07084v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07084","description":"<p>The notion of word embedding plays a fundamental role in natural language\nprocessing (NLP). However, pre-training word embedding for very large-scale\nvocabulary is computationally challenging for most existing methods. In this\nwork, we show that with merely a small fraction of contexts (Q-contexts)which\nare typical in the whole corpus (and their mutual information with words), one\ncan construct high-quality word embedding with negligible errors. Mutual\ninformation between contexts and words can be encoded canonically as a sampling\nstate, thus, Q-contexts can be fast constructed. Furthermore, we present an\nefficient and effective WEQ method, which is capable of extracting word\nembedding directly from these typical contexts. In practical scenarios, our\nalgorithm runs 11$\\sim$13 times faster than well-established methods. By\ncomparing with well-known methods such as matrix factorization, word2vec,\nGloVeand fasttext, we demonstrate that our method achieves comparable\nperformance on a variety of downstream NLP tasks, and in the meanwhile\nmaintains run-time and resource advantages over all these baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_J/0/1/0/all/0/1\">Junsheng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weizhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Ben Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiezhong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chang-Yu Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Document-Level Paraphrase Generation with Sentence Rewriting and Reordering. (arXiv:2109.07095v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07095","description":"<p>Paraphrase generation is an important task in natural language processing.\nPrevious works focus on sentence-level paraphrase generation, while ignoring\ndocument-level paraphrase generation, which is a more challenging and valuable\ntask. In this paper, we explore the task of document-level paraphrase\ngeneration for the first time and focus on the inter-sentence diversity by\nconsidering sentence rewriting and reordering. We propose CoRPG (Coherence\nRelationship guided Paraphrase Generation), which leverages graph GRU to encode\nthe coherence relationship graph and get the coherence-aware representation for\neach sentence, which can be used for re-arranging the multiple (possibly\nmodified) input sentences. We create a pseudo document-level paraphrase dataset\nfor training CoRPG. Automatic evaluation results show CoRPG outperforms several\nstrong baseline models on the BERTScore and diversity scores. Human evaluation\nalso shows our model can generate document paraphrase with more diversity and\nsemantic preservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yitao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Edge Probing Tasks Reveal Linguistic Knowledge in QA Models?. (arXiv:2109.07102v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07102","description":"<p>There have been many efforts to try to understand what grammatical knowledge\n(e.g., ability to understand the part of speech of a token) is encoded in large\npre-trained language models (LM). This is done through `Edge Probing' (EP)\ntests: simple ML models that predict the grammatical properties of a span\n(whether it has a particular part of speech) using \\textit{only} the LM's token\nrepresentations. However, most NLP applications use \\finetuned\\ LMs. Here, we\nask: if a LM is \\finetuned, does the encoding of linguistic information in it\nchange, as measured by EP tests? Conducting experiments on multiple\nquestion-answering (QA) datasets, we answer that question negatively: the EP\ntest results do not change significantly when the fine-tuned QA model performs\nwell or in adversarial situations where the model is forced to learn wrong\ncorrelations. However, a critical analysis of the EP task datasets reveals that\nEP models may rely on spurious correlations to make predictions. This indicates\neven if \\finetuning\\ changes the encoding of such knowledge, the EP tests might\nfail to measure it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1\">Sagnik Ray Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1\">Nikita Bhutani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Named Entity Recognition Based on Multi-hop Dependency Trigger. (arXiv:2109.07118v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07118","description":"<p>This paper presents a simple and effective approach in low-resource named\nentity recognition (NER) based on multi-hop dependency trigger. Dependency\ntrigger refer to salient nodes relative to a entity in the dependency graph of\na context sentence. Our main observation is that there often exists trigger\nwhich play an important role to recognize the location and type of entity in\nsentence. Previous research has used manual labelling of trigger. Our main\ncontribution is to propose use a syntactic parser to automatically annotate\ntrigger. Experiments on two English datasets (CONLL 2003 and BC5CDR) show that\nthe proposed method is comparable to the previous trigger-based NER model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiangxu Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Does The User Want? Information Gain for Hierarchical Dialogue Policy Optimisation. (arXiv:2109.07129v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07129","description":"<p>The dialogue management component of a task-oriented dialogue system is\ntypically optimised via reinforcement learning (RL). Optimisation via RL is\nhighly susceptible to sample inefficiency and instability. The hierarchical\napproach called Feudal Dialogue Management takes a step towards more efficient\nlearning by decomposing the action space. However, it still suffers from\ninstability due to the reward only being provided at the end of the dialogue.\nWe propose the usage of an intrinsic reward based on information gain to\naddress this issue. Our proposed reward favours actions that resolve\nuncertainty or query the user whenever necessary. It enables the policy to\nlearn how to retrieve the users' needs efficiently, which is an integral aspect\nin every task-oriented conversation. Our algorithm, which we call FeudalGain,\nachieves state-of-the-art results in most environments of the PyDial framework,\noutperforming much more complex approaches. We confirm the sample efficiency\nand stability of our algorithm through experiments in simulation and a human\ntrial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Universality of Deep COntextual Language Models. (arXiv:2109.07140v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07140","description":"<p>Deep Contextual Language Models (LMs) like ELMO, BERT, and their successors\ndominate the landscape of Natural Language Processing due to their ability to\nscale across multiple tasks rapidly by pre-training a single model, followed by\ntask-specific fine-tuning. Furthermore, multilingual versions of such models\nlike XLM-R and mBERT have given promising results in zero-shot cross-lingual\ntransfer, potentially enabling NLP applications in many under-served and\nunder-resourced languages. Due to this initial success, pre-trained models are\nbeing used as `Universal Language Models' as the starting point across diverse\ntasks, domains, and languages. This work explores the notion of `Universality'\nby identifying seven dimensions across which a universal model should be able\nto scale, that is, perform equally well or reasonably well, to be useful across\ndiverse settings. We outline the current theoretical and empirical results that\nsupport model performance across these dimensions, along with extensions that\nmay help address some of their current limitations. Through this survey, we lay\nthe foundation for understanding the capabilities and limitations of massive\ncontextual language models and help discern research gaps and directions for\nfuture work to make these LMs inclusive and fair to diverse applications,\nusers, and linguistic phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1\">Shaily Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Poonam Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Glass-Box Features: Uncertainty Quantification Enhanced Quality Estimation for Neural Machine Translation. (arXiv:2109.07141v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07141","description":"<p>Quality Estimation (QE) plays an essential role in applications of Machine\nTranslation (MT). Traditionally, a QE system accepts the original source text\nand translation from a black-box MT system as input. Recently, a few studies\nindicate that as a by-product of translation, QE benefits from the model and\ntraining data's information of the MT system where the translations come from,\nand it is called the \"glass-box QE\". In this paper, we extend the definition of\n\"glass-box QE\" generally to uncertainty quantification with both \"black-box\"\nand \"glass-box\" approaches and design several features deduced from them to\nblaze a new trial in improving QE's performance. We propose a framework to fuse\nthe feature engineering of uncertainty quantification into a pre-trained\ncross-lingual language model to predict the translation quality. Experiment\nresults show that our method achieves state-of-the-art performances on the\ndatasets of WMT 2020 QE shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangbin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaolin Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantics of European poetry is shaped by conservative forces: The relationship between poetic meter and meaning in accentual-syllabic verse. (arXiv:2109.07148v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07148","description":"<p>Recent advances in cultural analytics and large-scale computational studies\nof art, literature and film often show that long-term change in the features of\nartistic works happens gradually. These findings suggest that conservative\nforces that shape creative domains might be underestimated. To this end, we\nprovide the first large-scale formal evidence of the persistent association\nbetween poetic meter and semantics in 18-19th European literatures, using\nCzech, German and Russian collections with additional data from English poetry\nand early modern Dutch songs. Our study traces this association through a\nseries of clustering experiments using the abstracted semantic features of\n150,000 poems. With the aid of topic modeling we infer semantic features for\nindividual poems. Texts were also lexically simplified across collections to\nincrease generalizability and decrease the sparseness of word frequency\ndistributions. Topics alone enable recognition of the meters in each observed\nlanguage, as may be seen from highly robust clustering of same-meter samples\n(median Adjusted Rand Index between 0.48 and 1). In addition, this study shows\nthat the strength of the association between form and meaning tends to decrease\nover time. This may reflect a shift in aesthetic conventions between the 18th\nand 19th centuries as individual innovation was increasingly favored in\nliterature. Despite this decline, it remains possible to recognize semantics of\nthe meters from past or future, which suggests the continuity of semantic\ntraditions while also revealing the historical variability of conditions across\nlanguages. This paper argues that distinct metrical forms, which are often\ncopied in a language over centuries, also maintain long-term semantic inertia\nin poetry. Our findings, thus, highlight the role of the formal features of\ncultural items in influencing the pace and shape of cultural evolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1\">Artjoms &#x160;e&#x13c;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plechac_P/0/1/0/all/0/1\">Petr Plech&#xe1;&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassche_A/0/1/0/all/0/1\">Alie Lassche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Residual and Normalization Layers into Analysis of Masked Language Models. (arXiv:2109.07152v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07152","description":"<p>Transformer architecture has become ubiquitous in the natural language\nprocessing field. To interpret the Transformer-based models, their attention\npatterns have been extensively analyzed. However, the Transformer architecture\nis not only composed of the multi-head attention; other components can also\ncontribute to Transformers' progressive performance. In this study, we extended\nthe scope of the analysis of Transformers from solely the attention patterns to\nthe whole attention block, i.e., multi-head attention, residual connection, and\nlayer normalization. Our analysis of Transformer-based masked language models\nshows that the token-to-token interaction performed via attention has less\nimpact on the intermediate representations than previously assumed. These\nresults provide new intuitive explanations of existing reports; for example,\ndiscarding the learned attention patterns tends not to adversely affect the\nperformance. The codes of our experiments are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_G/0/1/0/all/0/1\">Goro Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuribayashi_T/0/1/0/all/0/1\">Tatsuki Kuribayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoi_S/0/1/0/all/0/1\">Sho Yokoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Models be Biomedical Knowledge Bases?. (arXiv:2109.07154v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07154","description":"<p>Pre-trained language models (LMs) have become ubiquitous in solving various\nnatural language processing (NLP) tasks. There has been increasing interest in\nwhat knowledge these LMs contain and how we can extract that knowledge,\ntreating LMs as knowledge bases (KBs). While there has been much work on\nprobing LMs in the general domain, there has been little attention to whether\nthese powerful LMs can be used as domain-specific KBs. To this end, we create\nthe BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge\ntriples for probing biomedical LMs. We find that biomedical LMs with recently\nproposed probing methods can achieve up to 18.51% Acc@5 on retrieving\nbiomedical knowledge. Although this seems promising given the task difficulty,\nour detailed analyses reveal that most predictions are highly correlated with\nprompt templates without any subjects, hence producing similar results on each\nrelation and hindering their capabilities to be used as domain-specific KBs. We\nhope that BioLAMA can serve as a challenging benchmark for biomedical factual\nprobing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Mujeen Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Sean Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Minji Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Match Job Candidates Using Multilingual Bi-Encoder BERT. (arXiv:2109.07157v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07157","description":"<p>In this talk, we will show how we used Randstad history of candidate\nplacements to generate labeled CV-vacancy pairs dataset. Afterwards we\nfine-tune a multilingual BERT with bi encoder structure over this dataset, by\nadding a cosine similarity log loss layer. We will explain how using the\nmentioned structure helps us overcome most of the challenges described above,\nand how it enables us to build a maintainable and scalable pipeline to match\nCVs and vacancies. In addition, we show how we gain a better semantic\nunderstanding, and learn to bridge the vocabulary gap. Finally, we highlight\nhow multilingual transformers help us handle cross language barrier and might\nreduce discrimination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lavi_D/0/1/0/all/0/1\">Dor Lavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Generative Factors in Natural Language with Discrete Variational Autoencoders. (arXiv:2109.07169v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07169","description":"<p>The ability of learning disentangled representations represents a major step\nfor interpretable NLP systems as it allows latent linguistic features to be\ncontrolled. Most approaches to disentanglement rely on continuous variables,\nboth for images and text. We argue that despite being suitable for image\ndatasets, continuous variables may not be ideal to model features of textual\ndata, due to the fact that most generative factors in text are discrete. We\npropose a Variational Autoencoder based method which models language features\nas discrete variables and encourages independence between variables for\nlearning disentangled representations. The proposed model outperforms\ncontinuous and discrete baselines on several qualitative and quantitative\nbenchmarks for disentanglement as well as on a text style transfer downstream\napplication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mercatali_G/0/1/0/all/0/1\">Giangiacomo Mercatali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup. (arXiv:2109.07177v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07177","description":"<p>Mixup is a recent regularizer for current deep classification networks.\nThrough training a neural network on convex combinations of pairs of examples\nand their labels, it imposes locally linear constraints on the model's input\nspace. However, such strict linear constraints often lead to under-fitting\nwhich degrades the effects of regularization. Noticeably, this issue is getting\nmore serious when the resource is extremely limited. To address these issues,\nwe propose the Adversarial Mixing Policy (AMP), organized in a min-max-rand\nformulation, to relax the Locally Linear Constraints in Mixup. Specifically,\nAMP adds a small adversarial perturbation to the mixing coefficients rather\nthan the examples. Thus, slight non-linearity is injected in-between the\nsynthetic examples and synthetic labels. By training on these data, the deep\nnetworks are further regularized, and thus achieve a lower predictive error\nrate. Experiments on five text classification benchmarks and five backbone\nmodels have empirically shown that our methods reduce the error rate over Mixup\nvariants in a significant margin (up to 31.3%), especially in low-resource\nconditions (up to 17.5%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuzhao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hailong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Weiguo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Language Models for Factoid Question Answering at BioASQ9b. (arXiv:2109.07185v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07185","description":"<p>In this work, we describe our experiments and participating systems in the\nBioASQ Task 9b Phase B challenge of biomedical question answering. We have\nfocused on finding the ideal answers and investigated multi-task fine-tuning\nand gradual unfreezing techniques on transformer-based language models. For\nfactoid questions, our ALBERT-based systems ranked first in test batch 1 and\nfourth in test batch 2. Our DistilBERT systems outperformed the ALBERT variants\nin test batches 4 and 5 despite having 81% fewer parameters than ALBERT.\nHowever, we observed that gradual unfreezing had no significant impact on the\nmodel's accuracy compared to standard fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanna_U/0/1/0/all/0/1\">Urvashi Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molla_D/0/1/0/all/0/1\">Diego Moll&#xe1;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiagent Multimodal Categorization for Symbol Emergence: Emergent Communication via Interpersonal Cross-modal Inference. (arXiv:2109.07194v1 [cs.AI])","link":"http://arxiv.org/abs/2109.07194","description":"<p>This paper describes a computational model of multiagent multimodal\ncategorization that realizes emergent communication. We clarify whether the\ncomputational model can reproduce the following functions in a symbol emergence\nsystem, comprising two agents with different sensory modalities playing a\nnaming game. (1) Function for forming a shared lexical system that comprises\nperceptual categories and corresponding signs, formed by agents through\nindividual learning and semiotic communication between agents. (2) Function to\nimprove the categorization accuracy in an agent via semiotic communication with\nanother agent, even when some sensory modalities of each agent are missing. (3)\nFunction that an agent infers unobserved sensory information based on a sign\nsampled from another agent in the same manner as cross-modal inference. We\npropose an interpersonal multimodal Dirichlet mixture (Inter-MDM), which is\nderived by dividing an integrative probabilistic generative model, which is\nobtained by integrating two Dirichlet mixtures (DMs). The Markov chain Monte\nCarlo algorithm realizes emergent communication. The experimental results\ndemonstrated that Inter-MDM enables agents to form multimodal categories and\nappropriately share signs between agents. It is shown that emergent\ncommunication improves categorization accuracy, even when some sensory\nmodalities are missing. Inter-MDM enables an agent to predict unobserved\ninformation based on a shared sign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagiwara_Y/0/1/0/all/0/1\">Yoshinobu Hagiwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furukawa_K/0/1/0/all/0/1\">Kazuma Furukawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_A/0/1/0/all/0/1\">Akira Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1\">Tadahiro Taniguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis in Poems in Misurata Sub-dialect -- A Sentiment Detection in an Arabic Sub-dialect. (arXiv:2109.07203v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07203","description":"<p>Over the recent decades, there has been a significant increase and\ndevelopment of resources for Arabic natural language processing. This includes\nthe task of exploring Arabic Language Sentiment Analysis (ALSA) from Arabic\nutterances in both Modern Standard Arabic (MSA) and different Arabic dialects.\nThis study focuses on detecting sentiment in poems written in Misurata Arabic\nsub-dialect spoken in Misurata, Libya. The tools used to detect sentiment from\nthe dataset are Sklearn as well as Mazajak sentiment tool 1. Logistic\nRegression, Random Forest, Naive Bayes (NB), and Support Vector Machines (SVM)\nclassifiers are used with Sklearn, while the Convolutional Neural Network (CNN)\nis implemented with Mazajak. The results show that the traditional classifiers\nscore a higher level of accuracy as compared to Mazajak which is built on an\nalgorithm that includes deep learning techniques. More research is suggested to\nanalyze Arabic sub-dialect poetry in order to investigate the aspects that\ncontribute to sentiments in these multi-line texts; for example, the use of\nfigurative language such as metaphors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abugharsa_A/0/1/0/all/0/1\">Azza Abugharsa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Relation-Oriented Clustering Method for Open Relation Extraction. (arXiv:2109.07205v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07205","description":"<p>The clustering-based unsupervised relation discovery method has gradually\nbecome one of the important methods of open relation extraction (OpenRE).\nHowever, high-dimensional vectors can encode complex linguistic information\nwhich leads to the problem that the derived clusters cannot explicitly align\nwith the relational semantic classes. In this work, we propose a\nrelation-oriented clustering model and use it to identify the novel relations\nin the unlabeled data. Specifically, to enable the model to learn to cluster\nrelational data, our method leverages the readily available labeled data of\npre-defined relations to learn a relation-oriented representation. We minimize\ndistance between the instance with same relation by gathering the instances\ntowards their corresponding relation centroids to form a cluster structure, so\nthat the learned representation is cluster-friendly. To reduce the clustering\nbias on predefined classes, we optimize the model by minimizing a joint\nobjective on both labeled and unlabeled data. Experimental results show that\nour method reduces the error rate by 29.2% and 15.7%, on two datasets\nrespectively, compared with current SOTA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yaqian Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"{E}fficient{BERT}: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation. (arXiv:2109.07222v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07222","description":"<p>Pre-trained language models have shown remarkable results on various NLP\ntasks. Nevertheless, due to their bulky size and slow inference speed, it is\nhard to deploy them on edge devices. In this paper, we have a critical insight\nthat improving the feed-forward network (FFN) in BERT has a higher gain than\nimproving the multi-head attention (MHA) since the computational cost of FFN is\n2$\\sim$3 times larger than MHA. Hence, to compact BERT, we are devoted to\ndesigning efficient FFN as opposed to previous works that pay attention to MHA.\nSince FFN comprises a multilayer perceptron (MLP) that is essential in BERT\noptimization, we further design a thorough search space towards an advanced MLP\nand perform a coarse-to-fine mechanism to search for an efficient BERT\narchitecture. Moreover, to accelerate searching and enhance model\ntransferability, we employ a novel warm-up knowledge distillation strategy at\neach search stage. Extensive experiments show our searched EfficientBERT is\n6.9$\\times$ smaller and 4.4$\\times$ faster than BERT$\\rm_{BASE}$, and has\ncompetitive performances on GLUE and SQuAD Benchmarks. Concretely,\nEfficientBERT attains a 77.7 average score on GLUE \\emph{test}, 0.7 higher than\nMobileBERT$\\rm_{TINY}$, and achieves an 85.3/74.5 F1 score on SQuAD v1.1/v2.0\n\\emph{dev}, 3.2/2.7 higher than TinyBERT$_4$ even without data augmentation.\nThe code is released at https://github.com/cheneydon/efficient-bert.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chenhe Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiefeng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Much do Lyrics Matter? Analysing Lyrical Simplicity Preferences for Individuals At Risk of Depression. (arXiv:2109.07227v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07227","description":"<p>Music affects and in some cases reflects one's emotional state. Key to this\ninfluence is lyrics and their meaning in conjunction with the acoustic\nproperties of the track. Recent work has focused on analysing these acoustic\nproperties and showing that individuals prone to depression primarily consume\nlow valence and low energy music. However, no studies yet have explored lyrical\ncontent preferences in relation to online music consumption of such\nindividuals. In the current study, we examine lyrical simplicity, measured as\nthe Compressibility and Absolute Information Content of the text, associated\nwith preferences of individuals at risk for depression. Using the six-month\nlistening history of 541 Last.fm users, we compare lyrical simplicity trends\nfor users grouped as being at risk (At-Risk) of depression from those that are\nnot (No-Risk). Our findings reveal that At-Risk individuals prefer songs with\ngreater information content (lower Compressibility) on average, especially for\nsongs characterised as Sad. Furthermore, we found that At-Risk individuals also\nhave greater variability of Absolute Information Content across their listening\nhistory. We discuss the results in light of existing socio-psychological\nlab-based research on music habits associated with depression and their\nrelevance to naturally occurring online music listening behaviour.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shriram_J/0/1/0/all/0/1\">Jaidev Shriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paruchuri_S/0/1/0/all/0/1\">Sreeharsha Paruchuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alluri_V/0/1/0/all/0/1\">Vinoo Alluri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialog speech sentiment classification for imbalanced datasets. (arXiv:2109.07228v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07228","description":"<p>Speech is the most common way humans express their feelings, and sentiment\nanalysis is the use of tools such as natural language processing and\ncomputational algorithms to identify the polarity of these feelings. Even\nthough this field has seen tremendous advancements in the last two decades, the\ntask of effectively detecting under represented sentiments in different kinds\nof datasets is still a challenging task. In this paper, we use single and\nbi-modal analysis of short dialog utterances and gain insights on the main\nfactors that aid in sentiment detection, particularly in the underrepresented\nclasses, in datasets with and without inherent sentiment component.\nFurthermore, we propose an architecture which uses a learning rate scheduler\nand different monitoring criteria and provides state-of-the-art results for the\nSWITCHBOARD imbalanced sentiment dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nicolaou_S/0/1/0/all/0/1\">Sergis Nicolaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavrides_L/0/1/0/all/0/1\">Lambros Mavrides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tryfou_G/0/1/0/all/0/1\">Georgina Tryfou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_K/0/1/0/all/0/1\">Kyriakos Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panousis_K/0/1/0/all/0/1\">Konstantinos Panousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzis_S/0/1/0/all/0/1\">Sotirios Chatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodoridis_S/0/1/0/all/0/1\">Sergios Theodoridis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Mathematical Properties of Integers. (arXiv:2109.07230v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07230","description":"<p>Embedding words in high-dimensional vector spaces has proven valuable in many\nnatural language applications. In this work, we investigate whether\nsimilarly-trained embeddings of integers can capture concepts that are useful\nfor mathematical applications. We probe the integer embeddings for mathematical\nknowledge, apply them to a set of numerical reasoning tasks, and show that by\nlearning the representations from mathematical sequence data, we can\nsubstantially improve over number embeddings learned from English text corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ryskina_M/0/1/0/all/0/1\">Maria Ryskina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knight_K/0/1/0/all/0/1\">Kevin Knight</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SWEAT: Scoring Polarization of Topics across Different Corpora. (arXiv:2109.07231v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07231","description":"<p>Understanding differences of viewpoints across corpora is a fundamental task\nfor computational social sciences. In this paper, we propose the Sliced Word\nEmbedding Association Test (SWEAT), a novel statistical measure to compute the\nrelative polarization of a topical wordset across two distributional\nrepresentations. To this end, SWEAT uses two additional wordsets, deemed to\nhave opposite valence, to represent two different poles. We validate our\napproach and illustrate a case study to show the usefulness of the introduced\nmeasure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marelli_M/0/1/0/all/0/1\">Marco Marelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicoli_P/0/1/0/all/0/1\">Paolo Nicoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmonari_M/0/1/0/all/0/1\">Matteo Palmonari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Unreasonable Effectiveness of the Baseline: Discussing SVMs in Legal Text Classification. (arXiv:2109.07234v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07234","description":"<p>We aim to highlight an interesting trend to contribute to the ongoing debate\naround advances within legal Natural Language Processing. Recently, the focus\nfor most legal text classification tasks has shifted towards large pre-trained\ndeep learning models such as BERT. In this paper, we show that a more\ntraditional approach based on Support Vector Machine classifiers reaches\ncompetitive performance with deep learning models. We also highlight that error\nreduction obtained by using specialised BERT-based models over baselines is\nnoticeably smaller in the legal domain when compared to general language tasks.\nWe discuss some hypotheses for these results to support future discussions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clavie_B/0/1/0/all/0/1\">Benjamin Clavi&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alphonsus_M/0/1/0/all/0/1\">Marc Alphonsus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regressive Ensemble for Machine Translation Quality Evaluation. (arXiv:2109.07242v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07242","description":"<p>This work introduces a simple regressive ensemble for evaluating machine\ntranslation quality based on a set of novel and established metrics. We\nevaluate the ensemble using a correlation to expert-based MQM scores of the WMT\n2021 Metrics workshop. In both monolingual and zero-shot cross-lingual\nsettings, we show a significant performance improvement over single metrics. In\nthe cross-lingual settings, we also demonstrate that an ensemble approach is\nwell-applicable to unseen languages. Furthermore, we identify a strong\nreference-free baseline that consistently outperforms the commonly-used BLEU\nand METEOR measures and significantly improves our ensemble's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Clinical Information Extraction with Transferred Contextual Embeddings. (arXiv:2109.07243v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07243","description":"<p>The Bidirectional Encoder Representations from Transformers (BERT) model has\nachieved the state-of-the-art performance for many natural language processing\n(NLP) tasks. Yet, limited research has been contributed to studying its\neffectiveness when the target domain is shifted from the pre-training corpora,\nfor example, for biomedical or clinical NLP applications. In this paper, we\napplied it to a widely studied a hospital information extraction (IE) task and\nanalyzed its performance under the transfer learning setting. Our application\nbecame the new state-of-the-art result by a clear margin, compared with a range\nof existing IE models. Specifically, on this nursing handover data set, the\nmacro-average F1 score from our model was 0.438, whilst the previous best deep\nlearning models had 0.416. In conclusion, we showed that BERT based\npre-training models can be transferred to health-related documents under mild\nconditions and with a proper fine-tuning process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zimin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenchen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suominen_H/0/1/0/all/0/1\">Hanna Suominen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs. (arXiv:2109.07263v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07263","description":"<p>We propose a novel problem within end-to-end learning of task-oriented\ndialogs (TOD), in which the dialog system mimics a troubleshooting agent who\nhelps a user by diagnosing their problem (e.g., car not starting). Such dialogs\nare grounded in domain-specific flowcharts, which the agent is supposed to\nfollow during the conversation. Our task exposes novel technical challenges for\nneural TOD, such as grounding an utterance to the flowchart without explicit\nannotation, referring to additional manual pages when user asks a clarification\nquestion, and ability to follow unseen flowcharts at test time. We release a\ndataset (FloDial) consisting of 2,738 dialogs grounded on 12 different\ntroubleshooting flowcharts. We also design a neural model, FloNet, which uses a\nretrieval-augmented generation architecture to train the dialog agent. Our\nexperiments find that FloNet can do zero-shot transfer to unseen flowcharts,\nand sets a strong baseline for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raghu_D/0/1/0/all/0/1\">Dinesh Raghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shantanu Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scope resolution of predicted negation cues: A two-step neural network-based approach. (arXiv:2109.07264v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07264","description":"<p>Neural network-based methods are the state of the art in negation scope\nresolution. However, they often use the unrealistic assumption that cue\ninformation is completely accurate. Even if this assumption holds, there\nremains a dependency on engineered features from state-of-the-art machine\nlearning methods. The current study adopted a two-step negation resolving\napporach to assess whether a Bidirectional Long Short-Term Memory-based method\ncan be used for cue detection as well, and how inaccurate cue predictions would\naffect the scope resolution performance. Results suggest that this method is\nnot suitable for negation detection. Scope resolution performance is most\nrobust against inaccurate information for models with a recurrent layer only,\ncompared to extensions with a Conditional Random Fields layer or a\npost-processing algorithm. We advocate for more research into the application\nof deep learning on negation detection and the effect of imperfect information\non scope resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jong_D/0/1/0/all/0/1\">Daan de Jong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Length is a Domain: Length-based Overfitting in Transformer Models. (arXiv:2109.07276v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07276","description":"<p>Transformer-based sequence-to-sequence architectures, while achieving\nstate-of-the-art results on a large number of NLP tasks, can still suffer from\noverfitting during training. In practice, this is usually countered either by\napplying regularization methods (e.g. dropout, L2-regularization) or by\nproviding huge amounts of training data. Additionally, Transformer and other\narchitectures are known to struggle when generating very long sequences. For\nexample, in machine translation, the neural-based systems perform worse on very\nlong sequences when compared to the preceding phrase-based translation\napproaches (Koehn and Knowles, 2017).\n</p>\n<p>We present results which suggest that the issue might also be in the mismatch\nbetween the length distributions of the training and validation data combined\nwith the aforementioned tendency of the neural networks to overfit to the\ntraining data. We demonstrate on a simple string editing task and a machine\ntranslation task that the Transformer model performance drops significantly\nwhen facing sequences of length diverging from the length distribution in the\ntraining data. Additionally, we show that the observed drop in performance is\ndue to the hypothesis length corresponding to the lengths seen by the model\nduring training rather than the length of the input sequence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varis_D/0/1/0/all/0/1\">Du&#x161;an Vari&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context. (arXiv:2109.07293v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07293","description":"<p>Embedding based methods are widely used for unsupervised keyphrase extraction\n(UKE) tasks. Generally, these methods simply calculate similarities between\nphrase embeddings and document embedding, which is insufficient to capture\ndifferent context for a more effective UKE model. In this paper, we propose a\nnovel method for UKE, where local and global contexts are jointly modeled. From\na global view, we calculate the similarity between a certain phrase and the\nwhole document in the vector space as transitional embedding based models do.\nIn terms of the local view, we first build a graph structure based on the\ndocument where phrases are regarded as vertices and the edges are similarities\nbetween vertices. Then, we proposed a new centrality computation method to\ncapture local salient information based on the graph structure. Finally, we\nfurther combine the modeling of global and local context for ranking. We\nevaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010)\nand compare with existing state-of-the-art models. The results show that our\nmodel outperforms most models while generalizing better on input documents with\ndifferent domains and length. Additional ablation study shows that both the\nlocal and global information is crucial for unsupervised keyphrase extraction\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinnian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Vision-Language Models `See' when they See Scenes. (arXiv:2109.07301v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07301","description":"<p>Images can be described in terms of the objects they contain, or in terms of\nthe types of scene or place that they instantiate. In this paper we address to\nwhat extent pretrained Vision and Language models can learn to align\ndescriptions of both types with images. We compare 3 state-of-the-art models,\nVisualBERT, LXMERT and CLIP. We find that (i) V&amp;L models are susceptible to\nstylistic biases acquired during pretraining; (ii) only CLIP performs\nconsistently well on both object- and scene-level descriptions. A follow-up\nablation study shows that CLIP uses object-level information in the visual\nmodality to align with scene-level textual descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cafagna_M/0/1/0/all/0/1\">Michele Cafagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Allocating Large Vocabulary Capacity for Cross-lingual Language Model Pre-training. (arXiv:2109.07306v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07306","description":"<p>Compared to monolingual models, cross-lingual models usually require a more\nexpressive vocabulary to represent all languages adequately. We find that many\nlanguages are under-represented in recent cross-lingual language models due to\nthe limited vocabulary capacity. To this end, we propose an algorithm VoCap to\ndetermine the desired vocabulary capacity of each language. However, increasing\nthe vocabulary size significantly slows down the pre-training speed. In order\nto address the issues, we propose k-NN-based target sampling to accelerate the\nexpensive softmax. Our experiments show that the multilingual vocabulary\nlearned with VoCap benefits cross-lingual language model pre-training.\nMoreover, k-NN-based target sampling mitigates the side-effects of increasing\nthe vocabulary size while achieving comparable performance and faster\npre-training speed. The code and the pretrained multilingual vocabularies are\navailable at https://github.com/bozheng-hit/VoCapXLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding Convolutions for Short Text Extreme Classification with Millions of Labels. (arXiv:2109.07319v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07319","description":"<p>Automatic annotation of short-text data to a large number of target labels,\nreferred to as Short Text Extreme Classification, has recently found numerous\napplications in prediction of related searches and product recommendation\ntasks. The conventional usage of Convolutional Neural Network (CNN) to capture\nn-grams in text-classification relies heavily on uniformity in word-ordering\nand the presence of long input sequences to convolve over. However, this is\nmissing in short and unstructured text sequences encountered in search and\nrecommendation. In order to tackle this, we propose an orthogonal approach by\nrecasting the convolution operation to capture coupled semantics along the\nembedding dimensions, and develop a word-order agnostic embedding enhancement\nmodule to deal with the lack of structure in such queries. Benefitting from the\ncomputational efficiency of the convolution operation, Embedding Convolutions,\nwhen applied on the enriched word embeddings, result in a light-weight and yet\npowerful encoder (InceptionXML) that is robust to the inherent lack of\nstructure in short-text extreme classification.\n</p>\n<p>Towards scaling our model to problems with millions of labels, we also\npropose InceptionXML+, which addresses the shortcomings of the dynamic\nhard-negative mining framework in the recently proposed LightXML by improving\nthe alignment between the label-shortlister and extreme classifier. On popular\nbenchmark datasets, we empirically demonstrate that the proposed method\noutperforms state-of-the-art deep extreme classifiers such as Astec by an\naverage of 5% and 8% on the P@k and propensity-scored PSP@k metrics\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kharbanda_S/0/1/0/all/0/1\">Siddhant Kharbanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Atmadeep Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palrecha_A/0/1/0/all/0/1\">Akash Palrecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babbar_R/0/1/0/all/0/1\">Rohit Babbar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mi{\\dh}eind's WMT 2021 submission. (arXiv:2109.07343v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07343","description":"<p>We present Mi{\\dh}eind's submission for the English$\\to$Icelandic and\nIcelandic$\\to$English subsets of the 2021 WMT news translation task.\nTransformer-base models are trained for translation on parallel data to\ngenerate backtranslations iteratively. A pretrained mBART-25 model is then\nadapted for translation using parallel data as well as the last backtranslation\niteration. This adapted pretrained model is then used to re-generate\nbacktranslations, and the training of the adapted model is continued.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simonarson_H/0/1/0/all/0/1\">Haukur Barri S&#xed;monarson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snaebjarnarson_V/0/1/0/all/0/1\">V&#xe9;steinn Sn&#xe6;bjarnarson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragnarsson_P/0/1/0/all/0/1\">P&#xe9;tur Orri Ragnarsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_H/0/1/0/all/0/1\">Haukur P&#xe1;ll J&#xf3;nsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+%7B%5CTH%7Dorsteinsson_V/0/1/0/all/0/1\">Vilhj&#xe1;lmur &#xde;orsteinsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introducing an Abusive Language Classification Framework for Telegram to Investigate the German Hater Community. (arXiv:2109.07346v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07346","description":"<p>Since traditional social media platforms ban more and more actors that\ndistribute hate speech or other forms of abusive language (deplatforming),\nthese actors migrate to alternative platforms that do not moderate the users'\ncontent. One known platform that is relevant for the German hater community is\nTelegram, for which there have only been made limited research efforts so far.\n</p>\n<p>The goal of this study is to develop a broad framework that consists of (i)\nan abusive language classification model for German Telegram messages and (ii)\na classification model for the hatefulness of Telegram channels. For the first\npart, we employ existing abusive language datasets containing posts from other\nplatforms to build our classification models. For the channel classification\nmodel, we develop a method that combines channel specific content information\ncoming from a topic model with a social graph to predict the hatefulness of\nchannels. Furthermore, we complement these two approaches for hate speech\ndetection with insightful results on the evolution of the hater community on\nTelegram in Germany. Moreover, we propose methods to the hate speech research\ncommunity for scalable network analyses for social media platforms. As an\nadditional output of the study, we release an annotated abusive language\ndataset containing 1,149 annotated Telegram messages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wich_M/0/1/0/all/0/1\">Maximilian Wich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorniak_A/0/1/0/all/0/1\">Adrian Gorniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eder_T/0/1/0/all/0/1\">Tobias Eder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartmann_D/0/1/0/all/0/1\">Daniel Bartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cakici_B/0/1/0/all/0/1\">Burak Enes &#xc7;akici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Transfer of Monolingual Models. (arXiv:2109.07348v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07348","description":"<p>Recent studies in zero-shot cross-lingual learning using multilingual models\nhave falsified the previous hypothesis that shared vocabulary and joint\npre-training are the keys to cross-lingual generalization. Inspired by this\nadvancement, we introduce a cross-lingual transfer method for monolingual\nmodels based on domain adaptation. We study the effects of such transfer from\nfour different languages to English. Our experimental results on GLUE show that\nthe transferred models outperform the native English model independently of the\nsource language. After probing the English linguistic knowledge encoded in the\nrepresentations before and after transfer, we find that semantic information is\nretained from the source language, while syntactic information is learned\nduring transfer. Additionally, the results of evaluating the transferred models\nin source language tasks reveal that their performance in the source domain\ndeteriorates after transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gogoulou_E/0/1/0/all/0/1\">Evangelia Gogoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekgren_A/0/1/0/all/0/1\">Ariel Ekgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isbister_T/0/1/0/all/0/1\">Tim Isbister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahlgren_M/0/1/0/all/0/1\">Magnus Sahlgren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The ELITR ECA Corpus. (arXiv:2109.07351v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07351","description":"<p>We present the ELITR ECA corpus, a multilingual corpus derived from\npublications of the European Court of Auditors. We use automatic translation\ntogether with Bleualign to identify parallel sentence pairs in all 506\ntranslation directions. The result is a corpus comprising 264k document pairs\nand 41.9M sentence pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Williams_P/0/1/0/all/0/1\">Philip Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1\">Barry Haddow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU. (arXiv:2109.07364v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07364","description":"<p>Incremental processing allows interactive systems to respond based on partial\ninputs, which is a desirable property e.g. in dialogue agents. The currently\npopular Transformer architecture inherently processes sequences as a whole,\nabstracting away the notion of time. Recent work attempts to apply Transformers\nincrementally via restart-incrementality by repeatedly feeding, to an unchanged\nmodel, increasingly longer input prefixes to produce partial outputs. However,\nthis approach is computationally costly and does not scale efficiently for long\nsequences. In parallel, we witness efforts to make Transformers more efficient,\ne.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we\nexamine the feasibility of LT for incremental NLU in English. Our results show\nthat the recurrent LT model has better incremental performance and faster\ninference speed compared to the standard Transformer and LT with\nrestart-incrementality, at the cost of part of the non-incremental (full\nsequence) quality. We show that the performance drop can be mitigated by\ntraining the model to wait for right context before committing to an output and\nthat training with input prefixes is beneficial for delivering correct partial\noutputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kahardipraja_P/0/1/0/all/0/1\">Patrick Kahardipraja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madureira_B/0/1/0/all/0/1\">Brielen Madureira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlangen_D/0/1/0/all/0/1\">David Schlangen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniST: Unified End-to-end Model for Streaming and Non-streaming Speech Translation. (arXiv:2109.07368v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07368","description":"<p>This paper presents a unified end-to-end frame-work for both streaming and\nnon-streamingspeech translation. While the training recipes for non-streaming\nspeech translation have been mature, the recipes for streaming\nspeechtranslation are yet to be built. In this work, wefocus on developing a\nunified model (UniST) which supports streaming and non-streaming ST from the\nperspective of fundamental components, including training objective, attention\nmechanism and decoding policy. Experiments on the most popular speech-to-text\ntranslation benchmark dataset, MuST-C, show that UniST achieves significant\nimprovement for non-streaming ST, and a better-learned trade-off for BLEU score\nand latency metrics for streaming ST, compared with end-to-end baselines and\nthe cascaded models. We will make our codes and evaluation tools publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qianqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaoming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Transferable Table Question Answering. (arXiv:2109.07377v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07377","description":"<p>Weakly-supervised table question-answering(TableQA) models have achieved\nstate-of-art performance by using pre-trained BERT transformer to jointly\nencoding a question and a table to produce structured query for the question.\nHowever, in practical settings TableQA systems are deployed over table corpora\nhaving topic and word distributions quite distinct from BERT's pretraining\ncorpus. In this work we simulate the practical topic shift scenario by\ndesigning novel challenge benchmarks WikiSQL-TS and WikiTQ-TS, consisting of\ntrain-dev-test splits in five distinct topic groups, based on the popular\nWikiSQL and WikiTableQuestions datasets. We empirically show that, despite\npre-training on large open-domain text, performance of models degrades\nsignificantly when they are evaluated on unseen topics. In response, we propose\nT3QA (Topic Transferable Table Question Answering) a pragmatic adaptation\nframework for TableQA comprising of: (1) topic-specific vocabulary injection\ninto BERT, (2) a novel text-to-text transformer generator (such as T5, GPT2)\nbased natural language question generation pipeline focused on generating topic\nspecific training data, and (3) a logical form reranker. We show that T3QA\nprovides a reasonably good baseline for our topic shift benchmarks. We believe\nour topic split benchmarks will lead to robust TableQA solutions that are\nbetter suited for practical deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1\">Saneem Ahmed Chemmengath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Samarth Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_J/0/1/0/all/0/1\">Jaydeep Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canim_M/0/1/0/all/0/1\">Mustafa Canim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_K/0/1/0/all/0/1\">Karthik Sankaranarayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankNAS: Efficient Neural Architecture Search by Pairwise Ranking. (arXiv:2109.07383v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07383","description":"<p>This paper addresses the efficiency challenge of Neural Architecture Search\n(NAS) by formulating the task as a ranking problem. Previous methods require\nnumerous training examples to estimate the accurate performance of\narchitectures, although the actual goal is to find the distinction between\n\"good\" and \"bad\" candidates. Here we do not resort to performance predictors.\nInstead, we propose a performance ranking method (RankNAS) via pairwise\nranking. It enables efficient architecture search using much fewer training\nexamples. Moreover, we develop an architecture selection method to prune the\nsearch space and concentrate on more promising candidates. Extensive\nexperiments on machine translation and language modeling tasks show that\nRankNAS can design high-performance architectures while being orders of\nmagnitude faster than state-of-the-art NAS systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiangnan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xia Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinqiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changliang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constraint based Knowledge Base Distillation in End-to-End Task Oriented Dialogs. (arXiv:2109.07396v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07396","description":"<p>End-to-End task-oriented dialogue systems generate responses based on dialog\nhistory and an accompanying knowledge base (KB). Inferring those KB entities\nthat are most relevant for an utterance is crucial for response generation.\nExisting state of the art scales to large KBs by softly filtering over\nirrelevant KB information. In this paper, we propose a novel filtering\ntechnique that consists of (1) a pairwise similarity based filter that\nidentifies relevant information by respecting the n-ary structure in a KB\nrecord. and, (2) an auxiliary loss that helps in separating contextually\nunrelated KB information. We also propose a new metric -- multiset entity F1\nwhich fixes a correctness issue in the existing entity F1 metric. Experimental\nresults on three publicly available task-oriented dialog datasets show that our\nproposed approach outperforms existing state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raghu_D/0/1/0/all/0/1\">Dinesh Raghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Atishya Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching with Transformers in MELT. (arXiv:2109.07401v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07401","description":"<p>One of the strongest signals for automated matching of ontologies and\nknowledge graphs are the textual descriptions of the concepts. The methods that\nare typically applied (such as character- or token-based comparisons) are\nrelatively simple, and therefore do not capture the actual meaning of the\ntexts. With the rise of transformer-based language models, text comparison\nbased on meaning (rather than lexical features) is possible. In this paper, we\nmodel the ontology matching task as classification problem and present\napproaches based on transformer models. We further provide an easy to use\nimplementation in the MELT framework which is suited for ontology and knowledge\ngraph matching. We show that a transformer-based filter helps to choose the\ncorrect correspondences given a high-recall alignment and already achieves a\ngood result with simple alignment post-processing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hertling_S/0/1/0/all/0/1\">Sven Hertling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portisch_J/0/1/0/all/0/1\">Jan Portisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT is Robust! A Case Against Synonym-Based Adversarial Examples in Text Classification. (arXiv:2109.07403v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07403","description":"<p>Deep Neural Networks have taken Natural Language Processing by storm. While\nthis led to incredible improvements across many tasks, it also initiated a new\nresearch field, questioning the robustness of these neural networks by\nattacking them. In this paper, we investigate four word substitution-based\nattacks on BERT. We combine a human evaluation of individual word substitutions\nand a probabilistic analysis to show that between 96% and 99% of the analyzed\nattacks do not preserve semantics, indicating that their success is mainly\nbased on feeding poor data to the model. To further confirm that, we introduce\nan efficient data augmentation procedure and show that many adversarial\nexamples can be prevented by including data similar to the attacks during\ntraining. An additional post-processing step reduces the success rates of\nstate-of-the-art attacks below 5%. Finally, by looking at more reasonable\nthresholds on constraints for word substitutions, we conclude that BERT is a\nlot more robust than research on attacks suggests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hauser_J/0/1/0/all/0/1\">Jens Hauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascual_D/0/1/0/all/0/1\">Dami&#xe1;n Pascual</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document. (arXiv:2109.07410v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07410","description":"<p>Given the recent proliferation of false claims online, there has been a lot\nof manual fact-checking effort. As this is very time-consuming, human\nfact-checkers can benefit from tools that can support them and make them more\nefficient. Here, we focus on building a system that could provide such support.\nGiven an input document, it aims to detect all sentences that contain a claim\nthat can be verified by some previously fact-checked claims (from a given\ndatabase). The output is a re-ranked list of the document sentences, so that\nthose that can be verified are ranked as high as possible, together with\ncorresponding evidence. Unlike previous work, which has looked into claim\nretrieval, here we take a document-level perspective. We create a new manually\nannotated dataset for the task, and we propose suitable evaluation measures. We\nfurther experiment with a learning-to-rank approach, achieving sizable\nperformance gains over several strong baselines. Our analysis demonstrates the\nimportance of modeling text similarity and stance, while also taking into\naccount the veracity of the retrieved previously fact-checked claims. We\nbelieve that this research would be of interest to fact-checkers, journalists,\nmedia, and regulatory authorities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations. (arXiv:2109.07424v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07424","description":"<p>While contrastive learning is proven to be an effective training strategy in\ncomputer vision, Natural Language Processing (NLP) is only recently adopting it\nas a self-supervised alternative to Masked Language Modeling (MLM) for\nimproving sequence representations. This paper introduces SupCL-Seq, which\nextends the supervised contrastive learning from computer vision to the\noptimization of sequence representations in NLP. By altering the dropout mask\nprobability in standard Transformer architectures, for every representation\n(anchor), we generate augmented altered views. A supervised contrastive loss is\nthen utilized to maximize the system's capability of pulling together similar\nsamples (e.g., anchors and their altered views) and pushing apart the samples\nbelonging to the other classes. Despite its simplicity, SupCLSeq leads to large\ngains in many sequence classification tasks on the GLUE benchmark compared to a\nstandard BERTbase, including 6% absolute improvement on CoLA, 5.4% on MRPC,\n4.7% on RTE and 2.6% on STSB. We also show consistent gains over self\nsupervised contrastively learned representations, especially in non-semantic\ntasks. Finally we show that these gains are not solely due to augmentation, but\nrather to a downstream optimized sequence representation. Code:\nhttps://github.com/hooman650/SupCL-Seq\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sedghamiz_H/0/1/0/all/0/1\">Hooman Sedghamiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raval_S/0/1/0/all/0/1\">Shivam Raval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santus_E/0/1/0/all/0/1\">Enrico Santus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhanai_T/0/1/0/all/0/1\">Tuka Alhanai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1\">Mohammad Ghassemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative and Generative Transformer-based Models For Situation Entity Classification. (arXiv:2109.07434v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07434","description":"<p>We re-examine the situation entity (SE) classification task with varying\namounts of available training data. We exploit a Transformer-based variational\nautoencoder to encode sentences into a lower dimensional latent space, which is\nused to generate the text and learn a SE classifier. Test set and cross-genre\nevaluations show that when training data is plentiful, the proposed model can\nimprove over the previous discriminative state-of-the-art models. Our approach\nperforms disproportionately better with smaller amounts of training data, but\nwhen faced with extremely small sets (4 instances per label), generative RNN\nmethods outperform transformers. Our work provides guidance for future efforts\non SE and semantic prediction tasks, and low-label training regimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaee_M/0/1/0/all/0/1\">Mehdi Rezaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darvish_K/0/1/0/all/0/1\">Kasra Darvish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kebe_G/0/1/0/all/0/1\">Gaoussou Youssouf Kebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative. (arXiv:2109.07437v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07437","description":"<p>Pre-training, where models are trained on an auxiliary objective with\nabundant data before being fine-tuned on data from the downstream task, is now\nthe dominant paradigm in NLP. In general, the pre-training step relies on\nlittle to no direct knowledge of the task on which the model will be\nfine-tuned, even when the end-task is known in advance. Our work challenges\nthis status-quo of end-task agnostic pre-training. First, on three different\nlow-resource NLP tasks from two domains, we demonstrate that multi-tasking the\nend-task and auxiliary objectives results in significantly better downstream\ntask performance than the widely-used task-agnostic continued pre-training\nparadigm of Gururangan et al. (2020). We next introduce an online meta-learning\nalgorithm that learns a set of multi-task weights to better balance among our\nmultiple auxiliary objectives, achieving further improvements on end task\nperformance and data efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dery_L/0/1/0/all/0/1\">Lucio M. Dery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_P/0/1/0/all/0/1\">Paul Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is \"moby dick\" a Whale or a Bird? Named Entities and Terminology in Speech Translation. (arXiv:2109.07439v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07439","description":"<p>Automatic translation systems are known to struggle with rare words. Among\nthese, named entities (NEs) and domain-specific terms are crucial, since errors\nin their translation can lead to severe meaning distortions. Despite their\nimportance, previous speech translation (ST) studies have neglected them, also\ndue to the dearth of publicly available resources tailored to their specific\nevaluation. To fill this gap, we i) present the first systematic analysis of\nthe behavior of state-of-the-art ST systems in translating NEs and terminology,\nand ii) release NEuRoparl-ST, a novel benchmark built from European Parliament\nspeeches annotated with NEs and terminology. Our experiments on the three\nlanguage directions covered by our benchmark (en-&gt;es/fr/it) show that ST\nsystems correctly translate 75-80% of terms and 65-70% of NEs, with very low\nperformance (37-40%) on person names.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1\">Susana Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentivogli_L/0/1/0/all/0/1\">Luisa Bentivogli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges in Detoxifying Language Models. (arXiv:2109.07445v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07445","description":"<p>Large language models (LM) generate remarkably fluent text and can be\nefficiently adapted across NLP tasks. Measuring and guaranteeing the quality of\ngenerated text in terms of safety is imperative for deploying LMs in the real\nworld; to this end, prior work often relies on automatic evaluation of LM\ntoxicity. We critically discuss this approach, evaluate several toxicity\nmitigation strategies with respect to both automatic and human evaluation, and\nanalyze consequences of toxicity mitigation in terms of model bias and LM\nquality. We demonstrate that while basic intervention strategies can\neffectively optimize previously established automatic metrics on the\nRealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for\nboth texts about, and dialects of, marginalized groups. Additionally, we find\nthat human raters often disagree with high automatic toxicity scores after\nstrong toxicity reduction interventions -- highlighting further the nuances\ninvolved in careful evaluation of LM toxicity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Welbl_J/0/1/0/all/0/1\">Johannes Welbl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaese_A/0/1/0/all/0/1\">Amelia Glaese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uesato_J/0/1/0/all/0/1\">Jonathan Uesato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dathathri_S/0/1/0/all/0/1\">Sumanth Dathathri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellor_J/0/1/0/all/0/1\">John Mellor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1\">Lisa Anne Hendricks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_K/0/1/0/all/0/1\">Kirsty Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohli_P/0/1/0/all/0/1\">Pushmeet Kohli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppin_B/0/1/0/all/0/1\">Ben Coppin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Sen Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Does Translation Require Context? A Data-driven, Multilingual Exploration. (arXiv:2109.07446v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07446","description":"<p>Although proper handling of discourse phenomena significantly contributes to\nthe quality of machine translation (MT), common translation quality metrics do\nnot adequately capture them. Recent works in context-aware MT attempt to target\na small set of these phenomena during evaluation. In this paper, we propose a\nnew metric, P-CXMI, which allows us to identify translations that require\ncontext systematically and confirm the difficulty of previously studied\nphenomena as well as uncover new ones that have not been addressed in previous\nwork. We then develop the Multilingual Discourse-Aware (MuDA) benchmark, a\nseries of taggers for these phenomena in 14 different language pairs, which we\nuse to evaluate context-aware MT. We find that state-of-the-art context-aware\nMT models find marginal improvements over context-agnostic models on our\nbenchmark, which suggests current models do not handle these ambiguities\neffectively. We release code and data to invite the MT research community to\nincrease efforts on context-aware translation on discourse phenomena and\nlanguages that are currently overlooked.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiGUM: Exhaustive Entity Linking for Wikification in 12 Genres. (arXiv:2109.07449v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07449","description":"<p>Previous work on Entity Linking has focused on resources targeting non-nested\nproper named entity mentions, often in data from Wikipedia, i.e. Wikification.\nIn this paper, we present and evaluate WikiGUM, a fully wikified dataset,\ncovering all mentions of named entities, including their non-named and\npronominal mentions, as well as mentions nested within other mentions. The\ndataset covers a broad range of 12 written and spoken genres, most of which\nhave not been included in Entity Linking efforts to date, leading to poor\nperformance by a pretrained SOTA system in our evaluation. The availability of\na variety of other annotations for the same data also enables further research\non entities in context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jessica Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Machines Read Coding Manuals Yet? -- A Benchmark for Building Better Language Models for Code Understanding. (arXiv:2109.07452v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07452","description":"<p>Code understanding is an increasingly important application of Artificial\nIntelligence. A fundamental aspect of understanding code is understanding text\nabout code, e.g., documentation and forum discussions. Pre-trained language\nmodels (e.g., BERT) are a popular approach for various NLP tasks, and there are\nnow a variety of benchmarks, such as GLUE, to help improve the development of\nsuch models for natural language understanding. However, little is known about\nhow well such models work on textual artifacts about code, and we are unaware\nof any systematic set of downstream tasks for such an evaluation. In this\npaper, we derive a set of benchmarks (BLANCA - Benchmarks for LANguage models\non Coding Artifacts) that assess code understanding based on tasks such as\npredicting the best answer to a question in a forum post, finding related forum\nposts, or predicting classes related in a hierarchy from class documentation.\nWe evaluate the performance of current state-of-the-art language models on\nthese tasks and show that there is a significant improvement on each task from\nfine tuning. We also show that multi-task training over BLANCA tasks helps\nbuild better language models for code understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1\">Ibrahim Abdelaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolby_J/0/1/0/all/0/1\">Julian Dolby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCusker_J/0/1/0/all/0/1\">Jamie McCusker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_K/0/1/0/all/0/1\">Kavitha Srinivas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Text Representations: A Theory-Driven Approach. (arXiv:2109.07458v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07458","description":"<p>Much of the progress in contemporary NLP has come from learning\nrepresentations, such as masked language model (MLM) contextual embeddings,\nthat turn challenging problems into simple classification tasks. But how do we\nquantify and explain this effect? We adapt general tools from computational\nlearning theory to fit the specific characteristics of text datasets and\npresent a method to evaluate the compatibility between representations and\ntasks. Even though many tasks can be easily solved with simple bag-of-words\n(BOW) representations, BOW does poorly on hard natural language inference\ntasks. For one such task we find that BOW cannot distinguish between real and\nrandomized labelings, while pre-trained MLM representations show 72x greater\ndistinction between real and random labelings than BOW. This method provides a\ncalibrated, quantitative measure of the difficulty of a classification-based\nNLP task, enabling comparisons between representations without requiring\nempirical evaluations that may be sensitive to initializations and\nhyperparameters. The method provides a fresh perspective on the patterns in a\ndataset and the alignment of those patterns with specific labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yauney_G/0/1/0/all/0/1\">Gregory Yauney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mimno_D/0/1/0/all/0/1\">David Mimno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Domain Adaptation of Language Models via Adaptive Tokenization. (arXiv:2109.07460v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07460","description":"<p>Contextual embedding-based language models trained on large data sets, such\nas BERT and RoBERTa, provide strong performance across a wide range of tasks\nand are ubiquitous in modern NLP. It has been observed that fine-tuning these\nmodels on tasks involving data from domains different from that on which they\nwere pretrained can lead to suboptimal performance. Recent work has explored\napproaches to adapt pretrained language models to new domains by incorporating\nadditional pretraining using domain-specific corpora and task data. We propose\nan alternative approach for transferring pretrained language models to new\ndomains by adapting their tokenizers. We show that domain-specific subword\nsequences can be efficiently determined directly from divergences in the\nconditional token distributions of the base and domain-specific corpora. In\ndatasets from four disparate domains, we find adaptive tokenization on a\npretrained RoBERTa model provides &gt;97% of the performance benefits of domain\nspecific pretraining. Our approach produces smaller models and less training\nand inference time than other approaches using tokenizer augmentation. While\nadaptive tokenization incurs a 6% increase in model parameters in our\nexperimentation, due to the introduction of 10k new domain-specific tokens, our\napproach, using 64 vCPUs, is 72x faster than further pretraining the language\nmodel on domain-specific corpora on 8 TPUs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachidananda_V/0/1/0/all/0/1\">Vin Sachidananda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kessler_J/0/1/0/all/0/1\">Jason S. Kessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yi-an Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnnIE: An Annotation Platform for Constructing Complete Open Information Extraction Benchmark. (arXiv:2109.07464v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07464","description":"<p>Open Information Extraction (OIE) is the task of extracting facts from\nsentences in the form of relations and their corresponding arguments in\nschema-free manner. Intrinsic performance of OIE systems is difficult to\nmeasure due to the incompleteness of existing OIE benchmarks: the ground truth\nextractions do not group all acceptable surface realizations of the same fact\nthat can be extracted from a sentence. To measure performance of OIE systems\nmore realistically, it is necessary to manually annotate complete facts (i.e.,\nclusters of all acceptable surface realizations of the same fact) from input\nsentences. We propose AnnIE: an interactive annotation platform that\nfacilitates such challenging annotation tasks and supports creation of complete\nfact-oriented OIE evaluation benchmarks. AnnIE is modular and flexible in order\nto support different use case scenarios (i.e., benchmarks covering different\ntypes of facts). We use AnnIE to build two complete OIE benchmarks: one with\nverb-mediated facts and another with facts encompassing named entities.\nFinally, we evaluate several OIE systems on our complete benchmarks created\nwith AnnIE. Our results suggest that existing incomplete benchmarks are overly\nlenient, and that OIE systems are not as robust as previously reported. We\npublicly release AnnIE under non-restrictive license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_N/0/1/0/all/0/1\">Niklas Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gashteovski_K/0/1/0/all/0/1\">Kiril Gashteovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingying Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotnis_B/0/1/0/all/0/1\">Bhushan Kotnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1\">Carolin Lawrence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1\">Mathias Niepert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Limits of Minimal Pairs in Contrastive Evaluation. (arXiv:2109.07465v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07465","description":"<p>Minimal sentence pairs are frequently used to analyze the behavior of\nlanguage models. It is often assumed that model behavior on contrastive pairs\nis predictive of model behavior at large. We argue that two conditions are\nnecessary for this assumption to hold: First, a tested hypothesis should be\nwell-motivated, since experiments show that contrastive evaluation can lead to\nfalse positives. Secondly, test data should be chosen such as to minimize\ndistributional discrepancy between evaluation time and deployment time. For a\ngood approximation of deployment-time decoding, we recommend that minimal pairs\nare created based on machine-generated text, as opposed to human-written\nreferences. We present a contrastive evaluation suite for English-German MT\nthat implements this recommendation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1\">Jannis Vamvas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negative Statements Considered Useful. (arXiv:2001.04425v5 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2001.04425","description":"<p>Knowledge bases (KBs) about notable entities and their properties are an\nimportant asset in applications such as search, question answering and\ndialogue. All popular KBs capture virtually only positive statements, and\nabstain from taking any stance on statements not stored in the KB. This paper\nmakes the case for explicitly stating salient statements that do not hold.\nNegative statements are useful to overcome limitations of question answering\nsystems that are mainly geared for positive questions; they can also contribute\nto informative summaries of entities. Due to the abundance of such invalid\nstatements, any effort to compile them needs to address ranking by saliency. We\npresent a statisticalinference method for compiling and ranking negative\nstatements, based on expectations from positive statements of related entities\nin peer groups. Experimental results, with a variety of datasets, show that the\nmethod can effectively discover notable negative statements, and extrinsic\nstudies underline their usefulness for entity summarization. Datasets and code\nare released as resources for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnaout_H/0/1/0/all/0/1\">Hiba Arnaout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Neural Program Synthesis from Multimodal Specifications. (arXiv:2010.01678v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.01678","description":"<p>Multimodal program synthesis, which leverages different types of user input\nto synthesize a desired program, is an attractive way to scale program\nsynthesis to challenging settings; however, it requires integrating noisy\nsignals from the user, like natural language, with hard constraints on the\nprogram's behavior. This paper proposes an optimal neural synthesis approach\nwhere the goal is to find a program that satisfies user-provided constraints\nwhile also maximizing the program's score with respect to a neural model.\nSpecifically, we focus on multimodal synthesis tasks in which the user intent\nis expressed using a combination of natural language (NL) and input-output\nexamples. At the core of our method is a top-down recurrent neural model that\nplaces distributions over abstract syntax trees conditioned on the NL input.\nThis model not only allows for efficient search over the space of syntactically\nvalid programs, but it allows us to leverage automated program analysis\ntechniques for pruning the search space based on infeasibility of partial\nprograms with respect to the user's constraints. The experimental results on a\nmultimodal synthesis dataset (StructuredRegex) show that our method\nsubstantially outperforms prior state-of-the-art techniques in terms of\naccuracy and efficiency, and finds model-optimal programs more frequently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiaochu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dillig_I/0/1/0/all/0/1\">Isil Dillig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infusing Multi-Source Knowledge with Heterogeneous Graph Neural Network for Emotional Conversation Generation. (arXiv:2012.04882v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.04882","description":"<p>The success of emotional conversation systems depends on sufficient\nperception and appropriate expression of emotions. In a real-world\nconversation, we firstly instinctively perceive emotions from multi-source\ninformation, including the emotion flow of dialogue history, facial\nexpressions, and personalities of speakers, and then express suitable emotions\naccording to our personalities, but these multiple types of information are\ninsufficiently exploited in emotional conversation fields. To address this\nissue, we propose a heterogeneous graph-based model for emotional conversation\ngeneration. Specifically, we design a Heterogeneous Graph-Based Encoder to\nrepresent the conversation content (i.e., the dialogue history, its emotion\nflow, facial expressions, and speakers' personalities) with a heterogeneous\ngraph neural network, and then predict suitable emotions for feedback. After\nthat, we employ an Emotion-Personality-Aware Decoder to generate a response not\nonly relevant to the conversation context but also with appropriate emotions,\nby taking the encoded graph representations, the predicted emotions from the\nencoder and the personality of the current speaker as inputs. Experimental\nresults show that our model can effectively perceive emotions from multi-source\nknowledge and generate a satisfactory response, which significantly outperforms\nprevious state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Synthetic Data Improves Neural Machine Translation with Knowledge Distillation. (arXiv:2012.15455v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15455","description":"<p>This paper explores augmenting monolingual data for knowledge distillation in\nneural machine translation. Source language monolingual text can be\nincorporated as a forward translation. Interestingly, we find the best way to\nincorporate target language monolingual text is to translate it to the source\nlanguage and round-trip translate it back to the target language, resulting in\na fully synthetic corpus. We find that combining monolingual data from both\nsource and target languages yields better performance than a corpus twice as\nlarge only in one language. Moreover, experiments reveal that the improvement\ndepends upon the provenance of the test set. If the test set was originally in\nthe source language (with the target side written by translators), then forward\ntranslating source monolingual data matters. If the test set was originally in\nthe target language (with the source written by translators), then\nincorporating target monolingual data matters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1\">Kenneth Heafield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-to-text Generation by Splicing Together Nearest Neighbors. (arXiv:2101.08248v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.08248","description":"<p>We propose to tackle data-to-text generation tasks by directly splicing\ntogether retrieved segments of text from \"neighbor\" source-target pairs. Unlike\nrecent work that conditions on retrieved neighbors but generates text\ntoken-by-token, left-to-right, we learn a policy that directly manipulates\nsegments of neighbor text, by inserting or replacing them in partially\nconstructed generations. Standard techniques for training such a policy require\nan oracle derivation for each generation, and we prove that finding the\nshortest such derivation can be reduced to parsing under a particular weighted\ncontext-free grammar. We find that policies learned in this way perform on par\nwith strong baselines in terms of automatic and human evaluation, but allow for\nmore interpretable and controllable generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1\">Sam Wiseman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1\">Arturs Backurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stratos_K/0/1/0/all/0/1\">Karl Stratos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute. (arXiv:2102.12459v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.12459","description":"<p>Large language models have become increasingly difficult to train because of\nthe growing computation time and cost. In this work, we present SRU++, a\nhighly-efficient architecture that combines fast recurrence and attention for\nsequence modeling. SRU++ exhibits strong modeling capacity and training\nefficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and\nBillion Word datasets, our model obtains better bits-per-character and\nperplexity while using 3x-10x less training cost compared to top-performing\nTransformer models. For instance, our model achieves a state-of-the-art result\non the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We\nfurther demonstrate that SRU++ requires minimal attention for near\nstate-of-the-art performance. Our results suggest jointly leveraging fast\nrecurrence with little attention as a promising direction for accelerating\nmodel training and inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribute Alignment: Controlling Text Generation from Pre-trained Language Models. (arXiv:2103.11070v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11070","description":"<p>Large language models benefit from training with a large amount of unlabeled\ntext, which gives them increasingly fluent and diverse generation capabilities.\nHowever, using these models for text generation that takes into account target\nattributes, such as sentiment polarity or specific topics, remains a challenge.\nWe propose a simple and flexible method for controlling text generation by\naligning disentangled attribute representations. In contrast to recent efforts\non training a discriminator to perturb the token level distribution for an\nattribute, we use the same data to learn an alignment function to guide the\npre-trained, non-controlled language model to generate texts with the target\nattribute without changing the original language model parameters. We evaluate\nour method on sentiment- and topic-controlled generation, and show large\nperformance gains over previous methods while retaining fluency and diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagae_K/0/1/0/all/0/1\">Kenji Sagae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Explanations from Empirical Explainers. (arXiv:2103.15429v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.15429","description":"<p>Amid a discussion about Green AI in which we see explainability neglected, we\nexplore the possibility to efficiently approximate computationally expensive\nexplainers. To this end, we propose feature attribution modelling with\nEmpirical Explainers. Empirical Explainers learn from data to predict the\nattribution maps of expensive explainers. We train and test Empirical\nExplainers in the language domain and find that they model their expensive\ncounterparts surprisingly well, at a fraction of the cost. They could thus\nmitigate the computational burden of neural explanations significantly, in\napplications that tolerate an approximation error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwarzenberg_R/0/1/0/all/0/1\">Robert Schwarzenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldhus_N/0/1/0/all/0/1\">Nils Feldhus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1\">Sebastian M&#xf6;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach. (arXiv:2104.04886v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.04886","description":"<p>Adversarial regularization has been shown to improve the generalization\nperformance of deep learning models in various natural language processing\ntasks. Existing works usually formulate the method as a zero-sum game, which is\nsolved by alternating gradient descent/ascent algorithms. Such a formulation\ntreats the adversarial and the defending players equally, which is undesirable\nbecause only the defending player contributes to the generalization\nperformance. To address this issue, we propose Stackelberg Adversarial\nRegularization (SALT), which formulates adversarial regularization as a\nStackelberg game. This formulation induces a competition between a leader and a\nfollower, where the follower generates perturbations, and the leader trains the\nmodel subject to the perturbations. Different from conventional approaches, in\nSALT, the leader is in an advantageous position. When the leader moves, it\nrecognizes the strategy of the follower and takes the anticipated follower's\noutcomes into consideration. Such a leader's advantage enables us to improve\nthe model fitting to the unperturbed data. The leader's strategic information\nis captured by the Stackelberg gradient, which is obtained using an unrolling\nalgorithm. Our experimental results on a set of machine translation and natural\nlanguage understanding tasks show that SALT outperforms existing adversarial\nregularization baselines across all tasks. Our code is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Span Pointer Networks for Non-Autoregressive Task-Oriented Semantic Parsing. (arXiv:2104.07275v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07275","description":"<p>An effective recipe for building seq2seq, non-autoregressive, task-oriented\nparsers to map utterances to semantic frames proceeds in three steps: encoding\nan utterance $x$, predicting a frame's length |y|, and decoding a |y|-sized\nframe with utterance and ontology tokens. Though empirically strong, these\nmodels are typically bottlenecked by length prediction, as even small\ninaccuracies change the syntactic and semantic characteristics of resulting\nframes. In our work, we propose span pointer networks, non-autoregressive\nparsers which shift the decoding task from text generation to span prediction;\nthat is, when imputing utterance spans into frame slots, our model produces\nendpoints (e.g., [i, j]) as opposed to text (e.g., \"6pm\"). This natural\nquantization of the output space reduces the variability of gold frames,\ntherefore improving length prediction and, ultimately, exact match.\nFurthermore, length prediction is now responsible for frame syntax and the\ndecoder is responsible for frame semantics, resulting in a coarse-to-fine\nmodel. We evaluate our approach on several task-oriented semantic parsing\ndatasets. Notably, we bridge the quality gap between non-autogressive and\nautoregressive parsers, achieving 87 EM on TOPv2 (Chen et al. 2020).\nFurthermore, due to our more consistent gold frames, we show strong\nimprovements in model generalization in both cross-domain and cross-lingual\ntransfer in low-resource settings. Finally, due to our diminished output\nvocabulary, we observe 70% reduction in latency and 83% reduction in memory at\nbeam size 5 compared to prior non-autoregressive parsers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1\">Pierce Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Arun Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shrey Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Abhinav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zotov_A/0/1/0/all/0/1\">Alexander Zotov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1\">Ahmed Aly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Open-Vocabulary Translation from Visual Text Representations. (arXiv:2104.08211v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08211","description":"<p>Machine translation models have discrete vocabularies and commonly use\nsubword segmentation techniques to achieve an 'open vocabulary.' This approach\nrelies on consistent and correct underlying unicode sequences, and makes models\nsusceptible to degradation from common types of noise and variation. Motivated\nby the robustness of human language processing, we propose the use of visual\ntext representations, which dispense with a finite set of text embeddings in\nfavor of continuous vocabularies created by processing visually rendered text\nwith sliding windows. We show that models using visual text representations\napproach or match performance of traditional text models on small and larger\ndatasets. More importantly, models with visual embeddings demonstrate\nsignificant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a\ncharacter permuted German-English task where subword models degrade to 1.9.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1\">Elizabeth Salesky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etter_D/0/1/0/all/0/1\">David Etter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does language help generalization in vision models?. (arXiv:2104.08313v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2104.08313","description":"<p>Vision models trained on multimodal datasets can benefit from the wide\navailability of large image-caption datasets. A recent model (CLIP) was found\nto generalize well in zero-shot and transfer learning settings. This could\nimply that linguistic or \"semantic grounding\" confers additional generalization\nabilities to the visual feature space. Here, we systematically evaluate various\nmultimodal architectures and vision-only models in terms of unsupervised\nclustering, few-shot learning, transfer learning and adversarial robustness. In\neach setting, multimodal training produced no additional generalization\ncapability compared to standard supervised visual training. We conclude that\nwork is still required for semantic grounding to help improve vision models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Devillers_B/0/1/0/all/0/1\">Benjamin Devillers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choksi_B/0/1/0/all/0/1\">Bhavin Choksi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielawski_R/0/1/0/all/0/1\">Romain Bielawski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1\">Rufin VanRullen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning. (arXiv:2104.08718v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08718","description":"<p>Image captioning has conventionally relied on reference-based automatic\nevaluations, where machine captions are compared against captions written by\nhumans. This is in contrast to the reference-free manner in which humans assess\ncaption quality.\n</p>\n<p>In this paper, we report the surprising empirical finding that CLIP (Radford\net al., 2021), a cross-modal model pretrained on 400M image+caption pairs from\nthe web, can be used for robust automatic evaluation of image captioning\nwithout the need for references. Experiments spanning several corpora\ndemonstrate that our new reference-free metric, CLIPScore, achieves the highest\ncorrelation with human judgements, outperforming existing reference-based\nmetrics like CIDEr and SPICE. Information gain experiments demonstrate that\nCLIPScore, with its tight focus on image-text compatibility, is complementary\nto existing reference-based metrics that emphasize text-text similarities.\nThus, we also present a reference-augmented version, RefCLIPScore, which\nachieves even higher correlation. Beyond literal description tasks, several\ncase studies reveal domains where CLIPScore performs well (clip-art images,\nalt-text rating), but also where it is relatively weaker in comparison to\nreference-based metrics, e.g., news captions that require richer contextual\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters. (arXiv:2105.06232v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06232","description":"<p>To diversify and enrich generated dialogue responses, knowledge-grounded\ndialogue has been investigated in recent years. The existing methods tackle the\nknowledge grounding challenge by retrieving the relevant sentences over a large\ncorpus and augmenting the dialogues with explicit extra information. Despite\ntheir success, however, the existing works have drawbacks on the inference\nefficiency. This paper proposes KnowExpert, an end-to-end framework to bypass\nthe explicit retrieval process and inject knowledge into the pre-trained\nlanguage models with lightweight adapters and adapt to the knowledge-grounded\ndialogue task. To the best of our knowledge, this is the first attempt to\ntackle this challenge without retrieval in this task under an open-domain\nchit-chat scenario. The experimental results show that KknowExpert performs\ncomparably with some retrieval-based baselines while being time-efficient in\ninference, demonstrating the potential of our proposed direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction. (arXiv:2105.06965v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06965","description":"<p>When language models process syntactically complex sentences, do they use\ntheir representations of syntax in a manner that is consistent with the grammar\nof the language? We propose AlterRep, an intervention-based method to address\nthis question. For any linguistic feature of a given sentence, AlterRep\ngenerates counterfactual representations by altering how the feature is\nencoded, while leaving intact all other aspects of the original representation.\nBy measuring the change in a model's word prediction behavior when these\ncounterfactual representations are substituted for the original ones, we can\ndraw conclusions about the causal effect of the linguistic feature in question\non the model's behavior. We apply this method to study how BERT models of\ndifferent sizes process relative clauses (RCs). We find that BERT variants use\nRC boundary information during word prediction in a manner that is consistent\nwith the rules of English grammar; this RC boundary information generalizes to\na considerable extent across different RC types, suggesting that BERT\nrepresents RCs as an abstract linguistic category.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_G/0/1/0/all/0/1\">Grusha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization. (arXiv:2105.09601v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.09601","description":"<p>In recent years, abstractive text summarization with multimodal inputs has\nstarted drawing attention due to its ability to accumulate information from\ndifferent source modalities and generate a fluent textual summary. However,\nexisting methods use short videos as the visual modality and short summary as\nthe ground-truth, therefore, perform poorly on lengthy videos and long\nground-truth summary. Additionally, there exists no benchmark dataset to\ngeneralize this task on videos of varying lengths. In this paper, we introduce\nAVIATE, the first large-scale dataset for abstractive text summarization with\nvideos of diverse duration, compiled from presentations in well-known academic\nconferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding\nresearch papers as the reference summaries, which ensure adequate quality and\nuniformity of the ground-truth. We then propose FLORAL, a factorized\nmulti-modal Transformer based decoder-only language model, which inherently\ncaptures the intra-modal and inter-modal dynamics within various input\nmodalities for the text summarization task. FLORAL utilizes an increasing\nnumber of self-attentions to capture multimodality and performs significantly\nbetter than traditional encoder-decoder based networks. Extensive experiments\nillustrate that FLORAL achieves significant improvement over the baselines in\nboth qualitative and quantitative evaluations on the existing How2 dataset for\nshort videos and newly introduced AVIATE dataset for videos with diverse\nduration, beating the best baseline on the two datasets by $1.39$ and $2.74$\nROUGE-L points respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1\">Yash Kumar Atri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PTR: Prompt Tuning with Rules for Text Classification. (arXiv:2105.11259v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.11259","description":"<p>Fine-tuned pre-trained language models (PLMs) have achieved awesome\nperformance on almost all NLP tasks. By using additional prompts to fine-tune\nPLMs, we can further stimulate the rich knowledge distributed in PLMs to better\nserve downstream tasks. Prompt tuning has achieved promising results on some\nfew-class classification tasks such as sentiment classification and natural\nlanguage inference. However, manually designing lots of language prompts is\ncumbersome and fallible. For those auto-generated prompts, it is also expensive\nand time-consuming to verify their effectiveness in non-few-shot scenarios.\nHence, it is still challenging for prompt tuning to address many-class\nclassification tasks. To this end, we propose prompt tuning with rules (PTR)\nfor many-class text classification and apply logic rules to construct prompts\nwith several sub-prompts. In this way, PTR is able to encode prior knowledge of\neach class into prompt tuning. We conduct experiments on relation\nclassification, a typical and complicated many-class classification task, and\nthe results show that PTR can significantly and consistently outperform\nexisting state-of-the-art baselines. This indicates that PTR is a promising\napproach to take advantage of both human prior knowledge and PLMs for those\ncomplicated classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-based Multimodal Fusion. (arXiv:2106.03821v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2106.03821","description":"<p>It is now well established from a variety of studies that there is a\nsignificant benefit from combining video and audio data in detecting active\nspeakers. However, either of the modalities can potentially mislead audiovisual\nfusion by inducing unreliable or deceptive information. This paper outlines\nactive speaker detection as a multi-objective learning problem to leverage best\nof each modalities using a novel self-attention, uncertainty-based multimodal\nfusion scheme. Results obtained show that the proposed multi-objective learning\narchitecture outperforms traditional approaches in improving both mAP and AUC\nscores. We further demonstrate that our fusion strategy surpasses, in active\nspeaker detection, other modality fusion methods reported in various\ndisciplines. We finally show that the proposed method significantly improves\nthe state-of-the-art on the AVA-ActiveSpeaker dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pouthier_B/0/1/0/all/0/1\">Baptiste Pouthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilati_L/0/1/0/all/0/1\">Laurent Pilati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gudupudi_L/0/1/0/all/0/1\">Leela K. Gudupudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouveyron_C/0/1/0/all/0/1\">Charles Bouveyron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precioso_F/0/1/0/all/0/1\">Frederic Precioso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Grounding with 3D Objects. (arXiv:2107.12514v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12514","description":"<p>Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" Flat images\nof candidate mice may not provide the discriminative information needed for\n\"wireless.\" The world, and objects in it, are not flat images but complex 3D\nshapes. If a human requests an object based on any of its basic properties,\nsuch as color, shape, or texture, robots should perform the necessary\nexploration to accomplish the task. In particular, while substantial effort and\nprogress has been made on understanding explicitly visual attributes like color\nand category, comparatively little progress has been made on understanding\nlanguage about shapes and contours. In this work, we introduce a novel\nreasoning task that targets both visual and non-visual language about 3D\nobjects. Our new benchmark, ShapeNet Annotated with Referring Expressions\n(SNARE) requires a model to choose which of two objects is being referenced by\na natural language description. We introduce several CLIP-based models for\ndistinguishing objects and demonstrate that while recent advances in jointly\nmodeling vision and language are useful for robotic language understanding, it\nis still the case that these image-based models are weaker at understanding the\n3D nature of objects -- properties which play a key role in manipulation. We\nfind that adding view estimation to language grounding models improves accuracy\non both SNARE and when identifying objects referred to in language on a robot\nplatform, but note that a large gap remains between these models and human\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Continual Entity Learning in Language Models for Conversational Agents. (arXiv:2108.00082v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00082","description":"<p>Neural language models (LM) trained on diverse corpora are known to work well\non previously seen entities, however, updating these models with dynamically\nchanging entities such as place names, song titles and shopping items requires\nre-training from scratch and collecting full sentences containing these\nentities. We aim to address this issue, by introducing entity-aware language\nmodels (EALM), where we integrate entity models trained on catalogues of\nentities into the pre-trained LMs. Our combined language model adaptively adds\ninformation from the entity models into the pre-trained LM depending on the\nsentence context. Our entity models can be updated independently of the\npre-trained LM, enabling us to influence the distribution of entities output by\nthe final LM, without any further training of the pre-trained LM. We show\nsignificant perplexity improvements on task-oriented dialogue datasets,\nespecially on long-tailed utterances, with an ability to continually adapt to\nnew entities (to an extent).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gadde_R/0/1/0/all/0/1\">Ravi Teja Gadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing. (arXiv:2108.04990v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04990","description":"<p>Interpretability methods like Integrated Gradient and LIME are popular\nchoices for explaining natural language model predictions with relative word\nimportance scores. These interpretations need to be robust for trustworthy NLP\napplications in high-stake areas like medicine or finance. Our paper\ndemonstrates how interpretations can be manipulated by making simple word\nperturbations on an input text. Via a small portion of word-level swaps, these\nadversarial perturbations aim to make the resulting text semantically and\nspatially similar to its seed input (therefore sharing similar\ninterpretations). Simultaneously, the generated examples achieve the same\nprediction label as the seed yet are given a substantially different\nexplanation by the interpretation methods. Our experiments generate fragile\ninterpretations to attack two SOTA interpretation methods, across three popular\nTransformer models and on two different NLP datasets. We observe that the rank\norder correlation drops by over 20% when less than 10% of words are perturbed\non average. Further, rank-order correlation keeps decreasing as more words get\nperturbed. Furthermore, we demonstrate that candidates generated from our\nmethod have good quality metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Sanchit Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1\">Arshdeep Sekhon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Span Fine-tuning for Pre-trained Language Models. (arXiv:2108.12848v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12848","description":"<p>Pre-trained language models (PrLM) have to carefully manage input units when\ntraining on a very large text with a vocabulary consisting of millions of\nwords. Previous works have shown that incorporating span-level information over\nconsecutive words in pre-training could further improve the performance of\nPrLMs. However, given that span-level clues are introduced and fixed in\npre-training, previous methods are time-consuming and lack of flexibility. To\nalleviate the inconvenience, this paper presents a novel span fine-tuning\nmethod for PrLMs, which facilitates the span setting to be adaptively\ndetermined by specific downstream tasks during the fine-tuning phase. In\ndetail, any sentences processed by the PrLM will be segmented into multiple\nspans according to a pre-sampled dictionary. Then the segmentation information\nwill be sent through a hierarchical CNN module together with the representation\noutputs of the PrLM and ultimately generate a span-enhanced representation.\nExperiments on GLUE benchmark show that the proposed span fine-tuning method\nsignificantly enhances the PrLM, and at the same time, offer more flexibility\nin an efficient way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_R/0/1/0/all/0/1\">Rongzhou Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\infty$-former: Infinite Memory Transformer. (arXiv:2109.00301v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00301","description":"<p>Transformers struggle when attending to long contexts, since the amount of\ncomputation grows with the context length, and therefore they cannot model\nlong-term memories effectively. Several variations have been proposed to\nalleviate this problem, but they all have a finite memory capacity, being\nforced to drop old information. In this paper, we propose the $\\infty$-former,\nwhich extends the vanilla transformer with an unbounded long-term memory. By\nmaking use of a continuous-space attention mechanism to attend over the\nlong-term memory, the $\\infty$-former's attention complexity becomes\nindependent of the context length. Thus, it is able to model arbitrarily long\ncontexts and maintain \"sticky memories\" while keeping a fixed computation\nbudget. Experiments on a synthetic sorting task demonstrate the ability of the\n$\\infty$-former to retain information from long sequences. We also perform\nexperiments on language modeling, by training a model from scratch and by\nfine-tuning a pre-trained language model, which show benefits of unbounded\nlong-term memories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1\">Pedro Henrique Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinho_Z/0/1/0/all/0/1\">Zita Marinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LegaLMFiT: Efficient Short Legal Text Classification with LSTM Language Model Pre-Training. (arXiv:2109.00993v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00993","description":"<p>Large Transformer-based language models such as BERT have led to broad\nperformance improvements on many NLP tasks. Domain-specific variants of these\nmodels have demonstrated excellent performance on a variety of specialised\ntasks. In legal NLP, BERT-based models have led to new state-of-the-art results\non multiple tasks. The exploration of these models has demonstrated the\nimportance of capturing the specificity of the legal language and its\nvocabulary. However, such approaches suffer from high computational costs,\nleading to a higher ecological impact and lower accessibility. Our findings,\nfocusing on English language legal text, show that lightweight LSTM-based\nLanguage Models are able to capture enough information from a small legal text\npretraining corpus and achieve excellent performance on short legal text\nclassification tasks. This is achieved with a significantly reduced\ncomputational overhead compared to BERT-based models. However, our method also\nshows degraded performance on a more complex task, multi-label classification\nof longer documents, highlighting the limitations of this lightweight approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clavie_B/0/1/0/all/0/1\">Benjamin Clavi&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gheewala_A/0/1/0/all/0/1\">Akshita Gheewala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briton_P/0/1/0/all/0/1\">Paul Briton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alphonsus_M/0/1/0/all/0/1\">Marc Alphonsus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laabiyad_R/0/1/0/all/0/1\">Rym Laabiyad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccoli_F/0/1/0/all/0/1\">Francesco Piccoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment. (arXiv:2109.02363v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02363","description":"<p>Cross-lingual entity alignment (EA) aims to find the equivalent entities\nbetween crosslingual KGs, which is a crucial step for integrating KGs.\nRecently, many GNN-based EA methods are proposed and show decent performance\nimprovements on several public datasets. Meanwhile, existing GNN-based EA\nmethods inevitably inherit poor interpretability and low efficiency from neural\nnetworks. Motivated by the isomorphic assumption of GNNbased methods, we\nsuccessfully transform the cross-lingual EA problem into the assignment\nproblem. Based on this finding, we propose a frustratingly Simple but Effective\nUnsupervised entity alignment method (SEU) without neural networks. Extensive\nexperiments show that our proposed unsupervised method even beats advanced\nsupervised methods across all public datasets and has high efficiency,\ninterpretability, and stability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1\">Man Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed Attention Transformer for Leveraging Word-Level Knowledge to Neural Cross-Lingual Information Retrieval. (arXiv:2109.02789v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2109.02789","description":"<p>Pretrained contextualized representations offer great success for many\ndownstream tasks, including document ranking. The multilingual versions of such\npretrained representations provide a possibility of jointly learning many\nlanguages with the same model. Although it is expected to gain big with such\njoint training, in the case of cross lingual information retrieval (CLIR), the\nmodels under a multilingual setting are not achieving the same level of\nperformance as those under a monolingual setting. We hypothesize that the\nperformance drop is due to the translation gap between query and documents. In\nthe monolingual retrieval task, because of the same lexical inputs, it is\neasier for model to identify the query terms that occurred in documents.\nHowever, in the multilingual pretrained models that the words in different\nlanguages are projected into the same hyperspace, the model tends to translate\nquery terms into related terms, i.e., terms that appear in a similar context,\nin addition to or sometimes rather than synonyms in the target language. This\nproperty is creating difficulties for the model to connect terms that cooccur\nin both query and document. To address this issue, we propose a novel Mixed\nAttention Transformer (MAT) that incorporates external word level knowledge,\nsuch as a dictionary or translation table. We design a sandwich like\narchitecture to embed MAT into the recent transformer based deep neural models.\nBy encoding the translation knowledge into an attention matrix, the model with\nMAT is able to focus on the mutually translated words in the input sequence.\nExperimental results demonstrate the effectiveness of the external knowledge\nand the significant improvement of MAT embedded neural reranking model on CLIR\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonab_H/0/1/0/all/0/1\">Hamed Bonab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1\">Razieh Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers. (arXiv:2109.05186v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05186","description":"<p>This paper investigates continual learning for semantic parsing. In this\nsetting, a neural semantic parser learns tasks sequentially without accessing\nfull training data from previous tasks. Direct application of the SOTA\ncontinual learning algorithms to this problem fails to achieve comparable\nperformance with re-training models with all seen tasks because they have not\nconsidered the special properties of structured outputs yielded by semantic\nparsers. Therefore, we propose TotalRecall, a continual learning method\ndesigned for neural semantic parsers from two aspects: i) a sampling method for\nmemory replay that diversifies logical form templates and balances\ndistributions of parse actions in a memory; ii) a two-stage training method\nthat significantly improves generalization capability of the parsers across\ntasks. We conduct extensive experiments to study the research problems involved\nin continual semantic parsing and demonstrate that a neural semantic parser\ntrained with TotalRecall achieves superior performance than the one trained\ndirectly with the SOTA continual learning algorithms and achieve a 3-6 times\nspeedup compared to re-training from scratch. Code and datasets are available\nat: https://github.com/zhuang-li/cl_nsp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good-Enough Example Extrapolation. (arXiv:2109.05602v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05602","description":"<p>This paper asks whether extrapolating the hidden space distribution of text\nexamples from one class onto another is a valid inductive bias for data\naugmentation. To operationalize this question, I propose a simple data\naugmentation protocol called \"good-enough example extrapolation\" (GE3). GE3 is\nlightweight and has no hyperparameters. Applied to three text classification\ndatasets for various data imbalance scenarios, GE3 improves performance more\nthan upsampling and other hidden-space data augmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids' Representations. (arXiv:2109.05958v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05958","description":"<p>Most of the recent works on probing representations have focused on BERT,\nwith the presumption that the findings might be similar to the other models. In\nthis work, we extend the probing studies to two other models in the family,\nnamely ELECTRA and XLNet, showing that variations in the pre-training\nobjectives or architectural choices can result in different behaviors in\nencoding linguistic information in the representations. Most notably, we\nobserve that ELECTRA tends to encode linguistic knowledge in the deeper layers,\nwhereas XLNet instead concentrates that in the earlier layers. Also, the former\nmodel undergoes a slight change during fine-tuning, whereas the latter\nexperiences significant adjustments. Moreover, we show that drawing conclusions\nbased on the weight mixing evaluation strategy -- which is widely used in the\ncontext of layer-wise probing -- can be misleading given the norm disparity of\nthe representations across different layers. Instead, we adopt an alternative\ninformation-theoretic probing with minimum description length, which has\nrecently been proven to provide more reliable and informative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1\">Mohsen Fayyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghazadeh_E/0/1/0/all/0/1\">Ehsan Aghazadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modarressi_A/0/1/0/all/0/1\">Ali Modarressi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohebbi_H/0/1/0/all/0/1\">Hosein Mohebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Emergence of the Shape Bias Results from Communicative Efficiency. (arXiv:2109.06232v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06232","description":"<p>By the age of two, children tend to assume that new word categories are based\non objects' shape, rather than their color or texture; this assumption is\ncalled the shape bias. They are thought to learn this bias by observing that\ntheir caregiver's language is biased towards shape based categories. This\npresents a chicken and egg problem: if the shape bias must be present in the\nlanguage in order for children to learn it, how did it arise in language in the\nfirst place? In this paper, we propose that communicative efficiency explains\nboth how the shape bias emerged and why it persists across generations. We\nmodel this process with neural emergent language agents that learn to\ncommunicate about raw pixelated images. First, we show that the shape bias\nemerges as a result of efficient communication strategies employed by agents.\nSecond, we show that pressure brought on by communicative need is also\nnecessary for it to persist across generations; simply having a shape bias in\nan agent's input language is insufficient. These results suggest that, over and\nabove the operation of other learning strategies, the shape bias in human\nlearners may emerge and be sustained by communicative pressures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Portelance_E/0/1/0/all/0/1\">Eva Portelance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_M/0/1/0/all/0/1\">Michael C. Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1\">Romain Laroche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-OCR Document Correction with large Ensembles of Character Sequence Models. (arXiv:2109.06264v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06264","description":"<p>In this paper, we propose a novel method based on character\nsequence-to-sequence models to correct documents already processed with Optical\nCharacter Recognition (OCR) systems. The main contribution of this paper is a\nset of strategies to accurately process strings much longer than the ones used\nto train the sequence model while being sample- and resource-efficient,\nsupported by thorough experimentation. The strategy with the best performance\ninvolves splitting the input document in character n-grams and combining their\nindividual corrections into the final output using a voting scheme that is\nequivalent to an ensemble of a large number of sequence models. We further\ninvestigate how to weigh the contributions from each one of the members of this\nensemble. We test our method on nine languages of the ICDAR 2019 competition on\npost-OCR text correction and achieve a new state-of-the-art performance in five\nof them. Our code for post-OCR correction is shared at\nhttps://github.com/jarobyte91/post_ocr_correction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_Orta_J/0/1/0/all/0/1\">Juan Ramirez-Orta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xamena_E/0/1/0/all/0/1\">Eduardo Xamena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguitman_A/0/1/0/all/0/1\">Ana Maguitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milios_E/0/1/0/all/0/1\">Evangelos Milios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Axel J. Soto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expert Knowledge-Guided Length-Variant Hierarchical Label Generation for Proposal Classification. (arXiv:2109.06661v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.06661","description":"<p>To advance the development of science and technology, research proposals are\nsubmitted to open-court competitive programs developed by government agencies\n(e.g., NSF). Proposal classification is one of the most important tasks to\nachieve effective and fair review assignments. Proposal classification aims to\nclassify a proposal into a length-variant sequence of labels. In this paper, we\nformulate the proposal classification problem into a hierarchical multi-label\nclassification task. Although there are certain prior studies, proposal\nclassification exhibit unique features: 1) the classification result of a\nproposal is in a hierarchical discipline structure with different levels of\ngranularity; 2) proposals contain multiple types of documents; 3) domain\nexperts can empirically provide partial labels that can be leveraged to improve\ntask performances. In this paper, we focus on developing a new deep proposal\nclassification framework to jointly model the three features. In particular, to\nsequentially generate labels, we leverage previously-generated labels to\npredict the label of next level; to integrate partial labels from experts, we\nuse the embedding of these empirical partial labels to initialize the state of\nneural networks. Our model can automatically identify the best length of label\nsequence to stop next label prediction. Finally, we present extensive results\nto demonstrate that our method can jointly model partial labels, textual\ninformation, and semantic dependencies in label sequences, and, thus, achieve\nadvanced performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Ziyue Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanjie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Fuzzy Attention for Structured Sentiment Analysis. (arXiv:2109.06719v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06719","description":"<p>Attention scorers have achieved success in parsing tasks like semantic and\nsyntactic dependency parsing. However, in tasks modeled into parsing, like\nstructured sentiment analysis, \"dependency edges\" are very sparse which hinders\nparser performance. Thus we propose a sparse and fuzzy attention scorer with\npooling layers which improves parser performance and sets the new\nstate-of-the-art on structured sentiment analysis. We further explore the\nparsing modeling on structured sentiment analysis with second-order parsing and\nintroduce a novel sparse second-order edge building procedure that leads to\nsignificant improvement in parsing performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letain Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding. (arXiv:2109.06838v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06838","description":"<p>While large language models have shown exciting progress on several NLP\nbenchmarks, evaluating their ability for complex analogical reasoning remains\nunder-explored. Here, we introduce a high-quality crowdsourced dataset of\nnarratives for employing proverbs in context as a benchmark for abstract\nlanguage understanding. The dataset provides fine-grained annotation of aligned\nspans between proverbs and narratives, and contains minimal lexical overlaps\nbetween narratives and proverbs, ensuring that models need to go beyond\nsurface-level reasoning to succeed. We explore three tasks: (1) proverb\nrecommendation and alignment prediction, (2) narrative generation for a given\nproverb and topic, and (3) identifying narratives with similar motifs. Our\nexperiments show that neural language models struggle in our tasks compared to\nhumans, and the tasks pose multiple learning challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Legal Transformer Models May Not Always Help. (arXiv:2109.06862v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06862","description":"<p>Deep learning-based Natural Language Processing methods, especially\ntransformers, have achieved impressive performance in the last few years.\nApplying those state-of-the-art NLP methods to legal activities to automate or\nsimplify some simple work is of great value. This work investigates the value\nof domain adaptive pre-training and language adapters in legal NLP tasks. By\ncomparing the performance of language models with domain adaptive pre-training\non different tasks and different dataset splits, we show that domain adaptive\npre-training is only helpful with low-resource downstream tasks, thus far from\nbeing a panacea. We also benchmark the performance of adapters in a typical\nlegal NLP task and show that they can yield similar performance to full model\ntuning with much smaller training costs. As an additional result, we release\nLegalRoBERTa, a RoBERTa model further pre-trained on legal corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Saibo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebret_R/0/1/0/all/0/1\">R&#xe9;mi Lebret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1\">Karl Aberer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Multi-Scale Aligned Distillation for Low-Resolution Detection. (arXiv:2109.06875v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06875","description":"<p>In instance-level detection tasks (e.g., object detection), reducing input\nresolution is an easy option to improve runtime efficiency. However, this\noption traditionally hurts the detection performance much. This paper focuses\non boosting the performance of low-resolution models by distilling knowledge\nfrom a high- or multi-resolution model. We first identify the challenge of\napplying knowledge distillation (KD) to teacher and student networks that act\non different input resolutions. To tackle it, we explore the idea of spatially\naligning feature maps between models of varying input resolutions by shifting\nfeature pyramid positions and introduce aligned multi-scale training to train a\nmulti-scale teacher that can distill its knowledge to a low-resolution student.\nFurther, we propose crossing feature-level fusion to dynamically fuse teacher's\nmulti-resolution features to guide the student better. On several\ninstance-level detection tasks and datasets, the low-resolution models trained\nvia our approach perform competitively with high-resolution models trained via\nconventional multi-scale training, while outperforming the latter's\nlow-resolution models by 2.1% to 3.6% in terms of mAP. Our code is made\npublicly available at https://github.com/dvlab-research/MSAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hardware-aware Real-time Myocardial Segmentation Quality Control in Contrast Echocardiography. (arXiv:2109.06909v1 [eess.IV])","link":"http://arxiv.org/abs/2109.06909","description":"<p>Automatic myocardial segmentation of contrast echocardiography has shown\ngreat potential in the quantification of myocardial perfusion parameters.\nSegmentation quality control is an important step to ensure the accuracy of\nsegmentation results for quality research as well as its clinical application.\nUsually, the segmentation quality control happens after the data acquisition.\nAt the data acquisition time, the operator could not know the quality of the\nsegmentation results. On-the-fly segmentation quality control could help the\noperator to adjust the ultrasound probe or retake data if the quality is\nunsatisfied, which can greatly reduce the effort of time-consuming manual\ncorrection. However, it is infeasible to deploy state-of-the-art DNN-based\nmodels because the segmentation module and quality control module must fit in\nthe limited hardware resource on the ultrasound machine while satisfying strict\nlatency constraints. In this paper, we propose a hardware-aware neural\narchitecture search framework for automatic myocardial segmentation and quality\ncontrol of contrast echocardiography. We explicitly incorporate the hardware\nlatency as a regularization term into the loss function during training. The\nproposed method searches the best neural network architecture for the\nsegmentation module and quality prediction module with strict latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zeng_D/0/1/0/all/0/1\">Dewen Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_Y/0/1/0/all/0/1\">Yukun Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_H/0/1/0/all/0/1\">Haiyun Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_M/0/1/0/all/0/1\">Meiping Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_J/0/1/0/all/0/1\">Jian Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1\">Jingtong Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A trainable monogenic ConvNet layer robust in front of large contrast changes in image classification. (arXiv:2109.06926v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06926","description":"<p>Convolutional Neural Networks (ConvNets) at present achieve remarkable\nperformance in image classification tasks. However, current ConvNets cannot\nguarantee the capabilities of the mammalian visual systems such as invariance\nto contrast and illumination changes. Some ideas to overcome the illumination\nand contrast variations usually have to be tuned manually and tend to fail when\ntested with other types of data degradation. In this context, we present a new\nbio-inspired {entry} layer, M6, which detects low-level geometric features\n(lines, edges, and orientations) which are similar to patterns detected by the\nV1 visual cortex. This new trainable layer is capable of coping with image\nclassification even with large contrast variations. The explanation for this\nbehavior is the monogenic signal geometry, which represents each pixel value in\na 3D space using quaternions, a fact that confers a degree of explainability to\nthe networks. We compare M6 with a conventional convolutional layer (C) and a\ndeterministic quaternion local phase layer (Q9). The experimental setup {is\ndesigned to evaluate the robustness} of our M6 enriched ConvNet model and\nincludes three architectures, four datasets, three types of contrast\ndegradation (including non-uniform haze degradations). The numerical results\nreveal that the models with M6 are the most robust in front of any kind of\ncontrast variations. This amounts to a significant enhancement of the C models,\nwhich usually have reasonably good performance only when the same training and\ntest degradation are used, except for the case of maximum degradation.\nMoreover, the Structural Similarity Index Measure (SSIM) is used to analyze and\nexplain the robustness effect of the M6 feature maps under any kind of contrast\ndegradations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moya_Sanchez_E/0/1/0/all/0/1\">E. Ulises Moya-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xambo_Descamps_S/0/1/0/all/0/1\">Sebasti&#xe1; Xambo-Descamps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_A/0/1/0/all/0/1\">Abraham S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salazar_Colores_S/0/1/0/all/0/1\">Sebasti&#xe1;n Salazar-Colores</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cortes_U/0/1/0/all/0/1\">Ulises Cort&#xe9;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Wound Classification using Wound Image and Location by Deep Neural Network. (arXiv:2109.06969v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06969","description":"<p>Wound classification is an essential step of wound diagnosis. An efficient\nclassifier can assist wound specialists in classifying wound types with less\nfinancial and time costs and help them decide an optimal treatment procedure.\nThis study developed a deep neural network-based multi-modal classifier using\nwound images and their corresponding locations to categorize wound images into\nmultiple classes, including diabetic, pressure, surgical, and venous ulcers. A\nbody map is also developed to prepare the location data, which can help wound\nspecialists tag wound locations more efficiently. Three datasets containing\nimages and their corresponding location information are designed with the help\nof wound specialists. The multi-modal network is developed by concatenating the\nimage-based and location-based classifier's outputs with some other\nmodifications. The maximum accuracy on mixed-class classifications (containing\nbackground and normal skin) varies from 77.33% to 100% on different\nexperiments. The maximum accuracy on wound-class classifications (containing\nonly diabetic, pressure, surgical, and venous) varies from 72.95% to 98.08% on\ndifferent experiments. The proposed multi-modal network also shows a\nsignificant improvement in results from the previous works of literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anisuzzaman_D/0/1/0/all/0/1\">D. M. Anisuzzaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_Y/0/1/0/all/0/1\">Yash Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_B/0/1/0/all/0/1\">Behrouz Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niezgoda_J/0/1/0/all/0/1\">Jeffrey Niezgoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1\">Sandeep Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zeyun Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining GEDI and Sentinel-2 for wall-to-wall mapping of tall and short crops. (arXiv:2109.06972v1 [eess.IV])","link":"http://arxiv.org/abs/2109.06972","description":"<p>High resolution crop type maps are an important tool for improving food\nsecurity, and remote sensing is increasingly used to create such maps in\nregions that possess ground truth labels for model training. However, these\nlabels are absent in many regions, and models trained in other regions on\ntypical satellite features, such as those from optical sensors, often exhibit\nlow performance when transferred. Here we explore the use of NASA's Global\nEcosystem Dynamics Investigation (GEDI) spaceborne lidar instrument, combined\nwith Sentinel-2 optical data, for crop type mapping. Using data from three\nmajor cropped regions (in China, France, and the United States) we first\ndemonstrate that GEDI energy profiles are capable of reliably distinguishing\nmaize, a crop typically above 2m in height, from crops like rice and soybean\nthat are shorter. We further show that these GEDI profiles provide much more\ninvariant features across geographies compared to spectral and phenological\nfeatures detected by passive optical sensors. GEDI is able to distinguish maize\nfrom other crops within each region with accuracies higher than 84%, and able\nto transfer across regions with accuracies higher than 82% compared to 64% for\ntransfer of optical features. Finally, we show that GEDI profiles can be used\nto generate training labels for models based on optical imagery from\nSentinel-2, thereby enabling the creation of 10m wall-to-wall maps of tall\nversus short crops in label-scarce regions. As maize is the second most widely\ngrown crop in the world and often the only tall crop grown within a landscape,\nwe conclude that GEDI offers great promise for improving global crop type maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tommaso_S/0/1/0/all/0/1\">Stefania Di Tommaso</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Sherrie Wang</a> (1,2 and 3), <a href=\"http://arxiv.org/find/eess/1/au:+Lobell_D/0/1/0/all/0/1\">David B. Lobell</a> (1) ((1) Department of Earth System Science and Center on Food Security and the Environment, Stanford University, (2) Institute for Computational and Mathematical Engineering, Stanford University, (3) Goldman School of Public Policy, University of California, Berkeley)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZFlow: Gated Appearance Flow-based Virtual Try-on with 3D Priors. (arXiv:2109.07001v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07001","description":"<p>Image-based virtual try-on involves synthesizing perceptually convincing\nimages of a model wearing a particular garment and has garnered significant\nresearch interest due to its immense practical applicability. Recent methods\ninvolve a two stage process: i) warping of the garment to align with the model\nii) texture fusion of the warped garment and target model to generate the\ntry-on output. Issues arise due to the non-rigid nature of garments and the\nlack of geometric information about the model or the garment. It often results\nin improper rendering of granular details. We propose ZFlow, an end-to-end\nframework, which seeks to alleviate these concerns regarding geometric and\ntextural integrity (such as pose, depth-ordering, skin and neckline\nreproduction) through a combination of gated aggregation of hierarchical flow\nestimates termed Gated Appearance Flow, and dense structural priors at various\nstage of the network. ZFlow achieves state-of-the-art results as observed\nqualitatively, and on quantitative benchmarks of image quality (PSNR, SSIM, and\nFID). The paper presents extensive comparisons with other existing solutions\nincluding a detailed user study and ablation studies to gauge the effect of\neach of our contributions on multiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ayush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rishabh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemani_M/0/1/0/all/0/1\">Mayur Hemani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeking an Optimal Approach for Computer-Aided Pulmonary Embolism Detection. (arXiv:2109.07029v1 [eess.IV])","link":"http://arxiv.org/abs/2109.07029","description":"<p>Pulmonary embolism (PE) represents a thrombus (\"blood clot\"), usually\noriginating from a lower extremity vein, that travels to the blood vessels in\nthe lung, causing vascular obstruction and in some patients, death. This\ndisorder is commonly diagnosed using CT pulmonary angiography (CTPA). Deep\nlearning holds great promise for the computer-aided CTPA diagnosis (CAD) of PE.\nHowever, numerous competing methods for a given task in the deep learning\nliterature exist, causing great confusion regarding the development of a CAD PE\nsystem. To address this confusion, we present a comprehensive analysis of\ncompeting deep learning methods applicable to PE diagnosis using CTPA at the\nboth image and exam levels. At the image level, we compare convolutional neural\nnetworks (CNNs) with vision transformers, and contrast self-supervised learning\n(SSL) with supervised learning, followed by an evaluation of transfer learning\ncompared with training from scratch. At the exam level, we focus on comparing\nconventional classification (CC) with multiple instance learning (MIL). Our\nextensive experiments consistently show: (1) transfer learning consistently\nboosts performance despite differences between natural images and CT scans, (2)\ntransfer learning with SSL surpasses its supervised counterparts; (3) CNNs\noutperform vision transformers, which otherwise show satisfactory performance;\nand (4) CC is, surprisingly, superior to MIL. Compared with the state of the\nart, our optimal approach provides an AUC gain of 0.2\\% and 1.05\\% for\nimage-level and exam-level, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Islam_N/0/1/0/all/0/1\">Nahid Ul Islam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gehlot_S/0/1/0/all/0/1\">Shiv Gehlot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1\">Zongwei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gotway_M/0/1/0/all/0/1\">Michael B Gotway</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1\">Jianming Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PnP-DETR: Towards Efficient Visual Analysis with Transformers. (arXiv:2109.07036v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07036","description":"<p>Recently, DETR~\\cite{carion2020end} pioneered the solution of vision tasks\nwith transformers, it directly translates the image feature map into the object\ndetection result. Though effective, translating the full feature map can be\ncostly due to redundant computation on some area like the background. In this\nwork, we encapsulate the idea of reducing spatial redundancy into a novel poll\nand pool (PnP) sampling module, with which we build an end-to-end PnP-DETR\narchitecture that adaptively allocates its computation spatially to be more\nefficient. Concretely, the PnP module abstracts the image feature map into fine\nforeground object feature vectors and a small number of coarse background\ncontextual feature vectors. The transformer models information interaction\nwithin the fine-coarse feature space and translates the features into the\ndetection result. Moreover, the PnP-augmented model can instantly achieve\nvarious desired trade-offs between performance and computation with a single\nmodel by varying the sampled feature length, without requiring to train\nmultiple models as existing methods. Thus it offers greater flexibility for\ndeployment in diverse scenarios with varying computation constraint. We further\nvalidate the generalizability of the PnP module on \\textbf{panoptic\nsegmentation} and the recent transformer-based image recognition model\n{\\textbf{ViT}}~\\cite{dosovitskiy2020image} and show consistent efficiency gain.\nWe believe our method makes a step for efficient visual analysis with\ntransformers, wherein spatial redundancy is commonly observed. Code will be\navailable at \\url{https://github.com/twangnh/pnp-detr}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Quantification in Medical Image Segmentation with Multi-decoder U-Net. (arXiv:2109.07045v1 [eess.IV])","link":"http://arxiv.org/abs/2109.07045","description":"<p>Accurate medical image segmentation is crucial for diagnosis and analysis.\nHowever, the models without calibrated uncertainty estimates might lead to\nerrors in downstream analysis and exhibit low levels of robustness. Estimating\nthe uncertainty in the measurement is vital to making definite, informed\nconclusions. Especially, it is difficult to make accurate predictions on\nambiguous areas and focus boundaries for both models and radiologists, even\nharder to reach a consensus with multiple annotations. In this work, the\nuncertainty under these areas is studied, which introduces significant\ninformation with anatomical structure and is as important as segmentation\nperformance. We exploit the medical image segmentation uncertainty\nquantification by measuring segmentation performance with multiple annotations\nin a supervised learning manner and propose a U-Net based architecture with\nmultiple decoders, where the image representation is encoded with the same\nencoder, and segmentation referring to each annotation is estimated with\nmultiple decoders. Nevertheless, a cross-loss function is proposed for bridging\nthe gap between different branches. The proposed architecture is trained in an\nend-to-end manner and able to improve predictive uncertainty estimates. The\nmodel achieves comparable performance with fewer parameters to the integrated\ntraining model that ranked the runner-up in the MICCAI-QUBIQ 2020 challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yanwu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1\">Xutao Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yiwei Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_P/0/1/0/all/0/1\">Pengcheng Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_H/0/1/0/all/0/1\">Haiyan Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_T/0/1/0/all/0/1\">Ting Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Synthesis via Semantic Composition. (arXiv:2109.07053v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07053","description":"<p>In this paper, we present a novel approach to synthesize realistic images\nbased on their semantic layouts. It hypothesizes that for objects with similar\nappearance, they share similar representation. Our method establishes\ndependencies between regions according to their appearance correlation,\nyielding both spatially variant and associated representations. Conditioning on\nthese features, we propose a dynamic weighted network constructed by spatially\nconditional computation (with both convolution and normalization). More than\npreserving semantic distinctions, the given dynamic network strengthens\nsemantic relevance, benefiting global structure and detail synthesis. We\ndemonstrate that our method gives the compelling generation performance\nqualitatively and quantitatively with extensive experiments on benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying-Cong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"F-CAM: Full Resolution CAM via Guided Parametric Upscaling. (arXiv:2109.07069v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07069","description":"<p>Class Activation Mapping (CAM) methods have recently gained much attention\nfor weakly-supervised object localization (WSOL) tasks, allowing for CNN\nvisualization and interpretation without training on fully annotated image\ndatasets. CAM methods are typically integrated within off-the-shelf CNN\nbackbones, such as ResNet50. Due to convolution and downsampling/pooling\noperations, these backbones yield low resolution CAMs with a down-scaling\nfactor of up to 32, making accurate localization more difficult. Interpolation\nis required to restore a full size CAMs, but without considering the\nstatistical properties of the objects, leading to activations with inconsistent\nboundaries and inaccurate localizations. As an alternative, we introduce a\ngeneric method for parametric upscaling of CAMs that allows constructing\naccurate full resolution CAMs (F-CAMs). In particular, we propose a trainable\ndecoding architecture that can be connected to any CNN classifier to produce\nmore accurate CAMs. Given an original (low resolution) CAM, foreground and\nbackground pixels are randomly sampled for fine-tuning the decoder. Additional\npriors such as image statistics, and size constraints are also considered to\nexpand and refine object boundaries. Extensive experiments using three CNN\nbackbones and six WSOL baselines on the CUB-200-2011 and OpenImages datasets,\nindicate that our F-CAM method yields a significant improvement in CAM\nlocalization accuracy. F-CAM performance is competitive with state-of-art WSOL\nmethods, yet it requires fewer computational resources during inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarraf_A/0/1/0/all/0/1\">Aydin Sarraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSOR: A Scalable Statistical Filter for Removing Falling Snow from LiDAR Point Clouds in Severe Winter Weather. (arXiv:2109.07078v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07078","description":"<p>For autonomous vehicles to viably replace human drivers they must contend\nwith inclement weather. Falling rain and snow introduce noise in LiDAR returns\nresulting in both false positive and false negative object detections. In this\narticle we introduce the Winter Adverse Driving dataSet (WADS) collected in the\nsnow belt region of Michigan's Upper Peninsula. WADS is the first multi-modal\ndataset featuring dense point-wise labeled sequential LiDAR scans collected in\nsevere winter weather; weather that would cause an experienced driver to alter\ntheir driving behavior. We have labelled and will make available over 7 GB or\n3.6 billion labelled LiDAR points out of over 26 TB of total LiDAR and camera\ndata collected. We also present the Dynamic Statistical Outlier Removal (DSOR)\nfilter, a statistical PCL-based filter capable or removing snow with a higher\nrecall than the state of the art snow de-noising filter while being 28\\%\nfaster. Further, the DSOR filter is shown to have a lower time complexity\ncompared to the state of the art resulting in an improved scalability.\n</p>\n<p>Our labeled dataset and DSOR filter will be made available at\nhttps://bitbucket.org/autonomymtu/dsor_filter\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurup_A/0/1/0/all/0/1\">Akhil Kurup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bos_J/0/1/0/all/0/1\">Jeremy Bos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Local-Global Transformer for Image Dehazing. (arXiv:2109.07100v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07100","description":"<p>Recently, the Vision Transformer (ViT) has shown impressive performance on\nhigh-level and low-level vision tasks. In this paper, we propose a new ViT\narchitecture, named Hybrid Local-Global Vision Transformer (HyLoG-ViT), for\nsingle image dehazing. The HyLoG-ViT block consists of two paths, the local ViT\npath and the global ViT path, which are used to capture local and global\ndependencies. The hybrid features are fused via convolution layers. As a\nresult, the HyLoG-ViT reduces the computational complexity and introduces\nlocality in the networks. Then, the HyLoG-ViT blocks are incorporated within\nour dehazing networks, which jointly learn the intrinsic image decomposition\nand image dehazing. Specifically, the network consists of one shared encoder\nand three decoders for reflectance prediction, shading prediction, and\nhaze-free image generation. The tasks of reflectance and shading prediction can\nproduce meaningful intermediate features that can serve as complementary\nfeatures for haze-free image generation. To effectively aggregate the\ncomplementary features, we propose a complementary features selection module\n(CFSM) to select the useful ones for image dehazing. Extensive experiments on\nhomogeneous, non-homogeneous, and nighttime dehazing tasks reveal that our\nproposed Transformer-based dehazing network can achieve comparable or even\nbetter performance than CNNs-based dehazing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Long Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anchor DETR: Query Design for Transformer-Based Detector. (arXiv:2109.07107v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07107","description":"<p>In this paper, we propose a novel query design for the transformer-based\ndetectors. In previous transformer-based detectors, the object queries are a\nset of learned embeddings. However, each learned embedding does not have an\nexplicit physical meaning and we can not explain where it will focus on. It is\ndifficult to optimize as the prediction slot of each object query does not have\na specific mode. In other words, each object query will not focus on a specific\nregion. To solved these problems, in our query design, object queries are based\non anchor points, which are widely used in CNN-based detectors. So each object\nquery focus on the objects near the anchor point. Moreover, our query design\ncan predict multiple objects at one position to solve the difficulty: \"one\nregion, multiple objects\". In addition, we design an attention variant, which\ncan reduce the memory cost while achieving similar or better performance than\nthe standard attention in DETR. Thanks to the query design and the attention\nvariant, the proposed detector that we called Anchor DETR, can achieve better\nperformance and run faster than the DETR with 10$\\times$ fewer training epochs.\nFor example, it achieves 44.2 AP with 16 FPS on the MSCOCO dataset when using\nthe ResNet50-DC5 feature for training 50 epochs. Extensive experiments on the\nMSCOCO benchmark prove the effectiveness of the proposed methods. Code is\navailable at https://github.com/megvii-model/AnchorDETR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-based medical image segmentation using Quantum Tensor Networks. (arXiv:2109.07138v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07138","description":"<p>Tensor networks are efficient factorisations of high dimensional tensors into\na network of lower order tensors. They have been most commonly used to model\nentanglement in quantum many-body systems and more recently are witnessing\nincreased applications in supervised machine learning. In this work, we\nformulate image segmentation in a supervised setting with tensor networks. The\nkey idea is to first lift the pixels in image patches to exponentially high\ndimensional feature spaces and using a linear decision hyper-plane to classify\nthe input pixels into foreground and background classes. The high dimensional\nlinear model itself is approximated using the matrix product state (MPS) tensor\nnetwork. The MPS is weight-shared between the non-overlapping image patches\nresulting in our strided tensor network model. The performance of the proposed\nmodel is evaluated on three 2D- and one 3D- biomedical imaging datasets. The\nperformance of the proposed tensor network segmentation model is compared with\nrelevant baseline methods. In the 2D experiments, the tensor network model\nyeilds competitive performance compared to the baseline methods while being\nmore resource efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Selvan_R/0/1/0/all/0/1\">Raghavendra Selvan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dam_E/0/1/0/all/0/1\">Erik B Dam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flensborg_S/0/1/0/all/0/1\">S&#xf8;ren Alexander Flensborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resolution-robust Large Mask Inpainting with Fourier Convolutions. (arXiv:2109.07161v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07161","description":"<p>Modern image inpainting systems, despite the significant progress, often\nstruggle with large missing areas, complex geometric structures, and\nhigh-resolution images. We find that one of the main reasons for that is the\nlack of an effective receptive field in both the inpainting network and the\nloss function. To alleviate this issue, we propose a new method called large\nmask inpainting (LaMa). LaMa is based on i) a new inpainting network\narchitecture that uses fast Fourier convolutions, which have the image-wide\nreceptive field; ii) a high receptive field perceptual loss; and iii) large\ntraining masks, which unlocks the potential of the first two components. Our\ninpainting network improves the state-of-the-art across a range of datasets and\nachieves excellent performance even in challenging scenarios, e.g. completion\nof periodic structures. Our model generalizes surprisingly well to resolutions\nthat are higher than those seen at train time, and achieves this at lower\nparameter&amp;compute costs than the competitive baselines. The code is available\nat https://github.com/saic-mdal/lama.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suvorov_R/0/1/0/all/0/1\">Roman Suvorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logacheva_E/0/1/0/all/0/1\">Elizaveta Logacheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mashikhin_A/0/1/0/all/0/1\">Anton Mashikhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remizova_A/0/1/0/all/0/1\">Anastasia Remizova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1\">Arsenii Ashukha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestrov_A/0/1/0/all/0/1\">Aleksei Silvestrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_N/0/1/0/all/0/1\">Naejin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goka_H/0/1/0/all/0/1\">Harshith Goka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kiwoong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lempitsky_V/0/1/0/all/0/1\">Victor Lempitsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MISSFormer: An Effective Medical Image Segmentation Transformer. (arXiv:2109.07162v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07162","description":"<p>The CNN-based methods have achieved impressive results in medical image\nsegmentation, but it failed to capture the long-range dependencies due to the\ninherent locality of convolution operation. Transformer-based methods are\npopular in vision tasks recently because of its capacity of long-range\ndependencies and get a promising performance. However, it lacks in modeling\nlocal context, although some works attempted to embed convolutional layer to\novercome this problem and achieved some improvement, but it makes the feature\ninconsistent and fails to leverage the natural multi-scale features of\nhierarchical transformer, which limit the performance of models. In this paper,\ntaking medical image segmentation as an example, we present MISSFormer, an\neffective and powerful Medical Image Segmentation tranSFormer. MISSFormer is a\nhierarchical encoder-decoder network and has two appealing designs: 1) A feed\nforward network is redesigned with the proposed Enhanced Transformer Block,\nwhich makes features aligned adaptively and enhances the long-range\ndependencies and local context. 2) We proposed Enhanced Transformer Context\nBridge, a context bridge with the enhanced transformer block to model the\nlong-range dependencies and local context of multi-scale features generated by\nour hierarchical transformer encoder. Driven by these two designs, the\nMISSFormer shows strong capacity to capture more valuable dependencies and\ncontext in medical image segmentation. The experiments on multi-organ and\ncardiac segmentation tasks demonstrate the superiority, effectiveness and\nrobustness of our MISSFormer, the exprimental results of MISSFormer trained\nfrom scratch even outperforms state-of-the-art methods pretrained on ImageNet,\nand the core designs can be generalized to other visual segmentation tasks. The\ncode will be released in Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaohong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhifang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dandan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xueguang Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Annotation Of Arbitrary Objects In The Wild. (arXiv:2109.07165v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07165","description":"<p>Recent years have produced a variety of learning based methods in the context\nof computer vision and robotics. Most of the recently proposed methods are\nbased on deep learning, which require very large amounts of data compared to\ntraditional methods. The performance of the deep learning methods are largely\ndependent on the data distribution they were trained on, and it is important to\nuse data from the robot's actual operating domain during training. Therefore,\nit is not possible to rely on pre-built, generic datasets when deploying robots\nin real environments, creating a need for efficient data collection and\nannotation in the specific operating conditions the robots will operate in. The\nchallenge is then: how do we reduce the cost of obtaining such datasets to a\npoint where we can easily deploy our robots in new conditions, environments and\nto support new sensors? As an answer to this question, we propose a data\nannotation pipeline based on SLAM, 3D reconstruction, and 3D-to-2D geometry.\nThe pipeline allows creating 3D and 2D bounding boxes, along with per-pixel\nannotations of arbitrary objects without needing accurate 3D models of the\nobjects prior to data collection and annotation. Our results showcase almost\n90% Intersection-over-Union (IoU) agreement on both semantic segmentation and\n2D bounding box detection across a variety of objects and scenes, while\nspeeding up the annotation process by several orders of magnitude compared to\ntraditional manual annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blomqvist_K/0/1/0/all/0/1\">Kenneth Blomqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hietala_J/0/1/0/all/0/1\">Julius Hietala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCA: Learning a 3D Full-coverage Vehicle Camouflage for Multi-view Physical Adversarial Attack. (arXiv:2109.07193v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07193","description":"<p>Physical adversarial attacks in object detection have attracted increasing\nattention. However, most previous works focus on hiding the objects from the\ndetector by generating an individual adversarial patch, which only covers the\nplanar part of the vehicle's surface and fails to attack the detector in\nphysical scenarios for multi-view, long-distance and partially occluded\nobjects. To bridge the gap between digital attacks and physical attacks, we\nexploit the full 3D vehicle surface to propose a robust Full-coverage\nCamouflage Attack (FCA) to fool detectors. Specifically, we first try rendering\nthe non-planar camouflage texture over the full vehicle surface. To mimic the\nreal-world environment conditions, we then introduce a transformation function\nto transfer the rendered camouflaged vehicle into a photo-realistic scenario.\nFinally, we design an efficient loss function to optimize the camouflage\ntexture. Experiments show that the full-coverage camouflage attack can not only\noutperform state-of-the-art methods under various test cases but also\ngeneralize to different environments, vehicles, and object detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DonghuaWang/0/1/0/all/0/1\">DonghuaWang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tingsong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jialiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weien Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoqian Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Hard-case Mining across Pyramid Levels in Object Detection. (arXiv:2109.07217v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07217","description":"<p>In object detection, multi-level prediction (e.g., FPN, YOLO) and resampling\nskills (e.g., focal loss, ATSS) have drastically improved one-stage detector\nperformance. However, how to improve the performance by optimizing the feature\npyramid level-by-level remains unexplored. We find that, during training, the\nratio of positive over negative samples varies across pyramid levels\n(\\emph{level imbalance}), which is not addressed by current one-stage\ndetectors. To mediate the influence of level imbalance, we propose a Unified\nMulti-level Optimization Paradigm (UMOP) consisting of two components: 1) an\nindependent classification loss supervising each pyramid level with individual\nresampling considerations; 2) a progressive hard-case mining loss defining all\nlosses across the pyramid levels without extra level-wise settings. With UMOP\nas a plug-and-play scheme, modern one-stage detectors can attain a ~1.5 AP\nimprovement with fewer training iterations and no additional computation\noverhead. Our best model achieves 55.1 AP on COCO test-dev. Code is available\nat https://github.com/zimoqingfeng/UMOP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Binghong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dalu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junde Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haifeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigation-Oriented Scene Understanding for Robotic Autonomy: Learning to Segment Driveability in Egocentric Images. (arXiv:2109.07245v1 [cs.RO])","link":"http://arxiv.org/abs/2109.07245","description":"<p>This work tackles scene understanding for outdoor robotic navigation, solely\nrelying on images captured by an on-board camera. Conventional visual scene\nunderstanding interprets the environment based on specific descriptive\ncategories. However, such a representation is not directly interpretable for\ndecision-making and constrains robot operation to a specific domain. Thus, we\npropose to segment egocentric images directly in terms of how a robot can\nnavigate in them, and tailor the learning problem to an autonomous navigation\ntask. Building around an image segmentation network, we present a generic and\nscalable affordance-based definition consisting of 3 driveability levels which\ncan be applied to arbitrary scenes. By encoding these levels with soft ordinal\nlabels, we incorporate inter-class distances during learning which improves\nsegmentation compared to standard one-hot labelling. In addition, we propose a\nnavigation-oriented pixel-wise loss weighting method which assigns higher\nimportance to safety-critical areas. We evaluate our approach on large-scale\npublic image segmentation datasets spanning off-road and urban scenes. In a\nzero-shot cross-dataset generalization experiment, we show that our affordance\nlearning scheme can be applied across a diverse mix of datasets and improves\ndriveability estimation in unseen environments compared to general-purpose,\nsingle-dataset segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humblot_Renaux_G/0/1/0/all/0/1\">Galadrielle Humblot-Renaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchegiani_L/0/1/0/all/0/1\">Letizia Marchegiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1\">Thomas B. Moeslund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gade_R/0/1/0/all/0/1\">Rikke Gade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGB-D Saliency Detection via Cascaded Mutual Information Minimization. (arXiv:2109.07246v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07246","description":"<p>Existing RGB-D saliency detection models do not explicitly encourage RGB and\ndepth to achieve effective multi-modal learning. In this paper, we introduce a\nnovel multi-stage cascaded learning framework via mutual information\nminimization to \"explicitly\" model the multi-modal information between RGB\nimage and depth data. Specifically, we first map the feature of each mode to a\nlower dimensional feature vector, and adopt mutual information minimization as\na regularizer to reduce the redundancy between appearance features from RGB and\ngeometric features from depth. We then perform multi-stage cascaded learning to\nimpose the mutual information minimization constraint at every stage of the\nnetwork. Extensive experiments on benchmark RGB-D saliency datasets illustrate\nthe effectiveness of our framework. Further, to prosper the development of this\nfield, we contribute the largest (7x larger than NJU2K) dataset, which contains\n15,625 image pairs with high quality\npolygon-/scribble-/object-/instance-/rank-level annotations. Based on these\nrich labels, we additionally construct four new benchmarks with strong\nbaselines and observe some interesting phenomena, which can motivate future\nmodel design. Source code and dataset are available at\n\"https://github.com/JingZhang617/cascaded_rgbd_sod\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Sensing and Communication in Cellular Networks via NR Sidelink. (arXiv:2109.07253v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07253","description":"<p>RF-sensing, the analysis and interpretation of movement or\nenvironment-induced patterns in received electromagnetic signals, has been\nactively investigated for more than a decade. Since electromagnetic signals,\nthrough cellular communication systems, are omnipresent, RF sensing has the\npotential to become a universal sensing mechanism with applications in smart\nhome, retail, localization, gesture recognition, intrusion detection, etc.\nSpecifically, existing cellular network installations might be dual-used for\nboth communication and sensing. Such communications and sensing convergence is\nenvisioned for future communication networks. We propose the use of NR-sidelink\ndirect device-to-device communication to achieve device-initiated,flexible\nsensing capabilities in beyond 5G cellular communication systems. In this\narticle, we specifically investigate a common issue related to sidelink-based\nRF-sensing, which is its angle and rotation dependence. In particular, we\ndiscuss transformations of mmWave point-cloud data which achieve rotational\ninvariance, as well as distributed processing based on such rotational\ninvariant inputs, at angle and distance diverse devices. To process the\ndistributed data, we propose a graph based encoder to capture spatio-temporal\nfeatures of the data and propose four approaches for multi-angle learning. The\napproaches are compared on a newly recorded and openly available dataset\ncomprising 15 subjects, performing 21 gestures which are recorded from 8\nangles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salami_D/0/1/0/all/0/1\">Dariush Salami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasibi_R/0/1/0/all/0/1\">Ramin Hasibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savazzi_S/0/1/0/all/0/1\">Stefano Savazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michoel_T/0/1/0/all/0/1\">Tom Michoel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigg_S/0/1/0/all/0/1\">Stephan Sigg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distract Your Attention: Multi-head Cross Attention Network for Facial Expression Recognition. (arXiv:2109.07270v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07270","description":"<p>We present a novel facial expression recognition network, called Distract\nyour Attention Network (DAN). Our method is based on two key observations.\nFirstly, multiple classes share inherently similar underlying facial\nappearance, and their differences could be subtle. Secondly, facial expressions\nexhibit themselves through multiple facial regions simultaneously, and the\nrecognition requires a holistic approach by encoding high-order interactions\namong local features. To address these issues, we propose our DAN with three\nkey components: Feature Clustering Network (FCN), Multi-head cross Attention\nNetwork (MAN), and Attention Fusion Network (AFN). The FCN extracts robust\nfeatures by adopting a large-margin learning objective to maximize class\nseparability. In addition, the MAN instantiates a number of attention heads to\nsimultaneously attend to multiple facial areas and build attention maps on\nthese regions. Further, the AFN distracts these attentions to multiple\nlocations before fusing the attention maps to a comprehensive one. Extensive\nexperiments on three public datasets (including AffectNet, RAF-DB, and SFEW\n2.0) verified that the proposed method consistently achieves state-of-the-art\nfacial expression recognition performance. Code will be made available at\nhttps://github.com/yaoing/DAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhengyao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wenzhong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Ge Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Perspective on Progressive GANs Distillationfor One-class Novelty Detection. (arXiv:2109.07295v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07295","description":"<p>One-class novelty detection is conducted to iden-tify anomalous instances,\nwith different distributions from theexpected normal instances. In this paper,\nthe Generative Adver-sarial Network based on the Encoder-Decoder-Encoder\nscheme(EDE-GAN) achieves state-of-the-art performance. The two fac-tors bellow\nserve the above purpose: 1) The EDE-GAN calculatesthe distance between two\nlatent vectors as the anomaly score,which is unlike the previous methods by\nutilizing the reconstruc-tion error between images. 2) The model obtains best\nresultswhen the batch size is set to 1. To illustrate their superiority,we\ndesign a new GAN architecture, and compareperformances according to different\nbatch sizes. Moreover, withexperimentation leads to discovery, our result\nimplies there is alsoevidence of just how beneficial constraint on the latent\nspace arewhen engaging in model training.In an attempt to learn compact and\nfast models, we present anew technology, Progressive Knowledge Distillation\nwith GANs(P-KDGAN), which connects two standard GANs through thedesigned\ndistillation loss. Two-step progressive learning continu-ously augments the\nperformance of student GANs with improvedresults over single-step approach. Our\nexperimental results onCIFAR-10, MNIST, and FMNIST datasets illustrate that\nP-KDGAN improves the performance of the student GAN by2.44%, 1.77%, and 1.73%\nwhen compressing the computationat ratios of 24.45:1, 311.11:1, and 700:1,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hanyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FFAVOD: Feature Fusion Architecture for Video Object Detection. (arXiv:2109.07298v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07298","description":"<p>A significant amount of redundancy exists between consecutive frames of a\nvideo. Object detectors typically produce detections for one image at a time,\nwithout any capabilities for taking advantage of this redundancy. Meanwhile,\nmany applications for object detection work with videos, including intelligent\ntransportation systems, advanced driver assistance systems and video\nsurveillance. Our work aims at taking advantage of the similarity between video\nframes to produce better detections. We propose FFAVOD, standing for feature\nfusion architecture for video object detection. We first introduce a novel\nvideo object detection architecture that allows a network to share feature maps\nbetween nearby frames. Second, we propose a feature fusion module that learns\nto merge feature maps to enhance them. We show that using the proposed\narchitecture and the fusion module can improve the performance of three base\nobject detectors on two object detection benchmarks containing sequences of\nmoving road users. Additionally, to further increase performance, we propose an\nimprovement to the SpotNet attention module. Using our architecture on the\nimproved SpotNet detector, we obtain the state-of-the-art performance on the\nUA-DETRAC public benchmark as well as on the UAVDT dataset. Code is available\nat https://github.com/hu64/FFAVOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perreault_H/0/1/0/all/0/1\">Hughes Perreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1\">Guillaume-Alexandre Bilodeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1\">Nicolas Saunier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heritier_M/0/1/0/all/0/1\">Maguelonne H&#xe9;ritier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MD-CSDNetwork: Multi-Domain Cross Stitched Network for Deepfake Detection. (arXiv:2109.07311v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07311","description":"<p>The rapid progress in the ease of creating and spreading ultra-realistic\nmedia over social platforms calls for an urgent need to develop a generalizable\ndeepfake detection technique. It has been observed that current deepfake\ngeneration methods leave discriminative artifacts in the frequency spectrum of\nfake images and videos. Inspired by this observation, in this paper, we present\na novel approach, termed as MD-CSDNetwork, for combining the features in the\nspatial and frequency domains to mine a shared discriminative representation\nfor classifying \\textit{deepfakes}. MD-CSDNetwork is a novel cross-stitched\nnetwork with two parallel branches carrying the spatial and frequency\ninformation, respectively. We hypothesize that these multi-domain input data\nstreams can be considered as related supervisory signals. The supervision from\nboth branches ensures better performance and generalization. Further, the\nconcept of cross-stitch connections is utilized where they are inserted between\nthe two branches to learn an optimal combination of domain-specific and shared\nrepresentations from other domains automatically. Extensive experiments are\nconducted on the popular benchmark dataset namely FaceForeniscs++ for forgery\nclassification. We report improvements over all the manipulation types in\nFaceForensics++ dataset and comparable results with state-of-the-art methods\nfor cross-database evaluation on the Celeb-DF dataset and the Deepfake\nDetection Dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aayushi Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Akshay Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Sayan Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatsa_M/0/1/0/all/0/1\">Mayank Vatsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Richa Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeFungi: Direct Mycological Examination of Microscopic Fungi Images. (arXiv:2109.07322v1 [eess.IV])","link":"http://arxiv.org/abs/2109.07322","description":"<p>Traditionally, diagnosis and treatment of fungal infections in humans depend\nheavily on face-to-face consultations or examinations made by specialized\nlaboratory scientists known as mycologists. In many cases, such as the recent\nmucormycosis spread in the COVID-19 pandemic, an initial treatment can be\nsafely suggested to the patient during the earliest stage of the mycological\ndiagnostic process by performing a direct examination of biopsies or samples\nthrough a microscope. Computer-aided diagnosis systems using deep learning\nmodels have been trained and used for the late mycological diagnostic stages.\nHowever, there are no reference literature works made for the early stages. A\nmycological laboratory in Colombia donated the images used for the development\nof this research work. They were manually labelled into five classes and\ncurated with a subject matter expert assistance. The images were later cropped\nand patched with automated code routines to produce the final dataset. This\npaper presents experimental results classifying five fungi types using two\ndifferent deep learning approaches and three different convolutional neural\nnetwork models, VGG16, Inception V3, and ResNet50. The first approach\nbenchmarks the classification performance for the models trained from scratch,\nwhile the second approach benchmarks the classification performance using\npre-trained models based on the ImageNet dataset. Using k-fold cross-validation\ntesting on the 5-class dataset, the best performing model trained from scratch\nwas Inception V3, reporting 73.2% accuracy. Also, the best performing model\nusing transfer learning was VGG16 reporting 85.04%. The statistics provided by\nthe two approaches create an initial point of reference to encourage future\nresearch works to improve classification performance. Furthermore, the dataset\nbuilt is published in Kaggle and GitHub to foster future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sopo_C/0/1/0/all/0/1\">Camilo Javier Pineda Sopo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hajati_F/0/1/0/all/0/1\">Farshid Hajati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gheisari_S/0/1/0/all/0/1\">Soheila Gheisari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointManifoldCut: Point-wise Augmentation in the Manifold for Point Clouds. (arXiv:2109.07324v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07324","description":"<p>Augmentation can benefit point cloud learning due to the limited availability\nof large-scale public datasets. This paper proposes a mix-up augmentation\napproach, PointManifoldCut, which replaces the neural network embedded points,\nrather than the Euclidean space coordinates. This approach takes the advantage\nthat points at the higher levels of the neural network are already trained to\nembed its neighbors relations and mixing these representation will not mingle\nthe relation between itself and its label. This allows to regularize the\nparameter space as the other augmentation methods but without worrying about\nthe proper label of the replaced points. The experiments show that our proposed\napproach provides a competitive performance on point cloud classification and\nsegmentation when it is combined with the cutting-edge vanilla point cloud\nnetworks. The result shows a consistent performance boosting compared to other\nstate-of-the-art point cloud augmentation method, such as PointMixup and\nPointCutMix. The code of this paper is available at:\nhttps://github.com/fun0515/PointManifoldCut.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tianfang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Anan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S3LAM: Structured Scene SLAM. (arXiv:2109.07339v1 [cs.RO])","link":"http://arxiv.org/abs/2109.07339","description":"<p>We propose a new general SLAM system that uses the semantic segmentation of\nobjects and structures in the scene. Semantic information is relevant as it\ncontains high level information which may make SLAM more accurate and robust.\nOur contribution is threefold: i) A new SLAM system based on ORB-SLAM2 that\ncreates a semantic map made of clusters of points corresponding to objects\ninstances and structures in the scene. ii) A modification of the classical\nBundle Adjustment formulation to constrain each cluster using geometrical\npriors, which improves both camera localization and reconstruction and enables\na better understanding of the scene. iii) A new Bundle Adjustment formulation\nat the level of clusters to improve the convergence of classical Bundle\nAdjustment. We evaluate our approach on several sequences from a public dataset\nand show that, with respect to ORB-SLAM2 it improves camera pose estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1\">Mathieu Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchand_E/0/1/0/all/0/1\">Eric Marchand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kacete_A/0/1/0/all/0/1\">Amine Kacete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Royan_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Royan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dynamical Human-Joint Affinity for 3D Pose Estimation in Videos. (arXiv:2109.07353v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07353","description":"<p>Graph Convolution Network (GCN) has been successfully used for 3D human pose\nestimation in videos. However, it is often built on the fixed human-joint\naffinity, according to human skeleton. This may reduce adaptation capacity of\nGCN to tackle complex spatio-temporal pose variations in videos. To alleviate\nthis problem, we propose a novel Dynamical Graph Network (DG-Net), which can\ndynamically identify human-joint affinity, and estimate 3D pose by adaptively\nlearning spatial/temporal joint relations from videos. Different from\ntraditional graph convolution, we introduce Dynamical Spatial/Temporal Graph\nconvolution (DSG/DTG) to discover spatial/temporal human-joint affinity for\neach video exemplar, depending on spatial distance/temporal movement similarity\nbetween human joints in this video. Hence, they can effectively understand\nwhich joints are spatially closer and/or have consistent motion, for reducing\ndepth ambiguity and/or motion uncertainty when lifting 2D pose to 3D pose. We\nconduct extensive experiments on three popular benchmarks, e.g., Human3.6M,\nHumanEva-I, and MPI-INF-3DHP, where DG-Net outperforms a number of recent SOTA\napproaches with fewer input frames and model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_T/0/1/0/all/0/1\">Tianyu Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct and Sparse Deformable Tracking. (arXiv:2109.07370v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07370","description":"<p>Deformable Monocular SLAM algorithms recover the localization of a camera in\nan unknown deformable environment. Current approaches use a template-based\ndeformable tracking to recover the camera pose and the deformation of the map.\nThese template-based methods use an underlying global deformation model. In\nthis paper, we introduce a novel deformable camera tracking method with a local\ndeformation model for each point. Each map point is defined as a single\ntextured surfel that moves independently of the other map points. Thanks to a\ndirect photometric error cost function, we can track the position and\norientation of the surfel without an explicit global deformation model. In our\nexperiments, we validate the proposed system and observe that our local\ndeformation model estimates more accurately and robustly the targeted\ndeformations of the map in both laboratory-controlled experiments and in-body\nscenarios undergoing non-isometric deformations, with changing topology or\ndiscontinuities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lamarca_J/0/1/0/all/0/1\">Jose Lamarca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1\">Juan J. Gomez Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tardos_J/0/1/0/all/0/1\">Juan D. Tardos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montiel_J/0/1/0/all/0/1\">J.M.M. Montiel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Framework for Biphasic Facial Age Translation with Noisy-Semantic Guided Generative Adversarial Networks. (arXiv:2109.07373v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07373","description":"<p>Biphasic facial age translation aims at predicting the appearance of the\ninput face at any age. Facial age translation has received considerable\nresearch attention in the last decade due to its practical value in cross-age\nface recognition and various entertainment applications. However, most existing\nmethods model age changes between holistic images, regardless of the human face\nstructure and the age-changing patterns of individual facial components.\nConsequently, the lack of semantic supervision will cause infidelity of\ngenerated faces in detail. To this end, we propose a unified framework for\nbiphasic facial age translation with noisy-semantic guided generative\nadversarial networks. Structurally, we project the class-aware noisy semantic\nlayouts to soft latent maps for the following injection operation on the\nindividual facial parts. In particular, we introduce two sub-networks,\nProjectionNet and ConstraintNet. ProjectionNet introduces the low-level\nstructural semantic information with noise map and produces soft latent maps.\nConstraintNet disentangles the high-level spatial features to constrain the\nsoft latent maps, which endows more age-related context into the soft latent\nmaps. Specifically, attention mechanism is employed in ConstraintNet for\nfeature disentanglement. Meanwhile, in order to mine the strongest mapping\nability of the network, we embed two types of learning strategies in the\ntraining procedure, supervised self-driven generation and unsupervised\ncondition-driven cycle-consistent generation. As a result, extensive\nexperiments conducted on MORPH and CACD datasets demonstrate the prominent\nability of our proposed method which achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation. (arXiv:2109.07407v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07407","description":"<p>The success of deep learning methods in medical image segmentation tasks\nheavily depends on a large amount of labeled data to supervise the training. On\nthe other hand, the annotation of biomedical images requires domain knowledge\nand can be laborious. Recently, contrastive learning has demonstrated great\npotential in learning latent representation of images even without any label.\nExisting works have explored its application to biomedical image segmentation\nwhere only a small portion of data is labeled, through a pre-training phase\nbased on self-supervised contrastive learning without using any labels followed\nby a supervised fine-tuning phase on the labeled portion of data only. In this\npaper, we establish that by including the limited label in formation in the\npre-training phase, it is possible to boost the performance of contrastive\nlearning. We propose a supervised local contrastive loss that leverages limited\npixel-wise annotation to force pixels with the same label to gather around in\nthe embedding space. Such loss needs pixel-wise computation which can be\nexpensive for large images, and we further propose two strategies, downsampling\nand block division, to address the issue. We evaluate our methods on two public\nbiomedical image datasets of different modalities. With different amounts of\nlabeled data, our methods consistently outperform the state-of-the-art\ncontrast-based methods and other semi-supervised learning techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinrong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dewen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Wide-area, Low-latency, and Power-efficient 6-DoF Pose Tracking System for Rigid Objects. (arXiv:2109.07428v1 [cs.RO])","link":"http://arxiv.org/abs/2109.07428","description":"<p>Position sensitive detectors (PSDs) offer possibility to track single active\nmarker's two (or three) degrees of freedom (DoF) position with a high accuracy,\nwhile having a fast response time with high update frequency and low latency,\nall using a very simple signal processing circuit. However they are not\nparticularly suitable for 6-DoF object pose tracking system due to lack of\norientation measurement, limited tracking range, and sensitivity to\nenvironmental variation. We propose a novel 6-DoF pose tracking system for a\nrigid object tracking requiring a single active marker. The proposed system\nuses a stereo-based PSD pair and multiple Inertial Measurement Units (IMUs).\nThis is done based on a practical approach to identify and control the power of\nInfrared-Light Emitting Diode (IR-LED) active markers, with an aim to increase\nthe tracking work space and reduce the power consumption. Our proposed tracking\nsystem is validated with three different work space sizes and for static and\ndynamic positional accuracy using robotic arm manipulator with three different\ndynamic motion patterns. The results show that the static position\nroot-mean-square (RMS) error is 0.6mm. The dynamic position RMS error is\n0.7-0.9mm. The orientation RMS error is between 0.04 and 0.9 degree at varied\ndynamic motion. Overall, our proposed tracking system is capable of tracking a\nrigid object pose with sub-millimeter accuracy at the mid range of the work\nspace and sub-degree accuracy for all work space under a lab setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Ho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1\">Ankur Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansi_T/0/1/0/all/0/1\">Tommaso Mansi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamen_A/0/1/0/all/0/1\">Ali Kamen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contact-Aware Retargeting of Skinned Motion. (arXiv:2109.07431v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07431","description":"<p>This paper introduces a motion retargeting method that preserves\nself-contacts and prevents interpenetration. Self-contacts, such as when hands\ntouch each other or the torso or the head, are important attributes of human\nbody language and dynamics, yet existing methods do not model or preserve these\ncontacts. Likewise, interpenetration, such as a hand passing into the torso,\nare a typical artifact of motion estimation methods. The input to our method is\na human motion sequence and a target skeleton and character geometry. The\nmethod identifies self-contacts and ground contacts in the input motion, and\noptimizes the motion to apply to the output skeleton, while preserving these\ncontacts and reducing interpenetration. We introduce a novel\ngeometry-conditioned recurrent network with an encoder-space optimization\nstrategy that achieves efficient retargeting while satisfying contact\nconstraints. In experiments, our results quantitatively outperform previous\nmethods and we conduct a user study where our retargeted motions are rated as\nhigher-quality than those produced by recent works. We also show our method\ngeneralizes to motion estimated from human videos where we improve over\nprevious works that produce noticeable interpenetration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1\">Ruben Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1\">Duygu Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertzmann_A/0/1/0/all/0/1\">Aaron Hertzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1\">Jun Saito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering. (arXiv:2109.07448v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07448","description":"<p>In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary\nhuman performance using sparse multi-view cameras. Recently, several works have\naddressed this problem by learning person-specific neural radiance fields\n(NeRF) to capture the appearance of a particular human. In parallel, some work\nproposed to use pixel-aligned features to generalize radiance fields to\narbitrary new scenes and objects. Adopting such generalization approaches to\nhumans, however, is highly challenging due to the heavy occlusions and dynamic\narticulations of body parts. To tackle this, we propose Neural Human Performer,\na novel approach that learns generalizable neural radiance fields based on a\nparametric human body model for robust performance capture. Specifically, we\nfirst introduce a temporal transformer that aggregates tracked visual features\nbased on the skeletal body motion over time. Moreover, a multi-view transformer\nis proposed to perform cross-attention between the temporally-fused features\nand the pixel-aligned features at each time step to integrate observations on\nthe fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets\nshow that our method significantly outperforms recent generalizable NeRF\nmethods on unseen identities and poses. The video results and code are\navailable at https://youngjoongunc.github.io/nhp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Youngjoong Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1\">Duygu Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuchs_H/0/1/0/all/0/1\">Henry Fuchs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Bregman Divergence for Contrastive Learning of Visual Representations. (arXiv:2109.07455v1 [cs.CV])","link":"http://arxiv.org/abs/2109.07455","description":"<p>Deep Bregman divergence measures divergence of data points using neural\nnetworks which is beyond Euclidean distance and capable of capturing divergence\nover distributions. In this paper, we propose deep Bregman divergences for\ncontrastive learning of visual representation and we aim to enhance contrastive\nloss used in self-supervised learning by training additional networks based on\nfunctional Bregman divergence. In contrast to the conventional contrastive\nlearning methods which are solely based on divergences between single points,\nour framework can capture the divergence between distributions which improves\nthe quality of learned representation. By combining conventional contrastive\nloss with the proposed divergence loss, our method outperforms baseline and\nmost of previous methods for self-supervised and semi-supervised learning on\nmultiple classifications and object detection tasks and datasets. The source\ncode of the method and of all the experiments are available at supplementary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_M/0/1/0/all/0/1\">Mina Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_F/0/1/0/all/0/1\">Farzin Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizi_S/0/1/0/all/0/1\">Shekoofeh Azizi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation. (arXiv:1805.02798v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1805.02798","description":"<p>Simultaneous segmentation of multiple organs from different medical imaging\nmodalities is a crucial task as it can be utilized for computer-aided\ndiagnosis, computer-assisted surgery, and therapy planning. Thanks to the\nrecent advances in deep learning, several deep neural networks for medical\nimage segmentation have been introduced successfully for this purpose. In this\npaper, we focus on learning a deep multi-organ segmentation network that labels\nvoxels. In particular, we examine the critical choice of a loss function in\norder to handle the notorious imbalance problem that plagues both the input and\noutput of a learning model. The input imbalance refers to the class-imbalance\nin the input training samples (i.e., small foreground objects embedded in an\nabundance of background voxels, as well as organs of varying sizes). The output\nimbalance refers to the imbalance between the false positives and false\nnegatives of the inference model. In order to tackle both types of imbalance\nduring training and inference, we introduce a new curriculum learning based\nloss function. Specifically, we leverage Dice similarity coefficient to deter\nmodel parameters from being held at bad local minima and at the same time\ngradually learn better model parameters by penalizing for false\npositives/negatives using a cross entropy term. We evaluated the proposed loss\nfunction on three datasets: whole body positron emission tomography (PET) scans\nwith 5 target organs, magnetic resonance imaging (MRI) prostate scans, and\nultrasound echocardigraphy images with a single target organ i.e., left\nventricular. We show that a simple network architecture with the proposed\nintegrative loss function can outperform state-of-the-art methods and results\nof the competing methods can be improved when our proposed loss is used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taghanaki_S/0/1/0/all/0/1\">Saeid Asgari Taghanaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_B/0/1/0/all/0/1\">Bogdan Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Puneet Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comaniciu_D/0/1/0/all/0/1\">Dorin Comaniciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAPnet: A Double Self-attention Convolutional Network for Point Cloud Semantic Labeling. (arXiv:2004.08596v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.08596","description":"<p>Airborne Laser Scanning (ALS) point clouds have complex structures, and their\n3D semantic labeling has been a challenging task. It has three problems: (1)\nthe difficulty of classifying point clouds around boundaries of objects from\ndifferent classes, (2) the diversity of shapes within the same class, and (3)\nthe scale differences between classes. In this study, we propose a novel double\nself-attention convolutional network called the DAPnet. The double\nself-attention includes the point attention module (PAM) and the group\nattention module (GAM). For problem (1), the PAM can effectively assign\ndifferent weights based on the relevance of point clouds in adjacent areas.\nMeanwhile, for the problem (2), the GAM enhances the correlation between\ngroups, i.e., grouped features within the same classes. To solve the problem\n(3), we adopt a multiscale radius to construct the groups and concatenate\nextracted hierarchical features with the output of the corresponding upsampling\nprocess. Under the ISPRS 3D Semantic Labeling Contest dataset, the DAPnet\noutperforms the benchmark by 85.2\\% with an overall accuracy of 90.7\\%. By\nconducting ablation comparisons, we find that the PAM effectively improves the\nmodel than the GAM. The incorporation of the double self-attention module has\nan average of 7\\% improvement on the pre-class accuracy. Plus, the DAPnet\nconsumes a similar training time to those without the attention modules for\nmodel convergence. The DAPnet can assign different weights to features based on\nthe relevance between point clouds and their neighbors, which effectively\nimproves classification performance. The source codes are available at:\nhttps://github.com/RayleighChen/point-attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zewei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yongjian Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haozhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haifeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Detection of Aedes aegypti Breeding Grounds Based on Deep Networks with Spatio-Temporal Consistency. (arXiv:2007.14863v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.14863","description":"<p>Every year, the Aedes aegypti mosquito infects millions of people with\ndiseases such as dengue, zika, chikungunya, and urban yellow fever. The main\nform to combat these diseases is to avoid mosquito reproduction by searching\nfor and eliminating the potential mosquito breeding grounds. In this work, we\nintroduce a comprehensive dataset of aerial videos, acquired with an unmanned\naerial vehicle, containing possible mosquito breeding sites. All frames of the\nvideo dataset were manually annotated with bounding boxes identifying all\nobjects of interest. This dataset was employed to develop an automatic\ndetection system of such objects based on deep convolutional networks. We\npropose the exploitation of the temporal information contained in the videos by\nthe incorporation, in the object detection pipeline, of a spatio-temporal\nconsistency module that can register the detected objects, minimizing most\nfalse-positive and false-negative occurrences. Using the ResNet-50-FPN as a\nbackbone, we achieve F$_1$-scores of 0.65 and 0.77 on the object-level\ndetection of `tires' and `water tanks', respectively, illustrating the system\ncapabilities to properly locate potential mosquito breeding objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passos_W/0/1/0/all/0/1\">Wesley L. Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_G/0/1/0/all/0/1\">Gabriel M. Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lima_A/0/1/0/all/0/1\">Amaro A. de Lima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netto_S/0/1/0/all/0/1\">Sergio L. Netto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_E/0/1/0/all/0/1\">Eduardo A. B. da Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Quantitative Approach for Optimizing Convolutional Neural Networks. (arXiv:2009.05236v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.05236","description":"<p>With the increasing popularity of deep learning, Convolutional Neural\nNetworks (CNNs) have been widely applied in various domains, such as image\nclassification and object detection, and achieve stunning success in terms of\ntheir high accuracy over the traditional statistical methods. To exploit the\npotential of CNN models, a huge amount of research and industry efforts have\nbeen devoted to optimizing CNNs. Among these endeavors, CNN architecture design\nhas attracted tremendous attention because of its great potential of improving\nmodel accuracy or reducing model complexity. However, existing work either\nintroduces repeated training overhead in the search process or lacks an\ninterpretable metric to guide the design. To clear these hurdles, we propose\n3D-Receptive Field (3DRF), an explainable and easy-to-compute metric, to\nestimate the quality of a CNN architecture and guide the search process of\ndesigns. To validate the effectiveness of 3DRF, we build a static optimizer to\nimprove the CNN architectures at both the stage level and the kernel level. Our\noptimizer not only provides a clear and reproducible procedure but also\nmitigates unnecessary training efforts in the architecture search process.\nExtensive experiments and studies show that the models generated by our\noptimizer can achieve up to 5.47% accuracy improvement and up to 65.38%\nparameters deduction, compared with state-of-the-art CNN structures like\nMobileNet and ResNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Boyuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xueqiao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yufei Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Likelihood-Based Diverse Sampling for Trajectory Forecasting. (arXiv:2011.15084v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.15084","description":"<p>Forecasting complex vehicle and pedestrian multi-modal distributions requires\npowerful probabilistic approaches. Normalizing flows (NF) have recently emerged\nas an attractive tool to model such distributions. However, a key drawback is\nthat independent samples drawn from a flow model often do not adequately\ncapture all the modes in the underlying distribution. We propose\nLikelihood-Based Diverse Sampling (LDS), a method for improving the quality and\nthe diversity of trajectory samples from a pre-trained flow model. Rather than\nproducing individual samples, LDS produces a set of trajectories in one shot.\nGiven a pre-trained forecasting flow model, we train LDS using gradients from\nthe model, to optimize an objective function that rewards high likelihood for\nindividual trajectories in the predicted set, together with high spatial\nseparation among trajectories. LDS outperforms state-of-art post-hoc neural\ndiverse forecasting methods for various pre-trained flow models as well as\nconditional variational autoencoder (CVAE) models. Crucially, it can also be\nused for transductive trajectory forecasting, where the diverse forecasts are\ntrained on-the-fly on unlabeled test examples. LDS is easy to implement, and we\nshow that it offers a simple plug-in improvement over baselines on two\nchallenging benchmarks. Code is at: https://github.com/JasonMa2016/LDS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yecheng Jason Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inala_J/0/1/0/all/0/1\">Jeevana Priya Inala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1\">Dinesh Jayaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1\">Osbert Bastani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Contrastive Learning Robust to Shortcuts. (arXiv:2012.09962v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.09962","description":"<p>Contrastive learning is one of the fastest growing research areas in machine\nlearning due to its ability to learn useful representations without labeled\ndata. However, contrastive learning is susceptible to shortcuts - i.e., it may\nlearn shortcut features irrelevant to the task of interest, and discard\nrelevant information. Past work has addressed this limitation via handcrafted\ndata augmentations that eliminate the shortcut. But, manually crafted\naugmentations do not work across all datasets and tasks. Further, data\naugmentations fail in addressing shortcuts in multi-attribute classification\nwhen one attribute acts as a shortcut around other attributes. In this paper,\nwe analyze the objective function of contrastive learning and formally prove\nthat it is vulnerable to shortcuts. We then present reconstructive contrastive\nlearning (RCL), a framework for learning unsupervised representations that are\nrobust to shortcuts. The key idea is to force the learned representation to\nreconstruct the input, which naturally counters potential shortcuts. Extensive\nexperiments verify that RCL is highly robust to shortcuts and outperforms\nstate-of-the-art contrastive learning methods on a variety of datasets and\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lijie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1\">Piotr Indyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1\">Dina Katabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Body Composition Analysis with Deep Regression Ensembles on UK Biobank MRI. (arXiv:2101.06963v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.06963","description":"<p>Along with rich health-related metadata, medical images have been acquired\nfor over 40,000 male and female UK Biobank participants, aged 44-82, since\n2014. Phenotypes derived from these images, such as measurements of body\ncomposition from MRI, can reveal new links between genetics, cardiovascular\ndisease, and metabolic conditions. In this work, six measurements of body\ncomposition and adipose tissues were automatically estimated by image-based,\ndeep regression with ResNet50 neural networks from neck-to-knee body MRI.\nDespite the potential for high speed and accuracy, these networks produce no\noutput segmentations that could indicate the reliability of individual\nmeasurements. The presented experiments therefore examine uncertainty\nquantification with mean-variance regression and ensembling to estimate\nindividual measurement errors and thereby identify potential outliers,\nanomalies, and other failure cases automatically. In 10-fold cross-validation\non data of about 8,500 subjects, mean-variance regression and ensembling showed\ncomplementary benefits, reducing the mean absolute error across all predictions\nby 12%. Both improved the calibration of uncertainties and their ability to\nidentify high prediction errors. With intra-class correlation coefficients\n(ICC) above 0.97, all targets except the liver fat content yielded relative\nmeasurement errors below 5%. Testing on another 1,000 subjects showed\nconsistent performance, and the method was finally deployed for inference to\n30,000 subjects with missing reference values. The results indicate that deep\nregression ensembles could ultimately provide automated, uncertainty-aware\nmeasurements of body composition for more than 120,000 UK Biobank neck-to-knee\nbody MRI that are to be acquired within the coming years.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Langner_T/0/1/0/all/0/1\">Taro Langner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gustafsson_F/0/1/0/all/0/1\">Fredrik K. Gustafsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Avelin_B/0/1/0/all/0/1\">Benny Avelin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strand_R/0/1/0/all/0/1\">Robin Strand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahlstrom_H/0/1/0/all/0/1\">H&#xe5;kan Ahlstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kullberg_J/0/1/0/all/0/1\">Joel Kullberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGRNet: Adaptive Graph Representation Learning and Reasoning for Face Parsing. (arXiv:2101.07034v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07034","description":"<p>Face parsing infers a pixel-wise label to each facial component, which has\ndrawn much attention recently.Previous methods have shown their success in face\nparsing, which however overlook the correlation among facial components.As a\nmatter of fact, the component-wise relationship is a critical clue in\ndiscriminating ambiguous pixels in facial area.To address this issue, we\npropose adaptive graph representation learning and reasoning over facial\ncomponents, aiming to learn representative vertices that describe each\ncomponent, exploit the component-wise relationship and thereby produce accurate\nparsing results against ambiguity. In particular, we devise an adaptive and\ndifferentiable graph abstraction method to represent the components on a graph\nvia pixel-to-vertex projection under the initial condition of a predicted\nparsing map, where pixel features within a certain facial region are aggregated\nonto a vertex. Further, we explicitly incorporate the image edge as a prior in\nthe model, which helps to discriminate edge and non-edge pixels during the\nprojection, thus leading to refined parsing results along the edges.Then, our\nmodel learns and reasons over the relations among components by propagating\ninformation across vertices on the graph. Finally, the refined vertex features\nare projected back to pixel grids for the prediction of the final parsing\nmap.To train our model, we propose a discriminative loss to penalize small\ndistances between vertices in the feature space, which leads to distinct\nvertices with strong semantics. Experimental results show the superior\nperformance of the proposed model on multiple face parsing datasets, along with\nthe validation on the human parsing task to demonstrate the generalizability of\nour model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Te_G/0/1/0/all/0/1\">Gusi Te</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinglu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hailin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Object Detection via Integrity Learning. (arXiv:2101.07663v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07663","description":"<p>Albeit current salient object detection (SOD) works have achieved fantastic\nprogress, they are cast into the shade when it comes to the integrity of the\npredicted salient regions. We define the concept of integrity at both the micro\nand macro level. Specifically, at the micro level, the model should highlight\nall parts that belong to a certain salient object, while at the macro level,\nthe model needs to discover all salient objects from the given image scene. To\nfacilitate integrity learning for salient object detection, we design a novel\nIntegrity Cognition Network (ICON), which explores three important components\nto learn strong integrity features. 1) Unlike the existing models that focus\nmore on feature discriminability, we introduce a diverse feature aggregation\n(DFA) component to aggregate features with various receptive fields (i.e.,,\nkernel shape and context) and increase the feature diversity. Such diversity is\nthe foundation for mining the integral salient objects. 2) Based on the DFA\nfeatures, we introduce the integrity channel enhancement (ICE) component with\nthe goal of enhancing feature channels that highlight the integral salient\nobjects at the macro level, while suppressing the other distracting ones. 3)\nAfter extracting the enhanced features, the part-whole verification (PWV)\nmethod is employed to determine whether the part and whole object features have\nstrong agreement. Such part-whole agreements can further improve the\nmicro-level integrity for each salient object. To demonstrate the effectiveness\nof ICON, comprehensive experiments are conducted on seven challenging\nbenchmarks, where promising results are achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuge_M/0/1/0/all/0/1\">Mingchen Zhuge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solid Texture Synthesis using Generative Adversarial Networks. (arXiv:2102.03973v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03973","description":"<p>Solid texture synthesis (STS), as an effective way to extend 2D exemplar to a\n3D solid volume, exhibits advantages in numerous application domains. However,\nexisting methods generally synthesize solid texture with specific features,\nwhich may result in the failure of capturing diversified textural information.\nIn this paper, we propose a novel generative adversarial nets-based approach\n(STS-GAN) to hierarchically learn solid texture with a feature-free nature. Our\nmulti-scale discriminators evaluate the similarity between patch from exemplar\nand slice from the generated volume, promoting the generator to synthesize\nrealistic solid textures. Experimental results demonstrate that the proposed\nmethod can generate high-quality solid textures with similar visual\ncharacteristics to the exemplar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jifeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fanqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Junteng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct Estimation of Appearance Models for Segmentation. (arXiv:2102.11121v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.11121","description":"<p>Image segmentation algorithms often depend on appearance models that\ncharacterize the distribution of pixel values in different image regions. We\ndescribe a new approach for estimating appearance models directly from an\nimage, without explicit consideration of the pixels that make up each region.\nOur approach is based on novel algebraic expressions that relate local image\nstatistics to the appearance of spatially coherent regions. We describe two\nalgorithms that can use the aforementioned algebraic expressions to estimate\nappearance models directly from an image. The first algorithm solves a system\nof linear and quadratic equations using a least squares formulation. The second\nalgorithm is a spectral method based on an eigenvector computation. We present\nexperimental results that demonstrate the proposed methods work well in\npractice and lead to effective image segmentation algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1\">Jeova F. S. Rocha Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felzenszwalb_P/0/1/0/all/0/1\">Pedro Felzenszwalb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_M/0/1/0/all/0/1\">Marilyn Vazquez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Land Cover Mapping in Limited Labels Scenario: A Survey. (arXiv:2103.02429v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.02429","description":"<p>Land cover mapping is essential for monitoring global environmental change\nand managing natural resources. Unfortunately, traditional classification\nmodels are plagued by limited training data available in existing land cover\nproducts and data heterogeneity over space and time. In this survey, we provide\na structured and comprehensive overview of challenges in land cover mapping and\nmachine learning methods used to address these problems. We also discuss the\ngaps and opportunities that exist for advancing research in this promising\ndirection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rahul Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Accurate: Video Enhancement using Sparse Depth. (arXiv:2103.08764v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08764","description":"<p>This paper presents a general framework to build fast and accurate algorithms\nfor video enhancement tasks such as super-resolution, deblurring, and\ndenoising. Essential to our framework is the realization that the accuracy,\nrather than the density, of pixel flows is what is required for high-quality\nvideo enhancement. Most of prior works take the opposite approach: they\nestimate dense (per-pixel)-but generally less robust-flows, mostly using\ncomputationally costly algorithms. Instead, we propose a lightweight flow\nestimation algorithm; it fuses the sparse point cloud data and (even sparser\nand less reliable) IMU data available in modern autonomous agents to estimate\nthe flow information. Building on top of the flow estimation, we demonstrate a\ngeneral framework that integrates the flows in a plug-and-play fashion with\ndifferent task-specific layers. Algorithms built in our framework achieve 1.78x\n- 187.41x speedup while providing a 0.42 dB - 6.70 dB quality improvement over\ncompeting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_P/0/1/0/all/0/1\">Patrick Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1\">Paul N. Whatmough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guoyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuhao Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Training Enhances Online Continual Learning. (arXiv:2103.14010v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14010","description":"<p>In continual learning, a system must incrementally learn from a\nnon-stationary data stream without catastrophic forgetting. Recently, multiple\nmethods have been devised for incrementally learning classes on large-scale\nimage classification tasks, such as ImageNet. State-of-the-art continual\nlearning methods use an initial supervised pre-training phase, in which the\nfirst 10% - 50% of the classes in a dataset are used to learn representations\nin an offline manner before continual learning of new classes begins. We\nhypothesize that self-supervised pre-training could yield features that\ngeneralize better than supervised learning, especially when the number of\nsamples used for pre-training is small. We test this hypothesis using the\nself-supervised MoCo-V2, Barlow Twins, and SwAV algorithms. On ImageNet, we\nfind that these methods outperform supervised pre-training considerably for\nonline continual learning, and the gains are larger when fewer samples are\navailable. Our findings are consistent across three online continual learning\nalgorithms. Our best system achieves a 14.95% relative increase in top-1\naccuracy on class incremental ImageNet over the prior state of the art for\nonline continual learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gallardo_J/0/1/0/all/0/1\">Jhair Gallardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Describing and Localizing Multiple Changes with Transformers. (arXiv:2103.14146v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14146","description":"<p>Change captioning tasks aim to detect changes in image pairs observed before\nand after a scene change and generate a natural language description of the\nchanges. Existing change captioning studies have mainly focused on a single\nchange.However, detecting and describing multiple changed parts in image pairs\nis essential for enhancing adaptability to complex scenarios. We solve the\nabove issues from three aspects: (i) We propose a simulation-based multi-change\ncaptioning dataset; (ii) We benchmark existing state-of-the-art methods of\nsingle change captioning on multi-change captioning; (iii) We further propose\nMulti-Change Captioning transformers (MCCFormers) that identify change regions\nby densely correlating different regions in image pairs and dynamically\ndetermines the related change regions with words in sentences. The proposed\nmethod obtained the highest scores on four conventional change captioning\nevaluation metrics for multi-change captioning. Additionally, our proposed\nmethod can separate attention maps for each change and performs well with\nrespect to change localization. Moreover, the proposed framework outperformed\nthe previous state-of-the-art methods on an existing change captioning\nbenchmark, CLEVR-Change, by a large margin (+6.1 on BLEU-4 and +9.7 on CIDEr\nscores), indicating its general ability in change captioning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yue Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_S/0/1/0/all/0/1\">Shintaro Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_K/0/1/0/all/0/1\">Kodai Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_R/0/1/0/all/0/1\">Ryota Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwata_K/0/1/0/all/0/1\">Kenji Iwata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kataoka_H/0/1/0/all/0/1\">Hirokatsu Kataoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_Y/0/1/0/all/0/1\">Yutaka Satoh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D2C-SR: A Divergence to Convergence Approach for Real-World Image Super-Resolution. (arXiv:2103.14373v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14373","description":"<p>In this paper, we present D2C-SR, a novel framework for the task of\nreal-world image super-resolution. As an ill-posed problem, the key challenge\nin super-resolution related tasks is there can be multiple predictions for a\ngiven low-resolution input. Most classical deep learning based approaches\nignored the fundamental fact and lack explicit modeling of the underlying\nhigh-frequency distribution which leads to blurred results. Recently, some\nmethods of GAN-based or learning super-resolution space can generate simulated\ntextures but do not promise the accuracy of the textures which have low\nquantitative performance. Rethinking both, we learn the distribution of\nunderlying high-frequency details in a discrete form and propose a two-stage\npipeline: divergence stage to convergence stage. At divergence stage, we\npropose a tree-based structure deep network as our divergence backbone.\nDivergence loss is proposed to encourage the generated results from the\ntree-based network to diverge into possible high-frequency representations,\nwhich is our way of discretely modeling the underlying high-frequency\ndistribution. At convergence stage, we assign spatial weights to fuse these\ndivergent predictions to obtain the final output with more accurate details.\nOur approach provides a convenient end-to-end manner to inference. We conduct\nevaluations on several real-world benchmarks, including a new proposed\nD2CRealSR dataset with x8 scaling factor. Our experiments demonstrate that\nD2C-SR achieves better accuracy and visual improvements against\nstate-of-the-art methods, with a significantly less parameters number.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Youwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_L/0/1/0/all/0/1\">Lanpeng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VR3Dense: Voxel Representation Learning for 3D Object Detection and Monocular Dense Depth Reconstruction. (arXiv:2104.05932v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05932","description":"<p>3D object detection and dense depth estimation are one of the most vital\ntasks in autonomous driving. Multiple sensor modalities can jointly attribute\ntowards better robot perception, and to that end, we introduce a method for\njointly training 3D object detection and monocular dense depth reconstruction\nneural networks. It takes as inputs, a LiDAR point-cloud, and a single RGB\nimage during inference and produces object pose predictions as well as a\ndensely reconstructed depth map. LiDAR point-cloud is converted into a set of\nvoxels, and its features are extracted using 3D convolution layers, from which\nwe regress object pose parameters. Corresponding RGB image features are\nextracted using another 2D convolutional neural network. We further use these\ncombined features to predict a dense depth map. While our object detection is\ntrained in a supervised manner, the depth prediction network is trained with\nboth self-supervised and supervised loss functions. We also introduce a loss\nfunction, edge-preserving smooth loss, and show that this results in better\ndepth estimation compared to the edge-aware smooth loss function, frequently\nused in depth prediction works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_S/0/1/0/all/0/1\">Shubham Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Multi-Target Domain Adaptation for Object Detection with Efficient Domain Transfer. (arXiv:2104.06476v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06476","description":"<p>Recent advances in unsupervised domain adaptation have significantly improved\nthe recognition accuracy of CNNs by alleviating the domain shift between\n(labeled) source and (unlabeled) target data distributions. While the problem\nof single-target domain adaptation (STDA) for object detection has recently\nreceived much attention, multi-target domain adaptation (MTDA) remains largely\nunexplored, despite its practical relevance in several real-world applications,\nsuch as multi-camera video surveillance. Compared to the STDA problem that may\ninvolve large domain shifts between complex source and target distributions,\nMTDA faces additional challenges, most notably the computational requirements\nand catastrophic forgetting of previously-learned targets, which can depend on\nthe order of target adaptations. STDA for detection can be applied to MTDA by\nadapting one model per target, or one common model with a mixture of data from\ntarget domains. However, these approaches are either costly or inaccurate. The\nonly state-of-art MTDA method specialized for detection learns targets\nincrementally, one target at a time, and mitigates the loss of knowledge by\nusing a duplicated detection model for knowledge distillation, which is\ncomputationally expensive and does not scale well to many domains. In this\npaper, we introduce an efficient approach for incremental learning that\ngeneralizes well to multiple target domains. Our MTDA approach is more suitable\nfor real-world applications since it allows updating the detection model\nincrementally, without storing data from previous-learned target domains, nor\nretraining when a new target domain becomes available. Our proposed method,\nMTDA-DTM, achieved the highest level of detection accuracy compared against\nstate-of-the-art approaches on several MTDA detection benchmarks and Wildtrack,\na benchmark for multi-camera pedestrian detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Meidine_L/0/1/0/all/0/1\">Le Thanh Nguyen-Meidine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiran_M/0/1/0/all/0/1\">Madhu Kiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blais_Morin_L/0/1/0/all/0/1\">Louis-Antoine Blais-Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does language help generalization in vision models?. (arXiv:2104.08313v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2104.08313","description":"<p>Vision models trained on multimodal datasets can benefit from the wide\navailability of large image-caption datasets. A recent model (CLIP) was found\nto generalize well in zero-shot and transfer learning settings. This could\nimply that linguistic or \"semantic grounding\" confers additional generalization\nabilities to the visual feature space. Here, we systematically evaluate various\nmultimodal architectures and vision-only models in terms of unsupervised\nclustering, few-shot learning, transfer learning and adversarial robustness. In\neach setting, multimodal training produced no additional generalization\ncapability compared to standard supervised visual training. We conclude that\nwork is still required for semantic grounding to help improve vision models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Devillers_B/0/1/0/all/0/1\">Benjamin Devillers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choksi_B/0/1/0/all/0/1\">Bhavin Choksi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielawski_R/0/1/0/all/0/1\">Romain Bielawski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1\">Rufin VanRullen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning. (arXiv:2104.08718v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08718","description":"<p>Image captioning has conventionally relied on reference-based automatic\nevaluations, where machine captions are compared against captions written by\nhumans. This is in contrast to the reference-free manner in which humans assess\ncaption quality.\n</p>\n<p>In this paper, we report the surprising empirical finding that CLIP (Radford\net al., 2021), a cross-modal model pretrained on 400M image+caption pairs from\nthe web, can be used for robust automatic evaluation of image captioning\nwithout the need for references. Experiments spanning several corpora\ndemonstrate that our new reference-free metric, CLIPScore, achieves the highest\ncorrelation with human judgements, outperforming existing reference-based\nmetrics like CIDEr and SPICE. Information gain experiments demonstrate that\nCLIPScore, with its tight focus on image-text compatibility, is complementary\nto existing reference-based metrics that emphasize text-text similarities.\nThus, we also present a reference-augmented version, RefCLIPScore, which\nachieves even higher correlation. Beyond literal description tasks, several\ncase studies reveal domains where CLIPScore performs well (clip-art images,\nalt-text rating), but also where it is relatively weaker in comparison to\nreference-based metrics, e.g., news captions that require richer contextual\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoGPT: Video Generation using VQ-VAE and Transformers. (arXiv:2104.10157v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10157","description":"<p>We present VideoGPT: a conceptually simple architecture for scaling\nlikelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE\nthat learns downsampled discrete latent representations of a raw video by\nemploying 3D convolutions and axial self-attention. A simple GPT-like\narchitecture is then used to autoregressively model the discrete latents using\nspatio-temporal position encodings. Despite the simplicity in formulation and\nease of training, our architecture is able to generate samples competitive with\nstate-of-the-art GAN models for video generation on the BAIR Robot dataset, and\ngenerate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset\n(TGIF). We hope our proposed architecture serves as a reproducible reference\nfor a minimalistic implementation of transformer based video generation models.\nSamples and code are available at\nhttps://wilson1yan.github.io/videogpt/index.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wilson Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1\">Aravind Srinivas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Unified Stereo Stimuli based Binocular Eye-Tracking System for Accurate 3D Gaze Estimation. (arXiv:2104.12167v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12167","description":"<p>In addition to the high cost and complex setup, the main reason for the\nlimitation of the three-dimensional (3D) display is the problem of accurately\nestimating the user's current point-of-gaze (PoG) in a 3D space. In this paper,\nwe present a novel noncontact technique for the PoG estimation in a\nstereoscopic environment, which integrates a 3D stereoscopic display system and\nan eye-tracking system. The 3D stereoscopic display system can provide users\nwith a friendly and immersive high-definition viewing experience without\nwearing any equipment. To accurately locate the user's 3D PoG in the field of\nview, we build a regression-based 3D eye-tracking model with the eye movement\ndata and stereo stimulus videos as input. Besides, to train an optimal\nregression model, we also design and annotate a dataset that contains 30 users'\neye-tracking data corresponding to two designed stereo test scenes.\nInnovatively, this dataset introduces feature vectors between eye region\nlandmarks for the gaze vector estimation and a combined feature set for the\ngaze depth estimation. Moreover, five traditional regression models are trained\nand evaluated based on this dataset. Experimental results show that the average\nerrors of the 3D PoG are about 0.90~cm on the X-axis, 0.83~cm on the Y-axis,\nand 1.48~cm$/$0.12~m along the Z-axis with the scene-depth range in\n75~cm$/$8~m, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sunjing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaochu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Improving Semantic Perception for Indoor Localisation. (arXiv:2105.01595v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2105.01595","description":"<p>We propose a novel robotic system that can improve its perception during\ndeployment. Contrary to the established approach of learning semantics from\nlarge datasets and deploying fixed models, we propose a framework in which\nsemantic models are continuously updated on the robot to adapt to the\ndeployment environments. By combining continual learning with self-supervision,\nour robotic system learns online during deployment without external\nsupervision. We conduct real-world experiments with robots localising in 3D\nfloorplans. Our experiments show how the robot's semantic perception improves\nduring deployment and how this translates into improved localisation, even\nacross drastically different environments. We further study the risk of\ncatastrophic forgetting that such a continuous learning setting poses. We find\nmemory replay an effective measure to reduce forgetting and show how the\nrobotic system can improve even when switching between different environments.\nOn average, our system improves by 60% in segmentation and 10% in localisation\naccuracy compared to deployment of a fixed model, and it maintains this\nimprovement while adapting to further environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blum_H/0/1/0/all/0/1\">Hermann Blum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milano_F/0/1/0/all/0/1\">Francesco Milano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zurbrugg_R/0/1/0/all/0/1\">Ren&#xe9; Zurbr&#xfc;gg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegward_R/0/1/0/all/0/1\">Roland Siegward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_C/0/1/0/all/0/1\">Cesar Cadena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gawel_A/0/1/0/all/0/1\">Abel Gawel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOTR: End-to-End Multiple-Object Tracking with TRansformer. (arXiv:2105.03247v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03247","description":"<p>The key challenge in multiple-object tracking task is temporal modeling of\nthe object under track. Existing tracking-by-detection methods adopt simple\nheuristics, such as spatial or appearance similarity. Such methods, in spite of\ntheir commonality, are overly simple and lack the ability to learn temporal\nvariations from data in an end-to-end manner. In this paper, we present MOTR, a\nfully end-to-end multiple-object tracking framework. It learns to model the\nlong-range temporal variation of the objects. It performs temporal association\nimplicitly and avoids previous explicit heuristics. Built upon DETR, MOTR\nintroduces the concept of \"track query\". Each track query models the entire\ntrack of an object. It is transferred and updated frame-by-frame to perform\niterative predictions in a seamless manner. Tracklet-aware label assignment is\nproposed for one-to-one assignment between track queries and object tracks.\nTemporal aggregation network together with collective average loss is further\nproposed to enhance the long-range temporal relation. Experimental results show\nthat MOTR achieves competitive performance and can serve as a strong\nTransformer-based baseline for future research. Code is available at\n\\url{https://github.com/megvii-model/MOTR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1\">Fangao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiancai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yichen Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SA-GAN: Structure-Aware GAN for Organ-Preserving Synthetic CT Generation. (arXiv:2105.07044v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.07044","description":"<p>In medical image synthesis, model training could be challenging due to the\ninconsistencies between images of different modalities even with the same\npatient, typically caused by internal status/tissue changes as different\nmodalities are usually obtained at a different time. This paper proposes a\nnovel deep learning method, Structure-aware Generative Adversarial Network\n(SA-GAN), that preserves the shapes and locations of in-consistent structures\nwhen generating medical images. SA-GAN is employed to generate synthetic\ncomputed tomography (synCT) images from magnetic resonance imaging (MRI) with\ntwo parallel streams: the global stream translates the input from the MRI to\nthe CT domain while the local stream automatically segments the inconsistent\norgans, maintains their locations and shapes in MRI, and translates the organ\nintensities to CT. Through extensive experiments on a pelvic dataset, we\ndemonstrate that SA-GAN provides clinically acceptable accuracy on both synCTs\nand organ segmentation and supports MR-only treatment planning in disease sites\nwith internal organ status changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Emami_H/0/1/0/all/0/1\">Hajar Emami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_M/0/1/0/all/0/1\">Ming Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nejad_Davarani_S/0/1/0/all/0/1\">Siamak Nejad-Davarani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glide_Hurst_C/0/1/0/all/0/1\">Carri Glide-Hurst</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polarimetric Spatio-Temporal Light Transport Probing. (arXiv:2105.11609v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11609","description":"<p>Light emitted from a source into a scene can undergo complex interactions\nwith scene surfaces of different material types before being reflected. During\nthis transport, every surface reflection is encoded in the properties of the\nphotons that reach the detector, including time, direction, intensity,\nwavelength and polarization. Conventional imaging systems capture intensity by\nintegrating over all other dimensions of the light, hiding this rich scene\ninformation. Existing methods are capable of untangling these measurements into\ntheir spatial and temporal dimensions, fueling geometric scene understanding\ntasks. However, examining material properties jointly with geometric properties\nis an open challenge that could enable unprecedented capabilities beyond\ngeometric scene understanding, allowing for material-dependent scene\nunderstanding and imaging through complex transport. In this work, we close\nthis gap, and propose a computational light transport imaging method that\ncaptures the spatially- and temporally-resolved complete polarimetric response\nof a scene. Our method hinges on a 7D tensor theory of light transport. We\ndiscover low-rank structure in the polarimetric tensor dimension and propose a\ndata-driven rotating ellipsometry method that learns to exploit redundancy of\npolarimetric structure. We instantiate our theory with two prototypes:\nspatio-polarimetric imaging and coaxial temporal-polarimetric imaging. This\nallows us, for the first time, to decompose scene light transport into\ntemporal, spatial, and complete polarimetric dimensions that unveil scene\nproperties hidden to conventional methods. We validate the applicability of our\nmethod on diverse tasks, including shape reconstruction with subsurface\nscattering, seeing through scattering media, untangling multi-bounce light\ntransport, breaking metamerism, and decomposition of crystals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Seung-Hwan Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Stylize Novel Views. (arXiv:2105.13509v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.13509","description":"<p>We tackle a 3D scene stylization problem - generating stylized images of a\nscene from arbitrary novel views given a set of images of the same scene and a\nreference image of the desired style as inputs. Direct solution of combining\nnovel view synthesis and stylization approaches lead to results that are blurry\nor not consistent across different views. We propose a point cloud-based method\nfor consistent 3D scene stylization. First, we construct the point cloud by\nback-projecting the image features to the 3D space. Second, we develop point\ncloud aggregation modules to gather the style information of the 3D scene, and\nthen modulate the features in the point cloud with a linear transformation\nmatrix. Finally, we project the transformed features to 2D space to obtain the\nnovel views. Experimental results on two diverse datasets of real-world scenes\nvalidate that our method generates consistent stylized novel view synthesis\nresults against other alternative approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hsin-Ping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1\">Hung-Yu Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_S/0/1/0/all/0/1\">Saurabh Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Encoder Learning for Captioning Heavy Rain Images via Semantic Visual Feature Matching. (arXiv:2105.13753v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.13753","description":"<p>Image captioning generates text that describes scenes from input images. It\nhas been developed for high quality images taken in clear weather. However, in\nbad weather conditions, such as heavy rain, snow, and dense fog, the poor\nvisibility owing to rain streaks, rain accumulation, and snowflakes causes a\nserious degradation of image quality. This hinders the extraction of useful\nvisual features and results in deteriorated image captioning performance. To\naddress practical issues, this study introduces a new encoder for captioning\nheavy rain images. The central idea is to transform output features extracted\nfrom heavy rain input images into semantic visual features associated with\nwords and sentence context. To achieve this, a target encoder is initially\ntrained in an encoder-decoder framework to associate visual features with\nsemantic words. Subsequently, the objects in a heavy rain image are rendered\nvisible by using an initial reconstruction subnetwork (IRS) based on a heavy\nrain model. The IRS is then combined with another semantic visual feature\nmatching subnetwork (SVFMS) to match the output features of the IRS with the\nsemantic visual features of the pretrained target encoder. The proposed encoder\nis based on the joint learning of the IRS and SVFMS. It is is trained in an\nend-to-end manner, and then connected to the pretrained decoder for image\ncaptioning. It is experimentally demonstrated that the proposed encoder can\ngenerate semantic visual features associated with words even from heavy rain\nimages, thereby increasing the accuracy of the generated captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_C/0/1/0/all/0/1\">Chang-Hwan Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Pung-Hwi Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepChange: A Long-Term Person Re-Identification Benchmark. (arXiv:2105.14685v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14685","description":"<p>Existing person re-identification (Re-ID) works mostly consider a short-term\nsearch problem assuming unchanged clothes and personal appearance. However, in\nreal-world we often dress differently across locations, time, dates, seasons,\nweather, and events. As a result, the existing methods are unsuitable for\nlong-term person Re-ID with clothes change involved. Whilst there are several\nrecent long-term Re-ID attempts, a large realistic dataset with clothes change\nis lacking and indispensable for enabling extensive study as already\nexperienced in short-term Re-ID setting. In this work, we contribute a large,\nrealistic long-term person identification benchmark. It consists of 178K\nbounding boxes from 1.1K person identities, collected and constructed over 12\nmonths. Unique characteristics of this dataset include: (1) Natural/native\npersonal appearance (e.g., clothes and hair style) variations: The\nclothes-change and dressing styles all are highly diverse, with the reappearing\ngap in time ranging from minutes, hours, and days to weeks, months, seasons,\nand years. (2) Diverse walks of life: Persons across a wide range of ages and\nprofessions appear in different weather conditions (e.g., sunny, cloudy, windy,\nrainy, snowy, extremely cold) and events (e.g., working, leisure, daily\nactivities). (3) Rich camera setups: The raw videos were recorded by 17 outdoor\nsecurity cameras with various resolutions operating in a real-world\nsurveillance system for a wide and dense block. (4) Largest scale: It covers\nthe largest number of (17) cameras, (1, 121) identities, and (178, 407)\nbounding boxes, as compared to alternative datasets. Our dataset and benchmark\ncodes are available on https://github.com/PengBoXiangShang/deepchange.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-based Multimodal Fusion. (arXiv:2106.03821v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2106.03821","description":"<p>It is now well established from a variety of studies that there is a\nsignificant benefit from combining video and audio data in detecting active\nspeakers. However, either of the modalities can potentially mislead audiovisual\nfusion by inducing unreliable or deceptive information. This paper outlines\nactive speaker detection as a multi-objective learning problem to leverage best\nof each modalities using a novel self-attention, uncertainty-based multimodal\nfusion scheme. Results obtained show that the proposed multi-objective learning\narchitecture outperforms traditional approaches in improving both mAP and AUC\nscores. We further demonstrate that our fusion strategy surpasses, in active\nspeaker detection, other modality fusion methods reported in various\ndisciplines. We finally show that the proposed method significantly improves\nthe state-of-the-art on the AVA-ActiveSpeaker dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pouthier_B/0/1/0/all/0/1\">Baptiste Pouthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilati_L/0/1/0/all/0/1\">Laurent Pilati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gudupudi_L/0/1/0/all/0/1\">Leela K. Gudupudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouveyron_C/0/1/0/all/0/1\">Charles Bouveyron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precioso_F/0/1/0/all/0/1\">Frederic Precioso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing Unrecognizable Faces for Improving Face Recognition. (arXiv:2106.04112v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04112","description":"<p>The common implementation of face recognition systems as a cascade of a\ndetection stage and a recognition or verification stage can cause problems\nbeyond failures of the detector. When the detector succeeds, it can detect\nfaces that cannot be recognized, no matter how capable the recognition system.\nRecognizability, a latent variable, should therefore be factored into the\ndesign and implementation of face recognition systems. We propose a measure of\nrecognizability of a face image that leverages a key empirical observation: an\nembedding of face images, implemented by a deep neural network trained using\nmostly recognizable identities, induces a partition of the hypersphere whereby\nunrecognizable identities cluster together. This occurs regardless of the\nphenomenon that causes a face to be unrecognizable, it be optical or motion\nblur, partial occlusion, spatial quantization, poor illumination. Therefore, we\nuse the distance from such an \"unrecognizable identity\" as a measure of\nrecognizability, and incorporate it in the design of the over-all system. We\nshow that accounting for recognizability reduces error rate of single-image\nface recognition by 58% at FAR=1e-5 on the IJB-C Covariate Verification\nbenchmark, and reduces verification error rate by 24% at FAR=1e-5 in set-based\nrecognition on the IJB-C benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Siqi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoAtNet: Marrying Convolution and Attention for All Data Sizes. (arXiv:2106.04803v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04803","description":"<p>Transformers have attracted increasing interests in computer vision, but they\nstill fall behind state-of-the-art convolutional networks. In this work, we\nshow that while Transformers tend to have larger model capacity, their\ngeneralization can be worse than convolutional networks due to the lack of the\nright inductive bias. To effectively combine the strengths from both\narchitectures, we present CoAtNets(pronounced \"coat\" nets), a family of hybrid\nmodels built from two key insights: (1) depthwise Convolution and\nself-Attention can be naturally unified via simple relative attention; (2)\nvertically stacking convolution layers and attention layers in a principled way\nis surprisingly effective in improving generalization, capacity and efficiency.\nExperiments show that our CoAtNets achieve state-of-the-art performance under\ndifferent resource constraints across various datasets: Without extra data,\nCoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M\nimages from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching\nViT-huge pre-trained with 300M images from JFT-300M while using 23x less data;\nNotably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1\naccuracy on ImageNet, establishing a new state-of-the-art result.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUDA-GHR: Controllable Unsupervised Domain Adaptation for Gaze and Head Redirection. (arXiv:2106.10852v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10852","description":"<p>The robustness of gaze and head pose estimation models is highly dependent on\nthe amount of labeled data. Recently, generative modeling has shown excellent\nresults in generating photo-realistic images, which can alleviate the need for\nlabeled data. However, adopting such generative models to new domains while\nmaintaining their ability to provide fine-grained control over different image\nattributes, e.g., gaze and head pose directions, has been a challenging\nproblem. This paper proposes CUDA-GHR, an unsupervised domain adaptation\nframework that enables fine-grained control over gaze and head pose directions\nwhile preserving the appearance-related factors of the person. Our framework\nsimultaneously learns to adapt to new domains and disentangle image attributes\nsuch as appearance, gaze direction, and head orientation by utilizing a\nlabel-rich source domain and an unlabeled target domain. Extensive experiments\non the benchmarking datasets show that the proposed method can outperform\nstate-of-the-art techniques on both quantitative and qualitative evaluations.\nFurthermore, we show that the generated image-label pairs in the target domain\neffectively transfer knowledge and boost the downstream tasks' performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jindal_S/0/1/0/all/0/1\">Swati Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RATCHET: Medical Transformer for Chest X-ray Diagnosis and Reporting. (arXiv:2107.02104v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02104","description":"<p>Chest radiographs are one of the most common diagnostic modalities in\nclinical routine. It can be done cheaply, requires minimal equipment, and the\nimage can be diagnosed by every radiologists. However, the number of chest\nradiographs obtained on a daily basis can easily overwhelm the available\nclinical capacities. We propose RATCHET: RAdiological Text Captioning for Human\nExamined Thoraces. RATCHET is a CNN-RNN-based medical transformer that is\ntrained end-to-end. It is capable of extracting image features from chest\nradiographs, and generates medically accurate text reports that fit seamlessly\ninto clinical work flows. The model is evaluated for its natural language\ngeneration ability using common metrics from NLP literature, as well as its\nmedically accuracy through a surrogate report classification task. The model is\navailable for download at: <a href=\"http://www.github.com/farrell236/RATCHET.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Benjamin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summers_R/0/1/0/all/0/1\">Ronald Summers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning based Micro-expression Recognition: A Survey. (arXiv:2107.02823v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02823","description":"<p>Micro-expressions (MEs) are involuntary facial movements revealing people's\nhidden feelings in high-stake situations and have practical importance in\nmedical treatment, national security, interrogations and many human-computer\ninteraction systems. Early methods for MER mainly based on traditional\nappearance and geometry features. Recently, with the success of deep learning\n(DL) in various fields, neural networks have received increasing interests in\nMER. Different from macro-expressions, MEs are spontaneous, subtle, and rapid\nfacial movements, leading to difficult data collection, thus have small-scale\ndatasets. DL based MER becomes challenging due to above ME characters. To date,\nvarious DL approaches have been proposed to solve the ME issues and improve MER\nperformance. In this survey, we provide a comprehensive review of deep\nmicro-expression recognition (MER), including datasets, deep MER pipeline, and\nthe bench-marking of most influential methods. This survey defines a new\ntaxonomy for the field, encompassing all aspects of MER based on DL. For each\naspect, the basic approaches and advanced developments are summarized and\ndiscussed. In addition, we conclude the remaining challenges and and potential\ndirections for the design of robust deep MER systems. To the best of our\nknowledge, this is the first survey of deep MER methods, and this survey can\nserve as a reference point for future MER research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yante Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jinsheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadifoumani_S/0/1/0/all/0/1\">Seyednavid Mohammadifoumani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. (arXiv:2107.06011v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06011","description":"<p>In the context of visual navigation, the capacity to map a novel environment\nis necessary for an agent to exploit its observation history in the considered\nplace and efficiently reach known goals. This ability can be associated with\nspatial reasoning, where an agent is able to perceive spatial relationships and\nregularities, and discover object characteristics. In classical Reinforcement\nLearning (RL) setups, this capacity is learned from reward alone. We introduce\nsupplementary supervision in the form of auxiliary tasks designed to favor the\nemergence of spatial perception capabilities in agents trained for a\ngoal-reaching downstream objective. We show that learning to estimate metrics\nquantifying the spatial relationships between an agent at a given location and\na goal to reach has a high positive impact in Multi-Object Navigation settings.\nOur method significantly improves the performance of different baseline agents,\nthat either build an explicit or implicit representation of the environment,\neven matching the performance of incomparable oracle agents taking ground-truth\nmaps as input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marza_P/0/1/0/all/0/1\">Pierre Marza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matignon_L/0/1/0/all/0/1\">Laetitia Matignon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonin_O/0/1/0/all/0/1\">Olivier Simonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CalCROP21: A Georeferenced multi-spectral dataset of Satellite Imagery and Crop Labels. (arXiv:2107.12499v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12499","description":"<p>Mapping and monitoring crops is a key step towards sustainable\nintensification of agriculture and addressing global food security. A dataset\nlike ImageNet that revolutionized computer vision applications can accelerate\ndevelopment of novel crop mapping techniques. Currently, the United States\nDepartment of Agriculture (USDA) annually releases the Cropland Data Layer\n(CDL) which contains crop labels at 30m resolution for the entire United States\nof America. While CDL is state of the art and is widely used for a number of\nagricultural applications, it has a number of limitations (e.g., pixelated\nerrors, labels carried over from previous errors and absence of input imagery\nalong with class labels). In this work, we create a new semantic segmentation\nbenchmark dataset, which we call CalCROP21, for the diverse crops in the\nCentral Valley region of California at 10m spatial resolution using a Google\nEarth Engine based robust image processing pipeline and a novel attention based\nspatio-temporal semantic segmentation algorithm STATT. STATT uses re-sampled\n(interpolated) CDL labels for training, but is able to generate a better\nprediction than CDL by leveraging spatial and temporal patterns in Sentinel2\nmulti-spectral image series to effectively capture phenologic differences\namongst crops and uses attention to reduce the impact of clouds and other\natmospheric disturbances. We also present a comprehensive evaluation to show\nthat STATT has significantly better results when compared to the resampled CDL\nlabels. We have released the dataset and the processing pipeline code for\ngenerating the benchmark dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rahul Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravirathinam_P/0/1/0/all/0/1\">Praveen Ravirathinam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Ankush Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulla_D/0/1/0/all/0/1\">David Mulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Grounding with 3D Objects. (arXiv:2107.12514v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12514","description":"<p>Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" Flat images\nof candidate mice may not provide the discriminative information needed for\n\"wireless.\" The world, and objects in it, are not flat images but complex 3D\nshapes. If a human requests an object based on any of its basic properties,\nsuch as color, shape, or texture, robots should perform the necessary\nexploration to accomplish the task. In particular, while substantial effort and\nprogress has been made on understanding explicitly visual attributes like color\nand category, comparatively little progress has been made on understanding\nlanguage about shapes and contours. In this work, we introduce a novel\nreasoning task that targets both visual and non-visual language about 3D\nobjects. Our new benchmark, ShapeNet Annotated with Referring Expressions\n(SNARE) requires a model to choose which of two objects is being referenced by\na natural language description. We introduce several CLIP-based models for\ndistinguishing objects and demonstrate that while recent advances in jointly\nmodeling vision and language are useful for robotic language understanding, it\nis still the case that these image-based models are weaker at understanding the\n3D nature of objects -- properties which play a key role in manipulation. We\nfind that adding view estimation to language grounding models improves accuracy\non both SNARE and when identifying objects referred to in language on a robot\nplatform, but note that a large gap remains between these models and human\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Free Lunch for Co-Saliency Detection: Context Adjustment. (arXiv:2108.02093v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02093","description":"<p>We unveil a long-standing problem in the prevailing co-saliency detection\nsystems: there is indeed inconsistency between training and testing.\nConstructing a high-quality co-saliency detection dataset involves\ntime-consuming and labor-intensive pixel-level labeling, which has forced most\nrecent works to rely instead on semantic segmentation or saliency detection\ndatasets for training. However, the lack of proper co-saliency and the absence\nof multiple foreground objects in these datasets can lead to spurious\nvariations and inherent biases learned by models. To tackle this, we introduce\nthe idea of counterfactual training through context adjustment and propose a\n\"cost-free\" group-cut-paste (GCP) procedure to leverage off-the-shelf images\nand synthesize new samples. Following GCP, we collect a novel dataset called\nContext Adjustment Training (CAT). CAT consists of 33,500 images, which is four\ntimes larger than the current co-saliency detection datasets. All samples are\nautomatically annotated with high-quality mask annotations, object categories,\nand edge maps. Extensive experiments on recent benchmarks are conducted, show\nthat CAT can improve various state-of-the-art models by a large margin (5% ~\n25%). We hope that the scale, diversity, and quality of our dataset can benefit\nresearchers in this area and beyond. Our dataset will be publicly accessible\nthrough our project page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingdong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_P/0/1/0/all/0/1\">Prakhar Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models. (arXiv:2108.02938v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02938","description":"<p>Denoising diffusion probabilistic models (DDPM) have shown remarkable\nperformance in unconditional image generation. However, due to the\nstochasticity of the generative process in DDPM, it is challenging to generate\nimages with the desired semantics. In this work, we propose Iterative Latent\nVariable Refinement (ILVR), a method to guide the generative process in DDPM to\ngenerate high-quality images based on a given reference image. Here, the\nrefinement of the generative process in DDPM enables a single DDPM to sample\nimages from various sets directed by the reference image. The proposed ILVR\nmethod generates high-quality images while controlling the generation. The\ncontrollability of our method allows adaptation of a single DDPM without any\nadditional learning in various image generation tasks, such as generation from\nvarious downsampling factors, multi-domain image translation, paint-to-image,\nand editing with scribbles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jooyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yonghyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwon_Y/0/1/0/all/0/1\">Youngjune Gwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-weighted Central Similarity for Multi-label Image Retrieval. (arXiv:2108.05274v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05274","description":"<p>Deep hashing has been widely applied to large-scale image retrieval by\nencoding high-dimensional data points into binary codes for efficient\nretrieval. Compared with pairwise/triplet similarity based hash learning,\ncentral similarity based hashing can more efficiently capture the global data\ndistribution. For multi-label image retrieval, however, previous methods only\nuse multiple hash centers with equal weights to generate one centroid as the\nlearning target, which ignores the relationship between the weights of hash\ncenters and the proportion of instance regions in the image. To address the\nabove issue, we propose a two-step alternative optimization approach,\nInstance-weighted Central Similarity (ICS), to automatically learn the center\nweight corresponding to a hash code. Firstly, we apply the maximum entropy\nregularizer to prevent one hash center from dominating the loss function, and\ncompute the center weights via projection gradient descent. Secondly, we update\nneural network parameters by standard back-propagation with fixed center\nweights. More importantly, the learned center weights can well reflect the\nproportion of foreground instances in the image. Our method achieves the\nstate-of-the-art performance on the image retrieval benchmarks, and especially\nimproves the mAP by 1.6%-6.4% on the MS COCO dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1\">Allen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation. (arXiv:2108.06962v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06962","description":"<p>In this work, we address the task of unsupervised domain adaptation (UDA) for\nsemantic segmentation in presence of multiple target domains: The objective is\nto train a single model that can handle all these domains at test time. Such a\nmulti-target adaptation is crucial for a variety of scenarios that real-world\nautonomous systems must handle. It is a challenging setup since one faces not\nonly the domain gap between the labeled source set and the unlabeled target\nset, but also the distribution shifts existing within the latter among the\ndifferent target domains. To this end, we introduce two adversarial frameworks:\n(i) multi-discriminator, which explicitly aligns each target domain to its\ncounterparts, and (ii) multi-target knowledge transfer, which learns a\ntarget-agnostic model thanks to a multi-teacher/single-student distillation\nmechanism.The evaluation is done on four newly-proposed multi-target benchmarks\nfor UDA in semantic segmentation. In all tested scenarios, our approaches\nconsistently outperform baselines, setting competitive standards for the novel\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Antoine Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tuan-Hung Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOKI: Long Term and Key Intentions for Trajectory Prediction. (arXiv:2108.08236v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08236","description":"<p>Recent advances in trajectory prediction have shown that explicit reasoning\nabout agents' intent is important to accurately forecast their motion. However,\nthe current research activities are not directly applicable to intelligent and\nsafety critical systems. This is mainly because very few public datasets are\navailable, and they only consider pedestrian-specific intents for a short\ntemporal horizon from a restricted egocentric view. To this end, we propose\nLOKI (LOng term and Key Intentions), a novel large-scale dataset that is\ndesigned to tackle joint trajectory and intention prediction for heterogeneous\ntraffic agents (pedestrians and vehicles) in an autonomous driving setting. The\nLOKI dataset is created to discover several factors that may affect intention,\nincluding i) agent's own will, ii) social interactions, iii) environmental\nconstraints, and iv) contextual information. We also propose a model that\njointly performs trajectory and intention prediction, showing that recurrently\nreasoning about intention can assist with trajectory prediction. We show our\nmethod outperforms state-of-the-art trajectory prediction methods by upto\n$27\\%$ and also provide a baseline for frame-wise intention estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girase_H/0/1/0/all/0/1\">Harshayu Girase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gang_H/0/1/0/all/0/1\">Haiming Gang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1\">Srikanth Malla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanehara_A/0/1/0/all/0/1\">Akira Kanehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Chiho Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CenterPoly: real-time instance segmentation using bounding polygons. (arXiv:2108.08923v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08923","description":"<p>We present a novel method, called CenterPoly, for real-time instance\nsegmentation using bounding polygons. We apply it to detect road users in dense\nurban environments, making it suitable for applications in intelligent\ntransportation systems like automated vehicles. CenterPoly detects objects by\ntheir center keypoint while predicting a fixed number of polygon vertices for\neach object, thus performing detection and segmentation in parallel. Most of\nthe network parameters are shared by the network heads, making it fast and\nlightweight enough to run at real-time speed. To properly convert mask\nground-truth to polygon ground-truth, we designed a vertex selection strategy\nto facilitate the learning of the polygons. Additionally, to better segment\noverlapping objects in dense urban scenes, we also train a relative depth\nbranch to determine which instances are closer and which are further, using\navailable weak annotations. We propose several models with different backbones\nto show the possible speed / accuracy trade-offs. The models were trained and\nevaluated on Cityscapes, KITTI and IDD and the results are reported on their\npublic benchmark, which are state-of-the-art at real-time speeds. Code is\navailable at https://github.com/hu64/CenterPoly\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perreault_H/0/1/0/all/0/1\">Hughes Perreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1\">Guillaume-Alexandre Bilodeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1\">Nicolas Saunier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heritier_M/0/1/0/all/0/1\">Maguelonne H&#xe9;ritier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving 3D Object Detection with Channel-wise Transformer. (arXiv:2108.10723v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10723","description":"<p>Though 3D object detection from point clouds has achieved rapid progress in\nrecent years, the lack of flexible and high-performance proposal refinement\nremains a great hurdle for existing state-of-the-art two-stage detectors.\nPrevious works on refining 3D proposals have relied on human-designed\ncomponents such as keypoints sampling, set abstraction and multi-scale feature\nfusion to produce powerful 3D object representations. Such methods, however,\nhave limited ability to capture rich contextual dependencies among points. In\nthis paper, we leverage the high-quality region proposal network and a\nChannel-wise Transformer architecture to constitute our two-stage 3D object\ndetection framework (CT3D) with minimal hand-crafted design. The proposed CT3D\nsimultaneously performs proposal-aware embedding and channel-wise context\naggregation for the point features within each proposal. Specifically, CT3D\nuses proposal's keypoints for spatial contextual modelling and learns attention\npropagation in the encoding module, mapping the proposal to point embeddings.\nNext, a new channel-wise decoding module enriches the query-key interaction via\nchannel-wise re-weighting to effectively merge multi-level contexts, which\ncontributes to more accurate object predictions. Extensive experiments\ndemonstrate that our CT3D method has superior performance and excellent\nscalability. Remarkably, CT3D achieves the AP of 81.77% in the moderate car\ncategory on the KITTI test 3D detection benchmark, outperforms state-of-the-art\n3D detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_H/0/1/0/all/0/1\">Hualian Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Sijia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Min-Jian Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Tutorial on Learning Disentangled Representations in the Imaging Domain. (arXiv:2108.12043v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12043","description":"<p>Disentangled representation learning has been proposed as an approach to\nlearning general representations. This can be done in the absence of, or with\nlimited, annotations. A good general representation can be readily fine-tuned\nfor new target tasks using modest amounts of data, or even be used directly in\nunseen domains achieving remarkable performance in the corresponding task. This\nalleviation of the data and annotation requirements offers tantalising\nprospects for tractable and affordable applications in computer vision and\nhealthcare. Finally, disentangled representations can offer model\nexplainability and can help us understand the underlying causal relations of\nthe factors of variation, increasing their suitability for real-world\ndeployment. In this tutorial paper, we will offer an overview of the\ndisentangled representation learning, its building blocks and criteria, and\ndiscuss applications in computer vision and medical imaging. We conclude our\ntutorial by presenting the identified opportunities for the integration of\nrecent machine learning advances into disentanglement, as well as the remaining\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1\">Pedro Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison Q. O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A.Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YouRefIt: Embodied Reference Understanding with Language and Gesture. (arXiv:2109.03413v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03413","description":"<p>We study the understanding of embodied reference: One agent uses both\nlanguage and gesture to refer to an object to another agent in a shared\nphysical environment. Of note, this new visual task requires understanding\nmultimodal cues with perspective-taking to identify which object is being\nreferred to. To tackle this problem, we introduce YouRefIt, a new crowd-sourced\ndataset of embodied reference collected in various physical scenes; the dataset\ncontains 4,195 unique reference clips in 432 indoor scenes. To the best of our\nknowledge, this is the first embodied reference dataset that allows us to study\nreferring expressions in daily physical scenes to understand referential\nbehavior, human communication, and human-robot interaction. We further devise\ntwo benchmarks for image-based and video-based embodied reference\nunderstanding. Comprehensive baselines and extensive experiments provide the\nvery first result of machine perception on how the referring expressions and\ngestures affect the embodied reference understanding. Our results provide\nessential evidence that gestural cues are as critical as language cues in\nunderstanding the embodied reference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deqian Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kei_Y/0/1/0/all/0/1\">Yik Lun Kei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RobustART: Benchmarking Robustness on Architecture Design and Training Techniques. (arXiv:2109.05211v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05211","description":"<p>Deep neural networks (DNNs) are vulnerable to adversarial noises, which\nmotivates the benchmark of model robustness. Existing benchmarks mainly focus\non evaluating the defenses, but there are no comprehensive studies of how\narchitecture design and general training techniques affect robustness.\nComprehensively benchmarking their relationships will be highly beneficial for\nbetter understanding and developing robust DNNs. Thus, we propose RobustART,\nthe first comprehensive Robustness investigation benchmark on ImageNet\n(including open-source toolkit, pre-trained model zoo, datasets, and analyses)\nregarding ARchitecture design (44 human-designed off-the-shelf architectures\nand 1200+ networks from neural architecture search) and Training techniques\n(10+ general techniques, e.g., data augmentation) towards diverse noises\n(adversarial, natural, and system noises). Extensive experiments revealed and\nsubstantiated several insights for the first time, for example: (1) adversarial\ntraining largely improves the clean accuracy and all types of robustness for\nTransformers and MLP-Mixers; (2) with comparable sizes, CNNs &gt; Transformers &gt;\nMLP-Mixers on robustness against natural and system noises; Transformers &gt;\nMLP-Mixers &gt; CNNs on adversarial robustness; (3) for some light-weight\narchitectures (e.g., EfficientNet, MobileNetV2, and MobileNetV3), increasing\nmodel sizes or using extra training data cannot improve robustness. Our\nbenchmark <a href=\"http://robust.art/\">this http URL</a> : (1) presents an open-source platform for\nconducting comprehensive evaluation on diverse robustness types; (2) provides a\nvariety of pre-trained models with different training techniques to facilitate\nrobustness evaluation; (3) proposes a new view to better understand the\nmechanism towards designing robust DNN architectures, backed up by the\nanalysis. We will continuously contribute to building this ecosystem for the\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shiyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aishan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiakai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLFW: A Database for Face Recognition on Masked Faces. (arXiv:2109.05804v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05804","description":"<p>As more and more people begin to wear masks due to current COVID-19 pandemic,\nexisting face recognition systems may encounter severe performance degradation\nwhen recognizing masked faces. To figure out the impact of masks on face\nrecognition model, we build a simple but effective tool to generate masked\nfaces from unmasked faces automatically, and construct a new database called\nMasked LFW (MLFW) based on Cross-Age LFW (CALFW) database. The mask on the\nmasked face generated by our method has good visual consistency with the\noriginal face. Moreover, we collect various mask templates, covering most of\nthe common styles appeared in the daily life, to achieve diverse generation\neffects. Considering realistic scenarios, we design three kinds of combinations\nof face pairs. The recognition accuracy of SOTA models declines 5%-16% on MLFW\ndatabase compared with the accuracy on the original images. MLFW database can\nbe viewed and downloaded at \\url{<a href=\"http://whdeng.cn/mlfw\">this http URL</a>}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yaoyao Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Few-shot Segmentation by Redefinition of the Roles of Multi-level CNN Features. (arXiv:2109.06432v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06432","description":"<p>This study is concerned with few-shot segmentation, i.e., segmenting the\nregion of an unseen object class in a query image, given support image(s) of\nits instances. The current methods rely on the pretrained CNN features of the\nsupport and query images. The key to good performance depends on the proper\nfusion of their mid-level and high-level features; the former contains\nshape-oriented information, while the latter has class-oriented information.\nCurrent state-of-the-art methods follow the approach of Tian et al., which\ngives the mid-level features the primary role and the high-level features the\nsecondary role. In this paper, we reinterpret this widely employed approach by\nredifining the roles of the multi-level features; we swap the primary and\nsecondary roles. Specifically, we regard that the current methods improve the\ninitial estimate generated from the high-level features using the mid-level\nfeatures. This reinterpretation suggests a new application of the current\nmethods: to apply the same network multiple times to iteratively update the\nestimate of the object's region, starting from its initial estimate. Our\nexperiments show that this method is effective and has updated the previous\nstate-of-the-art on COCO-20$^i$ in the 1-shot and 5-shot settings and on\nPASCAL-5$^i$ in the 1-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1\">Masanori Suganuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">Takayuki Okatani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Fidelity GAN Inversion for Image Attribute Editing. (arXiv:2109.06590v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06590","description":"<p>We present a novel high-fidelity generative adversarial network (GAN)\ninversion framework that enables attribute editing with image-specific details\nwell-preserved (e.g., background, appearance and illumination). We first\nformulate GAN inversion as a lossy data compression problem and carefully\ndiscuss the Rate-Distortion-Edit trade-off. Due to this trade-off, previous\nworks fail to achieve high-fidelity reconstruction while keeping compelling\nediting ability with a low bit-rate latent code only. In this work, we propose\na distortion consultation approach that employs the distortion map as a\nreference for reconstruction. In the distortion consultation inversion (DCI),\nthe distortion map is first projected to a high-rate latent map, which then\ncomplements the basic low-rate latent code with (lost) details via consultation\nfusion. To achieve high-fidelity editing, we propose an adaptive distortion\nalignment (ADA) module with a self-supervised training scheme. Extensive\nexperiments in the face and car domains show a clear improvement in terms of\nboth inversion and editing quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sampling Network Guided Cross-Entropy Method for Unsupervised Point Cloud Registration. (arXiv:2109.06619v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06619","description":"<p>In this paper, by modeling the point cloud registration task as a Markov\ndecision process, we propose an end-to-end deep model embedded with the\ncross-entropy method (CEM) for unsupervised 3D registration. Our model consists\nof a sampling network module and a differentiable CEM module. In our sampling\nnetwork module, given a pair of point clouds, the sampling network learns a\nprior sampling distribution over the transformation space. The learned sampling\ndistribution can be used as a \"good\" initialization of the differentiable CEM\nmodule. In our differentiable CEM module, we first propose a maximum consensus\ncriterion based alignment metric as the reward function for the point cloud\nregistration task. Based on the reward function, for each state, we then\nconstruct a fused score function to evaluate the sampled transformations, where\nwe weight the current and future rewards of the transformations. Particularly,\nthe future rewards of the sampled transforms are obtained by performing the\niterative closest point (ICP) algorithm on the transformed state. By selecting\nthe top-k transformations with the highest scores, we iteratively update the\nsampling distribution. Furthermore, in order to make the CEM differentiable, we\nuse the sparsemax function to replace the hard top-$k$ selection. Finally, we\nformulate a Geman-McClure estimator based loss to train our end-to-end\nregistration model. Extensive experimental results demonstrate the good\nregistration performance of our method on benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haobo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yaqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jianjun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Discrete Wavelet Pooling (LDW-Pooling) For Convolutional Networks. (arXiv:2109.06638v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06638","description":"<p>Pooling is a simple but essential layer in modern deep CNN architectures for\nfeature aggregation and extraction. Typical CNN design focuses on the conv\nlayers and activation functions, while leaving the pooling layers with fewer\noptions. We introduce the Learning Discrete Wavelet Pooling (LDW-Pooling) that\ncan be applied universally to replace standard pooling operations to better\nextract features with improved accuracy and efficiency. Motivated from the\nwavelet theory, we adopt the low-pass (L) and high-pass (H) filters\nhorizontally and vertically for pooling on a 2D feature map. Feature signals\nare decomposed into four (LL, LH, HL, HH) subbands to retain features better\nand avoid information dropping. The wavelet transform ensures features after\npooling can be fully preserved and recovered. We next adopt an energy-based\nattention learning to fine-select crucial and representative features.\nLDW-Pooling is effective and efficient when compared with other\nstate-of-the-art pooling techniques such as WaveletPooling and LiftPooling.\nExtensive experimental validation shows that LDW-Pooling can be applied to a\nwide range of standard CNN architectures and consistently outperform standard\n(max, mean, mixed, and stochastic) pooling operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1\">Jun-Wei Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bor-Shiun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping-Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Lipeng Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying partial mouse brain microscopy images from Allen reference atlas using a contrastively learned semantic space. (arXiv:2109.06662v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06662","description":"<p>Precise identification of mouse brain microscopy images is a crucial first\nstep when anatomical structures in the mouse brain are to be registered to a\nreference atlas. Practitioners usually rely on manual comparison of images or\ntools that assume the presence of complete images. This work explores Siamese\nNetworks as the method for finding corresponding 2D reference atlas plates for\ngiven partial 2D mouse brain images. Siamese networks are a class of\nconvolutional neural networks (CNNs) that use weight-shared paths to obtain low\ndimensional embeddings of pairs of input images. The correspondence between the\npartial mouse brain image and reference atlas plate is determined based on the\ndistance between low dimensional embeddings of brain slices and atlas plates\nthat are obtained from Siamese networks using contrastive learning. Experiments\nshowed that Siamese CNNs can precisely identify brain slices using the Allen\nmouse brain atlas when training and testing images come from the same source.\nThey achieved TOP-1 and TOP-5 accuracy of 25% and 100%, respectively, taking\nonly 7.2 seconds to identify 29 images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antanavicius_J/0/1/0/all/0/1\">Justinas Antanavicius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leiras_R/0/1/0/all/0/1\">Roberto Leiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvan_R/0/1/0/all/0/1\">Raghavendra Selvan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MotionHint: Self-Supervised Monocular Visual Odometry with Motion Constraints. (arXiv:2109.06768v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06768","description":"<p>We present a novel self-supervised algorithm named MotionHint for monocular\nvisual odometry (VO) that takes motion constraints into account. A key aspect\nof our approach is to use an appropriate motion model that can help existing\nself-supervised monocular VO (SSM-VO) algorithms to overcome issues related to\nthe local minima within their self-supervised loss functions. The motion model\nis expressed with a neural network named PPnet. It is trained to coarsely\npredict the next pose of the camera and the uncertainty of this prediction. Our\nself-supervised approach combines the original loss and the motion loss, which\nis the weighted difference between the prediction and the generated ego-motion.\nTaking two existing SSM-VO systems as our baseline, we evaluate our MotionHint\nalgorithm on the standard KITTI benchmark. Experimental results show that our\nMotionHint algorithm can be easily applied to existing open-sourced\nstate-of-the-art SSM-VO systems to greatly improve the performance by reducing\nthe resulting ATE by up to 28.73%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptive Learning via Synthetic Data for Person Re-identification. (arXiv:2109.05542v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2109.05542","description":"<p>Person re-identification (re-ID) has gained more and more attention due to\nits widespread applications in intelligent video surveillance. Unfortunately,\nthe mainstream deep learning methods still need a large quantity of labeled\ndata to train models, and annotating data is an expensive work in real-world\nscenarios. In addition, due to domain gaps between different datasets, the\nperformance is dramatically decreased when re-ID models pre-trained on\nlabel-rich datasets (source domain) are directly applied to other unlabeled\ndatasets (target domain). In this paper, we attempt to remedy these problems\nfrom two aspects, namely data and methodology. Firstly, we develop a data\ncollector to automatically generate synthetic re-ID samples in a computer game,\nand construct a data labeler to simultaneously annotate them, which free humans\nfrom heavy data collections and annotations. Based on them, we build two\nsynthetic person re-ID datasets with different scales, \"GSPR\" and \"mini-GSPR\"\ndatasets. Secondly, we propose a synthesis-based multi-domain collaborative\nrefinement (SMCR) network, which contains a synthetic pretraining module and\ntwo collaborative-refinement modules to implement sufficient learning for the\nvaluable knowledge from multiple domains. Extensive experiments show that our\nproposed framework obtains significant performance improvements over the\nstate-of-the-art methods on multiple unsupervised domain adaptation tasks of\nperson re-ID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Sikai Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}