{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer. (arXiv:2109.03819v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03819","description":"<p>We study Comparative Preference Classification (CPC) which aims at predicting\nwhether a preference comparison exists between two entities in a given sentence\nand, if so, which entity is preferred over the other. High-quality CPC models\ncan significantly benefit applications such as comparative question answering\nand review-based recommendations. Among the existing approaches, non-deep\nlearning methods suffer from inferior performances. The state-of-the-art graph\nneural network-based ED-GAT (Ma et al., 2020) only considers syntactic\ninformation while ignoring the critical semantic relations and the sentiments\nto the compared entities. We proposed sentiment Analysis Enhanced COmparative\nNetwork (SAECON) which improves CPC ac-curacy with a sentiment analyzer that\nlearns sentiments to individual entities via domain adaptive knowledge\ntransfer. Experiments on the CompSent-19 (Panchenko et al., 2019) dataset\npresent a significant improvement on the F1 scores over the best existing CPC\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yilong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge mining of unstructured information: application to cyber-domain. (arXiv:2109.03848v1 [cs.CR])","link":"http://arxiv.org/abs/2109.03848","description":"<p>Cyber intelligence is widely and abundantly available in numerous open online\nsources with reports on vulnerabilities and incidents. This constant stream of\nnoisy information requires new tools and techniques if it is to be used for the\nbenefit of analysts and investigators in various organizations. In this paper\nwe present and implement a novel knowledge graph and knowledge mining framework\nfor extracting relevant information from free-form text about incidents in the\ncyber domain. Our framework includes a machine learning based pipeline as well\nas crawling methods for generating graphs of entities, attackers and the\nrelated information with our non-technical cyber ontology. We test our\nframework on publicly available cyber incident datasets to evaluate the\naccuracy of our knowledge mining methods as well as the usefulness of the\nframework in the use of cyber analysts. Our results show analyzing the\nknowledge graph constructed using the novel framework, an analyst can infer\nadditional information from the current cyber landscape in terms of risk to\nvarious entities and the propagation of risk between industries and countries.\nExpanding the framework to accommodate more technical and operational level\ninformation can increase the accuracy and explainability of trends and risk in\nthe knowledge graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takko_T/0/1/0/all/0/1\">Tuomas Takko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_K/0/1/0/all/0/1\">Kunal Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehto_M/0/1/0/all/0/1\">Martti Lehto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalasvirta_P/0/1/0/all/0/1\">Pertti Jalasvirta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cederberg_A/0/1/0/all/0/1\">Aapo Cederberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_K/0/1/0/all/0/1\">Kimmo Kaski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bayesian Framework for Information-Theoretic Probing. (arXiv:2109.03853v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03853","description":"<p>Pimentel et al. (2020) recently analysed probing from an\ninformation-theoretic perspective. They argue that probing should be seen as\napproximating a mutual information. This led to the rather unintuitive\nconclusion that representations encode exactly the same information about a\ntarget task as the original sentences. The mutual information, however, assumes\nthe true probability distribution of a pair of random variables is known,\nleading to unintuitive results in settings where it is not. This paper proposes\na new framework to measure what we term Bayesian mutual information, which\nanalyses information from the perspective of Bayesian agents -- allowing for\nmore intuitive findings in scenarios with finite data. For instance, under\nBayesian MI we have that data can add information, processing can help, and\ninformation can hurt, which makes it more intuitive for machine learning\napplications. Finally, we apply our framework to probing where we believe\nBayesian mutual information naturally operationalises ease of extraction by\nexplicitly limiting the available background knowledge to solve a task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation. (arXiv:2109.03858v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03858","description":"<p>Recent works have found evidence of gender bias in models of machine\ntranslation and coreference resolution using mostly synthetic diagnostic\ndatasets. While these quantify bias in a controlled experiment, they often do\nso on a small scale and consist mostly of artificial, out-of-distribution\nsentences. In this work, we find grammatical patterns indicating stereotypical\nand non-stereotypical gender-role assignments (e.g., female nurses versus male\ndancers) in corpora from three domains, resulting in a first large-scale gender\nbias dataset of 108K diverse real-world English sentences. We manually verify\nthe quality of our corpus and use it to evaluate gender bias in various\ncoreference resolution and machine translation models. We find that all tested\nmodels tend to over-rely on gender stereotypes when presented with natural\ninputs, which may be especially harmful when deployed in commercial systems.\nFinally, we show that our dataset lends itself to finetuning a coreference\nresolution model, finding it mitigates bias on a held out set. Our dataset and\nmodels are publicly available at www.github.com/SLAB-NLP/BUG. We hope they will\nspur future research into gender bias evaluation mitigation techniques in\nrealistic settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Shahar Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazar_K/0/1/0/all/0/1\">Koren Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_a/0/1/0/all/0/1\">abriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems. (arXiv:2109.03888v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03888","description":"<p>Transformer models have achieved state-of-the-art results in a wide range of\nNLP tasks including summarization. Training and inference using large\ntransformer models can be computationally expensive. Previous work has focused\non one important bottleneck, the quadratic self-attention mechanism in the\nencoder. Modified encoder architectures such as LED or LoBART use local\nattention patterns to address this problem for summarization. In contrast, this\nwork focuses on the transformer's encoder-decoder attention mechanism. The cost\nof this attention becomes more significant in inference or training approaches\nthat require model-generated histories. First, we examine the complexity of the\nencoder-decoder attention. We demonstrate empirically that there is a sparse\nsentence structure in document summarization that can be exploited by\nconstraining the attention mechanism to a subset of input sentences, whilst\nmaintaining system performance. Second, we propose a modified architecture that\nselects the subset of sentences to constrain the encoder-decoder attention.\nExperiments are carried out on abstractive summarization tasks, including\nCNN/DailyMail, XSum, Spotify Podcast, and arXiv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manakul_P/0/1/0/all/0/1\">Potsawee Manakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models. (arXiv:2109.03892v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03892","description":"<p>We investigate the use of multimodal information contained in images as an\neffective method for enhancing the commonsense of Transformer models for text\ngeneration. We perform experiments using BART and T5 on concept-to-text\ngeneration, specifically the task of generative commonsense reasoning, or\nCommonGen. We call our approach VisCTG: Visually Grounded Concept-to-Text\nGeneration. VisCTG involves captioning images representing appropriate everyday\nscenarios, and using these captions to enrich and steer the generation process.\nComprehensive evaluation and analysis demonstrate that VisCTG noticeably\nimproves model performance while successfully addressing several issues of the\nbaseline generations, including poor commonsense, fluency, and specificity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kevin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhuofu Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELIT: Emory Language and Information Toolkit. (arXiv:2109.03903v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03903","description":"<p>We introduce ELIT, the Emory Language and Information Toolkit, which is a\ncomprehensive NLP framework providing transformer-based end-to-end models for\ncore tasks with a special focus on memory efficiency while maintaining\nstate-of-the-art accuracy and speed. Compared to existing toolkits, ELIT\nfeatures an efficient Multi-Task Learning (MTL) model with many downstream\ntasks that include lemmatization, part-of-speech tagging, named entity\nrecognition, dependency parsing, constituency parsing, semantic role labeling,\nand AMR parsing. The backbone of ELIT's MTL framework is a pre-trained\ntransformer encoder that is shared across tasks to speed up their inference.\nELIT provides pre-trained models developed on a remix of eight datasets. To\nscale up its service, ELIT also integrates a RESTful Client/Server combination.\nOn the server side, ELIT extends its functionality to cover other tasks such as\ntokenization and coreference resolution, providing an end user with agile\nresearch experience. All resources including the source codes, documentation,\nand pre-trained models are publicly available at\nhttps://github.com/emorynlp/elit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Han He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Recipe For Arbitrary Text Style Transfer with Large Language Models. (arXiv:2109.03910v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03910","description":"<p>In this paper, we leverage large language models (LMs) to perform zero-shot\ntext style transfer. We present a prompting method that we call augmented\nzero-shot learning, which frames style transfer as a sentence rewriting task\nand requires only a natural language instruction, without model fine-tuning or\nexemplars in the target style. Augmented zero-shot learning is simple and\ndemonstrates promising results not just on standard style transfer tasks such\nas sentiment, but also on arbitrary transformations such as \"make this\nmelodramatic\" or \"insert a metaphor.\"\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Ann Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1\">Andy Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble Fine-tuned mBERT for Translation Quality Estimation. (arXiv:2109.03914v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03914","description":"<p>Quality Estimation (QE) is an important component of the machine translation\nworkflow as it assesses the quality of the translated output without consulting\nreference translations. In this paper, we discuss our submission to the WMT\n2021 QE Shared Task. We participate in Task 2 sentence-level sub-task that\nchallenge participants to predict the HTER score for sentence-level\npost-editing effort. Our proposed system is an ensemble of multilingual BERT\n(mBERT)-based regression models, which are generated by fine-tuning on\ndifferent input settings. It demonstrates comparable performance with respect\nto the Pearson's correlation and beats the baseline system in MAE/ RMSE for\nseveral language pairs. In addition, we adapt our system for the zero-shot\nsetting by exploiting target language-relevant language pairs and\npseudo-reference translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shaika Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baili_N/0/1/0/all/0/1\">Naouel Baili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vannah_B/0/1/0/all/0/1\">Brian Vannah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers in the loop: Polarity in neural models of language. (arXiv:2109.03926v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03926","description":"<p>Representation of linguistic phenomena in computational language models is\ntypically assessed against the predictions of existing linguistic theories of\nthese phenomena. Using the notion of polarity as a case study, we show that\nthis is not always the most adequate set-up. We probe polarity via so-called\n'negative polarity items' (in particular, English 'any') in two pre-trained\nTransformer-based models (BERT and GPT-2). We show that -- at least for\npolarity -- metrics derived from language models are more consistent with data\nfrom psycholinguistic experiments than linguistic theory predictions.\nEstablishing this allows us to more adequately evaluate the performance of\nlanguage models and also to use language models to discover new insights into\nnatural language grammar beyond existing linguistic theories. Overall, our\nresults encourage a closer tie between experiments with human subjects and with\nlanguage models. We propose methods to enable this closer tie, with language\nmodels as part of experimental pipeline, and show this pipeline at work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bylinina_L/0/1/0/all/0/1\">Lisa Bylinina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's Hidden in a One-layer Randomly Weighted Transformer?. (arXiv:2109.03939v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03939","description":"<p>We demonstrate that, hidden within one-layer randomly weighted neural\nnetworks, there exist subnetworks that can achieve impressive performance,\nwithout ever modifying the weight initializations, on machine translation\ntasks. To find subnetworks for one-layer randomly weighted neural networks, we\napply different binary masks to the same weight matrix to generate different\nlayers. Hidden within a one-layer randomly weighted Transformer, we find that\nsubnetworks that can achieve 29.45/17.29 BLEU on IWSLT14/WMT14. Using a fixed\npre-trained embedding layer, the previously found subnetworks are smaller than,\nbut can match 98%/92% (34.14/25.24 BLEU) of the performance of, a trained\nTransformer small/base on IWSLT14/WMT14. Furthermore, we demonstrate the\neffectiveness of larger and deeper transformers in this setting, as well as the\nimpact of different initialization methods. We released the source code at\nhttps://github.com/sIncerass/one_layer_lottery_ticket.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Pre-training with Structured Knowledge for Improving Natural Language Inference. (arXiv:2109.03941v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03941","description":"<p>While recent research on natural language inference has considerably\nbenefited from large annotated datasets, the amount of inference-related\nknowledge (including commonsense) provided in the annotated data is still\nrather limited. There have been two lines of approaches that can be used to\nfurther address the limitation: (1) unsupervised pretraining can leverage\nknowledge in much larger unstructured text data; (2) structured (often\nhuman-curated) knowledge has started to be considered in neural-network-based\nmodels for NLI. An immediate question is whether these two approaches\ncomplement each other, or how to develop models that can bring together their\nadvantages. In this paper, we propose models that leverage structured knowledge\nin different components of pre-trained models. Our results show that the\nproposed models perform better than previous BERT-based state-of-the-art\nmodels. Although our models are proposed for NLI, they can be easily extended\nto other sentence or sentence-pair classification problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianda Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Formal Description of Sorani Kurdish Morphology. (arXiv:2109.03942v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03942","description":"<p>Sorani Kurdish, also known as Central Kurdish, has a complex morphology,\nparticularly due to the patterns in which morphemes appear. Although several\naspects of Kurdish morphology have been studied, such as pronominal endoclitics\nand Izafa constructions, Sorani Kurdish morphology has received trivial\nattention in computational linguistics. Moreover, some morphemes, such as the\nemphasis endoclitic =\\^i\\c{s}, and derivational morphemes have not been\npreviously studied. To tackle the complex morphology of Sorani, we provide a\nthorough description of Sorani Kurdish morphological and morphophonological\nconstructions in a formal way such that they can be used as finite-state\ntransducers for morphological analysis and synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1\">Sina Ahmadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Speech Recognition for Low-Resource Indian Languages using Multi-Task conformer. (arXiv:2109.03969v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03969","description":"<p>Transformers have recently become very popular for sequence-to-sequence\napplications such as machine translation and speech recognition. In this work,\nwe propose a multi-task learning-based transformer model for low-resource\nmultilingual speech recognition for Indian languages. Our proposed model\nconsists of a conformer [1] encoder and two parallel transformer decoders. We\nuse a phoneme decoder (PHN-DEC) for the phoneme recognition task and a grapheme\ndecoder (GRP-DEC) to predict grapheme sequence. We consider the phoneme\nrecognition task as an auxiliary task for our multi-task learning framework. We\njointly optimize the network for both phoneme and grapheme recognition tasks\nusing Joint CTC-Attention [2] training. We use a conditional decoding scheme to\ninject the language information into the model before predicting the grapheme\nsequence. Our experiments show that our proposed approach can obtain\nsignificant improvement over previous approaches [4]. We also show that our\nconformer-based dual-decoder approach outperforms both the transformer-based\ndual-decoder approach and single decoder approach. Finally, We compare\nmonolingual ASR models with our proposed multilingual ASR approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+N_K/0/1/0/all/0/1\">Krishna D N</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Competence-based Curriculum Learning for Multilingual Machine Translation. (arXiv:2109.04002v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04002","description":"<p>Currently, multilingual machine translation is receiving more and more\nattention since it brings better performance for low resource languages (LRLs)\nand saves more space. However, existing multilingual machine translation models\nface a severe challenge: imbalance. As a result, the translation performance of\ndifferent languages in multilingual translation models are quite different. We\nargue that this imbalance problem stems from the different learning\ncompetencies of different languages. Therefore, we focus on balancing the\nlearning competencies of different languages and propose Competence-based\nCurriculum Learning for Multilingual Machine Translation, named CCL-M.\nSpecifically, we firstly define two competencies to help schedule the high\nresource languages (HRLs) and the low resource languages: 1) Self-evaluated\nCompetence, evaluating how well the language itself has been learned; and 2)\nHRLs-evaluated Competence, evaluating whether an LRL is ready to be learned\naccording to HRLs' Self-evaluated Competence. Based on the above competencies,\nwe utilize the proposed CCL-M algorithm to gradually add new languages into the\ntraining set in a curriculum learning manner. Furthermore, we propose a novel\ncompetenceaware dynamic balancing sampling strategy for better selecting\ntraining samples in multilingual training. Experimental results show that our\napproach has achieved a steady and significant performance gain compared to the\nprevious state-of-the-art approach on the TED talks dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Based Network with Contextualized Representations of Turns in Dialogue. (arXiv:2109.04008v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04008","description":"<p>Dialogue-based relation extraction (RE) aims to extract relation(s) between\ntwo arguments that appear in a dialogue. Because dialogues have the\ncharacteristics of high personal pronoun occurrences and low information\ndensity, and since most relational facts in dialogues are not supported by any\nsingle sentence, dialogue-based relation extraction requires a comprehensive\nunderstanding of dialogue. In this paper, we propose the TUrn COntext awaRE\nGraph Convolutional Network (TUCORE-GCN) modeled by paying attention to the way\npeople understand dialogues. In addition, we propose a novel approach which\ntreats the task of emotion recognition in conversations (ERC) as a\ndialogue-based RE. Experiments on a dialogue-based RE dataset and three ERC\ndatasets demonstrate that our model is very effective in various dialogue-based\nnatural language understanding tasks. In these experiments, TUCORE-GCN\noutperforms the state-of-the-art models on most of the benchmark datasets. Our\ncode is available at https://github.com/BlackNoodle/TUCORE-GCN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bongseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yong Suk Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering. (arXiv:2109.04014v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04014","description":"<p>Knowledge-based visual question answering (VQA) requires answering questions\nwith external knowledge in addition to the content of images. One dataset that\nis mostly used in evaluating knowledge-based VQA is OK-VQA, but it lacks a gold\nstandard knowledge corpus for retrieval. Existing work leverage different\nknowledge bases (e.g., ConceptNet and Wikipedia) to obtain external knowledge.\nBecause of varying knowledge bases, it is hard to fairly compare models'\nperformance. To address this issue, we collect a natural language knowledge\nbase that can be used for any VQA system. Moreover, we propose a Visual\nRetriever-Reader pipeline to approach knowledge-based VQA. The visual retriever\naims to retrieve relevant knowledge, and the visual reader seeks to predict\nanswers based on given knowledge. We introduce various ways to retrieve\nknowledge using text and images and two reader styles: classification and\nextraction. Both the retriever and reader are trained with weak supervision.\nOur experimental results show that a good retriever can significantly improve\nthe reader's performance on the OK-VQA challenge. The code and corpus are\nprovided in https://github.com/luomancs/retriever\\_reader\\_for\\_okvqa.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yankai Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graphine: A Dataset for Graph-aware Terminology Definition Generation. (arXiv:2109.04018v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04018","description":"<p>Precisely defining the terminology is the first step in scientific\ncommunication. Developing neural text generation models for definition\ngeneration can circumvent the labor-intensity curation, further accelerating\nscientific discovery. Unfortunately, the lack of large-scale terminology\ndefinition dataset hinders the process toward definition generation. In this\npaper, we present a large-scale terminology definition dataset Graphine\ncovering 2,010,648 terminology definition pairs, spanning 227 biomedical\nsubdisciplines. Terminologies in each subdiscipline further form a directed\nacyclic graph, opening up new avenues for developing graph-aware text\ngeneration models. We then proposed a novel graph-aware definition generation\nmodel Graphex that integrates transformer with graph neural network. Our model\noutperforms existing text generation models by exploiting the graph structure\nof terminologies. We further demonstrated how Graphine can be used to evaluate\npretrained language models, compare graph representation learning methods and\npredict sentence granularity. We envision Graphine to be a unique resource for\ndefinition generation and many other NLP tasks in biomedicine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zequn Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shukai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yiyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributionally Robust Multilingual Machine Translation. (arXiv:2109.04020v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04020","description":"<p>Multilingual neural machine translation (MNMT) learns to translate multiple\nlanguage pairs with a single model, potentially improving both the accuracy and\nthe memory-efficiency of deployed models. However, the heavy data imbalance\nbetween languages hinders the model from performing uniformly across language\npairs. In this paper, we propose a new learning objective for MNMT based on\ndistributionally robust optimization, which minimizes the worst-case expected\nloss over the set of language pairs. We further show how to practically\noptimize this objective for large translation corpora using an iterated best\nresponse scheme, which is both effective and incurs negligible additional\ncomputational cost compared to standard empirical risk minimization. We perform\nextensive experiments on three sets of languages from two datasets and show\nthat our method consistently outperforms strong baseline methods in terms of\naverage and per-language performance under both many-to-one and one-to-many\ntranslation settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_D/0/1/0/all/0/1\">Daniel Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1\">Marjan Ghazvininejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Table-based Fact Verification with Salience-aware Learning. (arXiv:2109.04053v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04053","description":"<p>Tables provide valuable knowledge that can be used to verify textual\nstatements. While a number of works have considered table-based fact\nverification, direct alignments of tabular data with tokens in textual\nstatements are rarely available. Moreover, training a generalized fact\nverification model requires abundant labeled training data. In this paper, we\npropose a novel system to address these problems. Inspired by counterfactual\ncausality, our system identifies token-level salience in the statement with\nprobing-based salience estimation. Salience estimation allows enhanced learning\nof fact verification from two perspectives. From one perspective, our system\nconducts masked salient token prediction to enhance the model for alignment and\nreasoning between the table and the statement. From the other perspective, our\nsystem applies salience-aware data augmentation to generate a more diverse set\nof training instances by replacing non-salient terms. Experimental results on\nTabFact show the effective improvement by the proposed salience-aware learning\ntechniques, leading to the new SOTA performance on the benchmark. Our code is\npublicly available at https://github.com/luka-group/Salience-aware-Learning .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kexuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szekely_P/0/1/0/all/0/1\">Pedro Szekely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Speaker-aware Multi-party Multi-turn Dialogue Comprehension. (arXiv:2109.04066v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04066","description":"<p>Multi-party multi-turn dialogue comprehension brings unprecedented challenges\non handling the complicated scenarios from multiple speakers and criss-crossed\ndiscourse relationship among speaker-aware utterances. Most existing methods\ndeal with dialogue contexts as plain texts and pay insufficient attention to\nthe crucial speaker-aware clues. In this work, we propose an enhanced\nspeaker-aware model with masking attention and heterogeneous graph networks to\ncomprehensively capture discourse clues from both sides of speaker property and\nspeaker-aware relationships. With such comprehensive speaker-aware modeling,\nexperimental results show that our speaker-aware model helps achieves\nstate-of-the-art performance on the benchmark dataset Molweni. Case analysis\nshows that our model enhances the connections between utterances and their own\nspeakers and captures the speaker-aware discourse relations, which are critical\nfor dialogue modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinbei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining. (arXiv:2109.04080v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04080","description":"<p>With the rapid increase in the volume of dialogue data from daily life, there\nis a growing demand for dialogue summarization. Unfortunately, training a large\nsummarization model is generally infeasible due to the inadequacy of dialogue\ndata with annotated summaries. Most existing works for low-resource dialogue\nsummarization directly pretrain models in other domains, e.g., the news domain,\nbut they generally neglect the huge difference between dialogues and\nconventional articles. To bridge the gap between out-of-domain pretraining and\nin-domain fine-tuning, in this work, we propose a multi-source pretraining\nparadigm to better leverage the external summary data. Specifically, we exploit\nlarge-scale in-domain non-summary data to separately pretrain the dialogue\nencoder and the summary decoder. The combined encoder-decoder model is then\npretrained on the out-of-domain summary data using adversarial critics, aiming\nto facilitate domain-agnostic summarization. The experimental results on two\npublic datasets show that with only limited training data, our approach\nachieves competitive performance and generalizes well in different dialogue\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bolin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xingwu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thinking Clearly, Talking Fast: Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems. (arXiv:2109.04084v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04084","description":"<p>Human dialogue contains evolving concepts, and speakers naturally associate\nmultiple concepts to compose a response. However, current dialogue models with\nthe seq2seq framework lack the ability to effectively manage concept\ntransitions and can hardly introduce multiple concepts to responses in a\nsequential decoding manner. To facilitate a controllable and coherent dialogue,\nin this work, we devise a concept-guided non-autoregressive model (CG-nAR) for\nopen-domain dialogue generation. The proposed model comprises a multi-concept\nplanning module that learns to identify multiple associated concepts from a\nconcept graph and a customized Insertion Transformer that performs\nconcept-guided non-autoregressive generation to complete a response. The\nexperimental results on two public datasets show that CG-nAR can produce\ndiverse and coherent responses, outperforming state-of-the-art baselines in\nboth automatic and human evaluations with substantially faster inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhihua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xingwu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiasing Methods in Natural Language Understanding Make Bias More Accessible. (arXiv:2109.04095v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04095","description":"<p>Model robustness to bias is often determined by the generalization on\ncarefully designed out-of-distribution datasets. Recent debiasing methods in\nnatural language understanding (NLU) improve performance on such datasets by\npressuring models into making unbiased predictions. An underlying assumption\nbehind such methods is that this also leads to the discovery of more robust\nfeatures in the model's inner representations. We propose a general\nprobing-based framework that allows for post-hoc interpretation of biases in\nlanguage models, and use an information-theoretic approach to measure the\nextractability of certain biases from the model's representations. We\nexperiment with several NLU datasets and known biases, and show that,\ncounter-intuitively, the more a language model is pushed towards a debiased\nregime, the more bias is actually encoded in its inner representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mendelson_M/0/1/0/all/0/1\">Michael Mendelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation. (arXiv:2109.04096v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04096","description":"<p>Neural conversation models have shown great potentials towards generating\nfluent and informative responses by introducing external background knowledge.\nNevertheless, it is laborious to construct such knowledge-grounded dialogues,\nand existing models usually perform poorly when transfer to new domains with\nlimited training samples. Therefore, building a knowledge-grounded dialogue\nsystem under the low-resource setting is a still crucial issue. In this paper,\nwe propose a novel three-stage learning framework based on weakly supervised\nlearning which benefits from large scale ungrounded dialogues and unstructured\nknowledge base. To better cooperate with this framework, we devise a variant of\nTransformer with decoupled decoder which facilitates the disentangled learning\nof response generation and knowledge incorporation. Evaluation results on two\nbenchmarks indicate that our approach can outperform other state-of-the-art\nmethods with less training data, and even in zero-resource scenario, our\napproach still performs well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bochao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Feiliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1\">Shujuan Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARMAN: Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization. (arXiv:2109.04098v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04098","description":"<p>Abstractive text summarization is one of the areas influenced by the\nemergence of pre-trained language models. Current pre-training works in\nabstractive summarization give more points to the summaries with more words in\ncommon with the main text and pay less attention to the semantic similarity\nbetween generated sentences and the original document. We propose ARMAN, a\nTransformer-based encoder-decoder model pre-trained with three novel objectives\nto address this issue. In ARMAN, salient sentences from a document are selected\naccording to a modified semantic score to be masked and form a pseudo summary.\nTo summarize more accurately and similar to human writing patterns, we applied\nmodified sentence reordering. We evaluated our proposed models on six\ndownstream Persian summarization tasks. Experimental results show that our\nproposed model achieves state-of-the-art performance on all six summarization\ntasks measured by ROUGE and BERTScore. Our models also outperform prior works\nin textual entailment, question paraphrasing, and multiple choice question\nanswering. Finally, we established a human evaluation and show that using the\nsemantic score significantly improves summarization results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salemi_A/0/1/0/all/0/1\">Alireza Salemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kebriaei_E/0/1/0/all/0/1\">Emad Kebriaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minaei_G/0/1/0/all/0/1\">Ghazal Neisi Minaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakery_A/0/1/0/all/0/1\">Azadeh Shakery</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting. (arXiv:2109.04101v1 [cs.LG])","link":"http://arxiv.org/abs/2109.04101","description":"<p>Temporal knowledge graph (TKG) reasoning is a crucial task that has gained\nincreasing research interest in recent years. Most existing methods focus on\nreasoning at past timestamps to complete the missing facts, and there are only\na few works of reasoning on known TKGs to forecast future facts. Compared with\nthe completion task, the forecasting task is more difficult that faces two main\nchallenges: (1) how to effectively model the time information to handle future\ntimestamps? (2) how to make inductive inference to handle previously unseen\nentities that emerge over time? To address these challenges, we propose the\nfirst reinforcement learning method for forecasting. Specifically, the agent\ntravels on historical knowledge graph snapshots to search for the answer. Our\nmethod defines a relative time encoding function to capture the timespan\ninformation, and we design a novel time-shaped reward based on Dirichlet\ndistribution to guide the model learning. Furthermore, we propose a novel\nrepresentation method for unseen entities to improve the inductive inference\nability of the model. We evaluate our method for this link prediction task at\nfuture timestamps. Extensive experiments on four benchmark datasets demonstrate\nsubstantial performance improvement meanwhile with higher explainability, less\ncalculation, and fewer parameters when compared with existing state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haohai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jialun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunpu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MapRE: An Effective Semantic Mapping Approach for Low-resource Relation Extraction. (arXiv:2109.04108v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04108","description":"<p>Neural relation extraction models have shown promising results in recent\nyears; however, the model performance drops dramatically given only a few\ntraining samples. Recent works try leveraging the advance in few-shot learning\nto solve the low resource problem, where they train label-agnostic models to\ndirectly compare the semantic similarities among context sentences in the\nembedding space. However, the label-aware information, i.e., the relation label\nthat contains the semantic knowledge of the relation itself, is often neglected\nfor prediction. In this work, we propose a framework considering both\nlabel-agnostic and label-aware semantic mapping information for low resource\nrelation extraction. We show that incorporating the above two types of mapping\ninformation in both pretraining and fine-tuning can significantly improve the\nmodel performance on low-resource relation extraction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Manqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chunguang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fixing exposure bias with imitation learning needs powerful oracles. (arXiv:2109.04114v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04114","description":"<p>We apply imitation learning (IL) to tackle the NMT exposure bias problem with\nerror-correcting oracles, and evaluate an SMT lattice-based oracle which,\ndespite its excellent performance in an unconstrained oracle translation task,\nturned out to be too pruned and idiosyncratic to serve as the oracle for IL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hormann_L/0/1/0/all/0/1\">Luca Hormann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word-Level Coreference Resolution. (arXiv:2109.04127v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04127","description":"<p>Recent coreference resolution models rely heavily on span representations to\nfind coreference links between word spans. As the number of spans is $O(n^2)$\nin the length of text and the number of potential links is $O(n^4)$, various\npruning techniques are necessary to make this approach computationally\nfeasible. We propose instead to consider coreference links between individual\nwords rather than word spans and then reconstruct the word spans. This reduces\nthe complexity of the coreference model to $O(n^2)$ and allows it to consider\nall potential mentions without pruning any of them out. We also demonstrate\nthat, with these changes, SpanBERT for coreference resolution will be\nsignificantly outperformed by RoBERTa. While being highly efficient, our model\nperforms competitively with recent coreference resolution systems on the\nOntoNotes benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dobrovolskii_V/0/1/0/all/0/1\">Vladimir Dobrovolskii</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing task-oriented and open-domain dialogues in conversational agents. (arXiv:2109.04137v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04137","description":"<p>The goal of building intelligent dialogue systems has largely been\n\\textit{separately} pursued under two paradigms: task-oriented dialogue (TOD)\nsystems, which perform goal-oriented functions, and open-domain dialogue (ODD)\nsystems, which focus on non-goal-oriented chitchat. The two dialogue modes can\npotentially be intertwined together seamlessly in the same conversation, as\neasily done by a friendly human assistant. Such ability is desirable in\nconversational agents, as the integration makes them more accessible and\nuseful. Our paper addresses this problem of fusing TODs and ODDs in multi-turn\ndialogues. Based on the popular TOD dataset MultiWOZ, we build a new dataset\nFusedChat, by rewriting the existing TOD turns and adding new ODD turns. This\nprocedure constructs conversation sessions containing exchanges from both\ndialogue modes. It features inter-mode contextual dependency, i.e., the\ndialogue turns from the two modes depend on each other. Rich dependency\npatterns including co-reference and ellipsis are features. The new dataset,\nwith 60k new human-written ODD turns and 5k re-written TOD turns, offers a\nbenchmark to test a dialogue model's ability to perform inter-mode\nconversations. This is a more challenging task since the model has to determine\nthe appropriate dialogue mode and generate the response based on the inter-mode\ncontext. But such models would better mimic human-level conversation\ncapabilities. We evaluate baseline models on this task, including\n\\textit{classification-based} two-stage models and \\textit{two-in-one} fused\nmodels. We publicly release FusedChat and the baselines to propel future work\non inter-mode dialogue systems https://github.com/tomyoung903/FusedChat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Young_T/0/1/0/all/0/1\">Tom Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Frank Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandelea_V/0/1/0/all/0/1\">Vlad Pandelea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jinjie Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning. (arXiv:2109.04144v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04144","description":"<p>Recent prompt-based approaches allow pretrained language models to achieve\nstrong performances on few-shot finetuning by reformulating downstream tasks as\na language modeling problem. In this work, we demonstrate that, despite its\nadvantages on low data regimes, finetuned prompt-based models for sentence pair\nclassification tasks still suffer from a common pitfall of adopting inference\nheuristics based on lexical overlap, e.g., models incorrectly assuming a\nsentence pair is of the same meaning because they consist of the same set of\nwords. Interestingly, we find that this particular inference heuristic is\nsignificantly less present in the zero-shot evaluation of the prompt-based\nmodel, indicating how finetuning can be destructive to useful knowledge learned\nduring the pretraining. We then show that adding a regularization that\npreserves pretraining weights is effective in mitigating this destructive\ntendency of few-shot finetuning. Our evaluation on three datasets demonstrates\npromising improvements on the three corresponding challenge datasets used to\ndiagnose the inference heuristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Utama_P/0/1/0/all/0/1\">Prasetya Ajie Utama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_N/0/1/0/all/0/1\">Nafise Sadat Moosavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexico-semantic and affective modelling of Spanish poetry: A semi-supervised learning approach. (arXiv:2109.04152v1 [cs.AI])","link":"http://arxiv.org/abs/2109.04152","description":"<p>Text classification tasks have improved substantially during the last years\nby the usage of transformers. However, the majority of researches focus on\nprose texts, with poetry receiving less attention, specially for Spanish\nlanguage. In this paper, we propose a semi-supervised learning approach for\ninferring 21 psychological categories evoked by a corpus of 4572 sonnets, along\nwith 10 affective and lexico-semantic multiclass ones. The subset of poems used\nfor training an evaluation includes 270 sonnets. With our approach, we achieve\nan AUC beyond 0.7 for 76% of the psychological categories, and an AUC over 0.65\nfor 60% on the multiclass ones. The sonnets are modelled using transformers,\nthrough sentence embeddings, along with lexico-semantic and affective features,\nobtained by using external lexicons. Consequently, we see that this approach\nprovides an AUC increase of up to 0.12, as opposed to using transformers alone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbado_A/0/1/0/all/0/1\">Alberto Barbado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1\">Mar&#xed;a Dolores Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrera_D/0/1/0/all/0/1\">D&#xe9;bora Carrera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Nearest Neighbor Language Models. (arXiv:2109.04212v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04212","description":"<p>Non-parametric neural language models (NLMs) learn predictive distributions\nof text utilizing an external datastore, which allows them to learn through\nexplicitly memorizing the training datapoints. While effective, these models\noften require retrieval from a large datastore at test time, significantly\nincreasing the inference overhead and thus limiting the deployment of\nnon-parametric NLMs in practical applications. In this paper, we take the\nrecently proposed $k$-nearest neighbors language model (Khandelwal et al.,\n2019) as an example, exploring methods to improve its efficiency along various\ndimensions. Experiments on the standard WikiText-103 benchmark and\ndomain-adaptation datasets show that our methods are able to achieve up to a 6x\nspeed-up in inference speed while retaining comparable performance. The\nempirical analysis we present may provide guidelines for future research\nseeking to develop or deploy more efficient non-parametric NLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs. (arXiv:2109.04223v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04223","description":"<p>Incorporating factual knowledge into pre-trained language models (PLM) such\nas BERT is an emerging trend in recent NLP studies. However, most of the\nexisting methods combine the external knowledge integration module with a\nmodified pre-training loss and re-implement the pre-training process on the\nlarge-scale corpus. Re-pretraining these models is usually resource-consuming,\nand difficult to adapt to another domain with a different knowledge graph (KG).\nBesides, those works either cannot embed knowledge context dynamically\naccording to textual context or struggle with the knowledge ambiguity issue. In\nthis paper, we propose a novel knowledge-aware language model framework based\non fine-tuning process, which equips PLM with a unified knowledge-enhanced text\ngraph that contains both text and multi-relational sub-graphs extracted from\nKG. We design a hierarchical relational-graph-based message passing mechanism,\nwhich can allow the representations of injected KG and text to mutually update\neach other and can dynamically select ambiguous mentioned entities that share\nthe same text. Our empirical results show that our model can efficiently\nincorporate world knowledge from KGs into existing language models such as\nBERT, and achieve significant improvement on the machine reading comprehension\n(MRC) task compared with other knowledge-enhanced models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yinquan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haonan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guirong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaXT: Meta Cross-Task Transfer between Disparate Label Spaces. (arXiv:2109.04240v1 [cs.LG])","link":"http://arxiv.org/abs/2109.04240","description":"<p>Albeit the universal representational power of pre-trained language models,\nadapting them onto a specific NLP task still requires a considerably large\namount of labeled data. Effective task fine-tuning meets challenges when only a\nfew labeled examples are present for the task. In this paper, we aim to the\naddress of the problem of few shot task learning by exploiting and transferring\nfrom a different task which admits a related but disparate label space.\nSpecifically, we devise a label transfer network (LTN) to transform the labels\nfrom source task to the target task of interest for training. Both the LTN and\nthe model for task prediction are learned via a bi-level optimization\nframework, which we term as MetaXT. MetaXT offers a principled solution to best\nadapt a pre-trained language model to the target task by transferring knowledge\nfrom the source task. Empirical evaluations on cross-task transfer settings for\nfour NLP tasks, from two different types of label space disparities,\ndemonstrate the effectiveness of MetaXT, especially when the labeled data in\nthe target task is limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Srinagesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cartography Active Learning. (arXiv:2109.04282v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04282","description":"<p>We propose Cartography Active Learning (CAL), a novel Active Learning (AL)\nalgorithm that exploits the behavior of the model on individual instances\nduring training as a proxy to find the most informative instances for labeling.\nCAL is inspired by data maps, which were recently proposed to derive insights\ninto dataset quality (Swayamdipta et al., 2020). We compare our method on\npopular text classification tasks to commonly used AL strategies, which instead\nrely on post-training behavior. We demonstrate that CAL is competitive to other\ncommon AL methods, showing that training dynamics derived from small seed data\ncan be successfully used for AL. We provide insights into our new AL method by\nanalyzing batch-level statistics utilizing the data maps. Our results further\nshow that CAL results in a more data-efficient learning strategy, achieving\ncomparable or better results with considerably less training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection. (arXiv:2109.04292v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04292","description":"<p>This paper considers the unsupervised domain adaptation problem for neural\nmachine translation (NMT), where we assume the access to only monolingual text\nin either the source or target language in the new domain. We propose a\ncross-lingual data selection method to extract in-domain sentences in the\nmissing language side from a large generic monolingual corpus. Our proposed\nmethod trains an adaptive layer on top of multilingual BERT by contrastive\nlearning to align the representation between the source and target language.\nThis then enables the transferability of the domain classifier between the\nlanguages in a zero-shot manner. Once the in-domain data is detected by the\nclassifier, the NMT model is then adapted to the new domain by jointly learning\ntranslation and domain discrimination tasks. We evaluate our cross-lingual data\nselection method on NMT across five diverse domains in three language pairs, as\nwell as a real-world scenario of translation for COVID-19. The results show\nthat our proposed method outperforms other selection baselines up to +1.5 BLEU\nscore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thuy-Trang Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MATE: Multi-view Attention for Table Transformer Efficiency. (arXiv:2109.04312v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04312","description":"<p>This work presents a sparse-attention Transformer architecture for modeling\ndocuments that contain large tables. Tables are ubiquitous on the web, and are\nrich in information. However, more than 20% of relational tables on the web\nhave 20 or more rows (Cafarella et al., 2008), and these large tables present a\nchallenge for current Transformer models, which are typically limited to 512\ntokens. Here we propose MATE, a novel Transformer architecture designed to\nmodel the structure of web tables. MATE uses sparse attention in a way that\nallows heads to efficiently attend to either rows or columns in a table. This\narchitecture scales linearly with respect to speed and memory, and can handle\ndocuments containing more than 8000 tokens with current accelerators. MATE also\nhas a more appropriate inductive bias for tabular data, and sets a new\nstate-of-the-art for three table reasoning datasets. For HybridQA (Chen et al.,\n2020b), a dataset that involves large documents containing tables, we improve\nthe best prior result by 19 points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Martin Eisenschlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gor_M/0/1/0/all/0/1\">Maharshi Gor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Latent-State GPT for Semi-supervised Task-Oriented Dialog Systems. (arXiv:2109.04314v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04314","description":"<p>Recently, two approaches, fine-tuning large pre-trained language models and\nvariational training, have attracted significant interests, separately, for\nsemi-supervised end-to-end task-oriented dialog (TOD) systems. In this paper,\nwe propose Variational Latent-State GPT model (VLS-GPT), which is the first to\ncombine the strengths of the two approaches. Among many options of models, we\npropose the generative model and the inference model for variational learning\nof the end-to-end TOD system, both as auto-regressive language models based on\nGPT-2, which can be further trained over a mix of labeled and unlabeled dialog\ndata in a semi-supervised manner. We develop the strategy of\nsampling-then-forward-computation, which successfully overcomes the memory\nexplosion issue of using GPT in variational learning and speeds up training.\nSemi-supervised TOD experiments are conducted on two benchmark multi-domain\ndatasets of different languages - MultiWOZ2.1 and CrossWOZ. VLS-GPT is shown to\nsignificantly outperform both supervised-only and semi-supervised baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yucheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenru Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Junlan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data. (arXiv:2109.04319v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04319","description":"<p>While multilingual pretrained language models (LMs) fine-tuned on a single\nlanguage have shown substantial cross-lingual task transfer capabilities, there\nis still a wide performance gap in semantic parsing tasks when target language\nsupervision is available. In this paper, we propose a novel Translate-and-Fill\n(TaF) method to produce silver training data for a multilingual semantic\nparser. This method simplifies the popular Translate-Align-Project (TAP)\npipeline and consists of a sequence-to-sequence filler model that constructs a\nfull parse conditioned on an utterance and a view of the same parse. Our filler\nis trained on English data only but can accurately complete instances in other\nlanguages (i.e., translations of the English training utterances), in a\nzero-shot fashion. Experimental results on three multilingual semantic parsing\ndatasets show that data augmentation with TaF reaches accuracies competitive\nwith similar systems which rely on traditional alignment techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nicosia_M/0/1/0/all/0/1\">Massimo Nicosia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1\">Zhongdi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altun_Y/0/1/0/all/0/1\">Yasemin Altun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2109.04321v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04321","description":"<p>Contrastive learning has been gradually applied to learn high-quality\nunsupervised sentence embedding. Among the previous un-supervised methods, the\nlatest state-of-the-art method, as far as we know, is unsupervised SimCSE\n(unsup-SimCSE). Unsup-SimCSE uses the InfoNCE1loss function in the training\nstage by pulling semantically similar sentences together and pushing apart\ndis-similar ones.Theoretically, we expect to use larger batches in unsup-SimCSE\nto get more adequate comparisons among samples and avoid overfitting. However,\nincreasing the batch size does not always lead to improvements, but instead\neven lead to performance degradation when the batch size exceeds a threshold.\nThrough statistical observation, we find that this is probably due to the\nintroduction of low-confidence negative pairs after in-creasing the batch size.\nTo alleviate this problem, we introduce a simple smoothing strategy upon the\nInfoNCE loss function, termedGaussian Smoothing InfoNCE\n(GS-InfoNCE).Specifically, we add random Gaussian noise vectors as negative\nsamples, which act asa smoothing of the negative sample space.Though being\nsimple, the proposed smooth-ing strategy brings substantial improvements to\nunsup-SimCSE. We evaluate GS-InfoNCEon the standard semantic text similarity\n(STS)task. GS-InfoNCE outperforms the state-of-the-art unsup-SimCSE by an\naverage Spear-man correlation of 1.38%, 0.72%, 1.17% and0.28% on the base of\nBERT-base, BERT-large,RoBERTa-base and RoBERTa-large, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chaochen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_L/0/1/0/all/0/1\">Liangjun Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Opinion Summarizers by Selecting Informative Reviews. (arXiv:2109.04325v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04325","description":"<p>Opinion summarization has been traditionally approached with unsupervised,\nweakly-supervised and few-shot learning techniques. In this work, we collect a\nlarge dataset of summaries paired with user reviews for over 31,000 products,\nenabling supervised training. However, the number of reviews per product is\nlarge (320 on average), making summarization - and especially training a\nsummarizer - impractical. Moreover, the content of many reviews is not\nreflected in the human-written summaries, and, thus, the summarizer trained on\nrandom review subsets hallucinates. In order to deal with both of these\nchallenges, we formulate the task as jointly learning to select informative\nsubsets of reviews and summarizing the opinions expressed in these subsets. The\nchoice of the review subset is treated as a latent variable, predicted by a\nsmall and simple selector. The subset is then fed into a more powerful\nsummarizer. For joint training, we use amortized variational inference and\npolicy gradient methods. Our experiments demonstrate the importance of\nselecting informative reviews resulting in improved quality of summaries and\nreduced hallucinations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brazinskas_A/0/1/0/all/0/1\">Arthur Bra&#x17e;inskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PPT: Pre-trained Prompt Tuning for Few-shot Learning. (arXiv:2109.04332v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04332","description":"<p>Prompts for pre-trained language models (PLMs) have shown remarkable\nperformance by bridging the gap between pre-training tasks and various\ndownstream tasks. Among these methods, prompt tuning, which freezes PLMs and\nonly tunes soft prompts, provides an efficient and effective solution for\nadapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to\nbe fully explored. In our pilot experiments, we find that prompt tuning\nperforms comparably with conventional full-model fine-tuning when downstream\ndata are sufficient, whereas it performs much worse under few-shot learning\nsettings, which may hinder the application of prompt tuning in practice. We\nattribute this low performance to the manner of initializing soft prompts.\nTherefore, in this work, we propose to pre-train prompts by adding soft prompts\ninto the pre-training stage to obtain a better initialization. We name this\nPre-trained Prompt Tuning framework \"PPT\". To ensure the generalization of PPT,\nwe formulate similar classification tasks into a unified task form and\npre-train soft prompts for this unified task. Extensive experiments show that\ntuning pre-trained prompts for downstream tasks can reach or even outperform\nfull-model fine-tuning under both full-data and few-shot settings. Our approach\nis effective and efficient for using large-scale PLMs in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance. (arXiv:2109.04349v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04349","description":"<p>The ability to identify and resolve uncertainty is crucial for the robustness\nof a dialogue system. Indeed, this has been confirmed empirically on systems\nthat utilise Bayesian approaches to dialogue belief tracking. However, such\nsystems consider only confidence estimates and have difficulty scaling to more\ncomplex settings. Neural dialogue systems, on the other hand, rarely take\nuncertainties into account. They are therefore overconfident in their decisions\nand less robust. Moreover, the performance of the tracking task is often\nevaluated in isolation, without consideration of its effect on the downstream\npolicy optimisation. We propose the use of different uncertainty measures in\nneural belief tracking. The effects of these measures on the downstream task of\npolicy optimisation are evaluated by adding selected measures of uncertainty to\nthe feature space of the policy and training policies through interaction with\na user simulator. Both human and simulated user results show that incorporating\nthese measures leads to improvements both of the performance and of the\nrobustness of the downstream dialogue policy. This highlights the importance of\ndeveloping neural dialogue belief trackers that take uncertainty into account.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1\">Andrey Malinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-granularity Textual Adversarial Attack with Behavior Cloning. (arXiv:2109.04367v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04367","description":"<p>Recently, the textual adversarial attack models become increasingly popular\ndue to their successful in estimating the robustness of NLP models. However,\nexisting works have obvious deficiencies. (1) They usually consider only a\nsingle granularity of modification strategies (e.g. word-level or\nsentence-level), which is insufficient to explore the holistic textual space\nfor generation; (2) They need to query victim models hundreds of times to make\na successful attack, which is highly inefficient in practice. To address such\nproblems, in this paper we propose MAYA, a Multi-grAnularitY Attack model to\neffectively generate high-quality adversarial samples with fewer queries to\nvictim models. Furthermore, we propose a reinforcement-learning based method to\ntrain a multi-granularity attack agent through behavior cloning with the expert\nknowledge from our MAYA algorithm to further reduce the query times.\nAdditionally, we also adapt the agent to attack black-box models that only\noutput labels without confidence scores. We conduct comprehensive experiments\nto evaluate our attack models by attacking BiLSTM, BERT and RoBERTa in two\ndifferent black-box attack settings and three benchmark datasets. Experimental\nresults show that our models achieve overall better attacking performance and\nproduce more fluent and grammatical adversarial samples compared to baseline\nmodels. Besides, our adversarial attack agent significantly reduces the query\ntimes in both attack settings. Our codes are released at\nhttps://github.com/Yangyi-Chen/MAYA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking Turbulence Through Financial News During COVID-19. (arXiv:2109.04369v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04369","description":"<p>Grave human toll notwithstanding, the COVID-19 pandemic created uniquely\nunstable conditions in financial markets. In this work we uncover and discuss\nrelationships involving sentiment in financial publications during the 2020\npandemic-motivated U.S. financial crash. First, we introduce a set of expert\nannotations of financial sentiment for articles from major American financial\nnews publishers. After an exploratory data analysis, we then describe a\nCNN-based architecture to address the task of predicting financial sentiment in\nthis anomalous, tumultuous setting. Our best performing model achieves a\nmaximum weighted F1 score of 0.746, establishing a strong performance\nbenchmark. Using predictions from our top performing model, we close by\nconducting a statistical correlation study with real stock market data, finding\ninteresting and strong relationships between financial news and the S\\&amp;P 500\nindex, trading volume, market volatility, and different single-factor ETFs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hossu_P/0/1/0/all/0/1\">Philip Hossu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parde_N/0/1/0/all/0/1\">Natalie Parde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding. (arXiv:2109.04380v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04380","description":"<p>Contrastive learning has been attracting much attention for learning\nunsupervised sentence embeddings. The current state-of-the-art unsupervised\nmethod is the unsupervised SimCSE (unsup-SimCSE). Unsup-SimCSE takes dropout as\na minimal data augmentation method, and passes the same input sentence to a\npre-trained Transformer encoder (with dropout turned on) twice to obtain the\ntwo corresponding embeddings to build a positive pair. As the length\ninformation of a sentence will generally be encoded into the sentence\nembeddings due to the usage of position embedding in Transformer, each positive\npair in unsup-SimCSE actually contains the same length information. And thus\nunsup-SimCSE trained with these positive pairs is probably biased, which would\ntend to consider that sentences of the same or similar length are more similar\nin semantics. Through statistical observations, we find that unsup-SimCSE does\nhave such a problem. To alleviate it, we apply a simple repetition operation to\nmodify the input sentence, and then pass the input sentence and its modified\ncounterpart to the pre-trained Transformer encoder, respectively, to get the\npositive pair. Additionally, we draw inspiration from the community of computer\nvision and introduce a momentum contrast, enlarging the number of negative\npairs without additional calculations. The proposed two modifications are\napplied on positive and negative pairs separately, and build a new sentence\nembedding method, termed Enhanced Unsup-SimCSE (ESimCSE). We evaluate the\nproposed ESimCSE on several benchmark datasets w.r.t the semantic text\nsimilarity (STS) task. Experimental results show that ESimCSE outperforms the\nstate-of-the-art unsup-SimCSE by an average Spearman correlation of 2.02% on\nBERT-base.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chaochen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_L/0/1/0/all/0/1\">Liangjun Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification. (arXiv:2109.04385v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04385","description":"<p>Research shows that natural language processing models are generally\nconsidered to be vulnerable to adversarial attacks; but recent work has drawn\nattention to the issue of validating these adversarial inputs against certain\ncriteria (e.g., the preservation of semantics and grammaticality). Enforcing\nconstraints to uphold such criteria may render attacks unsuccessful, raising\nthe question of whether valid attacks are actually feasible. In this work, we\ninvestigate this through the lens of human language ability. We report on\ncrowdsourcing studies in which we task humans with iteratively modifying words\nin an input text, while receiving immediate model feedback, with the aim of\ncausing a sentiment classification model to misclassify the example. Our\nfindings suggest that humans are capable of generating a substantial amount of\nadversarial examples using semantics-preserving word substitutions. We analyze\nhow human-generated adversarial examples compare to the recently proposed\nTextFooler, Genetic, BAE and SememePSO attack algorithms on the dimensions\nnaturalness, preservation of sentiment, grammaticality and substitution rate.\nOur findings suggest that human-generated adversarial examples are not more\nable than the best algorithms to generate natural-reading, sentiment-preserving\nexamples, though they do so by being much more computationally efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mozes_M/0/1/0/all/0/1\">Maximilian Mozes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1\">Max Bartolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1\">Bennett Kleinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffin_L/0/1/0/all/0/1\">Lewis D. Griffin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph. (arXiv:2109.04400v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04400","description":"<p>In cross-lingual text classification, it is required that task-specific\ntraining data in high-resource source languages are available, where the task\nis identical to that of a low-resource target language. However, collecting\nsuch training data can be infeasible because of the labeling cost, task\ncharacteristics, and privacy concerns. This paper proposes an alternative\nsolution that uses only task-independent word embeddings of high-resource\nlanguages and bilingual dictionaries. First, we construct a dictionary-based\nheterogeneous graph (DHG) from bilingual dictionaries. This opens the\npossibility to use graph neural networks for cross-lingual transfer. The\nremaining challenge is the heterogeneity of DHG because multiple languages are\nconsidered. To address this challenge, we propose dictionary-based\nheterogeneous graph neural network (DHGNet) that effectively handles the\nheterogeneity of DHG by two-step aggregations, which are word-level and\nlanguage-level aggregations. Experimental results demonstrate that our method\noutperforms pretrained models even though it does not access to large corpora.\nFurthermore, it can perform well even though dictionaries contain many\nincorrect translations. Its robustness allows the usage of a wider range of\ndictionaries such as an automatically constructed dictionary and crowdsourced\ndictionary, which are convenient for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chairatanakul_N/0/1/0/all/0/1\">Nuttapong Chairatanakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriwatanasakdi_N/0/1/0/all/0/1\">Noppayut Sriwatanasakdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charoenphakdee_N/0/1/0/all/0/1\">Nontawat Charoenphakdee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murata_T/0/1/0/all/0/1\">Tsuyoshi Murata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality. (arXiv:2109.04404v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04404","description":"<p>Similarity measures are a vital tool for understanding how language models\nrepresent and process language. Standard representational similarity measures\nsuch as cosine similarity and Euclidean distance have been successfully used in\nstatic word embedding models to understand how words cluster in semantic space.\nRecently, these measures have been applied to embeddings from contextualized\nmodels such as BERT and GPT-2. In this work, we call into question the\ninformativity of such measures for contextualized language models. We find that\na small number of rogue dimensions, often just 1-3, dominate these measures.\nMoreover, we find a striking mismatch between the dimensions that dominate\nsimilarity measures and those which are important to the behavior of the model.\nWe show that simple postprocessing techniques such as standardization are able\nto correct for rogue dimensions and reveal underlying representational quality.\nWe argue that accounting for rogue dimensions is essential for any\nsimilarity-based analysis of contextual language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Timkey_W/0/1/0/all/0/1\">William Timkey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schijndel_M/0/1/0/all/0/1\">Marten van Schijndel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Uneven Training Data: Unlabeled, Single Label, and Multiple Labels. (arXiv:2109.04408v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04408","description":"<p>Training NLP systems typically assumes access to annotated data that has a\nsingle human label per example. Given imperfect labeling from annotators and\ninherent ambiguity of language, we hypothesize that single label is not\nsufficient to learn the spectrum of language interpretation. We explore new\nlabel annotation distribution schemes, assigning multiple labels per example\nfor a small subset of training examples. Introducing such multi label examples\nat the cost of annotating fewer examples brings clear gains on natural language\ninference task and entity typing task, even when we simply first train with a\nsingle label data and then fine tune with multi label examples. Extending a\nMixUp data augmentation framework, we propose a learning algorithm that can\nlearn from uneven training examples (with zero, one, or multiple labels). This\nalgorithm efficiently combines signals from uneven training data and brings\nadditional gains in low annotation budget and cross domain settings. Together,\nour method achieves consistent gains in both accuracy and label distribution\nmetrics in two tasks, suggesting training with uneven training data can be\nbeneficial for many NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chengyue Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-autoregressive End-to-end Speech Translation with Parallel Autoregressive Rescoring. (arXiv:2109.04411v1 [eess.AS])","link":"http://arxiv.org/abs/2109.04411","description":"<p>This article describes an efficient end-to-end speech translation (E2E-ST)\nframework based on non-autoregressive (NAR) models. End-to-end speech\ntranslation models have several advantages over traditional cascade systems\nsuch as inference latency reduction. However, conventional AR decoding methods\nare not fast enough because each token is generated incrementally. NAR models,\nhowever, can accelerate the decoding speed by generating multiple tokens in\nparallel on the basis of the token-wise conditional independence assumption. We\npropose a unified NAR E2E-ST framework called Orthros, which has an NAR decoder\nand an auxiliary shallow AR decoder on top of the shared encoder. The auxiliary\nshallow AR decoder selects the best hypothesis by rescoring multiple candidates\ngenerated from the NAR decoder in parallel (parallel AR rescoring). We adopt\nconditional masked language model (CMLM) and a connectionist temporal\nclassification (CTC)-based model as NAR decoders for Orthros, referred to as\nOrthros-CMLM and Orthros-CTC, respectively. We also propose two training\nmethods to enhance the CMLM decoder. Experimental evaluations on three\nbenchmark datasets with six language directions demonstrated that Orthros\nachieved large improvements in translation quality with a very small overhead\ncompared with the baseline NAR model. Moreover, the Conformer encoder\narchitecture enabled large quality improvements, especially for CTC-based\nmodels. Orthros-CTC with the Conformer encoder increased decoding speed by\n3.63x on CPU with translation quality comparable to that of an AR model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models. (arXiv:2109.04413v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04413","description":"<p>Despite their success in a variety of NLP tasks, pre-trained language models,\ndue to their heavy reliance on compositionality, fail in effectively capturing\nthe meanings of multiword expressions (MWEs), especially idioms. Therefore,\ndatasets and methods to improve the representation of MWEs are urgently needed.\nExisting datasets are limited to providing the degree of idiomaticity of\nexpressions along with the literal and, where applicable, (a single)\nnon-literal interpretation of MWEs. This work presents a novel dataset of\nnaturally occurring sentences containing MWEs manually classified into a\nfine-grained set of meanings, spanning both English and Portuguese. We use this\ndataset in two tasks designed to test i) a language model's ability to detect\nidiom usage, and ii) the effectiveness of a language model in generating\nrepresentations of sentences containing idioms. Our experiments demonstrate\nthat, on the task of detecting idiomatic usage, these models perform reasonably\nwell in the one-shot and few-shot scenarios, but that there is significant\nscope for improvement in the zero-shot scenario. On the task of representing\nidiomaticity, we find that pre-training is not always effective, while\nfine-tuning could provide a sample efficient method of learning representations\nof sentences containing MWEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gow_Smith_E/0/1/0/all/0/1\">Edward Gow-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villavicencio_A/0/1/0/all/0/1\">Aline Villavicencio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TxT: Crossmodal End-to-End Learning with Transformers. (arXiv:2109.04422v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04422","description":"<p>Reasoning over multiple modalities, e.g. in Visual Question Answering (VQA),\nrequires an alignment of semantic concepts across domains. Despite the\nwidespread success of end-to-end learning, today's multimodal pipelines by and\nlarge leverage pre-extracted, fixed features from object detectors, typically\nFaster R-CNN, as representations of the visual world. The obvious downside is\nthat the visual representation is not specifically tuned to the multimodal task\nat hand. At the same time, while transformer-based object detectors have gained\npopularity, they have not been employed in today's multimodal pipelines. We\naddress both shortcomings with TxT, a transformer-based crossmodal pipeline\nthat enables fine-tuning both language and visual components on the downstream\ntask in a fully end-to-end manner. We overcome existing limitations of\ntransformer-based detectors for multimodal reasoning regarding the integration\nof global context and their scalability. Our transformer-based multimodal model\nachieves considerable gains from end-to-end learning for multimodal question\nanswering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steitz_J/0/1/0/all/0/1\">Jan-Martin O. Steitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_S/0/1/0/all/0/1\">Stefan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints. (arXiv:2109.04443v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04443","description":"<p>Back-translation (BT) of target monolingual corpora is a widely used data\naugmentation strategy for neural machine translation (NMT), especially for\nlow-resource language pairs. To improve effectiveness of the available BT data,\nwe introduce HintedBT -- a family of techniques which provides hints (through\ntags) to the encoder and decoder. First, we propose a novel method of using\nboth high and low quality BT data by providing hints (as source tags on the\nencoder) to the model about the quality of each source-target pair. We don't\nfilter out low quality data but instead show that these hints enable the model\nto learn effectively from noisy data. Second, we address the problem of\npredicting whether a source token needs to be translated or transliterated to\nthe target language, which is common in cross-script translation tasks (i.e.,\nwhere source and target do not share the written script). For such cases, we\npropose training the model with additional hints (as target tags on the\ndecoder) that provide information about the operation required on the source\n(translation or both translation and transliteration). We conduct experiments\nand detailed analyses on standard WMT benchmarks for three cross-script\nlow/medium-resource language pairs: {Hindi,Gujarati,Tamil}-to-English. Our\nmethods compare favorably with five strong and well established baselines. We\nshow that using these hints, both separately and together, significantly\nimproves translation quality and leads to state-of-the-art performance in all\nthree language pairs in corresponding bilingual settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramnath_S/0/1/0/all/0/1\">Sahana Ramnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhirut Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghuveer_A/0/1/0/all/0/1\">Aravindan Raghuveer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers. (arXiv:2109.04448v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04448","description":"<p>Pretrained vision-and-language BERTs aim to learn representations that\ncombine information from both modalities. We propose a diagnostic method based\non cross-modal input ablation to assess the extent to which these models\nactually integrate cross-modal information. This method involves ablating\ninputs from one modality, either entirely or selectively based on cross-modal\ngrounding alignments, and evaluating the model prediction performance on the\nother modality. Model performance is measured by modality-specific tasks that\nmirror the model pretraining objectives (e.g. masked language modelling for\ntext). Models that have learned to construct cross-modal representations using\nboth modalities are expected to perform worse when inputs are missing from a\nmodality. We find that recently proposed models have much greater relative\ndifficulty predicting text when visual information is ablated, compared to\npredicting visual object categories when text is ablated, indicating that these\nmodels are not symmetrically cross-modal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frank_S/0/1/0/all/0/1\">Stella Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Language Change in Collaborative Instruction Following. (arXiv:2109.04452v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04452","description":"<p>We analyze language change over time in a collaborative, goal-oriented\ninstructional task, where utility-maximizing participants form conventions and\nincrease their expertise. Prior work studied such scenarios mostly in the\ncontext of reference games, and consistently found that language complexity is\nreduced along multiple dimensions, such as utterance length, as conventions are\nformed. In contrast, we find that, given the ability to increase instruction\nutility, instructors increase language complexity along these previously\nstudied dimensions to better collaborate with increasingly skilled instruction\nfollowers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Effenberger_A/0/1/0/all/0/1\">Anna Effenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1\">Eva Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rhia Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization. (arXiv:1911.03437v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1911.03437","description":"<p>Transfer learning has fundamentally changed the landscape of natural language\nprocessing (NLP) research. Many existing state-of-the-art models are first\npre-trained on a large text corpus and then fine-tuned on downstream tasks.\nHowever, due to limited data resources from downstream tasks and the extremely\nlarge capacity of pre-trained models, aggressive fine-tuning often causes the\nadapted model to overfit the data of downstream tasks and forget the knowledge\nof the pre-trained model. To address the above issue in a more principled\nmanner, we propose a new computational framework for robust and efficient\nfine-tuning for pre-trained language models. Specifically, our proposed\nframework contains two important ingredients: 1. Smoothness-inducing\nregularization, which effectively manages the capacity of the model; 2. Bregman\nproximal point optimization, which is a class of trust-region methods and can\nprevent knowledge forgetting. Our experiments demonstrate that our proposed\nmethod achieves the state-of-the-art performance on multiple NLP benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Linguistic Capacity of Real-Time Counter Automata. (arXiv:2004.06866v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.06866","description":"<p>Counter machines have achieved a newfound relevance to the field of natural\nlanguage processing (NLP): recent work suggests some strong-performing\nrecurrent neural networks utilize their memory as counters. Thus, one potential\nway to understand the success of these networks is to revisit the theory of\ncounter computation. Therefore, we study the abilities of real-time counter\nmachines as formal grammars, focusing on formal properties that are relevant\nfor NLP models. We first show that several variants of the counter machine\nconverge to express the same class of formal languages. We also prove that\ncounter languages are closed under complement, union, intersection, and many\nother common set operations. Next, we show that counter machines cannot\nevaluate boolean expressions, even though they can weakly validate their\nsyntax. This has implications for the interpretability and evaluation of neural\nnetwork systems: successfully matching syntactic patterns does not guarantee\nthat counter memory accurately encodes compositional semantics. Finally, we\nconsider whether counter languages are semilinear. This work makes general\ncontributions to the theory of formal languages that are of potential interest\nfor understanding recurrent neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time-Aware Evidence Ranking for Fact-Checking. (arXiv:2009.06402v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.06402","description":"<p>Truth can vary over time. Fact-checking decisions on claim veracity should\ntherefore take into account temporal information of both the claim and\nsupporting or refuting evidence. In this work, we investigate the hypothesis\nthat the timestamp of a Web page is crucial to how it should be ranked for a\ngiven claim. We delineate four temporal ranking methods that constrain evidence\nranking differently and simulate hypothesis-specific evidence rankings given\nthe evidence timestamps as gold standard. Evidence ranking in three\nfact-checking models is ultimately optimized using a learning-to-rank loss\nfunction. Our study reveals that time-aware evidence ranking not only surpasses\nrelevance assumptions based purely on semantic similarity or position in a\nsearch results list, but also improves veracity predictions of time-sensitive\nclaims in particular.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allein_L/0/1/0/all/0/1\">Liesbeth Allein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Mention Detector-Linker Interaction in Neural Coreference Resolution. (arXiv:2009.09363v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.09363","description":"<p>Despite significant recent progress in coreference resolution, the quality of\ncurrent state-of-the-art systems still considerably trails behind human-level\nperformance. Using the CoNLL-2012 and PreCo datasets, we dissect the best\ninstantiation of the mainstream end-to-end coreference resolution model that\nunderlies most current best-performing coreference systems, and empirically\nanalyze the behavior of its two components: mention detector and mention\nlinker. While the detector traditionally focuses heavily on recall as a design\ndecision, we demonstrate the importance of precision, calling for their\nbalance. However, we point out the difficulty in building a precise detector\ndue to its inability to make important anaphoricity decisions. We also\nhighlight the enormous room for improving the linker and show that the rest of\nits errors mainly involve pronoun resolution. We propose promising next steps\nand hope our findings will help future research in coreference resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.03060","description":"<p>A key challenge in training neural networks for a given medical imaging task\nis often the difficulty of obtaining a sufficient number of manually labeled\nexamples. In contrast, textual imaging reports, which are often readily\navailable in medical records, contain rich but unstructured interpretations\nwritten by experts as part of standard clinical practice. We propose using\nthese textual reports as a form of weak supervision to improve the image\ninterpretation performance of a neural network without requiring additional\nmanually labeled examples. We use an image-text matching task to train a\nfeature extractor and then fine-tune it in a transfer learning setting for a\nsupervised task using a small labeled dataset. The end result is a neural\nnetwork that automatically interprets imagery without requiring textual reports\nduring inference. This approach can be applied to any task for which text-image\npairs are readily available. We evaluate our method on three classification\ntasks and find consistent performance improvements, reducing the need for\nlabeled data by 67%-98%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Gongbo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenwell_C/0/1/0/all/0/1\">Connor Greenwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1\">Nathan Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mathematical Word Problem Generation from Commonsense Knowledge Graph and Equations. (arXiv:2010.06196v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.06196","description":"<p>There is an increasing interest in the use of mathematical word problem (MWP)\ngeneration in educational assessment. Different from standard natural question\ngeneration, MWP generation needs to maintain the underlying mathematical\noperations between quantities and variables, while at the same time ensuring\nthe relevance between the output and the given topic. To address above problem,\nwe develop an end-to-end neural model to generate diverse MWPs in real-world\nscenarios from commonsense knowledge graph and equations. The proposed model\n(1) learns both representations from edge-enhanced Levi graphs of symbolic\nequations and commonsense knowledge; (2) automatically fuses equation and\ncommonsense knowledge information via a self-planning module when generating\nthe MWPs. Experiments on an educational gold-standard set and a large-scale\ngenerated MWP set show that our approach is superior on the MWP generation\ntask, and it outperforms the SOTA models in terms of both automatic evaluation\nmetrics, i.e., BLEU-4, ROUGE-L, Self-BLEU, and human evaluation metrics, i.e.,\nequation relevance, topic relevance, and language coherence. To encourage\nreproducible results, we make our code and MWP dataset public available at\n\\url{https://github.com/tal-ai/MaKE_EMNLP2021}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Generalization via Semantic Tagging. (arXiv:2010.11818v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.11818","description":"<p>Although neural sequence-to-sequence models have been successfully applied to\nsemantic parsing, they fail at compositional generalization, i.e., they are\nunable to systematically generalize to unseen compositions of seen components.\nMotivated by traditional semantic parsing where compositionality is explicitly\naccounted for by symbolic grammars, we propose a new decoding framework that\npreserves the expressivity and generality of sequence-to-sequence models while\nfeaturing lexicon-style alignments and disentangled information processing.\nSpecifically, we decompose decoding into two phases where an input utterance is\nfirst tagged with semantic symbols representing the meaning of individual\nwords, and then a sequence-to-sequence model is used to predict the final\nmeaning representation conditioning on the utterance and the predicted tag\nsequence. Experimental results on three semantic parsing datasets show that the\nproposed approach consistently improves compositional generalization across\nmodel architectures, domains, and semantic formalisms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NUANCED: Natural Utterance Annotation for Nuanced Conversation with Estimated Distributions. (arXiv:2010.12758v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12758","description":"<p>Existing conversational systems are mostly agent-centric, which assumes the\nuser utterances would closely follow the system ontology (for NLU or dialogue\nstate tracking). However, in real-world scenarios, it is highly desirable that\nthe users can speak freely in their own way. It is extremely hard, if not\nimpossible, for the users to adapt to the unknown system ontology. In this\nwork, we attempt to build a user-centric dialogue system. As there is no clean\nmapping for a user's free form utterance to an ontology, we first model the\nuser preferences as estimated distributions over the system ontology and map\nthe users' utterances to such distributions. Learning such a mapping poses new\nchallenges on reasoning over existing knowledge, ranging from factoid\nknowledge, commonsense knowledge to the users' own situations. To this end, we\nbuild a new dataset named NUANCED that focuses on such realistic settings for\nconversational recommendation. Collected via dialogue simulation and\nparaphrasing, NUANCED contains 5.1k dialogues, 26k turns of high-quality user\nresponses. We conduct experiments, showing both the usefulness and challenges\nof our problem setting. We believe NUANCED can serve as a valuable resource to\npush existing research from the agent-centric system to the user-centric\nsystem. The code and data is publicly available at\n\\url{https://github.com/facebookresearch/nuanced}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Honglei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seungwhan Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Few-Shot Commonsense Knowledge Models. (arXiv:2101.00297v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00297","description":"<p>Providing natural language processing systems with commonsense knowledge is a\ncritical challenge for achieving language understanding. Recently, commonsense\nknowledge models have emerged as a suitable approach for hypothesizing\nsituation-relevant commonsense knowledge on-demand in natural language\napplications. However, these systems are limited by the fixed set of relations\ncaptured by schemas of the knowledge bases on which they're trained.\n</p>\n<p>To address this limitation, we investigate training commonsense knowledge\nmodels in a few-shot setting with limited tuples per commonsense relation in\nthe graph. We perform five separate studies on different dimensions of few-shot\ncommonsense knowledge learning, providing a roadmap on best practices for\ntraining these systems efficiently. Importantly, we find that human quality\nratings for knowledge produced from a few-shot trained system can achieve\nperformance within 6% of knowledge produced from fully supervised systems. This\nfew-shot performance enables coverage of a wide breadth of relations in future\ncommonsense systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Da_J/0/1/0/all/0/1\">Jeff Da</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical Question Answering: A Survey of Approaches and Challenges. (arXiv:2102.05281v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.05281","description":"<p>Automatic Question Answering (QA) has been successfully applied in various\ndomains such as search engines and chatbots. Biomedical QA (BQA), as an\nemerging QA task, enables innovative applications to effectively perceive,\naccess and understand complex biomedical knowledge. There have been tremendous\ndevelopments of BQA in the past two decades, which we classify into 5\ndistinctive approaches: classic, information retrieval, machine reading\ncomprehension, knowledge base and question entailment approaches. In this\nsurvey, we introduce available datasets and representative methods of each BQA\napproach in detail. Despite the developments, BQA systems are still immature\nand rarely used in real-life settings. We identify and characterize several key\nchallenges in BQA that might lead to this issue, and discuss some potential\nfuture directions to explore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1\">Guangzhi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qianlan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_H/0/1/0/all/0/1\">Huaiyuan Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Position Information in Transformers: An Overview. (arXiv:2102.11090v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.11090","description":"<p>Transformers are arguably the main workhorse in recent Natural Language\nProcessing research. By definition a Transformer is invariant with respect to\nreordering of the input. However, language is inherently sequential and word\norder is essential to the semantics and syntax of an utterance. In this\narticle, we provide an overview and theoretical comparison of existing methods\nto incorporate position information into Transformer models. The objectives of\nthis survey are to (1) showcase that position information in Transformer is a\nvibrant and extensive research area; (2) enable the reader to compare existing\nmethods by providing a unified notation and systematization of different\napproaches along important model dimensions; (3) indicate what characteristics\nof an application should be taken into account when selecting a position\nencoding; (4) provide stimuli for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1\">Philipp Dufter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Martin Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP. (arXiv:2103.00453v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.00453","description":"<p>When trained on large, unfiltered crawls from the internet, language models\npick up and reproduce all kinds of undesirable biases that can be found in the\ndata: they often generate racist, sexist, violent or otherwise toxic language.\nAs large models require millions of training examples to achieve good\nperformance, it is difficult to completely prevent them from being exposed to\nsuch content. In this paper, we first demonstrate a surprising finding:\npretrained language models recognize, to a considerable degree, their\nundesirable biases and the toxicity of the content they produce. We refer to\nthis capability as self-diagnosis. Based on this finding, we then propose a\ndecoding algorithm that, given only a textual description of the undesired\nbehavior, reduces the probability of a language model producing problematic\ntext. We refer to this approach as self-debiasing. Self-debiasing does not rely\non manually curated word lists, nor does it require any training data or\nchanges to the model's parameters. While we by no means eliminate the issue of\nlanguage models generating biased text, we believe our approach to be an\nimportant step in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udupa_S/0/1/0/all/0/1\">Sahana Udupa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Embedding Calibration for Symbolic Music Similarity. (arXiv:2103.07656v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2103.07656","description":"<p>In natural language processing (NLP), the semantic similarity task requires\nlarge-scale, high-quality human-annotated labels for fine-tuning or evaluation.\nBy contrast, in cases of music similarity, such labels are expensive to collect\nand largely dependent on the annotator's artistic preferences. Recent research\nhas demonstrated that embedding calibration technique can greatly increase\nsemantic similarity performance of the pre-trained language model without\nfine-tuning. However, it is yet unknown which calibration method is the best\nand how much performance improvement can be achieved. To address these issues,\nwe propose using composer information to construct labels for automatically\nevaluating music similarity. Under this paradigm, we discover the optimal\ncombination of embedding calibration which achieves superior metrics than the\nbaseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiafeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization. (arXiv:2103.12235v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.12235","description":"<p>Question Answering (QA) tasks requiring information from multiple documents\noften rely on a retrieval model to identify relevant information for reasoning.\nThe retrieval model is typically trained to maximize the likelihood of the\nlabeled supporting evidence. However, when retrieving from large text corpora\nsuch as Wikipedia, the correct answer can often be obtained from multiple\nevidence candidates. Moreover, not all such candidates are labeled as positive\nduring annotation, rendering the training signal weak and noisy. This problem\nis exacerbated when the questions are unanswerable or when the answers are\nBoolean, since the model cannot rely on lexical overlap to make a connection\nbetween the answer and supporting evidence. We develop a new parameterization\nof set-valued retrieval that handles unanswerable queries, and we show that\nmarginalizing over this set during training allows a model to mitigate false\nnegatives in supporting evidence annotations. We test our method on two\nmulti-document QA datasets, IIRC and HotpotQA. On IIRC, we show that joint\nmodeling with marginalization improves model performance by 5.5 F1 points and\nachieves a new state-of-the-art performance of 50.5 F1. We also show that\nretrieval marginalization results in 4.1 QA F1 improvement over a\nnon-marginalized baseline on HotpotQA in the fullwiki setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasigi_P/0/1/0/all/0/1\">Pradeep Dasigi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06669","description":"<p>Many implicit inferences exist in text depending on how it is structured that\ncan critically impact the text's interpretation and meaning. One such\nstructural aspect present in text with chronology is the order of its\npresentation. For narratives or stories, this is known as the narrative order.\nReordering a narrative can impact the temporal, causal, event-based, and other\ninferences readers draw from it, which in turn can have strong effects both on\nits interpretation and interestingness. In this paper, we propose and\ninvestigate the task of Narrative Reordering (NAREOR) which involves rewriting\na given story in a different narrative order while preserving its plot. We\npresent a dataset, NAREORC, with human rewritings of stories within ROCStories\nin non-linear orders, and conduct a detailed analysis of it. Further, we\npropose novel task-specific training methods with suitable evaluation metrics.\nWe perform experiments on NAREORC using state-of-the-art models such as BART\nand T5 and conduct extensive automatic and human evaluations. We demonstrate\nthat although our models can perform decently, NAREOR is a challenging task\nwith potential for further exploration. We also investigate two applications of\nNAREOR: generation of more interesting variations of stories and serving as\nadversarial sets for temporal/event-related tasks, besides discussing other\nprospective ones, such as for pedagogical setups related to language skills\nlike essay writing and applications to medicine involving clinical narratives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07650","description":"<p>Recently, prompt-tuning has achieved promising results for certain few-shot\nclassification tasks. The core idea of prompt-tuning is to insert text pieces\n(i.e., templates) into the input and transform a classification task into a\nmasked language modeling problem. However, for relation extraction, determining\nan appropriate prompt template requires domain expertise, and it is cumbersome\nand time-consuming to obtain a suitable label word. Furthermore, there exist\nabundant semantic knowledge among the entities and relations that cannot be\nignored. To this end, we focus on incorporating knowledge into prompt-tuning\nfor relation extraction and propose a knowledge-aware prompt-tuning approach\nwith synergistic optimization (KnowPrompt). Specifically, we inject entity and\nrelation knowledge into prompt construction with learnable virtual template\nwords as well as answer words and synergistically optimize their representation\nwith knowledge constraints. Extensive experimental results on five datasets\nwith standard and low-resource settings demonstrate the effectiveness of our\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Train BERT with an Academic Budget. (arXiv:2104.07705v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07705","description":"<p>While large language models a la BERT are used ubiquitously in NLP,\npretraining them is considered a luxury that only a few well-funded industry\nlabs can afford. How can one train such models with a more modest budget? We\npresent a recipe for pretraining a masked language model in 24 hours using a\nsingle low-end deep learning server. We demonstrate that through a combination\nof software optimizations, design choices, and hyperparameter tuning, it is\npossible to produce models that are competitive with BERT-base on GLUE tasks at\na fraction of the original pretraining cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izsak_P/0/1/0/all/0/1\">Peter Izsak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berchansky_M/0/1/0/all/0/1\">Moshe Berchansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders. (arXiv:2104.08027v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08027","description":"<p>Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent\nyears. However, previous work has indicated that off-the-shelf MLMs are not\neffective as universal lexical or sentence encoders without further\ntask-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks\nusing annotated task data. In this work, we demonstrate that it is possible to\nturn MLMs into effective universal lexical and sentence encoders even without\nany additional data and without any supervision. We propose an extremely\nsimple, fast and effective contrastive learning technique, termed Mirror-BERT,\nwhich converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30\nseconds without any additional external knowledge. Mirror-BERT relies on fully\nidentical or slightly modified string pairs as positive (i.e., synonymous)\nfine-tuning examples, and aims to maximise their similarity during identity\nfine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in\nboth lexical-level and sentence-level tasks, across different domains and\ndifferent languages. Notably, in the standard sentence semantic similarity\n(STS) tasks, our self-supervised Mirror-BERT model even matches the performance\nof the task-tuned Sentence-BERT models from prior work. Finally, we delve\ndeeper into the inner workings of MLMs, and suggest some evidence on why this\nsimple approach can yield effective universal lexical and sentence encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Factual Knowledge in Language Models. (arXiv:2104.08164v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08164","description":"<p>The factual knowledge acquired during pre-training and stored in the\nparameters of Language Models (LMs) can be useful in downstream tasks (e.g.,\nquestion answering or textual inference). However, some facts can be\nincorrectly induced or become obsolete over time. We present KnowledgeEditor, a\nmethod which can be used to edit this knowledge and, thus, fix 'bugs' or\nunexpected predictions without the need for expensive re-training or\nfine-tuning. Besides being computationally efficient, KnowledgeEditordoes not\nrequire any modifications in LM pre-training (e.g., the use of meta-learning).\nIn our approach, we train a hyper-network with constrained optimization to\nmodify a fact without affecting the rest of the knowledge; the trained\nhyper-network is then used to predict the weight update at test time. We show\nKnowledgeEditor's efficacy with two popular architectures and\nknowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and\nii) a sequence-to-sequence BART model for question answering. With our method,\nchanging a prediction on the specific wording of a query tends to result in a\nconsistent change in predictions also for its paraphrases. We show that this\ncan be further encouraged by exploiting (e.g., automatically-generated)\nparaphrases during training. Interestingly, our hyper-network can be regarded\nas a 'probe' revealing which components need to be changed to manipulate\nfactual knowledge; our analysis shows that the updates tend to be concentrated\non a small subset of components. Source code available at\nhttps://github.com/nicola-decao/KnowledgeEditor\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1\">Nicola De Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$Q^{2}$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering. (arXiv:2104.08202v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08202","description":"<p>Neural knowledge-grounded generative models for dialogue often produce\ncontent that is factually inconsistent with the knowledge they rely on, making\nthem unreliable and limiting their applicability. Inspired by recent work on\nevaluating factual consistency in abstractive summarization, we propose an\nautomatic evaluation metric for factual consistency in knowledge-grounded\ndialogue using automatic question generation and question answering. Our\nmetric, denoted $Q^2$, compares answer spans using natural language inference\n(NLI), instead of token-based matching as done in previous work. To foster\nproper evaluation, we curate a novel dataset of dialogue system outputs for the\nWizard-of-Wikipedia dataset, manually annotated for factual consistency. We\nperform a thorough meta-evaluation of $Q^2$ against other metrics using this\ndataset and two others, where it consistently shows higher correlation with\nhuman judgements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Honovich_O/0/1/0/all/0/1\">Or Honovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neeman_E/0/1/0/all/0/1\">Ella Neeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1\">Idan Szpektor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Topic Confusion Task: A Novel Scenario for Authorship Attribution. (arXiv:2104.08530v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08530","description":"<p>Authorship attribution is the problem of identifying the most plausible\nauthor of an anonymous text from a set of candidate authors. Researchers have\ninvestigated same-topic and cross-topic scenarios of authorship attribution,\nwhich differ according to whether new, unseen topics are used in the testing\nphase. However, neither scenario allows us to explain whether errors are caused\nby a failure to capture authorship writing style or by a topic shift. Motivated\nby this, we propose the \\emph{topic confusion} task where we switch the\nauthor-topic configuration between the training and testing sets. This setup\nallows us to distinguish two types of errors: those caused by the topic shift\nand those caused by the features' inability to capture the writing styles. We\nshow that stylometric features with part-of-speech tags are the least\nsusceptible to topic variations. We further show that combining them with other\nfeatures leads to significantly lower topic confusion and higher attribution\naccuracy. Finally, we show that pretrained language models such as BERT and\nRoBERTa perform poorly on this task and are surpassed by simple features such\nas word-level $n$-grams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Altakrori_M/0/1/0/all/0/1\">Malik H. Altakrori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_B/0/1/0/all/0/1\">Benjamin C. M. Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval. (arXiv:2104.08801v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08801","description":"<p>In this work, we introduce back-training, an alternative to self-training for\nunsupervised domain adaptation (UDA) from source to target domain. While\nself-training generates synthetic training data where natural inputs are\naligned with noisy outputs, back-training results in natural outputs aligned\nwith noisy inputs. This significantly reduces the gap between the target domain\nand synthetic data distribution, and reduces model overfitting to the source\ndomain. We run UDA experiments on question generation and passage retrieval\nfrom the \\textit{Natural Questions} domain to machine learning and biomedical\ndomains. We find that back-training vastly outperforms self-training by a mean\nimprovement of 7.8 BLEU-4 points on generation, and 17.6\\% top-20 retrieval\naccuracy across both domains. We further propose consistency filters to remove\nlow-quality synthetic data before training. We also release a new\ndomain-adaptation dataset- \\textit{MLQuestions} containing 35K unaligned\nquestions, 50K unaligned passages, and 3K aligned question-passage pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulshreshtha_D/0/1/0/all/0/1\">Devang Kulshreshtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belfer_R/0/1/0/all/0/1\">Robert Belfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serban_I/0/1/0/all/0/1\">Iulian Vlad Serban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Accelerated Inference via Confident Adaptive Transformers. (arXiv:2104.08803v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08803","description":"<p>We develop a novel approach for confidently accelerating inference in the\nlarge and expensive multilayer Transformers that are now ubiquitous in natural\nlanguage processing (NLP). Amortized or approximate computational methods\nincrease efficiency, but can come with unpredictable performance costs. In this\nwork, we present CATs -- Confident Adaptive Transformers -- in which we\nsimultaneously increase computational efficiency, while guaranteeing a\nspecifiable degree of consistency with the original model with high confidence.\nOur method trains additional prediction heads on top of intermediate layers,\nand dynamically decides when to stop allocating computational effort to each\ninput using a meta consistency classifier. To calibrate our early prediction\nstopping rule, we formulate a unique extension of conformal prediction. We\ndemonstrate the effectiveness of this approach on four classification and\nregression tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1\">Adam Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi Jaakkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible Generation of Natural Language Deductions. (arXiv:2104.08825v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08825","description":"<p>An interpretable system for open-domain reasoning needs to express its\nreasoning process in a transparent form. Natural language is an attractive\nrepresentation for this purpose -- it is both highly expressive and easy for\nhumans to understand. However, manipulating natural language statements in\nlogically consistent ways is hard: models must cope with variation in how\nmeaning is expressed while remaining precise. In this paper, we describe\nParaPattern, a method for building models to generate deductive inferences from\ndiverse natural language inputs without direct human supervision. We train\nBART-based models (Lewis et al., 2020) to generate the result of applying a\nparticular logical operation to one or more premise statements. Crucially, we\ndevelop a largely automated pipeline for constructing suitable training\nexamples from Wikipedia. We evaluate our models using out-of-domain sentence\ncompositions from the QASC (Khot et al., 2020) and EntailmentBank (Dalvi et\nal., 2021) datasets as well as targeted perturbation sets. Our results show\nthat our models are substantially more accurate and flexible than baseline\nsystems. ParaPattern achieves 85% validity on examples of the 'substitution'\noperation from EntailmentBank without the use of any in-domain training data,\nmatching the performance of a model fine-tuned for EntailmentBank. The full\nsource code for our method is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bostrom_K/0/1/0/all/0/1\">Kaj Bostrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Swarat Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding. (arXiv:2104.08836v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08836","description":"<p>Multimodal pre-training with text, layout, and image has achieved SOTA\nperformance for visually-rich document understanding tasks recently, which\ndemonstrates the great potential for joint learning across different\nmodalities. In this paper, we present LayoutXLM, a multimodal pre-trained model\nfor multilingual document understanding, which aims to bridge the language\nbarriers for visually-rich document understanding. To accurately evaluate\nLayoutXLM, we also introduce a multilingual form understanding benchmark\ndataset named XFUND, which includes form understanding samples in 7 languages\n(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and\nkey-value pairs are manually labeled for each language. Experiment results show\nthat the LayoutXLM model has significantly outperformed the existing SOTA\ncross-lingual pre-trained models on the XFUND dataset. The pre-trained\nLayoutXLM model and the XFUND dataset are publicly available at\nhttps://aka.ms/layoutxlm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teacher-Student MixIT for Unsupervised and Semi-supervised Speech Separation. (arXiv:2106.07843v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2106.07843","description":"<p>In this paper, we introduce a novel semi-supervised learning framework for\nend-to-end speech separation. The proposed method first uses mixtures of\nunseparated sources and the mixture invariant training (MixIT) criterion to\ntrain a teacher model. The teacher model then estimates separated sources that\nare used to train a student model with standard permutation invariant training\n(PIT). The student model can be fine-tuned with supervised data, i.e., paired\nartificial mixtures and clean speech sources, and further improved via model\ndistillation. Experiments with single and multi channel mixtures show that the\nteacher-student training resolves the over-separation problem observed in the\noriginal MixIT method. Further, the semisupervised performance is comparable to\na fully-supervised separation system trained using ten times the amount of\nsupervised data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jisi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorila_C/0/1/0/all/0/1\">Catalin Zorila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1\">Rama Doddipatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barker_J/0/1/0/all/0/1\">Jon Barker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.01198","description":"<p>In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation. (arXiv:2108.04556v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04556","description":"<p>Code representation learning, which aims to encode the semantics of source\ncode into distributed vectors, plays an important role in recent\ndeep-learning-based models for code intelligence. Recently, many pre-trained\nlanguage models for source code (e.g., CuBERT and CodeBERT) have been proposed\nto model the context of code and serve as a basis for downstream code\nintelligence tasks such as code search, code clone detection, and program\ntranslation. Current approaches typically consider the source code as a plain\nsequence of tokens, or inject the structure information (e.g., AST and\ndata-flow) into the sequential model pre-training. To further explore the\nproperties of programming languages, this paper proposes SynCoBERT, a\nsyntax-guided multi-modal contrastive pre-training approach for better code\nrepresentations. Specially, we design two novel pre-training objectives\noriginating from the symbolic and syntactic properties of source code, i.e.,\nIdentifier Prediction (IP) and AST Edge Prediction (TEP), which are designed to\npredict identifiers, and edges between two nodes of AST, respectively.\nMeanwhile, to exploit the complementary information in semantically equivalent\nmodalities (i.e., code, comment, AST) of the code, we propose a multi-modal\ncontrastive learning strategy to maximize the mutual information among\ndifferent modalities. Extensive experiments on four downstream tasks related to\ncode intelligence show that SynCoBERT advances the state-of-the-art with the\nsame pre-training corpus and model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features is dependent\nupon each other. Experiment results on six public datasets show that our model\nperforms significantly better than previous approaches. In addition, contrary\nto what previous work has claimed, our auxiliary experiments suggest that\nrelation prediction is contributory to named entity prediction in a\nnon-negligible way. The source code can be found at\nhttps://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Text Evaluation through the Lens of Wasserstein Barycenters. (arXiv:2108.12463v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12463","description":"<p>A new metric \\texttt{BaryScore} to evaluate text generation based on deep\ncontextualized embeddings e.g., BERT, Roberta, ELMo) is introduced. This metric\nis motivated by a new framework relying on optimal transport tools, i.e.,\nWasserstein distance and barycenter. By modelling the layer output of deep\ncontextualized embeddings as a probability distribution rather than by a vector\nembedding; this framework provides a natural way to aggregate the different\noutputs through the Wasserstein space topology. In addition, it provides\ntheoretical grounds to our metric and offers an alternative to available\nsolutions e.g., MoverScore and BertScore). Numerical evaluation is performed on\nfour different tasks: machine translation, summarization, data2text generation\nand image captioning. Our results show that \\texttt{BaryScore} outperforms\nother BERT based metrics and exhibits more consistent behaviour in particular\nfor text summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staerman_G/0/1/0/all/0/1\">Guillaume Staerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code-switched inspired losses for generic spoken dialog representations. (arXiv:2108.12465v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12465","description":"<p>Spoken dialog systems need to be able to handle both multiple languages and\nmultilinguality inside a conversation (\\textit{e.g} in case of code-switching).\nIn this work, we introduce new pretraining losses tailored to learn\nmultilingual spoken dialog representations. The goal of these losses is to\nexpose the model to code-switched language. To scale up training, we\nautomatically build a pretraining corpus composed of multilingual conversations\nin five different languages (French, Italian, English, German and Spanish) from\n\\texttt{OpenSubtitles}, a huge multilingual corpus composed of 24.3G tokens. We\ntest the generic representations on \\texttt{MIAM}, a new benchmark composed of\nfive dialog act corpora on the same aforementioned languages as well as on two\nnovel multilingual downstream tasks (\\textit{i.e} multilingual mask utterance\nretrieval and multilingual inconsistency identification). Our experiments show\nthat our new code switched-inspired losses achieve a better performance in both\nmonolingual and multilingual settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chapuis_E/0/1/0/all/0/1\">Emile Chapuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_M/0/1/0/all/0/1\">Matthieu Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiplex Graph Neural Network for Extractive Text Summarization. (arXiv:2108.12870v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12870","description":"<p>Extractive text summarization aims at extracting the most representative\nsentences from a given document as its summary. To extract a good summary from\na long text document, sentence embedding plays an important role. Recent\nstudies have leveraged graph neural networks to capture the inter-sentential\nrelationship (e.g., the discourse graph) to learn contextual sentence\nembedding. However, those approaches neither consider multiple types of\ninter-sentential relationships (e.g., semantic similarity &amp; natural\nconnection), nor model intra-sentential relationships (e.g, semantic &amp;\nsyntactic relationship among words). To address these problems, we propose a\nnovel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model\ndifferent types of relationships among sentences and words. Based on Multi-GCN,\nwe propose a Multiplex Graph Summarization (Multi-GraS) model for extractive\ntext summarization. Finally, we evaluate the proposed models on the\nCNN/DailyMail benchmark dataset to demonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Baoyu Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zeyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightNER: A Lightweight Generative Framework with Prompt-guided Attention for Low-resource NER. (arXiv:2109.00720v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00720","description":"<p>Most existing NER methods rely on extensive labeled data for model training,\nwhich struggles in the low-resource scenarios with limited training data.\nRecently, prompt-tuning methods for pre-trained language models have achieved\nremarkable performance in few-shot learning by exploiting prompts as task\nguidance to reduce the gap between training progress and downstream tuning.\nInspired by prompt learning, we propose a novel lightweight generative\nframework with prompt-guided attention for low-resource NER (LightNER).\nSpecifically, we construct the semantic-aware answer space of entity categories\nfor prompt learning to generate the entity span sequence and entity categories\nwithout any label-specific classifiers. We further propose prompt-guided\nattention by incorporating continuous prompts into the self-attention layer to\nre-modulate the attention and adapt pre-trained weights. Note that we only tune\nthose continuous prompts with the whole parameter of the pre-trained language\nmodel fixed, thus, making our approach lightweight and flexible for\nlow-resource scenarios and can better transfer knowledge across domains.\nExperimental results show that LightNER can obtain comparable performance in\nthe standard supervised setting and outperform strong baselines in low-resource\nsettings by tuning only a small part of the parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multimodal fusion via Mutual Dependency Maximisation. (arXiv:2109.00922v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.00922","description":"<p>Multimodal sentiment analysis is a trending area of research, and the\nmultimodal fusion is one of its most active topic. Acknowledging humans\ncommunicate through a variety of channels (i.e visual, acoustic, linguistic),\nmultimodal systems aim at integrating different unimodal representations into a\nsynthetic one. So far, a consequent effort has been made on developing complex\narchitectures allowing the fusion of these modalities. However, such systems\nare mainly trained by minimising simple losses such as $L_1$ or cross-entropy.\nIn this work, we investigate unexplored penalties and propose a set of new\nobjectives that measure the dependency between modalities. We demonstrate that\nour new penalties lead to a consistent improvement (up to $4.3$ on accuracy)\nacross a large variety of state-of-the-art models on two well-known sentiment\nanalysis datasets: \\texttt{CMU-MOSI} and \\texttt{CMU-MOSEI}. Our method not\nonly achieves a new SOTA on both datasets but also produces representations\nthat are more robust to modality drops. Finally, a by-product of our methods\nincludes a statistical network which can be used to interpret the high\ndimensional representations learnt by the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chapuis_E/0/1/0/all/0/1\">Emile Chapuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_M/0/1/0/all/0/1\">Matthieu Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02747","description":"<p>We aim to automatically identify human action reasons in online videos. We\nfocus on the widespread genre of lifestyle vlogs, in which people perform\nactions while verbally describing them. We introduce and make publicly\navailable the WhyAct dataset, consisting of 1,077 visual actions manually\nannotated with their reasons. We describe a multimodal model that leverages\nvisual and textual information to automatically infer the reasons corresponding\nto an action presented in the video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_H/0/1/0/all/0/1\">Hanwen Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles. (arXiv:2109.03158v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03158","description":"<p>An individual's variation in writing style is often a function of both social\nand personal attributes. While structured social variation has been extensively\nstudied, e.g., gender based variation, far less is known about how to\ncharacterize individual styles due to their idiosyncratic nature. We introduce\na new approach to studying idiolects through a massive cross-author comparison\nto identify and encode stylistic features. The neural model achieves strong\nperformance at authorship identification on short texts and through an\nanalogy-based probing task, showing that the learned representations exhibit\nsurprising regularities that encode qualitative and quantitative shifts of\nidiolectal styles. Through text perturbation, we quantify the relative\ncontributions of different linguistic elements to idiolectal variation.\nFurthermore, we provide a description of idiolects through measuring inter- and\nintra-author variation, showing that variation in idiolects is often\ndistinctive yet consistent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How much pretraining data do language models need to learn syntax?. (arXiv:2109.03160v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03160","description":"<p>Transformers-based pretrained language models achieve outstanding results in\nmany well-known NLU benchmarks. However, while pretraining methods are very\nconvenient, they are expensive in terms of time and resources. This calls for a\nstudy of the impact of pretraining data size on the knowledge of the models. We\nexplore this impact on the syntactic capabilities of RoBERTa, using models\ntrained on incremental sizes of raw text data. First, we use syntactic\nstructural probes to determine whether models pretrained on more data encode a\nhigher amount of syntactic information. Second, we perform a targeted syntactic\nevaluation to analyze the impact of pretraining data size on the syntactic\ngeneralization performance of the models. Third, we compare the performance of\nthe different models on three downstream applications: part-of-speech tagging,\ndependency parsing and paraphrase identification. We complement our study with\nan analysis of the cost-benefit trade-off of training such models. Our\nexperiments show that while models pretrained on more data encode more\nsyntactic knowledge and perform better on downstream applications, they do not\nalways offer a better performance across the different syntactic phenomena and\ncome at a higher financial and environmental cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Mayos_L/0/1/0/all/0/1\">Laura P&#xe9;rez-Mayos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanner_L/0/1/0/all/0/1\">Leo Wanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It is AI's Turn to Ask Human a Question: Question and Answer Pair Generation for Children Storybooks in FairytaleQA Dataset. (arXiv:2109.03423v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03423","description":"<p>Existing question answering (QA) datasets are created mainly for the\napplication of having AI to be able to answer questions asked by humans. But in\neducational applications, teachers and parents sometimes may not know what\nquestions they should ask a child that can maximize their language learning\nresults. With a newly released book QA dataset (FairytaleQA), which educational\nexperts labeled on 46 fairytale storybooks for early childhood readers, we\ndeveloped an automated QA generation model architecture for this novel\napplication. Our model (1) extracts candidate answers from a given storybook\npassage through carefully designed heuristics based on a pedagogical framework;\n(2) generates appropriate questions corresponding to each extracted answer\nusing a language model; and, (3) uses another QA model to rank top QA-pairs.\nAutomatic and human evaluations show that our model outperforms baselines. We\nalso demonstrate that our method can help with the scarcity issue of the\nchildren's book QA dataset via data augmentation on 200 unlabeled storybooks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Tran Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Branda Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Toby Jia-Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArchivalQA: A Large-scale Benchmark Dataset for Open Domain Question Answering over Archival News Collections. (arXiv:2109.03438v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03438","description":"<p>In the last few years, open-domain question answering (ODQA) has advanced\nrapidly due to the development of deep learning techniques and the availability\nof large-scale QA datasets. However, the current datasets are essentially\ndesigned for synchronic document collections (e.g., Wikipedia). Temporal news\ncollections such as long-term news archives spanning several decades, are\nrarely used in training the models despite they are quite valuable for our\nsociety. In order to foster the research in the field of ODQA on such\nhistorical collections, we present ArchivalQA, a large question answering\ndataset consisting of 1,067,056 question-answer pairs which is designed for\ntemporal news QA. In addition, we create four subparts of our dataset based on\nthe question difficulty levels and the containment of temporal expressions,\nwhich we believe could be useful for training or testing ODQA systems\ncharacterized by different strengths and abilities. The novel QA\ndataset-constructing framework that we introduce can be also applied to create\ndatasets over other types of collections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1\">Masatoshi Yoshikawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"OSSR-PID: One-Shot Symbol Recognition in P&ID Sheets using Path Sampling and GCN. (arXiv:2109.03849v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03849","description":"<p>Piping and Instrumentation Diagrams (P&amp;ID) are ubiquitous in several\nmanufacturing, oil and gas enterprises for representing engineering schematics\nand equipment layout. There is an urgent need to extract and digitize\ninformation from P&amp;IDs without the cost of annotating a varying set of symbols\nfor each new use case. A robust one-shot learning approach for symbol\nrecognition i.e., localization followed by classification, would therefore go a\nlong way towards this goal. Our method works by sampling pixels sequentially\nalong the different contour boundaries in the image. These sampled points form\npaths which are used in the prototypical line diagram to construct a graph that\ncaptures the structure of the contours. Subsequently, the prototypical graphs\nare fed into a Dynamic Graph Convolutional Neural Network (DGCNN) which is\ntrained to classify graphs into one of the given symbol classes. Further, we\nappend embeddings from a Resnet-34 network which is trained on symbol images\ncontaining sampled points to make the classification network more robust.\nSince, many symbols in P&amp;ID are structurally very similar to each other, we\nutilize Arcface loss during DGCNN training which helps in maximizing symbol\nclass separability by producing highly discriminative embeddings. The images\nconsist of components attached on the pipeline (straight line). The sampled\npoints segregated around the symbol regions are used for the classification\ntask. The proposed pipeline, named OSSR-PID, is fast and gives outstanding\nperformance for recognition of symbols on a synthetic dataset of 100 P&amp;ID\ndiagrams. We also compare our method against prior-work on a real-world private\ndataset of 12 P&amp;ID sheets and obtain comparable/superior results. Remarkably,\nit is able to achieve such excellent performance using only one prototypical\nexample per symbol.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paliwal_S/0/1/0/all/0/1\">Shubham Paliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Monika Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_L/0/1/0/all/0/1\">Lovekesh Vig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated LoD-2 Model Reconstruction from Very-HighResolution Satellite-derived Digital Surface Model and Orthophoto. (arXiv:2109.03876v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03876","description":"<p>In this paper, we propose a model-driven method that reconstructs LoD-2\nbuilding models following a \"decomposition-optimization-fitting\" paradigm. The\nproposed method starts building detection results through a deep learning-based\ndetector and vectorizes individual segments into polygons using a \"three-step\"\npolygon extraction method, followed by a novel grid-based decomposition method\nthat decomposes the complex and irregularly shaped building polygons to tightly\ncombined elementary building rectangles ready to fit elementary building\nmodels. We have optionally introduced OpenStreetMap (OSM) and Graph-Cut (GC)\nlabeling to further refine the orientation of 2D building rectangle. The 3D\nmodeling step takes building-specific parameters such as hip lines, as well as\nnon-rigid and regularized transformations to optimize the flexibility for using\na minimal set of elementary models. Finally, roof type of building models s\nrefined and adjacent building models in one building segment are merged into\nthe complex polygonal model. Our proposed method has addressed a few technical\ncaveats over existing methods, resulting in practically high-quality results,\nbased on our evaluation and comparative study on a diverse set of experimental\ndatasets of cities with different urban patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gui_S/0/1/0/all/0/1\">Shengxi Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SORNet: Spatial Object-Centric Representations for Sequential Manipulation. (arXiv:2109.03891v1 [cs.RO])","link":"http://arxiv.org/abs/2109.03891","description":"<p>Sequential manipulation tasks require a robot to perceive the state of an\nenvironment and plan a sequence of actions leading to a desired goal state,\nwhere the ability to reason about spatial relationships among object entities\nfrom raw sensor inputs is crucial. Prior works relying on explicit state\nestimation or end-to-end learning struggle with novel objects. In this work, we\npropose SORNet (Spatial Object-Centric Representation Network), which extracts\nobject-centric representations from RGB images conditioned on canonical views\nof the objects of interest. We show that the object embeddings learned by\nSORNet generalize zero-shot to unseen object entities on three spatial\nreasoning tasks: spatial relationship classification, skill precondition\nclassification and relative direction regression, significantly outperforming\nbaselines. Further, we present real-world robotic experiments demonstrating the\nusage of the learned object embeddings in task planning for sequential\nmanipulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Wentao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1\">Karthik Desingh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Building Segmentation for Off-Nadir Satellite Imagery. (arXiv:2109.03961v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03961","description":"<p>Automatic building segmentation is an important task for satellite imagery\nanalysis and scene understanding. Most existing segmentation methods focus on\nthe case where the images are taken from directly overhead (i.e., low\noff-nadir/viewing angle). These methods often fail to provide accurate results\non satellite images with larger off-nadir angles due to the higher noise level\nand lower spatial resolution. In this paper, we propose a method that is able\nto provide accurate building segmentation for satellite imagery captured from a\nlarge range of off-nadir angles. Based on Bayesian deep learning, we explicitly\ndesign our method to learn the data noise via aleatoric and epistemic\nuncertainty modeling. Satellite image metadata (e.g., off-nadir angle and\nground sample distance) is also used in our model to further improve the\nresult. We show that with uncertainty modeling and metadata injection, our\nmethod achieves better performance than the baseline method, especially for\nnoisy images taken from large off-nadir angles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1\">Hanxiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baireddy_S/0/1/0/all/0/1\">Sriram Baireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LaTourette_K/0/1/0/all/0/1\">Kevin LaTourette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konz_L/0/1/0/all/0/1\">Latisha Konz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_M/0/1/0/all/0/1\">Moses Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comer_M/0/1/0/all/0/1\">Mary L. Comer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Deep Metric Learning by Divide and Conquer. (arXiv:2109.04003v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04003","description":"<p>Deep metric learning (DML) is a cornerstone of many computer vision\napplications. It aims at learning a mapping from the input domain to an\nembedding space, where semantically similar objects are located nearby and\ndissimilar objects far from another. The target similarity on the training data\nis defined by user in form of ground-truth class labels. However, while the\nembedding space learns to mimic the user-provided similarity on the training\ndata, it should also generalize to novel categories not seen during training.\nBesides user-provided groundtruth training labels, a lot of additional visual\nfactors (such as viewpoint changes or shape peculiarities) exist and imply\ndifferent notions of similarity between objects, affecting the generalization\non the images unseen during training. However, existing approaches usually\ndirectly learn a single embedding space on all available training data,\nstruggling to encode all different types of relationships, and do not\ngeneralize well. We propose to build a more expressive representation by\njointly splitting the embedding space and the data hierarchically into smaller\nsub-parts. We successively focus on smaller subsets of the training data,\nreducing its variance and learning a different embedding subspace for each data\nsubset. Moreover, the subspaces are learned jointly to cover not only the\nintricacies, but the breadth of the data as well. Only after that, we build the\nfinal embedding from the subspaces in the conquering stage. The proposed\nalgorithm acts as a transparent wrapper that can be placed around arbitrary\nexisting DML methods. Our approach significantly improves upon the\nstate-of-the-art on image retrieval, clustering, and re-identification tasks\nevaluated using CUB200-2011, CARS196, Stanford Online Products, In-shop\nClothes, and PKU VehicleID datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanakoyeu_A/0/1/0/all/0/1\">Artsiom Sanakoyeu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingchuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschernezki_V/0/1/0/all/0/1\">Vadim Tschernezki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modified Supervised Contrastive Learning for Detecting Anomalous Driving Behaviours. (arXiv:2109.04021v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04021","description":"<p>Detecting distracted driving behaviours is important to reduce millions of\ndeaths and injuries occurring worldwide. Distracted or anomalous driving\nbehaviours are deviations from the 'normal' driving that need to be identified\ncorrectly to alert the driver. However, these driving behaviours do not\ncomprise of one specific type of driving style and their distribution can be\ndifferent during training and testing phases of a classifier. We formulate this\nproblem as a supervised contrastive learning approach to learn a visual\nrepresentation to detect normal, and seen and unseen anomalous driving\nbehaviours. We made a change to the standard contrastive loss function to\nadjust the similarity of negative pairs to aid the optimization. Normally, the\n(self) supervised contrastive framework contains an encoder followed by a\nprojection head, which is omitted during testing phase as the encoding layers\nare considered to contain general visual representative information. However,\nwe assert that for supervised contrastive learning task, including projection\nhead will be beneficial. We showed our results on a Driver Anomaly Detection\ndataset that contains 783 minutes of video recordings of normal and anomalous\ndriving behaviours of 31 drivers from various from top and front cameras (both\ndepth and infrared). We also performed an extra step of fine tuning the labels\nin this dataset. Out of 9 video modalities combinations, our modified\ncontrastive approach improved the ROC AUC on 7 in comparison to the baseline\nmodels (from 3.12% to 8.91% for different modalities); the remaining two models\nalso had manual labelling. We performed statistical tests that showed evidence\nthat our modifications perform better than the baseline contrastive models.\nFinally, the results showed that the fusion of depth and infrared modalities\nfrom top and front view achieved the best AUC ROC of 0.9738 and AUC PR of\n0.9772.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shehroz S. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Ziting Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haoying Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ax Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abedi_A/0/1/0/all/0/1\">Ali Abedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Tensor Network Representation for High-Order Tensor Completion. (arXiv:2109.04022v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04022","description":"<p>This work studies the problem of high-dimensional data (referred to tensors)\ncompletion from partially observed samplings. We consider that a tensor is a\nsuperposition of multiple low-rank components. In particular, each component\ncan be represented as multilinear connections over several latent factors and\nnaturally mapped to a specific tensor network (TN) topology. In this paper, we\npropose a fundamental tensor decomposition (TD) framework: Multi-Tensor Network\nRepresentation (MTNR), which can be regarded as a linear combination of a range\nof TD models, e.g., CANDECOMP/PARAFAC (CP) decomposition, Tensor Train (TT),\nand Tensor Ring (TR). Specifically, MTNR represents a high-order tensor as the\naddition of multiple TN models, and the topology of each TN is automatically\ngenerated instead of manually pre-designed. For the optimization phase, an\nadaptive topology learning (ATL) algorithm is presented to obtain latent\nfactors of each TN based on a rank incremental strategy and a projection error\nmeasurement strategy. In addition, we theoretically establish the fundamental\nmultilinear operations for the tensors with TN representation, and reveal the\nstructural transformation of MTNR to a single TN. Finally, MTNR is applied to a\ntypical task, tensor completion, and two effective algorithms are proposed for\nthe exact recovery of incomplete data based on the Alternating Least Squares\n(ALS) scheme and Alternating Direction Method of Multiplier (ADMM) framework.\nExtensive numerical experiments on synthetic data and real-world datasets\ndemonstrate the effectiveness of MTNR compared with the start-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_C/0/1/0/all/0/1\">Chang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zhihui Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACP++: Action Co-occurrence Priors for Human-Object Interaction Detection. (arXiv:2109.04047v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04047","description":"<p>A common problem in the task of human-object interaction (HOI) detection is\nthat numerous HOI classes have only a small number of labeled examples,\nresulting in training sets with a long-tailed distribution. The lack of\npositive labels can lead to low classification accuracy for these classes.\nTowards addressing this issue, we observe that there exist natural correlations\nand anti-correlations among human-object interactions. In this paper, we model\nthe correlations as action co-occurrence matrices and present techniques to\nlearn these priors and leverage them for more effective training, especially on\nrare classes. The efficacy of our approach is demonstrated experimentally,\nwhere the performance of our approach consistently improves over the\nstate-of-the-art methods on both of the two leading HOI detection benchmark\ndatasets, HICO-Det and V-COCO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinsoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of the Singular Spectrum Analysis on electroluminescence images of thin-film photovoltaic modules. (arXiv:2109.04048v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04048","description":"<p>This paper discusses an application of the singular spectrum analysis method\n(SSA) in the context of electroluminescence (EL) images of thin-film\nphotovoltaic (PV) modules. We propose an EL image decomposition as a sum of\nthree components: global intensity, cell, and aperiodic components. A\nparametric model of the extracted signal is used to perform several image\nprocessing tasks. The cell component is used to identify interconnection lines\nbetween PV cells at sub-pixel accuracy, as well as to correct incorrect\nstitching of EL images. Furthermore, an explicit expression of the cell\ncomponent signal is used to estimate the inverse characteristic length, a\nphysical parameter related to the resistances in a PV module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sovetkin_E/0/1/0/all/0/1\">Evgenii Sovetkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pieters_B/0/1/0/all/0/1\">Bart E. Pieters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self Supervision to Distillation for Long-Tailed Visual Recognition. (arXiv:2109.04075v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04075","description":"<p>Deep learning has achieved remarkable progress for visual recognition on\nlarge-scale balanced datasets but still performs poorly on real-world\nlong-tailed data. Previous methods often adopt class re-balanced training\nstrategies to effectively alleviate the imbalance issue, but might be a risk of\nover-fitting tail classes. The recent decoupling method overcomes over-fitting\nissues by using a multi-stage training scheme, yet, it is still incapable of\ncapturing tail class information in the feature learning stage. In this paper,\nwe show that soft label can serve as a powerful solution to incorporate label\ncorrelation into a multi-stage training scheme for long-tailed recognition. The\nintrinsic relation between classes embodied by soft labels turns out to be\nhelpful for long-tailed recognition by transferring knowledge from head to tail\nclasses.\n</p>\n<p>Specifically, we propose a conceptually simple yet particularly effective\nmulti-stage training scheme, termed as Self Supervised to Distillation (SSD).\nThis scheme is composed of two parts. First, we introduce a self-distillation\nframework for long-tailed recognition, which can mine the label relation\nautomatically. Second, we present a new distillation label generation module\nguided by self-supervision. The distilled labels integrate information from\nboth label and data domains that can model long-tailed distribution\neffectively. We conduct extensive experiments and our method achieves the\nstate-of-the-art results on three long-tailed recognition benchmarks:\nImageNet-LT, CIFAR100-LT and iNaturalist 2018. Our SSD outperforms the strong\nLWS baseline by from $2.7\\%$ to $4.5\\%$ on various datasets. The code is\navailable at https://github.com/MCG-NJU/SSD-LT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Cross-Scale Visual Representations for Real-Time Image Geo-Localization. (arXiv:2109.04087v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04087","description":"<p>Robot localization remains a challenging task in GPS denied environments.\nState estimation approaches based on local sensors, e.g. cameras or IMUs, are\ndrifting-prone for long-range missions as error accumulates. In this study, we\naim to address this problem by localizing image observations in a 2D\nmulti-modal geospatial map. We introduce the cross-scale dataset and a\nmethodology to produce additional data from cross-modality sources. We propose\na framework that learns cross-scale visual representations without supervision.\nExperiments are conducted on data from two different domains, underwater and\naerial. In contrast to existing studies in cross-view image geo-localization,\nour approach a) performs better on smaller-scale multi-modal maps; b) is more\ncomputationally efficient for real-time applications; c) can serve directly in\nconcert with state estimation pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_Roberson_M/0/1/0/all/0/1\">Matthew Johnson-Roberson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taming Self-Supervised Learning for Presentation Attack Detection: In-Image De-Folding and Out-of-Image De-Mixing. (arXiv:2109.04100v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04100","description":"<p>Biometric systems are vulnerable to the Presentation Attacks (PA) performed\nusing various Presentation Attack Instruments (PAIs). Even though there are\nnumerous Presentation Attack Detection (PAD) techniques based on both deep\nlearning and hand-crafted features, the generalization of PAD for unknown PAI\nis still a challenging problem. The common problem with existing deep\nlearning-based PAD techniques is that they may struggle with local optima,\nresulting in weak generalization against different PAs. In this work, we\npropose to use self-supervised learning to find a reasonable initialization\nagainst local trap, so as to improve the generalization ability in detecting\nPAs on the biometric system.The proposed method, denoted as IF-OM, is based on\na global-local view coupled with De-Folding and De-Mixing to derive the\ntask-specific representation for PAD.During De-Folding, the proposed technique\nwill learn region-specific features to represent samples in a local pattern by\nexplicitly maximizing cycle consistency. While, De-Mixing drives detectors to\nobtain the instance-specific features with global information for more\ncomprehensive representation by maximizing topological consistency. Extensive\nexperimental results show that the proposed method can achieve significant\nimprovements in terms of both face and fingerprint PAD in more complicated and\nhybrid datasets, when compared with the state-of-the-art methods. Specifically,\nwhen training in CASIA-FASD and Idiap Replay-Attack, the proposed method can\nachieve 18.60% Equal Error Rate (EER) in OULU-NPU and MSU-MFSD, exceeding\nbaseline performance by 9.54%. Code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haozhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhe Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSMD: An object motion detection algorithm using a Hybrid Spiking Neural Network Architecture. (arXiv:2109.04119v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04119","description":"<p>The detection of moving objects is a trivial task performed by vertebrate\nretinas, yet a complex computer vision task. Object-motion-sensitive ganglion\ncells (OMS-GC) are specialised cells in the retina that sense moving objects.\nOMS-GC take as input continuous signals and produce spike patterns as output,\nthat are transmitted to the Visual Cortex via the optic nerve. The Hybrid\nSensitive Motion Detector (HSMD) algorithm proposed in this work enhances the\nGSOC dynamic background subtraction (DBS) algorithm with a customised 3-layer\nspiking neural network (SNN) that outputs spiking responses akin to the OMS-GC.\nThe algorithm was compared against existing background subtraction (BS)\napproaches, available on the OpenCV library, specifically on the 2012 change\ndetection (CDnet2012) and the 2014 change detection (CDnet2014) benchmark\ndatasets. The results show that the HSMD was ranked overall first among the\ncompeting approaches and has performed better than all the other algorithms on\nfour of the categories across all the eight test metrics. Furthermore, the HSMD\nproposed in this paper is the first to use an SNN to enhance an existing state\nof the art DBS (GSOC) algorithm and the results demonstrate that the SNN\nprovides near real-time performance in realistic applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1\">Pedro Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomou_A/0/1/0/all/0/1\">Andreas Oikonomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_J/0/1/0/all/0/1\">Joao Filipe Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGinnity_T/0/1/0/all/0/1\">T.M. McGinnity</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tiny CNN for feature point description for document analysis: approach and dataset. (arXiv:2109.04134v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04134","description":"<p>In this paper, we study the problem of feature points description in the\ncontext of document analysis and template matching. Our study shows that the\nspecific training data is required for the task especially if we are to train a\nlightweight neural network that will be usable on devices with limited\ncomputational resources. In this paper, we construct and provide a dataset with\na method of training patches retrieval. We prove the effectiveness of this data\nby training a lightweight neural network and show how it performs in both\ndocuments and general patches matching. The training was done on the provided\ndataset in comparison with HPatches training dataset and for the testing we use\nHPatches testing framework and two publicly available datasets with various\ndocuments pictured on complex backgrounds: MIDV-500 and MIDV-2019.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheshkus_A/0/1/0/all/0/1\">A. Sheshkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirvonaya_A/0/1/0/all/0/1\">A. Chirvonaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arlazarov_V/0/1/0/all/0/1\">V.L. Arlazarov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Audio-Visual Smartphone Dataset And Evaluation. (arXiv:2109.04138v1 [cs.CR])","link":"http://arxiv.org/abs/2109.04138","description":"<p>Smartphones have been employed with biometric-based verification systems to\nprovide security in highly sensitive applications. Audio-visual biometrics are\ngetting popular due to the usability and also it will be challenging to spoof\nbecause of multi-modal nature. In this work, we present an audio-visual\nsmartphone dataset captured in five different recent smartphones. This new\ndataset contains 103 subjects captured in three different sessions considering\nthe different real-world scenarios. Three different languages are acquired in\nthis dataset to include the problem of language dependency of the speaker\nrecognition systems. These unique characteristics of this dataset will pave the\nway to implement novel state-of-the-art unimodal or audio-visual speaker\nrecognition systems. We also report the performance of the bench-marked\nbiometric verification systems on our dataset. The robustness of biometric\nalgorithms is evaluated towards multiple dependencies like signal noise,\ndevice, language and presentation attacks like replay and synthesized signals\nwith extensive experiments. The obtained results raised many concerns about the\ngeneralization properties of state-of-the-art biometrics methods in\nsmartphones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandalapu_H/0/1/0/all/0/1\">Hareesh Mandalapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_A/0/1/0/all/0/1\">Aravinda Reddy P N</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">K Sreenivasa Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1\">Pabitra Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasanna_S/0/1/0/all/0/1\">S R Mahadeva Prasanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIMNet: A Parallel, Iterative and Mimicking Network for Scene Text Recognition. (arXiv:2109.04145v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04145","description":"<p>Nowadays, scene text recognition has attracted more and more attention due to\nits various applications. Most state-of-the-art methods adopt an\nencoder-decoder framework with attention mechanism, which generates text\nautoregressively from left to right. Despite the convincing performance, the\nspeed is limited because of the one-by-one decoding strategy. As opposed to\nautoregressive models, non-autoregressive models predict the results in\nparallel with a much shorter inference time, but the accuracy falls behind the\nautoregressive counterpart considerably. In this paper, we propose a Parallel,\nIterative and Mimicking Network (PIMNet) to balance accuracy and efficiency.\nSpecifically, PIMNet adopts a parallel attention mechanism to predict the text\nfaster and an iterative generation mechanism to make the predictions more\naccurate. In each iteration, the context information is fully explored. To\nimprove learning of the hidden layer, we exploit the mimicking learning in the\ntraining phase, where an additional autoregressive decoder is adopted and the\nparallel decoder mimics the autoregressive decoder with fitting outputs of the\nhidden layer. With the shared backbone between the two decoders, the proposed\nPIMNet can be trained end-to-end without pre-training. During inference, the\nbranch of the autoregressive decoder is removed for a faster speed. Extensive\nexperiments on public benchmarks demonstrate the effectiveness and efficiency\nof PIMNet. Our code will be available at https://github.com/Pay20Y/PIMNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhi Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Ning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongbin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Image 3D Object Estimation with Primitive Graph Networks. (arXiv:2109.04153v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04153","description":"<p>Reconstructing 3D object from a single image (RGB or depth) is a fundamental\nproblem in visual scene understanding and yet remains challenging due to its\nill-posed nature and complexity in real-world scenes. To address those\nchallenges, we adopt a primitive-based representation for 3D object, and\npropose a two-stage graph network for primitive-based 3D object estimation,\nwhich consists of a sequential proposal module and a graph reasoning module.\nGiven a 2D image, our proposal module first generates a sequence of 3D\nprimitives from input image with local feature attention. Then the graph\nreasoning module performs joint reasoning on a primitive graph to capture the\nglobal shape context for each primitive. Such a framework is capable of taking\ninto account rich geometry and semantic constraints during 3D structure\nrecovery, producing 3D objects with more coherent structure even under\nchallenging viewing conditions. We train the entire graph neural network in a\nstage-wise strategy and evaluate it on three benchmarks: Pix3D, ModelNet and\nNYU Depth V2. Extensive experiments show that our approach outperforms the\nprevious state of the arts with a considerable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Desen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_B/0/1/0/all/0/1\">Bo Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Transferable Adversarial Attacks on Vision Transformers. (arXiv:2109.04176v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04176","description":"<p>Vision transformers (ViTs) have demonstrated impressive performance on a\nseries of computer vision tasks, yet they still suffer from adversarial\nexamples. In this paper, we posit that adversarial attacks on transformers\nshould be specially tailored for their architecture, jointly considering both\npatches and self-attention, in order to achieve high transferability. More\nspecifically, we introduce a dual attack framework, which contains a Pay No\nAttention (PNA) attack and a PatchOut attack, to improve the transferability of\nadversarial samples across different ViTs. We show that skipping the gradients\nof attention during backpropagation can generate adversarial examples with high\ntransferability. In addition, adversarial perturbations generated by optimizing\nrandomly sampled subsets of patches at each iteration achieve higher attack\nsuccess rates than attacks using all patches. We evaluate the transferability\nof attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The\nresults of these experiments demonstrate that the proposed dual attack can\ngreatly boost transferability between ViTs and from ViTs to CNNs. In addition,\nthe proposed method can easily be combined with existing transfer methods to\nboost performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhipeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Data Distribution Alignment for Post-Training Quantization. (arXiv:2109.04186v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04186","description":"<p>While post-training quantization receives popularity mostly due to its\nevasion in accessing the original complete training dataset, its poor\nperformance also stems from this limitation. To alleviate this limitation, in\nthis paper, we leverage the synthetic data introduced by zero-shot quantization\nwith calibration dataset and we propose a fine-grained data distribution\nalignment (FDDA) method to boost the performance of post-training quantization.\nThe method is based on two important properties of batch normalization\nstatistics (BNS) we observed in deep layers of the trained network, i.e.,\ninter-class separation and intra-class incohesion. To preserve this\nfine-grained distribution information: 1) We calculate the per-class BNS of the\ncalibration dataset as the BNS centers of each class and propose a\nBNS-centralized loss to force the synthetic data distributions of different\nclasses to be close to their own centers. 2) We add Gaussian noise into the\ncenters to imitate the incohesion and propose a BNS-distorted loss to force the\nsynthetic data distribution of the same class to be close to the distorted\ncenters. By introducing these two fine-grained losses, our method shows the\nstate-of-the-art performance on ImageNet, especially when the first and last\nlayers are quantized to low-bit as well. Our project is available at\nhttps://github.com/viperit/FDDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yunshan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengzhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fully Automated Segmentation of Rat Cardiac MRI by Leveraging Deep Learning Frameworks. (arXiv:2109.04188v1 [eess.IV])","link":"http://arxiv.org/abs/2109.04188","description":"<p>Automated segmentation of human cardiac magnetic resonance datasets has been\nsteadily improving during recent years. However, these methods are not directly\napplicable in preclinical context due to limited datasets and lower image\nresolution. Successful application of deep architectures for rat cardiac\nsegmentation, although of critical importance for preclinical evaluation of\ncardiac function, has to our knowledge not yet been reported. We developed\nsegmentation models that expand on the standard U-Net architecture and\nevaluated separate models for systole and diastole phases, 2MSA, and one model\nfor all timepoints, 1MSA. Furthermore, we calibrated model outputs using a\nGaussian Process (GP)-based prior to improve phase selection. Resulting models\napproach human performance in terms of left ventricular segmentation quality\nand ejection fraction (EF) estimation in both 1MSA and 2MSA settings\n(S{\\o}rensen-Dice score 0.91 +/- 0.072 and 0.93 +/- 0.032, respectively). 2MSA\nachieved a mean absolute difference between estimated and reference EF of 3.5\n+/- 2.5 %, while 1MSA resulted in 4.1 +/- 3.0 %. Applying Gaussian Processes to\n1MSA allows to automate the selection of systole and diastole phases. Combined\nwith a novel cardiac phase selection strategy, our work presents an important\nfirst step towards a fully automated segmentation pipeline in the context of\nrat cardiac analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fernandez_Llaneza_D/0/1/0/all/0/1\">Daniel Fernandez-Llaneza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gondova_A/0/1/0/all/0/1\">Andrea Gondova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vince_H/0/1/0/all/0/1\">Harris Vince</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patra_A/0/1/0/all/0/1\">Arijit Patra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zurek_M/0/1/0/all/0/1\">Magdalena Zurek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Konings_P/0/1/0/all/0/1\">Peter Konings</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kagelid_P/0/1/0/all/0/1\">Patrik Kagelid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hultin_L/0/1/0/all/0/1\">Leif Hultin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IMG2SMI: Translating Molecular Structure Images to Simplified Molecular-input Line-entry System. (arXiv:2109.04202v1 [q-bio.QM])","link":"http://arxiv.org/abs/2109.04202","description":"<p>Like many scientific fields, new chemistry literature has grown at a\nstaggering pace, with thousands of papers released every month. A large portion\nof chemistry literature focuses on new molecules and reactions between\nmolecules. Most vital information is conveyed through 2-D images of molecules,\nrepresenting the underlying molecules or reactions described. In order to\nensure reproducible and machine-readable molecule representations, text-based\nmolecule descriptors like SMILES and SELFIES were created. These text-based\nmolecule representations provide molecule generation but are unfortunately\nrarely present in published literature. In the absence of molecule descriptors,\nthe generation of molecule descriptors from the 2-D images present in the\nliterature is necessary to understand chemistry literature at scale. Successful\nmethods such as Optical Structure Recognition Application (OSRA), and\nChemSchematicResolver are able to extract the locations of molecules structures\nin chemistry papers and infer molecular descriptions and reactions. While\neffective, existing systems expect chemists to correct outputs, making them\nunsuitable for unsupervised large-scale data mining. Leveraging the task\nformulation of image captioning introduced by DECIMER, we introduce IMG2SMI, a\nmodel which leverages Deep Residual Networks for image feature extraction and\nan encoder-decoder Transformer layers for molecule description generation.\nUnlike previous Neural Network-based systems, IMG2SMI builds around the task of\nmolecule description generation, which enables IMG2SMI to outperform OSRA-based\nsystems by 163% in molecule similarity prediction as measured by the molecular\nMACCS Fingerprint Tanimoto Similarity. Additionally, to facilitate further\nresearch on this task, we release a new molecule prediction dataset. including\n81 million molecules for molecule description generation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IICNet: A Generic Framework for Reversible Image Conversion. (arXiv:2109.04242v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04242","description":"<p>Reversible image conversion (RIC) aims to build a reversible transformation\nbetween specific visual content (e.g., short videos) and an embedding image,\nwhere the original content can be restored from the embedding when necessary.\nThis work develops Invertible Image Conversion Net (IICNet) as a generic\nsolution to various RIC tasks due to its strong capacity and task-independent\ndesign. Unlike previous encoder-decoder based methods, IICNet maintains a\nhighly invertible structure based on invertible neural networks (INNs) to\nbetter preserve the information during conversion. We use a relation module and\na channel squeeze layer to improve the INN nonlinearity to extract cross-image\nrelations and the network flexibility, respectively. Experimental results\ndemonstrate that IICNet outperforms the specifically-designed methods on\nexisting RIC tasks and can generalize well to various newly-explored tasks.\nWith our generic IICNet, we no longer need to hand-engineer task-specific\nembedding networks for rapidly occurring visual content. Our source codes are\navailable at: https://github.com/felixcheng97/IICNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Ka Leong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yueqi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M5Product: A Multi-modal Pretraining Benchmark for E-commercial Product Downstream Tasks. (arXiv:2109.04275v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04275","description":"<p>In this paper, we aim to advance the research of multi-modal pre-training on\nE-commerce and subsequently contribute a large-scale dataset, named M5Product,\nwhich consists of over 6 million multimodal pairs, covering more than 6,000\ncategories and 5,000 attributes. Generally, existing multi-modal datasets are\neither limited in scale or modality diversity. Differently, our M5Product is\nfeatured from the following aspects. First, the M5Product dataset is 500 times\nlarger than the public multimodal dataset with the same number of modalities\nand nearly twice larger compared with the largest available text-image\ncross-modal dataset. Second, the dataset contains rich information of multiple\nmodalities including image, text, table, video and audio, in which each\nmodality can capture different views of semantic information (e.g. category,\nattributes, affordance, brand, preference) and complements the other. Third, to\nbetter accommodate with real-world problems, a few portion of M5Product\ncontains incomplete modality pairs and noises while having the long-tailed\ndistribution, which aligns well with real-world scenarios. Finally, we provide\na baseline model M5-MMT that makes the first attempt to integrate the different\nmodality configuration into an unified model for feature fusion to address the\ngreat challenge for semantic alignment. We also evaluate various multi-model\npre-training state-of-the-arts for benchmarking their capabilities in learning\nfrom unlabeled data under the different number of modalities on the M5Product\ndataset. We conduct extensive experiments on four downstream tasks and provide\nsome interesting findings on these modalities. Our dataset and related code are\navailable at https://xiaodongsuper.github.io/M5Product_dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xunlin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoyong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Minlong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Cross-domain Image Understanding with Unsupervised Noise Removal. (arXiv:2109.04284v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04284","description":"<p>Deep learning models usually require a large amount of labeled data to\nachieve satisfactory performance. In multimedia analysis, domain adaptation\nstudies the problem of cross-domain knowledge transfer from a label rich source\ndomain to a label scarce target domain, thus potentially alleviates the\nannotation requirement for deep learning models. However, we find that\ncontemporary domain adaptation methods for cross-domain image understanding\nperform poorly when source domain is noisy. Weakly Supervised Domain Adaptation\n(WSDA) studies the domain adaptation problem under the scenario where source\ndata can be noisy. Prior methods on WSDA remove noisy source data and align the\nmarginal distribution across domains without considering the fine-grained\nsemantic structure in the embedding space, which have the problem of class\nmisalignment, e.g., features of cats in the target domain might be mapped near\nfeatures of dogs in the source domain. In this paper, we propose a novel\nmethod, termed Noise Tolerant Domain Adaptation, for WSDA. Specifically, we\nadopt the cluster assumption and learn cluster discriminatively with class\nprototypes in the embedding space. We propose to leverage the location\ninformation of the data points in the embedding space and model the location\ninformation with a Gaussian mixture model to identify noisy source data. We\nthen design a network which incorporates the Gaussian mixture noise model as a\nsub-module for unsupervised noise removal and propose a novel cluster-level\nadversarial adaptation method which aligns unlabeled target data with the less\nnoisy class prototypes for mapping the semantic structure across domains. We\nconduct extensive experiments to evaluate the effectiveness of our method on\nboth general images and medical images from COVID-19 and e-commerce datasets.\nThe results show that our method significantly outperforms state-of-the-art\nWSDA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhaojing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaiping Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy-Efficient Mobile Robot Control via Run-time Monitoring of Environmental Complexity and Computing Workload. (arXiv:2109.04285v1 [cs.RO])","link":"http://arxiv.org/abs/2109.04285","description":"<p>We propose an energy-efficient controller to minimize the energy consumption\nof a mobile robot by dynamically manipulating the mechanical and computational\nactuators of the robot. The mobile robot performs real-time vision-based\napplications based on an event-based camera. The actuators of the controller\nare CPU voltage/frequency for the computation part and motor voltage for the\nmechanical part. We show that independently considering speed control of the\nrobot and voltage/frequency control of the CPU does not necessarily result in\nan energy-efficient solution. In fact, to obtain the highest efficiency, the\ncomputation and mechanical parts should be controlled together in synergy. We\npropose a fast hill-climbing optimization algorithm to allow the controller to\nfind the best CPU/motor configuration at run-time and whenever the mobile robot\nis facing a new environment during its travel. Experimental results on a robot\nwith Brushless DC Motors, Jetson TX2 board as the computing unit, and a\nDAVIS-346 event-based camera show that the proposed control algorithm can save\nbattery energy by an average of 50.5%, 41%, and 30%, in low-complexity,\nmedium-complexity, and high-complexity environments, over baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_S/0/1/0/all/0/1\">Sherif A.S. Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghbayan_M/0/1/0/all/0/1\">Mohammad-Hashem Haghbayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miele_A/0/1/0/all/0/1\">Antonio Miele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1\">Onur Mutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plosila_J/0/1/0/all/0/1\">Juha Plosila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss. (arXiv:2109.04290v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04290","description":"<p>Employing large-scale pre-trained model CLIP to conduct video-text retrieval\ntask (VTR) has become a new trend, which exceeds previous VTR methods. Though,\ndue to the heterogeneity of structures and contents between video and text,\nprevious CLIP-based models are prone to overfitting in the training phase,\nresulting in relatively poor retrieval performance. In this paper, we propose a\nmulti-stream Corpus Alignment network with single gate Mixture-of-Experts\n(CAMoE) and a novel Dual Softmax Loss (DSL) to solve the two heterogeneity. The\nCAMoE employs Mixture-of-Experts (MoE) to extract multi-perspective video\nrepresentations, including action, entity, scene, etc., then align them with\nthe corresponding part of the text. In this stage, we conduct massive\nexplorations towards the feature extraction module and feature alignment\nmodule. DSL is proposed to avoid the one-way optimum-match which occurs in\nprevious contrastive methods. Introducing the intrinsic prior of each pair in a\nbatch, DSL serves as a reviser to correct the similarity matrix and achieves\nthe dual optimal match. DSL is easy to implement with only one-line code but\nimproves significantly. The results show that the proposed CAMoE and DSL are of\nstrong efficiency, and each of them is capable of achieving State-of-The-Art\n(SOTA) individually on various benchmarks such as MSR-VTT, MSVD, and LSMDC.\nFurther, with both of them, the performance is advanced to a big extend,\nsurpassing the previous SOTA methods for around 4.6\\% R@1 in MSR-VTT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hezheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiangyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dong Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy Attack: On Transferring Adversarial Examples. (arXiv:2109.04300v1 [cs.LG])","link":"http://arxiv.org/abs/2109.04300","description":"<p>In this work we propose Energy Attack, a transfer-based black-box\n$L_\\infty$-adversarial attack. The attack is parameter-free and does not\nrequire gradient approximation. In particular, we first obtain white-box\nadversarial perturbations of a surrogate model and divide these perturbations\ninto small patches. Then we extract the unit component vectors and eigenvalues\nof these patches with principal component analysis (PCA). Base on the\neigenvalues, we can model the energy distribution of adversarial perturbations.\nWe then perform black-box attacks by sampling from the perturbation patches\naccording to their energy distribution, and tiling the sampled patches to form\na full-size adversarial perturbation. This can be done without the available\naccess to victim models. Extensive experiments well demonstrate that the\nproposed Energy Attack achieves state-of-the-art performance in black-box\nattacks on various models and several datasets. Moreover, the extracted\ndistribution is able to transfer among different model architectures and\ndifferent datasets, and is therefore intrinsic to vision architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Ruoxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Borui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yangzhou Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenglong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Hough Voting for Robust Global Registration. (arXiv:2109.04310v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04310","description":"<p>Point cloud registration is the task of estimating the rigid transformation\nthat aligns a pair of point cloud fragments. We present an efficient and robust\nframework for pairwise registration of real-world 3D scans, leveraging Hough\nvoting in the 6D transformation parameter space. First, deep geometric features\nare extracted from a point cloud pair to compute putative correspondences. We\nthen construct a set of triplets of correspondences to cast votes on the 6D\nHough space, representing the transformation parameters in sparse tensors.\nNext, a fully convolutional refinement module is applied to refine the noisy\nvotes. Finally, we identify the consensus among the correspondences from the\nHough space, which we use to predict our final transformation parameters. Our\nmethod outperforms state-of-the-art methods on 3DMatch and 3DLoMatch benchmarks\nwhile achieving comparable performance on KITTI odometry dataset. We further\ndemonstrate the generalizability of our approach by setting a new\nstate-of-the-art on ICL-NUIM dataset, where we integrate our module into a\nmulti-way registration pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Event-Line Constraint for Closed-Form Velocity Initialization. (arXiv:2109.04313v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04313","description":"<p>Event cameras trigger events asynchronously and independently upon a\nsufficient change of the logarithmic brightness level. The neuromorphic sensor\nhas several advantages over standard cameras including low latency, absence of\nmotion blur, and high dynamic range. Event cameras are particularly well suited\nto sense motion dynamics in agile scenarios. We propose the continuous\nevent-line constraint, which relies on a constant-velocity motion assumption as\nwell as trifocal tensor geometry in order to express a relationship between\nline observations given by event clusters as well as first-order camera\ndynamics. Our core result is a closed-form solver for up-to-scale linear camera\nvelocity {with known angular velocity}. Nonlinear optimization is adopted to\nimprove the performance of the algorithm. The feasibility of the approach is\ndemonstrated through a careful analysis on both simulated and real data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xin_P/0/1/0/all/0/1\">Peng Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wangting_X/0/1/0/all/0/1\">Xu Wangting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiaqi_Y/0/1/0/all/0/1\">Yang Jiaqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_K/0/1/0/all/0/1\">Kneip Laurent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer. (arXiv:2109.04335v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04335","description":"<p>Most recent semantic segmentation methods adopt a U-Net framework with an\nencoder-decoder architecture. It is still challenging for U-Net with a simple\nskip connection scheme to model the global multi-scale context: 1) Not each\nskip connection setting is effective due to the issue of incompatible feature\nsets of encoder and decoder stage, even some skip connection negatively\ninfluence the segmentation performance; 2) The original U-Net is worse than the\none without any skip connection on some datasets. Based on our findings, we\npropose a new segmentation framework, named UCTransNet (with a proposed CTrans\nmodule in U-Net), from the channel perspective with attention mechanism.\nSpecifically, the CTrans module is an alternate of the U-Net skip connections,\nwhich consists of a sub-module to conduct the multi-scale Channel Cross fusion\nwith Transformer (named CCT) and a sub-module Channel-wise Cross-Attention\n(named CCA) to guide the fused multi-scale channel-wise information to\neffectively connect to the decoder features for eliminating the ambiguity.\nHence, the proposed connection consisting of the CCT and CCA is able to replace\nthe original skip connection to solve the semantic gaps for an accurate\nautomatic medical image segmentation. The experimental results suggest that our\nUCTransNet produces more precise segmentation performance and achieves\nconsistent improvements over the state-of-the-art for semantic segmentation\nacross different datasets and conventional architectures involving transformer\nor U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Peng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar R.Zaiane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PhysGNN: A Physics-Driven Graph Neural Network Based Model for Predicting Soft Tissue Deformation in Image-Guided Neurosurgery. (arXiv:2109.04352v1 [eess.IV])","link":"http://arxiv.org/abs/2109.04352","description":"<p>Correctly capturing intraoperative brain shift in image-guided neurosurgical\nprocedures is a critical task for aligning preoperative data with\nintraoperative geometry, ensuring effective surgical navigation and optimal\nsurgical precision. While the finite element method (FEM) is a proven technique\nto effectively approximate soft tissue deformation through biomechanical\nformulations, their degree of success boils down to a trade-off between\naccuracy and speed. To circumvent this problem, the most recent works in this\ndomain have proposed leveraging data-driven models obtained by training various\nmachine learning algorithms, e.g. random forests, artificial neural networks\n(ANNs), with the results of finite element analysis (FEA) to speed up tissue\ndeformation approximations by prediction. These methods, however, do not\naccount for the structure of the finite element (FE) mesh during training that\nprovides information on node connectivities as well as the distance between\nthem, which can aid with approximating tissue deformation based on the\nproximity of force load points with the rest of the mesh nodes. Therefore, this\nwork proposes a novel framework, PhysGNN, a data-driven model that approximates\nthe solution of FEA by leveraging graph neural networks (GNNs), which are\ncapable of accounting for the mesh structural information and inductive\nlearning over unstructured grids and complex topological structures.\nEmpirically, we demonstrate that the proposed architecture, PhysGNN, promises\naccurate and fast soft tissue deformation approximations while remaining\ncomputationally feasible, suitable for neurosurgical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Salehi_Y/0/1/0/all/0/1\">Yasmin Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Giannacopoulos_D/0/1/0/all/0/1\">Dennis Giannacopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IFBiD: Inference-Free Bias Detection. (arXiv:2109.04374v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04374","description":"<p>This paper is the first to explore an automatic way to detect bias in deep\nconvolutional neural networks by simply looking at their weights. Furthermore,\nit is also a step towards understanding neural networks and how they work. We\nshow that it is indeed possible to know if a model is biased or not simply by\nlooking at its weights, without the model inference for an specific input. We\nanalyze how bias is encoded in the weights of deep networks through a toy\nexample using the Colored MNIST database and we also provide a realistic case\nstudy in gender detection from face images using state-of-the-art methods and\nexperimental resources. To do so, we generated two databases with 36K and 48K\nbiased models each. In the MNIST models we were able to detect whether they\npresented a strong or low bias with more than 99% accuracy, and we were also\nable to classify between four levels of bias with more than 70% accuracy. For\nthe face models, we achieved 90% accuracy in distinguishing between models\nbiased towards Asian, Black, or Caucasian ethnicity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serna_I/0/1/0/all/0/1\">Ignacio Serna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Modeling of Hand-Object Interactions via Tactile Sensing. (arXiv:2109.04378v1 [cs.RO])","link":"http://arxiv.org/abs/2109.04378","description":"<p>Tactile sensing is critical for humans to perform everyday tasks. While\nsignificant progress has been made in analyzing object grasping from vision, it\nremains unclear how we can utilize tactile sensing to reason about and model\nthe dynamics of hand-object interactions. In this work, we employ a\nhigh-resolution tactile glove to perform four different interactive activities\non a diversified set of objects. We build our model on a cross-modal learning\nframework and generate the labels using a visual processing pipeline to\nsupervise the tactile model, which can then be used on its own during the test\ntime. The tactile model aims to predict the 3d locations of both the hand and\nthe object purely from the touch data by combining a predictive model and a\ncontrastive learning module. This framework can reason about the interaction\npatterns from the tactile data, hallucinate the changes in the environment,\nestimate the uncertainty of the prediction, and generalize to unseen objects.\nWe also provide detailed ablation studies regarding different system designs as\nwell as visualizations of the predicted trajectories. This work takes a step on\ndynamics modeling in hand-object interactions from dense tactile sensing, which\nopens the door for future applications in activity learning, human-computer\ninteractions, and imitation learning for robotics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yiyue Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_W/0/1/0/all/0/1\">Wan Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foshey_M/0/1/0/all/0/1\">Michael Foshey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matusik_W/0/1/0/all/0/1\">Wojciech Matusik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts. (arXiv:2109.04379v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04379","description":"<p>Preserving maximal information is one of principles of designing\nself-supervised learning methodologies. To reach this goal, contrastive\nlearning adopts an implicit way which is contrasting image pairs. However, we\nbelieve it is not fully optimal to simply use the contrastive estimation for\npreservation. Moreover, it is necessary and complemental to introduce an\nexplicit solution to preserve more information. From this perspective, we\nintroduce Preservational Learning to reconstruct diverse image contexts in\norder to preserve more information in learned representations. Together with\nthe contrastive loss, we present Preservational Contrastive Representation\nLearning (PCRL) for learning self-supervised medical representations. PCRL\nprovides very competitive results under the pretraining-finetuning protocol,\noutperforming both self-supervised and supervised counterparts in 5\nclassification/segmentation tasks substantially.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chixiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sibei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Copy-Move Image Forgery Detection Based on Evolving Circular Domains Coverage. (arXiv:2109.04381v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04381","description":"<p>The aim of this paper is to improve the accuracy of copy-move forgery\ndetection (CMFD) in image forensics by proposing a novel scheme. The proposed\nscheme integrates both block-based and keypoint-based forgery detection\nmethods. Firstly, speed-up robust feature (SURF) descriptor in log-polar space\nand scale invariant feature transform (SIFT) descriptor are extracted from an\nentire forged image. Secondly, generalized 2 nearest neighbor (g2NN) is\nemployed to get massive matched pairs. Then, random sample consensus (RANSAC)\nalgorithm is employed to filter out mismatched pairs, thus allowing rough\nlocalization of the counterfeit areas. To present more accurately these forgery\nareas more accurately, we propose an efficient and accurate algorithm, evolving\ncircular domains coverage (ECDC), to cover present them. This algorithm aims to\nfind satisfactory threshold areas by extracting block features from jointly\nevolving circular domains, which are centered on the matched pairs. Finally,\nmorphological operation is applied to refine the detected forgery areas. The\nexperimental results indicate that the proposed CMFD scheme can achieve better\ndetection performance under various attacks compared with other\nstate-of-the-art CMFD schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shilin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinghong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shulu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yuejia Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ErfAct: Non-monotonic smooth trainable Activation Functions. (arXiv:2109.04386v1 [cs.NE])","link":"http://arxiv.org/abs/2109.04386","description":"<p>An activation function is a crucial component of a neural network that\nintroduces non-linearity in the network. The state-of-the-art performance of a\nneural network depends on the perfect choice of an activation function. We\npropose two novel non-monotonic smooth trainable activation functions, called\nErfAct-1 and ErfAct-2. Experiments suggest that the proposed functions improve\nthe network performance significantly compared to the widely used activations\nlike ReLU, Swish, and Mish. Replacing ReLU by ErfAct-1 and ErfAct-2, we have\n5.21% and 5.04% improvement for top-1 accuracy on PreactResNet-34 network in\nCIFAR100 dataset, 2.58% and 2.76% improvement for top-1 accuracy on\nPreactResNet-34 network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean\naverage precision (mAP) on SSD300 model in Pascal VOC dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1\">Koushik Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sandeep Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Shilpak Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Ashish Kumar Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair Conformal Predictors for Applications in Medical Imaging. (arXiv:2109.04392v1 [eess.IV])","link":"http://arxiv.org/abs/2109.04392","description":"<p>Deep learning has the potential to augment many components of the clinical\nworkflow, such as medical image interpretation. However, the translation of\nthese black box algorithms into clinical practice has been marred by the\nrelative lack of transparency compared to conventional machine learning\nmethods, hindering in clinician trust in the systems for critical medical\ndecision-making. Specifically, common deep learning approaches do not have\nintuitive ways of expressing uncertainty with respect to cases that might\nrequire further human review. Furthermore, the possibility of algorithmic bias\nhas caused hesitancy regarding the use of developed algorithms in clinical\nsettings. To these ends, we explore how conformal methods can complement deep\nlearning models by providing both clinically intuitive way (by means of\nconfidence prediction sets) of expressing model uncertainty as well as\nfacilitating model transparency in clinical workflows. In this paper, we\nconduct a field survey with clinicians to assess clinical use-cases of\nconformal predictions. Next, we conduct experiments with a mammographic breast\ndensity and dermatology photography datasets to demonstrate the utility of\nconformal predictions in \"rule-in\" and \"rule-out\" disease scenarios. Further,\nwe show that conformal predictors can be used to equalize coverage with respect\nto patient demographics such as race and skin tone. We find that a conformal\npredictions to be a promising framework with potential to increase clinical\nusability and transparency for better collaboration between deep learning\nalgorithms and clinicians.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_C/0/1/0/all/0/1\">Charles Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemay_A/0/1/0/all/0/1\">Andreanne Lemay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Ken Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural-IMLS: Learning Implicit Moving Least-Squares for Surface Reconstruction from Unoriented Point clouds. (arXiv:2109.04398v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04398","description":"<p>Surface reconstruction from noisy, non-uniformly, and unoriented point clouds\nis a fascinating yet difficult problem in computer vision and computer\ngraphics. In this paper, we propose Neural-IMLS, a novel approach that learning\nnoise-resistant signed distance function (SDF) for reconstruction. Instead of\nexplicitly learning priors with the ground-truth signed distance values, our\nmethod learns the SDF from raw point clouds directly in a self-supervised\nfashion by minimizing the loss between the couple of SDFs, one obtained by the\nimplicit moving least-square function (IMLS) and the other by our network.\nFinally, a watertight and smooth 2-manifold triangle mesh is yielded by running\nMarching Cubes. We conduct extensive experiments on various benchmarks to\ndemonstrate the performance of Neural-IMLS, especially for point clouds with\nnoise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiujie Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junjie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuangmin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_S/0/1/0/all/0/1\">Shiqing Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1\">Changhe Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstructing and grounding narrated instructional videos in 3D. (arXiv:2109.04409v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04409","description":"<p>Narrated instructional videos often show and describe manipulations of\nsimilar objects, e.g., repairing a particular model of a car or laptop. In this\nwork we aim to reconstruct such objects and to localize associated narrations\nin 3D. Contrary to the standard scenario of instance-level 3D reconstruction,\nwhere identical objects or scenes are present in all views, objects in\ndifferent instructional videos may have large appearance variations given\nvarying conditions and versions of the same product. Narrations may also have\nlarge variation in natural language expressions. We address these challenges by\nthree contributions. First, we propose an approach for correspondence\nestimation combining learnt local features and dense flow. Second, we design a\ntwo-step divide and conquer reconstruction approach where the initial 3D\nreconstructions of individual videos are combined into a 3D alignment graph.\nFinally, we propose an unsupervised approach to ground natural language in\nobtained 3D reconstructions. We demonstrate the effectiveness of our approach\nfor the domain of car maintenance. Given raw instructional videos and no manual\nsupervision, our method successfully reconstructs engines of different car\nmodels and associates textual descriptions with corresponding objects in 3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhukov_D/0/1/0/all/0/1\">Dimitri Zhukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocco_I/0/1/0/all/0/1\">Ignacio Rocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnberger_J/0/1/0/all/0/1\">Johannes L. Schnberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekin_B/0/1/0/all/0/1\">Bugra Tekin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TxT: Crossmodal End-to-End Learning with Transformers. (arXiv:2109.04422v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04422","description":"<p>Reasoning over multiple modalities, e.g. in Visual Question Answering (VQA),\nrequires an alignment of semantic concepts across domains. Despite the\nwidespread success of end-to-end learning, today's multimodal pipelines by and\nlarge leverage pre-extracted, fixed features from object detectors, typically\nFaster R-CNN, as representations of the visual world. The obvious downside is\nthat the visual representation is not specifically tuned to the multimodal task\nat hand. At the same time, while transformer-based object detectors have gained\npopularity, they have not been employed in today's multimodal pipelines. We\naddress both shortcomings with TxT, a transformer-based crossmodal pipeline\nthat enables fine-tuning both language and visual components on the downstream\ntask in a fully end-to-end manner. We overcome existing limitations of\ntransformer-based detectors for multimodal reasoning regarding the integration\nof global context and their scalability. Our transformer-based multimodal model\nachieves considerable gains from end-to-end learning for multimodal question\nanswering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steitz_J/0/1/0/all/0/1\">Jan-Martin O. Steitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_S/0/1/0/all/0/1\">Stefan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talk-to-Edit: Fine-Grained Facial Editing via Dialog. (arXiv:2109.04425v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04425","description":"<p>Facial editing is an important task in vision and graphics with numerous\napplications. However, existing works are incapable to deliver a continuous and\nfine-grained editing mode (e.g., editing a slightly smiling face to a big\nlaughing one) with natural interactions with users. In this work, we propose\nTalk-to-Edit, an interactive facial editing framework that performs\nfine-grained attribute manipulation through dialog between the user and the\nsystem. Our key insight is to model a continual \"semantic field\" in the GAN\nlatent space. 1) Unlike previous works that regard the editing as traversing\nstraight lines in the latent space, here the fine-grained editing is formulated\nas finding a curving trajectory that respects fine-grained attribute landscape\non the semantic field. 2) The curvature at each step is location-specific and\ndetermined by the input image as well as the users' language requests. 3) To\nengage the users in a meaningful dialog, our system generates language feedback\nby considering both the user request and the current state of the semantic\nfield.\n</p>\n<p>We also contribute CelebA-Dialog, a visual-language facial editing dataset to\nfacilitate large-scale study. Specifically, each image has manually annotated\nfine-grained attribute annotations as well as template-based textual\ndescriptions in natural language. Extensive quantitative and qualitative\nexperiments demonstrate the superiority of our framework in terms of 1) the\nsmoothness of fine-grained editing, 2) the identity/attribute preservation, and\n3) the visual photorealism and dialog fluency. Notably, user study validates\nthat our overall system is consistently favored by around 80% of the\nparticipants. Our project page is https://www.mmlab-ntu.com/project/talkedit/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers. (arXiv:2109.04448v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04448","description":"<p>Pretrained vision-and-language BERTs aim to learn representations that\ncombine information from both modalities. We propose a diagnostic method based\non cross-modal input ablation to assess the extent to which these models\nactually integrate cross-modal information. This method involves ablating\ninputs from one modality, either entirely or selectively based on cross-modal\ngrounding alignments, and evaluating the model prediction performance on the\nother modality. Model performance is measured by modality-specific tasks that\nmirror the model pretraining objectives (e.g. masked language modelling for\ntext). Models that have learned to construct cross-modal representations using\nboth modalities are expected to perform worse when inputs are missing from a\nmodality. We find that recently proposed models have much greater relative\ndifficulty predicting text when visual information is ablated, compared to\npredicting visual object categories when text is ablated, indicating that these\nmodels are not symmetrically cross-modal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frank_S/0/1/0/all/0/1\">Stella Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvMLP: Hierarchical Convolutional MLPs for Vision. (arXiv:2109.04454v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04454","description":"<p>MLP-based architectures, which consist of a sequence of consecutive\nmulti-layer perceptron blocks, have recently been found to reach comparable\nresults to convolutional and transformer-based methods. However, most adopt\nspatial MLPs which take fixed dimension inputs, therefore making it difficult\nto apply them to downstream tasks, such as object detection and semantic\nsegmentation. Moreover, single-stage designs further limit performance in other\ncomputer vision tasks and fully connected layers bear heavy computation. To\ntackle these problems, we propose ConvMLP: a hierarchical Convolutional MLP for\nvisual recognition, which is a light-weight, stage-wise, co-design of\nconvolution layers, and MLPs. In particular, ConvMLP-S achieves 76.8% top-1\naccuracy on ImageNet-1k with 9M parameters and 2.4G MACs (15% and 19% of\nMLP-Mixer-B/16, respectively). Experiments on object detection and semantic\nsegmentation further show that visual representation learned by ConvMLP can be\nseamlessly transferred and achieve competitive results with fewer parameters.\nOur code and pre-trained models are publicly available at\nhttps://github.com/SHI-Labs/Convolutional-MLPs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NEAT: Neural Attention Fields for End-to-End Autonomous Driving. (arXiv:2109.04456v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04456","description":"<p>Efficient reasoning about the semantic, spatial, and temporal structure of a\nscene is a crucial prerequisite for autonomous driving. We present NEural\nATtention fields (NEAT), a novel representation that enables such reasoning for\nend-to-end imitation learning models. NEAT is a continuous function which maps\nlocations in Bird's Eye View (BEV) scene coordinates to waypoints and\nsemantics, using intermediate attention maps to iteratively compress\nhigh-dimensional 2D image features into a compact representation. This allows\nour model to selectively attend to relevant regions in the input while ignoring\ninformation irrelevant to the driving task, effectively associating the images\nwith the BEV representation. In a new evaluation setting involving adverse\nenvironmental conditions and challenging scenarios, NEAT outperforms several\nstrong baselines and achieves driving scores on par with the privileged CARLA\nexpert used to generate its training data. Furthermore, visualizing the\nattention maps for models with NEAT intermediate representations provides\nimproved interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chitta_K/0/1/0/all/0/1\">Kashyap Chitta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Aditya Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Local Domains for Image-to-Image Translation. (arXiv:2109.04468v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04468","description":"<p>Image-to-image (i2i) networks struggle to capture local changes because they\ndo not affect the global scene structure. For example, translating from highway\nscenes to offroad, i2i networks easily focus on global color features but\nignore obvious traits for humans like the absence of lane markings. In this\npaper, we leverage human knowledge about spatial domain characteristics which\nwe refer to as 'local domains' and demonstrate its benefit for image-to-image\ntranslation. Relying on a simple geometrical guidance, we train a patch-based\nGAN on few source data and hallucinate a new unseen domain which subsequently\neases transfer learning to target. We experiment on three tasks ranging from\nunstructured environments to adverse weather. Our comprehensive evaluation\nsetting shows we are able to generate realistic translations, with minimal\npriors, and training only on a few images. Furthermore, when trained on our\ntranslations images we show that all tested proxy tasks are significantly\nimproved, without ever seeing target domain at training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DellEva_A/0/1/0/all/0/1\">Anthony Dell&#x27;Eva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertozzi_M/0/1/0/all/0/1\">Massimo Bertozzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Binary-Ternary Quantization. (arXiv:1909.12205v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1909.12205","description":"<p>Neural network models are resource hungry. It is difficult to deploy such\ndeep networks on devices with limited resources, like smart wearables,\ncellphones, drones, and autonomous vehicles. Low bit quantization such as\nbinary and ternary quantization is a common approach to alleviate this resource\nrequirements. Ternary quantization provides a more flexible model and\noutperforms binary quantization in terms of accuracy, however doubles the\nmemory footprint and increases the computational cost. Contrary to these\napproaches, mixed quantized models allow a trade-off between accuracy and\nmemory footprint. In such models, quantization depth is often chosen manually,\nor is tuned using a separate optimization routine. The latter requires training\na quantized network multiple times. Here, we propose an adaptive combination of\nbinary and ternary quantization, namely Smart Quantization (SQ), in which the\nquantization depth is modified directly via a regularization function, so that\nthe model is trained only once. Our experimental results show that the proposed\nmethod adapts quantization depth successfully while keeping the model accuracy\nhigh on MNIST and CIFAR10 benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morin_G/0/1/0/all/0/1\">Gr&#xe9;goire Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razani_R/0/1/0/all/0/1\">Ryan Razani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sari_E/0/1/0/all/0/1\">Eyy&#xfc;b Sari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Partial Label Learning via Dual Bipartite Graph Autoencoder. (arXiv:2001.01290v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.01290","description":"<p>We formulate a practical yet challenging problem: General Partial Label\nLearning (GPLL). Compared to the traditional Partial Label Learning (PLL)\nproblem, GPLL relaxes the supervision assumption from instance-level -- a label\nset partially labels an instance -- to group-level: 1) a label set partially\nlabels a group of instances, where the within-group instance-label link\nannotations are missing, and 2) cross-group links are allowed -- instances in a\ngroup may be partially linked to the label set from another group. Such\nambiguous group-level supervision is more practical in real-world scenarios as\nadditional annotation on the instance-level is no longer required, e.g.,\nface-naming in videos where the group consists of faces in a frame, labeled by\na name set in the corresponding caption. In this paper, we propose a novel\ngraph convolutional network (GCN) called Dual Bipartite Graph Autoencoder\n(DB-GAE) to tackle the label ambiguity challenge of GPLL. First, we exploit the\ncross-group correlations to represent the instance groups as dual bipartite\ngraphs: within-group and cross-group, which reciprocally complements each other\nto resolve the linking ambiguities. Second, we design a GCN autoencoder to\nencode and decode them, where the decodings are considered as the refined\nresults. It is worth noting that DB-GAE is self-supervised and transductive, as\nit only uses the group-level supervision without a separate offline training\nstage. Extensive experiments on two real-world datasets demonstrate that DB-GAE\nsignificantly outperforms the best baseline over absolute 0.159 F1-score and\n24.8% accuracy. We further offer analysis on various levels of label\nambiguities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareian_A/0/1/0/all/0/1\">Alireza Zareian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sharing Matters for Generalization in Deep Metric Learning. (arXiv:2004.05582v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.05582","description":"<p>Learning the similarity between images constitutes the foundation for\nnumerous vision tasks. The common paradigm is discriminative metric learning,\nwhich seeks an embedding that separates different training classes. However,\nthe main challenge is to learn a metric that not only generalizes from training\nto novel, but related, test samples. It should also transfer to different\nobject classes. So what complementary information is missed by the\ndiscriminative paradigm? Besides finding characteristics that separate between\nclasses, we also need them to likely occur in novel categories, which is\nindicated if they are shared across training classes. This work investigates\nhow to learn such characteristics without the need for extra annotations or\ntraining data. By formulating our approach as a novel triplet sampling\nstrategy, it can be easily applied on top of recent ranking loss frameworks.\nExperiments show that, independent of the underlying network architecture and\nthe specific ranking loss, our approach significantly improves performance in\ndeep metric learning, leading to new the state-of-the-art results on various\nstandard benchmark datasets. Preliminary early access page can be found here:\nhttps://ieeexplore.ieee.org/document/9141449\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1\">Timo Milbich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1\">Karsten Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brattoli_B/0/1/0/all/0/1\">Biagio Brattoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localization Uncertainty Estimation for Anchor-Free Object Detection. (arXiv:2006.15607v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.15607","description":"<p>Since many safety-critical systems, such as surgical robots and autonomous\ndriving cars operate in unstable environments with sensor noise and incomplete\ndata, it is desirable for object detectors to take the localization uncertainty\ninto account. However, there are several limitations of the existing\nuncertainty estimation methods for anchor-based object detection. 1) They model\nthe uncertainty of the heterogeneous object properties with different\ncharacteristics and scales, such as location (center point) and scale (width,\nheight), which could be difficult to estimate. 2) They model box offsets as\nGaussian distributions, which is not compatible with the ground truth bounding\nboxes that follow the Dirac delta distribution. 3) Since anchor-based methods\nare sensitive to anchor hyperparameters, the localization uncertainty for them\ncould be also highly sensitive to the choice of hyperparameters as well. To\ntackle these limitations, we propose a new localization uncertainty estimation\nmethod called UAD for anchor-free object detection. Our method captures the\nuncertainty in four directions of box offsets~(left, right, top, bottom) that\nare homogeneous, so that it can tell which direction is uncertain, and provides\na quantitative value of uncertainty in $[0, 1]$. To enable such uncertainty\nestimation, we design a new uncertainty loss, negative power log-likelihood\nloss, to measure the localization uncertainty by weighting the likelihood loss\nby its IoU, which alleviates the model misspecification problem. Furthermore,\nwe propose an uncertainty-aware focal loss for reflecting the estimated\nuncertainty to the classification score. Experimental results on COCO datasets\ndemonstrate that our method significantly improves FCOS, by up to 1.8 points,\nwithout sacrificing computational efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Joong-won Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyung-Il Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_K/0/1/0/all/0/1\">Kimin Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yongjin Kwon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.03060","description":"<p>A key challenge in training neural networks for a given medical imaging task\nis often the difficulty of obtaining a sufficient number of manually labeled\nexamples. In contrast, textual imaging reports, which are often readily\navailable in medical records, contain rich but unstructured interpretations\nwritten by experts as part of standard clinical practice. We propose using\nthese textual reports as a form of weak supervision to improve the image\ninterpretation performance of a neural network without requiring additional\nmanually labeled examples. We use an image-text matching task to train a\nfeature extractor and then fine-tune it in a transfer learning setting for a\nsupervised task using a small labeled dataset. The end result is a neural\nnetwork that automatically interprets imagery without requiring textual reports\nduring inference. This approach can be applied to any task for which text-image\npairs are readily available. We evaluate our method on three classification\ntasks and find consistent performance improvements, reducing the need for\nlabeled data by 67%-98%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Gongbo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenwell_C/0/1/0/all/0/1\">Connor Greenwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1\">Nathan Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSAM: A Distance Shrinking with Angular Marginalizing Loss for High Performance Vehicle Re-identificatio. (arXiv:2011.06228v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.06228","description":"<p>Vehicle Re-identification (ReID) is an important yet challenging problem in\ncomputer vision. Compared to other visual objects like faces and persons,\nvehicles simultaneously exhibit much larger intraclass viewpoint variations and\ninterclass visual similarities, making most exiting loss functions designed for\nface recognition and person ReID unsuitable for vehicle ReID. To obtain a\nhigh-performance vehicle ReID model, we present a novel Distance Shrinking with\nAngular Marginalizing (DSAM) loss function to perform hybrid learning in both\nthe Original Feature Space (OFS) and the Feature Angular Space (FAS) using the\nlocal verification and the global identification information. Specifically, it\nshrinks the distance between samples of the same class locally in the Original\nFeature Space while keeps samples of different classes far away in the Feature\nAngular Space. The shrinking and marginalizing operations are performed during\neach iteration of the training process and are suitable for different SoftMax\nbased loss functions. We evaluate the DSAM loss function on three large vehicle\nReID datasets with detailed analyses and extensive comparisons with many\ncompeting vehicle ReID methods. Experimental results show that our DSAM loss\nenhances the SoftMax loss by a large margin on the PKU-VD1-Large dataset:\n10.41% for mAP, 5.29% for cmc1, and 4.60% for cmc5. Moreover, the mAP is\nincreased by 9.34% on the PKU-VehicleID dataset and 6.13% on the VeRi-776\ndataset. Source code will be released to facilitate further studies in this\nresearch direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_J/0/1/0/all/0/1\">Jiangtao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Benjia Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Junliang Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HoHoNet: 360 Indoor Holistic Understanding with Latent Horizontal Features. (arXiv:2011.11498v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11498","description":"<p>We present HoHoNet, a versatile and efficient framework for holistic\nunderstanding of an indoor 360-degree panorama using a Latent Horizontal\nFeature (LHFeat). The compact LHFeat flattens the features along the vertical\ndirection and has shown success in modeling per-column modality for room layout\nreconstruction. HoHoNet advances in two important aspects. First, the deep\narchitecture is redesigned to run faster with improved accuracy. Second, we\npropose a novel horizon-to-dense module, which relaxes the per-column output\nshape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is\nfast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones\nrespectively, for modeling dense modalities from a high-resolution $512 \\times\n1024$ panorama. HoHoNet is also accurate. On the tasks of layout estimation and\nsemantic segmentation, HoHoNet achieves results on par with current\nstate-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior\narts by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Deep Neural Networks via Layer-Peeled Model: Minority Collapse in Imbalanced Training. (arXiv:2101.12699v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.12699","description":"<p>In this paper, we introduce the \\textit{Layer-Peeled Model}, a nonconvex yet\nanalytically tractable optimization program, in a quest to better understand\ndeep neural networks that are trained for a sufficiently long time. As the name\nsuggests, this new model is derived by isolating the topmost layer from the\nremainder of the neural network, followed by imposing certain constraints\nseparately on the two parts of the network. We demonstrate that the\nLayer-Peeled Model, albeit simple, inherits many characteristics of\nwell-trained neural networks, thereby offering an effective tool for explaining\nand predicting common empirical patterns of deep learning training. First, when\nworking on class-balanced datasets, we prove that any solution to this model\nforms a simplex equiangular tight frame, which in part explains the recently\ndiscovered phenomenon of neural collapse \\cite{papyan2020prevalence}. More\nimportantly, when moving to the imbalanced case, our analysis of the\nLayer-Peeled Model reveals a hitherto unknown phenomenon that we term\n\\textit{Minority Collapse}, which fundamentally limits the performance of deep\nlearning models on the minority classes. In addition, we use the Layer-Peeled\nModel to gain insights into how to mitigate Minority Collapse. Interestingly,\nthis phenomenon is first predicted by the Layer-Peeled Model before being\nconfirmed by our computational experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Cong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hangfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1\">Qi Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Noticeable Difference for Machine Perception and Generation of Regularized Adversarial Images with Minimal Perturbation. (arXiv:2102.08079v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.08079","description":"<p>In this study, we introduce a measure for machine perception, inspired by the\nconcept of Just Noticeable Difference (JND) of human perception. Based on this\nmeasure, we suggest an adversarial image generation algorithm, which\niteratively distorts an image by an additive noise until the model detects the\nchange in the image by outputting a false label. The noise added to the\noriginal image is defined as the gradient of the cost function of the model. A\nnovel cost function is defined to explicitly minimize the amount of\nperturbation applied to the input image while enforcing the perceptual\nsimilarity between the adversarial and input images. For this purpose, the cost\nfunction is regularized by the well-known total variation and bounded range\nterms to meet the natural appearance of the adversarial image. We evaluate the\nadversarial images generated by our algorithm both qualitatively and\nquantitatively on CIFAR10, ImageNet, and MS COCO datasets. Our experiments on\nimage classification and object detection tasks show that adversarial images\ngenerated by our JND method are both more successful in deceiving the\nrecognition/detection models and less perturbed compared to the images\ngenerated by the state-of-the-art methods, namely, FGV, FSGM, and DeepFool\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1\">Adil Kaan Akan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1\">Emre Akbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vural_F/0/1/0/all/0/1\">Fatos T. Yarman Vural</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ISCL: Interdependent Self-Cooperative Learning for Unpaired Image Denoising. (arXiv:2102.09858v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.09858","description":"<p>With the advent of advances in self-supervised learning, paired clean-noisy\ndata are no longer required in deep learning-based image denoising. However,\nexisting blind denoising methods still require the assumption with regard to\nnoise characteristics, such as zero-mean noise distribution and pixel-wise\nnoise-signal independence; this hinders wide adaptation of the method in the\nmedical domain. On the other hand, unpaired learning can overcome limitations\nrelated to the assumption on noise characteristics, which makes it more\nfeasible for collecting the training data in real-world scenarios. In this\npaper, we propose a novel image denoising scheme, Interdependent\nSelf-Cooperative Learning (ISCL), that leverages unpaired learning by combining\ncyclic adversarial learning with self-supervised residual learning. Unlike the\nexisting unpaired image denoising methods relying on matching data\ndistributions in different domains, the two architectures in ISCL, designed for\ndifferent tasks, complement each other and boost the learning process. To\nassess the performance of the proposed method, we conducted extensive\nexperiments in various biomedical image degradation scenarios, such as noise\ncaused by physical characteristics of electron microscopy (EM) devices (film\nand charging noise), and structural noise found in low-dose computer tomography\n(CT). We demonstrate that the image quality of our method is superior to\nconventional and current state-of-the-art deep learning-based image denoising\nmethods, including supervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kanggeun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1\">Won-Ki Jeong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attacks are Reversible with Natural Supervision. (arXiv:2103.14222v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14222","description":"<p>We find that images contain intrinsic structure that enables the reversal of\nmany adversarial attacks. Attack vectors cause not only image classifiers to\nfail, but also collaterally disrupt incidental structure in the image. We\ndemonstrate that modifying the attacked image to restore the natural structure\nwill reverse many types of attacks, providing a defense. Experiments\ndemonstrate significantly improved robustness for several state-of-the-art\nmodels across the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Our results\nshow that our defense is still effective even if the attacker is aware of the\ndefense mechanism. Since our defense is deployed during inference instead of\ntraining, it is compatible with pre-trained networks as well as most other\ndefenses. Our results suggest deep networks are vulnerable to adversarial\nexamples partly because their representations do not enforce the natural\nstructure of images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chengzhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiquier_M/0/1/0/all/0/1\">Mia Chiquier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightSAL: Lightweight Sign Agnostic Learning for Implicit Surface Representation. (arXiv:2103.14273v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14273","description":"<p>Recently, several works have addressed modeling of 3D shapes using deep\nneural networks to learn implicit surface representations. Up to now, the\nmajority of works have concentrated on reconstruction quality, paying little or\nno attention to model size or training time. This work proposes LightSAL, a\nnovel deep convolutional architecture for learning 3D shapes; the proposed work\nconcentrates on efficiency both in network training time and resulting model\nsize. We build on the recent concept of Sign Agnostic Learning for training the\nproposed network, relying on signed distance fields, with unsigned distance as\nground truth. In the experimental section of the paper, we demonstrate that the\nproposed architecture outperforms previous work in model size and number of\nrequired training iterations, while achieving equivalent accuracy. Experiments\nare based on the D-Faust dataset that contains 41k 3D scans of human shapes.\nThe proposed model has been implemented in PyTorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basher_A/0/1/0/all/0/1\">Abol Basher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarmad_M/0/1/0/all/0/1\">Muhammad Sarmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutellier_J/0/1/0/all/0/1\">Jani Boutellier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Weight Pruning using Pre-trained Lottery Jackpots. (arXiv:2104.08700v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08700","description":"<p>Network pruning is an effective approach to reduce network complexity without\nperformance compromise. Existing studies achieve the sparsity of neural\nnetworks via time-consuming weight tuning or complex search on networks with\nexpanded width, which greatly limits the applications of network pruning. In\nthis paper, we show that high-performing and sparse sub-networks without the\ninvolvement of weight tuning, termed \"lottery jackpots\", exist in pre-trained\nmodels with unexpanded width. For example, we obtain a lottery jackpot that has\nonly 10% parameters and still reaches the performance of the original dense\nVGGNet-19 without any modifications on the pre-trained weights. Furthermore, we\nobserve that the sparse masks derived from many existing pruning criteria have\na high overlap with the searched mask of our lottery jackpot, among which, the\nmagnitude-based pruning results in the most similar mask with ours. Based on\nthis insight, we initialize our sparse mask using the magnitude pruning,\nresulting in at least 3x cost reduction on the lottery jackpot search while\nachieves comparable or even better performance. Specifically, our\nmagnitude-based lottery jackpot removes 90% weights in the ResNet-50, while\neasily obtains more than 70% top-1 accuracy using only 10 searching epochs on\nImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guannan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels. (arXiv:2105.03857v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03857","description":"<p>Detection faults in seismic data is a crucial step for seismic structural\ninterpretation, reservoir characterization and well placement. Some recent\nworks regard it as an image segmentation task. The task of image segmentation\nrequires huge labels, especially 3D seismic data, which has a complex structure\nand lots of noise. Therefore, its annotation requires expert experience and a\nhuge workload. In this study, we present lambda-BCE and lambda-smooth L1loss to\neffectively train 3D-CNN by some slices from 3D seismic data, so that the model\ncan learn the segmentation of 3D seismic data from a few 2D slices. In order to\nfully extract information from limited data and suppress seismic noise, we\npropose an attention module that can be used for active supervision training\nand embedded in the network. The attention heatmap label is generated by the\noriginal label, and letting it supervise the attention module using the\nlambda-smooth L1loss. The experiment demonstrates the effectiveness of our loss\nfunction, the method can extract 3D seismic features from a few 2D slice\nlabels. And it also shows the advanced performance of the attention module,\nwhich can significantly suppress the noise in the seismic data while increasing\nthe model's sensitivity to the foreground. Finally, on the public test set, we\nonly use the 2D slice labels training that accounts for 3.3% of the 3D volume\nlabel, and achieve similar performance to the 3D volume label training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">YiMin Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianbing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yingjie Xi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Cropping on Twitter: Fairness Metrics, their Limitations, and the Importance of Representation, Design, and Agency. (arXiv:2105.08667v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2105.08667","description":"<p>Twitter uses machine learning to crop images, where crops are centered around\nthe part predicted to be the most salient. In fall 2020, Twitter users raised\nconcerns that the automated image cropping system on Twitter favored\nlight-skinned over dark-skinned individuals, as well as concerns that the\nsystem favored cropping woman's bodies instead of their heads. In order to\naddress these concerns, we conduct an extensive analysis using formalized group\nfairness metrics. We find systematic disparities in cropping and identify\ncontributing factors, including the fact that the cropping based on the single\nmost salient point can amplify the disparities because of an effect we term\nargmax bias. However, we demonstrate that formalized fairness metrics and\nquantitative analysis on their own are insufficient for capturing the risk of\nrepresentational harm in automatic cropping. We suggest the removal of\nsaliency-based cropping in favor of a solution that better preserves user\nagency. For developing a new solution that sufficiently address concerns\nrelated to representational harm, our critique motivates a combination of\nquantitative and qualitative methods that include human-centered design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yee_K/0/1/0/all/0/1\">Kyra Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tantipongpipat_U/0/1/0/all/0/1\">Uthaipon Tantipongpipat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shubhanshu Mishra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Information Extraction by Character-Level Embedding and Multi-Stage Attentional U-Net. (arXiv:2106.00952v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00952","description":"<p>Information extraction from document images has received a lot of attention\nrecently, due to the need for digitizing a large volume of unstructured\ndocuments such as invoices, receipts, bank transfers, etc. In this paper, we\npropose a novel deep learning architecture for end-to-end information\nextraction on the 2D character-grid embedding of the document, namely the\n\\textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and\nspatial relations between 2D elements, our model leverages a specialized\nmulti-stage encoder-decoders design, in conjunction with efficient uses of the\nself-attention mechanism and the box convolution. Experimental results on\ndifferent datasets show that our model outperforms the baseline U-Net\narchitecture by a large margin while using 40\\% fewer parameters. Moreover, it\nalso significantly improved the baseline in erroneous OCR and limited training\ndata scenario, thus becomes practical for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tuan-Anh Nguyen Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat-Thanh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indoor Panorama Planar 3D Reconstruction via Divide and Conquer. (arXiv:2106.14166v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14166","description":"<p>Indoor panorama typically consists of human-made structures parallel or\nperpendicular to gravity. We leverage this phenomenon to approximate the scene\nin a 360-degree image with (H)orizontal-planes and (V)ertical-planes. To this\nend, we propose an effective divide-and-conquer strategy that divides pixels\nbased on their plane orientation estimation; then, the succeeding instance\nsegmentation module conquers the task of planes clustering more easily in each\nplane orientation group. Besides, parameters of V-planes depend on camera yaw\nrotation, but translation-invariant CNNs are less aware of the yaw change. We\nthus propose a yaw-invariant V-planar reparameterization for CNNs to learn. We\ncreate a benchmark for indoor panorama planar reconstruction by extending\nexisting 360 depth datasets with ground truth H\\&amp;V-planes (referred to as\nPanoH&amp;V dataset) and adopt state-of-the-art planar reconstruction methods to\npredict H\\&amp;V-planes as our baselines. Our method outperforms the baselines by a\nlarge margin on the proposed dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_C/0/1/0/all/0/1\">Chi-Wei Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning-Hsu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Superpoint-guided Semi-supervised Semantic Segmentation of 3D Point Clouds. (arXiv:2107.03601v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03601","description":"<p>3D point cloud semantic segmentation is a challenging topic in the computer\nvision field. Most of the existing methods in literature require a large amount\nof fully labeled training data, but it is extremely time-consuming to obtain\nthese training data by manually labeling massive point clouds. Addressing this\nproblem, we propose a superpoint-guided semi-supervised segmentation network\nfor 3D point clouds, which jointly utilizes a small portion of labeled scene\npoint clouds and a large number of unlabeled point clouds for network training.\nThe proposed network is iteratively updated with its predicted pseudo labels,\nwhere a superpoint generation module is introduced for extracting superpoints\nfrom 3D point clouds, and a pseudo-label optimization module is explored for\nautomatically assigning pseudo labels to the unlabeled points under the\nconstraint of the extracted superpoints. Additionally, there are some 3D points\nwithout pseudo-label supervision. We propose an edge prediction module to\nconstrain features of edge points. A superpoint feature aggregation module and\na superpoint feature consistency loss function are introduced to smooth\nsuperpoint features. Extensive experimental results on two 3D public datasets\ndemonstrate that our method can achieve better performance than several\nstate-of-the-art point cloud segmentation networks and several popular\nsemi-supervised segmentation methods with few labeled scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiulei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanyi Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer. (arXiv:2108.01390v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01390","description":"<p>Vision transformers (ViTs) have recently received explosive popularity, but\nthe huge computational cost is still a severe issue. Since the computation\ncomplexity of ViT is quadratic with respect to the input sequence length, a\nmainstream paradigm for computation reduction is to reduce the number of\ntokens. Existing designs include structured spatial compression that uses a\nprogressive shrinking pyramid to reduce the computations of large feature maps,\nand unstructured token pruning that dynamically drops redundant tokens.\nHowever, the limitation of existing token pruning lies in two folds: 1) the\nincomplete spatial structure caused by pruning is not compatible with\nstructured spatial compression that is commonly used in modern deep-narrow\ntransformers; 2) it usually requires a time-consuming pre-training procedure.\nTo tackle the limitations and expand the applicable scenario of token pruning,\nwe present Evo-ViT, a self-motivated slow-fast token evolution approach for\nvision transformers. Specifically, we conduct unstructured instance-wise token\nselection by taking advantage of the simple and effective global class\nattention that is native to vision transformers. Then, we propose to update the\nselected informative tokens and uninformative tokens with different computation\npaths, namely, slow-fast updating. Since slow-fast updating mechanism maintains\nthe spatial structure and information flow, Evo-ViT can accelerate vanilla\ntransformers of both flat and deep-narrow structures from the very beginning of\nthe training process. Experimental results demonstrate that our method\nsignificantly reduces the computational cost of vision transformers while\nmaintaining comparable performance on image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weiming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neighborhood Consensus Contrastive Learning for Backward-Compatible Representation. (arXiv:2108.03372v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03372","description":"<p>In object re-identification (ReID), the development of deep learning\ntechniques often involves model updates and deployment. It is unbearable to\nre-embedding and re-index with the system suspended when deploying new models.\nTherefore, backward-compatible representation is proposed to enable \"new\"\nfeatures to be compared with \"old\" features directly, which means that the\ndatabase is active when there are both \"new\" and \"old\" features in it. Thus we\ncan scroll-refresh the database or even do nothing on the database to update.\n</p>\n<p>The existing backward-compatible methods either require a strong overlap\nbetween old and new training data or simply conduct constraints at the instance\nlevel. Thus they are difficult in handling complicated cluster structures and\nare limited in eliminating the impact of outliers in old embeddings, resulting\nin a risk of damaging the discriminative capability of new features. In this\nwork, we propose a Neighborhood Consensus Contrastive Learning (NCCL) method.\nWith no assumptions about the new training data, we estimate the sub-cluster\nstructures of old embeddings. A new embedding is constrained with multiple old\nembeddings in both embedding space and discrimination space at the sub-class\nlevel. The effect of outliers diminished, as the multiple samples serve as\n\"mean teachers\". Besides, we also propose a scheme to filter the old embeddings\nwith low credibility, further improving the compatibility robustness. Our\nmethod ensures backward compatibility without impairing the accuracy of the new\nmodel. And it can even improve the new model's accuracy in most scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengsen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yihang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Minghua Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Lingyu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Domain Generalizable Person Re-Identification. (arXiv:2108.05045v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05045","description":"<p>Existing person re-identification (re-id) methods are stuck when deployed to\na new unseen scenario despite the success in cross-camera person matching.\nRecent efforts have been substantially devoted to domain adaptive person re-id\nwhere extensive unlabeled data in the new scenario are utilized in a\ntransductive learning manner. However, for each scenario, it is required to\nfirst collect enough data and then train such a domain adaptive re-id model,\nthus restricting their practical application. Instead, we aim to explore\nmultiple labeled datasets to learn generalized domain-invariant representations\nfor person re-id, which is expected universally effective for each new-coming\nre-id scenario. To pursue practicability in real-world systems, we collect all\nthe person re-id datasets (20 datasets) in this field and select the three most\nfrequently used datasets (i.e., Market1501, DukeMTMC, and MSMT17) as unseen\ntarget domains. In addition, we develop DataHunter that collects over 300K+\nweak annotated images named YouTube-Human from YouTube street-view videos,\nwhich joins 17 remaining full labeled datasets to form multiple source domains.\nOn such a large and challenging benchmark called FastHuman (~440K+ labeled\nimages), we further propose a simple yet effective Semi-Supervised Knowledge\nDistillation (SSKD) framework. SSKD effectively exploits the weakly annotated\ndata by assigning soft pseudo labels to YouTube-Human to improve models'\ngeneralization ability. Experiments on several protocols verify the\neffectiveness of the proposed SSKD framework on domain generalizable person\nre-id, which is even comparable to supervised learning on the target domains.\nLastly, but most importantly, we hope the proposed benchmark FastHuman could\nbring the next development of domain generalizable person re-id algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lingxiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xingyu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProAI: An Efficient Embedded AI Hardware for Automotive Applications -- a Benchmark Study. (arXiv:2108.05170v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05170","description":"<p>Development in the field of Single Board Computers (SBC) have been increasing\nfor several years. They provide a good balance between computing performance\nand power consumption which is usually required for mobile platforms, like\napplication in vehicles for Advanced Driver Assistance Systems (ADAS) and\nAutonomous Driving (AD). However, there is an ever-increasing need of more\npowerful and efficient SBCs which can run power intensive Deep Neural Networks\n(DNNs) in real-time and can also satisfy necessary functional safety\nrequirements such as Automotive Safety Integrity Level (ASIL). ProAI is being\ndeveloped by ZF mainly to run powerful and efficient applications such as\nmultitask DNNs and on top of that it also has the required safety certification\nfor AD. In this work, we compare and discuss state of the art SBC on the basis\nof power intensive multitask DNN architecture called Multitask-CenterNet with\nrespect to performance measures such as, FPS and power efficiency. As an\nautomotive supercomputer, ProAI delivers an excellent combination of\nperformance and efficiency, managing nearly twice the number of FPS per watt\nthan a modern workstation laptop and almost four times compared to the Jetson\nNano. Furthermore, it was also shown that there is still power in reserve for\nfurther and more complex tasks on the ProAI, based on the CPU and GPU\nutilization during the benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mantowsky_S/0/1/0/all/0/1\">Sven Mantowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heuer_F/0/1/0/all/0/1\">Falk Heuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukhari_S/0/1/0/all/0/1\">Syed Saqib Bukhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keckeisen_M/0/1/0/all/0/1\">Michael Keckeisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1\">Georg Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Visual Recognition with Deep Neural Networks: A Survey on Recent Advances and New Directions. (arXiv:2108.13055v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.13055","description":"<p>Visual recognition is currently one of the most important and active research\nareas in computer vision, pattern recognition, and even the general field of\nartificial intelligence. It has great fundamental importance and strong\nindustrial needs. Deep neural networks (DNNs) have largely boosted their\nperformances on many concrete tasks, with the help of large amounts of training\ndata and new powerful computation resources. Though recognition accuracy is\nusually the first concern for new progresses, efficiency is actually rather\nimportant and sometimes critical for both academic research and industrial\napplications. Moreover, insightful views on the opportunities and challenges of\nefficiency are also highly required for the entire community. While general\nsurveys on the efficiency issue of DNNs have been done from various\nperspectives, as far as we are aware, scarcely any of them focused on visual\nrecognition systematically, and thus it is unclear which progresses are\napplicable to it and what else should be concerned. In this paper, we present\nthe review of the recent advances with our suggestions on the new possible\ndirections towards improving the efficiency of DNN-related visual recognition\napproaches. We investigate not only from the model but also the data point of\nview (which is not the case in existing surveys), and focus on three most\nstudied data types (images, videos and points). This paper attempts to provide\na systematic summary via a comprehensive survey which can serve as a valuable\nreference and inspire both researchers and practitioners who work on visual\nrecognition problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaotong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weisheng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianbo Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse-MLP: A Fully-MLP Architecture with Conditional Computation. (arXiv:2109.02008v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.02008","description":"<p>Mixture-of-Experts (MoE) with sparse conditional computation has been proved\nan effective architecture for scaling attention-based models to more parameters\nwith comparable computation cost. In this paper, we propose Sparse-MLP, scaling\nthe recent MLP-Mixer model with sparse MoE layers, to achieve a more\ncomputation-efficient architecture. We replace a subset of dense MLP blocks in\nthe MLP-Mixer model with Sparse blocks. In each Sparse block, we apply two\nstages of MoE layers: one with MLP experts mixing information within channels\nalong image patch dimension, one with MLP experts mixing information within\npatches along the channel dimension. Besides, to reduce computational cost in\nrouting and improve expert capacity, we design Re-represent layers in each\nSparse block. These layers are to re-scale image representations by two simple\nbut effective linear transformations. When pre-training on ImageNet-1k with\nMoCo v3 algorithm, our models can outperform dense MLP models by 2.5\\% on\nImageNet Top-1 accuracy with fewer parameters and computational cost. On\nsmall-scale downstream image classification tasks, i.e. Cifar10 and Cifar100,\nour Sparse-MLP can still achieve better performance than baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Object-to-Zone Graph for Object Navigation. (arXiv:2109.02066v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02066","description":"<p>The goal of object navigation is to reach the expected objects according to\nvisual information in the unseen environments. Previous works usually implement\ndeep models to train an agent to predict actions in real-time. However, in the\nunseen environment, when the target object is not in egocentric view, the agent\nmay not be able to make wise decisions due to the lack of guidance. In this\npaper, we propose a hierarchical object-to-zone (HOZ) graph to guide the agent\nin a coarse-to-fine manner, and an online-learning mechanism is also proposed\nto update HOZ according to the real-time observation in new environments. In\nparticular, the HOZ graph is composed of scene nodes, zone nodes and object\nnodes. With the pre-learned HOZ graph, the real-time observation and the target\ngoal, the agent can constantly plan an optimal path from zone to zone. In the\nestimated path, the next potential zone is regarded as sub-goal, which is also\nfed into the deep reinforcement learning model for action prediction. Our\nmethods are evaluated on the AI2-Thor simulator. In addition to widely used\nevaluation metrics SR and SPL, we also propose a new evaluation metric of SAE\nthat focuses on the effective action rate. Experimental results demonstrate the\neffectiveness and efficiency of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sixian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xinhang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yubing Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yakui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02747","description":"<p>We aim to automatically identify human action reasons in online videos. We\nfocus on the widespread genre of lifestyle vlogs, in which people perform\nactions while verbally describing them. We introduce and make publicly\navailable the WhyAct dataset, consisting of 1,077 visual actions manually\nannotated with their reasons. We describe a multimodal model that leverages\nvisual and textual information to automatically infer the reasons corresponding\nto an action presented in the video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_H/0/1/0/all/0/1\">Hanwen Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Learned Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.03082","description":"<p>This paper proposes a Perceptual Learned Video Compression (PLVC) approach\nwith recurrent conditional generative adversarial network. In our approach, the\nrecurrent auto-encoder-based generator learns to fully explore the temporal\ncorrelation for compressing video. More importantly, we propose a recurrent\nconditional discriminator, which judges raw and compressed video conditioned on\nboth spatial and temporal information, including the latent representation,\ntemporal motion and hidden states in recurrent cells. This way, in the\nadversarial training, it pushes the generated video to be not only spatially\nphoto-realistic but also temporally consistent with groundtruth and coherent\namong video frames. The experimental results show that the proposed PLVC model\nlearns to compress video towards good perceptual quality at low bit-rate, and\noutperforms the previous traditional and learned approaches on several\nperceptual quality metrics. The user study further validates the outstanding\nperceptual performance of PLVC in comparison with the latest learned video\ncompression approaches and the official HEVC test model (HM 16.20). The codes\nwill be released at https://github.com/RenYang-home/PLVC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ren Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"nnFormer: Interleaved Transformer for Volumetric Segmentation. (arXiv:2109.03201v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03201","description":"<p>Transformers, the default model of choices in natural language processing,\nhave drawn scant attention from the medical imaging community. Given the\nability to exploit long-term dependencies, transformers are promising to help\natypical convolutional neural networks (convnets) to overcome its inherent\nshortcomings of spatial inductive bias. However, most of recently proposed\ntransformer-based segmentation approaches simply treated transformers as\nassisted modules to help encode global context into convolutional\nrepresentations without investigating how to optimally combine self-attention\n(i.e., the core of transformers) with convolution. To address this issue, in\nthis paper, we introduce nnFormer (i.e., Not-aNother transFormer), a powerful\nsegmentation model with an interleaved architecture based on empirical\ncombination of self-attention and convolution. In practice, nnFormer learns\nvolumetric representations from 3D local volumes. Compared to the naive\nvoxel-level self-attention implementation, such volume-based operations help to\nreduce the computational complexity by approximate 98% and 99.5% on Synapse and\nACDC datasets, respectively. In comparison to prior-art network configurations,\nnnFormer achieves tremendous improvements over previous transformer-based\nmethods on two commonly used datasets Synapse and ACDC. For instance, nnFormer\noutperforms Swin-UNet by over 7 percents on Synapse. Even when compared to\nnnUNet, currently the best performing fully-convolutional medical segmentation\nnetwork, nnFormer still provides slightly better performance on Synapse and\nACDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiansen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MRI Reconstruction Using Deep Energy-Based Model. (arXiv:2109.03237v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.03237","description":"<p>Purpose: Although recent deep energy-based generative models (EBMs) have\nshown encouraging results in many image generation tasks, how to take advantage\nof the self-adversarial cogitation in deep EBMs to boost the performance of\nMagnetic Resonance Imaging (MRI) reconstruction is still desired.\n</p>\n<p>Methods: With the successful application of deep learning in a wide range of\nMRI reconstruction, a line of emerging research involves formulating an\noptimization-based reconstruction method in the space of a generative model.\nLeveraging this, a novel regularization strategy is introduced in this article\nwhich takes advantage of self-adversarial cogitation of the deep energy-based\nmodel. More precisely, we advocate for alternative learning a more powerful\nenergy-based model with maximum likelihood estimation to obtain the deep\nenergy-based information, represented as image prior. Simultaneously, implicit\ninference with Langevin dynamics is a unique property of re-construction. In\ncontrast to other generative models for reconstruction, the proposed method\nutilizes deep energy-based information as the image prior in reconstruction to\nimprove the quality of image.\n</p>\n<p>Results: Experiment results that imply the proposed technique can obtain\nremarkable performance in terms of high reconstruction accuracy that is\ncompetitive with state-of-the-art methods, and does not suffer from mode\ncollapse.\n</p>\n<p>Conclusion: Algorithmically, an iterative approach was presented to\nstrengthen EBM training with the gradient of energy network. The robustness and\nthe reproducibility of the algorithm were also experimentally validated. More\nimportantly, the proposed reconstruction framework can be generalized for most\nMRI reconstruction scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_Z/0/1/0/all/0/1\">Zongjiang Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoadAtlas: Intelligent Platform for Automated Road Defect Detection and Asset Management. (arXiv:2109.03385v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03385","description":"<p>With the rapid development of intelligent detection algorithms based on deep\nlearning, much progress has been made in automatic road defect recognition and\nroad marking parsing. This can effectively address the issue of an expensive\nand time-consuming process for professional inspectors to review the street\nmanually. Towards this goal, we present RoadAtlas, a novel end-to-end\nintegrated system that can support 1) road defect detection, 2) road marking\nparsing, 3) a web-based dashboard for presenting and inputting data by users,\nand 4) a backend containing a well-structured database and developed APIs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuoxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jinjiang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Southon_A/0/1/0/all/0/1\">Anthony Southon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Discriminate Information for Online Action Detection: Analysis and Application. (arXiv:2109.03393v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03393","description":"<p>Online action detection, which aims to identify an ongoing action from a\nstreaming video, is an important subject in real-world applications. For this\ntask, previous methods use recurrent neural networks for modeling temporal\nrelations in an input sequence. However, these methods overlook the fact that\nthe input image sequence includes not only the action of interest but\nbackground and irrelevant actions. This would induce recurrent units to\naccumulate unnecessary information for encoding features on the action of\ninterest. To overcome this problem, we propose a novel recurrent unit, named\nInformation Discrimination Unit (IDU), which explicitly discriminates the\ninformation relevancy between an ongoing action and others to decide whether to\naccumulate the input information. This enables learning more discriminative\nrepresentations for identifying an ongoing action. In this paper, we further\npresent a new recurrent unit, called Information Integration Unit (IIU), for\naction anticipation. Our IIU exploits the outputs from IDU as pseudo action\nlabels as well as RGB frames to learn enriched features of observed actions\neffectively. In experiments on TVSeries and THUMOS-14, the proposed methods\noutperform state-of-the-art methods by a significant margin in online action\ndetection and action anticipation. Moreover, we demonstrate the effectiveness\nof the proposed units by conducting comprehensive ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sumin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eun_H/0/1/0/all/0/1\">Hyunjun Eun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jinyoung Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seokeon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonhyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1\">Chanho Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changick Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shuffled Patch-Wise Supervision for Presentation Attack Detection. (arXiv:2109.03484v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03484","description":"<p>Face anti-spoofing is essential to prevent false facial verification by using\na photo, video, mask, or a different substitute for an authorized person's\nface. Most of the state-of-the-art presentation attack detection (PAD) systems\nsuffer from overfitting, where they achieve near-perfect scores on a single\ndataset but fail on a different dataset with more realistic data. This problem\ndrives researchers to develop models that perform well under real-world\nconditions. This is an especially challenging problem for frame-based\npresentation attack detection systems that use convolutional neural networks\n(CNN). To this end, we propose a new PAD approach, which combines pixel-wise\nbinary supervision with patch-based CNN. We believe that training a CNN with\nface patches allows the model to distinguish spoofs without learning background\nor dataset-specific traces. We tested the proposed method both on the standard\nbenchmark datasets -- Replay-Mobile, OULU-NPU -- and on a real-world dataset.\nThe proposed approach shows its superiority on challenging experimental setups.\nNamely, it achieves higher performance on OULU-NPU protocol 3, 4 and on\ninter-dataset real-world experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kantarci_A/0/1/0/all/0/1\">Alperen Kantarc&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dertli_H/0/1/0/all/0/1\">Hasan Dertli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Haz&#x131;m Kemal Ekenel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Efficient Visual Abstractions for Autonomous Driving. (arXiv:2005.10091v2 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2005.10091","description":"<p>It is well known that semantic segmentation can be used as an effective\nintermediate representation for learning driving policies. However, the task of\nstreet scene semantic segmentation requires expensive annotations. Furthermore,\nsegmentation algorithms are often trained irrespective of the actual driving\ntask, using auxiliary image-space loss functions which are not guaranteed to\nmaximize driving metrics such as safety or distance traveled per intervention.\nIn this work, we seek to quantify the impact of reducing segmentation\nannotation costs on learned behavior cloning agents. We analyze several\nsegmentation-based intermediate representations. We use these visual\nabstractions to systematically study the trade-off between annotation\nefficiency and driving performance, i.e., the types of classes labeled, the\nnumber of image samples used to learn the visual abstraction model, and their\ngranularity (e.g., object masks vs. 2D bounding boxes). Our analysis uncovers\nseveral practical insights into how segmentation-based visual abstractions can\nbe exploited in a more label efficient manner. Surprisingly, we find that\nstate-of-the-art driving performance can be achieved with orders of magnitude\nreduction in annotation cost. Beyond label efficiency, we find several\nadditional training benefits when leveraging visual abstractions, such as a\nsignificant reduction in the variance of the learned policy when compared to\nstate-of-the-art end-to-end driving models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behl_A/0/1/0/all/0/1\">Aseem Behl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitta_K/0/1/0/all/0/1\">Kashyap Chitta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Aditya Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohn_Bar_E/0/1/0/all/0/1\">Eshed Ohn-Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}