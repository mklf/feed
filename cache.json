{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Visually Grounded Reasoning across Languages and Cultures. (arXiv:2109.13238v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13238","description":"<p>The design of widespread vision-and-language datasets and pre-trained\nencoders directly adopts, or draws inspiration from, the concepts and images of\nImageNet. While one can hardly overestimate how much this benchmark contributed\nto progress in computer vision, it is mostly derived from lexical databases and\nimage queries in English, resulting in source material with a North American or\nWestern European bias. Therefore, we devise a new protocol to construct an\nImageNet-style hierarchy representative of more languages and cultures. In\nparticular, we let the selection of both concepts and images be entirely driven\nby native speakers, rather than scraping them automatically. Specifically, we\nfocus on a typologically diverse set of languages, namely, Indonesian, Mandarin\nChinese, Swahili, Tamil, and Turkish. On top of the concepts and images\nobtained through this new protocol, we create a multilingual dataset for\n{M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting\nstatements from native speaker annotators about pairs of images. The task\nconsists of discriminating whether each grounded statement is true or false. We\nestablish a series of baselines using state-of-the-art models and find that\ntheir cross-lingual transfer performance lags dramatically behind supervised\nperformance in English. These results invite us to reassess the robustness and\naccuracy of current state-of-the-art models beyond a narrow domain, but also\nopen up new exciting challenges for the development of truly multilingual and\nmulticultural systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation. (arXiv:2109.13296v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13296","description":"<p>Recent progress in generative language models has enabled machines to\ngenerate astonishingly realistic texts. While there are many legitimate\napplications of such models, there is also a rising need to distinguish\nmachine-generated texts from human-written ones (e.g., fake news detection).\nHowever, to our best knowledge, there is currently no benchmark environment\nwith datasets and tasks to systematically study the so-called \"Turing Test\"\nproblem for neural text generation methods. In this work, we present the\nTuringBench benchmark environment, which is comprised of (1) a dataset with\n200K human- or machine-generated samples across 20 labels {Human, GPT-1,\nGPT-2_small, GPT-2_medium, GPT-2_large, GPT-2_xl, GPT-2_PyTorch, GPT-3,\nGROVER_base, GROVER_large, GROVER_mega, CTRL, XLM, XLNET_base, XLNET_large,\nFAIR_wmt19, FAIR_wmt20, TRANSFORMER_XL, PPLM_distil, PPLM_gpt2}, (2) two\nbenchmark tasks -- i.e., Turing Test (TT) and Authorship Attribution (AA), and\n(3) a website with leaderboards. Our preliminary experimental results using\nTuringBench show that FAIR_wmt20 and GPT-3 are the current winners, among all\nlanguage models tested, in generating the most human-like indistinguishable\ntexts with the lowest F1 score by five state-of-the-art TT detection models.\nThe TuringBench is available at: https://turingbench.ist.psu.edu/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uchendu_A/0/1/0/all/0/1\">Adaku Uchendu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zeyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thai Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Isotropy Calibration of Transformers. (arXiv:2109.13304v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13304","description":"<p>Different studies of the embedding space of transformer models suggest that\nthe distribution of contextual representations is highly anisotropic - the\nembeddings are distributed in a narrow cone. Meanwhile, static word\nrepresentations (e.g., Word2Vec or GloVe) have been shown to benefit from\nisotropic spaces. Therefore, previous work has developed methods to calibrate\nthe embedding space of transformers in order to ensure isotropy. However, a\nrecent study (Cai et al. 2021) shows that the embedding space of transformers\nis locally isotropic, which suggests that these models are already capable of\nexploiting the expressive capacity of their embedding space. In this work, we\nconduct an empirical evaluation of state-of-the-art methods for isotropy\ncalibration on transformers and find that they do not provide consistent\nimprovements across models and tasks. These results support the thesis that,\ngiven the local isotropy, transformers do not benefit from additional isotropy\ncalibration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yue Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinkus_K/0/1/0/all/0/1\">Karolis Martinkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascual_D/0/1/0/all/0/1\">Damian Pascual</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clematide_S/0/1/0/all/0/1\">Simon Clematide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Transformer Networks with Linear Competing Units: Application to end-to-end SL Translation. (arXiv:2109.13318v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13318","description":"<p>Automating sign language translation (SLT) is a challenging real world\napplication. Despite its societal importance, though, research progress in the\nfield remains rather poor. Crucially, existing methods that yield viable\nperformance necessitate the availability of laborious to obtain gloss sequence\ngroundtruth. In this paper, we attenuate this need, by introducing an\nend-to-end SLT model that does not entail explicit use of glosses; the model\nonly needs text groundtruth. This is in stark contrast to existing end-to-end\nmodels that use gloss sequence groundtruth, either in the form of a modality\nthat is recognized at an intermediate model stage, or in the form of a parallel\noutput process, jointly trained with the SLT model. Our approach constitutes a\nTransformer network with a novel type of layers that combines: (i) local\nwinner-takes-all (LWTA) layers with stochastic winner sampling, instead of\nconventional ReLU layers, (ii) stochastic weights with posterior distributions\nestimated via variational inference, and (iii) a weight compression technique\nat inference time that exploits estimated posterior variance to perform\nmassive, almost lossless compression. We demonstrate that our approach can\nreach the currently best reported BLEU-4 score on the PHOENIX 2014T benchmark,\nbut without making use of glosses for model training, and with a memory\nfootprint reduced by more than 70%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voskou_A/0/1/0/all/0/1\">Andreas Voskou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panousis_K/0/1/0/all/0/1\">Konstantinos P. Panousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosmopoulos_D/0/1/0/all/0/1\">Dimitrios Kosmopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzis_S/0/1/0/all/0/1\">Sotirios Chatzis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Biomedical BERT Models for Vocabulary Alignment at Scale in the UMLS Metathesaurus. (arXiv:2109.13348v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13348","description":"<p>The current UMLS (Unified Medical Language System) Metathesaurus construction\nprocess for integrating over 200 biomedical source vocabularies is expensive\nand error-prone as it relies on the lexical algorithms and human editors for\ndeciding if the two biomedical terms are synonymous. Recent advances in Natural\nLanguage Processing such as Transformer models like BERT and its biomedical\nvariants with contextualized word embeddings have achieved state-of-the-art\n(SOTA) performance on downstream tasks. We aim to validate if these approaches\nusing the BERT models can actually outperform the existing approaches for\npredicting synonymy in the UMLS Metathesaurus. In the existing Siamese Networks\nwith LSTM and BioWordVec embeddings, we replace the BioWordVec embeddings with\nthe biomedical BERT embeddings extracted from each BERT model using different\nways of extraction. In the Transformer architecture, we evaluate the use of the\ndifferent biomedical BERT models that have been pre-trained using different\ndatasets and tasks. Given the SOTA performance of these BERT models for other\ndownstream tasks, our experiments yield surprisingly interesting results: (1)\nin both model architectures, the approaches employing these biomedical\nBERT-based models do not outperform the existing approaches using Siamese\nNetwork with BioWordVec embeddings for the UMLS synonymy prediction task, (2)\nthe original BioBERT large model that has not been pre-trained with the UMLS\noutperforms the SapBERT models that have been pre-trained with the UMLS, and\n(3) using the Siamese Networks yields better performance for synonymy\nprediction when compared to using the biomedical BERT models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_G/0/1/0/all/0/1\">Goonmeet Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijesiriwardene_T/0/1/0/all/0/1\">Thilini Wijesiriwardene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yip_H/0/1/0/all/0/1\">Hong Yung Yip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javangula_V/0/1/0/all/0/1\">Vishesh Javangula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Srinivasan Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodenreider_O/0/1/0/all/0/1\">Olivier Bodenreider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SYGMA: System for Generalizable Modular Question Answering OverKnowledge Bases. (arXiv:2109.13430v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13430","description":"<p>Knowledge Base Question Answering (KBQA) tasks that in-volve complex\nreasoning are emerging as an important re-search direction. However, most KBQA\nsystems struggle withgeneralizability, particularly on two dimensions: (a)\nacrossmultiple reasoning types where both datasets and systems haveprimarily\nfocused on multi-hop reasoning, and (b) across mul-tiple knowledge bases, where\nKBQA approaches are specif-ically tuned to a single knowledge base. In this\npaper, wepresent SYGMA, a modular approach facilitating general-izability\nacross multiple knowledge bases and multiple rea-soning types. Specifically,\nSYGMA contains three high levelmodules: 1) KB-agnostic question understanding\nmodule thatis common across KBs 2) Rules to support additional reason-ing types\nand 3) KB-specific question mapping and answeringmodule to address the\nKB-specific aspects of the answer ex-traction. We demonstrate effectiveness of\nour system by evalu-ating on datasets belonging to two distinct knowledge\nbases,DBpedia and Wikidata. In addition, to demonstrate extensi-bility to\nadditional reasoning types we evaluate on multi-hopreasoning datasets and a new\nTemporal KBQA benchmarkdataset on Wikidata, namedTempQA-WD1, introduced in\nthispaper. We show that our generalizable approach has bettercompetetive\nperformance on multiple datasets on DBpediaand Wikidata that requires both\nmulti-hop and temporal rea-soning\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neelam_S/0/1/0/all/0/1\">Sumit Neelam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_U/0/1/0/all/0/1\">Udit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_H/0/1/0/all/0/1\">Hima Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikbal_S/0/1/0/all/0/1\">Shajith Ikbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapanipathi_P/0/1/0/all/0/1\">Pavan Kapanipathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1\">Ibrahim Abdelaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Young-Suk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Santosh Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pendus_C/0/1/0/all/0/1\">Cezar Pendus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dana_S/0/1/0/all/0/1\">Saswati Dana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1\">Dinesh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fokoue_A/0/1/0/all/0/1\">Achille Fokoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhargav_G/0/1/0/all/0/1\">G P Shrivatsa Bhargav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_D/0/1/0/all/0/1\">Dinesh Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravishankar_S/0/1/0/all/0/1\">Srinivas Ravishankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurajada_S/0/1/0/all/0/1\">Sairam Gurajada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Maria Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uceda_Sosa_R/0/1/0/all/0/1\">Rosario Uceda-Sosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1\">Salim Roukos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_A/0/1/0/all/0/1\">Alexander Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegel_G/0/1/0/all/0/1\">Guilherme LimaRyan Riegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luus_F/0/1/0/all/0/1\">Francois Luus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramaniam_L/0/1/0/all/0/1\">L Venkata Subramaniam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When in Doubt: Improving Classification Performance with Alternating Normalization. (arXiv:2109.13449v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13449","description":"<p>We introduce Classification with Alternating Normalization (CAN), a\nnon-parametric post-processing step for classification. CAN improves\nclassification accuracy for challenging examples by re-adjusting their\npredicted class probability distribution using the predicted class\ndistributions of high-confidence validation examples. CAN is easily applicable\nto any probabilistic classifier, with minimal computation overhead. We analyze\nthe properties of CAN using simulated experiments, and empirically demonstrate\nits effectiveness across a diverse set of classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Menglin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_A/0/1/0/all/0/1\">Austin Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Teacher-Student Learning Approach for Multi-lingual Speech-to-Intent Classification. (arXiv:2109.13486v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13486","description":"<p>End-to-end speech-to-intent classification has shown its advantage in\nharvesting information from both text and speech. In this paper, we study a\ntechnique to develop such an end-to-end system that supports multiple\nlanguages. To overcome the scarcity of multi-lingual speech corpus, we exploit\nknowledge from a pre-trained multi-lingual natural language processing model.\nMulti-lingual bidirectional encoder representations from transformers (mBERT)\nmodels are trained on multiple languages and hence expected to perform well in\nthe multi-lingual scenario. In this work, we employ a teacher-student learning\napproach to sufficiently extract information from an mBERT model to train a\nmulti-lingual speech model. In particular, we use synthesized speech generated\nfrom an English-Mandarin text corpus for analysis and training of a\nmulti-lingual intent classification model. We also demonstrate that the\nteacher-student learning approach obtains an improved performance (91.02%) over\nthe traditional end-to-end (89.40%) intent classification approach in a\npractical multi-lingual scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_B/0/1/0/all/0/1\">Bidisha Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhavi_M/0/1/0/all/0/1\">Maulik Madhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuehao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"How Robust r u?\": Evaluating Task-Oriented Dialogue Systems on Spoken Conversations. (arXiv:2109.13489v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13489","description":"<p>Most prior work in dialogue modeling has been on written conversations mostly\nbecause of existing data sets. However, written dialogues are not sufficient to\nfully capture the nature of spoken conversations as well as the potential\nspeech recognition errors in practical spoken dialogue systems. This work\npresents a new benchmark on spoken task-oriented conversations, which is\nintended to study multi-domain dialogue state tracking and knowledge-grounded\ndialogue modeling. We report that the existing state-of-the-art models trained\non written conversations are not performing well on our spoken data, as\nexpected. Furthermore, we observe improvements in task performances when\nleveraging n-best speech recognition hypotheses such as by combining\npredictions based on individual hypotheses. Our data set enables speech-based\nbenchmarking of task-oriented dialogue systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1\">Behnam Hedayatnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-Based Neural Dependency Parsing. (arXiv:2109.13497v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13497","description":"<p>Interpretable rationales for model predictions are crucial in practical\napplications. We develop neural models that possess an interpretable inference\nprocess for dependency parsing. Our models adopt instance-based inference,\nwhere dependency edges are extracted and labeled by comparing them to edges in\na training set. The training edges are explicitly used for the predictions;\nthus, it is easy to grasp the contribution of each edge to the predictions. Our\nexperiments show that our instance-based models achieve competitive accuracy\nwith standard neural models and have the reasonable plausibility of\ninstance-based explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouchi_H/0/1/0/all/0/1\">Hiroki Ouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Jun Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_S/0/1/0/all/0/1\">Sosuke Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoi_S/0/1/0/all/0/1\">Sho Yokoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuribayashi_T/0/1/0/all/0/1\">Tatsuki Kuribayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1\">Masashi Yoshikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoxCeleb Enrichment for Age and Gender Recognition. (arXiv:2109.13510v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13510","description":"<p>VoxCeleb datasets are widely used in speaker recognition studies. Our work\nserves two purposes. First, we provide speaker age labels and (an alternative)\nannotation of speaker gender. Second, we demonstrate the use of this metadata\nby constructing age and gender recognition models with different features and\nclassifiers. We query different celebrity databases and apply consensus rules\nto derive age and gender labels. We also compare the original VoxCeleb gender\nlabels with our labels to identify records that might be mislabeled in the\noriginal VoxCeleb data. On modeling side, we design a comprehensive study of\nmultiple features and models for recognizing gender and age. Our best system,\nusing i-vector features, achieved an F1-score of 0.9829 for gender recognition\ntask using logistic regression, and the lowest mean absolute error (MAE) in age\nregression, 9.443 years, is obtained with ridge regression. This indicates\nchallenge in age estimation from in-the-wild style speech data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hechmi_K/0/1/0/all/0/1\">Khaled Hechmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trong_T/0/1/0/all/0/1\">Trung Ngo Trong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hautamaki_V/0/1/0/all/0/1\">Ville Hautamaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinnunen_T/0/1/0/all/0/1\">Tomi Kinnunen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Template-free Prompt Tuning for Few-shot NER. (arXiv:2109.13532v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13532","description":"<p>Prompt-based methods have been successfully applied in sentence-level\nfew-shot learning tasks, mostly owing to the sophisticated design of templates\nand label words. However, when applied to token-level labeling tasks such as\nNER, it would be time-consuming to enumerate the template queries over all\npotential entity spans. In this work, we propose a more elegant method to\nreformulate NER tasks as LM problems without any templates. Specifically, we\ndiscard the template construction process while maintaining the word prediction\nparadigm of pre-training models to predict a class-related pivot word (or label\nword) at the entity position. Meanwhile, we also explore principled ways to\nautomatically search for appropriate label words that the pre-trained models\ncan easily adapt to. While avoiding complicated template-based process, the\nproposed LM objective also reduces the gap between different objectives used in\npre-training and fine-tuning, thus it can better benefit the few-shot\nperformance. Experimental results demonstrate the effectiveness of the proposed\nmethod over bert-tagger and template-based method under few-shot setting.\nMoreover, the decoding speed of the proposed method is up to 1930.12 times\nfaster than the template-based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yiding Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agreeing to Disagree: Annotating Offensive Language Datasets with Annotators' Disagreement. (arXiv:2109.13563v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13563","description":"<p>Since state-of-the-art approaches to offensive language detection rely on\nsupervised learning, it is crucial to quickly adapt them to the continuously\nevolving scenario of social media. While several approaches have been proposed\nto tackle the problem from an algorithmic perspective, so to reduce the need\nfor annotated data, less attention has been paid to the quality of these data.\nFollowing a trend that has emerged recently, we focus on the level of agreement\namong annotators while selecting data to create offensive language datasets, a\ntask involving a high level of subjectivity. Our study comprises the creation\nof three novel datasets of English tweets covering different topics and having\nfive crowd-sourced judgments each. We also present an extensive set of\nexperiments showing that selecting training and test data according to\ndifferent levels of annotators' agreement has a strong effect on classifiers\nperformance and robustness. Our findings are further validated in cross-domain\nexperiments and studied using a popular benchmark dataset. We show that such\nhard cases, where low agreement is present, are not necessarily due to\npoor-quality annotation and we advocate for a higher presence of ambiguous\ncases in future datasets, particularly in test sets, to better account for the\ndifferent points of view expressed online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leonardelli_E/0/1/0/all/0/1\">Elisa Leonardelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menini_S/0/1/0/all/0/1\">Stefano Menini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aprosio_A/0/1/0/all/0/1\">Alessio Palmero Aprosio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1\">Marco Guerini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonelli_S/0/1/0/all/0/1\">Sara Tonelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating texts under constraint through discriminator-guided MCTS. (arXiv:2109.13582v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13582","description":"<p>Large pre-trained language models (LM) based on Transformers allow to\ngenerate very plausible long texts. In this paper, we explore how this\ngeneration can be further controlled to satisfy certain constraints (eg. being\nnon-toxic, positive or negative, convey certain emotions, etc.) without\nfine-tuning the LM. Precisely, we formalize constrained generation as a tree\nexploration process guided by a discriminator according to how well the\nassociated sequence respects the constraint. Using a discriminator to guide\nthis generation, rather than fine-tuning the LM, in addition to be easier and\ncheaper to train, allows to apply the constraint more finely and dynamically.\nWe propose several original methods to search this generation tree, notably the\nMonte Carlo Tree Search (MCTS) which provides theoretical guarantees on the\nsearch efficiency, but also simpler methods based on re-ranking a pool of\ndiverse sequences using the discriminator scores. We evaluate these methods on\ntwo types of constraints and languages: review polarity and emotion control in\nFrench and English. We show that MCTS achieves state-of-the-art results in\nconstrained generation, without having to tune the language model, in both\ntasks and languages. We also demonstrate that our other proposed methods based\non re-ranking can be really effective when diversity among the generated\npropositions is encouraged.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaffin_A/0/1/0/all/0/1\">Antoine Chaffin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Claveau_V/0/1/0/all/0/1\">Vincent Claveau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kijak_E/0/1/0/all/0/1\">Ewa Kijak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Argument Mining: A Practical Approach. (arXiv:2109.13611v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13611","description":"<p>Despite considerable recent progress, the creation of well-balanced and\ndiverse resources remains a time-consuming and costly challenge in Argument\nMining. Active Learning reduces the amount of data necessary for the training\nof machine learning models by querying the most informative samples for\nannotation and therefore is a promising method for resource creation. In a\nlarge scale comparison of several Active Learning methods, we show that Active\nLearning considerably decreases the effort necessary to get good deep learning\nperformance on the task of Argument Unit Recognition and Classification (AURC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Solmsdorf_N/0/1/0/all/0/1\">Nikolai Solmsdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trautmann_D/0/1/0/all/0/1\">Dietrich Trautmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking. (arXiv:2109.13620v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13620","description":"<p>Recent progress in task-oriented neural dialogue systems is largely focused\non a handful of languages, as annotation of training data is tedious and\nexpensive. Machine translation has been used to make systems multilingual, but\nthis can introduce a pipeline of errors. Another promising solution is using\ncross-lingual transfer learning through pretrained multilingual models.\nExisting methods train multilingual models with additional code-mixed task data\nor refine the cross-lingual representations through parallel ontologies. In\nthis work, we enhance the transfer learning process by intermediate fine-tuning\nof pretrained multilingual models, where the multilingual models are fine-tuned\nwith different but related data and/or tasks. Specifically, we use parallel and\nconversational movie subtitles datasets to design cross-lingual intermediate\ntasks suitable for downstream dialogue tasks. We use only 200K lines of\nparallel data for intermediate fine-tuning which is already available for 1782\nlanguage pairs. We test our approach on the cross-lingual dialogue state\ntracking task for the parallel MultiWoZ (English -&gt; Chinese, Chinese -&gt;\nEnglish) and Multilingual WoZ (English -&gt; German, English -&gt; Italian) datasets.\nWe achieve impressive improvements (&gt; 20% on joint goal accuracy) on the\nparallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla\nbaseline with only 10% of the target language task data and zero-shot setup\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moghe_N/0/1/0/all/0/1\">Nikita Moghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPSL: End-to-end perception and reasoning with applications to zero shot learning. (arXiv:2109.13662v1 [eess.SY])","link":"http://arxiv.org/abs/2109.13662","description":"<p>We introduce DeepPSL a variant of Probabilistic Soft Logic (PSL) to produce\nan end-to-end trainable system that integrates reasoning and perception. PSL\nrepresents first-order logic in terms of a convex graphical model -- Hinge Loss\nMarkov random fields (HL-MRFs). PSL stands out among probabilistic logic\nframeworks due to its tractability having been applied to systems of more than\n1 billion ground rules. The key to our approach is to represent predicates in\nfirst-order logic using deep neural networks and then to approximately\nback-propagate through the HL-MRF and thus train every aspect of the\nfirst-order system being represented. We believe that this approach represents\nan interesting direction for the integration of deep learning and reasoning\ntechniques with applications to knowledge base learning, multi-task learning,\nand explainability. We evaluate DeepPSL on a zero shot learning problem in\nimage classification. State of the art results demonstrate the utility and\nflexibility of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Duffy_N/0/1/0/all/0/1\">Nigel Duffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puranam_S/0/1/0/all/0/1\">Sai Akhil Puranam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dasaratha_S/0/1/0/all/0/1\">Sridhar Dasaratha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phogat_K/0/1/0/all/0/1\">Karmvir Singh Phogat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tiyyagura_S/0/1/0/all/0/1\">Sunil Reddy Tiyyagura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Counter Narrative Type Classification. (arXiv:2109.13664v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13664","description":"<p>The growing interest in employing counter narratives for hatred intervention\nbrings with it a focus on dataset creation and automation strategies. In this\nscenario, learning to recognize counter narrative types from natural text is\nexpected to be useful for applications such as hate speech countering, where\noperators from non-governmental organizations are supposed to answer to hate\nwith several and diverse arguments that can be mined from online sources. This\npaper presents the first multilingual work on counter narrative type\nclassification, evaluating SoTA pre-trained language models in monolingual,\nmultilingual and cross-lingual settings. When considering a fine-grained\nannotation of counter narrative classes, we report strong baseline\nclassification results for the majority of the counter narrative types,\nespecially if we translate every language to English before cross-lingual\nprediction. This suggests that knowledge about counter narratives can be\nsuccessfully transferred across languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yi-Ling Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1\">Marco Guerini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nana-HDR: A Non-attentive Non-autoregressive Hybrid Model for TTS. (arXiv:2109.13673v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13673","description":"<p>This paper presents Nana-HDR, a new non-attentive non-autoregressive model\nwith hybrid Transformer-based Dense-fuse encoder and RNN-based decoder for TTS.\nIt mainly consists of three parts: Firstly, a novel Dense-fuse encoder with\ndense connections between basic Transformer blocks for coarse feature fusion\nand a multi-head attention layer for fine feature fusion. Secondly, a\nsingle-layer non-autoregressive RNN-based decoder. Thirdly, a duration\npredictor instead of an attention model that connects the above hybrid encoder\nand decoder. Experiments indicate that Nana-HDR gives full play to the\nadvantages of each component, such as strong text encoding ability of\nTransformer-based encoder, stateful decoding without being bothered by exposure\nbias and local information preference, and stable alignment provided by\nduration predictor. Due to these advantages, Nana-HDR achieves competitive\nperformance in naturalness and robustness on two Mandarin corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shilun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Wenchao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Li Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Fenglong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CIDEr-R: Robust Consensus-based Image Description Evaluation. (arXiv:2109.13701v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13701","description":"<p>This paper shows that CIDEr-D, a traditional evaluation metric for image\ndescription, does not work properly on datasets where the number of words in\nthe sentence is significantly greater than those in the MS COCO Captions\ndataset. We also show that CIDEr-D has performance hampered by the lack of\nmultiple reference sentences and high variance of sentence length. To bypass\nthis problem, we introduce CIDEr-R, which improves CIDEr-D, making it more\nflexible in dealing with datasets with high sentence length variance. We\ndemonstrate that CIDEr-R is more accurate and closer to human judgment than\nCIDEr-D; CIDEr-R is more robust regarding the number of available references.\nOur results reveal that using Self-Critical Sequence Training to optimize\nCIDEr-R generates descriptive captions. In contrast, when CIDEr-D is optimized,\nthe generated captions' length tends to be similar to the reference length.\nHowever, the models also repeat several times the same word to increase the\nsentence length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_G/0/1/0/all/0/1\">Gabriel Oliveira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombini_E/0/1/0/all/0/1\">Esther Luna Colombini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1\">Sandra Avila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One to rule them all: Towards Joint Indic Language Hate Speech Detection. (arXiv:2109.13711v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13711","description":"<p>This paper is a contribution to the Hate Speech and Offensive Content\nIdentification in Indo-European Languages (HASOC) 2021 shared task. Social\nmedia today is a hotbed of toxic and hateful conversations, in various\nlanguages. Recent news reports have shown that current models struggle to\nautomatically identify hate posted in minority languages. Therefore,\nefficiently curbing hate speech is a critical challenge and problem of\ninterest. We present a multilingual architecture using state-of-the-art\ntransformer language models to jointly learn hate and offensive speech\ndetection across three languages namely, English, Hindi, and Marathi. On the\nprovided testing corpora, we achieve Macro F1 scores of 0.7996, 0.7748, 0.8651\nfor sub-task 1A and 0.6268, 0.5603 during the fine-grained classification of\nsub-task 1B. These results show the efficacy of exploiting a multilingual\ntraining scheme.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_M/0/1/0/all/0/1\">Mehar Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotia_T/0/1/0/all/0/1\">Tenzin Singhay Bhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Akshat Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_P/0/1/0/all/0/1\">Prakash Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shubham Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1\">Kumar Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laumann_F/0/1/0/all/0/1\">Felix Laumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_A/0/1/0/all/0/1\">Ayushman Dash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Use of Character-Level Translation with Sparse and Noisy Datasets. (arXiv:2109.13723v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13723","description":"<p>This paper provides an analysis of character-level machine translation models\nused in pivot-based translation when applied to sparse and noisy datasets, such\nas crowdsourced movie subtitles. In our experiments, we find that such\ncharacter-level models cut the number of untranslated words by over 40% and are\nespecially competitive (improvements of 2-3 BLEU points) in the case of limited\ntraining data. We explore the impact of character alignment, phrase table\nfiltering, bitext size and the choice of pivot language on translation quality.\nWe further compare cascaded translation models to the use of synthetic training\ndata via multiple pivots, and we find that the latter works significantly\nbetter. Finally, we demonstrate that neither word-nor character-BLEU correlate\nperfectly with human judgments, due to BLEU's sensitivity to length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiedemann_J/0/1/0/all/0/1\">J&#xf6;rg Tiedemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translating from Morphologically Complex Languages: A Paraphrase-Based Approach. (arXiv:2109.13724v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13724","description":"<p>We propose a novel approach to translating from a morphologically complex\nlanguage. Unlike previous research, which has targeted word inflections and\nconcatenations, we focus on the pairwise relationship between morphologically\nrelated words, which we treat as potential paraphrases and handle using\nparaphrasing techniques at the word, phrase, and sentence level. An important\nadvantage of this framework is that it can cope with derivational morphology,\nwhich has so far remained largely beyond the capabilities of statistical\nmachine translation systems. Our experiments translating from Malay, whose\nmorphology is mostly derivational, into English show significant improvements\nover rivaling approaches based on five automatic evaluation measures (for\n320,000 sentence pairs; 9.5 million English word tokens).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis in Twitter for Macedonian. (arXiv:2109.13725v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13725","description":"<p>We present work on sentiment analysis in Twitter for Macedonian. As this is\npioneering work for this combination of language and genre, we created suitable\nresources for training and evaluating a system for sentiment analysis of\nMacedonian tweets. In particular, we developed a corpus of tweets annotated\nwith tweet-level sentiment polarity (positive, negative, and neutral), as well\nas with phrase-level sentiment, which we made freely available for research\npurposes. We further bootstrapped several large-scale sentiment lexicons for\nMacedonian, motivated by previous work for English. The impact of several\ndifferent pre-processing steps as well as of various features is shown in\nexperiments that represent the first attempt to build a system for sentiment\nanalysis in Twitter for the morphologically rich Macedonian language. Overall,\nour experimental results show an F1-score of 92.16, which is very strong and is\non par with the best results for English, which were achieved in recent SemEval\ncompetitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jovanoski_D/0/1/0/all/0/1\">Dame Jovanoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pachovski_V/0/1/0/all/0/1\">Veno Pachovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposing Paid Opinion Manipulation Trolls. (arXiv:2109.13726v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13726","description":"<p>Recently, Web forums have been invaded by opinion manipulation trolls. Some\ntrolls try to influence the other users driven by their own convictions, while\nin other cases they can be organized and paid, e.g., by a political party or a\nPR agency that gives them specific instructions what to write. Finding paid\ntrolls automatically using machine learning is a hard task, as there is no\nenough training data to train a classifier; yet some test data is possible to\nobtain, as these trolls are sometimes caught and widely exposed. In this paper,\nwe solve the training data problem by assuming that a user who is called a\ntroll by several different people is likely to be such, and one who has never\nbeen called a troll is unlikely to be such. We compare the profiles of (i) paid\ntrolls vs. (ii)\"mentioned\" trolls vs. (iii) non-trolls, and we further show\nthat a classifier trained to distinguish (ii) from (iii) does quite well also\nat telling apart (i) from (iii).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_G/0/1/0/all/0/1\">Georgi Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Triplet Loss for Named Entity Recognition using Supplementary Text. (arXiv:2109.13736v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13736","description":"<p>Retail item data contains many different forms of text like the title of an\nitem, the description of an item, item name and reviews. It is of interest to\nidentify the item name in the other forms of text using a named entity tagger.\nHowever, the title of an item and its description are syntactically different\n(but semantically similar) in that the title is not necessarily a well formed\nsentence while the description is made up of well formed sentences. In this\nwork, we use a triplet loss to contrast the embeddings of the item title with\nthe description to establish a proof of concept. We find that using the triplet\nloss in a multi-task NER algorithm improves both the precision and recall by a\nsmall percentage. While the improvement is small, we think it is a step in the\nright direction of using various forms of text in a multi-task algorithm. In\naddition to precision and recall, the multi task triplet loss method is also\nfound to significantly improve the exact match accuracy i.e. the accuracy of\ntagging the entire set of tokens in the text with correct tags.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siskind_R/0/1/0/all/0/1\">Ryan Siskind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Shalin Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Homophony and R\\'enyi Entropy. (arXiv:2109.13766v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13766","description":"<p>Homophony's widespread presence in natural languages is a controversial\ntopic. Recent theories of language optimality have tried to justify its\nprevalence, despite its negative effects on cognitive processing time; e.g.,\nPiantadosi et al. (2012) argued homophony enables the reuse of efficient\nwordforms and is thus beneficial for languages. This hypothesis has recently\nbeen challenged by Trott and Bergen (2020), who posit that good wordforms are\nmore often homophonous simply because they are more phonotactically probable.\nIn this paper, we join in on the debate. We first propose a new\ninformation-theoretic quantification of a language's homophony: the sample\nR\\'enyi entropy. Then, we use this quantification to revisit Trott and Bergen's\nclaims. While their point is theoretically sound, a specific methodological\nissue in their experiments raises doubts about their results. After addressing\nthis issue, we find no clear pressure either towards or against homophony -- a\nmuch more nuanced result than either Piantadosi et al.'s or Trott and Bergen's\nfindings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teufel_S/0/1/0/all/0/1\">Simone Teufel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying and Mitigating Gender Bias in Hyperbolic Word Embeddings. (arXiv:2109.13767v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13767","description":"<p>Euclidean word embedding models such as GloVe and Word2Vec have been shown to\nreflect human-like gender biases. In this paper, we extend the study of gender\nbias to the recently popularized hyperbolic word embeddings. We propose\ngyrocosine bias, a novel measure for quantifying gender bias in hyperbolic word\nrepresentations and observe a significant presence of gender bias. To address\nthis problem, we propose Poincar\\'e Gender Debias (PGD), a novel debiasing\nprocedure for hyperbolic word representations. Experiments on a suit of\nevaluation tests show that PGD effectively reduces bias while adding a minimal\nsemantic offset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vaibhav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotia_T/0/1/0/all/0/1\">Tenzin Singhay Bhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vaibhav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health. (arXiv:2109.13770v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13770","description":"<p>Many statistical models have high accuracy on test benchmarks, but are not\nexplainable, struggle in low-resource scenarios, cannot be reused for multiple\ntasks, and cannot easily integrate domain expertise. These factors limit their\nuse, particularly in settings such as mental health, where it is difficult to\nannotate datasets and model outputs have significant impact. We introduce a\nmicromodel architecture to address these challenges. Our approach allows\nresearchers to build interpretable representations that embed domain knowledge\nand provide explanations throughout the model's decision process. We\ndemonstrate the idea on multiple mental health tasks: depression\nclassification, PTSD classification, and suicidal risk assessment. Our systems\nconsistently produce strong results, even in low-resource scenarios, and are\nmore interpretable than alternative methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Andrew Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1\">Jonathan K. Kummerfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Lawrence C. An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chekhov's Gun Recognition. (arXiv:2109.13855v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13855","description":"<p>Chekhov's gun is a dramatic principle stating that every element in a story\nmust be necessary, and irrelevant elements should be removed. This paper\npresents a new natural language processing task - Chekhov's gun recognition or\n(CGR) - recognition of entities that are pivotal for the development of the\nplot. Though similar to classical Named Entity Recognition (NER) it has\nprofound differences and is crucial for the tasks of narrative processing,\nsince Chekhov's guns have a profound impact on the causal relationship in a\nstory. The paper presents a new benchmark dataset for the CGR task that\nincludes 5550 descriptions with one or more Chekhov's Gun in each and validates\nthe task on two more datasets available in the natural language processing\n(NLP) literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expectation-based Minimalist Grammars. (arXiv:2109.13871v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13871","description":"<p>Expectation-based Minimalist Grammars (e-MGs) are simplified versions of the\n(Conflated) Minimalist Grammars, (C)MGs, formalized by Stabler (Stabler, 2011,\n2013, 1997) and Phase-based Minimalist Grammars, PMGs (Chesi, 2005, 2007;\nStabler, 2011). The crucial simplification consists of driving structure\nbuilding only by relying on lexically encoded categorial top-down expectations.\nThe commitment on a top-down derivation (as in e-MGs and PMGs, as opposed to\n(C)MGs, Chomsky, 1995; Stabler, 2011) allows us to define a core derivation\nthat should be the same in both parsing and generation (Momma &amp; Phillips,\n2018).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chesi_C/0/1/0/all/0/1\">Cristiano Chesi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-dataset Experts for Multi-dataset Question Answering. (arXiv:2109.13880v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13880","description":"<p>Many datasets have been created for training reading comprehension models,\nand a natural question is whether we can combine them to build models that (1)\nperform better on all of the training datasets and (2) generalize and transfer\nbetter to new datasets. Prior work has addressed this goal by training one\nnetwork simultaneously on multiple datasets, which works well on average but is\nprone to over- or under-fitting different sub-distributions and might transfer\nworse compared to source models with more overlap with the target dataset. Our\napproach is to model multi-dataset question answering with a collection of\nsingle-dataset experts, by training a collection of lightweight,\ndataset-specific adapter modules (Houlsby et al., 2019) that share an\nunderlying Transformer model. We find that these Multi-Adapter Dataset Experts\n(MADE) outperform all our baselines in terms of in-distribution accuracy, and\nsimple methods based on parameter-averaging lead to better zero-shot\ngeneralization and few-shot transfer performance, offering a strong and\nversatile starting point for building new reading comprehension systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedman_D/0/1/0/all/0/1\">Dan Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_B/0/1/0/all/0/1\">Ben Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Different Text-preprocessing Techniques Using The BERT Model Affect The Gender Profiling of Authors. (arXiv:2109.13890v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13890","description":"<p>Forensic author profiling plays an important role in indicating possible\nprofiles for suspects. Among the many automated solutions recently proposed for\nauthor profiling, transfer learning outperforms many other state-of-the-art\ntechniques in natural language processing. Nevertheless, the sophisticated\ntechnique has yet to be fully exploited for author profiling. At the same time,\nwhereas current methods of author profiling, all largely based on features\nengineering, have spawned significant variation in each model used, transfer\nlearning usually requires a preprocessed text to be fed into the model. We\nreviewed multiple references in the literature and determined the most common\npreprocessing techniques associated with authors' genders profiling.\nConsidering the variations in potential preprocessing techniques, we conducted\nan experimental study that involved applying five such techniques to measure\neach technique's effect while using the BERT model, chosen for being one of the\nmost-used stock pretrained models. We used the Hugging face transformer library\nto implement the code for each preprocessing case. In our five experiments, we\nfound that BERT achieves the best accuracy in predicting the gender of the\nauthor when no preprocessing technique is applied. Our best case achieved\n86.67% accuracy in predicting the gender of authors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alzahrani_E/0/1/0/all/0/1\">Esam Alzahrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jololian_L/0/1/0/all/0/1\">Leon Jololian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Information and Event Markup Language: TIE-ML Markup Process and Schema Version 1.0. (arXiv:2109.13892v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13892","description":"<p>Temporal Information and Event Markup Language (TIE-ML) is a markup strategy\nand annotation schema to improve the productivity and accuracy of temporal and\nevent related annotation of corpora to facilitate machine learning based model\ntraining. For the annotation of events, temporal sequencing, and durations, it\nis significantly simpler by providing an extremely reduced tag set for just\ntemporal relations and event enumeration. In comparison to other standards, as\nfor example the Time Markup Language (TimeML), it is much easier to use by\ndropping sophisticated formalisms, theoretical concepts, and annotation\napproaches. Annotations of corpora using TimeML can be mapped to TIE-ML with a\nloss, and TIE-ML annotations can be fully mapped to TimeML with certain\nunder-specification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cavar_D/0/1/0/all/0/1\">Damir Cavar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickson_B/0/1/0/all/0/1\">Billy Dickson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljubailan_A/0/1/0/all/0/1\">Ali Aljubailan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soyoung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsolved Problems in ML Safety. (arXiv:2109.13916v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13916","description":"<p>Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), steering ML systems (\"Alignment\"), and\nreducing risks to how ML systems are handled (\"External Safety\"). Throughout,\nwe clarify each problem's motivation and provide concrete research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Equations: Inherently Interpretable Sparse Word Embeddingsthrough Sparse Coding. (arXiv:2004.13847v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.13847","description":"<p>Word embeddings are a powerful natural language processing technique, but\nthey are extremely difficult to interpret. To enable interpretable NLP models,\nwe create vectors where each dimension is inherently interpretable. By\ninherently interpretable, we mean a system where each dimension is associated\nwith some human understandable hint that can describe the meaning of that\ndimension. In order to create more interpretable word embeddings, we transform\npretrained dense word embeddings into sparse embeddings. These new embeddings\nare inherently interpretable: each of their dimensions is created from and\nrepresents a natural language word or specific grammatical concept. We\nconstruct these embeddings through sparse coding, where each vector in the\nbasis set is itself a word embedding. Therefore, each dimension of our sparse\nvectors corresponds to a natural language word. We also show that models\ntrained using these sparse embeddings can achieve good performance and are more\ninterpretable in practice, including through human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Templeton_A/0/1/0/all/0/1\">Adly Templeton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Knowledge Graphs Canonicalization using Variational Autoencoders. (arXiv:2012.04780v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.04780","description":"<p>Noun phrases and Relation phrases in open knowledge graphs are not\ncanonicalized, leading to an explosion of redundant and ambiguous\nsubject-relation-object triples. Existing approaches to solve this problem take\na two-step approach. First, they generate embedding representations for both\nnoun and relation phrases, then a clustering algorithm is used to group them\nusing the embeddings as features. In this work, we propose Canonicalizing Using\nVariational Autoencoders (CUVA), a joint model to learn both embeddings and\ncluster assignments in an end-to-end approach, which leads to a better vector\nrepresentation for the noun and relation phrases. Our evaluation over multiple\nbenchmarks shows that CUVA outperforms the existing state-of-the-art\napproaches. Moreover, we introduce CanonicNell, a novel dataset to evaluate\nentity canonicalization systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Sarthak Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Sugato Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models. (arXiv:2102.07988v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.07988","description":"<p>Model parallelism has become a necessity for training modern large-scale deep\nlanguage models. In this work, we identify a new and orthogonal dimension from\nexisting model parallel approaches: it is possible to perform pipeline\nparallelism within a single training sequence for Transformer-based language\nmodels thanks to its autoregressive property. This enables a more fine-grained\npipeline compared with previous work. With this key idea, we design TeraPipe, a\nhigh-performance token-level pipeline parallel algorithm for synchronous\nmodel-parallel training of Transformer-based language models. We develop a\nnovel dynamic programming-based algorithm to calculate the optimal pipelining\nexecution scheme given a specific model and cluster configuration. We show that\nTeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175\nbillion parameters on an AWS cluster with 48 p3.16xlarge instances compared\nwith state-of-the-art model-parallel methods. The code for reproduction can be\nfound at https://github.com/zhuohan123/terapipe\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Siyuan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shiyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_D/0/1/0/all/0/1\">Danyang Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving and Simplifying Pattern Exploiting Training. (arXiv:2103.11955v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11955","description":"<p>Recently, pre-trained language models (LMs) have achieved strong performance\nwhen fine-tuned on difficult benchmarks like SuperGLUE. However, performance\ncan suffer when there are very few labeled examples available for fine-tuning.\nPattern Exploiting Training (PET) is a recent approach that leverages patterns\nfor few-shot learning. However, PET uses task-specific unlabeled data. In this\npaper, we focus on few-shot learning without any unlabeled data and introduce\nADAPET, which modifies PET's objective to provide denser supervision during\nfine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any\ntask-specific unlabeled data. Our code can be found at\nhttps://github.com/rrmenon10/ADAPET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tam_D/0/1/0/all/0/1\">Derek Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_R/0/1/0/all/0/1\">Rakesh R Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intent Recognition and Unsupervised Slot Identification for Low Resourced Spoken Dialog Systems. (arXiv:2104.01287v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.01287","description":"<p>Intent Recognition and Slot Identification are crucial components in spoken\nlanguage understanding (SLU) systems. In this paper, we present a novel\napproach towards both these tasks in the context of low resourced and unwritten\nlanguages. We present an acoustic based SLU system that converts speech to its\nphonetic transcription using a universal phone recognition system. We build a\nword-free natural language understanding module that does intent recognition\nand slot identification from these phonetic transcription. Our proposed SLU\nsystem performs competitively for resource rich scenarios and significantly\noutperforms existing approaches as the amount of available data reduces. We\nobserve more than 10% improvement for intent classification in Tamil and more\nthan 5% improvement for intent classification in Sinhala. We also present a\nnovel approach towards unsupervised slot identification using normalized\nattention scores. This approach can be used for unsupervised slot labelling,\ndata augmentation and to generate data for a new slot in a one-shot way with\nonly one speech recording\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_O/0/1/0/all/0/1\">Olivia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushwaha_A/0/1/0/all/0/1\">Akruti Kushwaha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1\">Saloni Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">William Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallabandi_S/0/1/0/all/0/1\">Sai Krishna Rallabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hidden Backdoors in Human-Centric Language Models. (arXiv:2105.00164v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00164","description":"<p>Natural language processing (NLP) systems have been proven to be vulnerable\nto backdoor attacks, whereby hidden features (backdoors) are trained into a\nlanguage model and may only be activated by specific inputs (called triggers),\nto trick the model into producing unexpected behaviors. In this paper, we\ncreate covert and natural triggers for textual backdoor attacks, \\textit{hidden\nbackdoors}, where triggers can fool both modern language models and human\ninspection. We deploy our hidden backdoors through two state-of-the-art trigger\nembedding methods. The first approach via homograph replacement, embeds the\ntrigger into deep neural networks through the visual spoofing of lookalike\ncharacter replacement. The second approach uses subtle differences between text\ngenerated by language models and real natural text to produce trigger sentences\nwith correct grammar and high fluency. We demonstrate that the proposed hidden\nbackdoors can be effective across three downstream security-critical NLP tasks,\nrepresentative of modern human-centric NLP systems, including toxic comment\ndetection, neural machine translation (NMT), and question answering (QA). Our\ntwo hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at\nleast $97\\%$ with an injection rate of only $3\\%$ in toxic comment detection,\n$95.1\\%$ ASR in NMT with less than $0.5\\%$ injected data, and finally $91.12\\%$\nASR against QA updated with only 27 poisoning data samples on a model\npreviously trained with 92,024 samples (0.029\\%). We are able to demonstrate\nthe adversary's high success rate of attacks, while maintaining functionality\nfor regular users, with triggers inconspicuous by the human administrators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaofeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_T/0/1/0/all/0/1\">Tian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Benjamin Zi Hao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Minhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haojin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jialiang Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Low-resource Reading Comprehension via Cross-lingual Transposition Rethinking. (arXiv:2107.05002v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.05002","description":"<p>Extractive Reading Comprehension (ERC) has made tremendous advances enabled\nby the availability of large-scale high-quality ERC training data. Despite of\nsuch rapid progress and widespread application, the datasets in languages other\nthan high-resource languages such as English remain scarce. To address this\nissue, we propose a Cross-Lingual Transposition ReThinking (XLTT) model by\nmodelling existing high-quality extractive reading comprehension datasets in a\nmultilingual environment. To be specific, we present multilingual adaptive\nattention (MAA) to combine intra-attention and inter-attention to learn more\ngeneral generalizable semantic and lexical knowledge from each pair of language\nfamilies. Furthermore, to make full use of existing datasets, we adopt a new\ntraining framework to train our model by calculating task-level similarities\nbetween each existing dataset and target dataset. The experimental results show\nthat our XLTT model surpasses six baselines on two multilingual ERC benchmarks,\nespecially more effective for low-resource languages with 3.9 and 4.1 average\nimprovement in F1 and EM, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gaochen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuxin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1\">Fei Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bangchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongwen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dejie Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arabic aspect based sentiment analysis using BERT. (arXiv:2107.13290v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.13290","description":"<p>Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that\ndefines the polarity of opinions on certain aspects related to specific\ntargets. The majority of research on ABSA is in English, with a small amount of\nwork available in Arabic. Most previous Arabic research has relied on deep\nlearning models that depend primarily on context-independent word embeddings\n(e.g.word2vec), where each word has a fixed representation independent of its\ncontext. This article explores the modeling capabilities of contextual\nembeddings from pre-trained language models, such as BERT, and making use of\nsentence pair input on Arabic ABSA tasks. In particular, we are building a\nsimple but effective BERT-based neural baseline to handle this task. Our BERT\narchitecture with a simple linear classification layer surpassed the\nstate-of-the-art works, according to the experimental results on the\nbenchmarked Arabic hotel reviews dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelgwad_M/0/1/0/all/0/1\">Mohammed M.Abdelgwad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeqScore: Addressing Barriers to Reproducible Named Entity Recognition Evaluation. (arXiv:2107.14154v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.14154","description":"<p>To address a looming crisis of unreproducible evaluation for named entity\nrecognition, we propose guidelines and introduce SeqScore, a software package\nto improve reproducibility. The guidelines we propose are extremely simple and\ncenter around transparency regarding how chunks are encoded and scored. We\ndemonstrate that despite the apparent simplicity of NER evaluation, unreported\ndifferences in the scoring procedure can result in changes to scores that are\nboth of noticeable magnitude and statistically significant. We describe\nSeqScore, which addresses many of the issues that cause replication failures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palen_Michel_C/0/1/0/all/0/1\">Chester Palen-Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holley_N/0/1/0/all/0/1\">Nolan Holley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HeadlineCause: A Dataset of News Headlines for Detecting Causalities. (arXiv:2108.12626v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12626","description":"<p>Detecting implicit causal relations in texts is a task that requires both\ncommon sense and world knowledge. Existing datasets are focused either on\ncommonsense causal reasoning or explicit causal relations. In this work, we\npresent HeadlineCause, a dataset for detecting implicit causal relations\nbetween pairs of news headlines. The dataset includes over 5000 headline pairs\nfrom English news and over 9000 headline pairs from Russian news labeled\nthrough crowdsourcing. The pairs vary from totally unrelated or belonging to\nthe same general topic to the ones including causation and refutation\nrelations. We also present a set of models and experiments that demonstrates\nthe dataset validity, including a multilingual XLM-RoBERTa based model for\ncausality detection and a GPT-2 based model for possible effects prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gusev_I/0/1/0/all/0/1\">Ilya Gusev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Task Difficulty for Few-Shot Relation Extraction. (arXiv:2109.05473v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05473","description":"<p>Few-shot relation extraction (FSRE) focuses on recognizing novel relations by\nlearning with merely a handful of annotated instances. Meta-learning has been\nwidely adopted for such a task, which trains on randomly generated few-shot\ntasks to learn generic data representations. Despite impressive results\nachieved, existing models still perform suboptimally when handling hard FSRE\ntasks, where the relations are fine-grained and similar to each other. We argue\nthis is largely because existing models do not distinguish hard tasks from easy\nones in the learning process. In this paper, we introduce a novel approach\nbased on contrastive learning that learns better representations by exploiting\nrelation label information. We further design a method that allows the model to\nadaptively learn how to focus on hard tasks. Experiments on two standard\ndatasets demonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiale Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Something Old, Something New: Grammar-based CCG Parsing with Transformer Models. (arXiv:2109.10044v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10044","description":"<p>This report describes the parsing problem for Combinatory Categorial Grammar\n(CCG), showing how a combination of Transformer-based neural models and a\nsymbolic CCG grammar can lead to substantial gains over existing approaches.\nThe report also documents a 20-year research program, showing how NLP methods\nhave evolved over this time. The staggering accuracy improvements provided by\nneural models for CCG parsing can be seen as a reflection of the improvements\nseen in NLP more generally. The report provides a minimal introduction to CCG\nand CCG parsing, with many pointers to the relevant literature. It then\ndescribes the CCG supertagging problem, and some recent work from Tian et al.\n(2020) which applies Transformer-based models to supertagging with great\neffect. I use this existing model to develop a CCG multitagger, which can serve\nas a front-end to an existing CCG parser. Simply using this new multitagger\nprovides substantial gains in parsing accuracy. I then show how a\nTransformer-based model from the parsing literature can be combined with the\ngrammar-based CCG parser, setting a new state-of-the-art for the CCGbank\nparsing task of almost 93% F-score for labelled dependencies, with complete\nsentence accuracies of over 50%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clark_S/0/1/0/all/0/1\">Stephen Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursively Summarizing Books with Human Feedback. (arXiv:2109.10862v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10862","description":"<p>A major challenge for scaling machine learning is training models to perform\ntasks that are very difficult or time-consuming for humans to evaluate. We\npresent progress on this problem on the task of abstractive summarization of\nentire fiction novels. Our method combines learning from human feedback with\nrecursive task decomposition: we use models trained on smaller parts of the\ntask to assist humans in giving feedback on the broader task. We collect a\nlarge volume of demonstrations and comparisons from human labelers, and\nfine-tune GPT-3 using behavioral cloning and reward modeling to do\nsummarization recursively. At inference time, the model first summarizes small\nsections of the book and then recursively summarizes these summaries to produce\na summary of the entire book. Our human labelers are able to supervise and\nevaluate the models quickly, despite not having read the entire books\nthemselves. Our resulting model generates sensible summaries of entire books,\neven matching the quality of human-written summaries in a few cases ($\\sim5\\%$\nof books). We achieve state-of-the-art results on the recent BookSum dataset\nfor book-length summarization. A zero-shot question-answering model using these\nsummaries achieves state-of-the-art results on the challenging NarrativeQA\nbenchmark for answering questions about books and movie scripts. We release\ndatasets of samples from our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jeff Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1\">Long Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1\">Daniel M. Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiennon_N/0/1/0/all/0/1\">Nisan Stiennon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_R/0/1/0/all/0/1\">Ryan Lowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1\">Jan Leike</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1\">Paul Christiano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AES Systems Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses. (arXiv:2109.11728v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11728","description":"<p>Deep-learning based Automatic Essay Scoring (AES) systems are being actively\nused by states and language testing agencies alike to evaluate millions of\ncandidates for life-changing decisions ranging from college applications to\nvisa approvals. However, little research has been put to understand and\ninterpret the black-box nature of deep-learning based scoring algorithms.\nPrevious studies indicate that scoring models can be easily fooled. In this\npaper, we explore the reason behind their surprising adversarial brittleness.\nWe utilize recent advances in interpretability to find the extent to which\nfeatures such as coherence, content, vocabulary, and relevance are important\nfor automated scoring mechanisms. We use this to investigate the\noversensitivity i.e., large change in output score with a little change in\ninput essay content) and overstability i.e., little change in output scores\nwith large changes in input essay content) of AES. Our results indicate that\nautoscoring models, despite getting trained as \"end-to-end\" models with rich\ncontextual embeddings such as BERT, behave like bag-of-words models. A few\nwords determine the essay score without the requirement of any context making\nthe model largely overstable. This is in stark contrast to recent probing\nstudies on pre-trained representation learning models, which show that rich\nlinguistic features such as parts-of-speech and morphology are encoded by them.\nFurther, we also find that the models have learnt dataset biases, making them\noversensitive. To deal with these issues, we propose detection-based protection\nmodels that can detect oversensitivity and overstability causing samples with\nhigh accuracies. We find that our proposed models are able to detect unusual\nattribution patterns and flag adversarial samples successfully.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_Y/0/1/0/all/0/1\">Yaman Singla Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_S/0/1/0/all/0/1\">Swapnil Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Somesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts. (arXiv:2109.12761v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12761","description":"<p>In order to better simulate the real human conversation process, models need\nto generate dialogue utterances based on not only preceding textual contexts\nbut also visual contexts. However, with the development of multi-modal dialogue\nlearning, the dataset scale gradually becomes a bottleneck. In this report, we\nrelease OpenViDial 2.0, a larger-scale open-domain multi-modal dialogue dataset\ncompared to the previous version OpenViDial 1.0. OpenViDial 2.0 contains a\ntotal number of 5.6 million dialogue turns extracted from either movies or TV\nseries from different resources, and each dialogue turn is paired with its\ncorresponding visual context. We hope this large-scale dataset can help\nfacilitate future researches on open-domain multi-modal dialog generation,\ne.g., multi-modal pretraining for dialogue generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_R/0/1/0/all/0/1\">Rongbin Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations. (arXiv:2109.13059v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13059","description":"<p>In NLP, a large volume of tasks involve pairwise comparison between two\nsequences (e.g. sentence similarity and paraphrase identification).\nPredominantly, two formulations are used for sentence-pair tasks: bi-encoders\nand cross-encoders. Bi-encoders produce fixed-dimensional sentence\nrepresentations and are computationally efficient, however, they usually\nunderperform cross-encoders. Cross-encoders can leverage their attention heads\nto exploit inter-sentence interactions for better performance but they require\ntask fine-tuning and are computationally more expensive. In this paper, we\npresent a completely unsupervised sentence representation model termed as\nTrans-Encoder that combines the two learning paradigms into an iterative joint\nframework to simultaneously learn enhanced bi- and cross-encoders.\nSpecifically, on top of a pre-trained Language Model (PLM), we start with\nconverting it to an unsupervised bi-encoder, and then alternate between the bi-\nand cross-encoder task formulations. In each alternation, one task formulation\nwill produce pseudo-labels which are used as learning signals for the other\ntask formulation. We then propose an extension to conduct such\nself-distillation approach on multiple PLMs in parallel and use the average of\ntheir pseudo-labels for mutual-distillation. Trans-Encoder creates, to the best\nof our knowledge, the first completely unsupervised cross-encoder and also a\nstate-of-the-art unsupervised bi-encoder for sentence similarity. Both the\nbi-encoder and cross-encoder formulations of Trans-Encoder outperform recently\nproposed state-of-the-art unsupervised sentence encoders such as Mirror-BERT\nand SimCSE by up to 5% on the sentence similarity benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yunlong Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massiah_J/0/1/0/all/0/1\">Jordan Massiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havrylov_S/0/1/0/all/0/1\">Serhii Havrylov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prefix-to-SQL: Text-to-SQL Generation from Incomplete User Questions. (arXiv:2109.13066v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13066","description":"<p>Existing text-to-SQL research only considers complete questions as the input,\nbut lay-users might strive to formulate a complete question. To build a smarter\nnatural language interface to database systems (NLIDB) that also processes\nincomplete questions, we propose a new task, prefix-to-SQL which takes question\nprefix from users as the input and predicts the intended SQL. We construct a\nnew benchmark called PAGSAS that contains 124K user question prefixes and the\nintended SQL for 5 sub-tasks Advising, GeoQuery, Scholar, ATIS, and Spider.\nAdditionally, we propose a new metric SAVE to measure how much effort can be\nsaved by users. Experimental results show that PAGSAS is challenging even for\nstrong baseline models such as T5. As we observe the difficulty of\nprefix-to-SQL is related to the number of omitted tokens, we incorporate\ncurriculum learning of feeding examples with an increasing number of omitted\ntokens. This improves scores on various sub-tasks by as much as 9% recall\nscores on sub-task GeoQuery in PAGSAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Naihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaichen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Generation of Word Problems for Academic Education via Natural Language Processing (NLP). (arXiv:2109.13123v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13123","description":"<p>Digital learning platforms enable students to learn on a flexible and\nindividual schedule as well as providing instant feedback mechanisms. The field\nof STEM education requires students to solve numerous training exercises to\ngrasp underlying concepts. It is apparent that there are restrictions in\ncurrent online education in terms of exercise diversity and individuality. Many\nexercises show little variance in structure and content, hindering the adoption\nof abstraction capabilities by students. This thesis proposes an approach to\ngenerate diverse, context rich word problems. In addition to requiring the\ngenerated language to be grammatically correct, the nature of word problems\nimplies additional constraints on the validity of contents. The proposed\napproach is proven to be effective in generating valid word problems for\nmathematical statistics. The experimental results present a tradeoff between\ngeneration time and exercise validity. The system can easily be parametrized to\nhandle this tradeoff according to the requirements of specific use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keller_S/0/1/0/all/0/1\">Stanley Uros Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Impact of Domain Shift on Left and Right Ventricle Segmentation in Short Axis Cardiac MR Images. (arXiv:2109.13230v1 [eess.IV])","link":"http://arxiv.org/abs/2109.13230","description":"<p>Domain shift refers to the difference in the data distribution of two\ndatasets, normally between the training set and the test set for machine\nlearning algorithms. Domain shift is a serious problem for generalization of\nmachine learning models and it is well-established that a domain shift between\nthe training and test sets may cause a drastic drop in the model's performance.\nIn medical imaging, there can be many sources of domain shift such as different\nscanners or scan protocols, different pathologies in the patient population,\nanatomical differences in the patient population (e.g. men vs women) etc.\nTherefore, in order to train models that have good generalization performance,\nit is important to be aware of the domain shift problem, its potential causes\nand to devise ways to address it. In this paper, we study the effect of domain\nshift on left and right ventricle blood pool segmentation in short axis cardiac\nMR images. Our dataset contains short axis images from 4 different MR scanners\nand 3 different pathology groups. The training is performed with nnUNet. The\nresults show that scanner differences cause a greater drop in performance\ncompared to changing the pathology group, and that the impact of domain shift\nis greater on right ventricle segmentation compared to left ventricle\nsegmentation. Increasing the number of training subjects increased\ncross-scanner performance more than in-scanner performance at small training\nset sizes, but this difference in improvement decreased with larger training\nset sizes. Training models using data from multiple scanners improved\ncross-domain performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ugurlu_D/0/1/0/all/0/1\">Devran Ugurlu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puyol_Anton_E/0/1/0/all/0/1\">Esther Puyol-Anton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruijsink_B/0/1/0/all/0/1\">Bram Ruijsink</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Young_A/0/1/0/all/0/1\">Alistair Young</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Machado_I/0/1/0/all/0/1\">Ines Machado</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammernik_K/0/1/0/all/0/1\">Kerstin Hammernik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_A/0/1/0/all/0/1\">Andrew P. King</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DOODLER: Determining Out-Of-Distribution Likelihood from Encoder Reconstructions. (arXiv:2109.13237v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13237","description":"<p>Deep Learning models possess two key traits that, in combination, make their\nuse in the real world a risky prospect. One, they do not typically generalize\nwell outside of the distribution for which they were trained, and two, they\ntend to exhibit confident behavior regardless of whether or not they are\nproducing meaningful outputs. While Deep Learning possesses immense power to\nsolve realistic, high-dimensional problems, these traits in concert make it\ndifficult to have confidence in their real-world applications. To overcome this\ndifficulty, the task of Out-Of-Distribution (OOD) Detection has been defined,\nto determine when a model has received an input from outside of the\ndistribution for which it is trained to operate.\n</p>\n<p>This paper introduces and examines a novel methodology, DOODLER, for OOD\nDetection, which directly leverages the traits which result in its necessity.\nBy training a Variational Auto-Encoder (VAE) on the same data as another Deep\nLearning model, the VAE learns to accurately reconstruct In-Distribution (ID)\ninputs, but not to reconstruct OOD inputs, meaning that its failure state can\nbe used to perform OOD Detection. Unlike other work in the area, DOODLER\nrequires only very weak assumptions about the existence of an OOD dataset,\nallowing for more realistic application. DOODLER also enables pixel-wise\nsegmentations of input images by OOD likelihood, and experimental results show\nthat it matches or outperforms methodologies that operate under the same\nconstraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kent_J/0/1/0/all/0/1\">Jonathan S. Kent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually Grounded Reasoning across Languages and Cultures. (arXiv:2109.13238v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13238","description":"<p>The design of widespread vision-and-language datasets and pre-trained\nencoders directly adopts, or draws inspiration from, the concepts and images of\nImageNet. While one can hardly overestimate how much this benchmark contributed\nto progress in computer vision, it is mostly derived from lexical databases and\nimage queries in English, resulting in source material with a North American or\nWestern European bias. Therefore, we devise a new protocol to construct an\nImageNet-style hierarchy representative of more languages and cultures. In\nparticular, we let the selection of both concepts and images be entirely driven\nby native speakers, rather than scraping them automatically. Specifically, we\nfocus on a typologically diverse set of languages, namely, Indonesian, Mandarin\nChinese, Swahili, Tamil, and Turkish. On top of the concepts and images\nobtained through this new protocol, we create a multilingual dataset for\n{M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting\nstatements from native speaker annotators about pairs of images. The task\nconsists of discriminating whether each grounded statement is true or false. We\nestablish a series of baselines using state-of-the-art models and find that\ntheir cross-lingual transfer performance lags dramatically behind supervised\nperformance in English. These results invite us to reassess the robustness and\naccuracy of current state-of-the-art models beyond a narrow domain, but also\nopen up new exciting challenges for the development of truly multilingual and\nmulticultural systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Urban Driver: Learning to Drive from Real-world Demonstrations Using Policy Gradients. (arXiv:2109.13333v1 [cs.RO])","link":"http://arxiv.org/abs/2109.13333","description":"<p>In this work we are the first to present an offline policy gradient method\nfor learning imitative policies for complex urban driving from a large corpus\nof real-world demonstrations. This is achieved by building a differentiable\ndata-driven simulator on top of perception outputs and high-fidelity HD maps of\nthe area. It allows us to synthesize new driving experiences from existing\ndemonstrations using mid-level representations. Using this simulator we then\ntrain a policy network in closed-loop employing policy gradients. We train our\nproposed method on 100 hours of expert demonstrations on urban roads and show\nthat it learns complex driving policies that generalize well and can perform a\nvariety of driving maneuvers. We demonstrate this in simulation as well as\ndeploy our model to self-driving vehicles in the real-world. Our method\noutperforms previously demonstrated state-of-the-art for urban driving\nscenarios -- all this without the need for complex state perturbations or\ncollecting additional on-policy data during training. We make code and data\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scheel_O/0/1/0/all/0/1\">Oliver Scheel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergamini_L/0/1/0/all/0/1\">Luca Bergamini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolczyk_M/0/1/0/all/0/1\">Maciej Wo&#x142;czyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1\">B&#x142;a&#x17c;ej Osi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Computer Vision on Edge Devices with Pipeline-Parallel Hierarchical Neural Networks. (arXiv:2109.13356v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13356","description":"<p>Computer vision on low-power edge devices enables applications including\nsearch-and-rescue and security. State-of-the-art computer vision algorithms,\nsuch as Deep Neural Networks (DNNs), are too large for inference on low-power\nedge devices. To improve efficiency, some existing approaches parallelize DNN\ninference across multiple edge devices. However, these techniques introduce\nsignificant communication and synchronization overheads or are unable to\nbalance workloads across devices. This paper demonstrates that the hierarchical\nDNN architecture is well suited for parallel processing on multiple edge\ndevices. We design a novel method that creates a parallel inference pipeline\nfor computer vision problems that use hierarchical DNNs. The method balances\nloads across the collaborating devices and reduces communication costs to\nfacilitate the processing of multiple video frames simultaneously with higher\nthroughput. Our experiments consider a representative computer vision problem\nwhere image recognition is performed on each video frame, running on multiple\nRaspberry Pi 4Bs. With four collaborating low-power edge devices, our approach\nachieves 3.21X higher throughput, 68% less energy consumption per device per\nframe, and 58% decrease in memory when compared with existing single-device\nhierarchical DNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Abhinav Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_C/0/1/0/all/0/1\">Caleb Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1\">George K. Thiruvathukal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">James C. Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yung-Hsiang Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WarpedGANSpace: Finding non-linear RBF paths in GAN latent space. (arXiv:2109.13357v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13357","description":"<p>This work addresses the problem of discovering, in an unsupervised manner,\ninterpretable paths in the latent space of pretrained GANs, so as to provide an\nintuitive and easy way of controlling the underlying generative factors. In\ndoing so, it addresses some of the limitations of the state-of-the-art works,\nnamely, a) that they discover directions that are independent of the latent\ncode, i.e., paths that are linear, and b) that their evaluation relies either\non visual inspection or on laborious human labeling. More specifically, we\npropose to learn non-linear warpings on the latent space, each one parametrized\nby a set of RBF-based latent space warping functions, and where each warping\ngives rise to a family of non-linear paths via the gradient of the function.\nBuilding on the work of Voynov and Babenko, that discovers linear paths, we\noptimize the trainable parameters of the set of RBFs, so as that images that\nare generated by codes along different paths, are easily distinguishable by a\ndiscriminator network. This leads to easily distinguishable image\ntransformations, such as pose and facial expressions in facial images. We show\nthat linear paths can be derived as a special case of our method, and show\nexperimentally that non-linear paths in the latent space lead to steeper, more\ndisentangled and interpretable changes in the image space than in state-of-the\nart methods, both qualitatively and quantitatively. We make the code and the\npretrained models publicly available at:\nhttps://github.com/chi0tzp/WarpedGANSpace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1\">Christos Tzelepis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1\">Georgios Tzimiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IGAN: Inferent and Generative Adversarial Networks. (arXiv:2109.13360v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13360","description":"<p>I present IGAN (Inferent Generative Adversarial Networks), a neural\narchitecture that learns both a generative and an inference model on a complex\nhigh dimensional data distribution, i.e. a bidirectional mapping between data\nsamples and a simpler low-dimensional latent space. It extends the traditional\nGAN framework with inference by rewriting the adversarial strategy in both the\nimage and the latent space with an entangled game between data-latent encoded\nposteriors and priors. It brings a measurable stability and convergence to the\nclassical GAN scheme, while keeping its generative quality and remaining simple\nand frugal in order to run on a lab PC. IGAN fosters the encoded latents to\nspan the full prior space: this enables the exploitation of an enlarged and\nself-organised latent space in an unsupervised manner. An analysis of\npreviously published articles sets the theoretical ground for the proposed\nalgorithm. A qualitative demonstration of potential applications like\nself-supervision or multi-modal data translation is given on common image\ndatasets including SAR and optical imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vignaud_D/0/1/0/all/0/1\">Dr. Luc Vignaud</a> (ONERA, The French Aerospace Lab, France)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D. (arXiv:2109.13410v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13410","description":"<p>For the last few decades, several major subfields of artificial intelligence\nincluding computer vision, graphics, and robotics have progressed largely\nindependently from each other. Recently, however, the community has realized\nthat progress towards robust intelligent systems such as self-driving cars\nrequires a concerted effort across the different fields. This motivated us to\ndevelop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a\nsuburban driving dataset which comprises richer input modalities, comprehensive\nsemantic instance annotations and accurate localization to facilitate research\nat the intersection of vision, graphics and robotics. For efficient annotation,\nwe created a tool to label 3D scenes with bounding primitives and developed a\nmodel that transfers this information into the 2D image domain, resulting in\nover 150k semantic and instance annotated images and 1B annotated 3D points.\nMoreover, we established benchmarks and baselines for several tasks relevant to\nmobile perception, encompassing problems from computer vision, graphics, and\nrobotics on the same dataset. KITTI-360 will enable progress at the\nintersection of these research areas and thus contributing towards solving one\nof our grand challenges: the development of fully autonomous self-driving\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yiyi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative Attribution from Counterfactuals. (arXiv:2109.13412v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13412","description":"<p>We present a method for neural network interpretability by combining feature\nattribution with counterfactual explanations to generate attribution maps that\nhighlight the most discriminative features between pairs of classes. We show\nthat this method can be used to quantitatively evaluate the performance of\nfeature attribution methods in an objective manner, thus preventing potential\nobserver bias. We evaluate the proposed method on three diverse datasets,\nincluding a challenging artificial dataset and real-world biological data. We\nshow quantitatively and qualitatively that the highlighted features are\nsubstantially more discriminative than those extracted using conventional\nattribution methods and argue that this type of explanation is better suited\nfor understanding fine grained class differences as learned by a deep neural\nnetwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_N/0/1/0/all/0/1\">Nils Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_A/0/1/0/all/0/1\">Alexander S. Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jefferis_G/0/1/0/all/0/1\">Gregory S.X.E. Jefferis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funke_J/0/1/0/all/0/1\">Jan Funke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Deep Neural Network Domain Adaptation Techniques for Image Recognition. (arXiv:2109.13420v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13420","description":"<p>It has been well proved that deep networks are efficient at extracting\nfeatures from a given (source) labeled dataset. However, it is not always the\ncase that they can generalize well to other (target) datasets which very often\nhave a different underlying distribution. In this report, we evaluate four\ndifferent domain adaptation techniques for image classification tasks:\nDeepCORAL, DeepDomainConfusion, CDAN and CDAN+E. These techniques are\nunsupervised given that the target dataset dopes not carry any labels during\ntraining phase. We evaluate model performance on the office-31 dataset. A link\nto the github repository of this report can be found here:\nhttps://github.com/agrija9/Deep-Unsupervised-Domain-Adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Preciado_Grijalva_A/0/1/0/all/0/1\">Alan Preciado-Grijalva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthireddy_V/0/1/0/all/0/1\">Venkata Santosh Sai Ramireddy Muthireddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Keypoint Discovery. (arXiv:2109.13423v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13423","description":"<p>In this paper, we propose a method for keypoint discovery from a 2D image\nusing image-level supervision. Recent works on unsupervised keypoint discovery\nreliably discover keypoints of aligned instances. However, when the target\ninstances have high viewpoint or appearance variation, the discovered keypoints\ndo not match the semantic correspondences over different images. Our work aims\nto discover keypoints even when the target instances have high viewpoint and\nappearance variation by using image-level supervision. Motivated by the\nweakly-supervised learning approach, our method exploits image-level\nsupervision to identify discriminative parts and infer the viewpoint of the\ntarget instance. To discover diverse parts, we adopt a conditional image\ngeneration approach using a pair of images with structural deformation.\nFinally, we enforce a viewpoint-based equivariance constraint using the\nkeypoints from the image-level supervision to resolve the spatial correlation\nproblem that consistently appears in the images taken from various viewpoints.\nOur approach achieves state-of-the-art performance for the task of keypoint\nestimation on the limited supervision scenarios. Furthermore, the discovered\nkeypoints are directly applicable to downstream tasks without requiring any\nkeypoint labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ryou_S/0/1/0/all/0/1\">Serim Ryou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Warp-Refine Propagation: Semi-Supervised Auto-labeling via Cycle-consistency. (arXiv:2109.13432v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13432","description":"<p>Deep learning models for semantic segmentation rely on expensive,\nlarge-scale, manually annotated datasets. Labelling is a tedious process that\ncan take hours per image. Automatically annotating video sequences by\npropagating sparsely labeled frames through time is a more scalable\nalternative. In this work, we propose a novel label propagation method, termed\nWarp-Refine Propagation, that combines semantic cues with geometric cues to\nefficiently auto-label videos. Our method learns to refine geometrically-warped\nlabels and infuse them with learned semantic priors in a semi-supervised\nsetting by leveraging cycle consistency across time. We quantitatively show\nthat our method improves label-propagation by a noteworthy margin of 13.1 mIoU\non the ApolloScape dataset. Furthermore, by training with the auto-labelled\nframes, we achieve competitive results on three semantic-segmentation\nbenchmarks, improving the state-of-the-art by a large margin of 1.8 and 3.61\nmIoU on NYU-V2 and KITTI, while matching the current best results on\nCityscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganeshan_A/0/1/0/all/0/1\">Aditya Ganeshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallet_A/0/1/0/all/0/1\">Alexis Vallet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kudo_Y/0/1/0/all/0/1\">Yasunori Kudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maeda_S/0/1/0/all/0/1\">Shin-ichi Maeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerola_T/0/1/0/all/0/1\">Tommi Kerola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dennis Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Which Out-Of-Distribution Object Orientations Are DNNs Capable of Generalizing?. (arXiv:2109.13445v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13445","description":"<p>The capability of Deep Neural Networks (DNNs) to recognize objects in\norientations outside the distribution of the training data, ie.\nout-of-distribution (OoD) orientations, is not well understood. For humans,\nbehavioral studies showed that recognition accuracy varies across OoD\norientations, where generalization is much better for some orientations than\nfor others. In contrast, for DNNs, it remains unknown how generalization\nabilities are distributed among OoD orientations. In this paper, we investigate\nthe limitations of DNNs' generalization capacities by systematically inspecting\npatterns of success and failure of DNNs across OoD orientations. We use an\nintuitive and controlled, yet challenging learning paradigm, in which some\ninstances of an object category are seen at only a few geometrically restricted\norientations, while other instances are seen at all orientations. The effect of\ndata diversity is also investigated by increasing the number of instances seen\nat all orientations in the training set. We present a comprehensive analysis of\nDNNs' generalization abilities and limitations for representative architectures\n(ResNet, Inception, DenseNet and CORnet). Our results reveal an intriguing\npattern -- DNNs are only capable of generalizing to instances of objects that\nappear like 2D, ie. in-plane, rotations of in-distribution orientations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1\">Avi Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harari_D/0/1/0/all/0/1\">Daniel Harari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_P/0/1/0/all/0/1\">Pawan Sinha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When in Doubt: Improving Classification Performance with Alternating Normalization. (arXiv:2109.13449v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13449","description":"<p>We introduce Classification with Alternating Normalization (CAN), a\nnon-parametric post-processing step for classification. CAN improves\nclassification accuracy for challenging examples by re-adjusting their\npredicted class probability distribution using the predicted class\ndistributions of high-confidence validation examples. CAN is easily applicable\nto any probabilistic classifier, with minimal computation overhead. We analyze\nthe properties of CAN using simulated experiments, and empirically demonstrate\nits effectiveness across a diverse set of classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Menglin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_A/0/1/0/all/0/1\">Austin Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SiamEvent: Event-based Object Tracking via Edge-aware Similarity Learning with Siamese Networks. (arXiv:2109.13456v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13456","description":"<p>Event cameras are novel sensors that perceive the per-pixel intensity changes\nand output asynchronous event streams, showing lots of advantages over\ntraditional cameras, such as high dynamic range (HDR) and no motion blur. It\nhas been shown that events alone can be used for object tracking by motion\ncompensation or prediction. However, existing methods assume that the target\nalways moves and is the stand-alone object. Moreover, they fail to track the\nstopped non-independent moving objects on fixed scenes. In this paper, we\npropose a novel event-based object tracking framework, called SiamEvent, using\nSiamese networks via edge-aware similarity learning. Importantly, to find the\npart having the most similar edge structure of target, we propose to correlate\nthe embedded events at two timestamps to compute the target edge similarity.\nThe Siamese network enables tracking arbitrary target edge by finding the part\nwith the highest similarity score. This extends the possibility of event-based\nobject tracking applied not only for the independent stand-alone moving\nobjects, but also for various settings of the camera and scenes. In addition,\ntarget edge initialization and edge detector are also proposed to prevent\nSiamEvent from the drifting problem. Lastly, we built an open dataset including\nvarious synthetic and real scenes to train and evaluate SiamEvent. Extensive\nexperiments demonstrate that SiamEvent achieves up to 15% tracking performance\nenhancement than the baselines on the real-world scenes and more robust\ntracking performance in the challenging HDR and motion blur conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chae_Y/0/1/0/all/0/1\">Yujeong Chae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delve into the Performance Degradation of Differentiable Architecture Search. (arXiv:2109.13466v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13466","description":"<p>Differentiable architecture search (DARTS) is widely considered to be easy to\noverfit the validation set which leads to performance degradation. We first\nemploy a series of exploratory experiments to verify that neither high-strength\narchitecture parameters regularization nor warmup training scheme can\neffectively solve this problem. Based on the insights from the experiments, we\nconjecture that the performance of DARTS does not depend on the well-trained\nsupernet weights and argue that the architecture parameters should be trained\nby the gradients which are obtained in the early stage rather than the final\nstage of training. This argument is then verified by exchanging the learning\nrate schemes of weights and parameters. Experimental results show that the\nsimple swap of the learning rates can effectively solve the degradation and\nachieve competitive performance. Further empirical evidence suggests that the\ndegradation is not a simple problem of the validation set overfitting but\nexhibit some links between the degradation and the operation selection bias\nwithin bilevel optimization dynamics. We demonstrate the generalization of this\nbias and propose to utilize this bias to achieve an operation-magnitude-based\nselective stop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiuling Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhiming Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metal Artifact Reduction in 2D CT Images with Self-supervised Cross-domain Learning. (arXiv:2109.13483v1 [eess.IV])","link":"http://arxiv.org/abs/2109.13483","description":"<p>The presence of metallic implants often introduces severe metal artifacts in\nthe X-ray CT images, which could adversely influence clinical diagnosis or dose\ncalculation in radiation therapy. In this work, we present a novel\ndeep-learning-based approach for metal artifact reduction (MAR). In order to\nalleviate the need for anatomically identical CT image pairs (i.e., metal\nartifact-corrupted CT image and metal artifact-free CT image) for network\nlearning, we propose a self-supervised cross-domain learning framework.\nSpecifically, we train a neural network to restore the metal trace region\nvalues in the given metal-free sinogram, where the metal trace is identified by\nthe forward projection of metal masks. We then design a novel FBP\nreconstruction loss to encourage the network to generate more perfect\ncompletion results and a residual-learning-based image refinement module to\nreduce the secondary artifacts in the reconstructed CT images. To preserve the\nfine structure details and fidelity of the final MAR image, instead of directly\nadopting CNN-refined images as output, we incorporate the metal trace\nreplacement into our framework and replace the metal-affected projections of\nthe original sinogram with the prior sinogram generated by the forward\nprojection of the CNN output. We then use the filtered backward projection\n(FBP) algorithms for final MAR image reconstruction. We conduct an extensive\nevaluation on simulated and real artifact data to show the effectiveness of our\ndesign. Our method produces superior MAR results and outperforms other\ncompelling methods. We also demonstrate the potential of our framework for\nother organ sites.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhicheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_H/0/1/0/all/0/1\">Hongyi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Rotation Invariance in Object Detection. (arXiv:2109.13488v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13488","description":"<p>Rotation augmentations generally improve a model's invariance/equivariance to\nrotation - except in object detection. In object detection the shape is not\nknown, therefore rotation creates a label ambiguity. We show that the de-facto\nmethod for bounding box label rotation, the Largest Box Method, creates very\nlarge labels, leading to poor performance and in many cases worse performance\nthan using no rotation at all. We propose a new method of rotation augmentation\nthat can be implemented in a few lines of code. First, we create a\ndifferentiable approximation of label accuracy and show that axis-aligning the\nbounding box around an ellipse is optimal. We then introduce Rotation\nUncertainty (RU) Loss, allowing the model to adapt to the uncertainty of the\nlabels. On five different datasets (including COCO, PascalVOC, and Transparent\nObject Bin Picking), this approach improves the rotational invariance of both\none-stage and two-stage architectures when measured with AP, AP50, and AP75.\nThe code is available at \\url{https://github.com/akasha-imaging/ICCV2021}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalra_A/0/1/0/all/0/1\">Agastya Kalra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoppi_G/0/1/0/all/0/1\">Guy Stoppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_B/0/1/0/all/0/1\">Bradley Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Rishav Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadambi_A/0/1/0/all/0/1\">Achuta Kadambi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modelling Neighbor Relation in Joint Space-Time Graph for Video Correspondence Learning. (arXiv:2109.13499v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13499","description":"<p>This paper presents a self-supervised method for learning reliable visual\ncorrespondence from unlabeled videos. We formulate the correspondence as\nfinding paths in a joint space-time graph, where nodes are grid patches sampled\nfrom frames, and are linked by two types of edges: (i) neighbor relations that\ndetermine the aggregation strength from intra-frame neighbors in space, and\n(ii) similarity relations that indicate the transition probability of\ninter-frame paths across time. Leveraging the cycle-consistency in videos, our\ncontrastive learning objective discriminates dynamic objects from both their\nneighboring views and temporal views. Compared with prior works, our approach\nactively explores the neighbor relations of central instances to learn a latent\nassociation between center-neighbor pairs (e.g., \"hand -- arm\") across time,\nthus improving the instance discrimination. Without fine-tuning, our learned\nrepresentation outperforms the state-of-the-art self-supervised methods on a\nvariety of visual tasks including video object propagation, part propagation,\nand pose keypoint tracking. Our self-supervised method also surpasses some\nfully supervised algorithms designed for the specific tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zixu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Shapelet Transform: A new approach for time series shapelets. (arXiv:2109.13514v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13514","description":"<p>Shapelet-based algorithms are widely used for time series classification\nbecause of their ease of interpretation, but they are currently outperformed,\nnotably by methods using convolutional kernels, capable of reaching\nstate-of-the-art performance while being highly scalable. We present a new\nformulation of time series shapelets including the notion of dilation, and a\nshapelet extraction method based on convolutional kernels, which is able to\ntarget the discriminant information identified by convolutional kernels.\nExperiments performed on 108 datasets show that our method improves on the\nstate-of-the-art for shapelet algorithms, and we show that it can be used to\ninterpret results from convolutional kernels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guillaume_A/0/1/0/all/0/1\">Antoine Guillaume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vrain_C/0/1/0/all/0/1\">Christel Vrain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wael_E/0/1/0/all/0/1\">Elloumi Wael</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Semantic Image Recognition Model and Evaluating Index for explaining the deep learning models. (arXiv:2109.13531v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13531","description":"<p>Although deep learning models are powerful among various applications, most\ndeep learning models are still a black box, lacking verifiability and\ninterpretability, which means the decision-making process that human beings\ncannot understand. Therefore, how to evaluate deep neural networks with\nexplanations is still an urgent task. In this paper, we first propose a\nmulti-semantic image recognition model, which enables human beings to\nunderstand the decision-making process of the neural network. Then, we presents\na new evaluation index, which can quantitatively assess the model\ninterpretability. We also comprehensively summarize the semantic information\nthat affects the image classification results in the judgment process of neural\nnetworks. Finally, this paper also exhibits the relevant baseline performance\nwith current state-of-the-art deep learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qianmengke Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A hierarchical residual network with compact triplet-center loss for sketch recognition. (arXiv:2109.13536v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13536","description":"<p>With the widespread use of touch-screen devices, it is more and more\nconvenient for people to draw sketches on screen. This results in the demand\nfor automatically understanding the sketches. Thus, the sketch recognition task\nbecomes more significant than before. To accomplish this task, it is necessary\nto solve the critical issue of improving the distinction of the sketch\nfeatures. To this end, we have made efforts in three aspects. First, a novel\nmulti-scale residual block is designed. Compared with the conventional basic\nresidual block, it can better perceive multi-scale information and reduce the\nnumber of parameters during training. Second, a hierarchical residual structure\nis built by stacking multi-scale residual blocks in a specific way. In contrast\nwith the single-level residual structure, the learned features from this\nstructure are more sufficient. Last but not least, the compact triplet-center\nloss is proposed specifically for the sketch recognition task. It can solve the\nproblem that the triplet-center loss does not fully consider too large\nintra-class space and too small inter-class space in sketch field. By studying\nthe above modules, a hierarchical residual network as a whole is proposed for\nsketch recognition and evaluated on Tu-Berlin benchmark thoroughly. The\nexperimental results show that the proposed network outperforms most of\nbaseline methods and it is excellent among non-sequential models at present.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_Y/0/1/0/all/0/1\">Yu Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Strong Baseline for the VIPriors Data-Efficient Image Classification Challenge. (arXiv:2109.13561v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13561","description":"<p>Learning from limited amounts of data is the hallmark of intelligence,\nrequiring strong generalization and abstraction skills. In a machine learning\ncontext, data-efficient methods are of high practical importance since data\ncollection and annotation are prohibitively expensive in many domains. Thus,\ncoordinated efforts to foster progress in this area emerged recently, e.g., in\nthe form of dedicated workshops and competitions. Besides a common benchmark,\nmeasuring progress requires strong baselines. We present such a strong baseline\nfor data-efficient image classification on the VIPriors challenge dataset,\nwhich is a sub-sampled version of ImageNet-1k with 100 images per class. We do\nnot use any methods tailored to data-efficient classification but only standard\nmodels and techniques as well as common competition tricks and thorough\nhyper-parameter tuning. Our baseline achieves 69.7% accuracy on the VIPriors\nimage classification dataset and outperforms 50% of submissions to the VIPriors\n2021 challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barz_B/0/1/0/all/0/1\">Bj&#xf6;rn Barz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brigato_L/0/1/0/all/0/1\">Lorenzo Brigato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iocchi_L/0/1/0/all/0/1\">Luca Iocchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Elevation Network for Fast Online Action Detection. (arXiv:2109.13572v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13572","description":"<p>Online action detection (OAD) is a task that receives video segments within a\nstreaming video as inputs and identifies ongoing actions within them. It is\nimportant to retain past information associated with a current action. However,\nlong short-term memory (LSTM), a popular recurrent unit for modeling temporal\ninformation from videos, accumulates past information from the previous hidden\nand cell states and the extracted visual features at each timestep without\nconsidering the relationships between the past and current information.\nConsequently, the forget gate of the original LSTM can lose the accumulated\ninformation relevant to the current action because it determines which\ninformation to forget without considering the current action. We introduce a\nnovel information elevation unit (IEU) that lifts up and accumulate the past\ninformation relevant to the current action in order to model the past\ninformation that is especially relevant to the current action. To the best of\nour knowledge, our IEN is the first attempt that considers the computational\noverhead for the practical use of OAD. Through ablation studies, we design an\nefficient and effective OAD network using IEUs, called an information elevation\nnetwork (IEN). Our IEN uses visual features extracted by a fast action\nrecognition network taking only RGB frames because extracting optical flows\nrequires heavy computation overhead. On two OAD benchmark datasets, THUMOS-14\nand TVSeries, our IEN outperforms state-of-the-art OAD methods using only RGB\nframes. Furthermore, on the THUMOS-14 dataset, our IEN outperforms the\nstate-of-the-art OAD methods using two-stream features based on RGB frames and\noptical flows.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sunah Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jinyoung Moon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Curiosity Explicit in Vision-based RL. (arXiv:2109.13588v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13588","description":"<p>Vision-based reinforcement learning (RL) is a promising technique to solve\ncontrol tasks involving images as the main observation. State-of-the-art RL\nalgorithms still struggle in terms of sample efficiency, especially when using\nimage observations. This has led to an increased attention on integrating state\nrepresentation learning (SRL) techniques into the RL pipeline. Work in this\nfield demonstrates a substantial improvement in sample efficiency among other\nbenefits. However, to take full advantage of this paradigm, the quality of\nsamples used for training plays a crucial role. More importantly, the diversity\nof these samples could affect the sample efficiency of vision-based RL, but\nalso its generalization capability. In this work, we present an approach to\nimprove the sample diversity. Our method enhances the exploration capability of\nthe RL algorithms by taking advantage of the SRL setup. Our experiments show\nthat the presented approach outperforms the baseline for all tested\nenvironments. These results are most apparent for environments where the\nbaseline method struggles. Even in simple environments, our method stabilizes\nthe training, reduces the reward variance and boosts sample efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aljalbout_E/0/1/0/all/0/1\">Elie Aljalbout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulmer_M/0/1/0/all/0/1\">Maximilian Ulmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1\">Rudolph Triebel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Global-Local Memory for Real-time Instrument Segmentation of Robotic Surgical Video. (arXiv:2109.13593v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13593","description":"<p>Performing a real-time and accurate instrument segmentation from videos is of\ngreat significance for improving the performance of robotic-assisted surgery.\nWe identify two important clues for surgical instrument perception, including\nlocal temporal dependency from adjacent frames and global semantic correlation\nin long-range duration. However, most existing works perform segmentation\npurely using visual cues in a single frame. Optical flow is just used to model\nthe motion between only two frames and brings heavy computational cost. We\npropose a novel dual-memory network (DMNet) to wisely relate both global and\nlocal spatio-temporal knowledge to augment the current features, boosting the\nsegmentation performance and retaining the real-time prediction capability. We\npropose, on the one hand, an efficient local memory by taking the complementary\nadvantages of convolutional LSTM and non-local mechanisms towards the relating\nreception field. On the other hand, we develop an active global memory to\ngather the global semantic correlation in long temporal range to current one,\nin which we gather the most informative frames derived from model uncertainty\nand frame similarity. We have extensively validated our method on two public\nbenchmark surgical video datasets. Experimental results demonstrate that our\nmethod largely outperforms the state-of-the-art works on segmentation accuracy\nwhile maintaining a real-time speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiacheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shuntian Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SafetyNet: Safe planning for real-world self-driving vehicles using machine-learned policies. (arXiv:2109.13602v1 [cs.RO])","link":"http://arxiv.org/abs/2109.13602","description":"<p>In this paper we present the first safe system for full control of\nself-driving vehicles trained from human demonstrations and deployed in\nchallenging, real-world, urban environments. Current industry-standard\nsolutions use rule-based systems for planning. Although they perform reasonably\nwell in common scenarios, the engineering complexity renders this approach\nincompatible with human-level performance. On the other hand, the performance\nof machine-learned (ML) planning solutions can be improved by simply adding\nmore exemplar data. However, ML methods cannot offer safety guarantees and\nsometimes behave unpredictably. To combat this, our approach uses a simple yet\neffective rule-based fallback layer that performs sanity checks on an ML\nplanner's decisions (e.g. avoiding collision, assuring physical feasibility).\nThis allows us to leverage ML to handle complex situations while still assuring\nthe safety, reducing ML planner-only collisions by 95%. We train our ML planner\non 300 hours of expert driving demonstrations using imitation learning and\ndeploy it along with the fallback layer in downtown San Francisco, where it\ntakes complete control of a real vehicle and navigates a wide variety of\nchallenging urban driving scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vitelli_M/0/1/0/all/0/1\">Matt Vitelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yawei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolczyk_M/0/1/0/all/0/1\">Maciej Wo&#x142;czyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1\">B&#x142;a&#x17c;ej Osi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niendorf_M/0/1/0/all/0/1\">Moritz Niendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiangui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ashesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Glaucoma Detection from Digital Fundus Images using Self-ONNs. (arXiv:2109.13604v1 [eess.IV])","link":"http://arxiv.org/abs/2109.13604","description":"<p>Glaucoma leads to permanent vision disability by damaging the optical nerve\nthat transmits visual images to the brain. The fact that glaucoma does not show\nany symptoms as it progresses and cannot be stopped at the later stages, makes\nit critical to be diagnosed in its early stages. Although various deep learning\nmodels have been applied for detecting glaucoma from digital fundus images, due\nto the scarcity of labeled data, their generalization performance was limited\nalong with high computational complexity and special hardware requirements. In\nthis study, compact Self-Organized Operational Neural Networks (Self- ONNs) are\nproposed for early detection of glaucoma in fundus images and their performance\nis compared against the conventional (deep) Convolutional Neural Networks\n(CNNs) over three benchmark datasets: ACRIMA, RIM-ONE, and ESOGU. The\nexperimental results demonstrate that Self-ONNs not only achieve superior\ndetection performance but can also significantly reduce the computational\ncomplexity making it a potentially suitable network model for biomedical\ndatasets especially when the data is scarce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Devecioglu_O/0/1/0/all/0/1\">Ozer Can Devecioglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malik_J/0/1/0/all/0/1\">Junaid Malik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ince_T/0/1/0/all/0/1\">Turker Ince</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Atalay_E/0/1/0/all/0/1\">Eray Atalay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Network Design for Face Video Super-resolution. (arXiv:2109.13626v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13626","description":"<p>Face video super-resolution algorithm aims to reconstruct realistic face\ndetails through continuous input video sequences. However, existing video\nprocessing algorithms usually contain redundant parameters to guarantee\ndifferent super-resolution scenes. In this work, we focus on super-resolution\nof face areas in original video scenes, while rest areas are interpolated. This\nspecific super-resolved task makes it possible to cut redundant parameters in\ngeneral video super-resolution networks. We construct a dataset consisting\nentirely of face video sequences for network training and evaluation, and\nconduct hyper-parameter optimization in our experiments. We use three combined\nstrategies to optimize the network parameters with a simultaneous\ntrain-evaluation method to accelerate optimization process. Results show that\nsimultaneous train-evaluation method improves the training speed and\nfacilitates the generation of efficient networks. The generated network can\nreduce at least 52.4% parameters and 20.7% FLOPs, achieve better performance on\nPSNR, SSIM compared with state-of-art video super-resolution algorithms. When\nprocessing 36x36x1x3 input video frame sequences, the efficient network\nprovides 47.62 FPS real-time processing performance. We name our proposal as\nhyper-parameter optimization for face Video Super-Resolution (HO-FVSR), which\nis open-sourced at https://github.com/yphone/efficient-network-for-face-VSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Feng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">He Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1\">Sige Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yongming Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Diffeomorphic Surface Registration and Non-Linear Modelling. (arXiv:2109.13630v1 [eess.IV])","link":"http://arxiv.org/abs/2109.13630","description":"<p>Registration is an essential tool in image analysis. Deep learning based\nalternatives have recently become popular, achieving competitive performance at\na faster speed. However, many contemporary techniques are limited to volumetric\nrepresentations, despite increased popularity of 3D surface and shape data in\nmedical image analysis. We propose a one-step registration model for 3D\nsurfaces that internalises a lower dimensional probabilistic deformation model\n(PDM) using conditional variational autoencoders (CVAE). The deformations are\nconstrained to be diffeomorphic using an exponentiation layer. The one-step\nregistration model is benchmarked against iterative techniques, trading in a\nslightly lower performance in terms of shape fit for a higher compactness. We\nexperiment with two distance metrics, Chamfer distance (CD) and Sinkhorn\ndivergence (SD), as specific distance functions for surface data in real-world\nregistration scenarios. The internalised deformation model is benchmarked\nagainst linear principal component analysis (PCA) achieving competitive results\nand improved generalisability from lower dimensions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Croquet_B/0/1/0/all/0/1\">Balder Croquet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christiaens_D/0/1/0/all/0/1\">Daan Christiaens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weinberg_S/0/1/0/all/0/1\">Seth M. Weinberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bronstein_M/0/1/0/all/0/1\">Michael Bronstein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vandermeulen_D/0/1/0/all/0/1\">Dirk Vandermeulen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Claes_P/0/1/0/all/0/1\">Peter Claes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fail-Safe Human Detection for Drones Using a Multi-Modal Curriculum Learning Approach. (arXiv:2109.13666v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13666","description":"<p>Drones are currently being explored for safety-critical applications where\nhuman agents are expected to evolve in their vicinity. In such applications,\nrobust people avoidance must be provided by fusing a number of sensing\nmodalities in order to avoid collisions. Currently however, people detection\nsystems used on drones are solely based on standard cameras besides an emerging\nnumber of works discussing the fusion of imaging and event-based cameras. On\nthe other hand, radar-based systems provide up-most robustness towards\nenvironmental conditions but do not provide complete information on their own\nand have mainly been investigated in automotive contexts, not for drones. In\norder to enable the fusion of radars with both event-based and standard\ncameras, we present KUL-UAVSAFE, a first-of-its-kind dataset for the study of\nsafety-critical people detection by drones. In addition, we propose a baseline\nCNN architecture with cross-fusion highways and introduce a curriculum learning\nstrategy for multi-modal data termed SAUL, which greatly enhances the\nrobustness of the system towards hard RGB failures and provides a significant\ngain of 15% in peak F1 score compared to the use of BlackIn, previously\nproposed for cross-fusion networks. We demonstrate the real-time performance\nand feasibility of the approach by implementing the system in an edge-computing\nunit. We release our dataset and additional material in the project home page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safa_A/0/1/0/all/0/1\">Ali Safa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbelen_T/0/1/0/all/0/1\">Tim Verbelen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ocket_I/0/1/0/all/0/1\">Ilja Ocket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourdoux_A/0/1/0/all/0/1\">Andr&#xe9; Bourdoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catthoor_F/0/1/0/all/0/1\">Francky Catthoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gielen_G/0/1/0/all/0/1\">Georges G.E. Gielen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion Deblurring with Real Events. (arXiv:2109.13695v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13695","description":"<p>In this paper, we propose an end-to-end learning framework for event-based\nmotion deblurring in a self-supervised manner, where real-world events are\nexploited to alleviate the performance degradation caused by data\ninconsistency. To achieve this end, optical flows are predicted from events,\nwith which the blurry consistency and photometric consistency are exploited to\nenable self-supervision on the deblurring network with real-world data.\nFurthermore, a piece-wise linear motion model is proposed to take into account\nmotion non-linearities and thus leads to an accurate model for the physical\nformation of motion blurs in the real-world scenario. Extensive evaluation on\nboth synthetic and real motion blur datasets demonstrates that the proposed\nalgorithm bridges the gap between simulated and real-world motion blurs and\nshows remarkable performance for event-based motion deblurring in real-world\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bishan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhendong Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CIDEr-R: Robust Consensus-based Image Description Evaluation. (arXiv:2109.13701v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13701","description":"<p>This paper shows that CIDEr-D, a traditional evaluation metric for image\ndescription, does not work properly on datasets where the number of words in\nthe sentence is significantly greater than those in the MS COCO Captions\ndataset. We also show that CIDEr-D has performance hampered by the lack of\nmultiple reference sentences and high variance of sentence length. To bypass\nthis problem, we introduce CIDEr-R, which improves CIDEr-D, making it more\nflexible in dealing with datasets with high sentence length variance. We\ndemonstrate that CIDEr-R is more accurate and closer to human judgment than\nCIDEr-D; CIDEr-R is more robust regarding the number of available references.\nOur results reveal that using Self-Critical Sequence Training to optimize\nCIDEr-R generates descriptive captions. In contrast, when CIDEr-D is optimized,\nthe generated captions' length tends to be similar to the reference length.\nHowever, the models also repeat several times the same word to increase the\nsentence length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_G/0/1/0/all/0/1\">Gabriel Oliveira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombini_E/0/1/0/all/0/1\">Esther Luna Colombini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1\">Sandra Avila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compound eye inspired flat lensless imaging with spatially-coded Voronoi-Fresnel phase. (arXiv:2109.13703v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13703","description":"<p>Lensless cameras are a class of imaging devices that shrink the physical\ndimensions to the very close vicinity of the image sensor by integrating flat\noptics and computational algorithms. Here we report a flat lensless camera with\nspatially-coded Voronoi-Fresnel phase, partly inspired by biological apposition\ncompound eye, to achieve superior image quality. We propose a design principle\nof maximizing the information in optics to facilitate the computational\nreconstruction. By introducing a Fourier domain metric, Modulation Transfer\nFunction volume (MTFv), we devise an optimization framework to guide the\noptimal design of the optical element. The resulting Voronoi-Fresnel phase\nfeatures an irregular array of quasi-Centroidal Voronoi cells containing a base\nfirst-order Fresnel phase function. We demonstrate and verify the imaging\nperformance with a prototype Voronoi-Fresnel lensless camera on a 1.6-megapixel\nimage sensor in various illumination conditions. The proposed design could\nbenefit the development of compact imaging systems working in extreme physical\nconditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Dong-Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidrich_W/0/1/0/all/0/1\">Wolfgang Heidrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Attribute and Structure Subspace Clustering Network. (arXiv:2109.13742v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13742","description":"<p>Deep self-expressiveness-based subspace clustering methods have demonstrated\neffectiveness. However, existing works only consider the attribute information\nto conduct the self-expressiveness, which may limit the clustering performance.\nIn this paper, we propose a novel adaptive attribute and structure subspace\nclustering network (AASSC-Net) to simultaneously consider the attribute and\nstructure information in an adaptive graph fusion manner. Specifically, we\nfirst exploit an auto-encoder to represent input data samples with latent\nfeatures for the construction of an attribute matrix. We also construct a mixed\nsigned and symmetric structure matrix to capture the local geometric structure\nunderlying data samples. Then, we perform self-expressiveness on the\nconstructed attribute and structure matrices to learn their affinity graphs\nseparately. Finally, we design a novel attention-based fusion module to\nadaptively leverage these two affinity graphs to construct a more\ndiscriminative affinity graph. Extensive experimental results on commonly used\nbenchmark datasets demonstrate that our AASSC-Net significantly outperforms\nstate-of-the-art methods. In addition, we conduct comprehensive ablation\nstudies to discuss the effectiveness of the designed modules. The code will be\npublicly available at https://github.com/ZhihaoPENG-CityU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhihao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuheng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stable training of autoencoders for hyperspectral unmixing. (arXiv:2109.13748v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13748","description":"<p>Neural networks, autoencoders in particular, are one of the most promising\nsolutions for unmixing hyperspectral data, i.e. reconstructing the spectra of\nobserved substances (endmembers) and their relative mixing fractions\n(abundances). Unmixing is needed for effective hyperspectral analysis and\nclassification. However, as we show in this paper, the training of autoencoders\nfor unmixing is highly dependent on weights initialisation. Some sets of\nweights lead to degenerate or low performance solutions, introducing negative\nbias in expected performance. In this work we present the results of\nexperiments investigating autoencoders' stability, verifying the dependence of\nreconstruction error on initial weights and exploring conditions needed for\nsuccessful optimisation of autoencoder parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ksiazek_K/0/1/0/all/0/1\">Kamil Ksi&#x105;&#x17c;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glomb_P/0/1/0/all/0/1\">Przemys&#x142;aw G&#x142;omb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romaszewski_M/0/1/0/all/0/1\">Micha&#x142; Romaszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cholewa_M/0/1/0/all/0/1\">Micha&#x142; Cholewa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabowski_B/0/1/0/all/0/1\">Bartosz Grabowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StereoSpike: Depth Learning with a Spiking Neural Network. (arXiv:2109.13751v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13751","description":"<p>Depth estimation is an important computer vision task, useful in particular\nfor navigation in autonomous vehicles, or for object manipulation in robotics.\nHere we solved it using an end-to-end neuromorphic approach, combining two\nevent-based cameras and a Spiking Neural Network (SNN) with a slightly modified\nU-Net-like encoder-decoder architecture, that we named StereoSpike. More\nspecifically, we used the Multi Vehicle Stereo Event Camera Dataset (MVSEC). It\nprovides a depth ground-truth, which was used to train StereoSpike in a\nsupervised manner, using surrogate gradient descent. We propose a novel readout\nparadigm to obtain a dense analog prediction -- the depth of each pixel -- from\nthe spikes of the decoder. We demonstrate that this architecture generalizes\nvery well, even better than its non-spiking counterparts, leading to\nstate-of-the-art test accuracy. To the best of our knowledge, it is the first\ntime that such a large-scale regression problem is solved by a fully spiking\nnetwork. Finally, we show that low firing rates (&lt;10%) can be obtained via\nregularization, with a minimal cost in accuracy. This means that StereoSpike\ncould be efficiently implemented on neuromorphic chips, opening the door for\nlow power and real time embedded systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rancon_U/0/1/0/all/0/1\">Ulysse Ran&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuadrado_Anibarro_J/0/1/0/all/0/1\">Javier Cuadrado-Anibarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cottereau_B/0/1/0/all/0/1\">Benoit R. Cottereau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1\">Timoth&#xe9;e Masquelier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PFENet++: Boosting Few-shot Semantic Segmentation with the Noise-filtered Context-aware Prior Mask. (arXiv:2109.13788v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13788","description":"<p>In this work, we revisit the prior mask guidance proposed in \"Prior Guided\nFeature Enrichment Network for Few-Shot Segmentation\". The prior mask serves as\nan indicator that highlights the region of interests of unseen categories, and\nit is effective in achieving better performance on different frameworks of\nrecent studies.\n</p>\n<p>However, the current method directly takes the maximum element-to-element\ncorrespondence between the query and support features to indicate the\nprobability of belonging to the target class, thus the broader contextual\ninformation is seldom exploited during the prior mask generation. To address\nthis issue, first, we propose the Context-aware Prior Mask (CAPM) that\nleverages additional nearby semantic cues for better locating the objects in\nquery images. Second, since the maximum correlation value is vulnerable to\nnoisy features, we take one step further by incorporating a lightweight Noise\nSuppression Module (NSM) to screen out the unnecessary responses, yielding\nhigh-quality masks for providing the prior knowledge.\n</p>\n<p>Both two contributions are experimentally shown to have substantial practical\nmerit, and the new model named PFENet++ significantly outperforms the baseline\nPFENet as well as all other competitors on three challenging benchmarks\nPASCAL-5$^i$, COCO-20$^i$ and FSS-1000.\n</p>\n<p>The new state-of-the-art performance is achieved without compromising the\nefficiency, manifesting the potential for being a new strong baseline in\nfew-shot semantic segmentation.\n</p>\n<p>Our code will be available at https://github.com/dvlab-research/PFENet++.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaoliu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhuotao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Taiping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yuan Yan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The VVAD-LRS3 Dataset for Visual Voice Activity Detection. (arXiv:2109.13789v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13789","description":"<p>Robots are becoming everyday devices, increasing their interaction with\nhumans. To make human-machine interaction more natural, cognitive features like\nVisual Voice Activity Detection (VVAD), which can detect whether a person is\nspeaking or not, given visual input of a camera, need to be implemented. Neural\nnetworks are state of the art for tasks in Image Processing, Time Series\nPrediction, Natural Language Processing and other domains. Those Networks\nrequire large quantities of labeled data. Currently there are not many datasets\nfor the task of VVAD. In this work we created a large scale dataset called the\nVVAD-LRS3 dataset, derived by automatic annotations from the LRS3 dataset. The\nVVAD-LRS3 dataset contains over 44K samples, over three times the next\ncompetitive dataset (WildVVAD). We evaluate different baselines on four kinds\nof features: facial and lip images, and facial and lip landmark features. With\na Convolutional Neural Network Long Short Term Memory (CNN LSTM) on facial\nimages an accuracy of 92% was reached on the test set. A study with humans\nshowed that they reach an accuracy of 87.93% on the test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lubitz_A/0/1/0/all/0/1\">Adrian Lubitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1\">Matias Valdenegro-Toro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchner_F/0/1/0/all/0/1\">Frank Kirchner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not Color Blind: AI Predicts Racial Identity from Black and White Retinal Vessel Segmentations. (arXiv:2109.13845v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13845","description":"<p>Background: Artificial intelligence (AI) may demonstrate racial bias when\nskin or choroidal pigmentation is present in medical images. Recent studies\nhave shown that convolutional neural networks (CNNs) can predict race from\nimages that were not previously thought to contain race-specific features. We\nevaluate whether grayscale retinal vessel maps (RVMs) of patients screened for\nretinopathy of prematurity (ROP) contain race-specific features.\n</p>\n<p>Methods: 4095 retinal fundus images (RFIs) were collected from 245 Black and\nWhite infants. A U-Net generated RVMs from RFIs, which were subsequently\nthresholded, binarized, or skeletonized. To determine whether RVM differences\nbetween Black and White eyes were physiological, CNNs were trained to predict\nrace from color RFIs, raw RVMs, and thresholded, binarized, or skeletonized\nRVMs. Area under the precision-recall curve (AUC-PR) was evaluated.\n</p>\n<p>Findings: CNNs predicted race from RFIs near perfectly (image-level AUC-PR:\n0.999, subject-level AUC-PR: 1.000). Raw RVMs were almost as informative as\ncolor RFIs (image-level AUC-PR: 0.938, subject-level AUC-PR: 0.995).\nUltimately, CNNs were able to detect whether RFIs or RVMs were from Black or\nWhite babies, regardless of whether images contained color, vessel segmentation\nbrightness differences were nullified, or vessel segmentation widths were\nnormalized.\n</p>\n<p>Interpretation: AI can detect race from grayscale RVMs that were not thought\nto contain racial information. Two potential explanations for these findings\nare that: retinal vessels physiologically differ between Black and White babies\nor the U-Net segments the retinal vasculature differently for various fundus\npigmentations. Either way, the implications remain the same: AI algorithms have\npotential to demonstrate racial bias in practice, even when preliminary\nattempts to remove such information from the underlying images appear to be\nsuccessful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coyner_A/0/1/0/all/0/1\">Aaron S. Coyner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Praveer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1\">James M. Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostmo_S/0/1/0/all/0/1\">Susan Ostmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">R.V. Paul Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1\">Michael F. Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_J/0/1/0/all/0/1\">J. Peter Campbell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViT Cane: Visual Assistant for the Visually Impaired. (arXiv:2109.13857v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13857","description":"<p>Blind and visually challenged face multiple issues with navigating the world\nindependently. Some of these challenges include finding the shortest path to a\ndestination and detecting obstacles from a distance. To tackle this issue, this\npaper proposes ViT Cane, which leverages a vision transformer model in order to\ndetect obstacles in real-time. Our entire system consists of a Pi Camera Module\nv2, Raspberry Pi 4B with 8GB Ram and 4 motors. Based on tactile input using the\n4 motors, the obstacle detection model is highly efficient in helping visually\nimpaired navigate unknown terrain and is designed to be easily reproduced. The\npaper discusses the utility of a Visual Transformer model in comparison to\nother CNN based models for this specific application. Through rigorous testing,\nthe proposed obstacle detection model has achieved higher performance on the\nCommon Object in Context (COCO) data set than its CNN counterpart.\nComprehensive field tests were conducted to verify the effectiveness of our\nsystem for holistic indoor understanding and obstacle avoidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1\">Bhavesh Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization for Vision-based Driving Trajectory Generation. (arXiv:2109.13858v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13858","description":"<p>One of the challenges in vision-based driving trajectory generation is\ndealing with out-of-distribution scenarios. In this paper, we propose a domain\ngeneralization method for vision-based driving trajectory generation for\nautonomous vehicles in urban environments, which can be seen as a solution to\nextend the Invariant Risk Minimization (IRM) method in complex problems. We\nleverage an adversarial learning approach to train a trajectory generator as\nthe decoder. Based on the pre-trained decoder, we infer the latent variables\ncorresponding to the trajectories, and pre-train the encoder by regressing the\ninferred latent variable. Finally, we fix the decoder but fine-tune the encoder\nwith the final trajectory loss. We compare our proposed method with the\nstate-of-the-art trajectory generation method and some recent domain\ngeneralization methods on both datasets and simulation, demonstrating that our\nmethod has better generalization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunkai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongkun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yuxiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zexi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_W/0/1/0/all/0/1\">Wei Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NudgeSeg: Zero-Shot Object Segmentation by Repeated Physical Interaction. (arXiv:2109.13859v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13859","description":"<p>Recent advances in object segmentation have demonstrated that deep neural\nnetworks excel at object segmentation for specific classes in color and depth\nimages. However, their performance is dictated by the number of classes and\nobjects used for training, thereby hindering generalization to never seen\nobjects or zero-shot samples. To exacerbate the problem further, object\nsegmentation using image frames rely on recognition and pattern matching cues.\nInstead, we utilize the 'active' nature of a robot and their ability to\n'interact' with the environment to induce additional geometric constraints for\nsegmenting zero-shot samples.\n</p>\n<p>In this paper, we present the first framework to segment unknown objects in a\ncluttered scene by repeatedly 'nudging' at the objects and moving them to\nobtain additional motion cues at every step using only a monochrome monocular\ncamera. We call our framework NudgeSeg. These motion cues are used to refine\nthe segmentation masks. We successfully test our approach to segment novel\nobjects in various cluttered scenes and provide an extensive study with image\nand motion segmentation methods. We show an impressive average detection rate\nof over 86% on zero-shot objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1\">Chahat Deep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanket_N/0/1/0/all/0/1\">Nitin J. Sanket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parameshwara_C/0/1/0/all/0/1\">Chethan M. Parameshwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fermuller_C/0/1/0/all/0/1\">Cornelia Ferm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloimonos_Y/0/1/0/all/0/1\">Yiannis Aloimonos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introduce the Result Into Self-Attention. (arXiv:2109.13860v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13860","description":"<p>Traditional self-attention mechanisms in convolutional networks tend to use\nonly the output of the previous layer as input to the attention network, such\nas SENet, CBAM, etc. In this paper, we propose a new attention modification\nmethod that tries to get the output of the classification network in advance\nand use it as a part of the input of the attention network. We used the\nauxiliary classifier proposed in GoogLeNet to obtain the results in advance and\npass them into attention networks. we added this mechanism to SE-ResNet for our\nexperiments and achieved a classification accuracy improvement of at most 1.94%\non cifar100.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chengcheng Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual resemblance and communicative context constrain the emergence of graphical conventions. (arXiv:2109.13861v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13861","description":"<p>From photorealistic sketches to schematic diagrams, drawing provides a\nversatile medium for communicating about the visual world. How do images\nspanning such a broad range of appearances reliably convey meaning? Do viewers\nunderstand drawings based solely on their ability to resemble the entities they\nrefer to (i.e., as images), or do they understand drawings based on shared but\narbitrary associations with these entities (i.e., as symbols)? In this paper,\nwe provide evidence for a cognitive account of pictorial meaning in which both\nvisual and social information is integrated to support effective visual\ncommunication. To evaluate this account, we used a communication task where\npairs of participants used drawings to repeatedly communicate the identity of a\ntarget object among multiple distractor objects. We manipulated social cues\nacross three experiments and a full internal replication, finding pairs of\nparticipants develop referent-specific and interaction-specific strategies for\ncommunicating more efficiently over time, going beyond what could be explained\nby either task practice or a pure resemblance-based account alone. Using a\ncombination of model-based image analyses and crowdsourced sketch annotations,\nwe further determined that drawings did not drift toward arbitrariness, as\npredicted by a pure convention-based account, but systematically preserved\nthose visual features that were most distinctive of the target object. Taken\ntogether, these findings advance theories of pictorial meaning and have\nimplications for how successful graphical conventions emerge via complex\ninteractions between visual perception, communicative experience, and social\ncontext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sano_M/0/1/0/all/0/1\">Megumi Sano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Judith E. Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3N-GAN: Semi-Supervised Classification of X-Ray Images with a 3-Player Adversarial Framework. (arXiv:2109.13862v1 [eess.IV])","link":"http://arxiv.org/abs/2109.13862","description":"<p>The success of deep learning for medical imaging tasks, such as\nclassification, is heavily reliant on the availability of large-scale datasets.\nHowever, acquiring datasets with large quantities of labeled data is\nchallenging, as labeling is expensive and time-consuming. Semi-supervised\nlearning (SSL) is a growing alternative to fully-supervised learning, but\nrequires unlabeled samples for training. In medical imaging, many datasets lack\nunlabeled data entirely, so SSL can't be conventionally utilized. We propose\n3N-GAN, or 3 Network Generative Adversarial Networks, to perform\nsemi-supervised classification of medical images in fully-supervised settings.\nWe incorporate a classifier into the adversarial relationship such that the\ngenerator trains adversarially against both the classifier and discriminator.\nOur preliminary results show improved classification performance and GAN\ngenerations over various algorithms. Our work can seamlessly integrate with\nnumerous other medical imaging model architectures and SSL methods for greater\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Haque_S/0/1/0/all/0/1\">Shafin Haque</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Hand Pose and Shape Estimation from RGB Images for Improved Keypoint-Based Hand-Gesture Recognition. (arXiv:2109.13879v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13879","description":"<p>Estimating the 3D hand pose from a 2D image is a well-studied problem and a\nrequirement for several real-life applications such as virtual reality,\naugmented reality, and hand-gesture recognition. Currently, good estimations\ncan be computed starting from single RGB images, especially when forcing the\nsystem to also consider, through a multi-task learning approach, the hand shape\nwhen the pose is determined. However, when addressing the aforementioned\nreal-life tasks, performances can drop considerably depending on the hand\nrepresentation, thus suggesting that stable descriptions are required to\nachieve satisfactory results. As a consequence, in this paper we present a\nkeypoint-based end-to-end framework for the 3D hand and pose estimation, and\nsuccessfully apply it to the hand-gesture recognition task as a study case.\nSpecifically, after a pre-processing step where the images are normalized, the\nproposed pipeline comprises a multi-task semantic feature extractor generating\n2D heatmaps and hand silhouettes from RGB images; a viewpoint encoder\npredicting hand and camera view parameters; a stable hand estimator producing\nthe 3D hand pose and shape; and a loss function designed to jointly guide all\nof the components during the learning phase. To assess the proposed framework,\ntests were performed on a 3D pose and shape estimation benchmark dataset,\nobtaining state-of-the-art performances. What is more, the devised system was\nalso evaluated on 2 hand-gesture recognition benchmark datasets, where the\nframework significantly outperforms other keypoint-based approaches; indicating\nthat the presented method is an effective solution able to generate stable 3D\nestimates for the hand pose and shape.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avola_D/0/1/0/all/0/1\">Danilo Avola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinque_L/0/1/0/all/0/1\">Luigi Cinque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fagioli_A/0/1/0/all/0/1\">Alessio Fagioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foresti_G/0/1/0/all/0/1\">Gian Luca Foresti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragomeni_A/0/1/0/all/0/1\">Adriano Fragomeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pannone_D/0/1/0/all/0/1\">Daniele Pannone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turning old models fashion again: Recycling classical CNN networks using the Lattice Transformation. (arXiv:2109.13885v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13885","description":"<p>In the early 1990s, the first signs of life of the CNN era were given: LeCun\net al. proposed a CNN model trained by the backpropagation algorithm to\nclassify low-resolution images of handwritten digits. Undoubtedly, it was a\nbreakthrough in the field of computer vision. But with the rise of other\nclassification methods, it fell out fashion. That was until 2012, when\nKrizhevsky et al. revived the interest in CNNs by exhibiting considerably\nhigher image classification accuracy on the ImageNet challenge. Since then, the\ncomplexity of the architectures are exponentially increasing and many\nstructures are rapidly becoming obsolete. Using multistream networks as a base\nand the feature infusion precept, we explore the proposed LCNN cross-fusion\nstrategy to use the backbones of former state-of-the-art networks on image\nclassification in order to discover if the technique is able to put these\ndesigns back in the game. In this paper, we showed that we can obtain an\nincrease of accuracy up to 63.21% on the NORB dataset we comparing with the\noriginal structure. However, no technique is definitive. While our goal is to\ntry to reuse previous state-of-the-art architectures with few modifications, we\nalso expose the disadvantages of our explored strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almeida_A/0/1/0/all/0/1\">Ana Paula G. S. de Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_F/0/1/0/all/0/1\">Flavio de Barros Vidal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image scaling by de la Vall\\'ee-Poussin filtered interpolation. (arXiv:2109.13897v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13897","description":"<p>We present a new image scaling method both for downscaling and upscaling,\nrunning with any scale factor or desired size. It is based on the sampling of\nan approximating bivariate polynomial, which globally interpolates the data and\nis defined by a filter of de la Vall\\'ee Poussin type whose action ray is\nsuitable regulated to improve the approximation. The method has been tested on\na significant number of different image datasets. The results are evaluated in\nqualitative and quantitative terms and compared with other available\ncompetitive methods. The perceived quality of the resulting scaled images is\nsuch that important details are preserved, and the appearance of artifacts is\nlow. Very high-quality measure values in downscaling and the competitive ones\nin upscaling evidence the effectiveness of the method. Good visual quality,\nlimited computational effort, and moderate memory demanding make the method\nsuitable for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Occorsio_D/0/1/0/all/0/1\">Donatella Occorsio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramella_G/0/1/0/all/0/1\">Giuliana Ramella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Themistoclakis_W/0/1/0/all/0/1\">Woula Themistoclakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Contrastive Learning Approach to Auroral Identification and Classification. (arXiv:2109.13899v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13899","description":"<p>Unsupervised learning algorithms are beginning to achieve accuracies\ncomparable to their supervised counterparts on benchmark computer vision tasks,\nbut their utility for practical applications has not yet been demonstrated. In\nthis work, we present a novel application of unsupervised learning to the task\nof auroral image classification. Specifically, we modify and adapt the Simple\nframework for Contrastive Learning of Representations (SimCLR) algorithm to\nlearn representations of auroral images in a recently released auroral image\ndataset constructed using image data from Time History of Events and Macroscale\nInteractions during Substorms (THEMIS) all-sky imagers. We demonstrate that (a)\nsimple linear classifiers fit to the learned representations of the images\nachieve state-of-the-art classification performance, improving the\nclassification accuracy by almost 10 percentage points over the current\nbenchmark; and (b) the learned representations naturally cluster into more\nclusters than exist manually assigned categories, suggesting that existing\ncategorizations are overly coarse and may obscure important connections between\nauroral types, near-earth solar wind conditions, and geomagnetic disturbances\nat the earth's surface. Moreover, our model is much lighter than the previous\nbenchmark on this dataset, requiring in the area of fewer than 25\\% of the\nnumber of parameters. Our approach exceeds an established threshold for\noperational purposes, demonstrating readiness for deployment and utilization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Jeremiah W. Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hari_S/0/1/0/all/0/1\">Swathi Hari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hampton_D/0/1/0/all/0/1\">Donald Hampton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Connor_H/0/1/0/all/0/1\">Hyunju K. Connor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PDC-Net+: Enhanced Probabilistic Dense Correspondence Network. (arXiv:2109.13912v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13912","description":"<p>Establishing robust and accurate correspondences between a pair of images is\na long-standing computer vision problem with numerous applications. While\nclassically dominated by sparse methods, emerging dense approaches offer a\ncompelling alternative paradigm that avoids the keypoint detection step.\nHowever, dense flow estimation is often inaccurate in the case of large\ndisplacements, occlusions, or homogeneous regions. In order to apply dense\nmethods to real-world applications, such as pose estimation, image\nmanipulation, or 3D reconstruction, it is therefore crucial to estimate the\nconfidence of the predicted matches.\n</p>\n<p>We propose the Enhanced Probabilistic Dense Correspondence Network, PDC-Net+,\ncapable of estimating accurate dense correspondences along with a reliable\nconfidence map. We develop a flexible probabilistic approach that jointly\nlearns the flow prediction and its uncertainty. In particular, we parametrize\nthe predictive distribution as a constrained mixture model, ensuring better\nmodelling of both accurate flow predictions and outliers. Moreover, we develop\nan architecture and an enhanced training strategy tailored for robust and\ngeneralizable uncertainty prediction in the context of self-supervised\ntraining. Our approach obtains state-of-the-art results on multiple challenging\ngeometric matching and optical flow datasets. We further validate the\nusefulness of our probabilistic confidence estimation for the tasks of pose\nestimation, 3D reconstruction, image-based localization, and image retrieval.\nCode and models are available at https://github.com/PruneTruong/DenseMatching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_P/0/1/0/all/0/1\">Prune Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$f$-Cal: Calibrated aleatoric uncertainty estimation from neural networks for robot perception. (arXiv:2109.13913v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13913","description":"<p>While modern deep neural networks are performant perception modules,\nperformance (accuracy) alone is insufficient, particularly for safety-critical\nrobotic applications such as self-driving vehicles. Robot autonomy stacks also\nrequire these otherwise blackbox models to produce reliable and calibrated\nmeasures of confidence on their predictions. Existing approaches estimate\nuncertainty from these neural network perception stacks by modifying network\narchitectures, inference procedure, or loss functions. However, in general,\nthese methods lack calibration, meaning that the predictive uncertainties do\nnot faithfully represent the true underlying uncertainties (process noise). Our\nkey insight is that calibration is only achieved by imposing constraints across\nmultiple examples, such as those in a mini-batch; as opposed to existing\napproaches which only impose constraints per-sample, often leading to\noverconfident (thus miscalibrated) uncertainty estimates. By enforcing the\ndistribution of outputs of a neural network to resemble a target distribution\nby minimizing an $f$-divergence, we obtain significantly better-calibrated\nmodels compared to prior approaches. Our approach, $f$-Cal, outperforms\nexisting uncertainty calibration approaches on robot perception tasks such as\nobject detection and monocular depth estimation over multiple real-world\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_D/0/1/0/all/0/1\">Dhaivat Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mani_K/0/1/0/all/0/1\">Kaustubh Mani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_D/0/1/0/all/0/1\">Dishank Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murthy_K/0/1/0/all/0/1\">Krishna Murthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hanju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1\">Liam Paull</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsolved Problems in ML Safety. (arXiv:2109.13916v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13916","description":"<p>Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), steering ML systems (\"Alignment\"), and\nreducing risks to how ML systems are handled (\"External Safety\"). Throughout,\nwe clarify each problem's motivation and provide concrete research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics-guided Neural Networks (PGNN): An Application in Lake Temperature Modeling. (arXiv:1710.11431v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1710.11431","description":"<p>This paper introduces a framework for combining scientific knowledge of\nphysics-based models with neural networks to advance scientific discovery. This\nframework, termed physics-guided neural networks (PGNN), leverages the output\nof physics-based model simulations along with observational features in a\nhybrid modeling setup to generate predictions using a neural network\narchitecture. Further, this framework uses physics-based loss functions in the\nlearning objective of neural networks to ensure that the model predictions not\nonly show lower errors on the training set but are also scientifically\nconsistent with the known physics on the unlabeled set. We illustrate the\neffectiveness of PGNN for the problem of lake temperature modeling, where\nphysical relationships between the temperature, density, and depth of water are\nused to design a physics-based loss function. By using scientific knowledge to\nguide the construction and learning of neural networks, we are able to show\nthat the proposed framework ensures better generalizability as well as\nscientific consistency of results. All the code and datasets used in this study\nhave been made available on this link \\url{https://github.com/arkadaw9/PGNN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daw_A/0/1/0/all/0/1\">Arka Daw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpatne_A/0/1/0/all/0/1\">Anuj Karpatne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watkins_W/0/1/0/all/0/1\">William Watkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1\">Jordan Read</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding. (arXiv:1911.00232v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.00232","description":"<p>Videos capture events that typically contain multiple sequential, and\nsimultaneous, actions even in the span of only a few seconds. However, most\nlarge-scale datasets built to train models for action recognition in video only\nprovide a single label per video. Consequently, models can be incorrectly\npenalized for classifying actions that exist in the videos but are not\nexplicitly labeled and do not learn the full spectrum of information present in\neach video in training. Towards this goal, we present the Multi-Moments in Time\ndataset (M-MiT) which includes over two million action labels for over one\nmillion three second videos. This multi-label dataset introduces novel\nchallenges on how to train and analyze models for multi-action detection. Here,\nwe present baseline results for multi-action recognition using loss functions\nadapted for long tail multi-label learning, provide improved methods for\nvisualizing and interpreting models trained for multi-label action detection\nand show the strength of transferring models trained on M-MiT to smaller\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monfort_M/0/1/0/all/0/1\">Mathew Monfort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1\">Bowen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_K/0/1/0/all/0/1\">Kandan Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonian_A/0/1/0/all/0/1\">Alex Andonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McNamara_B/0/1/0/all/0/1\">Barry A McNamara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lascelles_A/0/1/0/all/0/1\">Alex Lascelles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Quanfu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1\">Dan Gutfreund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1\">Aude Oliva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Degenerative Adversarial NeuroImage Nets for Brain Scan Simulations: Application in Ageing and Dementia. (arXiv:1912.01526v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/1912.01526","description":"<p>Accurate and realistic simulation of high-dimensional medical images has\nbecome an important research area relevant to many AI-enabled healthcare\napplications. However, current state-of-the-art approaches lack the ability to\nproduce satisfactory high-resolution and accurate subject-specific images. In\nthis work, we present a deep learning framework, namely 4D-Degenerative\nAdversarial NeuroImage Net (4D-DANI-Net), to generate high-resolution,\nlongitudinal MRI scans that mimic subject-specific neurodegeneration in ageing\nand dementia. 4D-DANI-Net is a modular framework based on adversarial training\nand a set of novel spatiotemporal, biologically-informed constraints. To ensure\nefficient training and overcome memory limitations affecting such\nhigh-dimensional problems, we rely on three key technological advances: i) a\nnew 3D training consistency mechanism called Profile Weight Functions (PWFs),\nii) a 3D super-resolution module and iii) a transfer learning strategy to\nfine-tune the system for a given individual. To evaluate our approach, we\ntrained the framework on 9852 T1-weighted MRI scans from 876 participants in\nthe Alzheimer's Disease Neuroimaging Initiative dataset and held out a separate\ntest set of 1283 MRI scans from 170 participants for quantitative and\nqualitative assessment of the personalised time series of synthetic images. We\nperformed three evaluations: i) image quality assessment; ii) quantifying the\naccuracy of regional brain volumes over and above benchmark models; and iii)\nquantifying visual perception of the synthetic images by medical experts.\nOverall, both quantitative and qualitative results show that 4D-DANI-Net\nproduces realistic, low-artefact, personalised time series of synthetic T1 MRI\nthat outperforms benchmark models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ravi_D/0/1/0/all/0/1\">Daniele Ravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blumberg_S/0/1/0/all/0/1\">Stefano B. Blumberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ingala_S/0/1/0/all/0/1\">Silvia Ingala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barkhof_F/0/1/0/all/0/1\">Frederik Barkhof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C. Alexander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oxtoby_N/0/1/0/all/0/1\">Neil P. Oxtoby</a> (for the Alzheimer&#x27;s Disease Neuroimaging Initiative)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Head2Head++: Deep Facial Attributes Re-Targeting. (arXiv:2006.10199v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.10199","description":"<p>Facial video re-targeting is a challenging problem aiming to modify the\nfacial attributes of a target subject in a seamless manner by a driving\nmonocular sequence. We leverage the 3D geometry of faces and Generative\nAdversarial Networks (GANs) to design a novel deep learning architecture for\nthe task of facial and head reenactment. Our method is different to purely 3D\nmodel-based approaches, or recent image-based methods that use Deep\nConvolutional Neural Networks (DCNNs) to generate individual frames. We manage\nto capture the complex non-rigid facial motion from the driving monocular\nperformances and synthesise temporally consistent videos, with the aid of a\nsequential Generator and an ad-hoc Dynamics Discriminator network. We conduct a\ncomprehensive set of quantitative and qualitative tests and demonstrate\nexperimentally that our proposed method can successfully transfer facial\nexpressions, head pose and eye gaze from a source video to a target subject, in\na photo-realistic and faithful fashion, better than other state-of-the-art\nmethods. Most importantly, our system performs end-to-end reenactment in nearly\nreal-time speed (18 fps).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doukas_M/0/1/0/all/0/1\">Michail Christos Doukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koujan_M/0/1/0/all/0/1\">Mohammad Rami Koujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharmanska_V/0/1/0/all/0/1\">Viktoriia Sharmanska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roussos_A/0/1/0/all/0/1\">Anastasios Roussos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest X-Ray Images. (arXiv:2007.08637v6 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2007.08637","description":"<p>Coronaviruses constitute a family of viruses that gives rise to respiratory\ndiseases. As COVID-19 is highly contagious, early diagnosis of COVID-19 is\ncrucial for an effective treatment strategy. However, the RT-PCR test which is\nconsidered to be a gold standard in the diagnosis of COVID-19 suffers from a\nhigh false-negative rate. Chest X-ray (CXR) image analysis has emerged as a\nfeasible and effective diagnostic technique towards this objective. In this\nwork, we propose the COVID-19 classification problem as a three-class\nclassification problem to distinguish between COVID-19, normal, and pneumonia\nclasses. We propose a three-stage framework, named COV-ELM. Stage one deals\nwith preprocessing and transformation while stage two deals with feature\nextraction. These extracted features are passed as an input to the ELM at the\nthird stage, resulting in the identification of COVID-19. The choice of ELM in\nthis work has been motivated by its faster convergence, better generalization\ncapability, and shorter training time in comparison to the conventional\ngradient-based learning algorithms. As bigger and diverse datasets become\navailable, ELM can be quickly retrained as compared to its gradient-based\ncompetitor models. The proposed model achieved a macro average F1-score of 0.95\nand the overall sensitivity of ${0.94 \\pm 0.02} at a 95% confidence interval.\nWhen compared to state-of-the-art machine learning algorithms, the COV-ELM is\nfound to outperform its competitors in this three-class classification\nscenario. Further, LIME has been integrated with the proposed COV-ELM model to\ngenerate annotated CXR images. The annotations are based on the superpixels\nthat have contributed to distinguish between the different classes. It was\nobserved that the superpixels correspond to the regions of the human lungs that\nare clinically observed in COVID-19 and Pneumonia cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_S/0/1/0/all/0/1\">Sheetal Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_M/0/1/0/all/0/1\">Manoj Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_A/0/1/0/all/0/1\">Ankit Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lakhyani_N/0/1/0/all/0/1\">Navin Lakhyani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saggar_A/0/1/0/all/0/1\">Arpita Saggar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1\">Naveen Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Retinal Vessel Segmentation from a Data Augmentation Perspective. (arXiv:2007.15883v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2007.15883","description":"<p>Retinal vessel segmentation is a fundamental step in screening, diagnosis,\nand treatment of various cardiovascular and ophthalmic diseases. Robustness is\none of the most critical requirements for practical utilization, since the test\nimages may be captured using different fundus cameras, or be affected by\nvarious pathological changes. We investigate this problem from a data\naugmentation perspective, with the merits of no additional training data or\ninference time. In this paper, we propose two new data augmentation modules,\nnamely, channel-wise random Gamma correction and channel-wise random vessel\naugmentation. Given a training color fundus image, the former applies random\ngamma correction on each color channel of the entire image, while the latter\nintentionally enhances or decreases only the fine-grained blood vessel regions\nusing morphological transformations. With the additional training samples\ngenerated by applying these two modules sequentially, a model could learn more\ninvariant and discriminating features against both global and local\ndisturbances. Experimental results on both real-world and synthetic datasets\ndemonstrate that our method can improve the performance and robustness of a\nclassic convolutional neural network architecture. The source code is available\nat\n\\url{https://github.com/PaddlePaddle/Research/tree/master/CV/robust_vessel_segmentation}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_D/0/1/0/all/0/1\">Dongwei Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Junwei Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection With Partitioning Overfitting Autoencoder Ensembles. (arXiv:2009.02755v8 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.02755","description":"<p>In this paper, we propose POTATOES (Partitioning OverfiTting AuTOencoder\nEnSemble), a new method for unsupervised outlier detection (UOD). More\nprecisely, given any autoencoder for UOD, this technique can be used to improve\nits accuracy while at the same time removing the burden of tuning its\nregularization. The idea is to not regularize at all, but to rather randomly\npartition the data into sufficiently many equally sized parts, overfit each\npart with its own autoencoder, and to use the maximum over all autoencoder\nreconstruction errors as the anomaly score. We apply our model to various\nrealistic datasets and show that if the set of inliers is dense enough, our\nmethod indeed improves the UOD performance of a given autoencoder\nsignificantly. For reproducibility, the code is made available on github so the\nreader can recreate the results in this paper as well as apply the method to\nother autoencoders and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorbeer_B/0/1/0/all/0/1\">Boris Lorbeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botler_M/0/1/0/all/0/1\">Max Botler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Once Quantization-Aware Training: High Performance Extremely Low-bit Architecture Search. (arXiv:2010.04354v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.04354","description":"<p>Quantization Neural Networks (QNN) have attracted a lot of attention due to\ntheir high efficiency. To enhance the quantization accuracy, prior works mainly\nfocus on designing advanced quantization algorithms but still fail to achieve\nsatisfactory results under the extremely low-bit case. In this work, we take an\narchitecture perspective to investigate the potential of high-performance QNN.\nTherefore, we propose to combine Network Architecture Search methods with\nquantization to enjoy the merits of the two sides. However, a naive combination\ninevitably faces unacceptable time consumption or unstable training problem. To\nalleviate these problems, we first propose the joint training of architecture\nand quantization with a shared step size to acquire a large number of quantized\nmodels. Then a bit-inheritance scheme is introduced to transfer the quantized\nmodels to the lower bit, which further reduces the time cost and meanwhile\nimproves the quantization accuracy. Equipped with this overall framework,\ndubbed as Once Quantization-Aware Training~(OQAT), our searched model family,\nOQATNets, achieves a new state-of-the-art compared with various architectures\nunder different bit-widths. In particular, OQAT-2bit-M achieves 61.6% ImageNet\nTop-1 accuracy, outperforming 2-bit counterpart MobileNetV3 by a large margin\nof 9% with 10% less computation cost. A series of quantization-friendly\narchitectures are identified easily and extensive analysis can be made to\nsummarize the interaction between quantization and neural architectures. Codes\nand models are released at https://github.com/LaVieEnRoseSMZ/OQA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Mingzhu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1\">Feng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANIMC: A Soft Framework for Auto-weighted Noisy and Incomplete Multi-view Clustering. (arXiv:2011.10331v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.10331","description":"<p>Multi-view clustering has wide applications in many image processing\nscenarios. In these scenarios, original image data often contain missing\ninstances and noises, which is ignored by most multi-view clustering methods.\nHowever, missing instances may make these methods difficult to use directly and\nnoises will lead to unreliable clustering results. In this paper, we propose a\nnovel Auto-weighted Noisy and Incomplete Multi-view Clustering framework\n(ANIMC) via a soft auto-weighted strategy and a doubly soft regular regression\nmodel. Firstly, by designing adaptive semi-regularized nonnegative matrix\nfactorization (adaptive semi-RNMF), the soft auto-weighted strategy assigns a\nproper weight to each view and adds a soft boundary to balance the influence of\nnoises and incompleteness. Secondly, by proposing{\\theta}-norm, the doubly soft\nregularized regression model adjusts the sparsity of our model by choosing\ndifferent{\\theta}. Compared with existing methods, ANIMC has three unique\nadvantages: 1) it is a soft algorithm to adjust our framework in different\nscenarios, thereby improving its generalization ability; 2) it automatically\nlearns a proper weight for each view, thereby reducing the influence of noises;\n3) it performs doubly soft regularized regression that aligns the same\ninstances in different views, thereby decreasing the impact of missing\ninstances. Extensive experimental results demonstrate its superior advantages\nover other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuchong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dapeng Oliver Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Camera Convolutional Color Constancy. (arXiv:2011.11890v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11890","description":"<p>We present \"Cross-Camera Convolutional Color Constancy\" (C5), a\nlearning-based method, trained on images from multiple cameras, that accurately\nestimates a scene's illuminant color from raw images captured by a new camera\npreviously unseen during training. C5 is a hypernetwork-like extension of the\nconvolutional color constancy (CCC) approach: C5 learns to generate the weights\nof a CCC model that is then evaluated on the input image, with the CCC weights\ndynamically adapted to different input content. Unlike prior cross-camera color\nconstancy models, which are usually designed to be agnostic to the spectral\nproperties of test-set images from unobserved cameras, C5 approaches this\nproblem through the lens of transductive inference: additional unlabeled images\nare provided as input to the model at test time, which allows the model to\ncalibrate itself to the spectral properties of the test-set camera during\ninference. C5 achieves state-of-the-art accuracy for cross-camera color\nconstancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on\na GPU or CPU, respectively), and requires little memory (~2 MB), and thus is a\npractical solution to the problem of calibration-free automatic white balance\nfor mobile photography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeGendre_C/0/1/0/all/0/1\">Chloe LeGendre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Ta Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleibel_F/0/1/0/all/0/1\">Francois Bleibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v10 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01338","description":"<p>Training deep learning models in technical domains is often accompanied by\nthe challenge that although the task is clear, insufficient data for training\nis available. In this work, we propose a novel approach based on the\ncombination of Siamese networks and radial basis function networks to perform\ndata-efficient classification without pretraining by measuring the distance\nbetween images in semantic space in a data-efficient manner. We develop the\nmodels using three technical datasets, the NEU dataset, the BSD dataset, and\nthe TEX dataset. In addition to the technical domain, we show the general\napplicability to classical datasets (cifar10 and MNIST) as well. The approach\nis tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise\nreduction of the number of samples available for training. The authors show\nthat the proposed approach outperforms the state-of-the-art models in the low\ndata regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlagenhauf_T/0/1/0/all/0/1\">Tobias Schlagenhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildirim_F/0/1/0/all/0/1\">Faruk Yildirim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruckner_B/0/1/0/all/0/1\">Benedikt Br&#xfc;ckner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mDALU: Multi-Source Domain Adaptation and Label Unification with Partial Datasets. (arXiv:2012.08385v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.08385","description":"<p>One challenge of object recognition is to generalize to new domains, to more\nclasses and/or to new modalities. This necessitates methods to combine and\nreuse existing datasets that may belong to different domains, have partial\nannotations, and/or have different data modalities. This paper formulates this\nas a multi-source domain adaptation and label unification problem, and proposes\na novel method for it. Our method consists of a partially-supervised adaptation\nstage and a fully-supervised adaptation stage. In the former, partial knowledge\nis transferred from multiple source domains to the target domain and fused\ntherein. Negative transfer between unmatching label spaces is mitigated via\nthree new modules: domain attention, uncertainty maximization and\nattention-guided adversarial alignment. In the latter, knowledge is transferred\nin the unified label space after a label completion process with pseudo-labels.\nExtensive experiments on three different tasks - image classification, 2D\nsemantic image segmentation, and joint 2D-3D semantic segmentation - show that\nour method outperforms all competing methods significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Rui Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Bi-Level Optimization for Learning and Vision from a Unified Perspective: A Survey and Beyond. (arXiv:2101.11517v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.11517","description":"<p>Bi-Level Optimization (BLO) is originated from the area of economic game\ntheory and then introduced into the optimization community. BLO is able to\nhandle problems with a hierarchical structure, involving two levels of\noptimization tasks, where one task is nested inside the other. In machine\nlearning and computer vision fields, despite the different motivations and\nmechanisms, a lot of complex problems, such as hyper-parameter optimization,\nmulti-task and meta-learning, neural architecture search, adversarial learning\nand deep reinforcement learning, actually all contain a series of closely\nrelated subproblms. In this paper, we first uniformly express these complex\nlearning and vision problems from the perspective of BLO. Then we construct a\nbest-response-based single-level reformulation and establish a unified\nalgorithmic framework to understand and formulate mainstream gradient-based BLO\nmethodologies, covering aspects ranging from fundamental automatic\ndifferentiation schemes to various accelerations, simplifications, extensions\nand their convergence and complexity properties. Last but not least, we discuss\nthe potentials of our unified BLO framework for designing new algorithms and\npoint out some promising directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiaxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards clinically applicable automated aneurysm detection in TOF-MRA: weak labels, anatomical knowledge, and open data. (arXiv:2103.06168v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.06168","description":"<p>Purpose: 1) Develop a deep learning algorithm for brain aneurysm detection\nexploiting weak labels and prior anatomical knowledge. 2) Describe and release\nthe largest Time-Of-Flight Magnetic Resonance Angiography (TOF-MRA) dataset to\nthe community.\n</p>\n<p>Materials and Methods: In this retrospective study we retrieved TOF-MRA\nimages of 284 subjects (170 females) scanned between 2010 and 2015. Out of\nthese, 157 are patients with a total of 198 aneurysms, while 127 are controls.\nWe used spherical weak labels as detection ground truth, thus making data\nannotation, a major bottleneck for medical AI, noticeably faster. Since\naneurysms mainly occur in specific locations, we built our deep neural network\nleveraging the anatomy of the brain vasculature. To assess model robustness, we\nparticipated in the first public challenge for TOF-MRA data (93 patients, 20\ncontrols, 125 aneurysms). We stratified results according to aneurysm\nrisk-of-rupture, location, and size.\n</p>\n<p>Results: Our network achieves a sensitivity of 80% on the in-house data, with\nFalse Positive (FP) rate of 1.2 per patient. On the public challenge data,\nsensitivity was 68% (FP rate = 2.5), ranking 4th/16 on the open leaderboard. We\nfound no significant difference in sensitivity between risk groups (p = 0.75),\nlocations (p = 0.72), or sizes (p = 0.15).\n</p>\n<p>Conclusion: Competitive results can be obtained using fast weak labels and\nanatomical knowledge for automated aneurysm detection. Our open-source code and\nopen access dataset can foster reproducibility, and bring us closer to clinical\napplication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Noto_T/0/1/0/all/0/1\">Tommaso Di Noto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marie_G/0/1/0/all/0/1\">Guillaume Marie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tourbier_S/0/1/0/all/0/1\">Sebastien Tourbier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aleman_Gomez_Y/0/1/0/all/0/1\">Yasser Alem&#xe1;n-G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Esteban_O/0/1/0/all/0/1\">Oscar Esteban</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saliou_G/0/1/0/all/0/1\">Guillaume Saliou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cuadra_M/0/1/0/all/0/1\">Meritxell Bach Cuadra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hagmann_P/0/1/0/all/0/1\">Patric Hagmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Richiardi_J/0/1/0/all/0/1\">Jonas Richiardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning in Multi-Task Graphs through Iterative Consensus Shift. (arXiv:2103.14417v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.14417","description":"<p>The human ability to synchronize the feedback from all their senses inspired\nrecent works in multi-task and multi-modal learning. While these works rely on\nexpensive supervision, our multi-task graph requires only pseudo-labels from\nexpert models. Every graph node represents a task, and each edge learns between\ntasks transformations. Once initialized, the graph learns self-supervised,\nbased on a novel consensus shift algorithm that intelligently exploits the\nagreement between graph pathways to generate new pseudo-labels for the next\nlearning cycle. We demonstrate significant improvement from one unsupervised\nlearning iteration to the next, outperforming related recent methods in\nextensive multi-task learning experiments on two challenging datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haller_E/0/1/0/all/0/1\">Emanuela Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burceanu_E/0/1/0/all/0/1\">Elena Burceanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leordeanu_M/0/1/0/all/0/1\">Marius Leordeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Strong Baseline for Universal Targeted Attacks on Siamese Visual Tracking. (arXiv:2105.02480v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02480","description":"<p>Siamese trackers are shown to be vulnerable to adversarial attacks recently.\nHowever, the existing attack methods craft the perturbations for each video\nindependently, which comes at a non-negligible computational cost. In this\npaper, we show the existence of universal perturbations that can enable the\ntargeted attack, e.g., forcing a tracker to follow the ground-truth trajectory\nwith specified offsets, to be video-agnostic and free from inference in a\nnetwork. Specifically, we attack a tracker by adding a universal imperceptible\nperturbation to the template image and adding a fake target, i.e., a small\nuniversal adversarial patch, into the search images adhering to the predefined\ntrajectory, so that the tracker outputs the location and size of the fake\ntarget instead of the real target. Our approach allows perturbing a novel video\nto come at no additional cost except the mere addition operations -- and not\nrequire gradient optimization or network inference. Experimental results on\nseveral datasets demonstrate that our approach can effectively fool the Siamese\ntrackers in a targeted attack manner. We show that the proposed perturbations\nare not only universal across videos, but also generalize well across different\ntrackers. Such perturbations are therefore doubly universal, both with respect\nto the data and the network architectures. We will make our code publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenbang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yaya Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaoru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Pengpeng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-augmented Spatio-Temporal Segmentation for Land Cover Mapping. (arXiv:2105.02963v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02963","description":"<p>The availability of massive earth observing satellite data provide huge\nopportunities for land use and land cover mapping. However, such mapping effort\nis challenging due to the existence of various land cover classes, noisy data,\nand the lack of proper labels. Also, each land cover class typically has its\nown unique temporal pattern and can be identified only during certain periods.\nIn this article, we introduce a novel architecture that incorporates the UNet\nstructure with Bidirectional LSTM and Attention mechanism to jointly exploit\nthe spatial and temporal nature of satellite data and to better identify the\nunique temporal patterns of each land cover. We evaluate this method for\nmapping crops in multiple regions over the world. We compare our method with\nother state-of-the-art methods both quantitatively and qualitatively on two\nreal-world datasets which involve multiple land cover classes. We also\nvisualise the attention weights to study its effectiveness in mitigating noise\nand identifying discriminative time period.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rahul Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravirathinam_P/0/1/0/all/0/1\">Praveen Ravirathinam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenxi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhenong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey of Visual-Semantic Embedding Methods for Zero-Shot Image Retrieval. (arXiv:2105.07391v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07391","description":"<p>Visual-semantic embedding is an interesting research topic because it is\nuseful for various tasks, such as visual question answering (VQA), image-text\nretrieval, image captioning, and scene graph generation. In this paper, we\nfocus on zero-shot image retrieval using sentences as queries and present a\nsurvey of the technological trends in this area. First, we provide a\ncomprehensive overview of the history of the technology, starting with a\ndiscussion of the early studies of image-to-text matching and how the\ntechnology has evolved over time. In addition, a description of the datasets\ncommonly used in experiments and a comparison of the evaluation results of each\nmethod are presented. We also introduce the implementation available on github\nfor use in confirming the accuracy of experiments and for further improvement.\nWe hope that this survey paper will encourage researchers to further develop\ntheir research on bridging images and languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ueki_K/0/1/0/all/0/1\">Kazuya Ueki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalisable and distinctive 3D local deep descriptors for point cloud registration. (arXiv:2105.10382v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10382","description":"<p>An effective 3D descriptor should be invariant to different geometric\ntransformations, such as scale and rotation, repeatable in the case of\nocclusions and clutter, and generalisable in different contexts when data is\ncaptured with different sensors. We present a simple but yet effective method\nto learn generalisable and distinctive 3D local descriptors that can be used to\nregister point clouds captured in different contexts with different sensors.\nPoint cloud patches are extracted, canonicalised with respect to their local\nreference frame, and encoded into scale and rotation-invariant compact\ndescriptors by a point permutation-invariant deep neural network. Our\ndescriptors can effectively generalise across sensor modalities from locally\nand randomly sampled points. We evaluate and compare our descriptors with\nalternative handcrafted and deep learning-based descriptors on several indoor\nand outdoor datasets reconstructed using both RGBD sensors and laser scanners.\nOur descriptors outperform most recent descriptors by a large margin in terms\nof generalisation, and become the state of the art also in benchmarks where\ntraining and testing are performed in the same scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1\">Fabio Poiesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boscaini_D/0/1/0/all/0/1\">Davide Boscaini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy. (arXiv:2106.01100v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.01100","description":"<p>During lung cancer radiotherapy, the position of infrared reflective objects\non the chest can be recorded to estimate the tumor location. However,\nradiotherapy systems usually have a latency inherent to robot control\nlimitations that impedes the radiation delivery precision. Not taking this\nphenomenon into account may cause unwanted damage to healthy tissues and lead\nto side effects such as radiation pneumonitis. In this research, we use nine\nobservation records of the three-dimensional position of three external markers\non the chest and abdomen of healthy individuals breathing during intervals from\n73s to 222s. The sampling frequency is equal to 10Hz and the amplitudes of the\nrecorded trajectories range from 6mm to 40mm in the superior-inferior\ndirection. We forecast the location of each marker simultaneously with a\nhorizon value (the time interval in advance for which the prediction is made)\nbetween 0.1s and 2.0s, using a recurrent neural network (RNN) trained with\nunbiased online recurrent optimization (UORO). We compare its performance with\nan RNN trained with real-time recurrent learning, least mean squares (LMS), and\noffline linear regression. Training and cross-validation are performed during\nthe first minute of each sequence. On average, UORO achieves the lowest\nroot-mean-square (RMS) and maximum error, equal respectively to 1.3mm and\n8.8mm, with a prediction time per time step lower than 2.8ms (Dell Intel core\ni9-9900K 3.60Ghz). Linear regression has the lowest RMS error for the horizon\nvalues 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s,\nand UORO for horizon values greater than 0.6s.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pohl_M/0/1/0/all/0/1\">Michel Pohl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uesaka_M/0/1/0/all/0/1\">Mitsuru Uesaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takahashi_H/0/1/0/all/0/1\">Hiroyuki Takahashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demachi_K/0/1/0/all/0/1\">Kazuyuki Demachi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chhatkuli_R/0/1/0/all/0/1\">Ritu Bhusal Chhatkuli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inter Extreme Points Geodesics for Weakly Supervised Image Segmentation. (arXiv:2107.00583v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00583","description":"<p>We introduce $\\textit{InExtremIS}$, a weakly supervised 3D approach to train\na deep image segmentation network using particularly weak train-time\nannotations: only 6 extreme clicks at the boundary of the objects of interest.\nOur fully-automatic method is trained end-to-end and does not require any\ntest-time annotations. From the extreme points, 3D bounding boxes are extracted\naround objects of interest. Then, deep geodesics connecting extreme points are\ngenerated to increase the amount of \"annotated\" voxels within the bounding\nboxes. Finally, a weakly supervised regularised loss derived from a Conditional\nRandom Field formulation is used to encourage prediction consistency over\nhomogeneous regions. Extensive experiments are performed on a large open\ndataset for Vestibular Schwannoma segmentation. $\\textit{InExtremIS}$ obtained\ncompetitive performance, approaching full supervision and outperforming\nsignificantly other weakly supervised techniques based on bounding boxes.\nMoreover, given a fixed annotation time budget, $\\textit{InExtremIS}$\noutperforms full supervision. Our code and data are available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dorent_R/0/1/0/all/0/1\">Reuben Dorent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joutard_S/0/1/0/all/0/1\">Samuel Joutard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapey_J/0/1/0/all/0/1\">Jonathan Shapey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kujawa_A/0/1/0/all/0/1\">Aaron Kujawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modat_M/0/1/0/all/0/1\">Marc Modat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation. (arXiv:2107.00781v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00781","description":"<p>Transformer architecture has emerged to be successful in a number of natural\nlanguage processing tasks. However, its applications to medical vision remain\nlargely unexplored. In this study, we present UTNet, a simple yet powerful\nhybrid Transformer architecture that integrates self-attention into a\nconvolutional neural network for enhancing medical image segmentation. UTNet\napplies self-attention modules in both encoder and decoder for capturing\nlong-range dependency at different scales with minimal overhead. To this end,\nwe propose an efficient self-attention mechanism along with relative position\nencoding that reduces the complexity of self-attention operation significantly\nfrom $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is also\nproposed to recover fine-grained details from the skipped connections in the\nencoder. Our approach addresses the dilemma that Transformer requires huge\namounts of data to learn vision inductive bias. Our hybrid layer design allows\nthe initialization of Transformer into convolutional networks without a need of\npre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac\nmagnetic resonance imaging cohort. UTNet demonstrates superior segmentation\nperformance and robustness against the state-of-the-art approaches, holding the\npromise to generalize well on other medical image segmentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00394","description":"<p>Graph matching is an important problem that has received widespread\nattention, especially in the field of computer vision. Recently,\nstate-of-the-art methods seek to incorporate graph matching with deep learning.\nHowever, there is no research to explain what role the graph matching algorithm\nplays in the model. Therefore, we propose an approach integrating a MILP\nformulation of the graph matching problem. This formulation is solved to\noptimal and it provides inherent baseline. Meanwhile, similar approaches are\nderived by releasing the optimal guarantee of the graph matching solver and by\nintroducing a quality level. This quality level controls the quality of the\nsolutions provided by the graph matching solver. In addition, several\nrelaxations of the graph matching problem are put to the test. Our experimental\nevaluation gives several theoretical insights and guides the direction of deep\ngraph matching methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhoubo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Puqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raveaux_R/0/1/0/all/0/1\">Romain Raveaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huadong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refractive Geometry for Underwater Domes. (arXiv:2108.06575v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06575","description":"<p>Underwater cameras are typically placed behind glass windows to protect them\nfrom the water. Spherical glass, a dome port, is well suited for high water\npressures at great depth, allows for a large field of view, and avoids\nrefraction if a pinhole camera is positioned exactly at the sphere's center.\nAdjusting a real lens perfectly to the dome center is a challenging task, both\nin terms of how to actually guide the centering process (e.g. visual servoing)\nand how to measure the alignment quality, but also, how to mechanically perform\nthe alignment. Consequently, such systems are prone to being decentered by some\noffset, leading to challenging refraction patterns at the sphere that\ninvalidate the pinhole camera model. We show that the overall camera system\nbecomes an axial camera, even for thick domes as used for deep sea exploration\nand provide a non-iterative way to compute the center of refraction without\nrequiring knowledge of exact air, glass or water properties. We also analyze\nthe refractive geometry at the sphere, looking at effects such as forward- vs.\nbackward decentering, iso-refraction curves and obtain a 6th-degree polynomial\nequation for forward projection of 3D points in thin domes. We then propose a\npure underwater calibration procedure to estimate the decentering from multiple\nimages. This estimate can either be used during adjustment to guide the\nmechanical position of the lens, or can be considered in photogrammetric\nunderwater applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+She_M/0/1/0/all/0/1\">Mengkun She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakath_D/0/1/0/all/0/1\">David Nakath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koser_K/0/1/0/all/0/1\">Kevin K&#xf6;ser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FSNet: A Failure Detection Framework for Semantic Segmentation. (arXiv:2108.08748v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08748","description":"<p>Semantic segmentation is an important task that helps autonomous vehicles\nunderstand their surroundings and navigate safely. During deployment, even the\nmost mature segmentation models are vulnerable to various external factors that\ncan degrade the segmentation performance with potentially catastrophic\nconsequences for the vehicle and its surroundings. To address this issue, we\npropose a failure detection framework to identify pixel-level\nmisclassification. We do so by exploiting internal features of the segmentation\nmodel and training it simultaneously with a failure detection network. During\ndeployment, the failure detector can flag areas in the image where the\nsegmentation model have failed to segment correctly. We evaluate the proposed\napproach against state-of-the-art methods and achieve 12.30%, 9.46%, and 9.65%\nperformance improvement in the AUPR-Error metric for Cityscapes, BDD100K, and\nMapillary semantic segmentation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_Q/0/1/0/all/0/1\">Quazi Marufur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunderhauf_N/0/1/0/all/0/1\">Niko S&#xfc;nderhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corke_P/0/1/0/all/0/1\">Peter Corke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dayoub_F/0/1/0/all/0/1\">Feras Dayoub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fiducial marker recovery and detection from severely truncated data in navigation assisted spine surgery. (arXiv:2108.13844v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.13844","description":"<p>Fiducial markers are commonly used in navigation assisted minimally invasive\nspine surgery (MISS) and they help transfer image coordinates into real world\ncoordinates. In practice, these markers might be located outside the\nfield-of-view (FOV), due to the limited detector sizes of C-arm cone-beam\ncomputed tomography (CBCT) systems used in intraoperative surgeries. As a\nconsequence, reconstructed markers in CBCT volumes suffer from artifacts and\nhave distorted shapes, which sets an obstacle for navigation. In this work, we\npropose two fiducial marker detection methods: direct detection from distorted\nmarkers (direct method) and detection after marker recovery (recovery method).\nFor direct detection from distorted markers in reconstructed volumes, an\nefficient automatic marker detection method using two neural networks and a\nconventional circle detection algorithm is proposed. For marker recovery, a\ntask-specific learning strategy is proposed to recover markers from severely\ntruncated data. Afterwards, a conventional marker detection algorithm is\napplied for position detection. The two methods are evaluated on simulated data\nand real data, both achieving a marker registration error smaller than 0.2 mm.\nOur experiments demonstrate that the direct method is capable of detecting\ndistorted markers accurately and the recovery method with task-specific\nlearning has high robustness and generalizability on various data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_F/0/1/0/all/0/1\">Fuxin Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreher_B/0/1/0/all/0/1\">Bj&#xf6;rn Kreher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keil_H/0/1/0/all/0/1\">Holger Keil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-step Domain Adaptation for Mitosis Cell Detection in Histopathology Images. (arXiv:2109.00109v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00109","description":"<p>We propose a two-step domain shift-invariant mitosis cell detection method\nbased on Faster RCNN and a convolutional neural network (CNN). We generate\nvarious domain-shifted versions of existing histopathology images using a stain\naugmentation technique, enabling our method to effectively learn various stain\ndomains and achieve better generalization. The performance of our method is\nevaluated on the preliminary test data set of the MIDOG-2021 challenge. The\nexperimental results demonstrate that the proposed mitosis detection method can\nachieve promising performance for domain-shifted histopathology images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nateghi_R/0/1/0/all/0/1\">Ramin Nateghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourakpour_F/0/1/0/all/0/1\">Fattaneh Pourakpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Extreme Value Theory for Open Set Video Domain Adaptation. (arXiv:2109.00522v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00522","description":"<p>With the advent of media streaming, video action recognition has become\nprogressively important for various applications, yet at the high expense of\nrequiring large-scale data labelling. To overcome the problem of expensive data\nlabelling, domain adaptation techniques have been proposed that transfers\nknowledge from fully labelled data (i.e., source domain) to unlabelled data\n(i.e., target domain). The majority of video domain adaptation algorithms are\nproposed for closed-set scenarios in which all the classes are shared among the\ndomains. In this work, we propose an open-set video domain adaptation approach\nto mitigate the domain discrepancy between the source and target data, allowing\nthe target data to contain additional classes that do not belong to the source\ndomain. Different from previous works, which only focus on improving accuracy\nfor shared classes, we aim to jointly enhance the alignment of shared classes\nand recognition of unknown samples. Towards this goal, class-conditional\nextreme value theory is applied to enhance the unknown recognition.\nSpecifically, the entropy values of target samples are modelled as generalised\nextreme value distributions, which allows separating unknown samples lying in\nthe tail of the distribution. To alleviate the negative transfer issue, weights\ncomputed by the distance from the sample entropy to the threshold are leveraged\nin adversarial learning in the sense that confident source and target samples\nare aligned, and unconfident samples are pushed away. The proposed method has\nbeen thoroughly evaluated on both small-scale and large-scale cross-domain\nvideo datasets and achieved the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuoxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1\">Mahsa Baktashmotlagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Human Deformation Transfer. (arXiv:2109.01588v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01588","description":"<p>We consider the problem of human deformation transfer, where the goal is to\nretarget poses between different characters. Traditional methods that tackle\nthis problem require a clear definition of the pose, and use this definition to\ntransfer poses between characters. In this work, we take a different approach\nand transform the identity of a character into a new identity without modifying\nthe character's pose. This offers the advantage of not having to define\nequivalences between 3D human poses, which is not straightforward as poses tend\nto change depending on the identity of the character performing them, and as\ntheir meaning is highly contextual. To achieve the deformation transfer, we\npropose a neural encoder-decoder architecture where only identity information\nis encoded and where the decoder is conditioned on the pose. We use pose\nindependent representations, such as isometry-invariant shape characteristics,\nto represent identity features. Our model uses these features to supervise the\nprediction of offsets from the deformed pose to the result of the transfer. We\nshow experimentally that our method outperforms state-of-the-art methods both\nquantitatively and qualitatively, and generalises better to poses not seen\nduring training. We also introduce a fine-tuning step that allows to obtain\ncompetitive results for extreme identities, and allows to transfer simple\nclothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basset_J/0/1/0/all/0/1\">Jean Basset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhayma_A/0/1/0/all/0/1\">Adnane Boukhayma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuhrer_S/0/1/0/all/0/1\">Stefanie Wuhrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Multon_F/0/1/0/all/0/1\">Franck Multon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyer_E/0/1/0/all/0/1\">Edmond Boyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit 3D Representations. (arXiv:2109.02123v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02123","description":"<p>Neural Radiance Fields (NeRF) has become a popular framework for learning\nimplicit 3D representations and addressing different tasks such as novel-view\nsynthesis or depth-map estimation. However, in downstream applications where\ndecisions need to be made based on automatic predictions, it is critical to\nleverage the confidence associated with the model estimations. Whereas\nuncertainty quantification is a long-standing problem in Machine Learning, it\nhas been largely overlooked in the recent NeRF literature. In this context, we\npropose Stochastic Neural Radiance Fields (S-NeRF), a generalization of\nstandard NeRF that learns a probability distribution over all the possible\nradiance fields modeling the scene. This distribution allows to quantify the\nuncertainty associated with the scene information provided by the model. S-NeRF\noptimization is posed as a Bayesian learning problem which is efficiently\naddressed using the Variational Inference framework. Exhaustive experiments\nover benchmark datasets demonstrate that S-NeRF is able to provide more\nreliable predictions and confidence values than generic approaches previously\nproposed for uncertainty estimation in other domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianxiong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_A/0/1/0/all/0/1\">Adria Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1\">Antonio Agudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paint4Poem: A Dataset for Artistic Visualization of Classical Chinese Poems. (arXiv:2109.11682v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11682","description":"<p>In this work we propose a new task: artistic visualization of classical\nChinese poems, where the goal is to generatepaintings of a certain artistic\nstyle for classical Chinese poems. For this purpose, we construct a new dataset\ncalled Paint4Poem. Thefirst part of Paint4Poem consists of 301 high-quality\npoem-painting pairs collected manually from an influential modern Chinese\nartistFeng Zikai. As its small scale poses challenges for effectively training\npoem-to-painting generation models, we introduce the secondpart of Paint4Poem,\nwhich consists of 3,648 caption-painting pairs collected manually from Feng\nZikai's paintings and 89,204 poem-painting pairs collected automatically from\nthe web. We expect the former to help learning the artist painting style as it\ncontainshis most paintings, and the latter to help learning the semantic\nrelevance between poems and paintings. Further, we analyze Paint4Poem regarding\npoem diversity, painting style, and the semantic relevance between poems and\npaintings. We create abenchmark for Paint4Poem: we train two representative\ntext-to-image generation models: AttnGAN and MirrorGAN, and evaluate\ntheirperformance regarding painting pictorial quality, painting stylistic\nrelevance, and semantic relevance between poems and paintings.The results\nindicate that the models are able to generate paintings that have good\npictorial quality and mimic Feng Zikai's style, but thereflection of poem\nsemantics is limited. The dataset also poses many interesting research\ndirections on this task, including transferlearning, few-shot learning,\ntext-to-image generation for low-resource data etc. The dataset is publicly\navailable.(https://github.com/paint4poem/paint4poem)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jie Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieuwburg_E/0/1/0/all/0/1\">Elisha Nieuwburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fengyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1\">Evangelos Kanoulas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harrisz+: Harris Corner Selection for Next-Gen Image Matching Pipelines. (arXiv:2109.12925v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12925","description":"<p>Due to its role in many computer vision tasks, image matching has been\nsubjected to an active investigation by researchers, which has lead to better\nand more discriminant feature descriptors and to more robust matching\nstrategies, also thanks to the advent of the deep learning and the increased\ncomputational power of the modern hardware. Despite of these achievements, the\nkeypoint extraction process at the base of the image matching pipeline has not\nseen equivalent progresses. This paper presents Harrisz$^{+}$, an upgrade to\nthe HarrisZ corner detector, optimized to synergically take advance of the\nrecent improvements of the other steps of the image matching pipeline.\nHarrisz$^{+}$ does not only consists of a tuning of the setup parameters, but\nintroduces further refinements to the selection criteria delineated by HarrisZ,\nso providing more, yet discriminative, keypoints, which are better distributed\non the image and with higher localization accuracy. The image matching pipeline\nincluding Harrisz$^{+}$, together with the other modern components, obtained in\ndifferent recent matching benchmarks state-of-the-art results among the classic\nimage matching pipelines, closely following results of the more recent fully\ndeep end-to-end trainable approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1\">Fabio Bellavia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_D/0/1/0/all/0/1\">Dmytro Mishkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}