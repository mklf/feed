{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Reproducibility Issues for BERT-based Evaluation Metrics. (arXiv:2204.00004v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00004","description":"<p>Reproducibility is of utmost concern in machine learning and natural language\nprocessing (NLP). In the field of natural language generation (especially\nmachine translation), the seminal paper of Post (2018) has pointed out problems\nof reproducibility of the dominant metric, BLEU, at the time of publication.\nNowadays, BERT-based evaluation metrics considerably outperform BLEU. In this\npaper, we ask whether results and claims from four recent BERT-based metrics\ncan be reproduced. We find that reproduction of claims and results often fails\nbecause of (i) heavy undocumented preprocessing involved in the metrics, (ii)\nmissing code and (iii) reporting weaker results for the baseline metrics. (iv)\nIn one case, the problem stems from correlating not to human scores but to a\nwrong column in the csv file, inflating scores by 5 points. Motivated by the\nimpact of preprocessing, we then conduct a second study where we examine its\neffects more closely (for one of the metrics). We find that preprocessing can\nhave large effects, especially for highly inflectional languages. In this case,\nthe effect of preprocessing may be larger than the effect of the aggregation\nmechanism (e.g., greedy alignment vs. Word Mover Distance).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belouadi_J/0/1/0/all/0/1\">Jonas Belouadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-augmented cross-lingual synthesis in a teacher-student framework. (arXiv:2204.00061v1 [cs.SD])","link":"http://arxiv.org/abs/2204.00061","description":"<p>Cross-lingual synthesis can be defined as the task of letting a speaker\ngenerate fluent synthetic speech in another language. This is a challenging\ntask, and resulting speech can suffer from reduced naturalness, accented\nspeech, and/or loss of essential voice characteristics. Previous research shows\nthat many models appear to have insufficient generalization capabilities to\nperform well on every of these cross-lingual aspects. To overcome these\ngeneralization problems, we propose to apply the teacher-student paradigm to\ncross-lingual synthesis. While a teacher model is commonly used to produce\nteacher forced data, we propose to also use it to produce augmented data of\nunseen speaker-language pairs, where the aim is to retain essential speaker\ncharacteristics. Both sets of data are then used for student model training,\nwhich is trained to retain the naturalness and prosodic variation present in\nthe teacher forced data, while learning the speaker identity from the augmented\ndata. Some modifications to the student model are proposed to make the\nseparation of teacher forced and augmented data more straightforward. Results\nshow that the proposed approach improves the retention of speaker\ncharacteristics in the speech, while managing to retain high levels of\nnaturalness and prosodic variation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korte_M/0/1/0/all/0/1\">Marcel de Korte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaebok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunikoshi_A/0/1/0/all/0/1\">Aki Kunikoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adigwe_A/0/1/0/all/0/1\">Adaeze Adigwe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klabbers_E/0/1/0/all/0/1\">Esther Klabbers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filter-based Discriminative Autoencoders for Children Speech Recognition. (arXiv:2204.00164v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00164","description":"<p>Children speech recognition is indispensable but challenging due to the\ndiversity of children's speech. In this paper, we propose a filter-based\ndiscriminative autoencoder for acoustic modeling. To filter out the influence\nof various speaker types and pitches, auxiliary information of the speaker and\npitch features is input into the encoder together with the acoustic features to\ngenerate phonetic embeddings. In the training phase, the decoder uses the\nauxiliary information and the phonetic embedding extracted by the encoder to\nreconstruct the input acoustic features. The autoencoder is trained by\nsimultaneously minimizing the ASR loss and feature reconstruction error. The\nframework can make the phonetic embedding purer, resulting in more accurate\nsenone (triphone-state) scores. Evaluated on the test set of the CMU Kids\ncorpus, our system achieves a 7.8% relative WER reduction compared to the\nbaseline system. In the domain adaptation experiment, our system also\noutperforms the baseline system on the British-accent PF-STAR task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1\">Chiang-Lin Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Pre-trained Language Models End-to-end Few-shot Learners with Contrastive Prompt Tuning. (arXiv:2204.00166v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00166","description":"<p>Pre-trained Language Models (PLMs) have achieved remarkable performance for\nvarious language understanding tasks in IR systems, which require the\nfine-tuning process based on labeled training data. For low-resource scenarios,\nprompt-based learning for PLMs exploits prompts as task guidance and turns\ndownstream tasks into masked language problems for effective few-shot\nfine-tuning. In most existing approaches, the high performance of prompt-based\nlearning heavily relies on handcrafted prompts and verbalizers, which may limit\nthe application of such approaches in real-world scenarios. To solve this\nissue, we present CP-Tuning, the first end-to-end Contrastive Prompt Tuning\nframework for fine-tuning PLMs without any manual engineering of task-specific\nprompts and verbalizers. It is integrated with the task-invariant continuous\nprompt encoding technique with fully trainable prompt parameters. We further\npropose the pair-wise cost-sensitive contrastive learning procedure to optimize\nthe model in order to achieve verbalizer-free class mapping and enhance the\ntask-invariance of prompts. It explicitly learns to distinguish different\nclasses and makes the decision boundary smoother by assigning different costs\nto easy and hard cases. Experiments over a variety of language understanding\ntasks used in IR systems and different PLMs show that CP-Tuning outperforms\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fuli Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InterAug: Augmenting Noisy Intermediate Predictions for CTC-based ASR. (arXiv:2204.00174v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00174","description":"<p>This paper proposes InterAug: a novel training method for CTC-based ASR using\naugmented intermediate representations for conditioning. The proposed method\nexploits the conditioning framework of self-conditioned CTC to train robust\nmodels by conditioning with \"noisy\" intermediate predictions. During the\ntraining, intermediate predictions are changed to incorrect intermediate\npredictions, and fed into the next layer for conditioning. The subsequent\nlayers are trained to correct the incorrect intermediate predictions with the\nintermediate losses. By repeating the augmentation and the correction,\niterative refinements, which generally require a special decoder, can be\nrealized only with the audio encoder. To produce noisy intermediate\npredictions, we also introduce new augmentation: intermediate feature space\naugmentation and intermediate token space augmentation that are designed to\nsimulate typical errors. The combination of the proposed InterAug framework\nwith new augmentation allows explicit training of the robust audio encoders. In\nexperiments using augmentations simulating deletion, insertion, and\nsubstitution error, we confirmed that the trained model acquires robustness to\neach error, boosting the speech recognition performance of the strong\nself-conditioned CTC baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakagome_Y/0/1/0/all/0/1\">Yu Nakagome</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komatsu_T/0/1/0/all/0/1\">Tatsuya Komatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_Y/0/1/0/all/0/1\">Yusuke Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichimura_S/0/1/0/all/0/1\">Shuta Ichimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kida_Y/0/1/0/all/0/1\">Yusuke Kida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-sequence Intermediate Conditioning for CTC-based ASR. (arXiv:2204.00175v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00175","description":"<p>End-to-end automatic speech recognition (ASR) directly maps input speech to a\ncharacter sequence without using pronunciation lexica. However, in languages\nwith thousands of characters, such as Japanese and Mandarin, modeling all these\ncharacters is problematic due to data scarcity. To alleviate the problem, we\npropose a multi-task learning model with explicit interaction between\ncharacters and syllables by utilizing Self-conditioned connectionist temporal\nclassification (CTC) technique. While the original Self-conditioned CTC\nestimates character-level intermediate predictions by applying auxiliary CTC\nlosses to a set of intermediate layers, the proposed method additionally\nestimates syllable-level intermediate predictions in another set of\nintermediate layers. The character-level and syllable-level predictions are\nalternately used as conditioning features to deal with mutual dependency\nbetween characters and syllables. Experimental results on Japanese and Mandarin\ndatasets show that the proposed multi-sequence intermediate conditioning\noutperformed the conventional multi-task-based and Self-conditioned CTC-based\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fujita_Y/0/1/0/all/0/1\">Yusuke Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komatsu_T/0/1/0/all/0/1\">Tatsuya Komatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kida_Y/0/1/0/all/0/1\">Yusuke Kida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Intermediates Improve CTC Inference. (arXiv:2204.00176v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00176","description":"<p>This paper proposes a method for improved CTC inference with searched\nintermediates and multi-pass conditioning. The paper first formulates\nself-conditioned CTC as a probabilistic model with an intermediate prediction\nas a latent representation and provides a tractable conditioning framework. We\nthen propose two new conditioning methods based on the new formulation: (1)\nSearched intermediate conditioning that refines intermediate predictions with\nbeam-search, (2) Multi-pass conditioning that uses predictions of previous\ninference for conditioning the next inference. These new approaches enable\nbetter conditioning than the original self-conditioned CTC during inference and\nimprove the final performance. Experiments with the LibriSpeech dataset show\nrelative 3%/12% performance improvement at the maximum in test clean/other sets\ncompared to the original self-conditioned CTC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Komatsu_T/0/1/0/all/0/1\">Tatsuya Komatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_Y/0/1/0/all/0/1\">Yusuke Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaesong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lukas Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kida_Y/0/1/0/all/0/1\">Yusuke Kida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Enhanced Contrastive Learning for Radiology Findings Summarization. (arXiv:2204.00203v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00203","description":"<p>The impression section of a radiology report summarizes the most prominent\nobservation from the findings section and is the most important section for\nradiologists to communicate to physicians. Summarizing findings is\ntime-consuming and can be prone to error for inexperienced radiologists, and\nthus automatic impression generation has attracted substantial attention. With\nthe encoder-decoder framework, most previous studies explore incorporating\nextra knowledge (e.g., static pre-defined clinical ontologies or extra\nbackground information). Yet, they encode such knowledge by a separate encoder\nto treat it as an extra input to their models, which is limited in leveraging\ntheir relations with the original findings. To address the limitation, we\npropose a unified framework for exploiting both extra knowledge and the\noriginal findings in an integrated way so that the critical information (i.e.,\nkey words and their relations) can be extracted in an appropriate way to\nfacilitate impression generation. In detail, for each input findings, it is\nencoded by a text encoder, and a graph is constructed through its entities and\ndependency tree. Then, a graph encoder (e.g., graph neural networks (GNNs)) is\nadopted to model relation information in the constructed graph. Finally, to\nemphasize the key words in the findings, contrastive learning is introduced to\nmap positive samples (constructed by masking non-key words) closer and push\napart negative ones (constructed by masking key words). The experimental\nresults on OpenI and MIMIC-CXR confirm the effectiveness of our proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinpeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tsung-Hui Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect and Analysis of Large-scale Language Model Rescoring on Competitive ASR Systems. (arXiv:2204.00212v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00212","description":"<p>Large-scale language models (LLMs) such as GPT-2, BERT and RoBERTa have been\nsuccessfully applied to ASR N-best rescoring. However, whether or how they can\nbenefit competitive, near state-of-the-art ASR systems remains unexplored. In\nthis study, we incorporate LLM rescoring into one of the most competitive ASR\nbaselines: the Conformer-Transducer model. We demonstrate that consistent\nimprovement is achieved by the LLM's bidirectionality, pretraining, in-domain\nfinetuning and context augmentation. Furthermore, our lexical analysis sheds\nlight on how each of these components may be contributing to the ASR\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Udagawa_T/0/1/0/all/0/1\">Takuma Udagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1\">Masayuki Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurata_G/0/1/0/all/0/1\">Gakuto Kurata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itoh_N/0/1/0/all/0/1\">Nobuyasu Itoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1\">George Saon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Multi-speaker ASR with Independent Vector Analysis. (arXiv:2204.00218v1 [eess.AS])","link":"http://arxiv.org/abs/2204.00218","description":"<p>We develop an end-to-end system for multi-channel, multi-speaker automatic\nspeech recognition. We propose a frontend for joint source separation and\ndereverberation based on the independent vector analysis (IVA) paradigm. It\nuses the fast and stable iterative source steering algorithm together with a\nneural source model. The parameters from the ASR module and the neural source\nmodel are optimized jointly from the ASR loss itself. We demonstrate\ncompetitive performance with previous systems using neural beamforming\nfrontends. First, we explore the trade-offs when using various number of\nchannels for training and testing. Second, we demonstrate that the proposed IVA\nfrontend performs well on noisy data, even when trained on clean mixtures only.\nFurthermore, it extends without retraining to the separation of more speakers,\nwhich is demonstrated on mixtures of three and four speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Scheibler_R/0/1/0/all/0/1\">Robin Scheibler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wangyou Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_Y/0/1/0/all/0/1\">Yanmin Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NC-DRE: Leveraging Non-entity Clue Information for Document-level Relation Extraction. (arXiv:2204.00255v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00255","description":"<p>Document-level relation extraction (RE), which requires reasoning on multiple\nentities in different sentences to identify complex inter-sentence relations,\nis more challenging than sentence-level RE. To extract the complex\ninter-sentence relations, previous studies usually employ graph neural networks\n(GNN) to perform inference upon heterogeneous document-graphs. Despite their\ngreat successes, these graph-based methods, which normally only consider the\nwords within the mentions in the process of building graphs and reasoning, tend\nto ignore the non-entity clue words that are not in the mentions but provide\nimportant clue information for relation reasoning. To alleviate this problem,\nwe treat graph-based document-level RE models as an encoder-decoder framework,\nwhich typically uses a pre-trained language model as the encoder and a GNN\nmodel as the decoder, and propose a novel graph-based model NC-DRE that\nintroduces decoder-to-encoder attention mechanism to leverage Non-entity Clue\ninformation for Document-level Relation Extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yidong Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multifaceted Improvements for Conversational Open-Domain Question Answering. (arXiv:2204.00266v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00266","description":"<p>Open-domain question answering (OpenQA) is an important branch of textual QA\nwhich discovers answers for the given questions based on a large number of\nunstructured documents. Effectively mining correct answers from the open-domain\nsources still has a fair way to go. Existing OpenQA systems might suffer from\nthe issues of question complexity and ambiguity, as well as insufficient\nbackground knowledge. Recently, conversational OpenQA is proposed to address\nthese issues with the abundant contextual information in the conversation.\nPromising as it might be, there exist several fundamental limitations including\nthe inaccurate question understanding, the coarse ranking for passage\nselection, and the inconsistent usage of golden passage in the training and\ninference phases. To alleviate these limitations, in this paper, we propose a\nframework with Multifaceted Improvements for Conversational open-domain\nQuestion Answering (MICQA). Specifically, MICQA has three significant\nadvantages. First, the proposed KL-divergence based regularization is able to\nlead to a better question understanding for retrieval and answer reading.\nSecond, the added post-ranker module can push more relevant passages to the top\nplacements and be selected for reader with a two-aspect constrains. Third, the\nwell designed curriculum learning strategy effectively narrows the gap between\nthe golden passage settings of training and inference, and encourages the\nreader to find true answer without the golden passage assistance. Extensive\nexperiments conducted on the publicly available dataset OR-QuAC demonstrate the\nsuperiority of MICQA over the state-of-the-art model in conversational OpenQA\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tingting Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yixuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuyu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Intervention Approval in Clinical Trials through Multi-Document Summarization. (arXiv:2204.00290v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00290","description":"<p>Clinical trials offer a fundamental opportunity to discover new treatments\nand advance the medical knowledge. However, the uncertainty of the outcome of a\ntrial can lead to unforeseen costs and setbacks. In this study, we propose a\nnew method to predict the effectiveness of an intervention in a clinical trial.\nOur method relies on generating an informative summary from multiple documents\navailable in the literature about the intervention under study. Specifically,\nour method first gathers all the abstracts of PubMed articles related to the\nintervention. Then, an evidence sentence, which conveys information about the\neffectiveness of the intervention, is extracted automatically from each\nabstract. Based on the set of evidence sentences extracted from the abstracts,\na short summary about the intervention is constructed. Finally, the produced\nsummaries are used to train a BERT-based classifier, in order to infer the\neffectiveness of an intervention. To evaluate our proposed method, we introduce\na new dataset which is a collection of clinical trials together with their\nassociated PubMed articles. Our experiments, demonstrate the effectiveness of\nproducing short informative summaries and using them to predict the\neffectiveness of an intervention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katsimpras_G/0/1/0/all/0/1\">Georgios Katsimpras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-To-Speech Data Augmentation for Low Resource Speech Recognition. (arXiv:2204.00291v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00291","description":"<p>Nowadays, the main problem of deep learning techniques used in the\ndevelopment of automatic speech recognition (ASR) models is the lack of\ntranscribed data. The goal of this research is to propose a new data\naugmentation method to improve ASR models for agglutinative and low-resource\nlanguages. This novel data augmentation method generates both synthetic text\nand synthetic audio. Some experiments were conducted using the corpus of the\nQuechua language, which is an agglutinative and low-resource language. In this\nstudy, a sequence-to-sequence (seq2seq) model was applied to generate synthetic\ntext, in addition to generating synthetic speech using a text-to-speech (TTS)\nmodel for Quechua. The results show that the new data augmentation method works\nwell to improve the ASR model for Quechua. In this research, an 8.73%\nimprovement in the word-error-rate (WER) of the ASR model is obtained using a\ncombination of synthetic text and synthetic speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zevallos_R/0/1/0/all/0/1\">Rodolfo Zevallos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PriMock57: A Dataset Of Primary Care Mock Consultations. (arXiv:2204.00333v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00333","description":"<p>Recent advances in Automatic Speech Recognition (ASR) have made it possible\nto reliably produce automatic transcripts of clinician-patient conversations.\nHowever, access to clinical datasets is heavily restricted due to patient\nprivacy, thus slowing down normal research practices. We detail the development\nof a public access, high quality dataset comprising of57 mocked primary care\nconsultations, including audio recordings, their manual utterance-level\ntranscriptions, and the associated consultation notes. Our work illustrates how\nthe dataset can be used as a benchmark for conversational medical ASR as well\nas consultation note generation from transcripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korfiatis_A/0/1/0/all/0/1\">Alex Papadopoulos Korfiatis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moramarco_F/0/1/0/all/0/1\">Francesco Moramarco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarac_R/0/1/0/all/0/1\">Radmila Sarac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savkov_A/0/1/0/all/0/1\">Aleksandar Savkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cyberbullying detection across social media platforms via platform-aware adversarial encoding. (arXiv:2204.00334v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00334","description":"<p>Despite the increasing interest in cyberbullying detection, existing efforts\nhave largely been limited to experiments on a single platform and their\ngeneralisability across different social media platforms have received less\nattention. We propose XP-CB, a novel cross-platform framework based on\nTransformers and adversarial learning. XP-CB can enhance a Transformer\nleveraging unlabelled data from the source and target platforms to come up with\na common representation while preventing platform-specific training. To\nvalidate our proposed framework, we experiment on cyberbullying datasets from\nthree different platforms through six cross-platform configurations, showing\nits effectiveness with both BERT and RoBERTa as the underlying Transformer\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_P/0/1/0/all/0/1\">Peiling Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WavFT: Acoustic model finetuning with labelled and unlabelled data. (arXiv:2204.00348v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00348","description":"<p>Unsupervised and self-supervised learning methods have leveraged unlabelled\ndata to improve the pretrained models. However, these methods need\nsignificantly large amount of unlabelled data and the computational cost of\ntraining models with such large amount of data can be prohibitively high. We\naddress this issue by using unlabelled data during finetuning, instead of\npretraining. We propose acoustic model finetuning (FT) using labelled and\nunlabelled data. The model is jointly trained to learn representations to\nclassify senones, as well as learn contextual acoustic representations. Our\ntraining objective is a combination of cross entropy loss, suitable for\nclassification task, and contrastive loss, suitable to learn acoustic\nrepresentations. The proposed approach outperforms conventional finetuning with\n11.2% and 9.19% word error rate relative (WERR) reduction on Gujarati and\nBengali languages respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_U/0/1/0/all/0/1\">Utkarsh Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_V/0/1/0/all/0/1\">Vikas Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1\">Rupesh R. Mehta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Shallow Discourse Parsing in the PDTB-3: Handling Intra-sentential Implicits. (arXiv:2204.00350v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00350","description":"<p>In the PDTB-3, several thousand implicit discourse relations were newly\nannotated \\textit{within} individual sentences, adding to the over 15,000\nimplicit relations annotated \\textit{across} adjacent sentences in the PDTB-2.\nGiven that the position of the arguments to these \\textit{intra-sentential\nimplicits} is no longer as well-defined as with \\textit{inter-sentential\nimplicits}, a discourse parser must identify both their location and their\nsense. That is the focus of the current work. The paper provides a\ncomprehensive analysis of our results, showcasing model performance under\ndifferent scenarios, pointing out limitations and noting future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webber_B/0/1/0/all/0/1\">Bonnie Webber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Biomedical Term Clustering by Learning Fine-grained Term Representations. (arXiv:2204.00391v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00391","description":"<p>Term clustering is important in biomedical knowledge graph construction.\nUsing similarities between terms embedding is helpful for term clustering.\nState-of-the-art term embeddings leverage pretrained language models to encode\nterms, and use synonyms and relation knowledge from knowledge graphs to guide\ncontrastive learning. These embeddings provide close embeddings for terms\nbelonging to the same concept. However, from our probing experiments, these\nembeddings are not sensitive to minor textual differences which leads to\nfailure for biomedical term clustering. To alleviate this problem, we adjust\nthe sampling strategy in pretraining term embeddings by providing dynamic hard\npositive and negative samples during contrastive learning to learn fine-grained\nrepresentations which result in better biomedical term clustering. We name our\nproposed method as CODER++, and it has been applied in clustering biomedical\nconcepts in the newly released Biomedical Knowledge Graph named BIOS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Sihang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Speech Emotion Recognition Transformers for Linguistic Knowledge. (arXiv:2204.00400v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00400","description":"<p>Large, pre-trained neural networks consisting of self-attention layers\n(transformers) have recently achieved state-of-the-art results on several\nspeech emotion recognition (SER) datasets. These models are typically\npre-trained in self-supervised manner with the goal to improve automatic speech\nrecognition performance -- and thus, to understand linguistic information. In\nthis work, we investigate the extent in which this information is exploited\nduring SER fine-tuning. Using a reproducible methodology based on open-source\ntools, we synthesise prosodically neutral speech utterances while varying the\nsentiment of the text. Valence predictions of the transformer model are very\nreactive to positive and negative sentiment content, as well as negations, but\nnot to intensifiers or reducers, while none of those linguistic features impact\narousal or dominance. These findings show that transformers can successfully\nleverage linguistic information to improve their valence predictions, and that\nlinguistic analysis should be included in their testing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Triantafyllopoulos_A/0/1/0/all/0/1\">Andreas Triantafyllopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Johannes Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wierstorf_H/0/1/0/all/0/1\">Hagen Wierstorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Maximilian Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichel_U/0/1/0/all/0/1\">Uwe Reichel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyben_F/0/1/0/all/0/1\">Florian Eyben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burkhardt_F/0/1/0/all/0/1\">Felix Burkhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Pruning Learns Compact and Accurate Models. (arXiv:2204.00408v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00408","description":"<p>The growing size of neural language models has led to increased attention in\nmodel compression. The two predominant approaches are pruning, which gradually\nremoves weights from a pre-trained model, and distillation, which trains a\nsmaller compact model to match a larger one. Pruning methods can significantly\nreduce the model size but hardly achieve large speedups as distillation.\nHowever, distillation methods require large amounts of unlabeled data and are\nexpensive to train. In this work, we propose a task-specific structured pruning\nmethod CoFi (Coarse- and Fine-grained Pruning), which delivers highly\nparallelizable subnetworks and matches the distillation methods in both\naccuracy and latency, without resorting to any unlabeled data. Our key insight\nis to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads\nand hidden units) modules, which controls the pruning decision of each\nparameter with masks of different granularity. We also devise a layerwise\ndistillation strategy to transfer knowledge from unpruned to pruned models\nduring optimization. Our experiments on GLUE and SQuAD datasets show that CoFi\nyields models with over 10x speedups with a small accuracy drop, showing its\neffectiveness and efficiency compared to previous pruning and distillation\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Mengzhou Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sense disambiguation of compound constituents. (arXiv:2204.00429v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00429","description":"<p>In distributional semantic accounts of the meaning of noun-noun compounds\n(e.g. starfish, bank account, houseboat) the important role of constituent\npolysemy remains largely unaddressed(cf. the meaning of star in starfish vs.\nstar cluster vs. star athlete). Instead of semantic vectors that average over\nthe different meanings of a constituent, disambiguated vectors of the\nconstituents would be needed in order to see what these more specific\nconstituent meanings contribute to the meaning of the compound as a whole. This\npaper presents a novel approach to this specific problem of word sense\ndisambiguation: set expansion. We build on the approach developed by Mahabal et\nal. (2018) which was originally designed to solve the analogy problem. We\nmodified their method in such a way that it can address the problem of sense\ndisambiguation of compound constituents. The results of experiments with a data\nset of almost 9000 compounds (LADEC, Gagn\\'e et al. 2019) suggest that this\napproach is successful, yet the success is sensitive to the frequency with\nwhich the compounds are attested.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schackow_C/0/1/0/all/0/1\">Carlo Schackow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conrad_S/0/1/0/all/0/1\">Stefan Conrad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plag_I/0/1/0/all/0/1\">Ingo Plag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios. (arXiv:2204.00436v1 [eess.AS])","link":"http://arxiv.org/abs/2204.00436","description":"<p>Adaptive text to speech (TTS) can synthesize new voices in zero-shot\nscenarios efficiently, by using a well-trained source TTS model without\nadapting it on the speech data of new speakers. Considering seen and unseen\nspeakers have diverse characteristics, zero-shot adaptive TTS requires strong\ngeneralization ability on speaker characteristics, which brings modeling\nchallenges. In this paper, we develop AdaSpeech 4, a zero-shot adaptive TTS\nsystem for high-quality speech synthesis. We model the speaker characteristics\nsystematically to improve the generalization on new speakers. Generally, the\nmodeling of speaker characteristics can be categorized into three steps:\nextracting speaker representation, taking this speaker representation as\ncondition, and synthesizing speech/mel-spectrogram given this speaker\nrepresentation. Accordingly, we improve the modeling in three steps: 1) To\nextract speaker representation with better generalization, we factorize the\nspeaker characteristics into basis vectors and extract speaker representation\nby weighted combining of these basis vectors through attention. 2) We leverage\nconditional layer normalization to integrate the extracted speaker\nrepresentation to TTS model. 3) We propose a novel supervision loss based on\nthe distribution of basis vectors to maintain the corresponding speaker\ncharacteristics in generated mel-spectrograms. Without any fine-tuning,\nAdaSpeech 4 achieves better voice quality and similarity than baselines in\nmultiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yihan Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Evaluation and Correlation with Automatic Metrics in Consultation Note Generation. (arXiv:2204.00447v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00447","description":"<p>In recent years, machine learning models have rapidly become better at\ngenerating clinical consultation notes; yet, there is little work on how to\nproperly evaluate the generated consultation notes to understand the impact\nthey may have on both the clinician using them and the patient's clinical\nsafety. To address this we present an extensive human evaluation study of\nconsultation notes where 5 clinicians (i) listen to 57 mock consultations, (ii)\nwrite their own notes, (iii) post-edit a number of automatically generated\nnotes, and (iv) extract all the errors, both quantitative and qualitative. We\nthen carry out a correlation study with 18 automatic quality metrics and the\nhuman judgements. We find that a simple, character-based Levenshtein distance\nmetric performs on par if not better than common model-based metrics like\nBertScore. All our findings and annotations are open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moramarco_F/0/1/0/all/0/1\">Francesco Moramarco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korfiatis_A/0/1/0/all/0/1\">Alex Papadopoulos Korfiatis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_M/0/1/0/all/0/1\">Mark Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juric_D/0/1/0/all/0/1\">Damir Juric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flann_J/0/1/0/all/0/1\">Jack Flann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_E/0/1/0/all/0/1\">Ehud Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belz_A/0/1/0/all/0/1\">Anya Belz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savkov_A/0/1/0/all/0/1\">Aleksandar Savkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Cross-lingual Aphasia Detection using Automatic Speech Recognition. (arXiv:2204.00448v1 [cs.LG])","link":"http://arxiv.org/abs/2204.00448","description":"<p>Aphasia is a common speech and language disorder, typically caused by a brain\ninjury or a stroke, that affects millions of people worldwide. Detecting and\nassessing Aphasia in patients is a difficult, time-consuming process, and\nnumerous attempts to automate it have been made, the most successful using\nmachine learning models trained on aphasic speech data. Like in many medical\napplications, aphasic speech data is scarce and the problem is exacerbated in\nso-called \"low resource\" languages, which are, for this task, most languages\nexcluding English. We attempt to leverage available data in English and achieve\nzero-shot aphasia detection in low-resource languages such as Greek and French,\nby using language-agnostic linguistic features. Current cross-lingual aphasia\ndetection approaches rely on manually extracted transcripts. We propose an\nend-to-end pipeline using pre-trained Automatic Speech Recognition (ASR) models\nthat share cross-lingual speech representations and are fine-tuned for our\ndesired low-resource languages. To further boost our ASR model's performance,\nwe also combine it with a language model. We show that our ASR-based end-to-end\npipeline offers comparable results to previous setups using human-annotated\ntranscripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatzoudis_G/0/1/0/all/0/1\">Gerasimos Chatzoudis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plitsis_M/0/1/0/all/0/1\">Manos Plitsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamouli_S/0/1/0/all/0/1\">Spyridoula Stamouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimou_A/0/1/0/all/0/1\">Athanasia-Lida Dimou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsamanis_A/0/1/0/all/0/1\">Athanasios Katsamanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsouros_V/0/1/0/all/0/1\">Vassilis Katsouros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Fake News Detection with Knowledge-Enhanced Language Models. (arXiv:2204.00458v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00458","description":"<p>Recent advances in fake news detection have exploited the success of\nlarge-scale pre-trained language models (PLMs). The predominant\nstate-of-the-art approaches are based on fine-tuning PLMs on labelled fake news\ndatasets. However, large-scale PLMs are generally not trained on structured\nfactual data and hence may not possess priors that are grounded in factually\naccurate knowledge. The use of existing knowledge bases (KBs) with rich\nhuman-curated factual information has thus the potential to make fake news\ndetection more effective and robust. In this paper, we investigate the impact\nof knowledge integration into PLMs for fake news detection. We study several\nstate-of-the-art approaches for knowledge integration, mostly using Wikidata as\nKB, on two popular fake news datasets - LIAR, a politics-based dataset, and\nCOVID-19, a dataset of messages posted on social media relating to the COVID-19\npandemic. Our experiments show that knowledge-enhanced models can significantly\nimprove fake news detection on LIAR where the KB is relevant and up-to-date.\nThe mixed results on COVID-19 highlight the reliance on stylistic features and\nthe importance of domain specific and current KBs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whitehouse_C/0/1/0/all/0/1\">Chenxi Whitehouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyde_T/0/1/0/all/0/1\">Tillman Weyde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komninos_N/0/1/0/all/0/1\">Nikos Komninos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Determines the Adequacy of the Mode and the Tractability of Decoding in Sequence-to-Sequence Models. (arXiv:2204.00471v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00471","description":"<p>In many natural language processing (NLP) tasks the same input (e.g. source\nsentence) can have multiple possible outputs (e.g. translations). To analyze\nhow this ambiguity (also known as intrinsic uncertainty) shapes the\ndistribution learned by neural sequence models we measure sentence-level\nuncertainty by computing the degree of overlap between references in\nmulti-reference test sets from two different NLP tasks: machine translation\n(MT) and grammatical error correction (GEC). At both the sentence- and the\ntask-level, intrinsic uncertainty has major implications for various aspects of\nsearch such as the inductive biases in beam search and the complexity of exact\nsearch. In particular, we show that well-known pathologies such as a high\nnumber of beam search errors, the inadequacy of the mode, and the drop in\nsystem performance with large beam sizes apply to tasks with high level of\nambiguity such as MT but not to less uncertain tasks such as GEC. Furthermore,\nwe propose a novel exact $n$-best search algorithm for neural sequence models,\nand show that intrinsic uncertainty affects model uncertainty as the model\ntends to overly spread out the probability mass for uncertain tasks and\nsentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stahlberg_F/0/1/0/all/0/1\">Felix Stahlberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1\">Ilia Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shankar Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Text-to-SQL Capabilities of Large Language Models. (arXiv:2204.00498v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00498","description":"<p>We perform an empirical evaluation of Text-to-SQL capabilities of the Codex\nlanguage model. We find that, without any finetuning, Codex is a strong\nbaseline on the Spider benchmark; we also analyze the failure modes of Codex in\nthis setting. Furthermore, we demonstrate on the GeoQuery and Scholar\nbenchmarks that a small number of in-domain examples provided in the prompt\nenables Codex to perform better than state-of-the-art models finetuned on such\nfew-shot examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajkumar_N/0/1/0/all/0/1\">Nitarshan Rajkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Disentangled Representations of Negation and Uncertainty. (arXiv:2204.00511v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00511","description":"<p>Negation and uncertainty modeling are long-standing tasks in natural language\nprocessing. Linguistic theory postulates that expressions of negation and\nuncertainty are semantically independent from each other and the content they\nmodify. However, previous works on representation learning do not explicitly\nmodel this independence. We therefore attempt to disentangle the\nrepresentations of negation, uncertainty, and content using a Variational\nAutoencoder. We find that simply supervising the latent representations results\nin good disentanglement, but auxiliary objectives based on adversarial learning\nand mutual information minimization can provide additional disentanglement\ngains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasilakes_J/0/1/0/all/0/1\">Jake Vasilakes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerva_C/0/1/0/all/0/1\">Chrysoula Zerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miwa_M/0/1/0/all/0/1\">Makoto Miwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Integration of Speech Recognition, Speech Enhancement, and Self-Supervised Learning Representation. (arXiv:2204.00540v1 [cs.SD])","link":"http://arxiv.org/abs/2204.00540","description":"<p>This work presents our end-to-end (E2E) automatic speech recognition (ASR)\nmodel targetting at robust speech recognition, called Integraded speech\nRecognition with enhanced speech Input for Self-supervised learning\nrepresentation (IRIS). Compared with conventional E2E ASR models, the proposed\nE2E model integrates two important modules including a speech enhancement (SE)\nmodule and a self-supervised learning representation (SSLR) module. The SE\nmodule enhances the noisy speech. Then the SSLR module extracts features from\nenhanced speech to be used for speech recognition (ASR). To train the proposed\nmodel, we establish an efficient learning scheme. Evaluation results on the\nmonaural CHiME-4 task show that the IRIS model achieves the best performance\nreported in the literature for the single-channel CHiME-4 benchmark (2.0% for\nthe real development and 3.9% for the real test) thanks to the powerful\npre-trained SSLR module and the fine-tuned SE module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maekaku_T/0/1/0/all/0/1\">Takashi Maekaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_Y/0/1/0/all/0/1\">Yuya Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Multimodal Approach for Studying the Dynamics of Curiosity in Small Group Learning. (arXiv:2204.00545v1 [cs.LG])","link":"http://arxiv.org/abs/2204.00545","description":"<p>Curiosity is a vital metacognitive skill in educational contexts, leading to\ncreativity, and a love of learning. And while many school systems increasingly\nundercut curiosity by teaching to the test, teachers are increasingly\ninterested in how to evoke curiosity in their students to prepare them for a\nworld in which lifelong learning and reskilling will be more and more\nimportant. One aspect of curiosity that has received little attention, however,\nis the role of peers in eliciting curiosity. We present what we believe to be\nthe first theoretical framework that articulates an integrated socio-cognitive\naccount of curiosity that ties observable behaviors in peers to underlying\ncuriosity states. We make a bipartite distinction between individual and\ninterpersonal functions that contribute to curiosity, and multimodal behaviors\nthat fulfill these functions. We validate the proposed framework by leveraging\na longitudinal latent variable modeling approach. Findings confirm a positive\npredictive relationship between the latent variables of individual and\ninterpersonal functions and curiosity, with the interpersonal functions\nexercising a comparatively stronger influence. Prominent behavioral\nrealizations of these functions are also discovered in a data-driven manner. We\ninstantiate the proposed theoretical framework in a set of strategies and\ntactics that can be incorporated into learning technologies to indicate, evoke,\nand scaffold curiosity. This work is a step towards designing learning\ntechnologies that can recognize and evoke moment-by-moment curiosity during\nlearning in social contexts and towards a more complete multimodal learning\nanalytics. The underlying rationale is applicable more generally for developing\ncomputer support for other metacognitive and socio-emotional skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_T/0/1/0/all/0/1\">Tanmay Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zhen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassell_J/0/1/0/all/0/1\">Justine Cassell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified and Effective Ensemble Knowledge Distillation. (arXiv:2204.00548v1 [cs.LG])","link":"http://arxiv.org/abs/2204.00548","description":"<p>Ensemble knowledge distillation can extract knowledge from multiple teacher\nmodels and encode it into a single student model. Many existing methods learn\nand distill the student model on labeled data only. However, the teacher models\nare usually learned on the same labeled data, and their predictions have high\ncorrelations with groudtruth labels. Thus, they cannot provide sufficient\nknowledge complementary to task labels for student teaching. Distilling on\nunseen unlabeled data has the potential to enhance the knowledge transfer from\nthe teachers to the student. In this paper, we propose a unified and effective\nensemble knowledge distillation method that distills a single student model\nfrom an ensemble of teacher models on both labeled and unlabeled data. Since\ndifferent teachers may have diverse prediction correctness on the same sample,\non labeled data we weight the predictions of different teachers according to\ntheir correctness. In addition, we weight the distillation loss based on the\noverall prediction correctness of the teacher ensemble to distill high-quality\nknowledge. On unlabeled data, there is no groundtruth to evaluate prediction\ncorrectness. Fortunately, the disagreement among teachers is an indication of\nsample hardness, and thereby we weight the distillation loss based on teachers'\ndisagreement to emphasize knowledge distillation on important samples.\nExtensive experiments on four datasets show the effectiveness of our proposed\nensemble distillation method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nowruz at SemEval-2022 Task 7: Tackling Cloze Tests with Transformers and Ordinal Regression. (arXiv:2204.00556v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00556","description":"<p>This paper outlines the system using which team Nowruz participated in\nSemEval 2022 Task 7 Identifying Plausible Clarifications of Implicit and\nUnderspecified Phrases for both subtasks A and B. Using a pre-trained\ntransformer as a backbone, the model targeted the task of multi-task\nclassification and ranking in the context of finding the best fillers for a\ncloze task related to instructional texts on the website Wikihow.\n</p>\n<p>The system employed a combination of two ordinal regression components to\ntackle this task in a multi-task learning scenario. According to the official\nleaderboard of the shared task, this system was ranked 5th in the ranking and\n7th in the classification subtasks out of 21 participating teams. With\nadditional experiments, the models have since been further optimised.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nouriborji_M/0/1/0/all/0/1\">Mohammadmahdi Nouriborji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohanian_O/0/1/0/all/0/1\">Omid Rohanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David Clifton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task RNN-T with Semantic Decoder for Streamable Spoken Language Understanding. (arXiv:2204.00558v1 [cs.CL])","link":"http://arxiv.org/abs/2204.00558","description":"<p>End-to-end Spoken Language Understanding (E2E SLU) has attracted increasing\ninterest due to its advantages of joint optimization and low latency when\ncompared to traditionally cascaded pipelines. Existing E2E SLU models usually\nfollow a two-stage configuration where an Automatic Speech Recognition (ASR)\nnetwork first predicts a transcript which is then passed to a Natural Language\nUnderstanding (NLU) module through an interface to infer semantic labels, such\nas intent and slot tags. This design, however, does not consider the NLU\nposterior while making transcript predictions, nor correct the NLU prediction\nerror immediately by considering the previously predicted word-pieces. In\naddition, the NLU model in the two-stage system is not streamable, as it must\nwait for the audio segments to complete processing, which ultimately impacts\nthe latency of the SLU system. In this work, we propose a streamable multi-task\nsemantic transducer model to address these considerations. Our proposed\narchitecture predicts ASR and NLU labels auto-regressively and uses a semantic\ndecoder to ingest both previously predicted word-pieces and slot tags while\naggregating them through a fusion network. Using an industry scale SLU and a\npublic FSC dataset, we show the proposed model outperforms the two-stage E2E\nSLU model for both ASR and NLU metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xuandi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_F/0/1/0/all/0/1\">Feng-Ju Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radfar_M/0/1/0/all/0/1\">Martin Radfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1\">Grant P. Strimel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathyendra_K/0/1/0/all/0/1\">Kanthashree Mysore Sathyendra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. (arXiv:2204.00598v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00598","description":"<p>Large foundation models can exhibit unique capabilities depending on the\ndomain of data they are trained on. While these domains are generic, they may\nonly barely overlap. For example, visual-language models (VLMs) are trained on\nInternet-scale image captions, but large language models (LMs) are further\ntrained on Internet-scale text with no images (e.g. from spreadsheets, to SAT\nquestions). As a result, these models store different forms of commonsense\nknowledge across different domains. In this work, we show that this model\ndiversity is symbiotic, and can be leveraged to build AI systems with\nstructured Socratic dialogue -- in which new multimodal tasks are formulated as\na guided language-based exchange between different pre-existing foundation\nmodels, without additional finetuning. In the context of egocentric perception,\nwe present a case study of Socratic Models (SMs) that can provide meaningful\nresults for complex tasks such as generating free-form answers to contextual\nquestions about egocentric video, by formulating video Q&amp;A as short story Q&amp;A,\ni.e. summarizing the video into a short story, then answering questions about\nit. Additionally, SMs can generate captions for Internet images, and are\ncompetitive with state-of-the-art on zero-shot video-to-text retrieval with\n42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate how to compose foundation models\nzero-shot to capture new multimodal functionalities, without domain-specific\ndata collection. Prototypes are available at socraticmodels.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Adrian Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welker_S/0/1/0/all/0/1\">Stefan Welker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_A/0/1/0/all/0/1\">Aveek Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindhwani_V/0/1/0/all/0/1\">Vikas Sindhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Johnny Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhoucke_V/0/1/0/all/0/1\">Vincent Vanhoucke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.04631","description":"<p>Machine translation between many languages at once is highly challenging,\nsince training with ground truth requires supervision between all language\npairs, which is difficult to obtain. Our key insight is that, while languages\nmay vary drastically, the underlying visual appearance of the world remains\nconsistent. We introduce a method that uses visual observations to bridge the\ngap between languages, rather than relying on parallel corpora or topological\nproperties of the representations. We train a model that aligns segments of\ntext from different languages if and only if the images associated with them\nare similar and each image in turn is well-aligned with its textual\ndescription. We train our model from scratch on a new dataset of text in over\nfifty languages with accompanying images. Experiments show that our method\noutperforms previous work on unsupervised word and sentence translation using\nretrieval. Code, models and data are available on globetrotter.cs.columbia.edu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suris_D/0/1/0/all/0/1\">D&#xed;dac Sur&#xed;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epstein_D/0/1/0/all/0/1\">Dave Epstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Sensitivity and Stability of Model Interpretations in NLP. (arXiv:2104.08782v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08782","description":"<p>Recent years have witnessed the emergence of a variety of post-hoc\ninterpretations that aim to uncover how natural language processing (NLP)\nmodels make predictions. Despite the surge of new interpretation methods, it\nremains an open problem how to define and quantitatively measure the\nfaithfulness of interpretations, i.e., to what extent interpretations reflect\nthe reasoning process by a model. We propose two new criteria, sensitivity and\nstability, that provide complementary notions of faithfulness to the existed\nremoval-based criteria. Our results show that the conclusion for how faithful\ninterpretations are could vary substantially based on different notions.\nMotivated by the desiderata of sensitivity and stability, we introduce a new\nclass of interpretation methods that adopt techniques from adversarial\nrobustness. Empirical results show that our proposed methods are effective\nunder the new criteria and overcome limitations of gradient-based methods on\nremoval-based criteria. Besides text classification, we also apply\ninterpretation methods and metrics to dependency parsing. Our results shed\nlight on understanding the diverse set of interpretations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Fan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhouxing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comparative evaluation and analysis of three generations of Distributional Semantic Models. (arXiv:2105.09825v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.09825","description":"<p>Distributional semantics has deeply changed in the last decades. First,\npredict models stole the thunder from traditional count ones, and more recently\nboth of them were replaced in many NLP applications by contextualized vectors\nproduced by Transformer neural language models. Although an extensive body of\nresearch has been devoted to Distributional Semantic Model (DSM) evaluation, we\nstill lack a thorough comparison with respect to tested models, semantic tasks,\nand benchmark datasets. Moreover, previous work has mostly focused on\ntask-driven evaluation, instead of exploring the differences between the way\nmodels represent the lexical semantic space. In this paper, we perform a\ncomprehensive evaluation of type distributional vectors, either produced by\nstatic DSMs or obtained by averaging the contextualized vectors generated by\nBERT. First of all, we investigate the performance of embeddings in several\nsemantic tasks, carrying out an in-depth statistical analysis to identify the\nmajor factors influencing the behavior of DSMs. The results show that i.) the\nalleged superiority of predict based models is more apparent than real, and\nsurely not ubiquitous and ii.) static DSMs surpass contextualized\nrepresentations in most out-of-context semantic tasks and datasets.\nFurthermore, we borrow from cognitive neuroscience the methodology of\nRepresentational Similarity Analysis (RSA) to inspect the semantic spaces\ngenerated by distributional models. RSA reveals important differences related\nto the frequency and part-of-speech of lexical items.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1\">Alessandro Lenci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahlgren_M/0/1/0/all/0/1\">Magnus Sahlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeuniaux_P/0/1/0/all/0/1\">Patrick Jeuniaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyllensten_A/0/1/0/all/0/1\">Amaru Cuba Gyllensten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miliani_M/0/1/0/all/0/1\">Martina Miliani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMMS Reloaded: Transformer-based Sense Embeddings for Disambiguation and Beyond. (arXiv:2105.12449v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.12449","description":"<p>Distributional semantics based on neural approaches is a cornerstone of\nNatural Language Processing, with surprising connections to human meaning\nrepresentation as well. Recent Transformer-based Language Models have proven\ncapable of producing contextual word representations that reliably convey\nsense-specific information, simply as a product of self-supervision. Prior work\nhas shown that these contextual representations can be used to accurately\nrepresent large sense inventories as sense embeddings, to the extent that a\ndistance-based solution to Word Sense Disambiguation (WSD) tasks outperforms\nmodels trained specifically for the task. Still, there remains much to\nunderstand on how to use these Neural Language Models (NLMs) to produce sense\nembeddings that can better harness each NLM's meaning representation abilities.\nIn this work we introduce a more principled approach to leverage information\nfrom all layers of NLMs, informed by a probing analysis on 14 NLM variants. We\nalso emphasize the versatility of these sense embeddings in contrast to\ntask-specific models, applying them on several sense-related tasks, besides\nWSD, while demonstrating improved performance using our proposed approach over\nprior work focused on sense embeddings. Finally, we discuss unexpected findings\nregarding layer and model performance variations, and potential applications\nfor downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loureiro_D/0/1/0/all/0/1\">Daniel Loureiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorge_A/0/1/0/all/0/1\">Al&#xed;pio M&#xe1;rio Jorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spanish Language Models. (arXiv:2107.07253v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.07253","description":"<p>This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as\nwell as the corresponding performance evaluations. Both models were pre-trained\nusing the largest Spanish corpus known to date, with a total of 570GB of clean\nand deduplicated text processed for this work, compiled from the web crawlings\nperformed by the National Library of Spain from 2009 to 2019. We extended the\ncurrent evaluation datasets with an extractive Question Answering dataset and\nour models outperform the existing Spanish models across tasks and settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silveira_Ocampo_J/0/1/0/all/0/1\">Joaqu&#xed;n Silveira-Ocampo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1\">Carme Armentano-Oller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1\">Carlos Rodriguez-Penagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems. (arXiv:2110.07679v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07679","description":"<p>Much recent progress in task-oriented dialogue (ToD) systems has been driven\nby available annotation data across multiple domains for training. Over the\nlast few years, there has been a move towards data curation for multilingual\nToD systems that are applicable to serve people speaking different languages.\nHowever, existing multilingual ToD datasets either have a limited coverage of\nlanguages due to the high cost of data curation, or ignore the fact that\ndialogue entities barely exist in countries speaking these languages. To tackle\nthese limitations, we introduce a novel data curation method that generates\nGlobalWoZ -- a large-scale multilingual ToD dataset globalized from an English\nToD dataset for three unexplored use cases. Our method is based on translating\ndialogue templates and filling them with local entities in the target-language\ncountries. We release our dataset as well as a set of strong baselines to\nencourage research on learning multilingual ToD systems for real use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bosheng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljunied_S/0/1/0/all/0/1\">Sharifah Mahani Aljunied</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Power of Prompt Tuning for Low-Resource Semantic Parsing. (arXiv:2110.08525v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08525","description":"<p>Prompt tuning has recently emerged as an effective method for adapting\npre-trained language models to a number of language understanding and\ngeneration tasks. In this paper, we investigate prompt tuning for semantic\nparsing -- the task of mapping natural language utterances onto formal meaning\nrepresentations. On the low-resource splits of Overnight and TOPv2, we find\nthat a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart,\nas well as strong GPT-3 and BART baselines. We also conduct ablation studies\nacross different model scales and target representations, finding that, with\nincreasing model scale, prompt tuned T5 models improve at generating target\nrepresentations that are far from the pre-training distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schucher_N/0/1/0/all/0/1\">Nathan Schucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1\">Harm de Vries</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paperswithtopic: Topic Identification from Paper Title Only. (arXiv:2110.15721v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15721","description":"<p>The deep learning field is growing rapidly as witnessed by the exponential\ngrowth of papers submitted to journals, conferences, and pre-print servers. To\ncope with the sheer number of papers, several text mining tools from natural\nlanguage processing (NLP) have been proposed that enable researchers to keep\ntrack of recent findings. In this context, our paper makes two main\ncontributions: first, we collected and annotated a dataset of papers paired by\ntitle and sub-field from the field of artificial intelligence (AI), and,\nsecond, we present results on how to predict a paper's AI sub-field from a\ngiven paper title only. Importantly, for the latter, short-text classification\ntask we compare several algorithms from conventional machine learning all the\nway up to recent, larger transformer architectures. Finally, for the\ntransformer models, we also present gradient-based, attention visualizations to\nfurther explain the model's classification process. All code can be found at\n\\url{https://github.com/1pha/paperswithtopic}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_D/0/1/0/all/0/1\">Daehyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallraven_C/0/1/0/all/0/1\">Christian Wallraven</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Generating Grounded Navigation Instructions from Landmarks. (arXiv:2111.12872v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12872","description":"<p>We study the automatic generation of navigation instructions from 360-degree\nimages captured on indoor routes. Existing generators suffer from poor visual\ngrounding, causing them to rely on language priors and hallucinate objects. Our\nMARKY-MT5 system addresses this by focusing on visual landmarks; it comprises a\nfirst stage landmark detector and a second stage generator -- a multimodal,\nmultilingual, multitask encoder-decoder. To train it, we bootstrap grounded\nlandmark annotations on top of the Room-across-Room (RxR) dataset. Using text\nparsers, weak supervision from RxR's pose traces, and a multilingual image-text\nencoder trained on 1.8b images, we identify 1.1m English, Hindi and Telugu\nlandmark descriptions and ground them to specific regions in panoramas. On\nRoom-to-Room, human wayfinders obtain success rates (SR) of 71% following\nMARKY-MT5's instructions, just shy of their 75% SR following human instructions\n-- and well above SRs with other generators. Evaluations on RxR's longer,\ndiverse paths obtain 61-64% SRs on three languages. Generating such\nhigh-quality navigation instructions in novel environments is a step towards\nconversational navigation tools and could facilitate larger-scale training of\ninstruction-following agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montgomery_C/0/1/0/all/0/1\">Ceslee Montgomery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orbay_J/0/1/0/all/0/1\">Jordi Orbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birodkar_V/0/1/0/all/0/1\">Vighnesh Birodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1\">Aleksandra Faust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gur_I/0/1/0/all/0/1\">Izzeddin Gur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1\">Natasha Jaques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1\">Austin Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1\">Peter Anderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Facial Representation Learning in a Visual-Linguistic Manner. (arXiv:2112.03109v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03109","description":"<p>How to learn a universal facial representation that boosts all face analysis\ntasks? This paper takes one step toward this goal. In this paper, we study the\ntransfer performance of pre-trained models on face analysis tasks and introduce\na framework, called FaRL, for general Facial Representation Learning in a\nvisual-linguistic manner. On one hand, the framework involves a contrastive\nloss to learn high-level semantic meaning from image-text pairs. On the other\nhand, we propose exploring low-level information simultaneously to further\nenhance the face representation, by adding a masked image modeling. We perform\npre-training on LAION-FACE, a dataset containing large amount of face\nimage-text pairs, and evaluate the representation capability on multiple\ndownstream tasks. We show that FaRL achieves better transfer performance\ncompared with previous pre-trained models. We also verify its superiority in\nthe low-data regime. More importantly, our model surpasses the state-of-the-art\nmethods on face analysis tasks including face parsing and face alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinglin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yangyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Ming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pair-Level Supervised Contrastive Learning for Natural Language Inference. (arXiv:2201.10927v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.10927","description":"<p>Natural language inference (NLI) is an increasingly important task for\nnatural language understanding, which requires one to infer the relationship\nbetween the sentence pair (premise and hypothesis). Many recent works have used\ncontrastive learning by incorporating the relationship of the sentence pair\nfrom NLI datasets to learn sentence representation. However, these methods only\nfocus on comparisons with sentence-level representations. In this paper, we\npropose a Pair-level Supervised Contrastive Learning approach (PairSCL). We\nadopt a cross attention module to learn the joint representations of the\nsentence pairs. A contrastive learning objective is designed to distinguish the\nvaried classes of sentence pairs by pulling those in one class together and\npushing apart the pairs in other classes. We evaluate PairSCL on two public\ndatasets of NLI where the accuracy of PairSCL outperforms other methods by 2.1%\non average. Furthermore, our method outperforms the previous state-of-the-art\nmethod on seven transfer tasks of text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu&#x27;ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TimeLMs: Diachronic Language Models from Twitter. (arXiv:2202.03829v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03829","description":"<p>Despite its importance, the time variable has been largely neglected in the\nNLP and language model literature. In this paper, we present TimeLMs, a set of\nlanguage models specialized on diachronic Twitter data. We show that a\ncontinual learning strategy contributes to enhancing Twitter-based language\nmodels' capacity to deal with future and out-of-distribution tweets, while\nmaking them competitive with standardized and more monolithic benchmarks. We\nalso perform a number of qualitative analyses showing how they cope with trends\nand peaks in activity involving specific named entities or concept drift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loureiro_D/0/1/0/all/0/1\">Daniel Loureiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbieri_F/0/1/0/all/0/1\">Francesco Barbieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_L/0/1/0/all/0/1\">Leonardo Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anke_L/0/1/0/all/0/1\">Luis Espinosa Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality Controlled Paraphrase Generation. (arXiv:2203.10940v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10940","description":"<p>Paraphrase generation has been widely used in various downstream tasks. Most\ntasks benefit mainly from high quality paraphrases, namely those that are\nsemantically similar to, yet linguistically diverse from, the original\nsentence. Generating high-quality paraphrases is challenging as it becomes\nincreasingly hard to preserve meaning as linguistic diversity increases. Recent\nworks achieve nice results by controlling specific aspects of the paraphrase,\nsuch as its syntactic tree. However, they do not allow to directly control the\nquality of the generated paraphrase, and suffer from low flexibility and\nscalability. Here we propose $QCPG$, a quality-guided controlled paraphrase\ngeneration model, that allows directly controlling the quality dimensions.\nFurthermore, we suggest a method that given a sentence, identifies points in\nthe quality control space that are expected to yield optimal generated\nparaphrases. We show that our method is able to generate paraphrases which\nmaintain the original meaning while achieving higher diversity than the\nuncontrolled baseline. The models, the code, and the data can be found in\nhttps://github.com/IBM/quality-controlled-paraphrase-generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandel_E/0/1/0/all/0/1\">Elron Bandel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharonov_R/0/1/0/all/0/1\">Ranit Aharonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmueli_Scheuer_M/0/1/0/all/0/1\">Michal Shmueli-Scheuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shnayderman_I/0/1/0/all/0/1\">Ilya Shnayderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ein_Dor_L/0/1/0/all/0/1\">Liat Ein-Dor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLSP 2021 -- ViMRC Challenge: Vietnamese Machine Reading Comprehension. (arXiv:2203.11400v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.11400","description":"<p>One of the emerging research trends in natural language understanding is\nmachine reading comprehension (MRC) which is the task to find answers to human\nquestions based on textual data. Existing Vietnamese datasets for MRC research\nconcentrate solely on answerable questions. However, in reality, questions can\nbe unanswerable for which the correct answer is not stated in the given textual\ndata. To address the weakness, we provide the research community with a\nbenchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question\nanswering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a\nbenchmark dataset for the challenge on Vietnamese MRC at the Eighth Workshop on\nVietnamese Language and Speech Processing (VLSP 2021). This task attracted 77\nparticipant teams from 34 universities and other organizations. In this\narticle, we present details of the organization of the challenge, an overview\nof the methods employed by shared-task participants, and the results. The\nhighest performances are 77.24% in F1-score and 67.43% in Exact Match on the\nprivate test set. The Vietnamese MRC systems proposed by the top 3 teams use\nXLM-RoBERTa, a powerful pre-trained language model based on the transformer\narchitecture. The UIT-ViQuAD 2.0 dataset motivates researchers to further\nexplore the Vietnamese machine reading comprehension task and related tasks\nsuch as question answering, question generation, and natural language\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Quoc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Luan Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1\">Tin Van Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LDKP: A Dataset for Identifying Keyphrases from Long Scientific Documents. (arXiv:2203.15349v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.15349","description":"<p>Identifying keyphrases (KPs) from text documents is a fundamental task in\nnatural language processing and information retrieval. Vast majority of the\nbenchmark datasets for this task are from the scientific domain containing only\nthe document title and abstract information. This limits keyphrase extraction\n(KPE) and keyphrase generation (KPG) algorithms to identify keyphrases from\nhuman-written summaries that are often very short (approx 8 sentences). This\npresents three challenges for real-world applications: human-written summaries\nare unavailable for most documents, the documents are almost always long, and a\nhigh percentage of KPs are directly found beyond the limited context of title\nand abstract. Therefore, we release two extensive corpora mapping KPs of ~1.3M\nand ~100K scientific articles with their fully extracted text and additional\nmetadata including publication venue, year, author, field of study, and\ncitations for facilitating research on this real-world problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahata_D/0/1/0/all/0/1\">Debanjan Mahata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1\">Navneet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1\">Dibya Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Amardeep Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_S/0/1/0/all/0/1\">Swapnil Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Anish Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Indian Language Datasets used for Text Summarization. (arXiv:2203.16127v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16127","description":"<p>In this paper, we survey Text Summarization (TS) datasets in Indian Languages\n(ILs), which are also low-resource languages (LRLs). We seek to answer one\nprimary question: is the pool of Indian Language Text Summarization (ILTS)\ndataset growing or is there a resource poverty? To an-swer the primary\nquestion, we pose two sub-questions that we seek about ILTS datasets: first,\nwhat characteristics: format and domain do ILTS datasets have? Second, how\ndifferent are those characteristics of ILTS datasets from high-resource\nlanguages (HRLs) particularly English. We focus on datasets reported in\npublished ILTS research works during 2012-2022. The survey of ILTS and English\ndatasets reveals two similarities and one contrast. The two similarities are:\nfirst, the domain of dataset commonly is news (Hermann et al., 2015). The\nsecond similarity is the format of the dataset which is both extractive and\nabstractive. The contrast is in how the research in dataset development has\nprogressed. ILs face a slow speed of development and public release of datasets\nas compared with English. We argue that the relatively lower number of ILTS\ndatasets is because of two reasons: first, absence of a dedicated forum for\ndeveloping TS tools and resources; and second, lack of shareable standard\ndatasets in the public domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Shagun Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_G/0/1/0/all/0/1\">Girish Nath Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMER: Multimodal Multi-task learning for Emotion Recognition in Spoken Utterances. (arXiv:2203.16794v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16794","description":"<p>Emotion Recognition (ER) aims to classify human utterances into different\nemotion categories. Based on early-fusion and self-attention-based multimodal\ninteraction between text and acoustic modalities, in this paper, we propose a\nmultimodal multitask learning approach for ER from individual utterances in\nisolation. Experiments on the IEMOCAP benchmark show that our proposed model\nperforms better than our re-implementation of state-of-the-art and achieves\nbetter performance than all other unimodal and multimodal approaches in\nliterature. In addition, strong baselines and ablation studies prove the\neffectiveness of our proposed approach. We make all our codes publicly\navailable on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_H/0/1/0/all/0/1\">Harshvardhan Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Discourse Aware Sequence Learning Approach for Emotion Recognition in Conversations. (arXiv:2203.16799v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16799","description":"<p>The expression of emotions is a crucial part of daily human communication.\nModeling the conversational and sequential context has seen much success and\nplays a vital role in Emotion Recognition in Conversations (ERC). However,\nexisting approaches either model only one of the two or employ naive\nlate-fusion methodologies to obtain final utterance representations. This paper\nproposes a novel idea to incorporate both these contexts and better model the\nintrinsic structure within a conversation. More precisely, we propose a novel\narchitecture boosted by a modified LSTM cell, which we call DiscLSTM, that\nbetter captures the interaction between conversational and sequential context.\nDiscLSTM brings together the best of both worlds and provides a more intuitive\nand efficient way to model the information flow between individual utterances\nby better capturing long-distance conversational background through discourse\nrelations and sequential context through recurrence. We conduct experiments on\nfour benchmark datasets for ERC and show that our model achieves performance\ncompetitive to state-of-the-art and at times performs better than other\ngraph-based approaches in literature, with a conversational graph that is both\nsparse and avoids complicated edge relations like much of previous work. We\nmake all our codes publicly available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_H/0/1/0/all/0/1\">Harshvardhan Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study on Speaker-attributed Automatic Speech Recognition in Multi-party Meetings. (arXiv:2203.16834v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.16834","description":"<p>In this paper, we conduct a comparative study on speaker-attributed automatic\nspeech recognition (SA-ASR) in the multi-party meeting scenario, a topic with\nincreasing attention in meeting rich transcription. Specifically, three\napproaches are evaluated in this study. The first approach, FD-SOT, consists of\na frame-level diarization model to identify speakers and a multi-talker ASR to\nrecognize utterances. The speaker-attributed transcriptions are obtained by\naligning the diarization results and recognized hypotheses. However, such an\nalignment strategy may suffer from erroneous timestamps due to the modular\nindependence, severely hindering the model performance. Therefore, we propose\nthe second approach, WD-SOT, to address alignment errors by introducing a\nword-level diarization model, which can get rid of such timestamp alignment\ndependency. To further mitigate the alignment issues, we propose the third\napproach, TS-ASR, which trains a target-speaker separation module and an ASR\nmodule jointly. By comparing various strategies for each SA-ASR approach,\nexperimental results on a real meeting scenario corpus, AliMeeting, reveal that\nthe WD-SOT approach achieves 10.7% relative reduction on averaged\nspeaker-dependent character error rate (SD-CER), compared with the FD-SOT\napproach. In addition, the TS-ASR approach also outperforms the FD-SOT approach\nand brings 16.5% relative average SD-CER reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhihao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuxiao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Contrast Stretching on Target Feature for Speech Enhancement. (arXiv:2203.17152v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.17152","description":"<p>Speech enhancement (SE) performance has improved considerably since the use\nof deep learning (DL) models as a base function. In this study, we propose a\nperceptual contrast stretching (PCS) approach to further improve SE\nperformance. PCS is derived based on the critical band importance function and\napplied to modify the targets of the SE model. Specifically, PCS stretches the\ncontract of target features according to perceptual importance, thereby\nimproving the overall SE performance. Compared to post-processing based\nimplementations, incorporating PCS into the training phase preserves\nperformance and reduces online computation. It is also worth noting that PCS\ncan be suitably combined with different SE model architectures and training\ncriteria. Meanwhile, PCS does not affect the causality or convergence of the SE\nmodel training. Experimental results on the VoiceBank-DEMAND dataset showed\nthat the proposed method can achieve state-of-the-art performance on both\ncausal (PESQ=3.07) and non-causal (PESQ=3.35) SE tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chao_R/0/1/0/all/0/1\">Rong Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Szu-Wei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xugang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Ball 3D localization from a single calibrated image. (arXiv:2204.00003v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00003","description":"<p>Ball 3D localization in team sports has various applications including\nautomatic offside detection in soccer, or shot release localization in\nbasketball. Today, this task is either resolved by using expensive multi-views\nsetups, or by restricting the analysis to ballistic trajectories. In this work,\nwe propose to address the task on a single image from a calibrated monocular\ncamera by estimating ball diameter in pixels and use the knowledge of real ball\ndiameter in meters. This approach is suitable for any game situation where the\nball is (even partly) visible. To achieve this, we use a small neural network\ntrained on image patches around candidates generated by a conventional ball\ndetector. Besides predicting ball diameter, our network outputs the confidence\nof having a ball in the image patch. Validations on 3 basketball datasets\nreveals that our model gives remarkable predictions on ball 3D localization. In\naddition, through its confidence output, our model improves the detection rate\nby filtering the candidates produced by the detector. The contributions of this\nwork are (i) the first model to address 3D ball localization on a single image,\n(ii) an effective method for ball 3D annotation from single calibrated images,\n(iii) a high quality 3D ball evaluation dataset annotated from a single\nviewpoint. In addition, the code to reproduce this research is be made freely\navailable at https://github.com/gabriel-vanzandycke/deepsport.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zandycke_G/0/1/0/all/0/1\">Gabriel Van Zandycke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vleeshouwer_C/0/1/0/all/0/1\">Christophe De Vleeshouwer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-based Active Learning for Semi-supervised Classification of SAR Data. (arXiv:2204.00005v1 [cs.LG])","link":"http://arxiv.org/abs/2204.00005","description":"<p>We present a novel method for classification of Synthetic Aperture Radar\n(SAR) data by combining ideas from graph-based learning and neural network\nmethods within an active learning framework. Graph-based methods in machine\nlearning are based on a similarity graph constructed from the data. When the\ndata consists of raw images composed of scenes, extraneous information can make\nthe classification task more difficult. In recent years, neural network methods\nhave been shown to provide a promising framework for extracting patterns from\nSAR images. These methods, however, require ample training data to avoid\noverfitting. At the same time, such training data are often unavailable for\napplications of interest, such as automatic target recognition (ATR) and SAR\ndata. We use a Convolutional Neural Network Variational Autoencoder (CNNVAE) to\nembed SAR data into a feature space, and then construct a similarity graph from\nthe embedded data and apply graph-based semi-supervised learning techniques.\nThe CNNVAE feature embedding and graph construction requires no labeled data,\nwhich reduces overfitting and improves the generalization performance of graph\nlearning at low label rates. Furthermore, the method easily incorporates a\nhuman-in-the-loop for active learning in the data-labeling process. We present\npromising results and compare them to other standard machine learning methods\non the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset\nfor ATR with small amounts of labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miller_K/0/1/0/all/0/1\">Kevin Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mauro_J/0/1/0/all/0/1\">John Mauro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Setiadi_J/0/1/0/all/0/1\">Jason Setiadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baca_X/0/1/0/all/0/1\">Xoaquin Baca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calder_J/0/1/0/all/0/1\">Jeff Calder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertozzi_A/0/1/0/all/0/1\">Andrea L. Bertozzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digitizing Historical Balance Sheet Data: A Practitioner's Guide. (arXiv:2204.00052v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00052","description":"<p>This paper discusses how to successfully digitize large-scale historical\nmicro-data by augmenting optical character recognition (OCR) engines with pre-\nand post-processing methods. Although OCR software has improved dramatically in\nrecent years due to improvements in machine learning, off-the-shelf OCR\napplications still present high error rates which limits their applications for\naccurate extraction of structured information. Complementing OCR with\nadditional methods can however dramatically increase its success rate, making\nit a powerful and cost-efficient tool for economic historians. This paper\nshowcases these methods and explains why they are useful. We apply them against\ntwo large balance sheet datasets and introduce \"quipucamayoc\", a Python package\ncontaining these methods in a unified framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Correia_S/0/1/0/all/0/1\">Sergio Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luck_S/0/1/0/all/0/1\">Stephan Luck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Classification of Alzheimer's Disease using brain MRI data and deep Convolutional Neural Networks. (arXiv:2204.00068v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00068","description":"<p>Alzheimer's disease (AD) is one of the most common public health issues the\nworld is facing today. This disease has a high prevalence primarily in the\nelderly accompanying memory loss and cognitive decline. AD detection is a\nchallenging task which many authors have developed numerous computerized\nautomatic diagnosis systems utilizing neuroimaging and other clinical data. MRI\nscans provide high-intensity visible features, making these scans the most\nwidely used brain imaging technique. In recent years deep learning has achieved\nleading success in medical image analysis. But a relatively little\ninvestigation has been done to apply deep learning techniques for the brain MRI\nclassification. This paper explores the construction of several deep learning\narchitectures evaluated on brain MRI images and segmented images. The idea\nbehind segmented images investigates the influence of image segmentation step\non deep learning classification. The image processing presented a pipeline\nconsisting of pre-processing to enhance the MRI scans and post-processing\nconsisting of a segmentation method for segmenting the brain tissues. The\nresults show that the processed images achieved a better accuracy in the binary\nclassification of AD vs. CN (Cognitively Normal) across four different\narchitectures. ResNet architecture resulted in the highest prediction accuracy\namongst the other architectures (90.83% for the original brain images and\n93.50% for the processed images).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Aaraji_Z/0/1/0/all/0/1\">Zahraa Sh. Aaraji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abbas_H/0/1/0/all/0/1\">Hawraa H. Abbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Maximal Coding Rate Reduction by Variational Forms. (arXiv:2204.00077v1 [cs.LG])","link":"http://arxiv.org/abs/2204.00077","description":"<p>The principle of Maximal Coding Rate Reduction (MCR$^2$) has recently been\nproposed as a training objective for learning discriminative low-dimensional\nstructures intrinsic to high-dimensional data to allow for more robust training\nthan standard approaches, such as cross-entropy minimization. However, despite\nthe advantages that have been shown for MCR$^2$ training, MCR$^2$ suffers from\na significant computational cost due to the need to evaluate and differentiate\na significant number of log-determinant terms that grows linearly with the\nnumber of classes. By taking advantage of variational forms of spectral\nfunctions of a matrix, we reformulate the MCR$^2$ objective to a form that can\nscale significantly without compromising training accuracy. Experiments in\nimage classification demonstrate that our proposed formulation results in a\nsignificant speed up over optimizing the original MCR$^2$ objective directly\nand often results in higher quality learned representations. Further, our\napproach may be of independent interest in other models that require\ncomputation of log-determinant forms, such as in system identification or\nnormalizing flow models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baek_C/0/1/0/all/0/1\">Christina Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1\">Kwan Ho Ryan Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1\">Tianjiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1\">Benjamin D. Haeffele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"4Weed Dataset: Annotated Imagery Weeds Dataset. (arXiv:2204.00080v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00080","description":"<p>Weeds are a major threat to crops and are responsible for reducing crop yield\nworldwide. To mitigate their negative effect, it is advantageous to accurately\nidentify them early in the season to prevent their spread throughout the field.\nTraditionally, farmers rely on manually scouting fields and applying herbicides\nfor different weeds. However, it is easy to confuse between crops with weeds\nduring the early growth stages. Recently, deep learning-based weed\nidentification has become popular as deep learning relies on convolutional\nneural networks that are capable of learning important distinguishable features\nbetween weeds and crops. However, training robust deep learning models requires\naccess to large imagery datasets. Therefore, an early-season weeds dataset was\nacquired under field conditions. The dataset consists of 159 Cocklebur images,\n139 Foxtail images, 170 Redroot Pigweed images and 150 Giant Ragweed images\ncorresponding to four common weed species found in corn and soybean production\nsystems.. Bounding box annotations were created for each image to prepare the\ndataset for training both image classification and object detection deep\nlearning networks capable of accurately locating and identifying weeds within\ncorn and soybean fields. (https://osf.io/w9v3j/)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1\">Varun Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1\">Aanis Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etienne_A/0/1/0/all/0/1\">Aaron Etienne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saraswat_D/0/1/0/all/0/1\">Dharmendra Saraswat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Top-$k$ White-Box and Transferable Black-box Attack. (arXiv:2204.00089v1 [cs.LG])","link":"http://arxiv.org/abs/2204.00089","description":"<p>Existing works have identified the limitation of top-$1$ attack success rate\n(ASR) as a metric to evaluate the attack strength but exclusively investigated\nit in the white-box setting, while our work extends it to a more practical\nblack-box setting: transferable attack. It is widely reported that stronger\nI-FGSM transfers worse than simple FGSM, leading to a popular belief that\ntransferability is at odds with the white-box attack strength. Our work\nchallenges this belief with empirical finding that stronger attack actually\ntransfers better for the general top-$k$ ASR indicated by the interest class\nrank (ICR) after attack. For increasing the attack strength, with an intuitive\ninterpretation of the logit gradient from the geometric perspective, we\nidentify that the weakness of the commonly used losses lie in prioritizing the\nspeed to fool the network instead of maximizing its strength. To this end, we\npropose a new normalized CE loss that guides the logit to be updated in the\ndirection of implicitly maximizing its rank distance from the ground-truth\nclass. Extensive results in various settings have verified that our proposed\nnew loss is simple yet effective for top-$k$ attack. Code is available at:\n\\url{https://bit.ly/3uCiomP}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benz_P/0/1/0/all/0/1\">Philipp Benz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karjauv_A/0/1/0/all/0/1\">Adil Karjauv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jae Won Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tooth Instance Segmentation on Panoramic Dental Radiographs Using U-Nets and Morphological Processing. (arXiv:2204.00095v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00095","description":"<p>Automatic teeth segmentation in panoramic x-ray images is an important\nresearch subject of the image analysis in dentistry. In this study, we propose\na post-processing stage to obtain a segmentation map in which the objects in\nthe image are separated, and apply this technique to tooth instance\nsegmentation with U-Net network. The post-processing consists of grayscale\nmorphological and filtering operations, which are applied to the sigmoid output\nof the network before binarization. A dice overlap score of 95.4 - 0.3% is\nobtained in overall teeth segmentation. The proposed post-processing stages\nreduce the mean error of tooth count to 6.15%, whereas the error without\npost-processing is 26.81%. The performances of both segmentation and tooth\ncounting are the highest in the literature, to our knowledge. Moreover, this is\nachieved by using a relatively small training dataset, which consists of 105\nimages. Although the aim in this study is to segment tooth instances, the\npresented method is applicable to similar problems in other domains, such as\nseparating the cell instances\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Helli_S/0/1/0/all/0/1\">Selahattin Serdar Helli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamamci_A/0/1/0/all/0/1\">Andac Hamamci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransGeo: Transformer Is All You Need for Cross-view Image Geo-localization. (arXiv:2204.00097v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00097","description":"<p>The dominant CNN-based methods for cross-view image geo-localization rely on\npolar transform and fail to model global correlation. We propose a pure\ntransformer-based approach (TransGeo) to address these limitations from a\ndifferent perspective. TransGeo takes full advantage of the strengths of\ntransformer related to global information modeling and explicit position\ninformation encoding. We further leverage the flexibility of transformer input\nand propose an attention-guided non-uniform cropping method, so that\nuninformative image patches are removed with negligible drop on performance to\nreduce computation cost. The saved computation can be reallocated to increase\nresolution only for informative patches, resulting in performance improvement\nwith no additional computation cost. This \"attend and zoom-in\" strategy is\nhighly similar to human behavior when observing images. Remarkably, TransGeo\nachieves state-of-the-art results on both urban and rural datasets, with\nsignificantly less computation cost than CNN-based methods. It does not rely on\npolar transform and infers faster than CNN-based methods. Code is available at\nhttps://github.com/Jeff-Zilence/TransGeo2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Multimodal Fusion. (arXiv:2204.00102v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00102","description":"<p>Deep multimodal learning has achieved great progress in recent years.\nHowever, current fusion approaches are static in nature, i.e., they process and\nfuse multimodal inputs with identical computation, without accounting for\ndiverse computational demands of different multimodal data. In this work, we\npropose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses\nmultimodal data and generates data-dependent forward paths during inference.\nDynMM can reduce redundant computations for \"easy\" multimodal inputs (that can\nbe predicted correctly using only one modality or simple fusion techniques) and\nretain representation power for \"hard\" samples by adopting all modalities and\ncomplex fusion operations for prediction. Results on various multimodal tasks\ndemonstrate the efficiency and wide applicability of our approach. For\ninstance, DynMM can reduce the computation cost by 46.5% with a negligible\naccuracy loss on CMU-MOSEI sentiment analysis. For RGB-D semantic segmentation\non NYU Depth data, DynMM achieves a +0.7% mIoU improvement with over 21%\nreductions for the depth encoder when compared with strong baselines. We\nbelieve this opens a novel direction towards dynamic multimodal network design,\nwith applications to a wide range of multimodal tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zihui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marculescu_R/0/1/0/all/0/1\">Radu Marculescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Robust 3D Object Detection Methods in Point Clouds. (arXiv:2204.00106v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00106","description":"<p>The purpose of this work is to review the state-of-the-art LiDAR-based 3D\nobject detection methods, datasets, and challenges. We describe novel data\naugmentation methods, sampling strategies, activation functions, attention\nmechanisms, and regularization methods. Furthermore, we list recently\nintroduced normalization methods, learning rate schedules and loss functions.\nMoreover, we also cover advantages and limitations of 10 novel autonomous\ndriving datasets. We evaluate novel 3D object detectors on the KITTI, nuScenes,\nand Waymo dataset and show their accuracy, speed, and robustness. Finally, we\nmention the current challenges in 3D object detection in LiDAR point clouds and\nlist some open issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_W/0/1/0/all/0/1\">Walter Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ercelik_E/0/1/0/all/0/1\">Emec Ercelik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xingcheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_X/0/1/0/all/0/1\">Xavier Jair Diaz Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GALA: Toward Geometry-and-Lighting-Aware Object Search for Compositing. (arXiv:2204.00125v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00125","description":"<p>Compositing-aware object search aims to find the most compatible objects for\ncompositing given a background image and a query bounding box. Previous works\nfocus on learning compatibility between the foreground object and background,\nbut fail to learn other important factors from large-scale data, i.e. geometry\nand lighting. To move a step further, this paper proposes GALA\n(Geometry-and-Lighting-Aware), a generic foreground object search method with\ndiscriminative modeling on geometry and lighting compatibility for open-world\nimage compositing. Remarkably, it achieves state-of-the-art results on the CAIS\ndataset and generalizes well on large-scale open-world datasets, i.e. Pixabay\nand Open Images. In addition, our method can effectively handle non-box\nscenarios, where users only provide background images without any input\nbounding box. A web demo (see supplementary materials) is built to showcase\napplications of the proposed method for compositing-aware search and automatic\nlocation/scale prediction for the foreground object.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Scott Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Quality Assessment of UGC Gaming Videos. (arXiv:2204.00128v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00128","description":"<p>In recent years, with the vigorous development of the video game industry,\nthe proportion of gaming videos on major video websites like YouTube has\ndramatically increased. However, relatively little research has been done on\nthe automatic quality prediction of gaming videos, especially on those that\nfall in the category of \"User-Generated-Content\" (UGC). Since current leading\ngeneral-purpose Video Quality Assessment (VQA) models do not perform well on\nthis type of gaming videos, we have created a new VQA model specifically\ndesigned to succeed on UGC gaming videos, which we call the Gaming Video\nQuality Predictor (GAME-VQP). GAME-VQP successfully predicts the unique\nstatistical characteristics of gaming videos by drawing upon features designed\nunder modified natural scene statistics models, combined with gaming specific\nfeatures learned by a Convolution Neural Network. We study the performance of\nGAME-VQP on a very recent large UGC gaming video database called\nLIVE-YT-Gaming, and find that it both outperforms other mainstream general VQA\nmodels as well as VQA models specifically designed for gaming videos. The new\nmodel will be made public after paper being accepted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1\">Xiangxu Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzhong Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Birkbeck_N/0/1/0/all/0/1\">Neil Birkbeck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adsumilli_B/0/1/0/all/0/1\">Balu Adsumilli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time and Robust 3D Object Detection Within Road-Side LiDARs Using Domain Adaptation. (arXiv:2204.00132v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00132","description":"<p>This work aims to address the challenges in domain adaptation of 3D object\ndetection using infrastructure LiDARs. We design a model DASE-ProPillars that\ncan detect vehicles in infrastructure-based LiDARs in real-time. Our model uses\nPointPillars as the baseline model with additional modules to improve the 3D\ndetection performance. To prove the effectiveness of our proposed modules in\nDASE-ProPillars, we train and evaluate the model on two datasets, the open\nsource A9-Dataset and a semi-synthetic infrastructure dataset created within\nthe Regensburg Next project. We do several sets of experiments for each module\nin the DASE-ProPillars detector that show that our model outperforms the\nSE-ProPillars baseline on the real A9 test set and a semi-synthetic A9 test\nset, while maintaining an inference speed of 45 Hz (22 ms). We apply domain\nadaptation from the semi-synthetic A9-Dataset to the semi-synthetic dataset\nfrom the Regensburg Next project by applying transfer learning and achieve a 3D\nmAP@0.25 of 93.49% on the Car class of the target test set using 40 recall\npositions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_W/0/1/0/all/0/1\">Walter Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabler_M/0/1/0/all/0/1\">Marcus Grabler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Predictive Control for Fluid Human-to-Robot Handovers. (arXiv:2204.00134v1 [cs.RO])","link":"http://arxiv.org/abs/2204.00134","description":"<p>Human-robot handover is a fundamental yet challenging task in human-robot\ninteraction and collaboration. Recently, remarkable progressions have been made\nin human-to-robot handovers of unknown objects by using learning-based grasp\ngenerators. However, how to responsively generate smooth motions to take an\nobject from a human is still an open question. Specifically, planning motions\nthat take human comfort into account is not a part of the human-robot handover\nprocess in most prior works. In this paper, we propose to generate smooth\nmotions via an efficient model-predictive control (MPC) framework that\nintegrates perception and complex domain-specific constraints into the\noptimization problem. We introduce a learning-based grasp reachability model to\nselect candidate grasps which maximize the robot's manipulability, giving it\nmore freedom to satisfy these constraints. Finally, we integrate a neural net\nforce/torque classifier that detects contact events from noisy data. We\nconducted human-to-robot handover experiments on a diverse set of objects with\nseveral users (N=4) and performed a systematic evaluation of each module. The\nstudy shows that the users preferred our MPC approach over the baseline system\nby a large margin. More results and videos are available at\nhttps://sites.google.com/nvidia.com/mpc-for-handover.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaralingam_B/0/1/0/all/0/1\">Balakumar Sundaralingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akinola_I/0/1/0/all/0/1\">Iretiayo Akinola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1\">Yu-Wei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cakmak_M/0/1/0/all/0/1\">Maya Cakmak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Weakly Supervised Object Detection by Sampling Pseudo Ground-Truth Boxes. (arXiv:2204.00147v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00147","description":"<p>Semi- and weakly-supervised learning have recently attracted considerable\nattention in the object detection literature since they can alleviate the cost\nof annotation needed to successfully train deep learning models. State-of-art\napproaches for semi-supervised learning rely on student-teacher models trained\nusing a multi-stage process, and considerable data augmentation. Custom\nnetworks have been developed for the weakly-supervised setting, making it\ndifficult to adapt to different detectors. In this paper, a weakly\nsemi-supervised training method is introduced that reduces these training\nchallenges, yet achieves state-of-the-art performance by leveraging only a\nsmall fraction of fully-labeled images with information in weakly-labeled\nimages. In particular, our generic sampling-based learning strategy produces\npseudo-ground-truth (GT) bounding box annotations in an online fashion,\neliminating the need for multi-stage training, and student-teacher network\nconfigurations. These pseudo GT boxes are sampled from weakly-labeled images\nbased on the categorical score of object proposals accumulated via a score\npropagation process. Empirical results on the Pascal VOC dataset, indicate that\nthe proposed approach improves performance by 5.0% when using VOC 2007 as\nfully-labeled, and VOC 2012 as weak-labeled data. Also, with 5-10% fully\nannotated images, we observed an improvement of more than 10% in mAP, showing\nthat a modest investment in image-level annotation, can substantially improve\ndetection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meethal_A/0/1/0/all/0/1\">Akhil Meethal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhongwen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_F/0/1/0/all/0/1\">Francisco Perdigon Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An End-to-end Supervised Domain Adaptation Framework for Cross-Domain Change Detection. (arXiv:2204.00154v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00154","description":"<p>Existing deep learning-based change detection methods try to elaborately\ndesign complicated neural networks with powerful feature representations, but\nignore the universal domain shift induced by time-varying land cover changes,\nincluding luminance fluctuations and season changes between pre-event and\npost-event images, thereby producing sub-optimal results. In this paper, we\npropose an end-to-end Supervised Domain Adaptation framework for cross-domain\nChange Detection, namely SDACD, to effectively alleviate the domain shift\nbetween bi-temporal images for better change predictions. Specifically, our\nSDACD presents collaborative adaptations from both image and feature\nperspectives with supervised learning. Image adaptation exploits generative\nadversarial learning with cycle-consistency constraints to perform cross-domain\nstyle transformation, effectively narrowing the domain gap in a two-side\ngeneration fashion. As to feature adaptation, we extract domain-invariant\nfeatures to align different feature distributions in the feature space, which\ncould further reduce the domain gap of cross-domain images. To further improve\nthe performance, we combine three types of bi-temporal images for the final\nchange prediction, including the initial input bi-temporal images and two\ngenerated bi-temporal images from the pre-event and post-event domains.\nExtensive experiments and analyses on two benchmarks demonstrate the\neffectiveness and universality of our proposed framework. Notably, our\nframework pushes several representative baseline models up to new\nState-Of-The-Art records, achieving 97.34% and 92.36% on the CDD and WHU\nbuilding datasets, respectively. The source code and models are publicly\navailable at https://github.com/Perfect-You/SDACD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_W/0/1/0/all/0/1\">Wenjie Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yuhang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereo Unstructured Magnification: Multiple Homography Image for View Synthesis. (arXiv:2204.00156v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00156","description":"<p>This paper studies the problem of view synthesis with certain amount of\nrotations from a pair of images, what we called stereo unstructured\nmagnification. While the multi-plane image representation is well suited for\nview synthesis with depth invariant, how to generalize it to unstructured views\nremains a significant challenge. This is primarily due to the depth-dependency\ncaused by camera frontal parallel representation. Here we propose a novel\nmultiple homography image (MHI) representation, comprising of a set of scene\nplanes with fixed normals and distances. A two-stage network is developed for\nnovel view synthesis. Stage-1 is an MHI reconstruction module that predicts the\nMHIs and composites layered multi-normal images along the normal direction.\nStage-2 is a normal-blending module to find blending weights. We also derive an\nangle-based cost to guide the blending of multi-normal images by exploiting\nper-normal geometry. Compared with the state-of-the-art methods, our method\nachieves superior performance for view synthesis qualitatively and\nquantitatively, especially for cases when the cameras undergo rotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Ying Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LASER: LAtent SpacE Rendering for 2D Visual Localization. (arXiv:2204.00157v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00157","description":"<p>We present LASER, an image-based Monte Carlo Localization (MCL) framework for\n2D floor maps. LASER introduces the concept of latent space rendering, where 2D\npose hypotheses on the floor map are directly rendered into a\ngeometrically-structured latent space by aggregating viewing ray features.\nThrough a tightly coupled rendering codebook scheme, the viewing ray features\nare dynamically determined at rendering-time based on their geometries (i.e.\nlength, incident-angle), endowing our representation with view-dependent\nfine-grain variability. Our codebook scheme effectively disentangles feature\nencoding from rendering, allowing the latent space rendering to run at speeds\nabove 10KHz. Moreover, through metric learning, our geometrically-structured\nlatent space is common to both pose hypotheses and query images with arbitrary\nfield of views. As a result, LASER achieves state-of-the-art performance on\nlarge-scale indoor localization datasets (i.e. ZInD and Structured3D) for both\npanorama and perspective image queries, while significantly outperforming\nexisting learning-based methods in speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_Z/0/1/0/all/0/1\">Zhixiang Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravan_N/0/1/0/all/0/1\">Naji Khosravan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bessinger_Z/0/1/0/all/0/1\">Zachary Bessinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayana_M/0/1/0/all/0/1\">Manjunath Narayana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Sing Bing Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_E/0/1/0/all/0/1\">Enrique Dunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyadzhiev_I/0/1/0/all/0/1\">Ivaylo Boyadzhiev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Scene Synthesis for Mixed Reality Telepresence. (arXiv:2204.00161v1 [cs.HC])","link":"http://arxiv.org/abs/2204.00161","description":"<p>Remote telepresence via next-generation mixed reality platforms can provide\nhigher levels of immersion for computer-mediated communications, allowing\nparticipants to engage in a wide spectrum of activities, previously not\npossible in 2D screen-based communication methods. However, as mixed reality\nexperiences are limited to the local physical surrounding of each user, finding\na common virtual ground where users can freely move and interact with each\nother is challenging. In this paper, we propose a novel mutual scene synthesis\nmethod that takes the participants' spaces as input, and generates a virtual\nsynthetic scene that corresponds to the functional features of all\nparticipants' local spaces. Our method combines a mutual function optimization\nmodule with a deep-learning conditional scene augmentation process to generate\na scene mutually and physically accessible to all participants of a mixed\nreality telepresence scenario. The synthesized scene can hold mutual walkable,\nsittable and workable functions, all corresponding to physical objects in the\nusers' real environments. We perform experiments using the MatterPort3D dataset\nand conduct comparative user studies to evaluate the effectiveness of our\nsystem. Our results show that our proposed approach can be a promising research\ndirection for facilitating contextualized telepresence systems for\nnext-generation spatial computing platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keshavarzi_M/0/1/0/all/0/1\">Mohammad Keshavarzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Allen Y. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peluse_P/0/1/0/all/0/1\">Patrick Peluse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caldas_L/0/1/0/all/0/1\">Luisa Caldas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Framework for Domain Adaptive Pose Estimation. (arXiv:2204.00172v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00172","description":"<p>While pose estimation is an important computer vision task, it requires\nexpensive annotation and suffers from domain shift. In this paper, we\ninvestigate the problem of domain adaptive 2D pose estimation that transfers\nknowledge learned on a synthetic source domain to a target domain without\nsupervision. While several domain adaptive pose estimation models have been\nproposed recently, they are not generic but only focus on either human pose or\nanimal pose estimation, and thus their effectiveness is somewhat limited to\nspecific scenarios. In this work, we propose a unified framework that\ngeneralizes well on various domain adaptive pose estimation problems. We\npropose to align representations using both input-level and output-level cues\n(pixels and pose labels, respectively), which facilitates the knowledge\ntransfer from the source domain to the unlabeled target domain. Our experiments\nshow that our method achieves state-of-the-art performance under various domain\nshifts. Our method outperforms existing baselines on human pose estimation by\nup to 4.5 percent points (pp), hand pose estimation by up to 7.4 pp, and animal\npose estimation by up to 4.8 pp for dogs and 3.3 pp for sheep. These results\nsuggest that our method is able to mitigate domain shift on diverse tasks and\neven unseen domains and objects (e.g., trained on horse and tested on dog).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betke_M/0/1/0/all/0/1\">Margrit Betke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraftNet: Towards Domain Generalized Stereo Matching with a Broad-Spectrum and Task-Oriented Feature. (arXiv:2204.00179v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00179","description":"<p>Although supervised deep stereo matching networks have made impressive\nachievements, the poor generalization ability caused by the domain gap prevents\nthem from being applied to real-life scenarios. In this paper, we propose to\nleverage the feature of a model trained on large-scale datasets to deal with\nthe domain shift since it has seen various styles of images. With the cosine\nsimilarity based cost volume as a bridge, the feature will be grafted to an\nordinary cost aggregation module. Despite the broad-spectrum representation,\nsuch a low-level feature contains much general information which is not aimed\nat stereo matching. To recover more task-specific information, the grafted\nfeature is further input into a shallow network to be transformed before\ncalculating the cost. Extensive experiments show that the model generalization\nability can be improved significantly with this broad-spectrum and\ntask-oriented feature. Specifically, based on two well-known architectures\nPSMNet and GANet, our methods are superior to other robust algorithms when\ntransferring from SceneFlow to KITTI 2015, KITTI 2012, and Middlebury. Code is\navailable at https://github.com/SpadeLiu/Graft-PSMNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Biyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huimin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guodong Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Supervisor for Cross-dataset Object Detection. (arXiv:2204.00183v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00183","description":"<p>The application of cross-dataset training in object detection tasks is\ncomplicated because the inconsistency in the category range across datasets\ntransforms fully supervised learning into semi-supervised learning. To address\nthis problem, recent studies focus on the generation of high-quality missing\nannotations. In this study, we first point out that it is not enough to\ngenerate high-quality annotations using a single model, which only looks once\nfor annotations. Through detailed experimental analyses, we further conclude\nthat hard-label training is conducive to generating high-recall annotations,\nwhile soft-label training tends to obtain high-precision annotations. Inspired\nby the aspects mentioned above, we propose a dynamic supervisor framework that\nupdates the annotations multiple times through multiple-updated submodels\ntrained using hard and soft labels. In the final generated annotations, both\nrecall and precision improve significantly through the integration of\nhard-label training with soft-label training. Extensive experiments conducted\non various dataset combination settings support our analyses and demonstrate\nthe superior performance of the proposed dynamic supervisor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhihang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1\">Mingyuan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rongxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xiang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaowu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-sheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Epipolar Focus Spectrum: A Novel Light Field Representation and Application in Dense-view Reconstruction. (arXiv:2204.00193v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00193","description":"<p>Existing light field representations, such as epipolar plane image (EPI) and\nsub-aperture images, do not consider the structural characteristics across the\nviews, so they usually require additional disparity and spatial structure cues\nfor follow-up tasks. Besides, they have difficulties dealing with occlusions or\nlarger disparity scenes. To this end, this paper proposes a novel Epipolar\nFocus Spectrum (EFS) representation by rearranging the EPI spectrum. Different\nfrom the classical EPI representation where an EPI line corresponds to a\nspecific depth, there is a one-to-one mapping from the EFS line to the view.\nAccordingly, compared to a sparsely-sampled light field, a densely-sampled one\nwith the same field of view (FoV) leads to a more compact distribution of such\nlinear structures in the double-cone-shaped region with the identical opening\nangle in its corresponding EFS. Hence the EFS representation is invariant to\nthe scene depth. To demonstrate its effectiveness, we develop a trainable\nEFS-based pipeline for light field reconstruction, where a dense light field\ncan be reconstructed by compensating the \"missing EFS lines\" given a sparse\nlight field, yielding promising results with cross-view consistency, especially\nin the presence of severe occlusion and large disparity. Experimental results\non both synthetic and real-world datasets demonstrate the validity and\nsuperiority of the proposed method over SOTA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yaning Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xue Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_G/0/1/0/all/0/1\">Guoqing Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Gap between Classification and Localization for Weakly Supervised Object Localization. (arXiv:2204.00220v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00220","description":"<p>Weakly supervised object localization aims to find a target object region in\na given image with only weak supervision, such as image-level labels. Most\nexisting methods use a class activation map (CAM) to generate a localization\nmap; however, a CAM identifies only the most discriminative parts of a target\nobject rather than the entire object region. In this work, we find the gap\nbetween classification and localization in terms of the misalignment of the\ndirections between an input feature and a class-specific weight. We demonstrate\nthat the misalignment suppresses the activation of CAM in areas that are less\ndiscriminative but belong to the target object. To bridge the gap, we propose a\nmethod to align feature directions with a class-specific weight. The proposed\nmethod achieves a state-of-the-art localization performance on the CUB-200-2011\nand ImageNet-1K benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eunji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Siwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungbeom Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perception Prioritized Training of Diffusion Models. (arXiv:2204.00227v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00227","description":"<p>Diffusion models learn to restore noisy data, which is corrupted with\ndifferent levels of noise, by optimizing the weighted sum of the corresponding\nloss terms, i.e., denoising score matching loss. In this paper, we show that\nrestoring data corrupted with certain noise levels offers a proper pretext task\nfor the model to learn rich visual concepts. We propose to prioritize such\nnoise levels over other levels during training, by redesigning the weighting\nscheme of the objective function. We show that our simple redesign of the\nweighting scheme significantly improves the performance of diffusion models\nregardless of the datasets, architectures, and sampling strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jooyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungbeom Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1\">Chaehun Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online panoptic 3D reconstruction as a Linear Assignment Problem. (arXiv:2204.00231v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00231","description":"<p>Real-time holistic scene understanding would allow machines to interpret\ntheir surrounding in a much more detailed manner than is currently possible.\nWhile panoptic image segmentation methods have brought image segmentation\ncloser to this goal, this information has to be described relative to the 3D\nenvironment for the machine to be able to utilise it effectively. In this\npaper, we investigate methods for sequentially reconstructing static\nenvironments from panoptic image segmentations in 3D. We specifically target\nreal-time operation: the algorithm must process data strictly online and be\nable to run at relatively fast frame rates. Additionally, the method should be\nscalable for environments large enough for practical applications. By applying\na simple but powerful data-association algorithm, we outperform earlier similar\nworks when operating purely online. Our method is also capable of reaching\nframe-rates high enough for real-time applications and is scalable to larger\nenvironments as well. Source code and further demonstrations are released to\nthe public at: \\url{https://tutvision.github.io/Online-Panoptic-3D/}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raivio_L/0/1/0/all/0/1\">Leevi Raivio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ObjectMix: Data Augmentation by Copy-Pasting Objects in Videos for Action Recognition. (arXiv:2204.00239v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00239","description":"<p>In this paper, we propose a data augmentation method for action recognition\nusing instance segmentation. Although many data augmentation methods have been\nproposed for image recognition, few methods have been proposed for action\nrecognition. Our proposed method, ObjectMix, extracts each object region from\ntwo videos using instance segmentation and combines them to create new videos.\nExperiments on two action recognition datasets, UCF101 and HMDB51, demonstrate\nthe effectiveness of the proposed method and show its superiority over\nVideoMix, a prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kimata_J/0/1/0/all/0/1\">Jun Kimata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitta_T/0/1/0/all/0/1\">Tomoya Nitta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamaki_T/0/1/0/all/0/1\">Toru Tamaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MS-HLMO: Multi-scale Histogram of Local Main Orientation for Remote Sensing Image Registration. (arXiv:2204.00260v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00260","description":"<p>Multi-source image registration is challenging due to intensity, rotation,\nand scale differences among the images. Considering the characteristics and\ndifferences of multi-source remote sensing images, a feature-based registration\nalgorithm named Multi-scale Histogram of Local Main Orientation (MS-HLMO) is\nproposed. Harris corner detection is first adopted to generate feature points.\nThe HLMO feature of each Harris feature point is extracted on a Partial Main\nOrientation Map (PMOM) with a Generalized Gradient Location and Orientation\nHistogram-like (GGLOH) feature descriptor, which provides high intensity,\nrotation, and scale invariance. The feature points are matched through a\nmulti-scale matching strategy. Comprehensive experiments on 17 multi-source\nremote sensing scenes demonstrate that the proposed MS-HLMO and its simplified\nversion MS-HLMO$^+$ outperform other competitive registration algorithms in\nterms of effectiveness and generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_C/0/1/0/all/0/1\">Chenzhong Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selecting task with optimal transport self-supervised learning for few-shot classification. (arXiv:2204.00289v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00289","description":"<p>Few-Shot classification aims at solving problems that only a few samples are\navailable in the training process. Due to the lack of samples, researchers\ngenerally employ a set of training tasks from other domains to assist the\ntarget task, where the distribution between assistant tasks and the target task\nis usually different. To reduce the distribution gap, several lines of methods\nhave been proposed, such as data augmentation and domain alignment. However,\none common drawback of these algorithms is that they ignore the similarity task\nselection before training. The fundamental problem is to push the auxiliary\ntasks close to the target task. In this paper, we propose a novel task\nselecting algorithm, named Optimal Transport Task Selecting (OTTS), to\nconstruct a training set by selecting similar tasks for Few-Shot learning.\nSpecifically, the OTTS measures the task similarity by calculating the optimal\ntransport distance and completes the model training via a self-supervised\nstrategy. By utilizing the selected tasks with OTTS, the training process of\nFew-Shot learning become more stable and effective. Other proposed methods\nincluding data augmentation and domain alignment can be used in the meantime\nwith OTTS. We conduct extensive experiments on a variety of datasets, including\nMiniImageNet, CIFAR, CUB, Cars, and Places, to evaluate the effectiveness of\nOTTS. Experimental results validate that our OTTS outperforms the typical\nbaselines, i.e., MAML, matchingnet, protonet, by a large margin (averagely\n1.72\\% accuracy improvement).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinghao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Baodi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GrowliFlower: An image time series dataset for GROWth analysis of cauLIFLOWER. (arXiv:2204.00294v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00294","description":"<p>This article presents GrowliFlower, a georeferenced, image-based UAV time\nseries dataset of two monitored cauliflower fields of size 0.39 and 0.60 ha\nacquired in 2020 and 2021. The dataset contains RGB and multispectral\northophotos from which about 14,000 individual plant coordinates are derived\nand provided. The coordinates enable the dataset users the extraction of\ncomplete and incomplete time series of image patches showing individual plants.\nThe dataset contains collected phenotypic traits of 740 plants, including the\ndevelopmental stage as well as plant and cauliflower size. As the harvestable\nproduct is completely covered by leaves, plant IDs and coordinates are provided\nto extract image pairs of plants pre and post defoliation, to facilitate\nestimations of cauliflower head size. Moreover, the dataset contains\npixel-accurate leaf and plant instance segmentations, as well as stem\nannotations to address tasks like classification, detection, segmentation,\ninstance segmentation, and similar computer vision tasks. The dataset aims to\nfoster the development and evaluation of machine learning approaches. It\nspecifically focuses on the analysis of growth and development of cauliflower\nand the derivation of phenotypic traits to foster the development of automation\nin agriculture. Two baseline results of instance segmentation at plant and leaf\nlevel based on the labeled instance segmentation data are presented. The entire\ndata set is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kierdorf_J/0/1/0/all/0/1\">Jana Kierdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junker_Frohn_L/0/1/0/all/0/1\">Laura Verena Junker-Frohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delaney_M/0/1/0/all/0/1\">Mike Delaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olave_M/0/1/0/all/0/1\">Mariele Donoso Olave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burkart_A/0/1/0/all/0/1\">Andreas Burkart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaenicke_H/0/1/0/all/0/1\">Hannah Jaenicke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_O/0/1/0/all/0/1\">Onno Muller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rascher_U/0/1/0/all/0/1\">Uwe Rascher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1\">Ribana Roscher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unitail: Detecting, Reading, and Matching in Retail Scene. (arXiv:2204.00298v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00298","description":"<p>To make full use of computer vision technology in stores, it is required to\nconsider the actual needs that fit the characteristics of the retail scene.\nPursuing this goal, we introduce the United Retail Datasets (Unitail), a\nlarge-scale benchmark of basic visual tasks on products that challenges\nalgorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped\ninstances annotated, the Unitail offers a detection dataset to align product\nappearance better. Furthermore, it provides a gallery-style OCR dataset\ncontaining 1454 product categories, 30k text regions, and 21k transcriptions to\nenable robust reading on products and motivate enhanced product matching.\nBesides benchmarking the datasets using various state-of-the-arts, we customize\na new detector for product detection and provide a simple OCR-based matching\nsolution that verifies its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaiwang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_J/0/1/0/all/0/1\">Jiachen Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_U/0/1/0/all/0/1\">Uzair Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face identification by means of a neural net classifier. (arXiv:2204.00305v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00305","description":"<p>This paper describes a novel face identification method that combines the\neigenfaces theory with the Neural Nets. We use the eigenfaces methodology in\norder to reduce the dimensionality of the input image, and a neural net\nclassifier that performs the identification process. The method presented\nrecognizes faces in the presence of variations in facial expression, facial\ndetails and lighting conditions. A recognition rate of more than 87% has been\nachieved, while the classical method of Turk and Pentland achieves a 75.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Duro_V/0/1/0/all/0/1\">Virginia Espinosa-Duro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unimodal-Concentrated Loss: Fully Adaptive Label Distribution Learning for Ordinal Regression. (arXiv:2204.00309v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00309","description":"<p>Learning from a label distribution has achieved promising results on ordinal\nregression tasks such as facial age and head pose estimation wherein, the\nconcept of adaptive label distribution learning (ALDL) has drawn lots of\nattention recently for its superiority in theory. However, compared with the\nmethods assuming fixed form label distribution, ALDL methods have not achieved\nbetter performance. We argue that existing ALDL algorithms do not fully exploit\nthe intrinsic properties of ordinal regression. In this paper, we emphatically\nsummarize that learning an adaptive label distribution on ordinal regression\ntasks should follow three principles. First, the probability corresponding to\nthe ground-truth should be the highest in label distribution. Second, the\nprobabilities of neighboring labels should decrease with the increase of\ndistance away from the ground-truth, i.e., the distribution is unimodal. Third,\nthe label distribution should vary with samples changing, and even be distinct\nfor different instances with the same label, due to the different levels of\ndifficulty and ambiguity. Under the premise of these principles, we propose a\nnovel loss function for fully adaptive label distribution learning, namely\nunimodal-concentrated loss. Specifically, the unimodal loss derived from the\nlearning to rank strategy constrains the distribution to be unimodal.\nFurthermore, the estimation error and the variance of the predicted\ndistribution for a specific sample are integrated into the proposed\nconcentrated loss to make the predicted distribution maximize at the\nground-truth and vary according to the predicting uncertainty. Extensive\nexperimental results on typical ordinal regression tasks including age and head\npose estimation, show the superiority of our proposed unimodal-concentrated\nloss compared with existing loss functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingjing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhaoliang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yachun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Pengju Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jingwei Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunmao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAT-Det: Contrastively Augmented Transformer for Multi-modal 3D Object Detection. (arXiv:2204.00325v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00325","description":"<p>In autonomous driving, LiDAR point-clouds and RGB images are two major data\nmodalities with complementary cues for 3D object detection. However, it is\nquite difficult to sufficiently use them, due to large inter-modal\ndiscrepancies. To address this issue, we propose a novel framework, namely\nContrastively Augmented Transformer for multi-modal 3D object Detection\n(CAT-Det). Specifically, CAT-Det adopts a two-stream structure consisting of a\nPointformer (PT) branch, an Imageformer (IT) branch along with a Cross-Modal\nTransformer (CMT) module. PT, IT and CMT jointly encode intra-modal and\ninter-modal long-range contexts for representing an object, thus fully\nexploring multi-modal information for detection. Furthermore, we propose an\neffective One-way Multi-modal Data Augmentation (OMDA) approach via\nhierarchical contrastive learning at both the point and object levels,\nsignificantly improving the accuracy only by augmenting point-clouds, which is\nfree from complex generation of paired samples of the two modalities. Extensive\nexperiments on the KITTI benchmark show that CAT-Det achieves a new\nstate-of-the-art, highlighting its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIP: Deep Inverse Patchmatch for High-Resolution Optical Flow. (arXiv:2204.00330v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00330","description":"<p>Recently, the dense correlation volume method achieves state-of-the-art\nperformance in optical flow. However, the correlation volume computation\nrequires a lot of memory, which makes prediction difficult on high-resolution\nimages. In this paper, we propose a novel Patchmatch-based framework to work on\nhigh-resolution optical flow estimation. Specifically, we introduce the first\nend-to-end Patchmatch based deep learning optical flow. It can get\nhigh-precision results with lower memory benefiting from propagation and local\nsearch of Patchmatch. Furthermore, a new inverse propagation is proposed to\ndecouple the complex operations of propagation, which can significantly reduce\ncalculations in multiple iterations. At the time of submission, our method\nranks first on all the metrics on the popular KITTI2015 benchmark, and ranks\nsecond on EPE on the Sintel clean benchmark among published optical flow\nmethods. Experiment shows our method has a strong cross-dataset generalization\nability that the F1-all achieves 13.73%, reducing 21% from the best published\nresult 17.4% on KITTI2015. What's more, our method shows a good details\npreserving result on the high-resolution dataset DAVIS and consumes 2x less\nmemory than RAFT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zihua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_N/0/1/0/all/0/1\">Ni Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhi Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_P/0/1/0/all/0/1\">Pengfei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiankun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RMS-FlowNet: Efficient and Robust Multi-Scale Scene Flow Estimation for Large-Scale Point Clouds. (arXiv:2204.00354v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00354","description":"<p>The proposed RMS-FlowNet is a novel end-to-end learning-based architecture\nfor accurate and efficient scene flow estimation which can operate on point\nclouds of high density. For hierarchical scene flow estimation, the existing\nmethods depend on either expensive Farthest-Point-Sampling (FPS) or\nstructure-based scaling which decrease their ability to handle a large number\nof points. Unlike these methods, we base our fully supervised architecture on\nRandom-Sampling (RS) for multiscale scene flow prediction. To this end, we\npropose a novel flow embedding design which can predict more robust scene flow\nin conjunction with RS. Exhibiting high accuracy, our RMS-FlowNet provides a\nfaster prediction than state-of-the-art methods and works efficiently on\nconsecutive dense point clouds of more than 250K points at once. Our\ncomprehensive experiments verify the accuracy of RMS-FlowNet on the established\nFlyingThings3D data set with different point cloud densities and validate our\ndesign choices. Additionally, we show that our model presents a competitive\nability to generalize towards the real-world scenes of KITTI data set without\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Battrawy_R/0/1/0/all/0/1\">Ramy Battrawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_R/0/1/0/all/0/1\">Ren&#xe9; Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahani_M/0/1/0/all/0/1\">Mohammad-Ali Nikouei Mahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Deblur using Light Field Generated and Real Defocus Images. (arXiv:2204.00367v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00367","description":"<p>Defocus deblurring is a challenging task due to the spatially varying nature\nof defocus blur. While deep learning approach shows great promise in solving\nimage restoration problems, defocus deblurring demands accurate training data\nthat consists of all-in-focus and defocus image pairs, which is difficult to\ncollect. Naive two-shot capturing cannot achieve pixel-wise correspondence\nbetween the defocused and all-in-focus image pairs. Synthetic aperture of light\nfields is suggested to be a more reliable way to generate accurate image pairs.\nHowever, the defocus blur generated from light field data is different from\nthat of the images captured with a traditional digital camera. In this paper,\nwe propose a novel deep defocus deblurring network that leverages the strength\nand overcomes the shortcoming of light fields. We first train the network on a\nlight field-generated dataset for its highly accurate image correspondence.\nThen, we fine-tune the network using feature loss on another dataset collected\nby the two-shot method to alleviate the differences between the defocus blur\nexists in the two domains. This strategy is proved to be highly effective and\nable to achieve the state-of-the-art performance both quantitatively and\nqualitatively on multiple test sets. Extensive ablation studies have been\nconducted to analyze the effect of each network module to the final\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ruan_L/0/1/0/all/0/1\">Lingyan Ruan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jizhou Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lam_M/0/1/0/all/0/1\">Miuling Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot One-class Domain Adaptation Based on Frequency for Iris Presentation Attack Detection. (arXiv:2204.00376v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00376","description":"<p>Iris presentation attack detection (PAD) has achieved remarkable success to\nensure the reliability and security of iris recognition systems. Most existing\nmethods exploit discriminative features in the spatial domain and report\noutstanding performance under intra-dataset settings. However, the degradation\nof performance is inevitable under cross-dataset settings, suffering from\ndomain shift. In consideration of real-world applications, a small number of\nbonafide samples are easily accessible. We thus define a new domain adaptation\nsetting called Few-shot One-class Domain Adaptation (FODA), where adaptation\nonly relies on a limited number of target bonafide samples. To address this\nproblem, we propose a novel FODA framework based on the expressive power of\nfrequency information. Specifically, our method integrates frequency-related\ninformation through two proposed modules. Frequency-based Attention Module\n(FAM) aggregates frequency information into spatial attention and explicitly\nemphasizes high-frequency fine-grained features. Frequency Mixing Module (FMM)\nmixes certain frequency components to generate large-scale target-style samples\nfor adaptation with limited target bonafide samples. Extensive experiments on\nLivDet-Iris 2017 dataset demonstrate the proposed method achieves\nstate-of-the-art or competitive performance under both cross-dataset and\nintra-dataset settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yachun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1\">Ying Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingjing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunmao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Regional and Temporal Learning for Facial Action Unit Recognition. (arXiv:2204.00379v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00379","description":"<p>Automatic facial action unit (AU) recognition is a challenging task due to\nthe scarcity of manual annotations. To alleviate this problem, a large amount\nof efforts has been dedicated to exploiting various weakly supervised methods\nwhich leverage numerous unlabeled data. However, many aspects with regard to\nsome unique properties of AUs, such as the regional and relational\ncharacteristics, are not sufficiently explored in previous works. Motivated by\nthis, we take the AU properties into consideration and propose two auxiliary AU\nrelated tasks to bridge the gap between limited annotations and the model\nperformance in a self-supervised manner via the unlabeled data. Specifically,\nto enhance the discrimination of regional features with AU relation embedding,\nwe design a task of RoI inpainting to recover the randomly cropped AU patches.\nMeanwhile, a single image based optical flow estimation task is proposed to\nleverage the dynamic change of facial muscles and encode the motion information\ninto the global feature representation. Based on these two self-supervised\nauxiliary tasks, local features, mutual relation and motion cues of AUs are\nbetter captured in the backbone network. Furthermore, by incorporating\nsemi-supervised learning, we propose an end-to-end trainable framework named\nweakly supervised regional and temporal learning (WSRTL) for AU recognition.\nExtensive experiments on BP4D and DISFA demonstrate the superiority of our\nmethod and new state-of-the-art performances are achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jingwei Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingjing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunmao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoencoder Attractors for Uncertainty Estimation. (arXiv:2204.00382v1 [cs.LG])","link":"http://arxiv.org/abs/2204.00382","description":"<p>The reliability assessment of a machine learning model's prediction is an\nimportant quantity for the deployment in safety critical applications. Not only\ncan it be used to detect novel sceneries, either as out-of-distribution or\nanomaly sample, but it also helps to determine deficiencies in the training\ndata distribution. A lot of promising research directions have either proposed\ntraditional methods like Gaussian processes or extended deep learning based\napproaches, for example, by interpreting them from a Bayesian point of view. In\nthis work we propose a novel approach for uncertainty estimation based on\nautoencoder models: The recursive application of a previously trained\nautoencoder model can be interpreted as a dynamical system storing training\nexamples as attractors. While input images close to known samples will converge\nto the same or similar attractor, input samples containing unknown features are\nunstable and converge to different training samples by potentially removing or\nchanging characteristic features. The use of dropout during training and\ninference leads to a family of similar dynamical systems, each one being robust\non samples close to the training distribution but unstable on new features.\nEither the model reliably removes these features or the resulting instability\ncan be exploited to detect problematic input samples. We evaluate our approach\non several dataset combinations as well as on an industrial application for\noccupant classification in the vehicle interior for which we additionally\nrelease a new synthetic dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cruz_S/0/1/0/all/0/1\">Steve Dias Da Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taetz_B/0/1/0/all/0/1\">Bertram Taetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stifter_T/0/1/0/all/0/1\">Thomas Stifter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoencoder for Synthetic to Real Generalization: From Simple to More Complex Scenes. (arXiv:2204.00386v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00386","description":"<p>Learning on synthetic data and transferring the resulting properties to their\nreal counterparts is an important challenge for reducing costs and increasing\nsafety in machine learning. In this work, we focus on autoencoder architectures\nand aim at learning latent space representations that are invariant to\ninductive biases caused by the domain shift between simulated and real images\nshowing the same scenario. We train on synthetic images only, present\napproaches to increase generalizability and improve the preservation of the\nsemantics to real datasets of increasing visual complexity. We show that\npre-trained feature extractors (e.g. VGG) can be sufficient for generalization\non images of lower complexity, but additional improvements are required for\nvisually more complex scenes. To this end, we demonstrate a new sampling\ntechnique, which matches semantically important parts of the image, while\nrandomizing the other parts, leads to salient feature extraction and a\nneglection of unimportant parts. This helps the generalization to real data and\nwe further show that our approach outperforms fine-tuned classification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cruz_S/0/1/0/all/0/1\">Steve Dias Da Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taetz_B/0/1/0/all/0/1\">Bertram Taetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stifter_T/0/1/0/all/0/1\">Thomas Stifter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of convolutional neural networks for cloudy optical images reconstruction from single or multitemporal joint SAR and optical images. (arXiv:2204.00424v1 [eess.IV])","link":"http://arxiv.org/abs/2204.00424","description":"<p>With the increasing availability of optical and synthetic aperture radar\n(SAR) images thanks to the Sentinel constellation, and the explosion of deep\nlearning, new methods have emerged in recent years to tackle the reconstruction\nof optical images that are impacted by clouds. In this paper, we focus on the\nevaluation of convolutional neural networks that use jointly SAR and optical\nimages to retrieve the missing contents in one single polluted optical image.\nWe propose a simple framework that ease the creation of datasets for the\ntraining of deep nets targeting optical image reconstruction, and for the\nvalidation of machine learning based or deterministic approaches. These methods\nare quite different in terms of input images constraints, and comparing them is\na problematic task not addressed in the literature. We show how space\npartitioning data structures help to query samples in terms of cloud coverage,\nrelative acquisition date, pixel validity and relative proximity between SAR\nand optical images. We generate several datasets to compare the reconstructed\nimages from networks that use a single pair of SAR and optical image, versus\nnetworks that use multiple pairs, and a traditional deterministic approach\nperforming interpolation in temporal domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cresson_R/0/1/0/all/0/1\">R&#xe9;mi Cresson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Narcon_N/0/1/0/all/0/1\">Nicolas Nar&#xe7;on</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaetano_R/0/1/0/all/0/1\">Raffaele Gaetano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dupuis_A/0/1/0/all/0/1\">Aurore Dupuis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanguy_Y/0/1/0/all/0/1\">Yannick Tanguy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+May_S/0/1/0/all/0/1\">St&#xe9;phane May</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Commandre_B/0/1/0/all/0/1\">Benjamin Commandre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Fast and Efficient Conditional Learning for Tunable Trade-Off between Accuracy and Robustness. (arXiv:2204.00426v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00426","description":"<p>Existing models that achieve state-of-the-art (SOTA) performance on both\nclean and adversarially-perturbed images rely on convolution operations\nconditioned with feature-wise linear modulation (FiLM) layers. These layers\nrequire many new parameters and are hyperparameter sensitive. They\nsignificantly increase training time, memory cost, and potential latency which\ncan prove costly for resource-limited or real-time applications. In this paper,\nwe present a fast learnable once-for-all adversarial training (FLOAT)\nalgorithm, which instead of the existing FiLM-based conditioning, presents a\nunique weight conditioned learning that requires no additional layer, thereby\nincurring no significant increase in parameter count, training time, or network\nlatency compared to standard adversarial training. In particular, we add\nconfigurable scaled noise to the weight tensors that enables a trade-off\nbetween clean and adversarial performance. Extensive experiments show that\nFLOAT can yield SOTA performance improving both clean and perturbed image\nclassification by up to ~6% and ~10%, respectively. Moreover, real hardware\nmeasurement shows that FLOAT can reduce the training time by up to 1.43x with\nfewer model parameters of up to 1.47x on iso-hyperparameter settings compared\nto the FiLM-based alternatives. Additionally, to further improve memory\nefficiency we introduce FLOAT sparse (FLOATS), a form of non-iterative model\npruning and provide detailed empirical analysis to provide a three way\naccuracy-robustness-complexity trade-off for these new class of pruned\nconditionally trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Souvik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_S/0/1/0/all/0/1\">Sairam Sundaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1\">Massoud Pedram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1\">Peter A. Beerel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Marginal Contrastive Correspondence for Guided Image Generation. (arXiv:2204.00442v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00442","description":"<p>Exemplar-based image translation establishes dense correspondences between a\nconditional input and an exemplar (from two different domains) for leveraging\ndetailed exemplar styles to achieve realistic image translation. Existing work\nbuilds the cross-domain correspondences implicitly by minimizing feature-wise\ndistances across the two domains. Without explicit exploitation of\ndomain-invariant features, this approach may not reduce the domain gap\neffectively which often leads to sub-optimal correspondences and image\ntranslation. We design a Marginal Contrastive Learning Network (MCL-Net) that\nexplores contrastive learning to learn domain-invariant features for realistic\nexemplar-based image translation. Specifically, we design an innovative\nmarginal contrastive loss that guides to establish dense correspondences\nexplicitly. Nevertheless, building correspondence with domain-invariant\nsemantics alone may impair the texture patterns and lead to degraded texture\ngeneration. We thus design a Self-Correlation Map (SCM) that incorporates scene\nstructures as auxiliary information which improves the built correspondences\nsubstantially. Quantitative and qualitative experiments on multifarious image\ntranslation tasks show that the proposed method outperforms the\nstate-of-the-art consistently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rongliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changgong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition. (arXiv:2204.00452v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00452","description":"<p>We propose Multi-head Self/Cross-Attention (MSCA), which introduces a\ntemporal cross-attention mechanism for action recognition, based on the\nstructure of the Multi-head Self-Attention (MSA) mechanism of the Vision\nTransformer (ViT). Simply applying ViT to each frame of a video frame can\ncapture frame features, but cannot model temporal features. However, simply\nmodeling temporal information with CNN or Transfomer is computationally\nexpensive. TSM that perform feature shifting assume a CNN and cannot take\nadvantage of the ViT structure. The proposed model captures temporal\ninformation by shifting the Query, Key, and Value in the calculation of MSA of\nViT. This is efficient without additional coinformationmputational effort and\nis a suitable structure for extending ViT over temporal. Experiments on\nKineitcs400 show the effectiveness of the proposed method and its superiority\nover previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hashiguchi_R/0/1/0/all/0/1\">Ryota Hashiguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamaki_T/0/1/0/all/0/1\">Toru Tamaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomous crater detection on asteroids using a fully-convolutional neural network. (arXiv:2204.00477v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00477","description":"<p>This paper shows the application of autonomous Crater Detection using the\nU-Net, a Fully-Convolutional Neural Network, on Ceres. The U-Net is trained on\noptical images of the Moon Global Morphology Mosaic based on data collected by\nthe LRO and manual crater catalogues. The Moon-trained network will be tested\non Dawn optical images of Ceres: this task is accomplished by means of a\nTransfer Learning (TL) approach. The trained model has been fine-tuned using\n100, 500 and 1000 additional images of Ceres. The test performance was measured\non 350 never before seen images, reaching a testing accuracy of 96.24%, 96.95%\nand 97.19%, respectively. This means that despite the intrinsic differences\nbetween the Moon and Ceres, TL works with encouraging results. The output of\nthe U-Net contains predicted craters: it will be post-processed applying global\nthresholding for image binarization and a template matching algorithm to\nextract craters positions and radii in the pixel space. Post-processed craters\nwill be counted and compared to the ground truth data in order to compute image\nsegmentation metrics: precision, recall and F1 score. These indices will be\ncomputed, and their effect will be discussed for tasks such as automated crater\ncataloguing and optical navigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Latorre_F/0/1/0/all/0/1\">Francesco Latorre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curti_F/0/1/0/all/0/1\">Fabio Curti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proper Reuse of Image Classification Features Improves Object Detection. (arXiv:2204.00484v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00484","description":"<p>A common practice in transfer learning is to initialize the downstream model\nweights by pre-training on a data-abundant upstream task. In object detection\nspecifically, the feature backbone is typically initialized with Imagenet\nclassifier weights and fine-tuned on the object detection task. Recent works\nshow this is not strictly necessary under longer training regimes and provide\nrecipes for training the backbone from scratch. We investigate the opposite\ndirection of this end-to-end training trend: we show that an extreme form of\nknowledge preservation -- freezing the classifier-initialized backbone --\nconsistently improves many different detection models, and leads to\nconsiderable resource savings. We hypothesize and corroborate experimentally\nthat the remaining detector components capacity and structure is a crucial\nfactor in leveraging the frozen backbone. Immediate applications of our\nfindings include performance improvements on hard cases like detection of\nlong-tail object classes and computational and memory resource savings that\ncontribute to making the field more accessible to researchers with access to\nfewer computational resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1\">Cristina Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birodkar_V/0/1/0/all/0/1\">Vighnesh Birodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generic Event Boundary Captioning: A Benchmark for Status Changes Understanding. (arXiv:2204.00486v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00486","description":"<p>Cognitive science has shown that humans perceive videos in terms of events\nseparated by state changes of dominant subjects. State changes trigger new\nevents and are one of the most useful among the large amount of redundant\ninformation perceived. However, previous research focuses on the overall\nunderstanding of segments without evaluating the fine-grained status changes\ninside. In this paper, we introduce a new dataset called Kinetic-GEBC (Generic\nEvent Boundary Captioning). The dataset consists of over 170k boundaries\nassociated with captions describing status changes in the generic events in 12K\nvideos. Upon this new dataset, we propose three tasks supporting the\ndevelopment of a more fine-grained, robust, and human-like understanding of\nvideos through status changes. We evaluate many representative baselines in our\ndataset, where we also design a new TPD (Temporal-based Pairwise Difference)\nModeling method for current state-of-the-art backbones and achieve significant\nperformance improvements. Besides, the results show there are still formidable\nchallenges for current methods in the utilization of different granularities,\nrepresentation of visual difference, and the accurate localization of status\nchanges. Further analysis shows that our dataset can drive developing more\npowerful methods to understand status changes and thus improve video level\ncomprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Stan Weixian Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1\">Matt Feiszli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FrequencyLowCut Pooling -- Plug & Play against Catastrophic Overfitting. (arXiv:2204.00491v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00491","description":"<p>Over the last years, Convolutional Neural Networks (CNNs) have been the\ndominating neural architecture in a wide range of computer vision tasks. From\nan image and signal processing point of view, this success might be a bit\nsurprising as the inherent spatial pyramid design of most CNNs is apparently\nviolating basic signal processing laws, i.e. Sampling Theorem in their\ndown-sampling operations. However, since poor sampling appeared not to affect\nmodel accuracy, this issue has been broadly neglected until model robustness\nstarted to receive more attention. Recent work [17] in the context of\nadversarial attacks and distribution shifts, showed after all, that there is a\nstrong correlation between the vulnerability of CNNs and aliasing artifacts\ninduced by poor down-sampling operations. This paper builds on these findings\nand introduces an aliasing free down-sampling operation which can easily be\nplugged into any CNN architecture: FrequencyLowCut pooling. Our experiments\nshow, that in combination with simple and fast FGSM adversarial training, our\nhyper-parameter free operator significantly improves model robustness and\navoids catastrophic overfitting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grabinski_J/0/1/0/all/0/1\">Julia Grabinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Steffen Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1\">Margret Keuper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DFNet: Enhance Aboslute Pose Regression with Direct Feature Matching. (arXiv:2204.00559v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00559","description":"<p>We introduce a camera relocalization pipeline that combines absolute pose\nregression (APR) and direct feature matching. Existing photometric-based\nmethods have trouble on scenes with large photometric distortions, e.g. outdoor\nenvironments. By incorporating an exposure-adaptive novel view synthesis, our\nmethods can successfully address the challenges. Moreover, by introducing\ndomain-invariant feature matching, our solution can improve pose regression\naccuracy while using semi-supervised learning on unlabeled data. In particular,\nthe pipeline consists of two components, Novel View Synthesizer and FeatureNet\n(DFNet). The former synthesizes novel views compensating for changes in\nexposure and the latter regresses camera poses and extracts robust features\nthat bridge the domain gap between real images and synthetic ones. We show that\ndomain invariant feature matching effectively enhances camera pose estimation\nboth in indoor and outdoor scenes. Hence, our method achieves a\nstate-of-the-art accuracy by outperforming existing single-image APR methods by\nas much as 56%, comparable to 3D structure-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Prisacariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation. (arXiv:2204.00570v1 [cs.LG])","link":"http://arxiv.org/abs/2204.00570","description":"<p>We consider unsupervised domain adaptation (UDA), where labeled data from a\nsource domain (e.g., photographs) and unlabeled data from a target domain\n(e.g., sketches) are used to learn a classifier for the target domain.\nConventional UDA methods (e.g., domain adversarial training) learn\ndomain-invariant features to improve generalization to the target domain. In\nthis paper, we show that contrastive pre-training, which learns features on\nunlabeled source and target data and then fine-tunes on labeled source data, is\ncompetitive with strong UDA methods. However, we find that contrastive\npre-training does not learn domain-invariant features, diverging from\nconventional UDA intuitions. We show theoretically that contrastive\npre-training can learn features that vary subtantially across domains but still\ngeneralize to the target domain, by disentangling domain and class information.\nOur results suggest that domain invariance is not necessary for UDA. We\nempirically validate our theory on benchmark vision datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">Kendrick Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">Robbie Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ananya Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1\">Jeff Z. HaoChen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Automatic Object Registration for Human-Robot Collaboration in Industrial Manufacturing. (arXiv:2204.00597v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00597","description":"<p>We present an end-to-end framework for fast retraining of object detection\nmodels in human-robot-collaboration. Our Faster R-CNN based setup covers the\nwhole workflow of automatic image generation and labeling, model retraining\non-site as well as inference on a FPGA edge device. The intervention of a human\noperator reduces to providing the new object together with its label and\nstarting the training process. Moreover, we present a new loss, the\nintraspread-objectosphere loss, to tackle the problem of open world\nrecognition. Though it fails to completely solve the problem, it significantly\nreduces the number of false positive detections of unknown objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geiss_M/0/1/0/all/0/1\">Manuela Gei&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baresch_M/0/1/0/all/0/1\">Martin Baresch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chasparis_G/0/1/0/all/0/1\">Georgios Chasparis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweiger_E/0/1/0/all/0/1\">Edwin Schweiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teringl_N/0/1/0/all/0/1\">Nico Teringl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwick_M/0/1/0/all/0/1\">Michael Zwick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. (arXiv:2204.00598v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00598","description":"<p>Large foundation models can exhibit unique capabilities depending on the\ndomain of data they are trained on. While these domains are generic, they may\nonly barely overlap. For example, visual-language models (VLMs) are trained on\nInternet-scale image captions, but large language models (LMs) are further\ntrained on Internet-scale text with no images (e.g. from spreadsheets, to SAT\nquestions). As a result, these models store different forms of commonsense\nknowledge across different domains. In this work, we show that this model\ndiversity is symbiotic, and can be leveraged to build AI systems with\nstructured Socratic dialogue -- in which new multimodal tasks are formulated as\na guided language-based exchange between different pre-existing foundation\nmodels, without additional finetuning. In the context of egocentric perception,\nwe present a case study of Socratic Models (SMs) that can provide meaningful\nresults for complex tasks such as generating free-form answers to contextual\nquestions about egocentric video, by formulating video Q&amp;A as short story Q&amp;A,\ni.e. summarizing the video into a short story, then answering questions about\nit. Additionally, SMs can generate captions for Internet images, and are\ncompetitive with state-of-the-art on zero-shot video-to-text retrieval with\n42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate how to compose foundation models\nzero-shot to capture new multimodal functionalities, without domain-specific\ndata collection. Prototypes are available at socraticmodels.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Adrian Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welker_S/0/1/0/all/0/1\">Stefan Welker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_A/0/1/0/all/0/1\">Aveek Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindhwani_V/0/1/0/all/0/1\">Vikas Sindhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Johnny Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhoucke_V/0/1/0/all/0/1\">Vincent Vanhoucke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantized GAN for Complex Music Generation from Dance Videos. (arXiv:2204.00604v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00604","description":"<p>We present Dance2Music-GAN (D2M-GAN), a novel adversarial multi-modal\nframework that generates complex musical samples conditioned on dance videos.\nOur proposed framework takes dance video frames and human body motion as input,\nand learns to generate music samples that plausibly accompany the corresponding\ninput. Unlike most existing conditional music generation works that generate\nspecific types of mono-instrumental sounds using symbolic audio representations\n(e.g., MIDI), and that heavily rely on pre-defined musical synthesizers, in\nthis work we generate dance music in complex styles (e.g., pop, breakdancing,\netc.) by employing a Vector Quantized (VQ) audio representation, and leverage\nboth its generality and the high abstraction capacity of its symbolic and\ncontinuous counterparts. By performing an extensive set of experiments on\nmultiple datasets, and following a comprehensive evaluation protocol, we assess\nthe generative quality of our approach against several alternatives. The\nquantitative results, which measure the music consistency, beats\ncorrespondence, and music diversity, clearly demonstrate the effectiveness of\nour proposed method. Last but not least, we curate a challenging dance-music\ndataset of in-the-wild TikTok videos, which we use to further demonstrate the\nefficacy of our approach in real-world applications - and which we hope to\nserve as a starting point for relevant future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1\">Kyle Olszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achlioptas_P/0/1/0/all/0/1\">Panos Achlioptas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1\">Menglei Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Importance of Asymmetry for Siamese Representation Learning. (arXiv:2204.00613v1 [cs.CV])","link":"http://arxiv.org/abs/2204.00613","description":"<p>Many recent self-supervised frameworks for visual representation learning are\nbased on certain forms of Siamese networks. Such networks are conceptually\nsymmetric with two parallel encoders, but often practically asymmetric as\nnumerous mechanisms are devised to break the symmetry. In this work, we conduct\na formal study on the importance of asymmetry by explicitly distinguishing the\ntwo encoders within the network -- one produces source encodings and the other\ntargets. Our key insight is keeping a relatively lower variance in target than\nsource generally benefits learning. This is empirically justified by our\nresults from five case studies covering different variance-oriented designs,\nand is aligned with our preliminary theoretical analysis on the baseline.\nMoreover, we find the improvements from asymmetric designs generalize well to\nlonger training schedules, multiple other frameworks and newer backbones.\nFinally, the combined effect of several asymmetric designs achieves a\nstate-of-the-art accuracy on ImageNet linear probing and competitive results on\ndownstream transfer. We hope our exploration will inspire more research in\nexploiting asymmetry for Siamese representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kihara_D/0/1/0/all/0/1\">Daisuke Kihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinlei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplicial Embeddings in Self-Supervised Learning and Downstream Classification. (arXiv:2204.00616v1 [cs.LG])","link":"http://arxiv.org/abs/2204.00616","description":"<p>We introduce Simplicial Embeddings (SEMs) as a way to constrain the encoded\nrepresentations of a self-supervised model to $L$ simplices of $V$ dimensions\neach using a Softmax operation. This procedure imposes a structure on the\nrepresentations that reduce their expressivity for training downstream\nclassifiers, which helps them generalize better. Specifically, we show that the\ntemperature $\\tau$ of the Softmax operation controls for the SEM\nrepresentation's expressivity, allowing us to derive a tighter downstream\nclassifier generalization bound than that for classifiers using unnormalized\nrepresentations. We empirically demonstrate that SEMs considerably improve\ngeneralization on natural image datasets such as CIFAR-100 and ImageNet.\nFinally, we also present evidence of the emergence of semantically relevant\nfeatures in SEMs, a pattern that is absent from baseline self-supervised\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lavoie_S/0/1/0/all/0/1\">Samuel Lavoie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsirigotis_C/0/1/0/all/0/1\">Christos Tsirigotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarzer_M/0/1/0/all/0/1\">Max Schwarzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vani_A/0/1/0/all/0/1\">Ankit Vani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition. (arXiv:1905.12019v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1905.12019","description":"<p>Modern deep neural networks are well known to be brittle in the face of\nunknown data instances and recognition of the latter remains a challenge.\nAlthough it is inevitable for continual-learning systems to encounter such\nunseen concepts, the corresponding literature appears to nonetheless focus\nprimarily on alleviating catastrophic interference with learned\nrepresentations. In this work, we introduce a probabilistic approach that\nconnects these perspectives based on variational inference in a single deep\nautoencoder model. Specifically, we propose to bound the approximate posterior\nby fitting regions of high density on the basis of correctly classified data\npoints. These bounds are shown to serve a dual purpose: unseen unknown\nout-of-distribution data can be distinguished from already trained known tasks\ntowards robust application. Simultaneously, to retain already acquired\nknowledge, a generative replay process can be narrowed to strictly\nin-distribution samples, in order to significantly alleviate catastrophic\ninterference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mundt_M/0/1/0/all/0/1\">Martin Mundt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pliushch_I/0/1/0/all/0/1\">Iuliia Pliushch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1\">Sagnik Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yongwon Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1\">Visvanathan Ramesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Night-time Scene Parsing with a Large Real Dataset. (arXiv:2003.06883v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.06883","description":"<p>Although huge progress has been made on scene analysis in recent years, most\nexisting works assume the input images to be in day-time with good lighting\nconditions. In this work, we aim to address the night-time scene parsing (NTSP)\nproblem, which has two main challenges: 1) labeled night-time data are scarce,\nand 2) over- and under-exposures may co-occur in the input night-time images\nand are not explicitly modeled in existing pipelines. To tackle the scarcity of\nnight-time data, we collect a novel labeled dataset, named {\\it NightCity}, of\n4,297 real night-time images with ground truth pixel-level semantic\nannotations. To our knowledge, NightCity is the largest dataset for NTSP. In\naddition, we also propose an exposure-aware framework to address the NTSP\nproblem through augmenting the segmentation process with explicitly learned\nexposure features. Extensive experiments show that training on NightCity can\nsignificantly improve NTSP performances and that our exposure-aware model\noutperforms the state-of-the-art methods, yielding top performances on our\ndataset as well as existing datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Ying Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging. (arXiv:2008.12750v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2008.12750","description":"<p>Ultrafast ultrasound (US) revolutionized biomedical imaging with its\ncapability of acquiring full-view frames at over 1 kHz, unlocking breakthrough\nmodalities such as shear-wave elastography and functional US neuroimaging. Yet,\nit suffers from strong diffraction artifacts, mainly caused by grating lobes,\nside lobes, or edge waves. Multiple acquisitions are typically required to\nobtain a sufficient image quality, at the cost of a reduced frame rate. To\nanswer the increasing demand for high-quality imaging from single unfocused\nacquisitions, we propose a two-step convolutional neural network (CNN)-based\nimage reconstruction method, compatible with real-time imaging. A low-quality\nestimate is obtained by means of a backprojection-based operation, akin to\nconventional delay-and-sum beamforming, from which a high-quality image is\nrestored using a residual CNN with multiscale and multichannel filtering\nproperties, trained specifically to remove the diffraction artifacts inherent\nto ultrafast US imaging. To account for both the high dynamic range and the\noscillating properties of radio frequency US images, we introduce the mean\nsigned logarithmic absolute error (MSLAE) as a training loss function.\nExperiments were conducted with a linear transducer array, in single plane-wave\n(PW) imaging. Trainings were performed on a simulated dataset, crafted to\ncontain a wide diversity of structures and echogenicities. Extensive numerical\nevaluations demonstrate that the proposed approach can reconstruct images from\nsingle PWs with a quality similar to that of gold-standard synthetic aperture\nimaging, on a dynamic range in excess of 60 dB. In vitro and in vivo\nexperiments show that trainings carried out on simulated data perform well in\nexperimental settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Perdios_D/0/1/0/all/0/1\">Dimitris Perdios</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vonlanthen_M/0/1/0/all/0/1\">Manuel Vonlanthen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martinez_F/0/1/0/all/0/1\">Florian Martinez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arditi_M/0/1/0/all/0/1\">Marcel Arditi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thiran_J/0/1/0/all/0/1\">Jean-Philippe Thiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.04631","description":"<p>Machine translation between many languages at once is highly challenging,\nsince training with ground truth requires supervision between all language\npairs, which is difficult to obtain. Our key insight is that, while languages\nmay vary drastically, the underlying visual appearance of the world remains\nconsistent. We introduce a method that uses visual observations to bridge the\ngap between languages, rather than relying on parallel corpora or topological\nproperties of the representations. We train a model that aligns segments of\ntext from different languages if and only if the images associated with them\nare similar and each image in turn is well-aligned with its textual\ndescription. We train our model from scratch on a new dataset of text in over\nfifty languages with accompanying images. Experiments show that our method\noutperforms previous work on unsupervised word and sentence translation using\nretrieval. Code, models and data are available on globetrotter.cs.columbia.edu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suris_D/0/1/0/all/0/1\">D&#xed;dac Sur&#xed;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epstein_D/0/1/0/all/0/1\">Dave Epstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Analysis for Semantic Segmentation with Applications on Feature Truncation and Weak Annotation. (arXiv:2012.14123v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.14123","description":"<p>We propose spectral analysis to investigate the correlation between the\naccuracy and the resolution of segmentation maps for semantic segmentation. The\ncurrent networks predict segmentation maps on the down-sampled grid of images\nto alleviate the computational cost. Moreover, these networks can be trained by\nweak annotations that utilize only the coarse contour of segmentation maps.\nDespite the successful achievement of these works utilizing the low-frequency\ninformation of segmentation maps, however, the accuracy of resultant\nsegmentation maps may also be degraded in the regions near object boundaries.\nIt is yet unclear for a theoretical guideline to determine an optimal\ndown-sampled grid to strike the balance between the cost and the accuracy of\nsegmentation. We analyze the objective function (cross-entropy) and network\nback-propagation process in frequency domain. We discover that cross-entropy\nand key features of CNN are mainly contributed by the low-frequency components\nof segmentation maps. This further provides us quantitative results to\ndetermine the efficacy of down-sampled grid of segmentation maps. The analysis\nis then validated on the two applications: the feature truncation method and\nthe block-wise annotation that limit the high-frequency components of the CNN\nfeatures and annotation, respectively. The results agree with our analysis.\nThus the success of the existing work utilizing low-frequency information of\nsegmentation maps now has theoretical foundation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chin-Tien Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROAD: The ROad event Awareness Dataset for Autonomous Driving. (arXiv:2102.11585v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.11585","description":"<p>Humans drive in a holistic fashion which entails, in particular,\nunderstanding dynamic road events and their evolution. Injecting these\ncapabilities in autonomous vehicles can thus take situational awareness and\ndecision making closer to human-level performance. To this purpose, we\nintroduce the ROad event Awareness Dataset (ROAD) for Autonomous Driving, to\nour knowledge the first of its kind. ROAD is designed to test an autonomous\nvehicle's ability to detect road events, defined as triplets composed by an\nactive agent, the action(s) it performs and the corresponding scene locations.\nROAD comprises videos originally from the Oxford RobotCar Dataset annotated\nwith bounding boxes showing the location in the image plane of each road event.\nWe benchmark various detection tasks, proposing as a baseline a new incremental\nalgorithm for online road event awareness termed 3D-RetinaNet. We also report\nthe performance on the ROAD tasks of Slowfast and YOLOv5 detectors, as well as\nthat of the winners of the ICCV2021 ROAD challenge, which highlight the\nchallenges faced by situation awareness in autonomous driving. ROAD is designed\nto allow scholars to investigate exciting tasks such as complex (road) activity\ndetection, future event anticipation and continual learning. The dataset is\navailable at https://github.com/gurkirt/road-dataset; the baseline can be found\nat https://github.com/gurkirt/3D-RetinaNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gurkirt Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akrigg_S/0/1/0/all/0/1\">Stephen Akrigg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maio_M/0/1/0/all/0/1\">Manuele Di Maio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fontana_V/0/1/0/all/0/1\">Valentina Fontana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alitappeh_R/0/1/0/all/0/1\">Reza Javanmard Alitappeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Suman Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeddisaravi_K/0/1/0/all/0/1\">Kossar Jeddisaravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousefi_F/0/1/0/all/0/1\">Farzad Yousefi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culley_J/0/1/0/all/0/1\">Jacob Culley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholson_T/0/1/0/all/0/1\">Tom Nicholson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omokeowa_J/0/1/0/all/0/1\">Jordan Omokeowa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grazioso_S/0/1/0/all/0/1\">Stanislao Grazioso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1\">Andrew Bradley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gironimo_G/0/1/0/all/0/1\">Giuseppe Di Gironimo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuzzolin_F/0/1/0/all/0/1\">Fabio Cuzzolin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Low-Rank Simplicity Bias in Deep Networks. (arXiv:2103.10427v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.10427","description":"<p>Modern deep neural networks are highly over-parameterized compared to the\ndata on which they are trained, yet they often generalize remarkably well. A\nflurry of recent work has asked: why do deep networks not overfit to their\ntraining data? In this work, we make a series of empirical observations that\ninvestigate and extend the hypothesis that deeper networks are inductively\nbiased to find solutions with lower effective rank embeddings. We conjecture\nthat this bias exists because the volume of functions that maps to low\neffective rank embedding increases with depth. We show empirically that our\nclaim holds true on finite width linear and non-linear models on practical\nlearning paradigms and show that on natural data, these are often the solutions\nthat generalize well. We then show that the simplicity bias exists at both\ninitialization and after training and is resilient to hyper-parameters and\nlearning methods. We further demonstrate how linear over-parameterization of\ndeep non-linear models can be used to induce low-rank bias, improving\ngeneralization performance on CIFAR and ImageNet without changing the modeling\ncapacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huh_M/0/1/0/all/0/1\">Minyoung Huh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobahi_H/0/1/0/all/0/1\">Hossein Mobahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richard Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1\">Brian Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Pulkit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Correspondences: Video Prediction with Correspondence-wise Losses. (arXiv:2104.09498v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09498","description":"<p>Image prediction methods often struggle on tasks that require changing the\npositions of objects, such as video prediction, producing blurry images that\naverage over the many positions that objects might occupy. In this paper, we\npropose a simple change to existing image similarity metrics that makes them\nmore robust to positional errors: we match the images using optical flow, then\nmeasure the visual similarity of corresponding pixels. This change leads to\ncrisper and more perceptually accurate predictions, and does not require\nmodifications to the image prediction network. We apply our method to a variety\nof video prediction tasks, where it obtains strong performance with simple\nnetwork architectures, and to the closely related task of video interpolation.\nCode and results are available at our webpage:\nhttps://dangeng.github.io/CorrWiseLosses\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_D/0/1/0/all/0/1\">Daniel Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_M/0/1/0/all/0/1\">Max Hamilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owens_A/0/1/0/all/0/1\">Andrew Owens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyTr$^2$: Image Style Transfer with Transformers. (arXiv:2105.14576v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14576","description":"<p>The goal of image style transfer is to render an image with artistic features\nguided by a style reference while maintaining the original content. Owing to\nthe locality in convolutional neural networks (CNNs), extracting and\nmaintaining the global information of input images is difficult. Therefore,\ntraditional neural style transfer methods face biased content representation.\nTo address this critical issue, we take long-range dependencies of input images\ninto account for image style transfer by proposing a transformer-based approach\ncalled StyTr$^2$. In contrast with visual transformers for other vision tasks,\nStyTr$^2$ contains two different transformer encoders to generate\ndomain-specific sequences for content and style, respectively. Following the\nencoders, a multi-layer transformer decoder is adopted to stylize the content\nsequence according to the style sequence. We also analyze the deficiency of\nexisting positional encoding methods and propose the content-aware positional\nencoding (CAPE), which is scale-invariant and more suitable for image style\ntransfer tasks. Qualitative and quantitative experiments demonstrate the\neffectiveness of the proposed StyTr$^2$ compared with state-of-the-art\nCNN-based and flow-based approaches. Code and models are available at\nhttps://github.com/diyiiyiii/StyTR-2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yingying Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weiming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingjia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling the Challenges in Scene Graph Generation with Local-to-Global Interactions. (arXiv:2106.08543v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08543","description":"<p>In this work, we seek new insights into the underlying challenges of the\nScene Graph Generation (SGG) task. Quantitative and qualitative analysis of the\nVisual Genome dataset implies -- 1) Ambiguity: even if inter-object\nrelationship contains the same object (or predicate), they may not be visually\nor semantically similar, 2) Asymmetry: despite the nature of the relationship\nthat embodied the direction, it was not well addressed in previous studies, and\n3) Higher-order contexts: leveraging the identities of certain graph elements\ncan help to generate accurate scene graphs. Motivated by the analysis, we\ndesign a novel SGG framework, Local-to-Global Interaction Networks (LOGIN).\nLocally, interactions extract the essence between three instances of subject,\nobject, and background, while baking direction awareness into the network by\nexplicitly constraining the input order of subject and object. Globally,\ninteractions encode the contexts between every graph component (i.e., nodes and\nedges). Finally, Attract &amp; Repel loss is utilized to fine-tune the distribution\nof predicate embeddings. By design, our framework enables predicting the scene\ngraph in a bottom-up manner, leveraging the possible complementariness. To\nquantify how much LOGIN is aware of relational direction, a new diagnostic task\ncalled Bidirectional Relationship Classification (BRC) is also proposed.\nExperimental results demonstrate that LOGIN can successfully distinguish\nrelational direction than existing methods (in BRC task), while showing\nstate-of-the-art results on the Visual Genome benchmark (in SGG task).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sangmin Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1\">Junhyug Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kangil Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evolving Image Compositions for Feature Representation Learning. (arXiv:2106.09011v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09011","description":"<p>Convolutional neural networks for visual recognition require large amounts of\ntraining samples and usually benefit from data augmentation. This paper\nproposes PatchMix, a data augmentation method that creates new samples by\ncomposing patches from pairs of images in a grid-like pattern. These new\nsamples are assigned label scores that are proportional to the number of\npatches borrowed from each image. We then add a set of additional losses at the\npatch-level to regularize and to encourage good representations at both the\npatch and image levels. A ResNet-50 model trained on ImageNet using PatchMix\nexhibits superior transfer learning capabilities across a wide array of\nbenchmarks. Although PatchMix can rely on random pairings and random grid-like\npatterns for mixing, we explore evolutionary search as a guiding strategy to\njointly discover optimal grid-like patterns and image pairings. For this\npurpose, we conceive a fitness function that bypasses the need to re-train a\nmodel to evaluate each possible choice. In this way, PatchMix outperforms a\nbase model on CIFAR-10 (+1.91), CIFAR-100 (+5.31), Tiny Imagenet (+3.52), and\nImageNet (+1.16).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cascante_Bonilla_P/0/1/0/all/0/1\">Paola Cascante-Bonilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1\">Arshdeep Sekhon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Probing: Cognitive Framework for Explaining Self-Supervised Image Representations. (arXiv:2106.11054v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11054","description":"<p>Recently introduced self-supervised methods for image representation learning\nprovide on par or superior results to their fully supervised competitors, yet\nthe corresponding efforts to explain the self-supervised approaches lag behind.\nMotivated by this observation, we introduce a novel visual probing framework\nfor explaining the self-supervised models by leveraging probing tasks employed\npreviously in natural language processing. The probing tasks require knowledge\nabout semantic relationships between image parts. Hence, we propose a\nsystematic approach to obtain analogs of natural language in vision, such as\nvisual words, context, and taxonomy. Our proposal is grounded in Marr's\ncomputational theory of vision and concerns features like textures, shapes, and\nlines. We show the effectiveness and applicability of those analogs in the\ncontext of explaining self-supervised representations. Our key findings\nemphasize that relations between language and vision can serve as an effective\nyet intuitive tool for discovering how machine learning models work,\nindependently of data modality. Our work opens a plethora of research pathways\ntowards more explainable and transparent AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oleszkiewicz_W/0/1/0/all/0/1\">Witold Oleszkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basaj_D/0/1/0/all/0/1\">Dominika Basaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sieradzki_I/0/1/0/all/0/1\">Igor Sieradzki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorszczak_M/0/1/0/all/0/1\">Micha&#x142; G&#xf3;rszczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rychalska_B/0/1/0/all/0/1\">Barbara Rychalska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewandowska_K/0/1/0/all/0/1\">Koryna Lewandowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1\">Bartosz Zieli&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects. (arXiv:2106.14440v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14440","description":"<p>Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in\nhuman environments is an important yet challenging task for future\nhome-assistant robots. The space of 3D articulated objects is exceptionally\nrich in their myriad semantic categories, diverse shape geometry, and\ncomplicated part functionality. Previous works mostly abstract kinematic\nstructure with estimated joint parameters and part poses as the visual\nrepresentations for manipulating 3D articulated objects. In this paper, we\npropose object-centric actionable visual priors as a novel\nperception-interaction handshaking point that the perception system outputs\nmore actionable guidance than kinematic structure estimation, by predicting\ndense geometry-aware, interaction-aware, and task-aware visual action\naffordance and trajectory proposals. We design an interaction-for-perception\nframework VAT-Mart to learn such actionable visual representations by\nsimultaneously training a curiosity-driven reinforcement learning policy\nexploring diverse interaction trajectories and a perception module summarizing\nand generalizing the explored knowledge for pointwise predictions among diverse\nshapes. Experiments prove the effectiveness of the proposed approach using the\nlarge-scale PartNet-Mobility dataset in SAPIEN environment and show promising\ngeneralization capabilities to novel test shapes, unseen object categories, and\nreal-world data. Project page: https://hyperplane-lab.github.io/vat-mart\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruihai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zizheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuelin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails. (arXiv:2108.00602v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00602","description":"<p>In this paper, we study the task of hallucinating an authentic\nhigh-resolution (HR) face from an occluded thumbnail. We propose a multi-stage\nProgressive Upsampling and Inpainting Generative Adversarial Network, dubbed\nPro-UIGAN, which exploits facial geometry priors to replenish and upsample (8*)\nthe occluded and tiny faces (16*16 pixels). Pro-UIGAN iteratively (1) estimates\nfacial geometry priors for low-resolution (LR) faces and (2) acquires\nnon-occluded HR face images under the guidance of the estimated priors. Our\nmulti-stage hallucination network super-resolves and inpaints occluded LR faces\nin a coarse-to-fine manner, thus reducing unwanted blurriness and artifacts\nsignificantly. Specifically, we design a novel cross-modal transformer module\nfor facial priors estimation, in which an input face and its landmark features\nare formulated as queries and keys, respectively. Such a design encourages\njoint feature learning across the input facial and landmark features, and deep\nfeature correspondences will be discovered by attention. Thus, facial\nappearance features and facial geometry priors are learned in a mutual\npromotion manner. Extensive experiments demonstrate that our Pro-UIGAN achieves\nvisually pleasing HR faces, reaching superior performance in downstream tasks,\ni.e., face alignment, face parsing, face recognition and expression\nclassification, compared with other state-of-the-art (SotA) methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaobo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MmWave Radar and Vision Fusion for Object Detection in Autonomous Driving: A Review. (arXiv:2108.03004v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03004","description":"<p>With autonomous driving developing in a booming stage, accurate object\ndetection in complex scenarios attract wide attention to ensure the safety of\nautonomous driving. Millimeter wave (mmWave) radar and vision fusion is a\nmainstream solution for accurate obstacle detection. This article presents a\ndetailed survey on mmWave radar and vision fusion based obstacle detection\nmethods. First, we introduce the tasks, evaluation criteria, and datasets of\nobject detection for autonomous driving. The process of mmWave radar and vision\nfusion is then divided into three parts: sensor deployment, sensor calibration,\nand sensor fusion, which are reviewed comprehensively. Specifically, we\nclassify the fusion methods into data level, decision level, and feature level\nfusion methods. In addition, we introduce three-dimensional(3D) object\ndetection, the fusion of lidar and vision in autonomous driving and multimodal\ninformation fusion, which are promising for the future. Finally, we summarize\nthis article.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhiqing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fengkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuo Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huici Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiyong Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Expert Adversarial Attack Detection in Person Re-identification Using Context Inconsistency. (arXiv:2108.09891v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09891","description":"<p>The success of deep neural networks (DNNs) has promoted the widespread\napplications of person re-identification (ReID). However, ReID systems inherit\nthe vulnerability of DNNs to malicious attacks of visually inconspicuous\nadversarial perturbations. Detection of adversarial attacks is, therefore, a\nfundamental requirement for robust ReID systems. In this work, we propose a\nMulti-Expert Adversarial Attack Detection (MEAAD) approach to achieve this goal\nby checking context inconsistency, which is suitable for any DNN-based ReID\nsystems. Specifically, three kinds of context inconsistencies caused by\nadversarial attacks are employed to learn a detector for distinguishing the\nperturbed examples, i.e., a) the embedding distances between a perturbed query\nperson image and its top-K retrievals are generally larger than those between a\nbenign query image and its top-K retrievals, b) the embedding distances among\nthe top-K retrievals of a perturbed query image are larger than those of a\nbenign query image, c) the top-K retrievals of a benign query image obtained\nwith multiple expert ReID models tend to be consistent, which is not preserved\nwhen attacks are present. Extensive experiments on the Market1501 and\nDukeMTMC-ReID datasets show that, as the first adversarial attack detection\napproach for ReID, MEAAD effectively detects various adversarial attacks and\nachieves high ROC-AUC (over 97.5%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Min Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RSI-Net: Two-Stream Deep Neural Network for Remote Sensing Imagesbased Semantic Segmentation. (arXiv:2109.09148v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09148","description":"<p>For semantic segmentation of remote sensing images (RSI), trade-off between\nrepresentation power and location accuracy is quite important. How to get the\ntrade-off effectively is an open question,where current approaches of utilizing\nvery deep models result in complex models with large memory consumption. In\ncontrast to previous work that utilizes dilated convolutions or deep models, we\npropose a novel two-stream deep neural network for semantic segmentation of RSI\n(RSI-Net) to obtain improved performance through modeling and propagating\nspatial contextual structure effectively and a decoding scheme with image-level\nand graph-level combination. The first component explicitly models correlations\nbetween adjacent land covers and conduct flexible convolution on arbitrarily\nirregular image regions by using graph convolutional network, while densely\nconnected atrous convolution network (DenseAtrousCNet) with multi-scale atrous\nconvolution can expand the receptive fields and obtain image global\ninformation. Extensive experiments are implemented on the Vaihingen, Potsdam\nand Gaofen RSI datasets, where the comparison results demonstrate the superior\nperformance of RSI-Net in terms of overall accuracy (91.83%, 93.31% and 93.67%\non three datasets, respectively), F1 score (90.3%, 91.49% and 89.35% on three\ndatasets, respectively) and kappa coefficient (89.46%, 90.46% and 90.37% on\nthree datasets, respectively) when compared with six state-of-the-art RSI\nsemantic segmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xia Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jason Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haitong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kaiyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Haozhou Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chunqi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nizhuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MC-LCR: Multi-modal contrastive classification by locally correlated representations for effective face forgery detection. (arXiv:2110.03290v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03290","description":"<p>As the remarkable development of facial manipulation technologies is\naccompanied by severe security concerns, face forgery detection has become a\nrecent research hotspot. Most existing detection methods train a binary\nclassifier under global supervision to judge real or fake. However, advanced\nmanipulations only perform small-scale tampering, posing challenges to\ncomprehensively capture subtle and local forgery artifacts, especially in high\ncompression settings and cross-dataset scenarios. To address such limitations,\nwe propose a novel framework named Multi-modal Contrastive Classification by\nLocally Correlated Representations(MC-LCR), for effective face forgery\ndetection. Instead of specific appearance features, our MC-LCR aims to amplify\nimplicit local discrepancies between authentic and forged faces from both\nspatial and frequency domains. Specifically, we design the shallow style\nrepresentation block that measures the pairwise correlation of shallow feature\nmaps, which encodes local style information to extract more discriminative\nfeatures in the spatial domain. Moreover, we make a key observation that subtle\nforgery artifacts can be further exposed in the patch-wise phase and amplitude\nspectrum and exhibit different clues. According to the complementarity of\namplitude and phase information, we develop a patch-wise amplitude and phase\ndual attention module to capture locally correlated inconsistencies with each\nother in the frequency domain. Besides the above two modules, we further\nintroduce the collaboration of supervised contrastive loss with cross-entropy\nloss. It helps the network learn more discriminative and generalized\nrepresentations. Through extensive experiments and comprehensive studies, we\nachieve state-of-the-art performance and demonstrate the robustness and\ngeneralization of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gaojian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaohui Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Optimal Correlational Object Search. (arXiv:2110.09991v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2110.09991","description":"<p>In realistic applications of object search, robots will need to locate target\nobjects in complex environments while coping with unreliable sensors,\nespecially for small or hard-to-detect objects. In such settings, correlational\ninformation can be valuable for planning efficiently. Previous approaches that\nconsider correlational information typically resort to ad-hoc, greedy search\nstrategies. We introduce the Correlational Object Search POMDP (COS-POMDP),\nwhich models correlations while preserving optimal solutions with a reduced\nstate space. We propose a hierarchical planning algorithm to scale up\nCOS-POMDPs for practical domains. Our evaluation, conducted with the AI2-THOR\nhousehold simulator and the YOLOv5 object detector, shows that our method finds\nobjects more successfully and efficiently compared to baselines,particularly\nfor hard-to-detect objects such as srub brush and remote control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaiyu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitnis_R/0/1/0/all/0/1\">Rohan Chitnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yoonchang Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1\">George Konidaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1\">Stefanie Tellex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Baseline for Low-Budget Active Learning. (arXiv:2110.12033v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12033","description":"<p>Active learning focuses on choosing a subset of unlabeled data to be labeled.\nHowever, most such methods assume that a large subset of the data can be\nannotated. We are interested in low-budget active learning where only a small\nsubset (e.g., 0.2% of ImageNet) can be annotated. Instead of proposing a new\nquery strategy to iteratively sample batches of unlabeled data given an initial\npool, we learn rich features by an off-the-shelf self-supervised learning\nmethod only once, and then study the effectiveness of different sampling\nstrategies given a low labeling budget on a variety of datasets including\nImageNet. We show that although the state-of-the-art active learning methods\nwork well given a large labeling budget, a simple K-means clustering algorithm\ncan outperform them on low budgets. We believe this method can be used as a\nsimple baseline for low-budget active learning on image classification. Code is\navailable at: https://github.com/UCDvision/low-budget-al\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pourahmadi_K/0/1/0/all/0/1\">Kossar Pourahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nooralinejad_P/0/1/0/all/0/1\">Parsa Nooralinejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Generating Grounded Navigation Instructions from Landmarks. (arXiv:2111.12872v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12872","description":"<p>We study the automatic generation of navigation instructions from 360-degree\nimages captured on indoor routes. Existing generators suffer from poor visual\ngrounding, causing them to rely on language priors and hallucinate objects. Our\nMARKY-MT5 system addresses this by focusing on visual landmarks; it comprises a\nfirst stage landmark detector and a second stage generator -- a multimodal,\nmultilingual, multitask encoder-decoder. To train it, we bootstrap grounded\nlandmark annotations on top of the Room-across-Room (RxR) dataset. Using text\nparsers, weak supervision from RxR's pose traces, and a multilingual image-text\nencoder trained on 1.8b images, we identify 1.1m English, Hindi and Telugu\nlandmark descriptions and ground them to specific regions in panoramas. On\nRoom-to-Room, human wayfinders obtain success rates (SR) of 71% following\nMARKY-MT5's instructions, just shy of their 75% SR following human instructions\n-- and well above SRs with other generators. Evaluations on RxR's longer,\ndiverse paths obtain 61-64% SRs on three languages. Generating such\nhigh-quality navigation instructions in novel environments is a step towards\nconversational navigation tools and could facilitate larger-scale training of\ninstruction-following agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montgomery_C/0/1/0/all/0/1\">Ceslee Montgomery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orbay_J/0/1/0/all/0/1\">Jordi Orbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birodkar_V/0/1/0/all/0/1\">Vighnesh Birodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1\">Aleksandra Faust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gur_I/0/1/0/all/0/1\">Izzeddin Gur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1\">Natasha Jaques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1\">Austin Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1\">Peter Anderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiway Non-rigid Point Cloud Registration via Learned Functional Map Synchronization. (arXiv:2111.12878v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12878","description":"<p>We present SyNoRiM, a novel way to jointly register multiple non-rigid shapes\nby synchronizing the maps relating learned functions defined on the point\nclouds. Even though the ability to process non-rigid shapes is critical in\nvarious applications ranging from computer animation to 3D digitization, the\nliterature still lacks a robust and flexible framework to match and align a\ncollection of real, noisy scans observed under occlusions. Given a set of such\npoint clouds, our method first computes the pairwise correspondences\nparameterized via functional maps. We simultaneously learn potentially\nnon-orthogonal basis functions to effectively regularize the deformations,\nwhile handling the occlusions in an elegant way. To maximally benefit from the\nmulti-way information provided by the inferred pairwise deformation fields, we\nsynchronize the pairwise functional maps into a cycle-consistent whole thanks\nto our novel and principled optimization formulation. We demonstrate via\nextensive experiments that our method achieves a state-of-the-art performance\nin registration accuracy, while being flexible and efficient as we handle both\nnon-rigid and multi-body cases in a unified framework and avoid the costly\noptimization over point-wise permutations by the use of basis function maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiahui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1\">Tolga Birdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gojcic_Z/0/1/0/all/0/1\">Zan Gojcic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi-Min Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Fair Classifiers with Partially Annotated Group Labels. (arXiv:2111.14581v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.14581","description":"<p>Recently, fairness-aware learning have become increasingly crucial, but most\nof those methods operate by assuming the availability of fully annotated\ndemographic group labels. We emphasize that such assumption is unrealistic for\nreal-world applications since group label annotations are expensive and can\nconflict with privacy issues. In this paper, we consider a more practical\nscenario, dubbed as Algorithmic Group Fairness with the Partially annotated\nGroup labels (Fair-PG). We observe that the existing methods to achieve group\nfairness perform even worse than the vanilla training, which simply uses full\ndata only with target labels, under Fair-PG. To address this problem, we\npropose a simple Confidence-based Group Label assignment (CGL) strategy that is\nreadily applicable to any fairness-aware learning method. CGL utilizes an\nauxiliary group classifier to assign pseudo group labels, where random labels\nare assigned to low confident samples. We first theoretically show that our\nmethod design is better than the vanilla pseudo-labeling strategy in terms of\nfairness criteria. Then, we empirically show on several benchmark datasets that\nby combining CGL and the state-of-the-art fairness-aware in-processing methods,\nthe target accuracies and the fairness metrics can be jointly improved compared\nto the baselines. Furthermore, we convincingly show that CGL enables to\nnaturally augment the given group-labeled dataset with external target\nlabel-only datasets so that both accuracy and fairness can be improved. Code is\navailable at https://github.com/naver-ai/cgl_fairness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sangwon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Diverse 3D Reconstructions from a Single Occluded Face Image. (arXiv:2112.00879v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00879","description":"<p>Occlusions are a common occurrence in unconstrained face images. Single image\n3D reconstruction from such face images often suffers from corruption due to\nthe presence of occlusions. Furthermore, while a plurality of 3D\nreconstructions is plausible in the occluded regions, existing approaches are\nlimited to generating only a single solution. To address both of these\nchallenges, we present Diverse3DFace, which is specifically designed to\nsimultaneously generate a diverse and realistic set of 3D reconstructions from\na single occluded face image. It consists of three components: a global+local\nshape fitting process, a graph neural network-based mesh VAE, and a\nDeterminantal Point Process based diversity promoting iterative optimization\nprocedure. Quantitative and qualitative comparisons of 3D reconstruction on\noccluded faces show that Diverse3DFace can estimate 3D shapes that are\nconsistent with the visible regions in the target image while exhibiting high,\nyet realistic, levels of diversity on the occluded regions. On face images\noccluded by masks, glasses, and other random objects, Diverse3DFace generates a\ndistribution of 3D shapes having ~50% higher diversity on the occluded regions\ncompared to the baselines. Moreover, our closest sample to the ground truth has\n~40% lower MSE than the singular reconstructions by existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_R/0/1/0/all/0/1\">Rahul Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1\">Vishnu Naresh Boddeti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Learning in Semantic Segmentation from Image Labels. (arXiv:2112.01882v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01882","description":"<p>Although existing semantic segmentation approaches achieve impressive\nresults, they still struggle to update their models incrementally as new\ncategories are uncovered. Furthermore, pixel-by-pixel annotations are expensive\nand time-consuming. This paper proposes a novel framework for Weakly\nIncremental Learning for Semantic Segmentation, that aims at learning to\nsegment new classes from cheap and largely available image-level labels. As\nopposed to existing approaches, that need to generate pseudo-labels offline, we\nuse an auxiliary classifier, trained with image-level labels and regularized by\nthe segmentation model, to obtain pseudo-supervision online and update the\nmodel incrementally. We cope with the inherent noise in the process by using\nsoft-labels generated by the auxiliary classifier. We demonstrate the\neffectiveness of our approach on the Pascal VOC and COCO datasets,\noutperforming offline weakly-supervised methods and obtaining results\ncomparable with incremental learning methods with full supervision. Code can be\nfound at https://github.com/fcdl94/WILSON.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cermelli_F/0/1/0/all/0/1\">Fabio Cermelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fontanel_D/0/1/0/all/0/1\">Dario Fontanel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavera_A/0/1/0/all/0/1\">Antonio Tavera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1\">Marco Ciccone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Facial Representation Learning in a Visual-Linguistic Manner. (arXiv:2112.03109v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03109","description":"<p>How to learn a universal facial representation that boosts all face analysis\ntasks? This paper takes one step toward this goal. In this paper, we study the\ntransfer performance of pre-trained models on face analysis tasks and introduce\na framework, called FaRL, for general Facial Representation Learning in a\nvisual-linguistic manner. On one hand, the framework involves a contrastive\nloss to learn high-level semantic meaning from image-text pairs. On the other\nhand, we propose exploring low-level information simultaneously to further\nenhance the face representation, by adding a masked image modeling. We perform\npre-training on LAION-FACE, a dataset containing large amount of face\nimage-text pairs, and evaluate the representation capability on multiple\ndownstream tasks. We show that FaRL achieves better transfer performance\ncompared with previous pre-trained models. We also verify its superiority in\nthe low-data regime. More importantly, our model surpasses the state-of-the-art\nmethods on face analysis tasks including face parsing and face alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinglin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yangyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Ming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Models are Continual Learners. (arXiv:2112.04215v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04215","description":"<p>Self-supervised models have been shown to produce comparable or better visual\nrepresentations than their supervised counterparts when trained offline on\nunlabeled data at scale. However, their efficacy is catastrophically reduced in\na Continual Learning (CL) scenario where data is presented to the model\nsequentially. In this paper, we show that self-supervised loss functions can be\nseamlessly converted into distillation mechanisms for CL by adding a predictor\nnetwork that maps the current state of the representations to their past state.\nThis enables us to devise a framework for Continual self-supervised visual\nrepresentation Learning that (i) significantly improves the quality of the\nlearned representations, (ii) is compatible with several state-of-the-art\nself-supervised objectives, and (iii) needs little to no hyperparameter tuning.\nWe demonstrate the effectiveness of our approach empirically by training six\npopular self-supervised models in various CL settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_V/0/1/0/all/0/1\">Victor G. Turrisi da Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1\">Julien Mairal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Attention on Multi-Level Dense Difference Maps for Generic Event Boundary Detection. (arXiv:2112.04771v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04771","description":"<p>Generic event boundary detection is an important yet challenging task in\nvideo understanding, which aims at detecting the moments where humans naturally\nperceive event boundaries. The main challenge of this task is perceiving\nvarious temporal variations of diverse event boundaries. To this end, this\npaper presents an effective and end-to-end learnable framework (DDM-Net). To\ntackle the diversity and complicated semantics of event boundaries, we make\nthree notable improvements. First, we construct a feature bank to store\nmulti-level features of space and time, prepared for difference calculation at\nmultiple scales. Second, to alleviate inadequate temporal modeling of previous\nmethods, we present dense difference maps (DDM) to comprehensively characterize\nthe motion pattern. Finally, we exploit progressive attention on multi-level\nDDM to jointly aggregate appearance and motion clues. As a result, DDM-Net\nrespectively achieves a significant boost of 14% and 8% on Kinetics-GEBD and\nTAPOS benchmark, and outperforms the top-1 winner solution of LOVEU\nChallenge@CVPR 2021 without bells and whistles. The state-of-the-art result\ndemonstrates the effectiveness of richer motion representation and more\nsophisticated aggregation, in handling the diversity of generic event boundary\ndetection. The code is made available at \\url{https://github.com/MCG-NJU/DDM}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimedia Datasets for Anomaly Detection: A Review. (arXiv:2112.05410v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05410","description":"<p>Multimedia anomaly datasets play a crucial role in automated surveillance.\nThey have a wide range of applications expanding from outlier objects/\nsituation detection to the detection of life-threatening events. For more than\n1.5 decades, this field has attracted a lot of research attention, and as a\nresult, more and more datasets dedicated to anomalous actions and object\ndetection have been developed. Tapping these public anomaly datasets enable\nresearchers to generate and compare various anomaly detection frameworks with\nthe same input data. This paper presents a comprehensive survey on a variety of\nvideo, audio, as well as audio-visual datasets based on the application of\nanomaly detection. This survey aims to address the lack of a comprehensive\ncomparison and analysis of multimedia public datasets based on anomaly\ndetection. Also, it can assist researchers in selecting the best available\ndataset for bench-marking frameworks. Additionally, we discuss gaps in the\nexisting dataset and insights for future direction towards developing\nmultimodal anomaly detection datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumari_P/0/1/0/all/0/1\">Pratibha Kumari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedi_A/0/1/0/all/0/1\">Anterpreet Kaur Bedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_M/0/1/0/all/0/1\">Mukesh Saini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Keypoint Detection with Uncertainty Learning for Unseen Species. (arXiv:2112.06183v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06183","description":"<p>Current non-rigid object keypoint detectors perform well on a chosen kind of\nspecies and body parts, and require a large amount of labelled keypoints for\ntraining. Moreover, their heatmaps, tailored to specific body parts, cannot\nrecognize novel keypoints (keypoints not labelled for training) on unseen\nspecies. We raise an interesting yet challenging question: how to detect both\nbase (annotated for training) and novel keypoints for unseen species given a\nfew annotated samples? Thus, we propose a versatile Few-shot Keypoint Detection\n(FSKD) pipeline, which can detect a varying number of keypoints of different\nkinds. Our FSKD provides the uncertainty estimation of predicted keypoints.\nSpecifically, FSKD involves main and auxiliary keypoint representation\nlearning, similarity learning, and keypoint localization with uncertainty\nmodeling to tackle the localization noise. Moreover, we model the uncertainty\nacross groups of keypoints by multivariate Gaussian distribution to exploit\nimplicit correlations between neighboring keypoints. We show the effectiveness\nof our FSKD on (i) novel keypoint detection for unseen species, and (ii)\nfew-shot Fine-Grained Visual Recognition (FGVR) and (iii) Semantic Alignment\n(SA) downstream tasks. For FGVR, detected keypoints improve the classification\naccuracy. For SA, we showcase a novel thin-plate-spline warping that uses\nestimated keypoint uncertainty under imperfect keypoint corespondences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Changsheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manifold Learning Benefits GANs. (arXiv:2112.12618v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12618","description":"<p>In this paper, we improve Generative Adversarial Networks by incorporating a\nmanifold learning step into the discriminator. We consider locality-constrained\nlinear and subspace-based manifolds, and locality-constrained non-linear\nmanifolds. In our design, the manifold learning and coding steps are\nintertwined with layers of the discriminator, with the goal of attracting\nintermediate feature representations onto manifolds. We adaptively balance the\ndiscrepancy between feature representations and their manifold view, which is a\ntrade-off between denoising on the manifold and refining the manifold. We find\nthat locality-constrained non-linear manifolds outperform linear manifolds due\nto their non-uniform density and smoothness. We also substantially outperform\nstate-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1\">Richard Hartley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nock_R/0/1/0/all/0/1\">Richard Nock</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using maps to predict economic activity. (arXiv:2112.13850v2 [econ.GN] UPDATED)","link":"http://arxiv.org/abs/2112.13850","description":"<p>We introduce a novel machine learning approach to leverage historical and\ncontemporary maps and systematically predict economic statistics. Our simple\nalgorithm extracts meaningful features from the maps based on their color\ncompositions for predictions. We apply our method to grid-level population\nlevels in Sub-Saharan Africa in the 1950s and South Korea in 1930, 1970, and\n2015. Our results show that maps can reliably predict population density in the\nmid-20th century Sub-Saharan Africa using 9,886 map grids (5km by 5 km).\nSimilarly, contemporary South Korean maps can generate robust predictions on\nincome, consumption, employment, population density, and electric consumption.\nIn addition, our method is capable of predicting historical South Korean\npopulation growth over a century.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/econ/1/au:+Jeong_I/0/1/0/all/0/1\">Imryoung Jeong</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Yang_H/0/1/0/all/0/1\">Hyunjoo Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACDNet: Adaptively Combined Dilated Convolution for Monocular Panorama Depth Estimation. (arXiv:2112.14440v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14440","description":"<p>Depth estimation is a crucial step for 3D reconstruction with panorama images\nin recent years. Panorama images maintain the complete spatial information but\nintroduce distortion with equirectangular projection. In this paper, we propose\nan ACDNet based on the adaptively combined dilated convolution to predict the\ndense depth map for a monocular panoramic image. Specifically, we combine the\nconvolution kernels with different dilations to extend the receptive field in\nthe equirectangular projection. Meanwhile, we introduce an adaptive\nchannel-wise fusion module to summarize the feature maps and get diverse\nattention areas in the receptive field along the channels. Due to the\nutilization of channel-wise attention in constructing the adaptive channel-wise\nfusion module, the network can capture and leverage the cross-channel\ncontextual information efficiently. Finally, we conduct depth estimation\nexperiments on three datasets (both virtual and real-world) and the\nexperimental results demonstrate that our proposed ACDNet substantially\noutperforms the current state-of-the-art (SOTA) methods. Our codes and model\nparameters are accessed in https://github.com/zcq15/ACDNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1\">Chuanqing Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhengda Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiqun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negative Evidence Matters in Interpretable Histology Image Classification. (arXiv:2201.02445v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02445","description":"<p>Using only global image-class labels, weakly-supervised learning methods,\nsuch as class activation mapping, allow training CNNs to jointly classify an\nimage, and locate regions of interest associated with the predicted class.\nHowever, without any guidance at the pixel level, such methods may yield\ninaccurate regions. This problem is known to be more challenging with histology\nimages than with natural ones, since objects are less salient, structures have\nmore variations, and foreground and background regions have stronger\nsimilarities. Therefore, computer vision methods for visual interpretation of\nCNNs may not directly apply. In this paper, a simple yet efficient method based\non a composite loss is proposed to learn information from the fully negative\nsamples (i.e., samples without positive regions), and thereby reduce false\npositives/negatives. Our new loss function contains two complementary terms:\nthe first exploits positive evidence collected from the CNN classifier, while\nthe second leverages the fully negative samples from training data. In\nparticular, a pre-trained CNN is equipped with a decoder that allows refining\nthe regions of interest. The CNN is exploited to collect both positive and\nnegative evidence at the pixel level to train the decoder. Our method called\nNEGEV benefits from the fully negative samples that naturally occur in the\ndata, without any additional supervision signals beyond image-class labels.\nExtensive experiments show that our proposed method can substantial outperform\nrelated state-of-art methods on GlaS (public benchmark for colon cancer), and\nCamelyon16 (patch-based benchmark for breast cancer using three different\nbackbones). Our results highlight the benefits of using both positive and\nnegative evidence, the first obtained from a classifier, and the other\nnaturally available in datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AttentionHTR: Handwritten Text Recognition Based on Attention Encoder-Decoder Networks. (arXiv:2201.09390v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09390","description":"<p>This work proposes an attention-based sequence-to-sequence model for\nhandwritten word recognition and explores transfer learning for data-efficient\ntraining of HTR systems. To overcome training data scarcity, this work\nleverages models pre-trained on scene text images as a starting point towards\ntailoring the handwriting recognition models. ResNet feature extraction and\nbidirectional LSTM-based sequence modeling stages together form an encoder. The\nprediction stage consists of a decoder and a content-based attention mechanism.\nThe effectiveness of the proposed end-to-end HTR system has been empirically\nevaluated on a novel multi-writer dataset Imgur5K and the IAM dataset. The\nexperimental results evaluate the performance of the HTR framework, further\nsupported by an in-depth analysis of the error cases. Source code and\npre-trained models are available at https://github.com/dmitrijsk/AttentionHTR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kass_D/0/1/0/all/0/1\">Dmitrijs Kass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vats_E/0/1/0/all/0/1\">Ekta Vats</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paired Image to Image Translation for Strikethrough Removal From Handwritten Words. (arXiv:2201.09633v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09633","description":"<p>Transcribing struck-through, handwritten words, for example for the purpose\nof genetic criticism, can pose a challenge to both humans and machines, due to\nthe obstructive properties of the superimposed strokes. This paper investigates\nthe use of paired image to image translation approaches to remove strikethrough\nstrokes from handwritten words. Four different neural network architectures are\nexamined, ranging from a few simple convolutional layers to deeper ones,\nemploying Dense blocks. Experimental results, obtained from one synthetic and\none genuine paired strikethrough dataset, confirm that the proposed paired\nmodels outperform the CycleGAN-based state of the art, while using less than a\nsixth of the trainable parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heil_R/0/1/0/all/0/1\">Raphaela Heil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vats_E/0/1/0/all/0/1\">Ekta Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hast_A/0/1/0/all/0/1\">Anders Hast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aladdin: Joint Atlas Building and Diffeomorphic Registration Learning with Pairwise Alignment. (arXiv:2202.03563v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.03563","description":"<p>Atlas building and image registration are important tasks for medical image\nanalysis. Once one or multiple atlases from an image population have been\nconstructed, commonly (1) images are warped into an atlas space to study\nintra-subject or inter-subject variations or (2) a possibly probabilistic atlas\nis warped into image space to assign anatomical labels. Atlas estimation and\nnonparametric transformations are computationally expensive as they usually\nrequire numerical optimization. Additionally, previous approaches for atlas\nbuilding often define similarity measures between a fuzzy atlas and each\nindividual image, which may cause alignment difficulties because a fuzzy atlas\ndoes not exhibit clear anatomical structures in contrast to the individual\nimages. This work explores using a convolutional neural network (CNN) to\njointly predict the atlas and a stationary velocity field (SVF)\nparameterization for diffeomorphic image registration with respect to the\natlas. Our approach does not require affine pre-registrations and utilizes\npairwise image alignment losses to increase registration accuracy. We evaluate\nour model on 3D knee magnetic resonance images (MRI) from the OAI-ZIB dataset.\nOur results show that the proposed framework achieves better performance than\nother state-of-the-art image registration algorithms, allows for end-to-end\ntraining, and for fast inference at test time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ding_Z/0/1/0/all/0/1\">Zhipeng Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveling Down in Computer Vision: Pareto Inefficiencies in Fair Deep Classifiers. (arXiv:2203.04913v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04913","description":"<p>Algorithmic fairness is frequently motivated in terms of a trade-off in which\noverall performance is decreased so as to improve performance on disadvantaged\ngroups where the algorithm would otherwise be less accurate. Contrary to this,\nwe find that applying existing fairness approaches to computer vision improve\nfairness by degrading the performance of classifiers across all groups (with\nincreased degradation on the best performing groups).\n</p>\n<p>Extending the bias-variance decomposition for classification to fairness, we\ntheoretically explain why the majority of fairness classifiers designed for low\ncapacity models should not be used in settings involving high-capacity models,\na scenario common to computer vision. We corroborate this analysis with\nextensive experimental support that shows that many of the fairness heuristics\nused in computer vision also degrade performance on the most disadvantaged\ngroups. Building on these insights, we propose an adaptive augmentation\nstrategy that, uniquely, of all methods tested, improves performance for the\ndisadvantaged groups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zietlow_D/0/1/0/all/0/1\">Dominik Zietlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lohaus_M/0/1/0/all/0/1\">Michael Lohaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_G/0/1/0/all/0/1\">Guha Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleindessner_M/0/1/0/all/0/1\">Matth&#xe4;us Kleindessner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial amplitude swap towards robust image classifiers. (arXiv:2203.07138v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07138","description":"<p>The vulnerability of convolutional neural networks (CNNs) to image\nperturbations such as common corruptions and adversarial perturbations has\nrecently been investigated from the perspective of frequency. In this study, we\ninvestigate the effect of the amplitude and phase spectra of adversarial images\non the robustness of CNN classifiers. Extensive experiments revealed that the\nimages generated by combining the amplitude spectrum of adversarial images and\nthe phase spectrum of clean images accommodates moderate and general\nperturbations, and training with these images equips a CNN classifier with more\ngeneral robustness, performing well under both common corruptions and\nadversarial perturbations. We also found that two types of overfitting\n(catastrophic overfitting and robust overfitting) can be circumvented by the\naforementioned spectrum recombination. We believe that these results contribute\nto the understanding and the training of truly robust classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chun Yang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawamoto_K/0/1/0/all/0/1\">Kazuhiko Kawamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kera_H/0/1/0/all/0/1\">Hiroshi Kera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Probabilistic Modeling for Video Generation. (arXiv:2203.09481v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09481","description":"<p>Denoising diffusion probabilistic models are a promising new class of\ngenerative models that are competitive with GANs on perceptual metrics. In this\npaper, we explore their potential for sequentially generating video. Inspired\nby recent advances in neural video compression, we use denoising diffusion\nmodels to stochastically generate a residual to a deterministic next-frame\nprediction. We compare this approach to two sequential VAE and two GAN\nbaselines on four datasets, where we test the generated frames for perceptual\nquality and forecasting accuracy against ground truth frames. We find\nsignificant improvements in terms of perceptual quality on all data and\nimprovements in terms of frame forecasting for complex high-resolution videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Prakhar Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Portrait Delighting. (arXiv:2203.12088v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12088","description":"<p>We present a deep neural network for removing undesirable shading features\nfrom an unconstrained portrait image, recovering the underlying texture. Our\ntraining scheme incorporates three regularization strategies: masked loss, to\nemphasize high-frequency shading features; soft-shadow loss, which improves\nsensitivity to subtle changes in lighting; and shading-offset estimation, to\nsupervise separation of shading and texture. Our method demonstrates improved\ndelighting quality and generalization when compared with the state-of-the-art.\nWe further demonstrate how our delighting method can enhance the performance of\nlight-sensitive computer vision tasks such as face relighting and semantic\nparsing, allowing them to handle extreme lighting conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weir_J/0/1/0/all/0/1\">Joshua Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junhong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalmers_A/0/1/0/all/0/1\">Andrew Chalmers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_T/0/1/0/all/0/1\">Taehyun Rhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection. (arXiv:2203.12208v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12208","description":"<p>Recent studies in deepfake detection have yielded promising results when the\ntraining and testing face forgeries are from the same dataset. However, the\nproblem remains challenging when one tries to generalize the detector to\nforgeries created by unseen methods in the training dataset. This work\naddresses the generalizable deepfake detection from a simple principle: a\ngeneralizable representation should be sensitive to diverse types of forgeries.\nFollowing this principle, we propose to enrich the \"diversity\" of forgeries by\nsynthesizing augmented forgeries with a pool of forgery configurations and\nstrengthen the \"sensitivity\" to the forgeries by enforcing the model to predict\nthe forgery configurations. To effectively explore the large forgery\naugmentation space, we further propose to use the adversarial training strategy\nto dynamically synthesize the most challenging forgeries to the current model.\nThrough extensive experiments, we show that the proposed strategies are\nsurprisingly effective (see Figure 1), and they could achieve superior\nperformance than the current state-of-the-art methods. Code is available at\n\\url{https://github.com/liangchen527/SLADD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yibing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the (Limited) Generalization of MasterFace Attacks and Its Relation to the Capacity of Face Representations. (arXiv:2203.12387v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12387","description":"<p>A MasterFace is a face image that can successfully match against a large\nportion of the population. Since their generation does not require access to\nthe information of the enrolled subjects, MasterFace attacks represent a\npotential security risk for widely-used face recognition systems. Previous\nworks proposed methods for generating such images and demonstrated that these\nattacks can strongly compromise face recognition. However, previous works\nfollowed evaluation settings consisting of older recognition models, limited\ncross-dataset and cross-model evaluations, and the use of low-scale testing\ndata. This makes it hard to state the generalizability of these attacks. In\nthis work, we comprehensively analyse the generalizability of MasterFace\nattacks in empirical and theoretical investigations. The empirical\ninvestigations include the use of six state-of-the-art FR models, cross-dataset\nand cross-model evaluation protocols, and utilizing testing datasets of\nsignificantly higher size and variance. The results indicate a low\ngeneralizability when MasterFaces are training on a different face recognition\nmodel than the one used for testing. In these cases, the attack performance is\nsimilar to zero-effort imposter attacks. In the theoretical investigations, we\ndefine and estimate the face capacity and the maximum MasterFace coverage under\nthe assumption that identities in the face space are well separated. The\ncurrent trend of increasing the fairness and generalizability in face\nrecognition indicates that the vulnerability of future systems might further\ndecrease. Future works might analyse the utility of MasterFaces for\nunderstanding and enhancing the robustness of face recognition models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Terhorst_P/0/1/0/all/0/1\">Philipp Terh&#xf6;rst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bierbaum_F/0/1/0/all/0/1\">Florian Bierbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1\">Marco Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1\">Kiran Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StructToken : Rethinking Semantic Segmentation with Structural Prior. (arXiv:2203.12612v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12612","description":"<p>In this paper, we present structure token (StructToken), a new paradigm for\nsemantic segmentation. From a perspective on semantic segmentation as per-pixel\nclassification, the previous deep learning-based methods learn the per-pixel\nrepresentation first through an encoder and a decoder head and then classify\neach pixel representation to a specific category to obtain the semantic masks.\nDifferently, we propose a structure-aware algorithm that takes structural\ninformation as prior to predict semantic masks directly without per-pixel\nclassification. Specifically, given an input image, the learnable structure\ntoken interacts with the image representations to reason the final semantic\nmasks. Three interaction approaches are explored and the results not only\noutperform the state-of-the-art methods but also contain more structural\ninformation. Experiments are conducted on three widely used datasets including\nADE20k, Cityscapes, and COCO-Stuff 10K. We hope that structure token could\nserve as an alternative for semantic segmentation and inspire future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangjian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhanhao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Miao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shengwei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simulation Benchmark for Vision-based Autonomous Navigation. (arXiv:2203.13048v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13048","description":"<p>This work introduces a simulator benchmark for vision-based autonomous\nnavigation. The simulator offers control over real world variables such as the\nenvironment, time of day, weather and traffic. The benchmark includes a modular\nintegration of different components of a full autonomous visual navigation\nstack. In the experimental part of the paper, state-of-the-art visual\nlocalization methods are evaluated as a part of the stack in realistic\nnavigation tasks. To the authors' best knowledge, the proposed benchmark is the\nfirst to study modern visual localization methods as part of a full autonomous\nvisual navigation stack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suomela_L/0/1/0/all/0/1\">Lauri Suomela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dag_A/0/1/0/all/0/1\">Atakan Dag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edelman_H/0/1/0/all/0/1\">Harry Edelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamarainen_J/0/1/0/all/0/1\">Joni-Kristian K&#xe4;m&#xe4;r&#xe4;inen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Selective Transformer for Semantic Image Segmentation. (arXiv:2203.14124v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14124","description":"<p>Recently, it has attracted more and more attentions to fuse multi-scale\nfeatures for semantic image segmentation. Various works were proposed to employ\nprogressive local or global fusion, but the feature fusions are not rich enough\nfor modeling multi-scale context features. In this work, we focus on fusing\nmulti-scale features from Transformer-based backbones for semantic\nsegmentation, and propose a Feature Selective Transformer (FeSeFormer), which\naggregates features from all scales (or levels) for each query feature.\nSpecifically, we first propose a Scale-level Feature Selection (SFS) module,\nwhich can choose an informative subset from the whole multi-scale feature set\nfor each scale, where those features that are important for the current scale\n(or level) are selected and the redundant are discarded. Furthermore, we\npropose a Full-scale Feature Fusion (FFF) module, which can adaptively fuse\nfeatures of all scales for queries. Based on the proposed SFS and FFF modules,\nwe develop a Feature Selective Transformer (FeSeFormer), and evaluate our\nFeSeFormer on four challenging semantic segmentation benchmarks, including\nPASCAL Context, ADE20K, COCO-Stuff 10K, and Cityscapes, outperforming the\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangjian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sitong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shengwei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reverse Engineering of Imperceptible Adversarial Image Perturbations. (arXiv:2203.14145v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14145","description":"<p>It has been well recognized that neural network based image classifiers are\neasily fooled by images with tiny perturbations crafted by an adversary. There\nhas been a vast volume of research to generate and defend such adversarial\nattacks. However, the following problem is left unexplored: How to\nreverse-engineer adversarial perturbations from an adversarial image? This\nleads to a new adversarial learning paradigm--Reverse Engineering of Deceptions\n(RED). If successful, RED allows us to estimate adversarial perturbations and\nrecover the original images. However, carefully crafted, tiny adversarial\nperturbations are difficult to recover by optimizing a unilateral RED\nobjective. For example, the pure image denoising method may overfit to\nminimizing the reconstruction error but hardly preserve the classification\nproperties of the true adversarial perturbations. To tackle this challenge, we\nformalize the RED problem and identify a set of principles crucial to the RED\napproach design. Particularly, we find that prediction alignment and proper\ndata augmentation (in terms of spatial transformations) are two criteria to\nachieve a generalizable RED approach. By integrating these RED principles with\nimage denoising, we propose a new Class-Discriminative Denoising based RED\nframework, termed CDD-RED. Extensive experiments demonstrate the effectiveness\nof CDD-RED under different evaluation metrics (ranging from the pixel-level,\nprediction-level to the attribution-level alignment) and a variety of attack\ngeneration methods (e.g., FGSM, PGD, CW, AutoAttack, and adaptive attacks).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuguang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yize Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yimeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Limited Parameter Denoising for Low-dose X-ray Computed Tomography Using Deep Reinforcement Learning. (arXiv:2203.14794v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.14794","description":"<p>The use of deep learning has successfully solved several problems in the\nfield of medical imaging. Deep learning has been applied to the CT denoising\nproblem successfully. However, the use of deep learning requires large amounts\nof data to train deep convolutional networks (CNNs). Moreover, due to large\nparameter count, such deep CNNs may cause unexpected results. In this study, we\nintroduce a novel CT denoising framework, which has interpretable behaviour,\nand provides useful results with limited data. We employ bilateral filtering in\nboth the projection and volume domains to remove noise. To account for\nnon-stationary noise, we tune the $\\sigma$ parameters of the volume for every\nprojection view, and for every volume pixel. The tuning is carried out by two\ndeep CNNs. Due to impracticality of labelling, the two deep CNNs are trained\nvia a Deep-Q reinforcement learning task. The reward for the task is generated\nby using a custom reward function represented by a neural network. Our\nexperiments were carried out on abdominal scans for the Mayo Clinic TCIA\ndataset, and the AAPM Low Dose CT Grand Challenge. Our denoising framework has\nexcellent denoising performance increasing the PSNR from 28.53 to 28.93, and\nincreasing the SSIM from 0.8952 to 0.9204. We outperform several\nstate-of-the-art deep CNNs, which have several orders of magnitude higher\nnumber of parameters (p-value (PSNR) = 0.000, p-value (SSIM) = 0.000). Our\nmethod does not introduce any blurring, which is introduced by MSE loss based\nmethods, or any deep learning artifacts, which are introduced by WGAN based\nmodels. Our ablation studies show that parameter tuning and using our reward\nnetwork results in the best possible results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Patwari_M/0/1/0/all/0/1\">Mayank Patwari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gutjahr_R/0/1/0/all/0/1\">Ralf Gutjahr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raupach_R/0/1/0/all/0/1\">Rainer Raupach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Deep is Your Art: An Experimental Study on the Limits of Artistic Understanding in a Single-Task, Single-Modality Neural Network. (arXiv:2203.16031v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16031","description":"<p>Mathematical modeling and aesthetic rule extraction of works of art are\ncomplex activities. This is because art is a multidimensional, subjective\ndiscipline. Perception and interpretation of art are, to many extents, relative\nand open-ended rather than measurable. Following the explainable Artificial\nIntelligence paradigm, this paper investigated in a human-understandable\nfashion the limits to which a single-task, single-modality benchmark computer\nvision model performs in classifying contemporary 2D visual arts. It is\nimportant to point out that this work does not introduce an interpreting method\nto open the black box of Deep Neural Networks, instead it uses existing\nevaluating metrics derived from the confusion matrix to try to uncover the\nmechanism with which Deep Neural Networks understand art. To achieve so,\nVGG-11, pre-trained on ImageNet and discriminatively fine-tuned, was used on\nhandcrafted small-data datasets designed from real-world photography gallery\nshows. We demonstrated that the artwork's Exhibited Properties or formal\nfactors such as shape and color, rather than Non-Exhibited Properties or\ncontent factors such as history and intention, have much higher potential to be\nthe determinant when art pieces have very similar Exhibited Properties. We also\nshowed that a single-task and single-modality model's understanding of art is\ninadequate as it largely ignores Non-Exhibited Properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zahedi_M/0/1/0/all/0/1\">Mahan Agha Zahedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholamrezaei_N/0/1/0/all/0/1\">Niloofar Gholamrezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doboli_A/0/1/0/all/0/1\">Alex Doboli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIT: A Bionic and Non-Linear Neuron for Spiking Neural Network. (arXiv:2203.16117v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16117","description":"<p>Spiking Neural Networks (SNNs) have piqued researchers' interest because of\ntheir capacity to process temporal information and low power consumption.\nHowever, current state-of-the-art methods limited their biological plausibility\nand performance because their neurons are generally built on the simple\nLeaky-Integrate-and-Fire (LIF) model. Due to the high level of dynamic\ncomplexity, modern neuron models have seldom been implemented in SNN practice.\nIn this study, we adopt the Phase Plane Analysis (PPA) technique, a technique\noften utilized in neurodynamics field, to integrate a recent neuron model,\nnamely, the Izhikevich neuron. Based on the findings in the advancement of\nneuroscience, the Izhikevich neuron model can be biologically plausible while\nmaintaining comparable computational cost with LIF neurons. By utilizing the\nadopted PPA, we have accomplished putting neurons built with the modified\nIzhikevich model into SNN practice, dubbed as the Standardized Izhikevich Tonic\n(SIT) neuron. For performance, we evaluate the suggested technique for image\nclassification tasks in self-built LIF-and-SIT-consisted SNNs, named Hybrid\nNeural Network (HNN) on static MNIST, Fashion-MNIST, CIFAR-10 datasets and\nneuromorphic N-MNIST, CIFAR10-DVS, and DVS128 Gesture datasets. The\nexperimental results indicate that the suggested method achieves comparable\naccuracy while exhibiting more biologically realistic behaviors on nearly all\ntest datasets, demonstrating the efficiency of this novel strategy in bridging\nthe gap between neurodynamics and SNN practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Cheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui-Jie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liang-Jian Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensor Data Validation and Driving Safety in Autonomous Driving Systems. (arXiv:2203.16130v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16130","description":"<p>Autonomous driving technology has drawn a lot of attention due to its fast\ndevelopment and extremely high commercial values. The recent technological leap\nof autonomous driving can be primarily attributed to the progress in the\nenvironment perception. Good environment perception provides accurate\nhigh-level environment information which is essential for autonomous vehicles\nto make safe and precise driving decisions and strategies. Moreover, such\nprogress in accurate environment perception would not be possible without deep\nlearning models and advanced onboard sensors, such as optical sensors (LiDARs\nand cameras), radars, GPS. However, the advanced sensors and deep learning\nmodels are prone to recently invented attack methods. For example, LiDARs and\ncameras can be compromised by optical attacks, and deep learning models can be\nattacked by adversarial examples. The attacks on advanced sensors and deep\nlearning models can largely impact the accuracy of the environment perception,\nposing great threats to the safety and security of autonomous vehicles. In this\nthesis, we study the detection methods against the attacks on onboard sensors\nand the linkage between attacked deep learning models and driving safety for\nautonomous vehicles. To detect the attacks, redundant data sources can be\nexploited, since information distortions caused by attacks in victim sensor\ndata result in inconsistency with the information from other redundant sources.\nTo study the linkage between attacked deep learning models and driving\nsafety...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jindi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Robot Active Mapping via Neural Bipartite Graph Matching. (arXiv:2203.16319v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16319","description":"<p>We study the problem of multi-robot active mapping, which aims for complete\nscene map construction in minimum time steps. The key to this problem lies in\nthe goal position estimation to enable more efficient robot movements. Previous\napproaches either choose the frontier as the goal position via a myopic\nsolution that hinders the time efficiency, or maximize the long-term value via\nreinforcement learning to directly regress the goal position, but does not\nguarantee the complete map construction. In this paper, we propose a novel\nalgorithm, namely NeuralCoMapping, which takes advantage of both approaches. We\nreduce the problem to bipartite graph matching, which establishes the node\ncorrespondences between two graphs, denoting robots and frontiers. We introduce\na multiplex graph neural network (mGNN) that learns the neural distance to fill\nthe affinity matrix for more effective graph matching. We optimize the mGNN\nwith a differentiable linear assignment layer by maximizing the long-term\nvalues that favor time efficiency and map completeness via reinforcement\nlearning. We compare our algorithm with several state-of-the-art multi-robot\nactive mapping approaches and adapted reinforcement-learning baselines.\nExperimental results demonstrate the superior performance and exceptional\ngeneralization ability of our algorithm on various indoor scenes and unseen\nnumber of robots, when only trained with 9 indoor scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Kai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Siyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoquan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Multimodal Depth Estimation from Light Fields. (arXiv:2203.16542v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16542","description":"<p>Light field applications, especially light field rendering and depth\nestimation, developed rapidly in recent years. While state-of-the-art light\nfield rendering methods handle semi-transparent and reflective objects well,\ndepth estimation methods either ignore these cases altogether or only deliver a\nweak performance. We argue that this is due current methods only considering a\nsingle \"true\" depth, even when multiple objects at different depths contributed\nto the color of a single pixel. Based on the simple idea of outputting a\nposterior depth distribution instead of only a single estimate, we develop and\nexplore several different deep-learning-based approaches to the problem.\nAdditionally, we contribute the first \"multimodal light field depth dataset\"\nthat contains the depths of all objects which contribute to the color of a\npixel. This allows us to supervise the multimodal depth prediction and also\nvalidate all methods by measuring the KL divergence of the predicted\nposteriors. With our thorough analysis and novel dataset, we aim to start a new\nline of depth estimation research that overcomes some of the long-standing\nlimitations of this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leistner_T/0/1/0/all/0/1\">Titus Leistner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackowiak_R/0/1/0/all/0/1\">Radek Mackowiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardizzone_L/0/1/0/all/0/1\">Lynton Ardizzone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1\">Ullrich K&#xf6;the</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1\">Carsten Rother</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning for the Classification of Tumor Infiltrating Lymphocytes. (arXiv:2203.16622v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.16622","description":"<p>We evaluate the performance of federated learning (FL) in developing deep\nlearning models for analysis of digitized tissue sections. A classification\napplication was considered as the example use case, on quantifiying the\ndistribution of tumor infiltrating lymphocytes within whole slide images\n(WSIs). A deep learning classification model was trained using 50*50 square\nmicron patches extracted from the WSIs. We simulated a FL environment in which\na dataset, generated from WSIs of cancer from numerous anatomical sites\navailable by The Cancer Genome Atlas repository, is partitioned in 8 different\nnodes. Our results show that the model trained with the federated training\napproach achieves similar performance, both quantitatively and qualitatively,\nto that of a model trained with all the training data pooled at a centralized\nlocation. Our study shows that FL has tremendous potential for enabling\ndevelopment of more robust and accurate models for histopathology image\nanalysis without having to collect large and diverse training data at a single\nlocation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1\">Ujjwal Baid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pati_S/0/1/0/all/0/1\">Sarthak Pati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin M. Kurc</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_R/0/1/0/all/0/1\">Rajarsi Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bremer_E/0/1/0/all/0/1\">Erich Bremer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abousamra_S/0/1/0/all/0/1\">Shahira Abousamra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thakur_S/0/1/0/all/0/1\">Siddhesh P. Thakur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saltz_J/0/1/0/all/0/1\">Joel H. Saltz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Augmentations for Video Representation Learning. (arXiv:2203.16632v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16632","description":"<p>This paper focuses on self-supervised video representation learning. Most\nexisting approaches follow the contrastive learning pipeline to construct\npositive and negative pairs by sampling different clips. However, this\nformulation tends to bias to static background and have difficulty establishing\nglobal temporal structures. The major reason is that the positive pairs, i.e.,\ndifferent clips sampled from the same video, have limited temporal receptive\nfield, and usually share similar background but differ in motions. To address\nthese problems, we propose a framework to jointly utilize local clips and\nglobal videos to learn from detailed region-level correspondence as well as\ngeneral long-term temporal relations. Based on a set of controllable\naugmentations, we achieve accurate appearance and motion pattern alignment\nthrough soft spatio-temporal region contrast. Our formulation is able to avoid\nthe low-level redundancy shortcut by mutual information minimization to improve\nthe generalization. We also introduce local-global temporal order dependency to\nfurther bridge the gap between clip-level and video-level representations for\nrobust temporal modeling. Extensive experiments demonstrate that our framework\nis superior on three video benchmarks in action recognition and video\nretrieval, capturing more accurate temporal dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1\">John See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset of Images of Public Streetlights with Operational Monitoring using Computer Vision Techniques. (arXiv:2203.16915v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16915","description":"<p>A dataset of street light images is presented. Our dataset consists of\n$\\sim350\\textrm{k}$ images, taken from 140 UMBRELLA nodes installed in the\nSouth Gloucestershire region in the UK. Each UMBRELLA node is installed on the\npole of a lamppost and is equipped with a Raspberry Pi Camera Module v1 facing\nupwards towards the sky and lamppost light bulb. Each node collects an image at\nhourly intervals for 24h every day. The data collection spans for a period of\nsix months.\n</p>\n<p>Each image taken is logged as a single entry in the dataset along with the\nGlobal Positioning System (GPS) coordinates of the lamppost. All entries in the\ndataset have been post-processed and labelled based on the operation of the\nlamppost, i.e., whether the lamppost is switched ON or OFF. The dataset can be\nused to train deep neural networks and generate pre-trained models providing\nfeature representations for smart city CCTV applications, smart weather\ndetection algorithms, or street infrastructure monitoring. The dataset can be\nfound at \\url{https://doi.org/10.5281/zenodo.6046758}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mavromatis_I/0/1/0/all/0/1\">Ioannis Mavromatis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanoev_A/0/1/0/all/0/1\">Aleksandar Stanoev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carnelli_P/0/1/0/all/0/1\">Pietro Carnelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yichao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sooriyabandara_M/0/1/0/all/0/1\">Mahesh Sooriyabandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Aftab Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher. (arXiv:2203.17008v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.17008","description":"<p>Model quantization is considered as a promising method to greatly reduce the\nresource requirements of deep neural networks. To deal with the performance\ndrop induced by quantization errors, a popular method is to use training data\nto fine-tune quantized networks. In real-world environments, however, such a\nmethod is frequently infeasible because training data is unavailable due to\nsecurity, privacy, or confidentiality concerns. Zero-shot quantization\naddresses such problems, usually by taking information from the weights of a\nfull-precision teacher network to compensate the performance drop of the\nquantized networks. In this paper, we first analyze the loss surface of\nstate-of-the-art zero-shot quantization techniques and provide several\nfindings. In contrast to usual knowledge distillation problems, zero-shot\nquantization often suffers from 1) the difficulty of optimizing multiple loss\nterms together, and 2) the poor generalization capability due to the use of\nsynthetic samples. Furthermore, we observe that many weights fail to cross the\nrounding threshold during training the quantized networks even when it is\nnecessary to do so for better performance. Based on the observations, we\npropose AIT, a simple yet powerful technique for zero-shot quantization, which\naddresses the aforementioned two problems in the following way: AIT i) uses a\nKL distance loss only without a cross-entropy loss, and ii) manipulates\ngradients to guarantee that a certain portion of weights are properly updated\nafter crossing the rounding thresholds. Experiments show that AIT outperforms\nthe performance of many existing methods by a great margin, taking over the\noverall state-of-the-art position in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kanghyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hye Yoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Deokki Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Joonsang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Noseong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngsok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinho Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Network Modelling for MODIS Land Surface Temperature Super-Resolution. (arXiv:2202.10753v2 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2202.10753","description":"<p>Nowadays, thermal infrared satellite remote sensors enable to extract very\ninteresting information at large scale, in particular Land Surface Temperature\n(LST). However such data are limited in spatial and/or temporal resolutions\nwhich prevents from an analysis at fine scales. For example, MODIS satellite\nprovides daily acquisitions with 1Km spatial resolutions which is not\nsufficient to deal with highly heterogeneous environments as agricultural\nparcels. Therefore, image super-resolution is a crucial task to better exploit\nMODIS LSTs. This issue is tackled in this paper. We introduce a deep\nlearning-based algorithm, named Multi-residual U-Net, for super-resolution of\nMODIS LST single-images. Our proposed network is a modified version of U-Net\narchitecture, which aims at super-resolving the input LST image from 1Km to\n250m per pixel. The results show that our Multi-residual U-Net outperforms\nother state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh Minh Nguyen</a> (IMT Atlantique), <a href=\"http://arxiv.org/find/cs/1/au:+Tian_G/0/1/0/all/0/1\">Ganglin Tian</a> (IMT Atlantique), <a href=\"http://arxiv.org/find/cs/1/au:+Vo_M/0/1/0/all/0/1\">Minh-Triet Vo</a> (IMT Atlantique), <a href=\"http://arxiv.org/find/cs/1/au:+Michel_A/0/1/0/all/0/1\">Aur&#xe9;lie Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corpetti_T/0/1/0/all/0/1\">Thomas Corpetti</a> (CNRS, LETG), <a href=\"http://arxiv.org/find/cs/1/au:+Granero_Belinchon_C/0/1/0/all/0/1\">Carlos Granero-Belinchon</a> (Lab-STICC\\_OSE, IMT Atlantique - MEE)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}