{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.6","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Findings from Experiments of On-line Joint Reinforcement Learning of Semantic Parser and Dialogue Manager with real Users. (arXiv:2110.13213v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13213","description":"<p>Design of dialogue systems has witnessed many advances lately, yet acquiring\nhuge set of data remains an hindrance to their fast development for a new task\nor language. Besides, training interactive systems with batch data is not\nsatisfactory. On-line learning is pursued in this paper as a convenient way to\nalleviate these difficulties. After the system modules are initiated, a single\nprocess handles data collection, annotation and use in training algorithms. A\nnew challenge is to control the cost of the on-line learning borne by the user.\nOur work focuses on learning the semantic parsing and dialogue management\nmodules (speech recognition and synthesis offer ready-for-use solutions). In\nthis context we investigate several variants of simultaneous learning which are\ntested in user trials. In our experiments, with varying merits, they can all\nachieve good performance with only a few hundreds of training dialogues and\noverstep a handcrafted system. The analysis of these experiments gives us some\ninsights, discussed in the paper, into the difficulty for the system's trainers\nto establish a coherent and constant behavioural strategy to enable a fast and\ngood-quality training phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riou_M/0/1/0/all/0/1\">Matthieu Riou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabaian_B/0/1/0/all/0/1\">Bassam Jabaian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huet_S/0/1/0/all/0/1\">St&#xe9;phane Huet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefevre_F/0/1/0/all/0/1\">Fabrice Lef&#xe8;vre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. (arXiv:2110.13214v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13214","description":"<p>Current visual question answering (VQA) tasks mainly consider answering\nhuman-annotated questions for natural images. However, aside from natural\nimages, abstract diagrams with semantic richness are still understudied in\nvisual understanding and reasoning research. In this work, we introduce a new\nchallenge of Icon Question Answering (IconQA) with the goal of answering a\nquestion in an icon image context. We release IconQA, a large-scale dataset\nthat consists of 107,439 questions and three sub-tasks: multi-image-choice,\nmulti-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by\nreal-world diagram word problems that highlight the importance of abstract\ndiagram understanding and comprehensive cognitive reasoning. Thus, IconQA\nrequires not only perception skills like object recognition and text\nunderstanding, but also diverse cognitive reasoning skills, such as geometric\nreasoning, commonsense reasoning, and arithmetic reasoning. To facilitate\npotential IconQA models to learn semantic representations for icon images, we\nfurther release an icon dataset Icon645 which contains 645,687 colored icons on\n377 classes. We conduct extensive user studies and blind experiments and\nreproduce a wide range of advanced VQA methods to benchmark the IconQA task.\nAlso, we develop a strong IconQA baseline Patch-TRM that applies a pyramid\ncross-modal Transformer with input diagram embeddings pre-trained on the icon\ndataset. IconQA and Icon645 are available at https://iconqa.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tony Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributionally Robust Recurrent Decoders with Random Network Distillation. (arXiv:2110.13229v1 [cs.LG])","link":"http://arxiv.org/abs/2110.13229","description":"<p>Neural machine learning models can successfully model language that is\nsimilar to their training distribution, but they are highly susceptible to\ndegradation under distribution shift, which occurs in many practical\napplications when processing out-of-domain (OOD) text. This has been attributed\nto \"shortcut learning\": relying on weak correlations over arbitrary large\ncontexts.\n</p>\n<p>We propose a method based on OOD detection with Random Network Distillation\nto allow an autoregressive language model to automatically disregard OOD\ncontext during inference, smoothly transitioning towards a less expressive but\nmore robust model as the data becomes more OOD while retaining its full context\ncapability when operating in-distribution. We apply our method to a GRU\narchitecture, demonstrating improvements on multiple language modeling (LM)\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miceli_Barone_A/0/1/0/all/0/1\">Antonio Valerio Miceli-Barone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Diversity of Unsupervised Paraphrasing with Embedding Outputs. (arXiv:2110.13231v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13231","description":"<p>We present a novel technique for zero-shot paraphrase generation. The key\ncontribution is an end-to-end multilingual paraphrasing model that is trained\nusing translated parallel corpora to generate paraphrases into \"meaning spaces\"\n-- replacing the final softmax layer with word embeddings. This architectural\nmodification, plus a training procedure that incorporates an autoencoding\nobjective, enables effective parameter sharing across languages for more fluent\nmonolingual rewriting, and facilitates fluency and diversity in generation. Our\ncontinuous-output paraphrase generation models outperform zero-shot\nparaphrasing baselines when evaluated on two languages using a battery of\ncomputational metrics as well as in human assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jegadeesan_M/0/1/0/all/0/1\">Monisha Jegadeesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepHelp: Deep Learning for Shout Crisis Text Conversations. (arXiv:2110.13244v1 [cs.LG])","link":"http://arxiv.org/abs/2110.13244","description":"<p>The Shout Crisis Text Line provides individuals undergoing mental health\ncrises an opportunity to have an anonymous text message conversation with a\ntrained Crisis Volunteer (CV). This project partners with Shout and its parent\norganisation, Mental Health Innovations, to explore the applications of Machine\nLearning in understanding Shout's conversations and improving its service. The\noverarching aim of this project is to develop a proof-of-concept model to\ndemonstrate the potential of applying deep learning to crisis text messages.\n</p>\n<p>Specifically, this project aims to use deep learning to (1) predict an\nindividual's risk of suicide or self-harm, (2) assess conversation success and\nCV skill using robust metrics, and (3) extrapolate demographic information from\na texter survey to conversations where the texter did not complete the survey.\nTo these ends, contributions to deep learning include a modified\nTransformer-over-BERT model; a framework for multitask learning to improve\ngeneralisation in the presence of sparse labels; and a mathematical model for\nusing imperfect machine learning models to estimate population parameters from\na biased training set.\n</p>\n<p>Key results include a deep learning model with likely better performance at\npredicting suicide risk than trained CVs and the ability to predict whether a\ntexter is 21 or under with 88.4% accuracy. We produce three metrics for\nconversation success and evaluate the validity and usefulness for each.\nFinally, reversal of participation bias provides evidence that women, who make\nup 80.3% of conversations with an associated texter survey, make up closer to\n73.5%- 74.8% of all conversations; and that if, after every conversation, the\ntexter had shared whether they found their conversation helpful, affirmative\nanswers would fall from 85.1% to 45.45% - 46.51%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cahn_D/0/1/0/all/0/1\">Daniel Cahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposure of occupations to technologies of the fourth industrial revolution. (arXiv:2110.13317v1 [cs.CY])","link":"http://arxiv.org/abs/2110.13317","description":"<p>The fourth industrial revolution (4IR) is likely to have a substantial impact\non the economy. Companies need to build up capabilities to implement new\ntechnologies, and automation may make some occupations obsolete. However,\nwhere, when, and how the change will happen remain to be determined. Robust\nempirical indicators of technological progress linked to occupations can help\nto illuminate this change. With this aim, we provide such an indicator based on\npatent data. Using natural language processing, we calculate patent exposure\nscores for more than 900 occupations, which represent the technological\nprogress related to them. To provide a lens on the impact of the 4IR, we\ndifferentiate between traditional and 4IR patent exposure. Our method differs\nfrom previous approaches in that it both accounts for the diversity of\ntask-level patent exposures within an occupation and reflects work activities\nmore accurately. We find that exposure to 4IR patents differs from traditional\npatent exposure. Manual tasks, and accordingly occupations such as construction\nand production, are exposed mainly to traditional (non-4IR) patents but have\nlow exposure to 4IR patents. The analysis suggests that 4IR technologies may\nhave a negative impact on job growth; this impact appears 10 to 20 years after\npatent filing. Further, we compared the 4IR exposure to other automation and AI\nexposure scores. Whereas many measures refer to theoretical automation\npotential, our patent-based indicator reflects actual technology diffusion. Our\nwork not only allows analyses of the impact of 4IR technologies as a whole, but\nalso provides exposure scores for more than 300 technology fields, such as AI\nand smart office technologies. Finally, the work provides a general mapping of\npatents to tasks and occupations, which enables future researchers to construct\nindividual exposure measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meindl_B/0/1/0/all/0/1\">Benjamin Meindl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_M/0/1/0/all/0/1\">Morgan R. Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendonca_J/0/1/0/all/0/1\">Joana Mendon&#xe7;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Specific Dependency-based Word Embedding Methods. (arXiv:2110.13376v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13376","description":"<p>Two task-specific dependency-based word embedding methods are proposed for\ntext classification in this work. In contrast with universal word embedding\nmethods that work for generic tasks, we design task-specific word embedding\nmethods to offer better performance in a specific task. Our methods follow the\nPPMI matrix factorization framework and derive word contexts from the\ndependency parse tree. The first one, called the dependency-based word\nembedding (DWE), chooses keywords and neighbor words of a target word in the\ndependency parse tree as contexts to build the word-context matrix. The second\nmethod, named class-enhanced dependency-based word embedding (CEDWE), learns\nfrom word-context as well as word-class co-occurrence statistics. DWE and CEDWE\nare evaluated on popular text classification datasets to demonstrate their\neffectiveness. It is shown by experimental results they outperform several\nstate-of-the-art word embedding methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chengwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis. (arXiv:2110.13398v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13398","description":"<p>Aspect-based Sentiment Analysis (ABSA) aims to determine the sentiment\npolarity towards an aspect. Because of the expensive and limited labelled data,\nthe pretraining strategy has become the de-facto standard for ABSA. However,\nthere always exists severe domain shift between the pretraining and downstream\nABSA datasets, hindering the effective knowledge transfer when directly\nfinetuning and making the downstream task performs sub-optimal. To mitigate\nsuch domain shift, we introduce a unified alignment pretraining framework into\nthe vanilla pretrain-finetune pipeline with both instance- and knowledge-level\nalignments. Specifically, we first devise a novel coarse-to-fine retrieval\nsampling approach to select target domain-related instances from the\nlarge-scale pretraining dataset, thus aligning the instances between\npretraining and target domains (\\textit{First Stage}). Then, we introduce a\nknowledge guidance-based strategy to further bridge the domain gap at the\nknowledge level. In practice, we formulate the model pretrained on the sampled\ninstances into a knowledge guidance model and a learner model, respectively. On\nthe target dataset, we design an on-the-fly teacher-student joint fine-tuning\napproach to progressively transfer the knowledge from the knowledge guidance\nmodel to the learner model (\\textit{Second Stage}). Thereby, the learner model\ncan maintain more domain-invariant knowledge when learning new knowledge from\nthe target dataset. In the \\textit{Third Stage,} the learner model is finetuned\nto better adapt its learned knowledge to the target dataset. Extensive\nexperiments and analyses on several ABSA benchmarks demonstrate the\neffectiveness and universality of our proposed pretraining framework. Notably,\nour pretraining framework pushes several strong baseline models up to the new\nstate-of-the-art records. We release our code and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qihuang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain. (arXiv:2110.13434v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13434","description":"<p>During the fine-tuning phase of transfer learning, the pretrained vocabulary\nremains unchanged, while model parameters are updated. The vocabulary generated\nbased on the pretrained data is suboptimal for downstream data when domain\ndiscrepancy exists. We propose to consider the vocabulary as an optimizable\nparameter, allowing us to update the vocabulary by expanding it with\ndomain-specific vocabulary based on a tokenization statistic. Furthermore, we\npreserve the embeddings of the added words from overfitting to downstream data\nby utilizing knowledge learned from a pretrained language model with a\nregularization term. Our method achieved consistent performance improvements on\ndiverse domains (i.e., biomedical, computer science, news, and reviews).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jimin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyesu Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing Complex Questions Makes Multi-Hop QA Easier and More Interpretable. (arXiv:2110.13472v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13472","description":"<p>Multi-hop QA requires the machine to answer complex questions through finding\nmultiple clues and reasoning, and provide explanatory evidence to demonstrate\nthe machine reasoning process. We propose Relation Extractor-Reader and\nComparator (RERC), a three-stage framework based on complex question\ndecomposition, which is the first work that the RERC model has been proposed\nand applied in solving the multi-hop QA challenges. The Relation Extractor\ndecomposes the complex question, and then the Reader answers the sub-questions\nin turn, and finally the Comparator performs numerical comparison and\nsummarizes all to get the final answer, where the entire process itself\nconstitutes a complete reasoning evidence path. In the 2WikiMultiHopQA dataset,\nour RERC model has achieved the most advanced performance, with a winning joint\nF1 score of 53.58 on the leaderboard. All indicators of our RERC are close to\nhuman performance, with only 1.95 behind the human level in F1 score of support\nfact. At the same time, the evidence path provided by our RERC framework has\nexcellent readability and faithfulness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Ruiliu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuejun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yonghong Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simultaneous Neural Machine Translation with Constituent Label Prediction. (arXiv:2110.13480v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13480","description":"<p>Simultaneous translation is a task in which translation begins before the\nspeaker has finished speaking, so it is important to decide when to start the\ntranslation process. However, deciding whether to read more input words or\nstart to translate is difficult for language pairs with different word orders\nsuch as English and Japanese. Motivated by the concept of pre-reordering, we\npropose a couple of simple decision rules using the label of the next\nconstituent predicted by incremental constituent label prediction. In\nexperiments on English-to-Japanese simultaneous translation, the proposed\nmethod outperformed baselines in the quality-latency trade-off.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kano_Y/0/1/0/all/0/1\">Yasumasa Kano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the Sufficiency of Arguments through Conclusion Generation. (arXiv:2110.13495v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13495","description":"<p>The premises of an argument give evidence or other reasons to support a\nconclusion. However, the amount of support required depends on the generality\nof a conclusion, the nature of the individual premises, and similar. An\nargument whose premises make its conclusion rationally worthy to be drawn is\ncalled sufficient in argument quality research. Previous work tackled\nsufficiency assessment as a standard text classification problem, not modeling\nthe inherent relation of premises and conclusion. In this paper, we hypothesize\nthat the conclusion of a sufficient argument can be generated from its\npremises. To study this hypothesis, we explore the potential of assessing\nsufficiency based on the output of large-scale pre-trained language models. Our\nbest model variant achieves an F1-score of .885, outperforming the previous\nstate-of-the-art and being on par with human experts. While manual evaluation\nreveals the quality of the generated conclusions, their impact remains low\nultimately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gurcke_T/0/1/0/all/0/1\">Timon Gurcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshomary_M/0/1/0/all/0/1\">Milad Alshomary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Part & Whole Extraction: Towards A Deep Understanding of Quantitative Facts for Percentages in Text. (arXiv:2110.13505v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13505","description":"<p>We study the problem of quantitative facts extraction for text with\npercentages. For example, given the sentence \"30 percent of Americans like\nwatching football, while 20% prefer to watch NBA.\", our goal is to obtain a\ndeep understanding of the percentage numbers (\"30 percent\" and \"20%\") by\nextracting their quantitative facts: part (\"like watching football\" and \"prefer\nto watch NBA\") and whole (\"Americans). These quantitative facts can empower new\napplications like automated infographic generation. We formulate part and whole\nextraction as a sequence tagging problem. Due to the large gap between\npart/whole and its corresponding percentage, we introduce skip mechanism in\nsequence modeling, and achieved improved performance on both our task and the\nCoNLL-2003 named entity recognition task. Experimental results demonstrate that\nlearning to skip in sequence tagging is promising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Lei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Entity Representation Model for Chain Reasoning over Knowledge Graphs. (arXiv:2110.13522v1 [cs.LG])","link":"http://arxiv.org/abs/2110.13522","description":"<p>Logical reasoning over Knowledge Graphs (KGs) is a fundamental technique that\ncan provide efficient querying mechanism over large and incomplete databases.\nCurrent approaches employ spatial geometries such as boxes to learn query\nrepresentations that encompass the answer entities and model the logical\noperations of projection and intersection. However, their geometry is\nrestrictive and leads to non-smooth strict boundaries, which further results in\nambiguous answer entities. Furthermore, previous works propose transformation\ntricks to handle unions which results in non-closure and, thus, cannot be\nchained in a stream. In this paper, we propose a Probabilistic Entity\nRepresentation Model (PERM) to encode entities as a Multivariate Gaussian\ndensity with mean and covariance parameters to capture its semantic position\nand smooth decision boundary, respectively. Additionally, we also define the\nclosed logical operations of projection, intersection, and union that can be\naggregated using an end-to-end objective function. On the logical query\nreasoning problem, we demonstrate that the proposed PERM significantly\noutperforms the state-of-the-art methods on various public benchmark KG\ndatasets on standard evaluation metrics. We also evaluate PERM's competence on\na COVID-19 drug-repurposing case study and show that our proposed work is able\nto recommend drugs with substantially better F1 than current methods. Finally,\nwe demonstrate the working of our PERM's query answering process through a\nlow-dimensional visualization of the Gaussian representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_N/0/1/0/all/0/1\">Nurendra Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_N/0/1/0/all/0/1\">Nikhil Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katariya_S/0/1/0/all/0/1\">Sumeet Katariya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subbian_K/0/1/0/all/0/1\">Karthik Subbian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Rule Induction. (arXiv:2110.13577v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13577","description":"<p>Rules have a number of desirable properties. It is easy to understand, infer\nnew knowledge, and communicate with other inference systems. One weakness of\nthe previous rule induction systems is that they only find rules within a\nknowledge base (KB) and therefore cannot generalize to more open and complex\nreal-world rules. Recently, the language model (LM)-based rule generation are\nproposed to enhance the expressive power of the rules. In this paper, we\nrevisit the differences between KB-based rule induction and LM-based rule\ngeneration. We argue that, while KB-based methods inducted rules by discovering\ndata commonalities, the current LM-based methods are \"learning rules from\nrules\". This limits these methods to only produce \"canned\" rules whose patterns\nare constrained by the annotated rules, while discarding the rich expressive\npower of LMs for free text.\n</p>\n<p>Therefore, in this paper, we propose the open rule induction problem, which\naims to induce open rules utilizing the knowledge in LMs. Besides, we propose\nthe Orion (\\underline{o}pen \\underline{r}ule \\underline{i}nducti\\underline{on})\nsystem to automatically mine open rules from LMs without supervision of\nannotated rules. We conducted extensive experiments to verify the quality and\nquantity of the inducted open rules. Surprisingly, when applying the open rules\nin downstream tasks (i.e. relation extraction), these automatically inducted\nrules even outperformed the manually annotated rules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wanyun Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingran Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"s2s-ft: Fine-Tuning Pretrained Transformer Encoders for Sequence-to-Sequence Learning. (arXiv:2110.13640v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13640","description":"<p>Pretrained bidirectional Transformers, such as BERT, have achieved\nsignificant improvements in a wide variety of language understanding tasks,\nwhile it is not straightforward to directly apply them for natural language\ngeneration. In this paper, we present a sequence-to-sequence fine-tuning\ntoolkit s2s-ft, which adopts pretrained Transformers for conditional generation\ntasks. Inspired by UniLM, we implement three sequence-to-sequence fine-tuning\nalgorithms, namely, causal fine-tuning, masked fine-tuning, and pseudo-masked\nfine-tuning. By leveraging the existing pretrained bidirectional Transformers,\nexperimental results show that s2s-ft achieves strong performance on several\nbenchmarks of abstractive summarization, and question generation. Moreover, we\ndemonstrate that the package s2s-ft supports both monolingual and multilingual\nNLG tasks. The s2s-ft toolkit is available at\nhttps://github.com/microsoft/unilm/tree/master/s2s-ft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Character-based Language Models Improve Downstream Task Performance in Low-Resource and Noisy Language Scenarios?. (arXiv:2110.13658v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13658","description":"<p>Recent impressive improvements in NLP, largely based on the success of\ncontextual neural language models, have been mostly demonstrated on at most a\ncouple dozen high-resource languages. Building language models and, more\ngenerally, NLP systems for non-standardized and low-resource languages remains\na challenging task. In this work, we focus on North-African colloquial\ndialectal Arabic written using an extension of the Latin script, called\nNArabizi, found mostly on social media and messaging communication. In this\nlow-resource scenario with data displaying a high level of variability, we\ncompare the downstream performance of a character-based language model on\npart-of-speech tagging and dependency parsing to that of monolingual and\nmultilingual models. We show that a character-based model trained on only 99k\nsentences of NArabizi and fined-tuned on a small treebank of this language\nleads to performance close to those obtained with the same architecture\npre-trained on large multilingual and monolingual models. Confirming these\nresults a on much larger data set of noisy French user-generated content, we\nargue that such character-based language models can be an asset for NLP in\nlow-resource and high language variability set-tings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riabi_A/0/1/0/all/0/1\">Arij Riabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seddah_D/0/1/0/all/0/1\">Djam&#xe9; Seddah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioIE: Biomedical Information Extraction with Multi-head Attention Enhanced Graph Convolutional Network. (arXiv:2110.13683v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13683","description":"<p>Constructing large-scaled medical knowledge graphs can significantly boost\nhealthcare applications for medical surveillance, bring much attention from\nrecent research. An essential step in constructing large-scale MKG is\nextracting information from medical reports. Recently, information extraction\ntechniques have been proposed and show promising performance in biomedical\ninformation extraction. However, these methods only consider limited types of\nentity and relation due to the noisy biomedical text data with complex entity\ncorrelations. Thus, they fail to provide enough information for constructing\nMKGs and restrict the downstream applications. To address this issue, we\npropose Biomedical Information Extraction, a hybrid neural network to extract\nrelations from biomedical text and unstructured medical reports. Our model\nutilizes a multi-head attention enhanced graph convolutional network to capture\nthe complex relations and context information while resisting the noise from\nthe data. We evaluate our model on two major biomedical relationship extraction\ntasks, chemical-disease relation and chemical-protein interaction, and a\ncross-hospital pan-cancer pathology report corpus. The results show that our\nmethod achieves superior performance than baselines. Furthermore, we evaluate\nthe applicability of our method under a transfer learning setting and show that\nBioIE achieves promising performance in processing medical text from different\nformats and writing styles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zeyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tieliang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunbao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Explicit-Joint and Supervised-Contrastive Learning Framework for Few-Shot Intent Classification and Slot Filling. (arXiv:2110.13691v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13691","description":"<p>Intent classification (IC) and slot filling (SF) are critical building blocks\nin task-oriented dialogue systems. These two tasks are closely-related and can\nflourish each other. Since only a few utterances can be utilized for\nidentifying fast-emerging new intents and slots, data scarcity issue often\noccurs when implementing IC and SF. However, few IC/SF models perform well when\nthe number of training samples per class is quite small. In this paper, we\npropose a novel explicit-joint and supervised-contrastive learning framework\nfor few-shot intent classification and slot filling. Its highlights are as\nfollows. (i) The model extracts intent and slot representations via\nbidirectional interactions, and extends prototypical network to achieve\nexplicit-joint learning, which guarantees that IC and SF tasks can mutually\nreinforce each other. (ii) The model integrates with supervised contrastive\nlearning, which ensures that samples from same class are pulled together and\nsamples from different classes are pushed apart. In addition, the model follows\na not common but practical way to construct the episode, which gets rid of the\ntraditional setting with fixed way and shot, and allows for unbalanced\ndatasets. Extensive experiments on three public datasets show that our model\ncan achieve promising performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaotong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Siyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianchao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotating Implicit Reasoning in Arguments with Causal Links. (arXiv:2110.13692v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13692","description":"<p>Most of the existing work that focus on the identification of implicit\nknowledge in arguments generally represent implicit knowledge in the form of\ncommonsense or factual knowledge. However, such knowledge is not sufficient to\nunderstand the implicit reasoning link between individual argumentative\ncomponents (i.e., claim and premise). In this work, we focus on identifying the\nimplicit knowledge in the form of argumentation knowledge which can help in\nunderstanding the reasoning link in arguments. Being inspired by the Argument\nfrom Consequences scheme, we propose a semi-structured template to represent\nsuch argumentation knowledge that explicates the implicit reasoning in\narguments via causality. We create a novel two-phase annotation process with\nsimplified guidelines and show how to collect and filter high-quality implicit\nreasonings via crowdsourcing. We find substantial inter-annotator agreement for\nquality evaluation between experts, but find evidence that casts a few\nquestions on the feasibility of collecting high-quality semi-structured\nimplicit reasoning through our crowdsourcing process. We release our\nmaterials(i.e., crowdsourcing guidelines and collected implicit reasonings) to\nfacilitate further research towards the structured representation of\nargumentation knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Keshav Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_N/0/1/0/all/0/1\">Naoya Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mim_F/0/1/0/all/0/1\">Farjana Sultana Mim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naitoh_S/0/1/0/all/0/1\">Shoichi Naitoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DASentimental: Detecting depression, anxiety and stress in texts via emotional recall, cognitive networks and machine learning. (arXiv:2110.13710v1 [cs.CY])","link":"http://arxiv.org/abs/2110.13710","description":"<p>Most current affect scales and sentiment analysis on written text focus on\nquantifying valence (sentiment) -- the most primary dimension of emotion.\nHowever, emotions are broader and more complex than valence. Distinguishing\nnegative emotions of similar valence could be important in contexts such as\nmental health. This project proposes a semi-supervised machine learning model\n(DASentimental) to extract depression, anxiety and stress from written text.\nFirst, we trained the model to spot how sequences of recalled emotion words by\n$N=200$ individuals correlated with their responses to the Depression Anxiety\nStress Scale (DASS-21). Within the framework of cognitive network science, we\nmodel every list of recalled emotions as a walk over a networked mental\nrepresentation of semantic memory, with emotions connected according to free\nassociations in people's memory. Among several tested machine learning\napproaches, we find that a multilayer perceptron neural network trained on word\nsequences and semantic network distances can achieve state-of-art,\ncross-validated predictions for depression ($R = 0.7$), anxiety ($R = 0.44$)\nand stress ($R = 0.52$). Though limited by sample size, this first-of-its-kind\napproach enables quantitative explorations of key semantic dimensions behind\nDAS levels. We find that semantic distances between recalled emotions and the\ndyad \"sad-happy\" are crucial features for estimating depression levels but are\nless important for anxiety and stress. We also find that semantic distance of\nrecalls from \"fear\" can boost the prediction of anxiety but it becomes\nredundant when the \"sad-happy\" dyad is considered. Adopting DASentimental as a\nsemi-supervised learning tool to estimate DAS in text, we apply it to a dataset\nof 142 suicide notes. We conclude by discussing key directions for future\nresearch enabled by artificial intelligence detecting stress, anxiety and\ndepression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fatima_A/0/1/0/all/0/1\">Asra Fatima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1\">Li Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hills_T/0/1/0/all/0/1\">Thomas Hills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stella_M/0/1/0/all/0/1\">Massimo Stella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Transformers Are More Efficient Language Models. (arXiv:2110.13711v1 [cs.LG])","link":"http://arxiv.org/abs/2110.13711","description":"<p>Transformer models yield impressive results on many NLP and sequence modeling\ntasks. Remarkably, Transformers can handle long sequences which allows them to\nproduce long coherent outputs: full paragraphs produced by GPT-3 or\nwell-structured images produced by DALL-E. These large language models are\nimpressive but also very inefficient and costly, which limits their\napplications and accessibility. We postulate that having an explicit\nhierarchical architecture is the key to Transformers that efficiently handle\nlong sequences. To verify this claim, we first study different ways to\ndownsample and upsample activations in Transformers so as to make them\nhierarchical. We use the best performing upsampling and downsampling layers to\ncreate Hourglass - a hierarchical Transformer language model. Hourglass\nimproves upon the Transformer baseline given the same amount of computation and\ncan yield the same results as Transformers more efficiently. In particular,\nHourglass sets new state-of-the-art for Transformer models on the ImageNet32\ngeneration task and improves language modeling efficiency on the widely studied\nenwik8 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1\">Piotr Nawrot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tworkowski_S/0/1/0/all/0/1\">Szymon Tworkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyrolski_M/0/1/0/all/0/1\">Micha&#x142; Tyrolski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_L/0/1/0/all/0/1\">&#x141;ukasz Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szegedy_C/0/1/0/all/0/1\">Christian Szegedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConE: Cone Embeddings for Multi-Hop Reasoning over Knowledge Graphs. (arXiv:2110.13715v1 [cs.AI])","link":"http://arxiv.org/abs/2110.13715","description":"<p>Query embedding (QE) -- which aims to embed entities and first-order logical\n(FOL) queries in low-dimensional spaces -- has shown great power in multi-hop\nreasoning over knowledge graphs. Recently, embedding entities and queries with\ngeometric shapes becomes a promising direction, as geometric shapes can\nnaturally represent answer sets of queries and logical relationships among\nthem. However, existing geometry-based models have difficulty in modeling\nqueries with negation, which significantly limits their applicability. To\naddress this challenge, we propose a novel query embedding model, namely Cone\nEmbeddings (ConE), which is the first geometry-based QE model that can handle\nall the FOL operations, including conjunction, disjunction, and negation.\nSpecifically, ConE represents entities and queries as Cartesian products of\ntwo-dimensional cones, where the intersection and union of cones naturally\nmodel the conjunction and disjunction operations. By further noticing that the\nclosure of complement of cones remains cones, we design geometric complement\noperators in the embedding space for the negation operations. Experiments\ndemonstrate that ConE significantly outperforms existing state-of-the-art\nmethods on benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhanqiu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"As long as you talk about me: The importance of family firm brands and the contingent role of family-firm identity. (arXiv:2110.13815v1 [econ.GN])","link":"http://arxiv.org/abs/2110.13815","description":"<p>This study explores the role of external audiences in determining the\nimportance of family firm brands and the relationship with firm performance.\nDrawing on text mining and social network analysis techniques, and considering\nthe brand prevalence, diversity, and connectivity dimensions, we use the\nsemantic brand score to measure the importance the media give to family firm\nbrands. The analysis of a sample of 52,555 news articles published in 2017\nabout 63 Italian entrepreneurial families reveals that brand importance is\npositively associated with family firm revenues, and this relationship is\nstronger when there is identity match between the family and the firm. This\nstudy advances current literature by offering a rich and multifaceted\nperspective on how external audiences perceptions of the brand shape family\nfirm performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/econ/1/au:+Rovelli_P/0/1/0/all/0/1\">P. Rovelli</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Benedetti_C/0/1/0/all/0/1\">C. Benedetti</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Massis_A/0/1/0/all/0/1\">A. De Massis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Evaluation Metrics for Speech-to-Speech Translation. (arXiv:2110.13877v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13877","description":"<p>Speech-to-speech translation combines machine translation with speech\nsynthesis, introducing evaluation challenges not present in either task alone.\nHow to automatically evaluate speech-to-speech translation is an open question\nwhich has not previously been explored. Translating to speech rather than to\ntext is often motivated by unwritten languages or languages without\nstandardized orthographies. However, we show that the previously used automatic\nmetric for this task is best equipped for standardized high-resource languages\nonly. In this work, we first evaluate current metrics for speech-to-speech\ntranslation, and second assess how translation to dialectal variants rather\nthan to standardized languages impacts various evaluation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1\">Elizabeth Salesky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mader_J/0/1/0/all/0/1\">Julian M&#xe4;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_S/0/1/0/all/0/1\">Severin Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Interlocking Dynamics of Cooperative Rationalization. (arXiv:2110.13880v1 [cs.LG])","link":"http://arxiv.org/abs/2110.13880","description":"<p>Selective rationalization explains the prediction of complex neural networks\nby finding a small subset of the input that is sufficient to predict the neural\nmodel output. The selection mechanism is commonly integrated into the model\nitself by specifying a two-component cascaded system consisting of a rationale\ngenerator, which makes a binary selection of the input features (which is the\nrationale), and a predictor, which predicts the output based only on the\nselected features. The components are trained jointly to optimize prediction\nperformance. In this paper, we reveal a major problem with such cooperative\nrationalization paradigm -- model interlocking. Interlocking arises when the\npredictor overfits to the features selected by the generator thus reinforcing\nthe generator's selection even if the selected rationales are sub-optimal. The\nfundamental cause of the interlocking problem is that the rationalization\nobjective to be minimized is concave with respect to the generator's selection\npolicy. We propose a new rationalization framework, called A2R, which\nintroduces a third component into the architecture, a predictor driven by soft\nattention as opposed to selection. The generator now realizes both soft and\nhard attention over the features and these are fed into the two different\npredictors. While the generator still seeks to support the original predictor\nperformance, it also minimizes a gap between the two predictors. As we will\nshow theoretically, since the attention-based predictor exhibits a better\nconvexity property, A2R can overcome the concavity barrier. Our experiments on\ntwo synthetic benchmarks and two real datasets demonstrate that A2R can\nsignificantly alleviate the interlock problem and find explanations that better\nalign with human judgments. We release our code at\nhttps://github.com/Gorov/Understanding_Interlocking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi S. Jaakkola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing. (arXiv:2110.13900v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13900","description":"<p>Self-supervised learning (SSL) achieves great success in speech recognition,\nwhile limited exploration has been attempted for other speech processing tasks.\nAs speech signal contains multi-faceted information including speaker identity,\nparalinguistics, spoken content, etc., learning universal representations for\nall speech tasks is challenging. In this paper, we propose a new pre-trained\nmodel, WavLM, to solve full-stack downstream speech tasks. WavLM is built based\non the HuBERT framework, with an emphasis on both spoken content modeling and\nspeaker identity preservation. We first equip the Transformer structure with\ngated relative position bias to improve its capability on recognition tasks.\nFor better speaker discrimination, we propose an utterance mixing training\nstrategy, where additional overlapped utterances are created unsupervisely and\nincorporated during model training. Lastly, we scale up the training dataset\nfrom 60k hours to 94k hours of public audio data, and optimize its training\nprocedure for better representation extraction. WavLM Large achieves\nstate-of-the-art performance on the SUPERB benchmark, and brings significant\nimprovements for various speech processing tasks on their representative\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yanmin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Micheal Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Variance of the Adaptive Learning Rate and Beyond. (arXiv:1908.03265v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1908.03265","description":"<p>The learning rate warmup heuristic achieves remarkable success in stabilizing\ntraining, accelerating convergence and improving generalization for adaptive\nstochastic optimization algorithms like RMSprop and Adam. Here, we study its\nmechanism in details. Pursuing the theory behind warmup, we identify a problem\nof the adaptive learning rate (i.e., it has problematically large variance in\nthe early stage), suggest warmup works as a variance reduction technique, and\nprovide both empirical and theoretical evidence to verify our hypothesis. We\nfurther propose RAdam, a new variant of Adam, by introducing a term to rectify\nthe variance of the adaptive learning rate. Extensive experimental results on\nimage classification, language modeling, and neural machine translation verify\nour intuition and demonstrate the effectiveness and robustness of our proposed\nmethod. All implementations are available at:\nhttps://github.com/LiyuanLucasLiu/RAdam.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global-aware Beam Search for Neural Abstractive Summarization. (arXiv:2009.06891v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.06891","description":"<p>This study develops a calibrated beam-based algorithm with awareness of the\nglobal attention distribution for neural abstractive summarization, aiming to\nimprove the local optimality problem of the original beam search in a rigorous\nway. Specifically, a novel global protocol is proposed based on the attention\ndistribution to stipulate how a global optimal hypothesis should attend to the\nsource. A global scoring mechanism is then developed to regulate beam search to\ngenerate summaries in a near-global optimal fashion. This novel design enjoys a\ndistinctive property, i.e., the global attention distribution could be\npredicted before inference, enabling step-wise improvements on the beam search\nthrough the global scoring mechanism. Extensive experiments on nine datasets\nshow that the global (attention)-aware inference significantly improves\nstate-of-the-art summarization models even using empirical hyper-parameters.\nThe algorithm is also proven robust as it remains to generate meaningful texts\nwith corrupted attention distributions. The codes and a comprehensive set of\nexamples are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Ye Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zixun Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_L/0/1/0/all/0/1\">Lu Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention over learned object embeddings enables complex visual reasoning. (arXiv:2012.08508v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.08508","description":"<p>Neural networks have achieved success in a wide array of perceptual tasks but\noften fail at tasks involving both perception and higher-level reasoning. On\nthese more challenging tasks, bespoke approaches (such as modular symbolic\ncomponents, independent dynamics models or semantic parsers) targeted towards\nthat specific type of task have typically performed better. The downside to\nthese targeted approaches, however, is that they can be more brittle than\ngeneral-purpose neural networks, requiring significant modification or even\nredesign according to the particular task at hand. Here, we propose a more\ngeneral neural-network-based approach to dynamic visual reasoning problems that\nobtains state-of-the-art performance on three different domains, in each case\noutperforming bespoke modular approaches tailored specifically to the task. Our\nmethod relies on learned object-centric representations, self-attention and\nself-supervised dynamics learning, and all three elements together are required\nfor strong performance to emerge. The success of this combination suggests that\nthere may be no need to trade off flexibility for performance on problems\ninvolving spatio-temporal or causal-style reasoning. With the right soft biases\nand learning objectives in a neural network we may be able to attain the best\nof both worlds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_M/0/1/0/all/0/1\">Malcolm Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matt Botvinick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning From Human Correction For Data-Centric Deep Learning. (arXiv:2102.00225v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.00225","description":"<p>In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and relabel\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we relabel the noisy\ndata in our dataset for our industry application. The experiment result shows\nthat our method improve the classification accuracy from 91.7% to 92.5%. The\n91.7% baseline is based on BERT training on the corrected dataset, which is\nhard to surpass.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind the Gap: Assessing Temporal Generalization in Neural Language Models. (arXiv:2102.01951v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.01951","description":"<p>Our world is open-ended, non-stationary, and constantly evolving; thus what\nwe talk about and how we talk about it change over time. This inherent dynamic\nnature of language contrasts with the current static language modelling\nparadigm, which trains and evaluates models on utterances from overlapping time\nperiods. Despite impressive recent progress, we demonstrate that Transformer-XL\nlanguage models perform worse in the realistic setup of predicting future\nutterances from beyond their training period, and that model performance\nbecomes increasingly worse with time. We find that, while increasing model size\nalone -- a key driver behind recent progress -- does not solve this problem,\nhaving models that continually update their knowledge with new information can\nindeed mitigate this performance degradation over time. Hence, given the\ncompilation of ever-larger language modelling datasets, combined with the\ngrowing list of language-model-based NLP applications that require up-to-date\nfactual knowledge about the world, we argue that now is the right time to\nrethink the static way in which we currently train and evaluate our language\nmodels, and develop adaptive language models that can remain up-to-date with\nrespect to our ever-changing and non-stationary world. We publicly release our\ndynamic, streaming language modelling benchmarks for WMT and arXiv to\nfacilitate language model evaluation that takes temporal dynamics into account.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lazaridou_A/0/1/0/all/0/1\">Angeliki Lazaridou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhiguna Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gribovskaya_E/0/1/0/all/0/1\">Elena Gribovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_D/0/1/0/all/0/1\">Devang Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liska_A/0/1/0/all/0/1\">Adam Liska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terzi_T/0/1/0/all/0/1\">Tayfun Terzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimenez_M/0/1/0/all/0/1\">Mai Gimenez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dAutume_C/0/1/0/all/0/1\">Cyprien de Masson d&#x27;Autume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocisky_T/0/1/0/all/0/1\">Tomas Kocisky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1\">Kris Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_S/0/1/0/all/0/1\">Susannah Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing. (arXiv:2103.02227v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.02227","description":"<p>Data augmentation has attracted a lot of research attention in the deep\nlearning era for its ability in alleviating data sparseness. The lack of\nlabeled data for unseen evaluation databases is exactly the major challenge for\ncross-domain text-to-SQL parsing. Previous works either require human\nintervention to guarantee the quality of generated data, or fail to handle\ncomplex SQL queries. This paper presents a simple yet effective data\naugmentation framework. First, given a database, we automatically produce a\nlarge number of SQL queries based on an abstract syntax tree grammar. For\nbetter distribution matching, we require that at least 80% of SQL patterns in\nthe training data are covered by generated queries. Second, we propose a\nhierarchical SQL-to-question generation model to obtain high-quality natural\nlanguage questions, which is the major contribution of this work. Finally, we\ndesign a simple sampling strategy that can greatly improve training efficiency\ngiven large amounts of generated data. Experiments on three cross-domain\ndatasets, i.e., WikiSQL and Spider in English, and DuSQL in Chinese, show that\nour proposed data augmentation framework can consistently improve performance\nover strong baselines, and the hierarchical generation component is the key for\nthe improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does the Magic of BERT Apply to Medical Code Assignment? A Quantitative Study. (arXiv:2103.06511v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.06511","description":"<p>Unsupervised pretraining is an integral part of many natural language\nprocessing systems, and transfer learning with language models has achieved\nremarkable results in many downstream tasks. In the clinical application of\nmedical code assignment, diagnosis and procedure codes are inferred from\nlengthy clinical notes such as hospital discharge summaries. However, it is not\nclear if pretrained models are useful for medical code prediction without\nfurther architecture engineering. This paper conducts a comprehensive\nquantitative analysis of various contextualized language models' performance,\npretrained in different domains, for medical code assignment from clinical\nnotes. We propose a hierarchical fine-tuning architecture to capture\ninteractions between distant words and adopt label-wise attention to exploit\nlabel information. Contrary to current trends, we demonstrate that a carefully\ntrained classical CNN outperforms attention-based models on a MIMIC-III subset\nwith frequent codes. Our empirical findings suggest directions for improving\nthe medical code assignment application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtta_M/0/1/0/all/0/1\">Matti H&#xf6;ltt&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1\">Pekka Marttinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. (arXiv:2103.12028v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.12028","description":"<p>With the success of large-scale pre-training and multilingual modeling in\nNatural Language Processing (NLP), recent years have seen a proliferation of\nlarge, web-mined text datasets covering hundreds of languages. We manually\naudit the quality of 205 language-specific corpora released with five major\npublic datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource\ncorpora have systematic issues: At least 15 corpora have no usable text, and a\nsignificant fraction contains less than 50% sentences of acceptable quality. In\naddition, many are mislabeled or use nonstandard/ambiguous language codes. We\ndemonstrate that these issues are easy to detect even for non-proficient\nspeakers, and supplement the human audit with automatic analyses. Finally, we\nrecommend techniques to evaluate and improve multilingual corpora and discuss\npotential risks that come with low-quality data releases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caswell_I/0/1/0/all/0/1\">Isaac Caswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lisa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahab_A/0/1/0/all/0/1\">Ahsan Wahab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esch_D/0/1/0/all/0/1\">Daan van Esch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulzii_Orshikh_N/0/1/0/all/0/1\">Nasanbayar Ulzii-Orshikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapo_A/0/1/0/all/0/1\">Allahsera Tapo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramani_N/0/1/0/all/0/1\">Nishant Subramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikasote_C/0/1/0/all/0/1\">Claytone Sikasote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Setyawan_M/0/1/0/all/0/1\">Monang Setyawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarin_S/0/1/0/all/0/1\">Supheakmungkol Sarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samb_S/0/1/0/all/0/1\">Sokhar Samb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_C/0/1/0/all/0/1\">Clara Rivera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rios_A/0/1/0/all/0/1\">Annette Rios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_I/0/1/0/all/0/1\">Isabel Papadimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1\">Salomey Osei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_P/0/1/0/all/0/1\">Pedro Ortiz Su&#xe1;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orife_I/0/1/0/all/0/1\">Iroro Orife</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogueji_K/0/1/0/all/0/1\">Kelechi Ogueji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubungo_A/0/1/0/all/0/1\">Andre Niyongabo Rubungo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Toan Q. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Mathias M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_A/0/1/0/all/0/1\">Andr&#xe9; M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_N/0/1/0/all/0/1\">Nanda Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mnyakeni_A/0/1/0/all/0/1\">Ayanda Mnyakeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzakhalov_J/0/1/0/all/0/1\">Jamshidbek Mirzakhalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matangira_T/0/1/0/all/0/1\">Tapiwanashe Matangira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Colin Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawson_N/0/1/0/all/0/1\">Nze Lawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kudugunta_S/0/1/0/all/0/1\">Sneha Kudugunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenny_M/0/1/0/all/0/1\">Mathias Jenny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dlamini_S/0/1/0/all/0/1\">Sakhile Dlamini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balli_S/0/1/0/all/0/1\">Sakine &#xc7;abuk Ball&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battisti_A/0/1/0/all/0/1\">Alessia Battisti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baruwa_A/0/1/0/all/0/1\">Ahmed Baruwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baljekar_P/0/1/0/all/0/1\">Pallavi Baljekar</a>, et al. (7 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fabula Entropy Indexing: Objective Measures of Story Coherence. (arXiv:2104.07472v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07472","description":"<p>Automated story generation remains a difficult area of research because it\nlacks strong objective measures. Generated stories may be linguistically sound,\nbut in many cases suffer poor narrative coherence required for a compelling,\nlogically-sound story. To address this, we present Fabula Entropy Indexing\n(FEI), an evaluation method to assess story coherence by measuring the degree\nto which human participants agree with each other when answering true/false\nquestions about stories. We devise two theoretically grounded measures of\nreader question-answering entropy, the entropy of world coherence (EWC), and\nthe entropy of transitional coherence (ETC), focusing on global and local\ncoherence, respectively. We evaluate these metrics by testing them on\nhuman-written stories and comparing against the same stories that have been\ncorrupted to introduce incoherencies. We show that in these controlled studies,\nour entropy indices provide a reliable objective measure of story coherence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1\">Louis Castricato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_S/0/1/0/all/0/1\">Spencer Frazier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balloch_J/0/1/0/all/0/1\">Jonathan Balloch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Customized determination of stop words using Random Matrix Theory approach. (arXiv:2104.08642v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08642","description":"<p>The distances between words calculated in word units are studied and compared\nwith the distributions of the Random Matrix Theory (RMT). It is found that the\ndistribution of distance between the same words can be well described by the\nsingle-parameter Brody distribution. Using the Brody distribution fit, we found\nthat the distance between given words in a set of texts can show mixed\ndynamics, coexisting regular and chaotic regimes. It is found that\ndistributions correctly fitted by the Brody distribution with a certain\ngoodness of the fit threshold can be identifid as stop words, usually\nconsidered as the uninformative part of the text. By applying various threshold\nvalues for the goodness of fit, we can extract uninformative words from the\ntexts under analysis to the desired extent. On this basis we formulate a fully\nagnostic recipe that can be used in the creation of a customized set of stop\nwords for texts in any language based on words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lobodzinski_B/0/1/0/all/0/1\">Bogdan &#x141;obodzi&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Redundancy in Clinical Text. (arXiv:2105.11832v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.11832","description":"<p>The current mode of use of Electronic Health Record (EHR) elicits text\nredundancy. Clinicians often populate new documents by duplicating existing\nnotes, then updating accordingly. Data duplication can lead to a propagation of\nerrors, inconsistencies and misreporting of care. Therefore, quantifying\ninformation redundancy can play an essential role in evaluating innovations\nthat operate on clinical narratives.\n</p>\n<p>This work is a quantitative examination of information redundancy in EHR\nnotes. We present and evaluate two strategies to measure redundancy: an\ninformation-theoretic approach and a lexicosyntactic and semantic model. We\nevaluate the measures by training large Transformer-based language models using\nclinical text from a large openly available US-based ICU dataset and a large\nmulti-site UK based Trust. By comparing the information-theoretic content of\nthe trained models with open-domain language models, the language models\ntrained using clinical text have shown ~1.5x to ~3x less efficient than\nopen-domain corpora. Manual evaluation shows a high correlation with\nlexicosyntactic and semantic redundancy, with averages ~43 to ~65%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Searle_T/0/1/0/all/0/1\">Thomas Searle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1\">Zina Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_J/0/1/0/all/0/1\">James Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1\">Richard JB Dobson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction. (arXiv:2106.01609v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01609","description":"<p>We investigate the problem of Chinese Grammatical Error Correction (CGEC) and\npresent a new framework named Tail-to-Tail (\\textbf{TtT}) non-autoregressive\nsequence prediction to address the deep issues hidden in CGEC. Considering that\nmost tokens are correct and can be conveyed directly from source to target, and\nthe error positions can be estimated and corrected based on the bidirectional\ncontext information, thus we employ a BERT-initialized Transformer Encoder as\nthe backbone model to conduct information modeling and conveying. Considering\nthat only relying on the same position substitution cannot handle the\nvariable-length correction cases, various operations such substitution,\ndeletion, insertion, and local paraphrasing are required jointly. Therefore, a\nConditional Random Fields (CRF) layer is stacked on the up tail to conduct\nnon-autoregressive sequence prediction by modeling the token dependencies.\nSince most tokens are correct and easily to be predicted/conveyed to the\ntarget, then the models may suffer from a severe class imbalance issue. To\nalleviate this problem, focal loss penalty strategies are integrated into the\nloss functions. Moreover, besides the typical fix-length error correction\ndatasets, we also construct a variable-length corpus to conduct experiments.\nExperimental results on standard datasets, especially on the variable-length\ndatasets, demonstrate the effectiveness of TtT in terms of sentence-level\nAccuracy, Precision, Recall, and F1-Measure on tasks of error Detection and\nCorrection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Maximum Likelihood Estimation for Training Deep Networks. (arXiv:2106.03831v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.03831","description":"<p>Although deep learning models have driven state-of-the-art performance on a\nwide array of tasks, they are prone to spurious correlations that should not be\nlearned as predictive clues. To mitigate this problem, we propose a\ncausality-based training framework to reduce the spurious correlations caused\nby observed confounders. We give theoretical analysis on the underlying general\nStructural Causal Model (SCM) and propose to perform Maximum Likelihood\nEstimation (MLE) on the interventional distribution instead of the\nobservational distribution, namely Counterfactual Maximum Likelihood Estimation\n(CMLE). As the interventional distribution, in general, is hidden from the\nobservational data, we then derive two different upper bounds of the expected\nnegative log-likelihood and propose two general algorithms, Implicit CMLE and\nExplicit CMLE, for causal predictions of deep learning models using\nobservational data. We conduct experiments on both simulated data and two\nreal-world tasks: Natural Language Inference (NLI) and Image Captioning. The\nresults show that CMLE methods outperform the regular MLE method in terms of\nout-of-domain generalization performance and reducing spurious correlations,\nwhile maintaining comparable performance on the regular evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition. (arXiv:2106.05933v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.05933","description":"<p>Self-supervised speech representation learning (speech SSL) has demonstrated\nthe benefit of scale in learning rich representations for Automatic Speech\nRecognition (ASR) with limited paired data, such as wav2vec 2.0. We investigate\nthe existence of sparse subnetworks in pre-trained speech SSL models that\nachieve even better low-resource ASR results. However, directly applying widely\nadopted pruning methods such as the Lottery Ticket Hypothesis (LTH) is\nsuboptimal in the computational cost needed. Moreover, we show that the\ndiscovered subnetworks yield minimal performance gain compared to the original\ndense network. We present Prune-Adjust-Re-Prune (PARP), which discovers and\nfinetunes subnetworks for much better performance, while only requiring a\nsingle downstream ASR finetuning run. PARP is inspired by our surprising\nobservation that subnetworks pruned for pre-training tasks need merely a slight\nadjustment to achieve a sizeable performance boost in downstream ASR tasks.\nExtensive experiments on low-resource ASR verify (1) sparse subnetworks exist\nin mono-lingual/multi-lingual pre-trained speech SSL, and (2) the computational\nadvantage and performance gain of PARP over baseline pruning methods. In\nparticular, on the 10min Librispeech split without LM decoding, PARP discovers\nsubnetworks from wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared\nto the full model. We further demonstrate the effectiveness of PARP via:\ncross-lingual pruning without any phone recognition degradation, the discovery\nof a multi-lingual subnetwork for 10 spoken languages in 1 finetuning run, and\nits applicability to pre-trained BERT/XLNet for natural language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Jeff Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alexander H. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yi-Lun Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kaizhi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1\">Sameer Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1\">David Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling morphology with Linear Discriminative Learning: considerations and design choices. (arXiv:2106.07936v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07936","description":"<p>This study addresses a series of methodological questions that arise when\nmodeling inflectional morphology with Linear Discriminative Learning. Taking\nthe semi-productive German noun system as example, we illustrate how decisions\nmade about the representation of form and meaning influence model performance.\nWe clarify that for modeling frequency effects in learning, it is essential to\nmake use of incremental learning rather than the endstate of learning. We also\ndiscuss how the model can be set up to approximate the learning of inflected\nwords in context. In addition, we illustrate how in this approach the wug task\ncan be modeled in considerable detail. In general, the model provides an\nexcellent memory for known words, but appropriately shows more limited\nperformance for unseen data, in line with the semi-productivity of German noun\ninflection and generalization performance of native German speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heitmeier_M/0/1/0/all/0/1\">Maria Heitmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yu-Ying Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baayen_R/0/1/0/all/0/1\">R. Harald Baayen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021. (arXiv:2106.13033v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13033","description":"<p>In this paper, inspired by the successes of visionlanguage pre-trained models\nand the benefits from training with adversarial attacks, we present a novel\ntransformerbased cross-modal fusion modeling by incorporating the both notions\nfor VQA challenge 2021. Specifically, the proposed model is on top of the\narchitecture of VinVL model [19], and the adversarial training strategy [4] is\napplied to make the model robust and generalized. Moreover, two implementation\ntricks are also used in our system to obtain better results. The experiments\ndemonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Ke-Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Bo-Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuan-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Extrapolation for Attribute-Enhanced Generation. (arXiv:2107.02968v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.02968","description":"<p>Attribute extrapolation in sample generation is challenging for deep neural\nnetworks operating beyond the training distribution. We formulate a new task\nfor extrapolation in sequence generation, focusing on natural language and\nproteins, and propose GENhance, a generative framework that enhances attributes\nthrough a learned latent space. Trained on movie reviews and a computed protein\nstability dataset, GENhance can generate strongly-positive text reviews and\nhighly stable protein sequences without being exposed to similar data during\ntraining. We release our benchmark tasks and models to contribute to the study\nof generative modeling extrapolation and data-driven design in biology and\nchemistry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Alvin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1\">Ali Madani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_B/0/1/0/all/0/1\">Ben Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1\">Nikhil Naik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EviDR: Evidence-Emphasized Discrete Reasoning for Reasoning Machine Reading Comprehension. (arXiv:2108.07994v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07994","description":"<p>Reasoning machine reading comprehension (R-MRC) aims to answer complex\nquestions that require discrete reasoning based on text. To support discrete\nreasoning, evidence, typically the concise textual fragments that describe\nquestion-related facts, including topic entities and attribute values, are\ncrucial clues from question to answer. However, previous end-to-end methods\nthat achieve state-of-the-art performance rarely solve the problem by paying\nenough emphasis on the modeling of evidence, missing the opportunity to further\nimprove the model's reasoning ability for R-MRC. To alleviate the above issue,\nin this paper, we propose an evidence-emphasized discrete reasoning approach\n(EviDR), in which sentence and clause level evidence is first detected based on\ndistant supervision, and then used to drive a reasoning module implemented with\na relational heterogeneous graph convolutional network to derive answers.\nExtensive experiments are conducted on DROP (discrete reasoning over\nparagraphs) dataset, and the results demonstrate the effectiveness of our\nproposed approach. In addition, qualitative analysis verifies the capability of\nthe proposed evidence-emphasized discrete reasoning for R-MRC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiahui Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond NED: Fast and Effective Search Space Reduction for Complex Question Answering over Knowledge Bases. (arXiv:2108.08597v5 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08597","description":"<p>Answering complex questions over knowledge bases (KB-QA) faces huge input\ndata with billions of facts, involving millions of entities and thousands of\npredicates. For efficiency, QA systems first reduce the answer search space by\nidentifying a set of facts that is likely to contain all answers and relevant\ncues. The most common technique or doing this is to apply named entity\ndisambiguation (NED) systems to the question, and retrieve KB facts for the\ndisambiguated entities. This work presents CLOCQ, an efficient method that\nprunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses\na top-k query processor over score-ordered lists of KB items that combine\nsignals about lexical matching, relevance to the question, coherence among\ncandidate items, and connectivity in the KB graph. Experiments with two recent\nQA benchmarks for complex questions demonstrate the superiority of CLOCQ over\nstate-of-the-art baselines with respect to answer presence, size of the search\nspace, and runtimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1\">Philipp Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness and Sensitivity of BERT Models Predicting Alzheimer's Disease from Text. (arXiv:2109.11888v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11888","description":"<p>Understanding robustness and sensitivity of BERT models predicting\nAlzheimer's disease from text is important for both developing better\nclassification models and for understanding their capabilities and limitations.\nIn this paper, we analyze how a controlled amount of desired and undesired text\nalterations impacts performance of BERT. We show that BERT is robust to natural\nlinguistic variations in text. On the other hand, we show that BERT is not\nsensitive to removing clinically important information from text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novikova_J/0/1/0/all/0/1\">Jekaterina Novikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cut the CARP: Fishing for zero-shot story evaluation. (arXiv:2110.03111v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03111","description":"<p>Recent advances in large-scale language models (Raffel et al., 2019; Brown et\nal., 2020) have brought significant qualitative and quantitative improvements\nin machine-driven text generation. Despite this, generation and evaluation of\nmachine-generated narrative text remains a challenging problem. Objective\nevaluation of computationally-generated stories may be prohibitively expensive,\nrequire meticulously annotated datasets, or may not adequately measure the\nlogical coherence of a generated story's narratological structure.\n</p>\n<p>Informed by recent advances in contrastive learning (Radford et al., 2021),\nwe present Contrastive Authoring and Reviewing Pairing (CARP): a scalable,\nefficient method for performing qualitatively superior, zero-shot evaluation of\nstories. We show a strong correlation between human evaluation of stories and\nthose of CARP. Model outputs more significantly correlate with corresponding\nhuman input than those language-model based methods which utilize finetuning or\nprompt engineering approaches. We also present and analyze the Story-Critique\nDataset, a new corpora composed of 1.3 million aligned story-critique pairs\nderived from over 80,000 stories. We expect this corpus to be of interest to\nNLP researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matiana_S/0/1/0/all/0/1\">Shahbuland Matiana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">JR Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teehan_R/0/1/0/all/0/1\">Ryan Teehan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1\">Louis Castricato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_S/0/1/0/all/0/1\">Spencer Frazier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transliteration of Foreign Words in Burmese. (arXiv:2110.03163v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03163","description":"<p>This manuscript provides general descriptions on transliteration of foreign\nwords in the Burmese language. Phenomena caused by phonetic and orthographic\nissues are discussed. Based on this work, we expect to gradually establish\nprescriptive guidelines to normalize the transliteration on modern words in\nBurmese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Chenchen Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Continual Knowledge Learning of Language Models. (arXiv:2110.03215v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03215","description":"<p>Large Language Models (LMs) are known to encode world knowledge in their\nparameters as they pretrain on a vast amount of web corpus, which is often\nutilized for performing knowledge-dependent downstream tasks such as question\nanswering, fact-checking, and open dialogue. In real-world scenarios, the world\nknowledge stored in the LMs can quickly become outdated as the world changes,\nbut it is non-trivial to avoid catastrophic forgetting and reliably acquire new\nknowledge while preserving invariant knowledge. To push the community towards\nbetter maintenance of ever-changing LMs, we formulate a new continual learning\n(CL) problem called Continual Knowledge Learning (CKL). We construct a new\nbenchmark and metric to quantify the retention of time-invariant world\nknowledge, the update of outdated knowledge, and the acquisition of new\nknowledge. We adopt applicable recent methods from literature to create several\nstrong baselines. Through extensive experiments, we find that CKL exhibits\nunique challenges that are not addressed in previous CL setups, where parameter\nexpansion is necessary to reliably retain and learn knowledge simultaneously.\nBy highlighting the critical causes of knowledge forgetting, we show that CKL\nis a challenging and important problem that helps us better understand and\ntrain ever-changing LMs. The benchmark datasets, evaluation script, and\nbaseline code to reproduce our results are available at\nhttps://github.com/joeljang/continual-knowledge-learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Joongbo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Janghoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeonghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Stanley Jungkyu Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VarArray: Array-Geometry-Agnostic Continuous Speech Separation. (arXiv:2110.05745v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.05745","description":"<p>Continuous speech separation using a microphone array was shown to be\npromising in dealing with the speech overlap problem in natural conversation\ntranscription. This paper proposes VarArray, an array-geometry-agnostic speech\nseparation neural network model. The proposed model is applicable to any number\nof microphones without retraining while leveraging the nonlinear correlation\nbetween the input channels. The proposed method adapts different elements that\nwere proposed before separately, including transform-average-concatenate,\nconformer speech separation, and inter-channel phase differences, and combines\nthem in an efficient and cohesive way. Large-scale evaluation was performed\nwith two real meeting transcription tasks by using a fully developed\ntranscription system requiring no prior knowledge such as reference\nsegmentations, which allowed us to measure the impact that the continuous\nspeech separation system could have in realistic settings. The proposed model\noutperformed a previous approach to array-geometry-agnostic modeling for all of\nthe geometry configurations considered, achieving asclite-based\nspeaker-agnostic word error rates of 17.5% and 20.4% for the AMI development\nand evaluation sets, respectively, in the end-to-end setting using no\nground-truth segmentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Dongmei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_M/0/1/0/all/0/1\">Min Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1\">Zirun Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Light-Field Microscopy for optical imaging of neuronal activity: when model-based methods meet data-driven approaches. (arXiv:2110.13142v1 [eess.IV])","link":"http://arxiv.org/abs/2110.13142","description":"<p>Understanding how networks of neurons process information is one of the key\nchallenges in modern neuroscience. A necessary step to achieve this goal is to\nbe able to observe the dynamics of large populations of neurons over a large\narea of the brain. Light-field microscopy (LFM), a type of scanless microscope,\nis a particularly attractive candidate for high-speed three-dimensional (3D)\nimaging. It captures volumetric information in a single snapshot, allowing\nvolumetric imaging at video frame-rates. Specific features of imaging neuronal\nactivity using LFM call for the development of novel machine learning\napproaches that fully exploit priors embedded in physics and optics models.\nSignal processing theory and wave-optics theory could play a key role in\nfilling this gap, and contribute to novel computational methods with enhanced\ninterpretability and generalization by integrating model-driven and data-driven\napproaches. This paper is devoted to a comprehensive survey to state-of-the-art\nof computational methods for LFM, with a focus on model-based and data-driven\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Song_P/0/1/0/all/0/1\">Pingfan Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jadan_H/0/1/0/all/0/1\">Herman Verinaz Jadan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Howe_C/0/1/0/all/0/1\">Carmel L. Howe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Foust_A/0/1/0/all/0/1\">Amanda J. Foust</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dragotti_P/0/1/0/all/0/1\">Pier Luigi Dragotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Embedded System for Image-based Crack Detection by using Fine-Tuning model of Adaptive Structural Learning of Deep Belief Network. (arXiv:2110.13145v1 [cs.NE])","link":"http://arxiv.org/abs/2110.13145","description":"<p>Deep learning has been a successful model which can effectively represent\nseveral features of input space and remarkably improve image recognition\nperformance on the deep architectures. In our research, an adaptive structural\nlearning method of Restricted Boltzmann Machine (Adaptive RBM) and Deep Belief\nNetwork (Adaptive DBN) have been developed as a deep learning model. The models\nhave a self-organize function which can discover an optimal number of hidden\nneurons for given input data in a RBM by neuron generation-annihilation\nalgorithm, and can obtain an appropriate number of RBM as hidden layers in the\ntrained DBN. The proposed method was applied to a concrete image benchmark data\nset SDNET 2018 for crack detection. The dataset contains about 56,000 crack\nimages for three types of concrete structures: bridge decks, walls, and paved\nroads. The fine-tuning method of the Adaptive DBN can show 99.7%, 99.7%, and\n99.4% classification accuracy for test dataset of three types of structures. In\nthis paper, our developed Adaptive DBN was embedded to a tiny PC with GPU for\nreal-time inference on a drone. For fast inference, the fine tuning algorithm\nalso removed some inactivated hidden neurons to make a small model and then the\nmodel was able to improve not only classification accuracy but also inference\nspeed simultaneously. The inference speed and running time of portable battery\ncharger were evaluated on three kinds of Nvidia embedded systems; Jetson Nano,\nAGX Xavier, and Xavier NX.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamada_S/0/1/0/all/0/1\">Shin Kamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1\">Takumi Ichimura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"As if by magic: self-supervised training of deep despeckling networks with MERLIN. (arXiv:2110.13148v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13148","description":"<p>Speckle fluctuations seriously limit the interpretability of synthetic\naperture radar (SAR) images. Speckle reduction has thus been the subject of\nnumerous works spanning at least four decades. Techniques based on deep neural\nnetworks have recently achieved a new level of performance in terms of SAR\nimage restoration quality. Beyond the design of suitable network architectures\nor the selection of adequate loss functions, the construction of training sets\nis of uttermost importance. So far, most approaches have considered a\nsupervised training strategy: the networks are trained to produce outputs as\nclose as possible to speckle-free reference images. Speckle-free images are\ngenerally not available, which requires resorting to natural or optical images\nor the selection of stable areas in long time series to circumvent the lack of\nground truth. Self-supervision, on the other hand, avoids the use of\nspeckle-free images. We introduce a self-supervised strategy based on the\nseparation of the real and imaginary parts of single-look complex SAR images,\ncalled MERLIN (coMplex sElf-supeRvised despeckLINg), and show that it offers a\nstraightforward way to train all kinds of deep despeckling networks. Networks\ntrained with MERLIN take into account the spatial correlations due to the SAR\ntransfer function specific to a given sensor and imaging mode. By requiring\nonly a single image, and possibly exploiting large archives, MERLIN opens the\ndoor to hassle-free as well as large-scale training of despeckling networks.\nThe code of the trained models is made freely available at\nhttps://gitlab.telecom-paris.fr/RING/MERLIN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dalsasso_E/0/1/0/all/0/1\">Emanuele Dalsasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denis_L/0/1/0/all/0/1\">Lo&#xef;c Denis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tupin_F/0/1/0/all/0/1\">Florence Tupin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised similarity search for large scientific datasets. (arXiv:2110.13151v1 [astro-ph.IM])","link":"http://arxiv.org/abs/2110.13151","description":"<p>We present the use of self-supervised learning to explore and exploit large\nunlabeled datasets. Focusing on 42 million galaxy images from the latest data\nrelease of the Dark Energy Spectroscopic Instrument (DESI) Legacy Imaging\nSurveys, we first train a self-supervised model to distil low-dimensional\nrepresentations that are robust to symmetries, uncertainties, and noise in each\nimage. We then use the representations to construct and publicly release an\ninteractive semantic similarity search tool. We demonstrate how our tool can be\nused to rapidly discover rare objects given only a single example, increase the\nspeed of crowd-sourcing campaigns, and construct and improve training sets for\nsupervised applications. While we focus on images from sky surveys, the\ntechnique is straightforward to apply to any scientific dataset of any\ndimensionality. The similarity search web app can be found at\nhttps://github.com/georgestein/galaxy_search\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Stein_G/0/1/0/all/0/1\">George Stein</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Harrington_P/0/1/0/all/0/1\">Peter Harrington</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Blaum_J/0/1/0/all/0/1\">Jacqueline Blaum</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Medan_T/0/1/0/all/0/1\">Tomislav Medan</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lukic_Z/0/1/0/all/0/1\">Zarija Lukic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Multi-Task Learning from Substantially Unlabeled Multi-Source Medical Image Data. (arXiv:2110.13185v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13185","description":"<p>Deep learning-based models, when trained in a fully-supervised manner, can be\neffective in performing complex image analysis tasks, although contingent upon\nthe availability of large labeled datasets. Especially in the medical imaging\ndomain, however, expert image annotation is expensive, time-consuming, and\nprone to variability. Semi-supervised learning from limited quantities of\nlabeled data has shown promise as an alternative. Maximizing knowledge gains\nfrom copious unlabeled data benefits semi-supervised learning models. Moreover,\nlearning multiple tasks within the same model further improves its\ngeneralizability. We propose MultiMix, a new multi-task learning model that\njointly learns disease classification and anatomical segmentation in a\nsemi-supervised manner, while preserving explainability through a novel\nsaliency bridge between the two tasks. Our experiments with varying quantities\nof multi-source labeled data in the training sets confirm the effectiveness of\nMultiMix in the simultaneous classification of pneumonia and segmentation of\nthe lungs in chest X-ray images. Moreover, both in-domain and cross-domain\nevaluations across these tasks further showcase the potential of our model to\nadapt to challenging generalization scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_A/0/1/0/all/0/1\">Abdullah-Al-Zubaer Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Adam Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terzopoulos_D/0/1/0/all/0/1\">Demetri Terzopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Meta-Learning Modification with Stochastic Approximation. (arXiv:2110.13188v1 [cs.LG])","link":"http://arxiv.org/abs/2110.13188","description":"<p>Meta-learning methods aim to build learning algorithms capable of quickly\nadapting to new tasks in low-data regime. One of the main benchmarks of such an\nalgorithms is a few-shot learning problem. In this paper we investigate the\nmodification of standard meta-learning pipeline that takes a multi-task\napproach during training. The proposed method simultaneously utilizes\ninformation from several meta-training tasks in a common loss function. The\nimpact of each of these tasks in the loss function is controlled by the\ncorresponding weight. Proper optimization of these weights can have a big\ninfluence on training of the entire model and might improve the quality on test\ntime tasks. In this work we propose and investigate the use of methods from the\nfamily of simultaneous perturbation stochastic approximation (SPSA) approaches\nfor meta-train tasks weights optimization. We have also compared the proposed\nalgorithms with gradient-based methods and found that stochastic approximation\ndemonstrates the largest quality boost in test time. Proposed multi-task\nmodification can be applied to almost all methods that use meta-learning\npipeline. In this paper we study applications of this modification on\nPrototypical Networks and Model-Agnostic Meta-Learning algorithms on CIFAR-FS,\nFC100, tieredImageNet and miniImageNet few-shot learning benchmarks. During\nthese experiments, multi-task modification has demonstrated improvement over\noriginal methods. The proposed SPSA-Tracking algorithm shows the largest\naccuracy boost. Our code is available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boiarov_A/0/1/0/all/0/1\">Andrei Boiarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabarlak_K/0/1/0/all/0/1\">Konstantin Khabarlak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yastrebov_I/0/1/0/all/0/1\">Igor Yastrebov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral unmixing of Raman microscopic images of single human cells using Independent Component Analysis. (arXiv:2110.13189v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13189","description":"<p>Application of independent component analysis (ICA) as an unmixing and image\nclustering technique for high spatial resolution Raman maps is reported. A\nhyperspectral map of a fixed human cell was collected by a Raman micro\nspectrometer in a raster pattern on a 0.5um grid. Unlike previously used\nunsupervised machine learning techniques such as principal component analysis,\nICA is based on non-Gaussianity and statistical independence of data which is\nthe case for mixture Raman spectra. Hence, ICA is a great candidate for\nassembling pseudo-colour maps from the spectral hypercube of Raman spectra. Our\nexperimental results revealed that ICA is capable of reconstructing false\ncolour maps of Raman hyperspectral data of human cells, showing the nuclear\nregion constituents as well as subcellular organelle in the cytoplasm and\ndistribution of mitochondria in the perinuclear region. Minimum preprocessing\nrequirements and label-free nature of the ICA method make it a great unmixed\nmethod for extraction of endmembers in Raman hyperspectral maps of living\ncells.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mozaffari_M/0/1/0/all/0/1\">M. Hamed Mozaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_L/0/1/0/all/0/1\">Li-Lin Tay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. (arXiv:2110.13214v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13214","description":"<p>Current visual question answering (VQA) tasks mainly consider answering\nhuman-annotated questions for natural images. However, aside from natural\nimages, abstract diagrams with semantic richness are still understudied in\nvisual understanding and reasoning research. In this work, we introduce a new\nchallenge of Icon Question Answering (IconQA) with the goal of answering a\nquestion in an icon image context. We release IconQA, a large-scale dataset\nthat consists of 107,439 questions and three sub-tasks: multi-image-choice,\nmulti-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by\nreal-world diagram word problems that highlight the importance of abstract\ndiagram understanding and comprehensive cognitive reasoning. Thus, IconQA\nrequires not only perception skills like object recognition and text\nunderstanding, but also diverse cognitive reasoning skills, such as geometric\nreasoning, commonsense reasoning, and arithmetic reasoning. To facilitate\npotential IconQA models to learn semantic representations for icon images, we\nfurther release an icon dataset Icon645 which contains 645,687 colored icons on\n377 classes. We conduct extensive user studies and blind experiments and\nreproduce a wide range of advanced VQA methods to benchmark the IconQA task.\nAlso, we develop a strong IconQA baseline Patch-TRM that applies a pyramid\ncross-modal Transformer with input diagram embeddings pre-trained on the icon\ndataset. IconQA and Icon645 are available at https://iconqa.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tony Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RBSRICNN: Raw Burst Super-Resolution through Iterative Convolutional Neural Network. (arXiv:2110.13217v1 [eess.IV])","link":"http://arxiv.org/abs/2110.13217","description":"<p>Modern digital cameras and smartphones mostly rely on image signal processing\n(ISP) pipelines to produce realistic colored RGB images. However, compared to\nDSLR cameras, low-quality images are usually obtained in many portable mobile\ndevices with compact camera sensors due to their physical limitations. The\nlow-quality images have multiple degradations i.e., sub-pixel shift due to\ncamera motion, mosaick patterns due to camera color filter array,\nlow-resolution due to smaller camera sensors, and the rest information are\ncorrupted by the noise. Such degradations limit the performance of current\nSingle Image Super-resolution (SISR) methods in recovering high-resolution (HR)\nimage details from a single low-resolution (LR) image. In this work, we propose\na Raw Burst Super-Resolution Iterative Convolutional Neural Network (RBSRICNN)\nthat follows the burst photography pipeline as a whole by a forward (physical)\nmodel. The proposed Burst SR scheme solves the problem with classical image\nregularization, convex optimization, and deep learning techniques, compared to\nexisting black-box data-driven methods. The proposed network produces the final\noutput by an iterative refinement of the intermediate SR estimates. We\ndemonstrate the effectiveness of our proposed approach in quantitative and\nqualitative experiments that generalize robustly to real LR burst inputs with\nonl synthetic burst data available for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Umer_R/0/1/0/all/0/1\">Rao Muhammad Umer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Micheloni_C/0/1/0/all/0/1\">Christian Micheloni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying and Benchmarking Natural Out-of-Context Prediction Problems. (arXiv:2110.13223v1 [cs.LG])","link":"http://arxiv.org/abs/2110.13223","description":"<p>Deep learning systems frequently fail at out-of-context (OOC) prediction, the\nproblem of making reliable predictions on uncommon or unusual inputs or\nsubgroups of the training distribution. To this end, a number of benchmarks for\nmeasuring OOC performance have recently been introduced. In this work, we\nintroduce a framework unifying the literature on OOC performance measurement,\nand demonstrate how rich auxiliary information can be leveraged to identify\ncandidate sets of OOC examples in existing datasets. We present NOOCh: a suite\nof naturally-occurring \"challenge sets\", and show how varying notions of\ncontext can be used to probe specific OOC failure modes. Experimentally, we\nexplore the tradeoffs between various learning approaches on these challenge\nsets and demonstrate how the choices made in designing OOC benchmarks can yield\nvarying conclusions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madras_D/0/1/0/all/0/1\">David Madras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pediatric Otoscopy Video Screening with Shift Contrastive Anomaly Detection. (arXiv:2110.13254v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13254","description":"<p>Ear related concerns and symptoms represents the leading indication for\nseeking pediatric healthcare attention. Despite the high incidence of such\nencounters, the diagnostic process of commonly encountered disease of the\nmiddle and external presents significant challenge. Much of this challenge\nstems from the lack of cost effective diagnostic testing, which necessitating\nthe presence or absence of ear pathology to be determined clinically. Research\nhas however demonstrated considerable variation among clinicians in their\nability to accurately diagnose and consequently manage ear pathology. With\nrecent advances in computer vision and machine learning, there is an increasing\ninterest in helping clinicians to accurately diagnose middle and external ear\npathology with computer-aided systems. It has been shown that AI has the\ncapacity to analyse a single clinical image captured during examination of the\near canal and eardrum from which it can determine the likelihood of a\npathognomonic pattern for a specific diagnosis being present. The capture of\nsuch an image can however be challenging especially to inexperienced\nclinicians. To help mitigate this technical challenge we have developed and\ntested a method using video sequences. We present a two stage method that\nfirst, identifies valid frames by detecting and extracting ear drum patches\nfrom the video sequence, and second, performs the proposed shift contrastive\nanomaly detection to flag the otoscopy video sequences as normal or abnormal.\nOur method achieves an AUROC of 88.0% on the patient-level and also outperforms\nthe average of a group of 25 clinicians in a comparative study, which is the\nlargest of such published to date. We conclude that the presented method\nachieves a promising first step towards automated analysis of otoscopy video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamhane_A/0/1/0/all/0/1\">Aniruddha Tamhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_C/0/1/0/all/0/1\">Christine Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rzasa_J/0/1/0/all/0/1\">John R. Rzasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">James H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canares_T/0/1/0/all/0/1\">Therese L. Canares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Deep Visual Tracking. (arXiv:2110.13259v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13259","description":"<p>Convolutional neural networks (CNNs) have been successfully applied to the\nsingle target tracking task in recent years. Generally, training a deep CNN\nmodel requires numerous labeled training samples, and the number and quality of\nthese samples directly affect the representational capability of the trained\nmodel. However, this approach is restrictive in practice, because manually\nlabeling such a large number of training samples is time-consuming and\nprohibitively expensive. In this paper, we propose an active learning method\nfor deep visual tracking, which selects and annotates the unlabeled samples to\ntrain the deep CNNs model. Under the guidance of active learning, the tracker\nbased on the trained deep CNNs model can achieve competitive tracking\nperformance while reducing the labeling cost. More specifically, to ensure the\ndiversity of selected samples, we propose an active learning method based on\nmulti-frame collaboration to select those training samples that should be and\nneed to be annotated. Meanwhile, considering the representativeness of these\nselected samples, we adopt a nearest neighbor discrimination method based on\nthe average nearest neighbor distance to screen isolated samples and\nlow-quality samples. Therefore, the training samples subset selected based on\nour method requires only a given budget to maintain the diversity and\nrepresentativeness of the entire sample set. Furthermore, we adopt a Tversky\nloss to improve the bounding box estimation of our tracker, which can ensure\nthat the tracker achieves more accurate target states. Extensive experimental\nresults confirm that our active learning-based tracker (ALT) achieves\ncompetitive tracking accuracy and speed compared with state-of-the-art trackers\non the seven most challenging evaluation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_D/0/1/0/all/0/1\">Di Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dehua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyu He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Quality Assessment using Contrastive Learning. (arXiv:2110.13266v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13266","description":"<p>We consider the problem of obtaining image quality representations in a\nself-supervised manner. We use prediction of distortion type and degree as an\nauxiliary task to learn features from an unlabeled image dataset containing a\nmixture of synthetic and realistic distortions. We then train a deep\nConvolutional Neural Network (CNN) using a contrastive pairwise objective to\nsolve the auxiliary problem. We refer to the proposed training framework and\nresulting deep IQA model as the CONTRastive Image QUality Evaluator\n(CONTRIQUE). During evaluation, the CNN weights are frozen and a linear\nregressor maps the learned representations to quality scores in a No-Reference\n(NR) setting. We show through extensive experiments that CONTRIQUE achieves\ncompetitive performance when compared to state-of-the-art NR image quality\nmodels, even without any additional fine-tuning of the CNN backbone. The\nlearned representations are highly robust and generalize well across images\nafflicted by either synthetic or authentic distortions. Our results suggest\nthat powerful quality representations with perceptual relevance can be obtained\nwithout requiring large labeled subjective image quality datasets. The\nimplementations used in this paper are available at\n\\url{https://github.com/pavancm/CONTRIQUE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhusudana_P/0/1/0/all/0/1\">Pavan C. Madhusudana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birkbeck_N/0/1/0/all/0/1\">Neil Birkbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adsumilli_B/0/1/0/all/0/1\">Balu Adsumilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Recognition in Collaborative Learning Videos. (arXiv:2110.13269v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13269","description":"<p>Face recognition in collaborative learning videos presents many challenges.\nIn collaborative learning videos, students sit around a typical table at\ndifferent positions to the recording camera, come and go, move around, get\npartially or fully occluded. Furthermore, the videos tend to be very long,\nrequiring the development of fast and accurate methods. We develop a dynamic\nsystem of recognizing participants in collaborative learning systems. We\naddress occlusion and recognition failures by using past information about the\nface detection history. We address the need for detecting faces from different\nposes and the need for speed by associating each participant with a collection\nof prototype faces computed through sampling or K-means clustering. Our results\nshow that the proposed system is proven to be very fast and accurate. We also\ncompare our system against a baseline system that uses InsightFace [2] and the\noriginal training video segments. We achieved an average accuracy of 86.2%\ncompared to 70.8% for the baseline system. On average, our recognition rate was\n28.1 times faster than the baseline system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_P/0/1/0/all/0/1\">Phuong Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattichis_M/0/1/0/all/0/1\">Marios Pattichis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celedon_Pattichis_S/0/1/0/all/0/1\">Sylvia Celed&#xf3;n-Pattichis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LopezLeiva_C/0/1/0/all/0/1\">Carlos L&#xf3;pezLeiva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Neural Transmittance for Efficient Rendering of Reflectance Fields. (arXiv:2110.13272v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13272","description":"<p>Recently neural volumetric representations such as neural reflectance fields\nhave been widely applied to faithfully reproduce the appearance of real-world\nobjects and scenes under novel viewpoints and lighting conditions. However, it\nremains challenging and time-consuming to render such representations under\ncomplex lighting such as environment maps, which requires individual ray\nmarching towards each single light to calculate the transmittance at every\nsampled point. In this paper, we propose a novel method based on precomputed\nNeural Transmittance Functions to accelerate the rendering of neural\nreflectance fields. Our neural transmittance functions enable us to efficiently\nquery the transmittance at an arbitrary point in space along an arbitrary ray\nwithout tedious ray marching, which effectively reduces the time-complexity of\nthe rendering. We propose a novel formulation for the neural transmittance\nfunction, and train it jointly with the neural reflectance fields on images\ncaptured under collocated camera and light, while enforcing monotonicity.\nResults on real and synthetic scenes demonstrate almost two order of magnitude\nspeedup for renderings under environment maps with minimal accuracy loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shafiei_M/0/1/0/all/0/1\">Mohammad Shafiei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sai Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liaudanskas_A/0/1/0/all/0/1\">Aidas Liaudanskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_Cayon_R/0/1/0/all/0/1\">Rodrigo Ortiz-Cayon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1\">Ravi Ramamoorthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Variational Graph Autoencoder for Manipulation Action Recognition and Prediction. (arXiv:2110.13280v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13280","description":"<p>Despite decades of research, understanding human manipulation activities is,\nand has always been, one of the most attractive and challenging research topics\nin computer vision and robotics. Recognition and prediction of observed human\nmanipulation actions have their roots in the applications related to, for\ninstance, human-robot interaction and robot learning from demonstration. The\ncurrent research trend heavily relies on advanced convolutional neural networks\nto process the structured Euclidean data, such as RGB camera images. These\nnetworks, however, come with immense computational complexity to be able to\nprocess high dimensional raw data.\n</p>\n<p>Different from the related works, we here introduce a deep graph autoencoder\nto jointly learn recognition and prediction of manipulation tasks from symbolic\nscene graphs, instead of relying on the structured Euclidean data. Our network\nhas a variational autoencoder structure with two branches: one for identifying\nthe input graph type and one for predicting the future graphs. The input of the\nproposed network is a set of semantic graphs which store the spatial relations\nbetween subjects and objects in the scene. The network output is a label set\nrepresenting the detected and predicted class types. We benchmark our new model\nagainst different state-of-the-art methods on two different datasets, MANIAC\nand MSRC-9, and show that our proposed model can achieve better performance. We\nalso release our source code https://github.com/gamzeakyol/GNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyol_G/0/1/0/all/0/1\">Gamze Akyol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sariel_S/0/1/0/all/0/1\">Sanem Sariel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksoy_E/0/1/0/all/0/1\">Eren Erdal Aksoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Flows as a General Purpose Solution for Inverse Problems. (arXiv:2110.13285v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13285","description":"<p>Due to the success of generative flows to model data distributions, they have\nbeen explored in inverse problems. Given a pre-trained generative flow,\nprevious work proposed to minimize the 2-norm of the latent variables as a\nregularization term in the main objective. The intuition behind it was to\nensure high likelihood latent variables, however this does not ensure the\ngeneration of realistic samples as we show in our experiments. We therefore\npropose a regularization term to directly produce high likelihood\nreconstructions. Our hypothesis is that our method could make generative flows\na general-purpose solver for inverse problems. We evaluate our method in image\ndenoising, image deblurring, image inpainting, and image colorization. We\nobserve a compelling improvement of our method over prior works in the PSNR and\nSSIM metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavez_J/0/1/0/all/0/1\">Jos&#xe9; A. Ch&#xe1;vez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty quantification in non-rigid image registration via stochastic gradient Markov chain Monte Carlo. (arXiv:2110.13289v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13289","description":"<p>We develop a new Bayesian model for non-rigid registration of\nthree-dimensional medical images, with a focus on uncertainty quantification.\nProbabilistic registration of large images with calibrated uncertainty\nestimates is difficult for both computational and modelling reasons. To address\nthe computational issues, we explore connections between the Markov chain Monte\nCarlo by backpropagation and the variational inference by backpropagation\nframeworks, in order to efficiently draw samples from the posterior\ndistribution of transformation parameters. To address the modelling issues, we\nformulate a Bayesian model for image registration that overcomes the existing\nbarriers when using a dense, high-dimensional, and diffeomorphic transformation\nparametrisation. This results in improved calibration of uncertainty estimates.\nWe compare the model in terms of both image registration accuracy and\nuncertainty quantification to VoxelMorph, a state-of-the-art image registration\nmodel based on deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grzech_D/0/1/0/all/0/1\">Daniel Grzech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azampour_M/0/1/0/all/0/1\">Mohammad Farid Azampour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Huaqi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Folgoc_L/0/1/0/all/0/1\">Lo&#xef;c Le Folgoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"History Aware Multimodal Transformer for Vision-and-Language Navigation. (arXiv:2110.13309v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13309","description":"<p>Vision-and-language navigation (VLN) aims to build autonomous visual agents\nthat follow instructions and navigate in real scenes. To remember previously\nvisited locations and actions taken, most approaches to VLN implement memory\nusing recurrent states. Instead, we introduce a History Aware Multimodal\nTransformer (HAMT) to incorporate a long-horizon history into multimodal\ndecision making. HAMT efficiently encodes all the past panoramic observations\nvia a hierarchical vision transformer (ViT), which first encodes individual\nimages with ViT, then models spatial relation between images in a panoramic\nobservation and finally takes into account temporal relation between panoramas\nin the history. It, then, jointly combines text, history and current\nobservation to predict the next action. We first train HAMT end-to-end using\nseveral proxy tasks including single step action prediction and spatial\nrelation prediction, and then use reinforcement learning to further improve the\nnavigation policy. HAMT achieves new state of the art on a broad range of VLN\ntasks, including VLN with fine-grained instructions (R2R, RxR), high-level\ninstructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN\n(R4R, R2R-Back). We demonstrate HAMT to be particularly effective for\nnavigation tasks with longer trajectories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guhur_P/0/1/0/all/0/1\">Pierre-Louis Guhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Ellipsoid-specific Fitting via Expectation Maximization. (arXiv:2110.13337v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13337","description":"<p>Ellipsoid fitting is of general interest in machine vision, such as object\ndetection and shape approximation. Most existing approaches rely on the\nleast-squares fitting of quadrics, minimizing the algebraic or geometric\ndistances, with additional constraints to enforce the quadric as an ellipsoid.\nHowever, they are susceptible to outliers and non-ellipsoid or biased results\nwhen the axis ratio exceeds certain thresholds. To address these problems, we\npropose a novel and robust method for ellipsoid fitting in a noisy,\noutlier-contaminated 3D environment. We explicitly model the ellipsoid by\nkernel density estimation (KDE) of the input data. The ellipsoid fitting is\ncast as a maximum likelihood estimation (MLE) problem without extra\nconstraints, where a weighting term is added to depress outliers, and then\neffectively solved via the Expectation-Maximization (EM) framework.\nFurthermore, we introduce the vector {\\epsilon} technique to accelerate the\nconvergence of the original EM. The proposed method is compared with\nrepresentative state-of-the-art approaches by extensive experiments, and\nresults show that our method is ellipsoid-specific, parameter free, and more\nrobust against noise, outliers, and the large axis ratio. Our implementation is\navailable at https://zikai1.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mingyang_Z/0/1/0/all/0/1\">Zhao Mingyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiaohong_J/0/1/0/all/0/1\">Jia Xiaohong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_M/0/1/0/all/0/1\">Ma Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xinlin_Q/0/1/0/all/0/1\">Qiu Xinlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jiang Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Ming_Y/0/1/0/all/0/1\">Yan Dong-Ming</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Camera-Based Physiological Sensing: Challenges and Future Directions. (arXiv:2110.13362v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13362","description":"<p>Numerous real-world applications have been driven by the recent algorithmic\nadvancement of artificial intelligence (AI). Healthcare is no exception and AI\ntechnologies have great potential to revolutionize the industry. Non-contact\ncamera-based physiological sensing, including remote photoplethysmography\n(rPPG), is a set of imaging methods that leverages ordinary RGB cameras (e.g.,\nwebcam or smartphone camera) to capture subtle changes in electromagnetic\nradiation (e.g., light) reflected by the body caused by physiological\nprocesses. Because of the relative ubiquity of cameras, these methods not only\nhave the ability to measure the signals without contact with the body but also\nhave the opportunity to capture multimodal information (e.g., facial\nexpressions, activities and other context) from the same sensor. However,\ndeveloping accessible, equitable and useful camera-based physiological sensing\nsystems comes with various challenges. In this article, we identify four\nresearch challenges for the field of camera-based physiological sensing and\nbroader AI driven healthcare communities and suggest future directions to\ntackle these. We believe solving these challenges will help deliver accurate,\nequitable and generalizable AI systems for healthcare that are practical in\nreal-world and clinical contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shwetak Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Automatic Detection Method Of Cerebral Aneurysms In Time-Of-Flight Magnetic Resonance Angiography Images Based On Attention 3D U-Net. (arXiv:2110.13367v1 [eess.IV])","link":"http://arxiv.org/abs/2110.13367","description":"<p>Background:Subarachnoid hemorrhage caused by ruptured cerebral aneurysm often\nleads to fatal consequences.However,if the aneurysm can be found and treated\nduring asymptomatic periods,the probability of rupture can be greatly\nreduced.At present,time-of-flight magnetic resonance angiography is one of the\nmost commonly used non-invasive screening techniques for cerebral aneurysm,and\nthe application of deep learning technology in aneurysm detection can\neffectively improve the screening effect of aneurysm.Existing studies have\nfound that three-dimensional features play an important role in aneurysm\ndetection,but they require a large amount of training data and have problems\nsuch as a high false positive rate. Methods:This paper proposed a novel method\nfor aneurysm detection.First,a fully automatic cerebral artery segmentation\nalgorithm without training data was used to extract the volume of interest,and\nthen the 3D U-Net was improved by the 3D SENet module to establish an aneurysm\ndetection model.Eventually a set of fully automated,end-to-end aneurysm\ndetection methods have been formed. Results:A total of 231 magnetic resonance\nangiography image data were used in this study,among which 132 were training\nsets,34 were internal test sets and 65 were external test sets.The presented\nmethod obtained 97.89% sensitivity in the five-fold cross-validation and\nobtained 91.0% sensitivity with 2.48 false positives/case in the detection of\nthe external test sets. Conclusions:Compared with the results of our previous\nstudies and other studies,the method in this paper achieves a very competitive\nsensitivity with less training data and maintains a low false positive rate.As\nthe only method currently using 3D U-Net for aneurysm detection,it proves the\nfeasibility and superior performance of this network in aneurysm detection,and\nalso explores the potential of the channel attention mechanism in this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Geng_C/0/1/0/all/0/1\">Chen Geng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Di_R/0/1/0/all/0/1\">Ruoyu Di</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Dongdong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Liqin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuxin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geng_D/0/1/0/all/0/1\">Daoying Geng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plug-and-Play Few-shot Object Detection with Meta Strategy and Explicit Localization Inference. (arXiv:2110.13377v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13377","description":"<p>Aiming at recognizing and localizing the object of novel categories by a few\nreference samples, few-shot object detection is a quite challenging task.\nPrevious works often depend on the fine-tuning process to transfer their model\nto the novel category and rarely consider the defect of fine-tuning, resulting\nin many drawbacks. For example, these methods are far from satisfying in the\nlow-shot or episode-based scenarios since the fine-tuning process in object\ndetection requires much time and high-shot support data. To this end, this\npaper proposes a plug-and-play few-shot object detection (PnP-FSOD) framework\nthat can accurately and directly detect the objects of novel categories without\nthe fine-tuning process. To accomplish the objective, the PnP-FSOD framework\ncontains two parallel techniques to address the core challenges in the few-shot\nlearning, i.e., across-category task and few-annotation support. Concretely, we\nfirst propose two simple but effective meta strategies for the box classifier\nand RPN module to enable the across-category object detection without\nfine-tuning. Then, we introduce two explicit inferences into the localization\nprocess to reduce its dependence on the annotated data, including explicit\nlocalization score and semi-explicit box regression. In addition to the\nPnP-FSOD framework, we propose a novel one-step tuning method that can avoid\nthe defects in fine-tuning. It is noteworthy that the proposed techniques and\ntuning method are based on the general object detector without other prior\nmethods, so they are easily compatible with the existing FSOD methods.\nExtensive experiments show that the PnP-FSOD framework has achieved the\nstate-of-the-art few-shot object detection performance without any tuning\nmethod. After applying the one-step tuning method, it further shows a\nsignificant lead in both efficiency, precision, and recall, under varied\nevaluation protocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junying Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViDA-MAN: Visual Dialog with Digital Humans. (arXiv:2110.13384v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13384","description":"<p>We demonstrate ViDA-MAN, a digital-human agent for multi-modal interaction,\nwhich offers realtime audio-visual responses to instant speech inquiries.\nCompared to traditional text or voice-based system, ViDA-MAN offers human-like\ninteractions (e.g, vivid voice, natural facial expression and body gestures).\nGiven a speech request, the demonstration is able to response with high quality\nvideos in sub-second latency. To deliver immersive user experience, ViDA-MAN\nseamlessly integrates multi-modal techniques including Acoustic Speech\nRecognition (ASR), multi-turn dialog, Text To Speech (TTS), talking heads video\ngeneration. Backed with large knowledge base, ViDA-MAN is able to chat with\nusers on a number of topics including chit-chat, weather, device control, News\nrecommendations, booking hotels, as well as answering questions via structured\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_J/0/1/0/all/0/1\">Jiawei Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Fan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liqin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengchen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IIP-Transformer: Intra-Inter-Part Transformer for Skeleton-Based Action Recognition. (arXiv:2110.13385v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13385","description":"<p>Recently, Transformer-based networks have shown great promise on\nskeleton-based action recognition tasks. The ability to capture global and\nlocal dependencies is the key to success while it also brings quadratic\ncomputation and memory cost. Another problem is that previous studies mainly\nfocus on the relationships among individual joints, which often suffers from\nthe noisy skeleton joints introduced by the noisy inputs of sensors or\ninaccurate estimations. To address the above issues, we propose a novel\nTransformer-based network (IIP-Transformer). Instead of exploiting interactions\namong individual joints, our IIP-Transformer incorporates body joints and parts\ninteractions simultaneously and thus can capture both joint-level (intra-part)\nand part-level (inter-part) dependencies efficiently and effectively. From the\ndata aspect, we introduce a part-level skeleton data encoding that\nsignificantly reduces the computational complexity and is more robust to\njoint-level skeleton noise. Besides, a new part-level data augmentation is\nproposed to improve the performance of the model. On two large-scale datasets,\nNTU-RGB+D 60 and NTU RGB+D 120, the proposed IIP-Transformer achieves\nthe-state-of-art performance with more than 8x less computational complexity\nthan DSTA-Net, which is the SOTA Transformer-based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingtian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianlin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuze Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiabin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1\">Renliang Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Denoising Neural Networks for Few Shot Learning. (arXiv:2110.13386v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13386","description":"<p>In this paper, we introduce a new architecture for few shot learning, the\ntask of teaching a neural network from as few as one or five labeled examples.\nInspired by the theoretical results of Alaine et al that Denoising Autoencoders\nrefine features to lie closer to the true data manifold, we present a new\ntraining scheme that adds noise at multiple stages of an existing neural\narchitecture while simultaneously learning to be robust to this added noise.\nThis architecture, which we call a Self-Denoising Neural Network (SDNN), can be\napplied easily to most modern convolutional neural architectures, and can be\nused as a supplement to many existing few-shot learning techniques. We\nempirically show that SDNNs out-perform previous state-of-the-art methods for\nfew shot image recognition using the Wide-ResNet architecture on the\n\\textit{mini}ImageNet, tiered-ImageNet, and CIFAR-FS few shot learning\ndatasets. We also perform a series of ablation experiments to empirically\njustify the construction of the SDNN architecture. Finally, we show that SDNNs\neven improve few shot performance on the task of human action detection in\nvideo using experiments on the ActEV SDL Surprise Activities challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwarcz_S/0/1/0/all/0/1\">Steven Schwarcz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1\">Sai Saketh Rambhatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Normalized Gaussian Wasserstein Distance for Tiny Object Detection. (arXiv:2110.13389v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13389","description":"<p>Detecting tiny objects is a very challenging problem since a tiny object only\ncontains a few pixels in size. We demonstrate that state-of-the-art detectors\ndo not produce satisfactory results on tiny objects due to the lack of\nappearance information. Our key observation is that Intersection over Union\n(IoU) based metrics such as IoU itself and its extensions are very sensitive to\nthe location deviation of the tiny objects, and drastically deteriorate the\ndetection performance when used in anchor-based detectors. To alleviate this,\nwe propose a new evaluation metric using Wasserstein distance for tiny object\ndetection. Specifically, we first model the bounding boxes as 2D Gaussian\ndistributions and then propose a new metric dubbed Normalized Wasserstein\nDistance (NWD) to compute the similarity between them by their corresponding\nGaussian distributions. The proposed NWD metric can be easily embedded into the\nassignment, non-maximum suppression, and loss function of any anchor-based\ndetector to replace the commonly used IoU metric. We evaluate our metric on a\nnew dataset for tiny object detection (AI-TOD) in which the average object size\nis much smaller than existing object detection datasets. Extensive experiments\nshow that, when equipped with NWD metric, our approach yields performance that\nis 6.7 AP points higher than a standard fine-tuning baseline, and 6.0 AP points\nhigher than state-of-the-art competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Domain-Agnostic Knowledge in Video Question Answering. (arXiv:2110.13395v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13395","description":"<p>Video question answering (VideoQA) is designed to answer a given question\nbased on a relevant video clip. The current available large-scale datasets have\nmade it possible to formulate VideoQA as the joint understanding of visual and\nlanguage information. However, this training procedure is costly and still less\ncompetent with human performance. In this paper, we investigate a transfer\nlearning method by the introduction of domain-agnostic knowledge and\ndomain-specific knowledge. First, we develop a novel transfer learning\nframework, which finetunes the pre-trained model by applying domain-agnostic\nknowledge as the medium. Second, we construct a new VideoQA dataset with 21,412\nhuman-generated question-answer samples for comparable transfer of knowledge.\nOur experiments show that: (i) domain-agnostic knowledge is transferable and\n(ii) our proposed transfer learning framework can boost VideoQA performance\neffectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianran Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Noa Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otani_M/0/1/0/all/0/1\">Mayu Otani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1\">Yuta Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takemura_H/0/1/0/all/0/1\">Haruo Takemura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Rich Features for Gait Recognition by Integrating Skeletons and Silhouettes. (arXiv:2110.13408v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13408","description":"<p>Gait recognition captures gait patterns from the walking sequence of an\nindividual for identification. Most existing gait recognition methods learn\nfeatures from silhouettes or skeletons for the robustness to clothing,\ncarrying, and other exterior factors. The combination of the two data\nmodalities, however, is not fully exploited. This paper proposes a simple yet\neffective bimodal fusion (BiFusion) network, which mines the complementary\nclues of skeletons and silhouettes, to learn rich features for gait\nidentification. Particularly, the inherent hierarchical semantics of body\njoints in a skeleton is leveraged to design a novel Multi-scale Gait Graph\n(MSGG) network for the feature extraction of skeletons. Extensive experiments\non CASIA-B and OUMVLP demonstrate both the superiority of the proposed MSGG\nnetwork in modeling skeletons and the effectiveness of the bimodal fusion for\ngait recognition. Under the most challenging condition of walking in different\nclothes on CASIA-B, our method achieves the rank-1 accuracy of 92.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yunjie Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Saihui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TriBERT: Full-body Human-centric Audio-visual Representation Learning for Visual Sound Separation. (arXiv:2110.13412v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13412","description":"<p>The recent success of transformer models in language, such as BERT, has\nmotivated the use of such architectures for multi-modal feature learning and\ntasks. However, most multi-modal variants (e.g., ViLBERT) have limited\nthemselves to visual-linguistic data. Relatively few have explored its use in\naudio-visual modalities, and none, to our knowledge, illustrate them in the\ncontext of granular audio-visual detection or segmentation tasks such as sound\nsource separation and localization. In this work, we introduce TriBERT -- a\ntransformer-based architecture, inspired by ViLBERT, which enables contextual\nfeature learning across three modalities: vision, pose, and audio, with the use\nof flexible co-attention. The use of pose keypoints is inspired by recent works\nthat illustrate that such representations can significantly boost performance\nin many audio-visual scenarios where often one or more persons are responsible\nfor the sound explicitly (e.g., talking) or implicitly (e.g., sound produced as\na function of human manipulating an object). From a technical perspective, as\npart of the TriBERT architecture, we introduce a learned visual tokenization\nscheme based on spatial attention and leverage weak-supervision to allow\ngranular cross-modal interactions for visual and pose modalities. Further, we\nsupplement learning with sound-source separation loss formulated across all\nthree streams. We pre-train our model on the large MUSIC21 dataset and\ndemonstrate improved performance in audio-visual sound source separation on\nthat dataset as well as other datasets through fine-tuning. In addition, we\nshow that the learned TriBERT representations are generic and significantly\nimprove performance on other audio-visual tasks such as cross-modal\naudio-visual-pose retrieval by as much as 66.7% in top-1 accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1\">Tanzila Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1\">Leonid Sigal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Host-free Trojan Attack. (arXiv:2110.13414v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13414","description":"<p>In this paper, we propose a novel host-free Trojan attack with triggers that\nare fixed in the semantic space but not necessarily in the pixel space. In\ncontrast to existing Trojan attacks which use clean input images as hosts to\ncarry small, meaningless trigger patterns, our attack considers triggers as\nfull-sized images belonging to a semantically meaningful object class. Since in\nour attack, the backdoored classifier is encouraged to memorize the abstract\nsemantics of the trigger images than any specific fixed pattern, it can be\nlater triggered by semantically similar but different looking images. This\nmakes our attack more practical to be applied in the real-world and harder to\ndefend against. Extensive experimental results demonstrate that with only a\nsmall number of Trojan patterns for training, our attack can generalize well to\nnew patterns of the same Trojan class and can bypass state-of-the-art defense\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harikumar_H/0/1/0/all/0/1\">Haripriya Harikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1\">Kien Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_S/0/1/0/all/0/1\">Santu Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Magnification Network for Vessel Segmentation in OCTA Images. (arXiv:2110.13428v1 [eess.IV])","link":"http://arxiv.org/abs/2110.13428","description":"<p>Optical coherence tomography angiography (OCTA) is a novel non-invasive\nimaging modality that allows micron-level resolution to visualize the retinal\nmicrovasculature. The retinal vessel segmentation in OCTA images is still an\nopen problem, and especially the thin and dense structure of the capillary\nplexus is an important challenge of this problem. In this work, we propose a\nnovel image magnification network (IMN) for vessel segmentation in OCTA images.\nContrary to the U-Net structure with a down-sampling encoder and up-sampling\ndecoder, the proposed IMN adopts the design of up-sampling encoding and then\ndown-sampling decoding. This design is to capture more image details and reduce\nthe omission of thin-and-small structures. The experimental results on three\nopen OCTA datasets show that the proposed IMN with an average dice score of\n90.2% achieves the best performance in vessel segmentation of OCTA images.\nBesides, we also demonstrate the superior performance of IMN in cross-field\nimage vessel segmentation and vessel skeleton extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1\">Mingchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yerui Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Weiwei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Similarity Aggregation with Self-attention for Visual Re-ranking. (arXiv:2110.13430v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13430","description":"<p>In content-based image retrieval, the first-round retrieval result by simple\nvisual feature comparison may be unsatisfactory, which can be refined by visual\nre-ranking techniques. In image retrieval, it is observed that the contextual\nsimilarity among the top-ranked images is an important clue to distinguish the\nsemantic relevance. Inspired by this observation, in this paper, we propose a\nvisual re-ranking method by contextual similarity aggregation with\nself-attention. In our approach, for each image in the top-K ranking list, we\nrepresent it into an affinity feature vector by comparing it with a set of\nanchor images. Then, the affinity features of the top-K images are refined by\naggregating the contextual information with a transformer encoder. Finally, the\naffinity features are used to recalculate the similarity scores between the\nquery and the top-K images for re-ranking of the latter. To further improve the\nrobustness of our re-ranking model and enhance the performance of our method, a\nnew data augmentation scheme is designed. Since our re-ranking model is not\ndirectly involved with the visual feature used in the initial retrieval, it is\nready to be applied to retrieval result lists obtained from various retrieval\nalgorithms. We conduct comprehensive experiments on four benchmark datasets to\ndemonstrate the generality and effectiveness of our proposed visual re-ranking\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1\">Jianbo Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Min Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-based Segmentation of Cerebral Aneurysms in 3D TOF-MRA using Coarse-to-Fine Framework. (arXiv:2110.13432v1 [eess.IV])","link":"http://arxiv.org/abs/2110.13432","description":"<p>BACKGROUND AND PURPOSE: Cerebral aneurysm is one of the most common\ncerebrovascular diseases, and SAH caused by its rupture has a very high\nmortality and disability rate. Existing automatic segmentation methods based on\nDLMs with TOF-MRA modality could not segment edge voxels very well, so that our\ngoal is to realize more accurate segmentation of cerebral aneurysms in 3D\nTOF-MRA with the help of DLMs. MATERIALS AND METHODS: In this research, we\nproposed an automatic segmentation framework of cerebral aneurysm in 3D\nTOF-MRA. The framework was composed of two segmentation networks ranging from\ncoarse to fine. The coarse segmentation network, namely DeepMedic, completed\nthe coarse segmentation of cerebral aneurysms, and the processed results were\nfed into the fine segmentation network, namely dual-channel SE_3D U-Net trained\nwith weighted loss function, for fine segmentation. Images from ADAM2020\n(n=113) were used for training and validation and images from another center\n(n=45) were used for testing. The segmentation metrics we used include DSC, HD,\nand VS. RESULTS: The trained cerebral aneurysm segmentation model achieved DSC\nof 0.75, HD of 1.52, and VS of 0.91 on validation cohort. On the totally\nindependent test cohort, our method achieved the highest DSC of 0.12, the\nlowest HD of 11.61, and the highest VS of 0.16 in comparison with\nstate-of-the-art segmentation networks. CONCLUSIONS: The coarse-to-fine\nframework, which composed of DeepMedic and dual-channel SE_3D U-Net can segment\ncerebral aneurysms in 3D TOF-MRA with a superior accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geng_C/0/1/0/all/0/1\">Chen Geng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Dongdong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Di_R/0/1/0/all/0/1\">Ruoyu Di</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1\">Fengmei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiyong Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Piao_S/0/1/0/all/0/1\">Sirong Piao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuxin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_Y/0/1/0/all/0/1\">Yaikang Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Role of Self-Supervised Learning in Out-of-Distribution Detection Task. (arXiv:2110.13435v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13435","description":"<p>Self-supervised learning (SSL) has achieved great success in a variety of\ncomputer vision tasks. However, the mechanism of how SSL works in these tasks\nremains a mystery. In this paper, we study how SSL can enhance the performance\nof the out-of-distribution (OOD) detection task. We first point out two general\nproperties that a good OOD detector should have: 1) the overall feature space\nshould be large and 2) the inlier feature space should be small. Then we\ndemonstrate that SSL can indeed increase the intrinsic dimension of the overall\nfeature space. In the meantime, SSL even has the potential to shrink the inlier\nfeature space. As a result, there will be more space spared for the outliers,\nmaking OOD detection much easier. The conditions when SSL can shrink the inlier\nfeature space is also discussed and validated. By understanding the role of SSL\nin the OOD detection task, our study can provide a guideline for designing\nbetter OOD detection algorithms. Moreover, this work can also shed light to\nother tasks where SSL can improve the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiuhai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bin Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A time-weighted metric for sets of trajectories to assess multi-object tracking algorithms. (arXiv:2110.13444v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13444","description":"<p>This paper proposes a metric for sets of trajectories to evaluate\nmulti-object tracking algorithms that includes time-weighted costs for\nlocalisation errors of properly detected targets, for false targets, missed\ntargets and track switches. The proposed metric extends the metric in [1] by\nincluding weights to the costs associated to different time steps. The\ntime-weighted costs increase the flexibility of the metric [1] to fit more\napplications and user preferences. We first introduce a metric based on\nmulti-dimensional assignments, and then its linear programming relaxation,\nwhich is computable in polynomial time and is also a metric. The metrics can\nalso be extended to metrics on random finite sets of trajectories to evaluate\nand rank algorithms across different scenarios, each with a ground truth set of\ntrajectories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Fernandez_A/0/1/0/all/0/1\">&#xc1;ngel F. Garc&#xed;a-Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmathullah_A/0/1/0/all/0/1\">Abu Sajana Rahmathullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1\">Lennart Svensson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subject Adaptive EEG-based Visual Recognition. (arXiv:2110.13470v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13470","description":"<p>This paper focuses on EEG-based visual recognition, aiming to predict the\nvisual object class observed by a subject based on his/her EEG signals. One of\nthe main challenges is the large variation between signals from different\nsubjects. It limits recognition systems to work only for the subjects involved\nin model training, which is undesirable for real-world scenarios where new\nsubjects are frequently added. This limitation can be alleviated by collecting\na large amount of data for each new user, yet it is costly and sometimes\ninfeasible. To make the task more practical, we introduce a novel problem\nsetting, namely subject adaptive EEG-based visual recognition. In this setting,\na bunch of pre-recorded data of existing users (source) is available, while\nonly a little training data from a new user (target) are provided. At inference\ntime, the model is evaluated solely on the signals from the target user. This\nsetting is challenging, especially because training samples from source\nsubjects may not be helpful when evaluating the model on the data from the\ntarget subject. To tackle the new problem, we design a simple yet effective\nbaseline that minimizes the discrepancy between feature distributions from\ndifferent subjects, which allows the model to extract subject-independent\nfeatures. Consequently, our model can learn the common knowledge shared among\nsubjects, thereby significantly improving the recognition performance for the\ntarget subject. In the experiments, we demonstrate the effectiveness of our\nmethod under various settings. Our code is available at\nhttps://github.com/DeepBCI/Deep-BCI/tree/master/1_Intelligent_BCI/Subject_Adaptive_EEG_based_Visual_Recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Pilhyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sunhee Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1\">Seogkyu Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1\">Hyeran Byun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Response-based Distillation for Incremental Object Detection. (arXiv:2110.13471v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13471","description":"<p>Traditional object detection are ill-equipped for incremental learning.\nHowever, fine-tuning directly on a well-trained detection model with only new\ndata will leads to catastrophic forgetting. Knowledge distillation is a\nstraightforward way to mitigate catastrophic forgetting. In Incremental Object\nDetection (IOD), previous work mainly focuses on feature-level knowledge\ndistillation, but the different response of detector has not been fully\nexplored yet. In this paper, we propose a fully response-based incremental\ndistillation method focusing on learning response from detection bounding boxes\nand classification predictions. Firstly, our method transferring category\nknowledge while equipping student model with the ability to retain localization\nknowledge during incremental learning. In addition, we further evaluate the\nqualities of all locations and provides valuable response by adaptive\npseudo-label selection (APS) strategies. Finally, we elucidate that knowledge\nfrom different responses should be assigned with different importance during\nincremental distillation. Extensive experiments conducted on MS COCO\ndemonstrate significant advantages of our method, which substantially narrow\nthe performance gap towards full training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTRN: Class-Temporal Relational Network for Action Detection. (arXiv:2110.13473v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13473","description":"<p>Action detection is an essential and challenging task, especially for densely\nlabelled datasets of untrimmed videos. There are many real-world challenges in\nthose datasets, such as composite action, co-occurring action, and high\ntemporal variation of instance duration. For handling these challenges, we\npropose to explore both the class and temporal relations of detected actions.\nIn this work, we introduce an end-to-end network: Class-Temporal Relational\nNetwork (CTRN). It contains three key components: (1) The Representation\nTransform Module filters the class-specific features from the mixed\nrepresentations to build graph-structured data. (2) The Class-Temporal Module\nmodels the class and temporal relations in a sequential manner. (3)\nG-classifier leverages the privileged knowledge of the snippet-wise\nco-occurring action pairs to further improve the co-occurring action detection.\nWe evaluate CTRN on three challenging densely labelled datasets and achieve\nstate-of-the-art performance, reflecting the effectiveness and robustness of\nour method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1\">Rui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srijan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Action Recognition from Diverse Object-Scene Compositions. (arXiv:2110.13479v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13479","description":"<p>This paper investigates the problem of zero-shot action recognition, in the\nsetting where no training videos with seen actions are available. For this\nchallenging scenario, the current leading approach is to transfer knowledge\nfrom the image domain by recognizing objects in videos using pre-trained\nnetworks, followed by a semantic matching between objects and actions. Where\nobjects provide a local view on the content in videos, in this work we also\nseek to include a global view of the scene in which actions occur. We find that\nscenes on their own are also capable of recognizing unseen actions, albeit more\nmarginally than objects, and a direct combination of object-based and\nscene-based scores degrades the action recognition performance. To get the best\nout of objects and scenes, we propose to construct them as a Cartesian product\nof all possible compositions. We outline how to determine the likelihood of\nobject-scene compositions in videos, as well as a semantic matching from\nobject-scene compositions to actions that enforces diversity among the most\nrelevant compositions for each action. While simple, our composition-based\napproach outperforms object-based approaches and even state-of-the-art\nzero-shot approaches that rely on large-scale video datasets with hundreds of\nseen actions for training and knowledge transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bretti_C/0/1/0/all/0/1\">Carlo Bretti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning for Multi-Label Few-Shot Classification. (arXiv:2110.13494v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13494","description":"<p>Even with the luxury of having abundant data, multi-label classification is\nwidely known to be a challenging task to address. This work targets the problem\nof multi-label meta-learning, where a model learns to predict multiple labels\nwithin a query (e.g., an image) by just observing a few supporting examples. In\ndoing so, we first propose a benchmark for Few-Shot Learning (FSL) with\nmultiple labels per sample. Next, we discuss and extend several solutions\nspecifically designed to address the conventional and single-label FSL, to work\nin the multi-label regime. Lastly, we introduce a neural module to estimate the\nlabel count of a given sample by exploiting the relational inference. We will\nshow empirically the benefit of the label count module, the label propagation\nalgorithm, and the extensions of conventional FSL methods on three challenging\ndatasets, namely MS-COCO, iMaterialist, and Open MIC. Overall, our thorough\nexperiments suggest that the proposed label-propagation algorithm in\nconjunction with the neural label count module (NLC) shall be considered as the\nmethod of choice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simon_C/0/1/0/all/0/1\">Christian Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Morphing Attack Detection using Feature Selection and Visualisation based on Mutual Information. (arXiv:2110.13552v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13552","description":"<p>Face morphing attack detection is a challenging task. Automatic\nclassification methods and manual inspection are realised in automatic border\ncontrol gates to detect morphing attacks. Understanding how a machine learning\nsystem can detect morphed faces and the most relevant facial areas is crucial.\nThose relevant areas contain texture signals that allow us to separate the bona\nfide and the morph images. Also, it helps in the manual examination to detect a\npassport generated with morphed images. This paper explores features extracted\nfrom intensity, shape, texture, and proposes a feature selection stage based on\nthe Mutual Information filter to select the most relevant and less redundant\nfeatures. This selection allows us to reduce the workload and know the exact\nlocalisation of such areas to understand the morphing impact and create a\nrobust classifier. The best results were obtained for the method based on\nConditional Mutual Information and Shape features using only 500 features for\nFERET images and 800 features for FRGCv2 images from 1,048 features available.\nThe eyes and nose are identified as the most critical areas to be analysed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1\">Juan Tapia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Directional Self-supervised Learning for Risky Image Augmentations. (arXiv:2110.13555v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13555","description":"<p>Only a few cherry-picked robust augmentation policies are beneficial to\nstandard self-supervised image representation learning, despite the large\naugmentation family. In this paper, we propose a directional self-supervised\nlearning paradigm (DSSL), which is compatible with significantly more\naugmentations. Specifically, we adapt risky augmentation policies after\nstandard views augmented by robust augmentations, to generate harder risky view\n(RV). The risky view usually has a higher deviation from the original image\nthan the standard robust view (SV). Unlike previous methods equally pairing all\naugmented views for symmetrical self-supervised training to maximize their\nsimilarities, DSSL treats augmented views of the same instance as a partially\nordered set (SV$\\leftrightarrow $SV, SV$\\leftarrow$RV), and then equips\ndirectional objective functions respecting to the derived relationships among\nviews. DSSL can be easily implemented with a few lines of Pseudocode and is\nhighly flexible to popular self-supervised learning frameworks, including\nSimCLR, SimSiam, BYOL. The extensive experimental results on CIFAR and ImageNet\ndemonstrated that DSSL can stably improve these frameworks with compatibility\nto a wider range of augmentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yalong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Region Building Counting in Satellite Imagery using Counting Consistency. (arXiv:2110.13558v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13558","description":"<p>Estimating the number of buildings in any geographical region is a vital\ncomponent of urban analysis, disaster management, and public policy decision.\nDeep learning methods for building localization and counting in satellite\nimagery, can serve as a viable and cheap alternative. However, these algorithms\nsuffer performance degradation when applied to the regions on which they have\nnot been trained. Current large datasets mostly cover the developed regions and\ncollecting such datasets for every region is a costly, time-consuming, and\ndifficult endeavor. In this paper, we propose an unsupervised domain adaptation\nmethod for counting buildings where we use a labeled source domain (developed\nregions) and adapt the trained model on an unlabeled target domain (developing\nregions). We initially align distribution maps across domains by aligning the\noutput space distribution through adversarial loss. We then exploit counting\nconsistency constraints, within-image count consistency, and across-image count\nconsistency, to decrease the domain shift. Within-image consistency enforces\nthat building count in the whole image should be greater than or equal to count\nin any of its sub-image. Across-image consistency constraint enforces that if\nan image contains considerably more buildings than the other image, then their\nsub-images shall also have the same order. These two constraints encourage the\nbehavior to be consistent across and within the images, regardless of the\nscale. To evaluate the performance of our proposed approach, we collected and\nannotated a large-scale dataset consisting of challenging South Asian regions\nhaving higher building densities and irregular structures as compared to\nexisting datasets. We perform extensive experiments to verify the efficacy of\nour approach and report improvements of approximately 7% to 20% over the\ncompetitive baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zakria_M/0/1/0/all/0/1\">Muaaz Zakria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawal_H/0/1/0/all/0/1\">Hamza Rawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultani_W/0/1/0/all/0/1\">Waqas Sultani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsen Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Graph Representation of Person-specific Cognitive Processes from Audio-visual Behaviours for Automatic Personality Recognition. (arXiv:2110.13570v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13570","description":"<p>This approach builds on two following findings in cognitive science: (i)\nhuman cognition partially determines expressed behaviour and is directly linked\nto true personality traits; and (ii) in dyadic interactions individuals'\nnonverbal behaviours are influenced by their conversational partner behaviours.\nIn this context, we hypothesise that during a dyadic interaction, a target\nsubject's facial reactions are driven by two main factors, i.e. their internal\n(person-specific) cognitive process, and the externalised nonverbal behaviours\nof their conversational partner. Consequently, we propose to represent the\ntarget subjects (defined as the listener) person-specific cognition in the form\nof a person-specific CNN architecture that has unique architectural parameters\nand depth, which takes audio-visual non-verbal cues displayed by the\nconversational partner (defined as the speaker) as input, and is able to\nreproduce the target subject's facial reactions. Each person-specific CNN is\nexplored by the Neural Architecture Search (NAS) and a novel adaptive loss\nfunction, which is then represented as a graph representation for recognising\nthe target subject's true personality. Experimental results not only show that\nthe produced graph representations are well associated with target subjects'\npersonality traits in both human-human and human-machine interaction scenarios,\nand outperform the existing approaches with significant advantages, but also\ndemonstrate that the proposed novel strategies such as adaptive loss, and the\nend-to-end vertices/edges feature learning, help the proposed approach in\nlearning more reliable personality representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Siyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zilong Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_S/0/1/0/all/0/1\">Shashank Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valstar_M/0/1/0/all/0/1\">Michel Valstar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunes_H/0/1/0/all/0/1\">Hatice Gunes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion recognition in talking-face videos using persistent entropy and neural networks. (arXiv:2110.13571v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13571","description":"<p>The automatic recognition of a person's emotional state has become a very\nactive research field that involves scientists specialized in different areas\nsuch as artificial intelligence, computer vision or psychology, among others.\nOur main objective in this work is to develop a novel approach, using\npersistent entropy and neural networks as main tools, to recognise and classify\nemotions from talking-face videos. Specifically, we combine audio-signal and\nimage-sequence information to compute a topology signature(a 9-dimensional\nvector) for each video. We prove that small changes in the video produce small\nchanges in the signature. These topological signatures are used to feed a\nneural network to distinguish between the following emotions: neutral, calm,\nhappy, sad, angry, fearful, disgust, and surprised. The results reached are\npromising and competitive, beating the performance reached in other\nstate-of-the-art works found in the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paluzo_Hidalgo_E/0/1/0/all/0/1\">Eduardo Paluzo-Hidalgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguirre_Carrazana_G/0/1/0/all/0/1\">Guillermo Aguirre-Carrazana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Diaz_R/0/1/0/all/0/1\">Rocio Gonzalez-Diaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Learning for Animal Pose Estimation using RBF k-DPP. (arXiv:2110.13598v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13598","description":"<p>Pose estimation is the task of locating keypoints for an object of interest\nin an image. Animal Pose estimation is more challenging than estimating human\npose due to high inter and intra class variability in animals. Existing works\nsolve this problem for a fixed set of predefined animal categories. Models\ntrained on such sets usually do not work well with new animal categories.\nRetraining the model on new categories makes the model overfit and leads to\ncatastrophic forgetting. Thus, in this work, we propose a novel problem of\n\"Incremental Learning for Animal Pose Estimation\". Our method uses an exemplar\nmemory, sampled using Determinantal Point Processes (DPP) to continually adapt\nto new animal categories without forgetting the old ones. We further propose a\nnew variant of k-DPP that uses RBF kernel (termed as \"RBF k-DPP\") which gives\nmore gain in performance over traditional k-DPP. Due to memory constraints, the\nlimited number of exemplars along with new class data can lead to class\nimbalance. We mitigate it by performing image warping as an augmentation\ntechnique. This helps in crafting diverse poses, which reduces overfitting and\nyields further improvement in performance. The efficacy of our proposed\napproach is demonstrated via extensive experiments and ablations where we\nobtain significant improvements over state-of-the-art baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Gaurav Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1\">Het Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dendritic Self-Organizing Maps for Continual Learning. (arXiv:2110.13611v1 [cs.NE])","link":"http://arxiv.org/abs/2110.13611","description":"<p>Current deep learning architectures show remarkable performance when trained\nin large-scale, controlled datasets. However, the predictive ability of these\narchitectures significantly decreases when learning new classes incrementally.\nThis is due to their inclination to forget the knowledge acquired from\npreviously seen data, a phenomenon termed catastrophic-forgetting. On the other\nhand, Self-Organizing Maps (SOMs) can model the input space utilizing\nconstrained k-means and thus maintain past knowledge. Here, we propose a novel\nalgorithm inspired by biological neurons, termed Dendritic-Self-Organizing Map\n(DendSOM). DendSOM consists of a single layer of SOMs, which extract patterns\nfrom specific regions of the input space accompanied by a set of hit matrices,\none per SOM, which estimate the association between units and labels. The\nbest-matching unit of an input pattern is selected using the maximum cosine\nsimilarity rule, while the point-wise mutual information is employed for class\ninference. DendSOM performs unsupervised feature extraction as it does not use\nlabels for targeted updating of the weights. It outperforms classical SOMs and\nseveral state-of-the-art continual learning algorithms on benchmark datasets,\nsuch as the Split-MNIST and Split-CIFAR-10. We propose that the incorporation\nof neuronal properties in SOMs may help remedy catastrophic forgetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pinitas_K/0/1/0/all/0/1\">Kosmas Pinitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavlis_S/0/1/0/all/0/1\">Spyridon Chavlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poirazi_P/0/1/0/all/0/1\">Panayiota Poirazi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Optimization and Deep Learning forsteering wheel angle prediction. (arXiv:2110.13629v1 [cs.LG])","link":"http://arxiv.org/abs/2110.13629","description":"<p>Automated driving systems (ADS) have undergone a significant improvement in\nthe last years. ADS and more precisely self-driving cars technologies will\nchange the way we perceive and know the world of transportation systems in\nterms of user experience, mode choices and business models. The emerging field\nof Deep Learning (DL) has been successfully applied for the development of\ninnovative ADS solutions. However, the attempt to single out the best deep\nneural network architecture and tuning its hyperparameters are all expensive\nprocesses, both in terms of time and computational resources. In this work,\nBayesian Optimization (BO) is used to optimize the hyperparameters of a\nSpatiotemporal-Long Short Term Memory (ST-LSTM) network with the aim to obtain\nan accurate model for the prediction of the steering angle in a ADS. BO was\nable to identify, within a limited number of trials, a model -- namely\nBOST-LSTM -- which resulted, on a public dataset, the most accurate when\ncompared to classical end-to-end driving models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riboni_A/0/1/0/all/0/1\">Alessandro Riboni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghioldi_N/0/1/0/all/0/1\">Nicol&#xf2; Ghioldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Candelieri_A/0/1/0/all/0/1\">Antonio Candelieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borrotti_M/0/1/0/all/0/1\">Matteo Borrotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Precision Diagnostic Framework of Renal Cell Carcinoma on Whole-Slide Images using Deep Learning. (arXiv:2110.13652v1 [eess.IV])","link":"http://arxiv.org/abs/2110.13652","description":"<p>Diagnostic pathology, which is the basis and gold standard of cancer\ndiagnosis, provides essential information on the prognosis of the disease and\nvital evidence for clinical treatment. Tumor region detection, subtype and\ngrade classification are the fundamental diagnostic indicators for renal cell\ncarcinoma (RCC) in whole-slide images (WSIs). However, pathological diagnosis\nis subjective, differences in observation and diagnosis between pathologists is\ncommon in hospitals with inadequate diagnostic capacity. The main challenge for\ndeveloping deep learning based RCC diagnostic system is the lack of large-scale\ndatasets with precise annotations. In this work, we proposed a deep\nlearning-based framework for analyzing histopathological images of patients\nwith renal cell carcinoma, which has the potential to achieve pathologist-level\naccuracy in diagnosis. A deep convolutional neural network (InceptionV3) was\ntrained on the high-quality annotated dataset of The Cancer Genome Atlas (TCGA)\nwhole-slide histopathological image for accurate tumor area detection,\nclassification of RCC subtypes, and ISUP grades classification of clear cell\ncarcinoma subtypes. These results suggest that our framework can help\npathologists in the detection of cancer region and classification of subtypes\nand grades, which could be applied to any cancer type, providing auxiliary\ndiagnosis and promoting clinical consensus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jialun Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Haichuan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zeyu Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bao_X/0/1/0/all/0/1\">Xinrui Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_T/0/1/0/all/0/1\">Tieliang Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chunbao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"W-Net: A Two-Stage Convolutional Network for Nucleus Detection in Histopathology Image. (arXiv:2110.13670v1 [eess.IV])","link":"http://arxiv.org/abs/2110.13670","description":"<p>Pathological diagnosis is the gold standard for cancer diagnosis, but it is\nlabor-intensive, in which tasks such as cell detection, classification, and\ncounting are particularly prominent. A common solution for automating these\ntasks is using nucleus segmentation technology. However, it is hard to train a\nrobust nucleus segmentation model, due to several challenging problems, the\nnucleus adhesion, stacking, and excessive fusion with the background. Recently,\nsome researchers proposed a series of automatic nucleus segmentation methods\nbased on point annotation, which can significant improve the model performance.\nNevertheless, the point annotation needs to be marked by experienced\npathologists. In order to take advantage of segmentation methods based on point\nannotation, further alleviate the manual workload, and make cancer diagnosis\nmore efficient and accurate, it is necessary to develop an automatic nucleus\ndetection algorithm, which can automatically and efficiently locate the\nposition of the nucleus in the pathological image and extract valuable\ninformation for pathologists. In this paper, we propose a W-shaped network for\nautomatic nucleus detection. Different from the traditional U-Net based method,\nmapping the original pathology image to the target mask directly, our proposed\nmethod split the detection task into two sub-tasks. The first sub-task maps the\noriginal pathology image to the binary mask, then the binary mask is mapped to\nthe density mask in the second sub-task. After the task is split, the task's\ndifficulty is significantly reduced, and the network's overall performance is\nimproved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mao_A/0/1/0/all/0/1\">Anyu Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jialun Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bao_X/0/1/0/all/0/1\">Xinrui Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zeyu Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_T/0/1/0/all/0/1\">Tieliang Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alpha-IoU: A Family of Power Intersection over Union Losses for Bounding Box Regression. (arXiv:2110.13675v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13675","description":"<p>Bounding box (bbox) regression is a fundamental task in computer vision. So\nfar, the most commonly used loss functions for bbox regression are the\nIntersection over Union (IoU) loss and its variants. In this paper, we\ngeneralize existing IoU-based losses to a new family of power IoU losses that\nhave a power IoU term and an additional power regularization term with a single\npower parameter $\\alpha$. We call this new family of losses the $\\alpha$-IoU\nlosses and analyze properties such as order preservingness and loss/gradient\nreweighting. Experiments on multiple object detection benchmarks and models\ndemonstrate that $\\alpha$-IoU losses, 1) can surpass existing IoU-based losses\nby a noticeable performance margin; 2) offer detectors more flexibility in\nachieving different levels of bbox regression accuracy by modulating $\\alpha$;\nand 3) are more robust to small datasets and noisy bboxes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiabo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Erfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1\">James Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1\">Ying Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Personalized Diagnostic Generation Framework Based on Multi-source Heterogeneous Data. (arXiv:2110.13677v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13677","description":"<p>Personalized diagnoses have not been possible due to sear amount of data\npathologists have to bear during the day-to-day routine. This lead to the\ncurrent generalized standards that are being continuously updated as new\nfindings are reported. It is noticeable that these effective standards are\ndeveloped based on a multi-source heterogeneous data, including whole-slide\nimages and pathology and clinical reports. In this study, we propose a\nframework that combines pathological images and medical reports to generate a\npersonalized diagnosis result for individual patient. We use nuclei-level image\nfeature similarity and content-based deep learning method to search for a\npersonalized group of population with similar pathological characteristics,\nextract structured prognostic information from descriptive pathology reports of\nthe similar patient population, and assign importance of different prognostic\nfactors to generate a personalized pathological diagnosis result. We use\nmulti-source heterogeneous data from TCGA (The Cancer Genome Atlas) database.\nThe result demonstrate that our framework matches the performance of\npathologists in the diagnosis of renal cell carcinoma. This framework is\ndesigned to be generic, thus could be applied for other types of cancer. The\nweights could provide insights to the known prognostic factors and further\nguide more precise clinical treatment protocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zeyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haichuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruonan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tieliang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunbao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioIE: Biomedical Information Extraction with Multi-head Attention Enhanced Graph Convolutional Network. (arXiv:2110.13683v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13683","description":"<p>Constructing large-scaled medical knowledge graphs can significantly boost\nhealthcare applications for medical surveillance, bring much attention from\nrecent research. An essential step in constructing large-scale MKG is\nextracting information from medical reports. Recently, information extraction\ntechniques have been proposed and show promising performance in biomedical\ninformation extraction. However, these methods only consider limited types of\nentity and relation due to the noisy biomedical text data with complex entity\ncorrelations. Thus, they fail to provide enough information for constructing\nMKGs and restrict the downstream applications. To address this issue, we\npropose Biomedical Information Extraction, a hybrid neural network to extract\nrelations from biomedical text and unstructured medical reports. Our model\nutilizes a multi-head attention enhanced graph convolutional network to capture\nthe complex relations and context information while resisting the noise from\nthe data. We evaluate our model on two major biomedical relationship extraction\ntasks, chemical-disease relation and chemical-protein interaction, and a\ncross-hospital pan-cancer pathology report corpus. The results show that our\nmethod achieves superior performance than baselines. Furthermore, we evaluate\nthe applicability of our method under a transfer learning setting and show that\nBioIE achieves promising performance in processing medical text from different\nformats and writing styles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zeyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tieliang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunbao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Reference Learning for Fourier Phase Retrieval. (arXiv:2110.13688v1 [eess.IV])","link":"http://arxiv.org/abs/2110.13688","description":"<p>Reconstructing images from their Fourier magnitude measurements is a problem\nthat often arises in different research areas. This process is also referred to\nas phase retrieval. In this work, we consider a modified version of the phase\nretrieval problem, which allows for a reference image to be added onto the\nimage before the Fourier magnitudes are measured. We analyze an unrolled\nGerchberg-Saxton (GS) algorithm that can be used to learn a good reference\nimage from a dataset. Furthermore, we take a closer look at the learned\nreference images and propose a simple and efficient heuristic to construct\nreference images that, in some cases, yields reconstructions of comparable\nquality as approaches that learn references. Our code is available at\nhttps://github.com/tuelwer/reference-learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Uelwer_T/0/1/0/all/0/1\">Tobias Uelwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rucks_N/0/1/0/all/0/1\">Nick Rucks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harmeling_S/0/1/0/all/0/1\">Stefan Harmeling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Horizon Detection Algorithm for Maritime Surveillance. (arXiv:2110.13694v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13694","description":"<p>The horizon line is a valuable feature in the maritime environment as it has\na high persistence when compared to other features (e.g., shore corners,\nwaves). It is used in several applications, especially in maritime\nsurveillance. The task of horizon detection may be easy for humans, but it is\nhard on computers due to the high change of color and texture on maritime\nscenes. Moreover, the computational complexity is an important constraint to\ntake into account while developing the algorithm. In this paper, we propose a\nnew method that we expect to enhance the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zardoua_Y/0/1/0/all/0/1\">Yassir Zardoua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Astito Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_B/0/1/0/all/0/1\">Boulaala Mohammed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Addressing out-of-distribution label noise in webly-labelled data. (arXiv:2110.13699v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13699","description":"<p>A recurring focus of the deep learning community is towards reducing the\nlabeling effort. Data gathering and annotation using a search engine is a\nsimple alternative to generating a fully human-annotated and human-gathered\ndataset. Although web crawling is very time efficient, some of the retrieved\nimages are unavoidably noisy, i.e. incorrectly labeled. Designing robust\nalgorithms for training on noisy data gathered from the web is an important\nresearch perspective that would render the building of datasets easier. In this\npaper we conduct a study to understand the type of label noise to expect when\nbuilding a dataset using a search engine. We review the current limitations of\nstate-of-the-art methods for dealing with noisy labels for image classification\ntasks in the case of web noise distribution. We propose a simple solution to\nbridge the gap with a fully clean dataset using Dynamic Softening of\nOut-of-distribution Samples (DSOS), which we design on corrupted versions of\nthe CIFAR-100 dataset, and compare against state-of-the-art algorithms on the\nweb noise perturbated MiniImageNet and Stanford datasets and on real label\nnoise datasets: WebVision 1.0 and Clothing1M. Our work is fully reproducible\nhttps://git.io/JKGcj\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albert_P/0/1/0/all/0/1\">Paul Albert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortego_D/0/1/0/all/0/1\">Diego Ortego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arazo_E/0/1/0/all/0/1\">Eric Arazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1\">Noel O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TNTC: two-stream network with transformer-based complementarity for gait-based emotion recognition. (arXiv:2110.13708v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13708","description":"<p>Recognizing the human emotion automatically from visual characteristics plays\na vital role in many intelligent applications. Recently, gait-based emotion\nrecognition, especially gait skeletons-based characteristic, has attracted much\nattention, while many available methods have been proposed gradually. The\npopular pipeline is to first extract affective features from joint skeletons,\nand then aggregate the skeleton joint and affective features as the feature\nvector for classifying the emotion. However, the aggregation procedure of these\nemerged methods might be rigid, resulting in insufficiently exploiting the\ncomplementary relationship between skeleton joint and affective features.\nMeanwhile, the long range dependencies in both spatial and temporal domains of\nthe gait sequence are scarcely considered. To address these issues, we propose\na novel two-stream network with transformer-based complementarity, termed as\nTNTC. Skeleton joint and affective features are encoded into two individual\nimages as the inputs of two streams, respectively. A new transformer-based\ncomplementarity module (TCM) is proposed to bridge the complementarity between\ntwo streams hierarchically via capturing long range dependencies. Experimental\nresults demonstrate TNTC outperforms state-of-the-art methods on the latest\ndataset in terms of accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuanfei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_W/0/1/0/all/0/1\">Weijie Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinde Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLO-ReT: Towards High Accuracy Real-time Object Detection on Edge GPUs. (arXiv:2110.13713v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13713","description":"<p>Performance of object detection models has been growing rapidly on two major\nfronts, model accuracy and efficiency. However, in order to map deep neural\nnetwork (DNN) based object detection models to edge devices, one typically\nneeds to compress such models significantly, thus compromising the model\naccuracy. In this paper, we propose a novel edge GPU friendly module for\nmulti-scale feature interaction by exploiting missing combinatorial connections\nbetween various feature scales in existing state-of-the-art methods.\nAdditionally, we propose a novel transfer learning backbone adoption inspired\nby the changing translational information flow across various tasks, designed\nto complement our feature interaction module and together improve both accuracy\nas well as execution speed on various edge GPU devices available in the market.\nFor instance, YOLO-ReT with MobileNetV2x0.75 backbone runs real-time on Jetson\nNano, and achieves 68.75 mAP on Pascal VOC and 34.91 mAP on COCO, beating its\npeers by 3.05 mAP and 0.91 mAP respectively, while executing faster by 3.05\nFPS. Furthermore, introducing our multi-scale feature interaction module in\nYOLOv4-tiny and YOLOv4-tiny (3l) improves their performance to 41.5 and 48.1\nmAP respectively on COCO, outperforming the original versions by 1.3 and 0.9\nmAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_P/0/1/0/all/0/1\">Prakhar Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Deming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winslett_M/0/1/0/all/0/1\">Marianne Winslett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised dry herbage mass estimation using automatic data and synthetic images. (arXiv:2110.13719v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13719","description":"<p>Monitoring species-specific dry herbage biomass is an important aspect of\npasture-based milk production systems. Being aware of the herbage biomass in\nthe field enables farmers to manage surpluses and deficits in herbage supply,\nas well as using targeted nitrogen fertilization when necessary. Deep learning\nfor computer vision is a powerful tool in this context as it can accurately\nestimate the dry biomass of a herbage parcel using images of the grass canopy\ntaken using a portable device. However, the performance of deep learning comes\nat the cost of an extensive, and in this case destructive, data gathering\nprocess. Since accurate species-specific biomass estimation is labor intensive\nand destructive for the herbage parcel, we propose in this paper to study low\nsupervision approaches to dry biomass estimation using computer vision. Our\ncontributions include: a synthetic data generation algorithm to generate data\nfor a herbage height aware semantic segmentation task, an automatic process to\nlabel data using semantic segmentation maps, and a robust regression network\ntrained to predict dry biomass using approximate biomass labels and a small\ntrusted dataset with gold standard labels. We design our approach on a herbage\nmass estimation dataset collected in Ireland and also report state-of-the-art\nresults on the publicly released Grass-Clover biomass estimation dataset from\nDenmark. Our code is available at https://git.io/J0L2a\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albert_P/0/1/0/all/0/1\">Paul Albert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadeldin_M/0/1/0/all/0/1\">Mohamed Saadeldin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_B/0/1/0/all/0/1\">Badri Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennessy_D/0/1/0/all/0/1\">Deirdre Hennessy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_A/0/1/0/all/0/1\">Aisling O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1\">Noel O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep DIC: Deep Learning-Based Digital Image Correlation for End-to-End Displacement and Strain Measurement. (arXiv:2110.13720v1 [eess.IV])","link":"http://arxiv.org/abs/2110.13720","description":"<p>Digital image correlation (DIC) has become an industry standard to retrieve\naccurate displacement and strain measurement in tensile testing and other\nmaterial characterization. Though traditional DIC offers a high precision\nestimation of deformation for general tensile testing cases, the prediction\nbecomes unstable at large deformation or when the speckle patterns start to\ntear. In addition, traditional DIC requires a long computation time and often\nproduces a low spatial resolution output affected by filtering and speckle\npattern quality. To address these challenges, we propose a new deep\nlearning-based DIC approach -- Deep DIC, in which two convolutional neural\nnetworks, DisplacementNet and StrainNet, are designed to work together for\nend-to-end prediction of displacements and strains. DisplacementNet predicts\nthe displacement field and adaptively tracks the change of a region of\ninterest. StrainNet predicts the strain field directly from the image input\nwithout relying on the displacement prediction, which significantly improves\nthe strain prediction accuracy. A new dataset generation method is proposed to\nsynthesize a realistic and comprehensive dataset including artificial speckle\npatterns, randomly generated displacement and strain fields, and deformed\nimages based on the given deformation. Proposed Deep DIC is trained purely on a\nsynthetic dataset, but designed to perform both on simulated and experimental\ndata. Its performance is systematically evaluated and compared with commercial\nDIC software. Deep DIC gives highly consistent and comparable predictions of\ndisplacement and strain with those obtained from commercial DIC software, while\nit outperforms commercial software with very robust strain prediction even with\nlarge and localized deformation and varied pattern qualities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ru Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_D/0/1/0/all/0/1\">Danielle Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1\">Ping Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DP-SSL: Towards Robust Semi-supervised Learning with A Few Labeled Samples. (arXiv:2110.13740v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13740","description":"<p>The scarcity of labeled data is a critical obstacle to deep learning.\nSemi-supervised learning (SSL) provides a promising way to leverage unlabeled\ndata by pseudo labels. However, when the size of labeled data is very small\n(say a few labeled samples per class), SSL performs poorly and unstably,\npossibly due to the low quality of learned pseudo labels. In this paper, we\npropose a new SSL method called DP-SSL that adopts an innovative data\nprogramming (DP) scheme to generate probabilistic labels for unlabeled data.\nDifferent from existing DP methods that rely on human experts to provide\ninitial labeling functions (LFs), we develop a multiple-choice learning~(MCL)\nbased approach to automatically generate LFs from scratch in SSL style. With\nthe noisy labels produced by the LFs, we design a label model to resolve the\nconflict and overlap among the noisy labels, and finally infer probabilistic\nlabels for unlabeled samples. Extensive experiments on four standard SSL\nbenchmarks show that DP-SSL can provide reliable labels for unlabeled data and\nachieve better classification performance on test sets than existing SSL\nmethods, especially when only a small number of labeled samples are available.\nConcretely, for CIFAR-10 with only 40 labeled samples, DP-SSL achieves 93.82%\nannotation accuracy on unlabeled data and 93.46% classification accuracy on\ntest data, which are higher than the SOTA results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiandong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuigeng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Multi-view Registration of Point Sets with Laplacian Mixture Model. (arXiv:2110.13744v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13744","description":"<p>Point set registration is an essential step in many computer vision\napplications, such as 3D reconstruction and SLAM. Although there exist many\nregistration algorithms for different purposes, however, this topic is still\nchallenging due to the increasing complexity of various real-world scenarios,\nsuch as heavy noise and outlier contamination. In this paper, we propose a\nnovel probabilistic generative method to simultaneously align multiple point\nsets based on the heavy-tailed Laplacian distribution. The proposed method\nassumes each data point is generated by a Laplacian Mixture Model (LMM), where\nits centers are determined by the corresponding points in other point sets.\nDifferent from the previous Gaussian Mixture Model (GMM) based method, which\nminimizes the quadratic distance between points and centers of Gaussian\nprobability density, LMM minimizes the sparsity-induced L1 distance, thereby it\nis more robust against noise and outliers. We adopt Expectation-Maximization\n(EM) framework to solve LMM parameters and rigid transformations. We\napproximate the L1 optimization as a linear programming problem by exponential\nmapping in Lie algebra, which can be effectively solved through the interior\npoint method. To improve efficiency, we also solve the L1 optimization by\nAlternating Direction Multiplier Method (ADMM). We demonstrate the advantages\nof our method by comparing it with representative state-of-the-art approaches\non benchmark challenging data sets, in terms of robustness and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Dong-Ming Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction of Humans in Motion. (arXiv:2110.13746v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13746","description":"<p>We present H-NeRF, neural radiance fields for rendering and temporal (4D)\nreconstruction of a human in motion as captured by a sparse set of cameras or\neven from a monocular video. Our NeRF-inspired approach combines ideas from\nneural scene representation, novel-view synthesis, and implicit statistical\ngeometric human representations. H-NeRF allows to accurately synthesize images\nof the observed subject under novel camera views and human poses. Instead of\nlearning a radiance field in empty space, we attach it to a structured implicit\nhuman body model, represented using signed distance functions. This allows us\nto robustly fuse information from sparse views and, at test time, to\nextrapolate beyond the observed poses or views. Moreover, we apply geometric\nconstraints to co-learn the structure of the observed subject (including both\nbody and clothing) and to regularize the radiance field to geometrical\nplausible solutions. Extensive experiments on multiple datasets demonstrate the\nrobustness and accuracy of our approach and its generalization capabilities\nbeyond the sparse training set of poses and views.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alldieck_T/0/1/0/all/0/1\">Thiemo Alldieck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DPCOVID: Privacy-Preserving Federated Covid-19 Detection. (arXiv:2110.13760v1 [cs.CR])","link":"http://arxiv.org/abs/2110.13760","description":"<p>Coronavirus (COVID-19) has shown an unprecedented global crisis by the\ndetrimental effect on the global economy and health. The number of COVID-19\ncases has been rapidly increasing, and there is no sign of stopping. It leads\nto a severe shortage of test kits and accurate detection models. A recent study\ndemonstrated that the chest X-ray radiography outperformed laboratory testing\nin COVID-19 detection. Therefore, using chest X-ray radiography analysis can\nhelp to screen suspected COVID-19 cases at an early stage. Moreover, the\npatient data is sensitive, and it must be protected to avoid revealing through\nmodel updates and reconstruction from the malicious attacker. In this paper, we\npresent a privacy-preserving Federated Learning system for COVID-19 detection\nbased on chest X-ray images. First, a Federated Learning system is constructed\nfrom chest X-ray images. The main idea is to build a decentralized model across\nmultiple hospitals without sharing data among hospitals. Second, we first show\nthat the accuracy of Federated Learning for COVID-19 identification reduces\nsignificantly for Non-IID data. We then propose a strategy to improve model's\naccuracy on Non-IID COVID-19 data by increasing the total number of clients,\nparallelism (client fraction), and computation per client. Finally, we apply a\nDifferential Privacy Stochastic Gradient Descent (DP-SGD) to enhance the\npreserving of patient data privacy for our Federated Learning model. A strategy\nis also proposed to keep the robustness of Federated Learning to ensure the\nsecurity and accuracy of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1\">Trang-Thi Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yennun-Huang/0/1/0/all/0/1\">Yennun-Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AugMax: Adversarial Composition of Random Augmentations for Robust Training. (arXiv:2110.13771v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13771","description":"<p>Data augmentation is a simple yet effective way to improve the robustness of\ndeep neural networks (DNNs). Diversity and hardness are two complementary\ndimensions of data augmentation to achieve robustness. For example, AugMix\nexplores random compositions of a diverse set of augmentations to enhance\nbroader coverage, while adversarial training generates adversarially hard\nsamples to spot the weakness. Motivated by this, we propose a data augmentation\nframework, termed AugMax, to unify the two aspects of diversity and hardness.\nAugMax first randomly samples multiple augmentation operators and then learns\nan adversarial mixture of the selected operators. Being a stronger form of data\naugmentation, AugMax leads to a significantly augmented input distribution\nwhich makes model training more challenging. To solve this problem, we further\ndesign a disentangled normalization module, termed DuBIN\n(Dual-Batch-and-Instance Normalization), that disentangles the instance-wise\nfeature heterogeneity arising from AugMax. Experiments show that AugMax-DuBIN\nleads to significantly improved out-of-distribution robustness, outperforming\nprior arts by 3.03%, 3.49%, 1.82% and 0.71% on CIFAR10-C, CIFAR100-C, Tiny\nImageNet-C and ImageNet-C. Codes and pretrained models are available:\nhttps://github.com/VITA-Group/AugMax.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haotao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1\">Jean Kossaifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramidal Blur Aware X-Corner Chessboard Detector. (arXiv:2110.13793v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13793","description":"<p>With camera resolution ever increasing and the need to rapidly recalibrate\nrobotic platforms in less than ideal environments, there is a need for faster\nand more robust chessboard fiducial marker detectors. A new chessboard detector\nis proposed that is specifically designed for: high resolution images,\nfocus/motion blur, harsh lighting conditions, and background clutter. This is\naccomplished using a new x-corner detector, where for the first time blur is\nestimated and used in a novel way to enhance corner localization, edge\nvalidation, and connectivity. Performance is measured and compared against\nother libraries using a diverse set of images created by combining multiple\nthird party datasets and including new specially crafted scenarios designed to\nstress the state-of-the-art. The proposed detector has the best F1- Score of\n0.97, runs 1.9x faster than next fastest, and is a top performer for corner\naccuracy, while being the only detector to have consistent good performance in\nall scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abeles_P/0/1/0/all/0/1\">Peter Abeles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting speaking persons in video. (arXiv:2110.13806v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13806","description":"<p>We present a novel method for detecting speaking persons in video, by\nextracting facial landmarks with a neural network and analysing these landmarks\nstatistically over time\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fassold_H/0/1/0/all/0/1\">Hannes Fassold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Segmentation for Urban-Scene Images. (arXiv:2110.13813v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13813","description":"<p>Urban-scene Image segmentation is an important and trending topic in computer\nvision with wide use cases like autonomous driving [1]. Starting with the\nbreakthrough work of Long et al. [2] that introduces Fully Convolutional\nNetworks (FCNs), the development of novel architectures and practical uses of\nneural networks in semantic segmentation has been expedited in the recent 5\nyears. Aside from seeking solutions in general model design for information\nshrinkage due to pooling, urban-scene image itself has intrinsic features like\npositional patterns [3]. Our project seeks an advanced and integrated solution\nthat specifically targets urban-scene image semantic segmentation among the\nmost novel approaches in the current field. We re-implement the cutting edge\nmodel DeepLabv3+ [4] with ResNet-101 [5] backbone as our strong baseline model.\nBased upon DeepLabv3+, we incorporate HANet [3] to account for the vertical\nspatial priors in urban-scene image tasks. To boost up model efficiency and\nperformance, we further explore the Atrous Spatial Pooling (ASP) layer in\nDeepLabv3+ and infuse a computational efficient variation called \"Waterfall\"\nAtrous Spatial Pooling (WASP) [6] architecture in our model. We find that our\ntwo-step integrated model improves the mean Intersection-Over-Union (mIoU)\nscore gradually from the baseline model. In particular, HANet successfully\nidentifies height-driven patterns and improves per-class IoU of common class\nlabels in urban scenario like fence and bus. We also demonstrate the\nimprovement of model efficiency with help of WASP in terms of computational\ntimes during training and parameter reduction from the original ASPP module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shorya Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CloudFindr: A Deep Learning Cloud Artifact Masker for Satellite DEM Data. (arXiv:2110.13819v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13819","description":"<p>Artifact removal is an integral component of cinematic scientific\nvisualization, and is especially challenging with big datasets in which\nartifacts are difficult to define. In this paper, we describe a method for\ncreating cloud artifact masks which can be used to remove artifacts from\nsatellite imagery using a combination of traditional image processing together\nwith deep learning based on U-Net. Compared to previous methods, our approach\ndoes not require multi-channel spectral imagery but performs successfully on\nsingle-channel Digital Elevation Models (DEMs). DEMs are a representation of\nthe topography of the Earth and have a variety applications including planetary\nscience, geology, flood modeling, and city planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borkiewicz_K/0/1/0/all/0/1\">Kalina Borkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1\">Viraj Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naiman_J/0/1/0/all/0/1\">J.P. Naiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chuanyue Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Stuart Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpenter_J/0/1/0/all/0/1\">Jeff Carpenter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time division-of-focal-plane polarization imaging system with progressive networks. (arXiv:2110.13823v1 [eess.IV])","link":"http://arxiv.org/abs/2110.13823","description":"<p>Division-of-focal-plane (DoFP) polarization imaging technical recently has\nbeen applied in many fields. However, the images captured by such sensors\ncannot be used directly because they suffer from instantaneous field-of-view\nerrors and low resolution problem. This paper builds a fast DoFP demosaicing\nsystem with proposed progressive polarization demosaicing convolutional neural\nnetwork (PPDN), which is specifically designed for edge-side GPU devices like\nNavidia Jetson TX2. The proposed network consists of two parts: reconstruction\nstage and refining stage. The former recovers four polarization channels from a\nsingle DoFP image. The latter fine-tune the four channels to obtain more\naccurate polarization information. PPDN can be implemented in another version:\nPPDN-L (large), for the platforms of high computing resources. Experiments show\nthat PPDN can compete with the best existing methods with fewer parameters and\nfaster inference speed and meet the real-time demands of imaging system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_R/0/1/0/all/0/1\">Rongyuan Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yongqiang Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1\">Ning Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_S/0/1/0/all/0/1\">Seong G.Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Light-weight Interpretable CompositionalNetwork for Nuclei Detection and Weakly-supervised Segmentation. (arXiv:2110.13846v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13846","description":"<p>The field of computational pathology has witnessed great advancements since\ndeep neural networks have been widely applied. These deep neural networks\nusually require large numbers of annotated data to train vast parameters.\nHowever, it takes significant effort to annotate a large histopathology\ndataset. We propose to build a data-efficient model, which only requires\npartial annotation, specifically on isolated nucleus, rather than on the whole\nslide image. It exploits shallow features as its backbone and is light-weight,\ntherefore a small number of data is sufficient for training. What's more, it is\na generative compositional model, which enjoys interpretability in its\nprediction. The proposed method could be an alternative solution for the\ndata-hungry problem of deep learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seyoun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1\">Benjamin Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engle_E/0/1/0/all/0/1\">Elizabeth Engle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almodovar_G/0/1/0/all/0/1\">Guillermo Almodovar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walk_R/0/1/0/all/0/1\">Ryan Walk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_Diaz_S/0/1/0/all/0/1\">Sigfredo Soto-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taube_J/0/1/0/all/0/1\">Janis Taube</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szalay_A/0/1/0/all/0/1\">Alex Szalay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defensive Tensorization. (arXiv:2110.13859v1 [cs.LG])","link":"http://arxiv.org/abs/2110.13859","description":"<p>We propose defensive tensorization, an adversarial defence technique that\nleverages a latent high-order factorization of the network. The layers of a\nnetwork are first expressed as factorized tensor layers. Tensor dropout is then\napplied in the latent subspace, therefore resulting in dense reconstructed\nweights, without the sparsity or perturbations typically induced by the\nrandomization.Our approach can be readily integrated with any arbitrary neural\narchitecture and combined with techniques like adversarial training. We\nempirically demonstrate the effectiveness of our approach on standard image\nclassification benchmarks. We validate the versatility of our approach across\ndomains and low-precision architectures by considering an audio classification\ntask and binary networks. In all cases, we demonstrate improved performance\ncompared to prior works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bulat_A/0/1/0/all/0/1\">Adrian Bulat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1\">Jean Kossaifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sourav Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagakis_Y/0/1/0/all/0/1\">Yannis Panagakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy Hospedales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1\">Georgios Tzimiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D Lane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FL-WBC: Enhancing Robustness against Model Poisoning Attacks in Federated Learning from a Client Perspective. (arXiv:2110.13864v1 [cs.LG])","link":"http://arxiv.org/abs/2110.13864","description":"<p>Federated learning (FL) is a popular distributed learning framework that\ntrains a global model through iterative communications between a central server\nand edge devices. Recent works have demonstrated that FL is vulnerable to model\npoisoning attacks. Several server-based defense approaches (e.g. robust\naggregation), have been proposed to mitigate such attacks. However, we\nempirically show that under extremely strong attacks, these defensive methods\nfail to guarantee the robustness of FL. More importantly, we observe that as\nlong as the global model is polluted, the impact of attacks on the global model\nwill remain in subsequent rounds even if there are no subsequent attacks. In\nthis work, we propose a client-based defense, named White Blood Cell for\nFederated Learning (FL-WBC), which can mitigate model poisoning attacks that\nhave already polluted the global model. The key idea of FL-WBC is to identify\nthe parameter space where long-lasting attack effect on parameters resides and\nperturb that space during local training. Furthermore, we derive a certified\nrobustness guarantee against model poisoning attacks and a convergence\nguarantee to FedAvg after applying our FL-WBC. We conduct experiments on\nFasionMNIST and CIFAR10 to evaluate the defense against state-of-the-art model\npoisoning attacks. The results demonstrate that our method can effectively\nmitigate model poisoning attack impact on the global model within 5\ncommunication rounds with nearly no accuracy drop under both IID and Non-IID\nsettings. Our defense is also complementary to existing server-based robust\naggregation approaches and can further improve the robustness of FL under\nextremely strong attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jingwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DiValentin_L/0/1/0/all/0/1\">Louis DiValentin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassanzadeh_A/0/1/0/all/0/1\">Amin Hassanzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hai Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HR-RCNN: Hierarchical Relational Reasoning for Object Detection. (arXiv:2110.13892v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13892","description":"<p>Incorporating relational reasoning in neural networks for object recognition\nremains an open problem. Although many attempts have been made for relational\nreasoning, they generally only consider a single type of relationship. For\nexample, pixel relations through self-attention (e.g., non-local networks),\nscale relations through feature fusion (e.g., feature pyramid networks), or\nobject relations through graph convolutions (e.g., reasoning-RCNN). Little\nattention has been given to more generalized frameworks that can reason across\nthese relationships. In this paper, we propose a hierarchical relational\nreasoning framework (HR-RCNN) for object detection, which utilizes a novel\ngraph attention module (GAM). This GAM is a concise module that enables\nreasoning across heterogeneous nodes by operating on the graph edges directly.\nLeveraging heterogeneous relationships, our HR-RCNN shows great improvement on\nCOCO dataset, for both object detection and instance segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRV: Neural Representations for Videos. (arXiv:2110.13903v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13903","description":"<p>We propose a novel neural representation for videos (NeRV) which encodes\nvideos in neural networks. Unlike conventional representations that treat\nvideos as frame sequences, we represent videos as neural networks taking frame\nindex as input. Given a frame index, NeRV outputs the corresponding RGB image.\nVideo encoding in NeRV is simply fitting a neural network to video frames and\ndecoding process is a simple feedforward operation. As an image-wise implicit\nrepresentation, NeRV output the whole image and shows great efficiency compared\nto pixel-wise implicit representation, improving the encoding speed by 25x to\n70x, the decoding speed by 38x to 132x, while achieving better video quality.\nWith such a representation, we can treat videos as neural networks, simplifying\nseveral video-related tasks. For example, conventional video compression\nmethods are restricted by a long and complex pipeline, specifically designed\nfor the task. In contrast, with NeRV, we can use any neural network compression\nmethod as a proxy for video compression, and achieve comparable performance to\ntraditional frame-based video compression approaches (H.264, HEVC \\etc).\nBesides compression, we demonstrate the generalization of NeRV for video\ndenoising. The source code and pre-trained model can be found at\nhttps://github.com/haochen-rye/NeRV.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yixuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overinterpretation reveals image classification model pathologies. (arXiv:2003.08907v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2003.08907","description":"<p>Image classifiers are typically scored on their test set accuracy, but high\naccuracy can mask a subtle type of model failure. We find that high scoring\nconvolutional neural networks (CNNs) on popular benchmarks exhibit troubling\npathologies that allow them to display high accuracy even in the absence of\nsemantically salient features. When a model provides a high-confidence decision\nwithout salient supporting input features, we say the classifier has\noverinterpreted its input, finding too much class-evidence in patterns that\nappear nonsensical to humans. Here, we demonstrate that neural networks trained\non CIFAR-10 and ImageNet suffer from overinterpretation, and we find models on\nCIFAR-10 make confident predictions even when 95% of input images are masked\nand humans cannot discern salient features in the remaining pixel-subsets. We\nintroduce Batched Gradient SIS, a new method for discovering sufficient input\nsubsets for complex datasets, and use this method to show the sufficiency of\nborder pixels in ImageNet for training and testing. Although these patterns\nportend potential model fragility in real-world deployment, they are in fact\nvalid statistical patterns of the benchmark that alone suffice to attain high\ntest accuracy. Unlike adversarial examples, overinterpretation relies upon\nunmodified image pixels. We find ensembling and input dropout can each help\nmitigate overinterpretation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carter_B/0/1/0/all/0/1\">Brandon Carter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Siddhartha Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Jonas Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gifford_D/0/1/0/all/0/1\">David Gifford</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"360-Degree Gaze Estimation in the Wild Using Multiple Zoom Scales. (arXiv:2009.06924v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.06924","description":"<p>Gaze estimation involves predicting where the person is looking at within an\nimage or video. Technically, the gaze information can be inferred from two\ndifferent magnification levels: face orientation and eye orientation. The\ninference is not always feasible for gaze estimation in the wild, given the\nlack of clear eye patches in conditions like extreme left/right gazes or\nocclusions. In this work, we design a model that mimics humans' ability to\nestimate the gaze by aggregating from focused looks, each at a different\nmagnification level of the face area. The model avoids the need to extract\nclear eye patches and at the same time addresses another important issue of\nface-scale variation for gaze estimation in the wild. We further extend the\nmodel to handle the challenging task of 360-degree gaze estimation by encoding\nthe backward gazes in the polar representation along with a robust averaging\nscheme. Experiment results on the ETH-XGaze dataset, which does not contain\nscale-varying faces, demonstrate the model's effectiveness to assimilate\ninformation from multiple scales. For other benchmark datasets with many\nscale-varying faces (Gaze360 and RT-GENE), the proposed model achieves\nstate-of-the-art performance for gaze estimation when using either images or\nvideos. Our code and pretrained models can be accessed at\nhttps://github.com/ashesh-0/MultiZoomGaze.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashesh/0/1/0/all/0/1\">Ashesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chu-Song Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsuan-Tien Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geography-Aware Self-Supervised Learning. (arXiv:2011.09980v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.09980","description":"<p>Contrastive learning methods have significantly narrowed the gap between\nsupervised and unsupervised learning on computer vision tasks. In this paper,\nwe explore their application to geo-located datasets, e.g. remote sensing,\nwhere unlabeled data is often abundant but labeled data is scarce. We first\nshow that due to their different characteristics, a non-trivial gap persists\nbetween contrastive and supervised learning on standard benchmarks. To close\nthe gap, we propose novel training methods that exploit the spatio-temporal\nstructure of remote sensing data. We leverage spatially aligned images over\ntime to construct temporal positive pairs in contrastive learning and\ngeo-location to design pre-text tasks. Our experiments show that our proposed\nmethod closes the gap between contrastive and supervised learning on image\nclassification, object detection and semantic segmentation for remote sensing.\nMoreover, we demonstrate that the proposed method can also be applied to\ngeo-tagged ImageNet images, improving downstream performance on various tasks.\nProject Webpage can be found at this link geography-aware-ssl.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayush_K/0/1/0/all/0/1\">Kumar Ayush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzkent_B/0/1/0/all/0/1\">Burak Uzkent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chenlin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanmay_K/0/1/0/all/0/1\">Kumar Tanmay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1\">Marshall Burke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1\">David Lobell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Partially Supervised Multi-Structure Medical Image Segmentation on Small-Scale Data. (arXiv:2011.14164v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14164","description":"<p>The data-driven nature of deep learning (DL) models for semantic segmentation\nrequires a large number of pixel-level annotations. However, large-scale and\nfully labeled medical datasets are often unavailable for practical tasks.\nRecently, partially supervised methods have been proposed to utilize images\nwith incomplete labels in the medical domain. To bridge the methodological gaps\nin partially supervised learning (PSL) under data scarcity, we propose Vicinal\nLabels Under Uncertainty (VLUU), a simple yet efficient framework utilizing the\nhuman structure similarity for partially supervised medical image segmentation.\nMotivated by multi-task learning and vicinal risk minimization, VLUU transforms\nthe partially supervised problem into a fully supervised problem by generating\nvicinal labels. We systematically evaluate VLUU under the challenges of\nsmall-scale data, dataset shift, and class imbalance on two commonly used\nsegmentation datasets for the tasks of chest organ segmentation and optic\ndisc-and-cup segmentation. The experimental results show that VLUU can\nconsistently outperform previous partially supervised models in these settings.\nOur research suggests a new research direction in label-efficient deep learning\nwith partial supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Nanqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael Kampffmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voiculescu_I/0/1/0/all/0/1\">Irina Voiculescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing implicit hierarchical structure in 3D biomedical images with self-supervised hyperbolic representations. (arXiv:2012.01644v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01644","description":"<p>We consider the task of representation learning for unsupervised segmentation\nof 3D voxel-grid biomedical images. We show that models that capture implicit\nhierarchical relationships between subvolumes are better suited for this task.\nTo that end, we consider encoder-decoder architectures with a hyperbolic latent\nspace, to explicitly capture hierarchical relationships present in subvolumes\nof the data. We propose utilizing a 3D hyperbolic variational autoencoder with\na novel gyroplane convolutional layer to map from the embedding space back to\n3D images. To capture these relationships, we introduce an essential\nself-supervised loss -- in addition to the standard VAE loss -- which infers\napproximate hierarchies and encourages implicitly related subvolumes to be\nmapped closer in the embedding space. We present experiments on both synthetic\ndata and biomedical data to validate our hypothesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_J/0/1/0/all/0/1\">Joy Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jeffrey Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gong-Her Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wah Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Serena Yeung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention over learned object embeddings enables complex visual reasoning. (arXiv:2012.08508v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.08508","description":"<p>Neural networks have achieved success in a wide array of perceptual tasks but\noften fail at tasks involving both perception and higher-level reasoning. On\nthese more challenging tasks, bespoke approaches (such as modular symbolic\ncomponents, independent dynamics models or semantic parsers) targeted towards\nthat specific type of task have typically performed better. The downside to\nthese targeted approaches, however, is that they can be more brittle than\ngeneral-purpose neural networks, requiring significant modification or even\nredesign according to the particular task at hand. Here, we propose a more\ngeneral neural-network-based approach to dynamic visual reasoning problems that\nobtains state-of-the-art performance on three different domains, in each case\noutperforming bespoke modular approaches tailored specifically to the task. Our\nmethod relies on learned object-centric representations, self-attention and\nself-supervised dynamics learning, and all three elements together are required\nfor strong performance to emerge. The success of this combination suggests that\nthere may be no need to trade off flexibility for performance on problems\ninvolving spatio-temporal or causal-style reasoning. With the right soft biases\nand learning objectives in a neural network we may be able to attain the best\nof both worlds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_M/0/1/0/all/0/1\">Malcolm Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matt Botvinick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AQuA: Analytical Quality Assessment for Optimizing Video Analytics Systems. (arXiv:2101.09752v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.09752","description":"<p>Millions of cameras at edge are being deployed to power a variety of\ndifferent deep learning applications. However, the frames captured by these\ncameras are not always pristine - they can be distorted due to lighting issues,\nsensor noise, compression etc. Such distortions not only deteriorate visual\nquality, they impact the accuracy of deep learning applications that process\nsuch video streams. In this work, we introduce AQuA, to protect application\naccuracy against such distorted frames by scoring the level of distortion in\nthe frames. It takes into account the analytical quality of frames, not the\nvisual quality, by learning a novel metric, classifier opinion score, and uses\na lightweight, CNN-based, object-independent feature extractor. AQuA accurately\nscores distortion levels of frames and generalizes to multiple different deep\nlearning applications. When used for filtering poor quality frames at edge, it\nreduces high-confidence errors for analytics applications by 17%. Through\nfiltering, and due to its low overhead (14ms), AQuA can also reduce computation\ntime and average bandwidth usage by 25%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Paul_S/0/1/0/all/0/1\">Sibendu Paul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drolia_U/0/1/0/all/0/1\">Utsav Drolia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Y. Charlie Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakradhar_S/0/1/0/all/0/1\">Srimat T. Chakradhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape, Appearance, and Pose. (arXiv:2102.06199v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.06199","description":"<p>While deep learning reshaped the classical motion capture pipeline with\nfeed-forward networks, generative models are required to recover fine alignment\nvia iterative refinement. Unfortunately, the existing models are usually\nhand-crafted or learned in controlled conditions, only applicable to limited\ndomains. We propose a method to learn a generative neural body model from\nunlabelled monocular videos by extending Neural Radiance Fields (NeRFs). We\nequip them with a skeleton to apply to time-varying and articulated motion. A\nkey insight is that implicit models require the inverse of the forward\nkinematics used in explicit surface models. Our reparameterization defines\nspatial latent variables relative to the pose of body parts and thereby\novercomes ill-posed inverse operations with an overparameterization. This\nenables learning volumetric body shape and appearance from scratch while\njointly refining the articulated pose; all without ground truth labels for\nappearance, pose, or 3D shape on the input videos. When used for\nnovel-view-synthesis and motion capture, our neural model improves accuracy on\ndiverse datasets. Project website: https://lemonatsu.github.io/anerf/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shih-Yang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Frank Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Lane Detection via Expanded Self Attention. (arXiv:2102.07037v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.07037","description":"<p>The image-based lane detection algorithm is one of the key technologies in\nautonomous vehicles. Modern deep learning methods achieve high performance in\nlane detection, but it is still difficult to accurately detect lanes in\nchallenging situations such as congested roads and extreme lighting conditions.\nTo be robust on these challenging situations, it is important to extract global\ncontextual information even from limited visual cues. In this paper, we propose\na simple but powerful self-attention mechanism optimized for lane detection\ncalled the Expanded Self Attention (ESA) module. Inspired by the simple\ngeometric structure of lanes, the proposed method predicts the confidence of a\nlane along the vertical and horizontal directions in an image. The prediction\nof the confidence enables estimating occluded locations by extracting global\ncontextual information. ESA module can be easily implemented and applied to any\nencoder-decoder-based model without increasing the inference time. The\nperformance of our method is evaluated on three popular lane detection\nbenchmarks (TuSimple, CULane and BDD100K). We achieve state-of-the-art\nperformance in CULane and BDD100K and distinct improvement on TuSimple dataset.\nThe experimental results show that our approach is robust to occlusion and\nextreme lighting conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junhyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dogyoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Woojin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sangwon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SWAD: Domain Generalization by Seeking Flat Minima. (arXiv:2102.08604v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.08604","description":"<p>Domain generalization (DG) methods aim to achieve generalizability to an\nunseen target domain by using only training data from the source domains.\nAlthough a variety of DG methods have been proposed, a recent study shows that\nunder a fair evaluation protocol, called DomainBed, the simple empirical risk\nminimization (ERM) approach works comparable to or even outperforms previous\nmethods. Unfortunately, simply solving ERM on a complex, non-convex loss\nfunction can easily lead to sub-optimal generalizability by seeking sharp\nminima. In this paper, we theoretically show that finding flat minima results\nin a smaller domain generalization gap. We also propose a simple yet effective\nmethod, named Stochastic Weight Averaging Densely (SWAD), to find flat minima.\nSWAD finds flatter minima and suffers less from overfitting than does the\nvanilla SWA by a dense and overfit-aware stochastic weight sampling strategy.\nSWAD shows state-of-the-art performances on five DG benchmarks, namely PACS,\nVLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large\nmargins of +1.6% averagely on out-of-domain accuracy. We also compare SWAD with\nconventional generalization methods, such as data augmentation and consistency\nregularization methods, to verify that the remarkable performance improvements\nare originated from by seeking flat minima, not from better in-domain\ngeneralizability. Last but not least, SWAD is readily adaptable to existing DG\nmethods without modification; the combination of SWAD and an existing DG method\nfurther improves DG performances. Source code is available at\nhttps://github.com/khanrc/swad.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1\">Junbum Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Han-Cheol Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yunsung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Input Gradients Highlight Discriminative Features?. (arXiv:2102.12781v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.12781","description":"<p>Post-hoc gradient-based interpretability methods [Simonyan et al., 2013,\nSmilkov et al., 2017] that provide instance-specific explanations of model\npredictions are often based on assumption (A): magnitude of input gradients --\ngradients of logits with respect to input -- noisily highlight discriminative\ntask-relevant features. In this work, we test the validity of assumption (A)\nusing a three-pronged approach. First, we develop an evaluation framework,\nDiffROAR, to test assumption (A) on four image classification benchmarks. Our\nresults suggest that (i) input gradients of standard models (i.e., trained on\noriginal data) may grossly violate (A), whereas (ii) input gradients of\nadversarially robust models satisfy (A). Second, we introduce BlockMNIST, an\nMNIST-based semi-real dataset, that by design encodes a priori knowledge of\ndiscriminative features. Our analysis on BlockMNIST leverages this information\nto validate as well as characterize differences between input gradient\nattributions of standard and robust models. Finally, we theoretically prove\nthat our empirical findings hold on a simplified version of the BlockMNIST\ndataset. Specifically, we prove that input gradients of standard\none-hidden-layer MLPs trained on this dataset do not highlight\ninstance-specific signal coordinates, thus grossly violating assumption (A).\nOur findings motivate the need to formalize and test common assumptions in\ninterpretability in a falsifiable manner [Leavitt and Morcos, 2020]. We believe\nthat the DiffROAR evaluation framework and BlockMNIST-based datasets can serve\nas sanity checks to audit instance-specific interpretability methods; code and\ndata available at https://github.com/harshays/inputgradients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1\">Harshay Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Prateek Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1\">Praneeth Netrapalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer in Transformer. (arXiv:2103.00112v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.00112","description":"<p>Transformer is a new kind of neural architecture which encodes the input data\nas powerful features via the attention mechanism. Basically, the visual\ntransformers first divide the input images into several local patches and then\ncalculate both representations and their relationship. Since natural images are\nof high complexity with abundant detail and color information, the granularity\nof the patch dividing is not fine enough for excavating features of objects in\ndifferent scales and locations. In this paper, we point out that the attention\ninside these local patches are also essential for building visual transformers\nwith high performance and we explore a new architecture, namely, Transformer iN\nTransformer (TNT). Specifically, we regard the local patches (e.g.,\n16$\\times$16) as \"visual sentences\" and present to further divide them into\nsmaller patches (e.g., 4$\\times$4) as \"visual words\". The attention of each\nword will be calculated with other words in the given visual sentence with\nnegligible computational costs. Features of both words and sentences will be\naggregated to enhance the representation ability. Experiments on several\nbenchmarks demonstrate the effectiveness of the proposed TNT architecture,\ne.g., we achieve an 81.5% top-1 accuracy on the ImageNet, which is about 1.7%\nhigher than that of the state-of-the-art visual transformer with similar\ncomputational cost. The PyTorch code is available at\nhttps://github.com/huawei-noah/CV-Backbones, and the MindSpore code is\navailable at https://gitee.com/mindspore/models/tree/master/research/cv/TNT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">An Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Enhua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Scene Flow from Point Clouds in the Real World. (arXiv:2103.01306v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01306","description":"<p>Autonomous vehicles operate in highly dynamic environments necessitating an\naccurate assessment of which aspects of a scene are moving and where they are\nmoving to. A popular approach to 3D motion estimation, termed scene flow, is to\nemploy 3D point cloud data from consecutive LiDAR scans, although such\napproaches have been limited by the small size of real-world, annotated LiDAR\ndata. In this work, we introduce a new large-scale dataset for scene flow\nestimation derived from corresponding tracked 3D objects, which is\n$\\sim$1,000$\\times$ larger than previous real-world datasets in terms of the\nnumber of annotated frames. We demonstrate how previous works were bounded\nbased on the amount of real LiDAR data available, suggesting that larger\ndatasets are required to achieve state-of-the-art predictive performance.\nFurthermore, we show how previous heuristics for operating on point clouds such\nas down-sampling heavily degrade performance, motivating a new class of models\nthat are tractable on the full point cloud. To address this issue, we introduce\nthe FastFlow3D architecture which provides real time inference on the full\npoint cloud. Additionally, we design human-interpretable metrics that better\ncapture real world aspects by accounting for ego-motion and providing\nbreakdowns per object type. We hope that this dataset may provide new\nopportunities for developing real world scene flow systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jund_P/0/1/0/all/0/1\">Philipp Jund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sweeney_C/0/1/0/all/0/1\">Chris Sweeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdo_N/0/1/0/all/0/1\">Nichola Abdo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1\">Jonathon Shlens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cut-Thumbnail: A Novel Data Augmentation for Convolutional Neural Network. (arXiv:2103.05342v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.05342","description":"<p>In this paper, we propose a novel data augmentation strategy named\nCut-Thumbnail, that aims to improve the shape bias of the network. We reduce an\nimage to a certain size and replace the random region of the original image\nwith the reduced image. The generated image not only retains most of the\noriginal image information but also has global information in the reduced\nimage. We call the reduced image as thumbnail. Furthermore, we find that the\nidea of thumbnail can be perfectly integrated with Mixed Sample Data\nAugmentation, so we put one image's thumbnail on another image while the ground\ntruth labels are also mixed, making great achievements on various computer\nvision tasks. Extensive experiments show that Cut-Thumbnail works better than\nstate-of-the-art augmentation strategies across classification, fine-grained\nimage classification, and object detection. On ImageNet classification,\nResNet-50 architecture with our method achieves 79.21\\% accuracy, which is more\nthan 2.8\\% improvement on the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianshu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiali Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaomin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating and Evaluating Explanations of Attended and Error-Inducing Input Regions for VQA Models. (arXiv:2103.14712v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14712","description":"<p>Attention maps, a popular heatmap-based explanation method for Visual\nQuestion Answering (VQA), are supposed to help users understand the model by\nhighlighting portions of the image/question used by the model to infer answers.\nHowever, we see that users are often misled by current attention map\nvisualizations that point to relevant regions despite the model producing an\nincorrect answer. Hence, we propose Error Maps that clarify the error by\nhighlighting image regions where the model is prone to err. Error maps can\nindicate when a correctly attended region may be processed incorrectly leading\nto an incorrect answer, and hence, improve users' understanding of those cases.\nTo evaluate our new explanations, we further introduce a metric that simulates\nusers' interpretation of explanations to evaluate their potential helpfulness\nto understand model correctness. We finally conduct user studies to see that\nour new explanations help users understand model correctness better than\nbaselines by an expected 30\\% and that our proxy helpfulness metrics correlate\nstrongly ($\\rho&gt;0.97$) with how well users can predict model correctness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Arijit Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cogswell_M/0/1/0/all/0/1\">Michael Cogswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alipour_K/0/1/0/all/0/1\">Kamran Alipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1\">Ajay Divakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burachas_G/0/1/0/all/0/1\">Giedrius Burachas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RTIC: Residual Learning for Text and Image Composition using Graph Convolutional Network. (arXiv:2104.03015v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03015","description":"<p>In this paper, we study the compositional learning of images and texts for\nimage retrieval. The query is given in the form of an image and text that\ndescribes the desired modifications to the image; the goal is to retrieve the\ntarget image that satisfies the given modifications and resembles the query by\ncomposing information in both the text and image modalities. To remedy this, we\npropose a novel architecture designed for the image-text composition task and\nshow that the proposed structure can effectively encode the differences between\nthe source and target images conditioned on the text. Furthermore, we introduce\na new joint training technique based on the graph convolutional network that is\ngenerally applicable for any existing composition methods in a plug-and-play\nmanner. We found that the proposed technique consistently improves performance\nand achieves state-of-the-art scores on various benchmarks. To avoid misleading\nexperimental results caused by trivial training hyper-parameters, we reproduce\nall individual baselines and train models with a unified training environment.\nWe expect this approach to suppress undesirable effects from irrelevant\ncomponents and emphasize the image-text composition module's ability. Also, we\nachieve the state-of-the-art score without restricting the training\nenvironment, which implies the superiority of our method considering the gains\nfrom hyper-parameter tuning. The code, including all the baseline methods, are\nreleased https://github.com/nashory/rtic-gcn-pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Minchul Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Yoonjae Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1\">Byungsoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1\">Geonmo Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Enabling Meta-Learning from Target Models. (arXiv:2104.03736v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.03736","description":"<p>Meta-learning can extract an inductive bias from previous learning experience\nand assist the training of new tasks. It is often realized through optimizing a\nmeta-model with the evaluation loss of task-specific solvers. Most existing\nalgorithms sample non-overlapping $\\mathit{support}$ sets and $\\mathit{query}$\nsets to train and evaluate the solvers respectively due to simplicity\n($\\mathcal{S}$/$\\mathcal{Q}$ protocol). Different from\n$\\mathcal{S}$/$\\mathcal{Q}$ protocol, we can also evaluate a task-specific\nsolver by comparing it to a target model $\\mathcal{T}$, which is the optimal\nmodel for this task or a model that behaves well enough on this task\n($\\mathcal{S}$/$\\mathcal{T}$ protocol). Although being short of research,\n$\\mathcal{S}$/$\\mathcal{T}$ protocol has unique advantages such as offering\nmore informative supervision, but it is computationally expensive. This paper\nlooks into this special evaluation method and takes a step towards putting it\ninto practice. We find that with a small ratio of tasks armed with target\nmodels, classic meta-learning algorithms can be improved a lot without\nconsuming many resources. We empirically verify the effectiveness of\n$\\mathcal{S}$/$\\mathcal{T}$ protocol in a typical application of meta-learning,\n$\\mathit{i.e.}$, few-shot learning. In detail, after constructing target models\nby fine-tuning the pre-trained network on those hard tasks, we match the\ntask-specific solvers and target models via knowledge distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Su Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Le Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RaidaR: A Rich Annotated Image Dataset of Rainy Street Scenes. (arXiv:2104.04606v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04606","description":"<p>We introduce RaidaR, a rich annotated image dataset of rainy street scenes,\nto support autonomous driving research. The new dataset contains the largest\nnumber of rainy images (58,542) to date, 5,000 of which provide semantic\nsegmentations and 3,658 provide object instance segmentations. The RaidaR\nimages cover a wide range of realistic rain-induced artifacts, including fog,\ndroplets, and road reflections, which can effectively augment existing street\nscene datasets to improve data-driven machine perception during rainy weather.\nTo facilitate efficient annotation of a large volume of images, we develop a\nsemi-automatic scheme combining manual segmentation and an automated processing\nakin to cross validation, resulting in 10-20 fold reduction on annotation time.\nWe demonstrate the utility of our new dataset by showing how data augmentation\nwith RaidaR can elevate the accuracy of existing segmentation algorithms. We\nalso present a novel unpaired image-to-image translation algorithm for\nadding/removing rain artifacts, which directly benefits from RaidaR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiongchao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatemi_A/0/1/0/all/0/1\">Arezou Fatemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lira_W/0/1/0/all/0/1\">Wallace Lira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fenggen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_B/0/1/0/all/0/1\">Biao Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1\">Ali Mahdavi-Amiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Inexact Projected Gradient Method with Rounding and Lifting by Nonlinear Programming for Solving Rank-One Semidefinite Relaxation of Polynomial Optimization. (arXiv:2105.14033v2 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2105.14033","description":"<p>We consider solving high-order semidefinite programming (SDP) relaxations of\nnonconvex polynomial optimization problems (POPs) that often admit degenerate\nrank-one optimal solutions. Instead of solving the SDP alone, we propose a new\nalgorithmic framework that blends local search using the nonconvex POP into\nglobal descent using the convex SDP. In particular, we first design a globally\nconvergent inexact projected gradient method (iPGM) for solving the SDP that\nserves as the backbone of our framework. We then accelerate iPGM by taking\nlong, but safeguarded, rank-one steps generated by fast nonlinear programming\nalgorithms. We prove that the new framework is still globally convergent for\nsolving the SDP. To solve the iPGM subproblem of projecting a given point onto\nthe feasible set of the SDP, we design a two-phase algorithm with phase one\nusing a symmetric Gauss-Seidel based accelerated proximal gradient method\n(sGS-APG) to generate a good initial point, and phase two using a modified\nlimited-memory BFGS (L-BFGS) method to obtain an accurate solution. We analyze\nthe convergence for both phases and establish a novel global convergence result\nfor the modified L-BFGS that does not require the objective function to be\ntwice continuously differentiable. We conduct numerical experiments for solving\nsecond-order SDP relaxations arising from a diverse set of POPs. Our framework\ndemonstrates state-of-the-art efficiency, scalability, and robustness in\nsolving degenerate rank-one SDPs to high accuracy, even in the presence of\nmillions of equality constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liang_L/0/1/0/all/0/1\">Ling Liang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>, <a href=\"http://arxiv.org/find/math/1/au:+Toh_K/0/1/0/all/0/1\">Kim-Chuan Toh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition. (arXiv:2105.15075v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.15075","description":"<p>Vision Transformers (ViT) have achieved remarkable success in large-scale\nimage recognition. They split every 2D image into a fixed number of patches,\neach of which is treated as a token. Generally, representing an image with more\ntokens would lead to higher prediction accuracy, while it also results in\ndrastically increased computational cost. To achieve a decent trade-off between\naccuracy and speed, the number of tokens is empirically set to 16x16 or 14x14.\nIn this paper, we argue that every image has its own characteristics, and\nideally the token number should be conditioned on each individual input. In\nfact, we have observed that there exist a considerable number of \"easy\" images\nwhich can be accurately predicted with a mere number of 4x4 tokens, while only\na small fraction of \"hard\" ones need a finer representation. Inspired by this\nphenomenon, we propose a Dynamic Transformer to automatically configure a\nproper number of tokens for each input image. This is achieved by cascading\nmultiple Transformers with increasing numbers of tokens, which are sequentially\nactivated in an adaptive fashion at test time, i.e., the inference is\nterminated once a sufficiently confident prediction is produced. We further\ndesign efficient feature reuse and relationship reuse mechanisms across\ndifferent components of the Dynamic Transformer to reduce redundant\ncomputations. Extensive empirical results on ImageNet, CIFAR-10, and CIFAR-100\ndemonstrate that our method significantly outperforms the competitive baselines\nin terms of both theoretical computational efficiency and practical inference\nspeed. Code and pre-trained models (based on PyTorch and MindSpore) are\navailable at https://github.com/blackfeather-wang/Dynamic-Vision-Transformer\nand https://github.com/blackfeather-wang/Dynamic-Vision-Transformer-MindSpore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zeyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. (arXiv:2106.02034v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02034","description":"<p>Attention is sparse in vision transformers. We observe the final prediction\nin vision transformers is only based on a subset of most informative tokens,\nwhich is sufficient for accurate image recognition. Based on this observation,\nwe propose a dynamic token sparsification framework to prune redundant tokens\nprogressively and dynamically based on the input. Specifically, we devise a\nlightweight prediction module to estimate the importance score of each token\ngiven the current features. The module is added to different layers to prune\nredundant tokens hierarchically. To optimize the prediction module in an\nend-to-end manner, we propose an attention masking strategy to differentiably\nprune a token by blocking its interactions with other tokens. Benefiting from\nthe nature of self-attention, the unstructured sparse tokens are still hardware\nfriendly, which makes our framework easy to achieve actual speed-up. By\nhierarchically pruning 66% of the input tokens, our method greatly reduces\n31%~37% FLOPs and improves the throughput by over 40% while the drop of\naccuracy is within 0.5% for various vision transformers. Equipped with the\ndynamic token sparsification framework, DynamicViT models can achieve very\ncompetitive complexity/accuracy trade-offs compared to state-of-the-art CNNs\nand vision transformers on ImageNet. Code is available at\nhttps://github.com/raoyongming/DynamicViT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Benlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CATs: Cost Aggregation Transformers for Visual Correspondence. (arXiv:2106.02520v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02520","description":"<p>We propose a novel cost aggregation network, called Cost Aggregation\nTransformers (CATs), to find dense correspondences between semantically similar\nimages with additional challenges posed by large intra-class appearance and\ngeometric variations. Cost aggregation is a highly important process in\nmatching tasks, which the matching accuracy depends on the quality of its\noutput. Compared to hand-crafted or CNN-based methods addressing the cost\naggregation, in that either lacks robustness to severe deformations or inherit\nthe limitation of CNNs that fail to discriminate incorrect matches due to\nlimited receptive fields, CATs explore global consensus among initial\ncorrelation map with the help of some architectural designs that allow us to\nfully leverage self-attention mechanism. Specifically, we include appearance\naffinity modeling to aid the cost aggregation process in order to disambiguate\nthe noisy initial correlation maps and propose multi-level aggregation to\nefficiently capture different semantics from hierarchical feature\nrepresentations. We then combine with swapping self-attention technique and\nresidual connections not only to enforce consistent matching but also to ease\nthe learning process, which we find that these result in an apparent\nperformance boost. We conduct experiments to demonstrate the effectiveness of\nthe proposed model over the latest methods and provide extensive ablation\nstudies. Code and trained models are available\nat~\\url{https://github.com/SunghwanHong/CATs}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seokju Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sunghwan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1\">Sangryul Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yunsung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recovery Analysis for Plug-and-Play Priors using the Restricted Eigenvalue Condition. (arXiv:2106.03668v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03668","description":"<p>The plug-and-play priors (PnP) and regularization by denoising (RED) methods\nhave become widely used for solving inverse problems by leveraging pre-trained\ndeep denoisers as image priors. While the empirical imaging performance and the\ntheoretical convergence properties of these algorithms have been widely\ninvestigated, their recovery properties have not previously been theoretically\nanalyzed. We address this gap by showing how to establish theoretical recovery\nguarantees for PnP/RED by assuming that the solution of these methods lies near\nthe fixed-points of a deep neural network. We also present numerical results\ncomparing the recovery performance of PnP/RED in compressive sensing against\nthat of recent compressive sensing algorithms based on generative models. Our\nnumerical results suggest that PnP with a pre-trained artifact removal network\nprovides significantly better results compared to the existing state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wohlberg_B/0/1/0/all/0/1\">Brendt Wohlberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamilov_U/0/1/0/all/0/1\">Ulugbek S. Kamilov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation. (arXiv:2106.04144v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04144","description":"<p>Convolutional neural networks typically perform poorly when the test (target\ndomain) and training (source domain) data have significantly different\ndistributions. While this problem can be mitigated by using the target domain\ndata to align the source and target domain feature representations, the target\ndomain data may be unavailable due to privacy concerns. Consequently, there is\na need for methods that generalize well despite restricted access to target\ndomain data during training. In this work, we propose an adversarial semantic\nhallucination approach (ASH), which combines a class-conditioned hallucination\nmodule and a semantic segmentation module. Since the segmentation performance\nvaries across different classes, we design a semantic-conditioned style\nhallucination module to generate affine transformation parameters from semantic\ninformation in the segmentation probability maps of the source domain image.\nUnlike previous adaptation approaches, which treat all classes equally, ASH\nconsiders the class-wise differences. The segmentation module and the\nhallucination module compete adversarially, with the hallucination module\ngenerating increasingly \"difficult\" stylized images to challenge the\nsegmentation module. In response, the segmentation module improves as it is\ntrained with generated samples at an appropriate class-wise difficulty level.\nOur results on the Cityscapes and Mapillary benchmark datasets show that our\nmethod is competitive with state of the art work. Code is made available at\nhttps://github.com/gabriel-tjio/ASH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tjio_G/0/1/0/all/0/1\">Gabriel Tjio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond BatchNorm: Towards a Unified Understanding of Normalization in Deep Learning. (arXiv:2106.05956v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.05956","description":"<p>Inspired by BatchNorm, there has been an explosion of normalization layers in\ndeep learning. Recent works have identified a multitude of beneficial\nproperties in BatchNorm to explain its success. However, given the pursuit of\nalternative normalization layers, these properties need to be generalized so\nthat any given layer's success/failure can be accurately predicted. In this\nwork, we take a first step towards this goal by extending known properties of\nBatchNorm in randomly initialized deep neural networks (DNNs) to several\nrecently proposed normalization layers. Our primary findings follow: (i)\nsimilar to BatchNorm, activations-based normalization layers can prevent\nexponential growth of activations in ResNets, but parametric techniques require\nexplicit remedies; (ii) use of GroupNorm can ensure an informative forward\npropagation, with different samples being assigned dissimilar activations, but\nincreasing group size results in increasingly indistinguishable activations for\ndifferent samples, explaining slow convergence speed in models with LayerNorm;\nand (iii) small group sizes result in large gradient norm in earlier layers,\nhence explaining training instability issues in Instance Normalization and\nillustrating a speed-stability tradeoff in GroupNorm. Overall, our analysis\nreveals a unified set of mechanisms that underpin the success of normalization\nmethods in deep learning, providing us with a compass to systematically explore\nthe vast design space of DNN normalization layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1\">Ekdeep Singh Lubana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1\">Hidenori Tanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with Unlabeled Data. (arXiv:2106.07807v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07807","description":"<p>Most existing works in few-shot learning rely on meta-learning the network on\na large base dataset which is typically from the same domain as the target\ndataset. We tackle the problem of cross-domain few-shot learning where there is\na large shift between the base and target domain. The problem of cross-domain\nfew-shot recognition with unlabeled target data is largely unaddressed in the\nliterature. STARTUP was the first method that tackles this problem using\nself-training. However, it uses a fixed teacher pretrained on a labeled base\ndataset to create soft labels for the unlabeled target samples. As the base\ndataset and unlabeled dataset are from different domains, projecting the target\nimages in the class-domain of the base dataset with a fixed pretrained model\nmight be sub-optimal. We propose a simple dynamic distillation-based approach\nto facilitate unlabeled images from the novel/base dataset. We impose\nconsistency regularization by calculating predictions from the weakly-augmented\nversions of the unlabeled images from a teacher network and matching it with\nthe strongly augmented versions of the same images from a student network. The\nparameters of the teacher network are updated as exponential moving average of\nthe parameters of the student network. We show that the proposed network learns\nrepresentation that can be easily adapted to the target domain even though it\nhas not been trained with target-specific classes during the pretraining phase.\nOur model outperforms the current state-of-the art method by 4.4% for 1-shot\nand 3.6% for 5-shot classification in the BSCD-FSL benchmark, and also shows\ncompetitive performance on traditional in-domain few-shot learning task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1\">Ashraful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radke_R/0/1/0/all/0/1\">Richard J. Radke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Calibration of Modern Neural Networks. (arXiv:2106.07998v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07998","description":"<p>Accurate estimation of predictive uncertainty (model calibration) is\nessential for the safe application of neural networks. Many instances of\nmiscalibration in modern neural networks have been reported, suggesting a trend\nthat newer, more accurate models produce poorly calibrated predictions. Here,\nwe revisit this question for recent state-of-the-art image classification\nmodels. We systematically relate model calibration and accuracy, and find that\nthe most recent models, notably those not using convolutions, are among the\nbest calibrated. Trends observed in prior model generations, such as decay of\ncalibration with distribution shift or model size, are less pronounced in\nrecent architectures. We also show that model size and amount of pretraining do\nnot fully explain these differences, suggesting that architecture is a major\ndeterminant of calibration properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1\">Matthias Minderer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1\">Josip Djolonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1\">Rob Romijnders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubis_F/0/1/0/all/0/1\">Frances Hubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dustin Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lucic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spot the Difference: Detection of Topological Changes via Geometric Alignment. (arXiv:2106.08233v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08233","description":"<p>Geometric alignment appears in a variety of applications, ranging from domain\nadaptation, optimal transport, and normalizing flows in machine learning;\noptical flow and learned augmentation in computer vision and deformable\nregistration within biomedical imaging. A recurring challenge is the alignment\nof domains whose topology is not the same; a problem that is routinely ignored,\npotentially introducing bias in downstream analysis. As a first step towards\nsolving such alignment problems, we propose an unsupervised algorithm for the\ndetection of changes in image topology. The model is based on a conditional\nvariational auto-encoder and detects topological changes between two images\nduring the registration step. We account for both topological changes in the\nimage under spatial variation and unexpected transformations. Our approach is\nvalidated on two tasks and datasets: detection of topological changes in\nmicroscopy images of cells, and unsupervised anomaly detection brain imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Czolbe_S/0/1/0/all/0/1\">Steffen Czolbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdgeConv with Attention Module for Monocular Depth Estimation. (arXiv:2106.08615v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08615","description":"<p>Monocular depth estimation is an especially important task in robotics and\nautonomous driving, where 3D structural information is essential. However,\nextreme lighting conditions and complex surface objects make it difficult to\npredict depth in a single image. Therefore, to generate accurate depth maps, it\nis important for the model to learn structural information about the scene. We\npropose a novel Patch-Wise EdgeConv Module (PEM) and EdgeConv Attention Module\n(EAM) to solve the difficulty of monocular depth estimation. The proposed\nmodules extract structural information by learning the relationship between\nimage patches close to each other in space using edge convolution. Our method\nis evaluated on two popular datasets, the NYU Depth V2 and the KITTI Eigen\nsplit, achieving state-of-the-art performance. We prove that the proposed model\npredicts depth robustly in challenging scenes through various comparative\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sangwon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chaewon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape from Blur: Recovering Textured 3D Shape and Motion of Fast Moving Objects. (arXiv:2106.08762v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08762","description":"<p>We address the novel task of jointly reconstructing the 3D shape, texture,\nand motion of an object from a single motion-blurred image. While previous\napproaches address the deblurring problem only in the 2D image domain, our\nproposed rigorous modeling of all object properties in the 3D domain enables\nthe correct description of arbitrary object motion. This leads to significantly\nbetter image decomposition and sharper deblurring results. We model the\nobserved appearance of a motion-blurred object as a combination of the\nbackground and a 3D object with constant translation and rotation. Our method\nminimizes a loss on reconstructing the input image via differentiable rendering\nwith suitable regularizers. This enables estimating the textured 3D mesh of the\nblurred object with high fidelity. Our method substantially outperforms\ncompeting approaches on several benchmarks for fast moving objects deblurring.\nQualitative results show that the reconstructed 3D mesh generates high-quality\ntemporal super-resolution and novel views of the deblurred object.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rozumnyi_D/0/1/0/all/0/1\">Denys Rozumnyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accumulative Poisoning Attacks on Real-time Data. (arXiv:2106.09993v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.09993","description":"<p>Collecting training data from untrusted sources exposes machine learning\nservices to poisoning adversaries, who maliciously manipulate training data to\ndegrade the model accuracy. When trained on offline datasets, poisoning\nadversaries have to inject the poisoned data in advance before training, and\nthe order of feeding these poisoned batches into the model is stochastic. In\ncontrast, practical systems are more usually trained/fine-tuned on sequentially\ncaptured real-time data, in which case poisoning adversaries could dynamically\npoison each data batch according to the current model state. In this paper, we\nfocus on the real-time settings and propose a new attacking strategy, which\naffiliates an accumulative phase with poisoning attacks to secretly (i.e.,\nwithout affecting accuracy) magnify the destructive effect of a (poisoned)\ntrigger batch. By mimicking online learning and federated learning on MNIST and\nCIFAR-10, we show that model accuracy significantly drops by a single update\nstep on the trigger batch after the accumulative phase. Our work validates that\na well-designed but straightforward attacking strategy can dramatically amplify\nthe poisoning effects, with no need to explore complex techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToAlign: Task-oriented Alignment for Unsupervised Domain Adaptation. (arXiv:2106.10812v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10812","description":"<p>Unsupervised domain adaptive classifcation intends to improve the\nclassifcation performance on unlabeled target domain. To alleviate the adverse\neffect of domain shift, many approaches align the source and target domains in\nthe feature space. However, a feature is usually taken as a whole for alignment\nwithout explicitly making domain alignment proactively serve the classifcation\ntask, leading to sub-optimal solution. In this paper, we propose an effective\nTask-oriented Alignment (ToAlign) for unsupervised domain adaptation (UDA). We\nstudy what features should be aligned across domains and propose to make the\ndomain alignment proactively serve classifcation by performing feature\ndecomposition and alignment under the guidance of the prior knowledge induced\nfrom the classifcation task itself. Particularly, we explicitly decompose a\nfeature in the source domain into a task-related/discriminative feature that\nshould be aligned, and a task-irrelevant feature that should be\navoided/ignored, based on the classifcation meta-knowledge. Extensive\nexperimental results on various benchmarks (e.g., Offce-Home, Visda-2017, and\nDomainNet) under different domain adaptation settings demonstrate the\neffectiveness of ToAlign which helps achieve the state-of-the-art performance.\nThe code is publicly available at https://github.com/microsoft/UDA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Guoqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021. (arXiv:2106.13033v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13033","description":"<p>In this paper, inspired by the successes of visionlanguage pre-trained models\nand the benefits from training with adversarial attacks, we present a novel\ntransformerbased cross-modal fusion modeling by incorporating the both notions\nfor VQA challenge 2021. Specifically, the proposed model is on top of the\narchitecture of VinVL model [19], and the adversarial training strategy [4] is\napplied to make the model robust and generalized. Moreover, two implementation\ntricks are also used in our system to obtain better results. The experiments\ndemonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Ke-Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Bo-Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuan-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early Convolutions Help Transformers See Better. (arXiv:2106.14881v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14881","description":"<p>Vision transformer (ViT) models exhibit substandard optimizability. In\nparticular, they are sensitive to the choice of optimizer (AdamW vs. SGD),\noptimizer hyperparameters, and training schedule length. In comparison, modern\nconvolutional neural networks are easier to optimize. Why is this the case? In\nthis work, we conjecture that the issue lies with the patchify stem of ViT\nmodels, which is implemented by a stride-p p*p convolution (p=16 by default)\napplied to the input image. This large-kernel plus large-stride convolution\nruns counter to typical design choices of convolutional layers in neural\nnetworks. To test whether this atypical design choice causes an issue, we\nanalyze the optimization behavior of ViT models with their original patchify\nstem versus a simple counterpart where we replace the ViT stem by a small\nnumber of stacked stride-two 3*3 convolutions. While the vast majority of\ncomputation in the two ViT designs is identical, we find that this small change\nin early visual processing results in markedly different training behavior in\nterms of the sensitivity to optimization settings as well as the final model\naccuracy. Using a convolutional stem in ViT dramatically increases optimization\nstability and also improves peak performance (by ~1-2% top-1 accuracy on\nImageNet-1k), while maintaining flops and runtime. The improvement can be\nobserved across the wide spectrum of model complexities (from 1G to 36G flops)\nand dataset scales (from ImageNet-1k to ImageNet-21k). These findings lead us\nto recommend using a standard, lightweight convolutional stem for ViT models in\nthis regime as a more robust architectural choice compared to the original ViT\nmodel design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tete Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mannat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mintun_E/0/1/0/all/0/1\">Eric Mintun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollar_P/0/1/0/all/0/1\">Piotr Doll&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1\">Ross Girshick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning. (arXiv:2106.15087v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15087","description":"<p>Contrary to the vast literature in modeling, perceiving, and understanding\nagent-object (e.g., human-object, hand-object, robot-object) interaction in\ncomputer vision and robotics, very few past works have studied the task of\nobject-object interaction, which also plays an important role in robotic\nmanipulation and planning tasks. There is a rich space of object-object\ninteraction scenarios in our daily life, such as placing an object on a messy\ntabletop, fitting an object inside a drawer, pushing an object using a tool,\netc. In this paper, we propose a unified affordance learning framework to learn\nobject-object interaction for various tasks. By constructing four object-object\ninteraction task environments using physical simulation (SAPIEN) and thousands\nof ShapeNet models with rich geometric diversity, we are able to conduct\nlarge-scale object-object affordance learning without the need for human\nannotations or demonstrations. At the core of technical contribution, we\npropose an object-kernel point convolution network to reason about detailed\ninteraction between two objects. Experiments on large-scale synthetic data and\nreal-world data prove the effectiveness of the proposed approach. Please refer\nto the project webpage for code, data, video, and more materials:\nhttps://cs.stanford.edu/~kaichun/o2oafford\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuzhe Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Filter Networks for Image Classification. (arXiv:2107.00645v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00645","description":"<p>Recent advances in self-attention and pure multi-layer perceptrons (MLP)\nmodels for vision have shown great potential in achieving promising performance\nwith fewer inductive biases. These models are generally based on learning\ninteraction among spatial locations from raw data. The complexity of\nself-attention and MLP grows quadratically as the image size increases, which\nmakes these models hard to scale up when high-resolution features are required.\nIn this paper, we present the Global Filter Network (GFNet), a conceptually\nsimple yet computationally efficient architecture, that learns long-term\nspatial dependencies in the frequency domain with log-linear complexity. Our\narchitecture replaces the self-attention layer in vision transformers with\nthree key operations: a 2D discrete Fourier transform, an element-wise\nmultiplication between frequency-domain features and learnable global filters,\nand a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity\ntrade-offs of our models on both ImageNet and downstream tasks. Our results\ndemonstrate that GFNet can be a very competitive alternative to\ntransformer-style models and CNNs in efficiency, generalization ability and\nrobustness. Code is available at https://github.com/raoyongming/GFNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search. (arXiv:2107.03463v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03463","description":"<p>A strong visual object tracker nowadays relies on its well-crafted modules,\nwhich typically consist of manually-designed network architectures to deliver\nhigh-quality tracking results. Not surprisingly, the manual design process\nbecomes a particularly challenging barrier, as it demands sufficient prior\nexperience, enormous effort, intuition, and perhaps some good luck. Meanwhile,\nneural architecture search has gaining grounds in practical applications as a\npromising method in tackling the issue of automated search of feasible network\nstructures. In this work, we propose a novel cell-level differentiable\narchitecture search mechanism with early stopping to automate the network\ndesign of the tracking module, aiming to adapt backbone features to the\nobjective of Siamese tracking networks during offline training. Besides, the\nproposed early stopping strategy avoids over-fitting and performance collapse\nproblems leading to generalization improvement. The proposed approach is\nsimple, efficient, and with no need to stack a series of modules to construct a\nnetwork. Our approach is easy to be incorporated into existing trackers, which\nis empirically validated using different differentiable architecture\nsearch-based methods and tracking objectives. Extensive experimental\nevaluations demonstrate the superior performance of our approach over five\ncommonly-used benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1\">Seyed Mojtaba Marvasti-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaghani_J/0/1/0/all/0/1\">Javad Khaghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanei_Yakhdan_H/0/1/0/all/0/1\">Hossein Ghanei-Yakhdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1\">Shohreh Kasaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Object Behavioral Feature Extraction for Potential Risk Analysis based on Video Sensor. (arXiv:2107.03554v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03554","description":"<p>Pedestrians are exposed to risk of death or serious injuries on roads,\nespecially unsignalized crosswalks, for a variety of reasons. To date, an\nextensive variety of studies have reported on vision based traffic safety\nsystem. However, many studies required manual inspection of the volumes of\ntraffic video to reliably obtain traffic related objects behavioral factors. In\nthis paper, we propose an automated and simpler system for effectively\nextracting object behavioral features from video sensors deployed on the road.\nWe conduct basic statistical analysis on these features, and show how they can\nbe useful for monitoring the traffic behavior on the road. We confirm the\nfeasibility of the proposed system by applying our prototype to two\nunsignalized crosswalks in Osan city, South Korea. To conclude, we compare\nbehaviors of vehicles and pedestrians in those two areas by simple statistical\nanalysis. This study demonstrates the potential for a network of connected\nvideo sensors to provide actionable data for smart cities to improve pedestrian\nsafety in dangerous road environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noh_B/0/1/0/all/0/1\">Byeongjoon Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ka_D/0/1/0/all/0/1\">Dongho Ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_W/0/1/0/all/0/1\">Wonjun Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hwasoo Yeo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CamTuner: Reinforcement-Learning based System for Camera Parameter Tuning to enhance Analytics. (arXiv:2107.03964v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.03964","description":"<p>Video analytics systems critically rely on video cameras, which capture\nhigh-quality video frames, to achieve high analytics accuracy. Although modern\nvideo cameras often expose tens of configurable parameter settings that can be\nset by end-users, deployment of surveillance cameras today often uses a fixed\nset of parameter settings because the end-users lack the skill or understanding\nto reconfigure these parameters.\n</p>\n<p>In this paper, we first show that in a typical surveillance camera\ndeployment, environmental condition changes can significantly affect the\naccuracy of analytics units such as person detection, face detection and face\nrecognition, and how such adverse impact can be mitigated by dynamically\nadjusting camera settings. We then propose CAMTUNER, a framework that can be\neasily applied to an existing video analytics pipeline (VAP) to enable\nautomatic and dynamic adaptation of complex camera settings to changing\nenvironmental conditions, and autonomously optimize the accuracy of analytics\nunits (AUs) in the VAP. CAMTUNER is based on SARSA reinforcement learning (RL)\nand it incorporates two novel components: a light-weight analytics quality\nestimator and a virtual camera. CAMTUNER is implemented in a system with AXIS\nsurveillance cameras and several VAPs (with various AUs) that processed\nday-long customer videos captured at airport entrances. Our evaluations show\nthat CAMTUNER can adapt quickly to changing environments. We compared CAMTUNER\nwith two alternative approaches where either static camera settings were used,\nor a strawman approach where camera settings were manually changed every hour\n(based on human perception of quality). We observed that for the face detection\nand person detection AUs, CAMTUNER is able to achieve up to 13.8% and 9.2%\nhigher accuracy, respectively, compared to the best of the two approaches\n(average improvement of 8% for both AUs).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sibendu Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Kunal Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coviello_G/0/1/0/all/0/1\">Giuseppe Coviello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaradas_M/0/1/0/all/0/1\">Murugan Sankaradas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_O/0/1/0/all/0/1\">Oliver Po</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Y. Charlie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakradhar_S/0/1/0/all/0/1\">Srimat T. Chakradhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CentripetalText: An Efficient Text Instance Representation for Scene Text Detection. (arXiv:2107.05945v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05945","description":"<p>Scene text detection remains a grand challenge due to the variation in text\ncurvatures, orientations, and aspect ratios. One of the hardest problems in\nthis task is how to represent text instances of arbitrary shapes. Although many\nmethods have been proposed to model irregular texts in a flexible manner, most\nof them lose simplicity and robustness. Their complicated post-processings and\nthe regression under Dirac delta distribution undermine the detection\nperformance and the generalization ability. In this paper, we propose an\nefficient text instance representation named CentripetalText (CT), which\ndecomposes text instances into the combination of text kernels and centripetal\nshifts. Specifically, we utilize the centripetal shifts to implement pixel\naggregation, guiding the external text pixels to the internal text kernels. The\nrelaxation operation is integrated into the dense regression for centripetal\nshifts, allowing the correct prediction in a range instead of a specific value.\nThe convenient reconstruction of text contours and the tolerance of prediction\nerrors in our method guarantee the high detection accuracy and the fast\ninference speed, respectively. Besides, we shrink our text detector into a\nproposal generation module, namely CentripetalText Proposal Network, replacing\nSegmentation Proposal Network in Mask TextSpotter v3 and producing more\naccurate proposals. To validate the effectiveness of our method, we conduct\nexperiments on several commonly used scene text benchmarks, including both\ncurved and multi-oriented text datasets. For the task of scene text detection,\nour approach achieves superior or competitive performance compared to other\nexisting methods, e.g., F-measure of 86.3% at 40.0 FPS on Total-Text, F-measure\nof 86.1% at 34.8 FPS on MSRA-TD500, etc. For the task of end-to-end scene text\nrecognition, our method outperforms Mask TextSpotter v3 by 1.1% on Total-Text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_T/0/1/0/all/0/1\">Tao Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhouhui Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCVS: Context-aware Controllable Video Synthesis. (arXiv:2107.08037v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08037","description":"<p>This presentation introduces a self-supervised learning approach to the\nsynthesis of new video clips from old ones, with several new key elements for\nimproved spatial resolution and realism: It conditions the synthesis process on\ncontextual information for temporal continuity and ancillary information for\nfine control. The prediction model is doubly autoregressive, in the latent\nspace of an autoencoder for forecasting, and in image space for updating\ncontextual information, which is also used to enforce spatio-temporal\nconsistency through a learnable optical flow module. Adversarial training of\nthe autoencoder in the appearance and temporal domains is used to further\nimprove the realism of its output. A quantizer inserted between the encoder and\nthe transformer in charge of forecasting future frames in latent space (and its\ninverse inserted between the transformer and the decoder) adds even more\nflexibility by affording simple mechanisms for handling multimodal ancillary\ninformation for controlling the synthesis process (eg, a few sample frames, an\naudio track, a trajectory in image space) and taking into account the\nintrinsically uncertain nature of the future by allowing multiple predictions.\nExperiments with an implementation of the proposed approach give very good\nqualitative and quantitative results on multiple tasks and standard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moing_G/0/1/0/all/0/1\">Guillaume Le Moing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1\">Jean Ponce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active 3D Shape Reconstruction from Vision and Touch. (arXiv:2107.09584v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.09584","description":"<p>Humans build 3D understandings of the world through active object\nexploration, using jointly their senses of vision and touch. However, in 3D\nshape reconstruction, most recent progress has relied on static datasets of\nlimited sensory data such as RGB images, depth maps or haptic readings, leaving\nthe active exploration of the shape largely unexplored. Inactive touch sensing\nfor 3D reconstruction, the goal is to actively select the tactile readings that\nmaximize the improvement in shape reconstruction accuracy. However, the\ndevelopment of deep learning-based active touch models is largely limited by\nthe lack of frameworks for shape exploration. In this paper, we focus on this\nproblem and introduce a system composed of: 1) a haptic simulator leveraging\nhigh spatial resolution vision-based tactile sensors for active touching of 3D\nobjects; 2)a mesh-based 3D shape reconstruction model that relies on tactile or\nvisuotactile signals; and 3) a set of data-driven solutions with either tactile\nor visuotactile priors to guide the shape exploration. Our framework enables\nthe development of the first fully data-driven solutions to active touch on top\nof learned models for object understanding. Our experiments show the benefits\nof such solutions in the task of 3D shape understanding where our models\nconsistently outperform natural baselines. We provide our framework as a tool\nto foster future research in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Edward J. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1\">David Meger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1\">Luis Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1\">Roberto Calandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_A/0/1/0/all/0/1\">Adriana Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdzal_M/0/1/0/all/0/1\">Michal Drozdzal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Graph with Meta Concepts for Video Captioning. (arXiv:2108.06458v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06458","description":"<p>Video captioning targets interpreting the complex visual contents as text\ndescriptions, which requires the model to fully understand video scenes\nincluding objects and their interactions. Prevailing methods adopt\noff-the-shelf object detection networks to give object proposals and use the\nattention mechanism to model the relations between objects. They often miss\nsome undefined semantic concepts of the pretrained model and fail to identify\nexact predicate relationships between objects. In this paper, we investigate an\nopen research task of generating text descriptions for the given videos, and\npropose Cross-Modal Graph (CMG) with meta concepts for video captioning.\nSpecifically, to cover the useful semantic concepts in video captions, we\nweakly learn the corresponding visual regions for text descriptions, where the\nassociated visual regions and textual words are named cross-modal meta\nconcepts. We further build meta concept graphs dynamically with the learned\ncross-modal meta concepts. We also construct holistic video-level and local\nframe-level video graphs with the predicted predicates to model video sequence\nstructures. We validate the efficacy of our proposed techniques with extensive\nexperiments and achieve state-of-the-art results on two public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C. H. Hoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification. (arXiv:2108.08728v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08728","description":"<p>Attention mechanism has demonstrated great potential in fine-grained visual\nrecognition tasks. In this paper, we present a counterfactual attention\nlearning method to learn more effective attention based on causal inference.\nUnlike most existing methods that learn visual attention based on conventional\nlikelihood, we propose to learn the attention with counterfactual causality,\nwhich provides a tool to measure the attention quality and a powerful\nsupervisory signal to guide the learning process. Specifically, we analyze the\neffect of the learned visual attention on network prediction through\ncounterfactual intervention and maximize the effect to encourage the network to\nlearn more useful attention for fine-grained image recognition. Empirically, we\nevaluate our method on a wide range of fine-grained recognition tasks where\nattention plays a crucial role, including fine-grained image categorization,\nperson re-identification, and vehicle re-identification. The consistent\nimprovement on all benchmarks demonstrates the effectiveness of our method.\nCode is available at https://github.com/raoyongming/CAL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifted Chunk Transformer for Spatio-Temporal Representational Learning. (arXiv:2108.11575v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11575","description":"<p>Spatio-temporal representational learning has been widely adopted in various\nfields such as action recognition, video object segmentation, and action\nanticipation. Previous spatio-temporal representational learning approaches\nprimarily employ ConvNets or sequential models,e.g., LSTM, to learn the\nintra-frame and inter-frame features. Recently, Transformer models have\nsuccessfully dominated the study of natural language processing (NLP), image\nclassification, etc. However, the pure-Transformer based spatio-temporal\nlearning can be prohibitively costly on memory and computation to extract\nfine-grained features from a tiny patch. To tackle the training difficulty and\nenhance the spatio-temporal learning, we construct a shifted chunk Transformer\nwith pure self-attention blocks. Leveraging the recent efficient Transformer\ndesign in NLP, this shifted chunk Transformer can learn hierarchical\nspatio-temporal features from a local tiny patch to a global video clip. Our\nshifted self-attention can also effectively model complicated inter-frame\nvariances. Furthermore, we build a clip encoder based on Transformer to model\nlong-term temporal dependencies. We conduct thorough ablation studies to\nvalidate each component and hyper-parameters in our shifted chunk Transformer,\nand it outperforms previous state-of-the-art approaches on Kinetics-400,\nKinetics-600, UCF101, and HMDB51. Code and trained models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_X/0/1/0/all/0/1\">Xuefan Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tingxun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Transfer Learning for Event-based End-task Prediction via Pluggable Event to Image Translation. (arXiv:2109.01801v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01801","description":"<p>Event cameras are novel sensors that perceive the per-pixel intensity changes\nand output asynchronous event streams with high dynamic range and less motion\nblur. It has been shown that events alone can be used for end-task learning,\ne.g., semantic segmentation, based on encoder-decoder-like networks. However,\nas events are sparse and mostly reflect edge information, it is difficult to\nrecover original details merely relying on the decoder. Moreover, most methods\nresort to pixel-wise loss alone for supervision, which might be insufficient to\nfully exploit the visual details from sparse events, thus leading to less\noptimal performance. In this paper, we propose a simple yet flexible two-stream\nframework named Dual Transfer Learning (DTL) to effectively enhance the\nperformance on the end-tasks without adding extra inference cost. The proposed\napproach consists of three parts: event to end-task learning (EEL) branch,\nevent to image translation (EIT) branch, and transfer learning (TL) module that\nsimultaneously explores the feature-level affinity information and pixel-level\nknowledge from the EIT branch to improve the EEL branch. This simple yet novel\nmethod leads to strong representation learning from events and is evidenced by\nthe significant performance boost on the end-tasks such as semantic\nsegmentation and depth estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chae_Y/0/1/0/all/0/1\">Yujeong Chae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Local Domains for Image-to-Image Translation. (arXiv:2109.04468v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04468","description":"<p>Image-to-image (i2i) networks struggle to capture local changes because they\ndo not affect the global scene structure. For example, translating from highway\nscenes to offroad, i2i networks easily focus on global color features but\nignore obvious traits for humans like the absence of lane markings. In this\npaper, we leverage human knowledge about spatial domain characteristics which\nwe refer to as 'local domains' and demonstrate its benefit for image-to-image\ntranslation. Relying on a simple geometrical guidance, we train a patch-based\nGAN on few source data and hallucinate a new unseen domain which subsequently\neases transfer learning to target. We experiment on three tasks ranging from\nunstructured environments to adverse weather. Our comprehensive evaluation\nsetting shows we are able to generate realistic translations, with minimal\npriors, and training only on a few images. Furthermore, when trained on our\ntranslations images we show that all tested proxy tasks are significantly\nimproved, without ever seeing target domain at training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DellEva_A/0/1/0/all/0/1\">Anthony Dell&#x27;Eva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertozzi_M/0/1/0/all/0/1\">Massimo Bertozzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiently Identifying Task Groupings for Multi-Task Learning. (arXiv:2109.04617v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.04617","description":"<p>Multi-task learning can leverage information learned by one task to benefit\nthe training of other tasks. Despite this capacity, naively training all tasks\ntogether in one model often degrades performance, and exhaustively searching\nthrough combinations of task groupings can be prohibitively expensive. As a\nresult, efficiently identifying the tasks that would benefit from training\ntogether remains a challenging design question without a clear solution. In\nthis paper, we suggest an approach to select which tasks should train together\nin multi-task learning models. Our method determines task groupings in a single\nrun by training all tasks together and quantifying the effect to which one\ntask's gradient would affect another task's loss. On the large-scale Taskonomy\ncomputer vision dataset, we find this method can decrease test loss by 10.0%\ncompared to simply training all tasks together while operating 11.6 times\nfaster than a state-of-the-art task grouping method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fifty_C/0/1/0/all/0/1\">Christopher Fifty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amid_E/0/1/0/all/0/1\">Ehsan Amid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptive Learning via Synthetic Data for Person Re-identification. (arXiv:2109.05542v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05542","description":"<p>Person re-identification (re-ID) has gained more and more attention due to\nits widespread applications in intelligent video surveillance. Unfortunately,\nthe mainstream deep learning methods still need a large quantity of labeled\ndata to train models, and annotating data is an expensive work in real-world\nscenarios. In addition, due to domain gaps between different datasets, the\nperformance is dramatically decreased when re-ID models pre-trained on\nlabel-rich datasets (source domain) are directly applied to other unlabeled\ndatasets (target domain). In this paper, we attempt to remedy these problems\nfrom two aspects, namely data and methodology. Firstly, we develop a data\ncollector to automatically generate synthetic re-ID samples in a computer game,\nand construct a data labeler to simultaneously annotate them, which free humans\nfrom heavy data collections and annotations. Based on them, we build two\nsynthetic person re-ID datasets with different scales, \"GSPR\" and \"mini-GSPR\"\ndatasets. Secondly, we propose a synthesis-based multi-domain collaborative\nrefinement (SMCR) network, which contains a synthetic pretraining module and\ntwo collaborative-refinement modules to implement sufficient learning for the\nvaluable knowledge from multiple domains. Extensive experiments show that our\nproposed framework obtains significant performance improvements over the\nstate-of-the-art methods on multiple unsupervised domain adaptation tasks of\nperson re-ID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Sikai Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow. (arXiv:2109.09406v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09406","description":"<p>High-quality training data play a key role in image segmentation tasks.\nUsually, pixel-level annotations are expensive, laborious and time-consuming\nfor the large volume of training data. To reduce labelling cost and improve\nsegmentation quality, interactive segmentation methods have been proposed,\nwhich provide the result with just a few clicks. However, their performance\ndoes not meet the requirements of practical segmentation tasks in terms of\nspeed and accuracy. In this work, we propose EdgeFlow, a novel architecture\nthat fully utilizes interactive information of user clicks with edge-guided\nflow. Our method achieves state-of-the-art performance without any\npost-processing or iterative optimization scheme. Comprehensive experiments on\nbenchmarks also demonstrate the superiority of our method. In addition, with\nthe proposed method, we develop an efficient interactive segmentation tool for\npractical data annotation tasks. The source code and tool is avaliable at\nhttps://github.com/PaddlePaddle/PaddleSeg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yuying Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zewu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yizhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guowei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lutao Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shiyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_B/0/1/0/all/0/1\">Baohua Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends. (arXiv:2109.09824v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09824","description":"<p>This paper investigates the effectiveness of systematically probing Google\nTrends against textual translations of visual aspects as exogenous knowledge to\npredict the sales of brand-new fashion items, where past sales data is not\navailable, but only an image and few metadata are available. In particular, we\npropose GTM-Transformer, standing for Google Trends Multimodal Transformer,\nwhose encoder works on the representation of the exogenous time series, while\nthe decoder forecasts the sales using the Google Trends encoding, and the\navailable visual and metadata information. Our model works in a\nnon-autoregressive manner, avoiding the compounding effect of the first-step\nerrors. As a second contribution, we present the VISUELLE dataset, which is the\nfirst publicly available dataset for the task of new fashion product sales\nforecasting, containing the sales of 5577 new products sold between 2016-2019,\nderived from genuine historical data of Nunalie, an Italian fast-fashion\ncompany. Our dataset is equipped with images of products, metadata, related\nsales, and associated Google Trends. We use VISUELLE to compare our approach\nagainst state-of-the-art alternatives and numerous baselines, showing that\nGTM-Transformer is the most accurate in terms of both percentage and absolute\nerror. It is worth noting that the addition of exogenous knowledge boosts the\nforecasting accuracy by 1.5% WAPE wise, showing the importance of exploiting\nGoogle Trends. The code and dataset are both available at\nhttps://github.com/HumaticsLAB/GTM-Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1\">Geri Skenderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joppi_C/0/1/0/all/0/1\">Christian Joppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denitto_M/0/1/0/all/0/1\">Matteo Denitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-loss ensemble deep learning for chest X-ray classification. (arXiv:2109.14433v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.14433","description":"<p>Medical images commonly exhibit multiple abnormalities. Predicting them\nrequires multi-class classifiers whose training and desired reliable\nperformance can be affected by a combination of factors, such as, dataset size,\ndata source, distribution, and the loss function used to train the deep neural\nnetworks. Currently, the cross-entropy loss remains the de-facto loss function\nfor training deep learning classifiers. This loss function, however, asserts\nequal learning from all classes, leading to a bias toward the majority class.\nIn this work, we benchmark various state-of-the-art loss functions that are\nsuitable for multi-class classification, critically analyze model performance,\nand propose improved loss functions. We select a pediatric chest X-ray (CXR)\ndataset that includes images with no abnormality (normal), and those exhibiting\nmanifestations consistent with bacterial and viral pneumonia. We construct\nprediction-level and model-level ensembles, respectively, to improve\nclassification performance. Our results show that compared to the individual\nmodels and the state-of-the-art literature, the weighted averaging of the\npredictions for top-3 and top-5 model-level ensembles delivered significantly\nsuperior classification performance (p &lt; 0.05) in terms of MCC (0.9068, 95%\nconfidence interval (0.8839, 0.9297)) metric. Finally, we performed\nlocalization studies to interpret model behaviors to visualize and confirm that\nthe individual models and ensembles learned meaningful features and highlighted\ndisease manifestations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rajaraman_S/0/1/0/all/0/1\">Sivaramakrishnan Rajaraman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antani_S/0/1/0/all/0/1\">Sameer Antani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Pose Transfer with Correspondence Learning and Mesh Refinement. (arXiv:2109.15025v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15025","description":"<p>3D pose transfer is one of the most challenging 3D generation tasks. It aims\nto transfer the pose of a source mesh to a target mesh and keep the identity\n(e.g., body shape) of the target mesh. Some previous works require key point\nannotations to build reliable correspondence between the source and target\nmeshes, while other methods do not consider any shape correspondence between\nsources and targets, which leads to limited generation quality. In this work,\nwe propose a correspondence-refinement network to help the 3D pose transfer for\nboth human and animal meshes. The correspondence between source and target\nmeshes is first established by solving an optimal transport problem. Then, we\nwarp the source mesh according to the dense correspondence and obtain a coarse\nwarped mesh. The warped mesh will be better refined with our proposed Elastic\nInstance Normalization, which is a conditional normalization layer and can help\nto generate high-quality meshes. Extensive experimental results show that the\nproposed architecture can effectively transfer the poses from source to target\nmeshes and produce better results with satisfied visual performance than\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chaoyue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiacheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruibo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Representation Learning for Fine-Grained Face Transfer. (arXiv:2110.01571v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01571","description":"<p>Identity transfer often faces the challenge of generalizing to new situations\nwhere large pose and expression or background gaps exist between source and\ntarget face images. To improve generalization in such situations, biases take a\nkey role \\cite{mitchell_1980_bias}. This paper proposes an Errors-in-Variables\nAdapter (EVA) model to induce learning of proper generalizations by explicitly\nemploying biases to identity estimation based on prior knowledge about the\ntarget situation. To better match the source face with the target situation in\nterms of pose, expression, and background factors, we model the bias as a\ncausal effect of the target situation on source identity and estimate this\neffect through a controlled intervention trial. To achieve smoother transfer\nfor the target face across the identity gap, we eliminate the target face\nspecificity through multiple kernel regressions. The kernels are used to\nconstrain the regressions to operate only on identity information in the\ninternal representations of the target image, while leaving other perceptual\ninformation invariant. Combining these post-regression representations with the\nbiased estimation for identity, EVA shows impressive performance even in the\npresence of large gaps, providing empirical evidence supporting the utility of\nthe inductive biases in identity estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Gege Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huaibo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chaoyou Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Top 3 in FG 2021 Families In the Wild Kinship Verification Challenge. (arXiv:2110.07020v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07020","description":"<p>Kinship verification is the task of determining whether a parent-child,\nsibling, or grandparent-grandchild relationship exists between two people and\nis important in social media applications, forensic investigations, finding\nmissing children, and reuniting families. We demonstrate high quality kinship\nverification by participating in the FG 2021 Recognizing Families in the Wild\nchallenge which provides the largest publicly available dataset in the field.\nOur approach is among the top 3 winning entries in the competition. We ensemble\nmodels written by both human experts and OpenAI Codex. We make our models and\ncode publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strome_M/0/1/0/all/0/1\">Maxwell Benjamin Strome</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_I/0/1/0/all/0/1\">Ian Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_P/0/1/0/all/0/1\">Parker Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bo Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Roman Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagri_V/0/1/0/all/0/1\">Vaibhav Bagri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Newman Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?. (arXiv:2110.07472v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.07472","description":"<p>Equivariance has emerged as a desirable property of representations of\nobjects subject to identity-preserving transformations that constitute a group,\nsuch as translations and rotations. However, the expressivity of a\nrepresentation constrained by group equivariance is still not fully understood.\nWe address this gap by providing a generalization of Cover's Function Counting\nTheorem that quantifies the number of linearly separable and group-invariant\nbinary dichotomies that can be assigned to equivariant representations of\nobjects. We find that the fraction of separable dichotomies is determined by\nthe dimension of the space that is fixed by the group action. We show how this\nrelation extends to operations such as convolutions, element-wise\nnonlinearities, and global and local pooling. While other operations do not\nchange the fraction of separable dichotomies, local pooling decreases the\nfraction, despite being a highly nonlinear operation. Finally, we test our\ntheory on intermediate representations of randomly initialized and fully\ntrained convolutional neural networks and find perfect agreement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farrell_M/0/1/0/all/0/1\">Matthew Farrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordelon_B/0/1/0/all/0/1\">Blake Bordelon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_S/0/1/0/all/0/1\">Shubhendu Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pehlevan_C/0/1/0/all/0/1\">Cengiz Pehlevan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unrestricted Adversarial Attacks on ImageNet Competition. (arXiv:2110.09903v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09903","description":"<p>Many works have investigated the adversarial attacks or defenses under the\nsettings where a bounded and imperceptible perturbation can be added to the\ninput. However in the real-world, the attacker does not need to comply with\nthis restriction. In fact, more threats to the deep model come from\nunrestricted adversarial examples, that is, the attacker makes large and\nvisible modifications on the image, which causes the model classifying\nmistakenly, but does not affect the normal observation in human perspective.\nUnrestricted adversarial attack is a popular and practical direction but has\nnot been studied thoroughly. We organize this competition with the purpose of\nexploring more effective unrestricted adversarial attack algorithm, so as to\naccelerate the academical research on the model robustness under stronger\nunbounded attacks. The competition is held on the TianChi platform\n(\\url{https://tianchi.aliyun.com/competition/entrance/531853/introduction}) as\none of the series of AI Security Challengers Program.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaofeng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qi-An Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wenzhao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangcheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wenzhao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yajie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1\">Haoran Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yidan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zixuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Taoyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoqiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Huanqian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Ying Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Bingyang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yunfei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yekui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haorong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised denoising for massive noisy images. (arXiv:2110.11911v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11911","description":"<p>We propose an effective deep learning model for signal reconstruction, which\nrequires no signal prior, no noise model calibration, and no clean samples.\nThis model only assumes that the noise is independent of the measurement and\nthat the true signals share the same structured information. We demonstrate its\nperformance on a variety of real-world applications, from sub-\\r{A}ngstr\\\"{o}m\nresolution atomic images to sub-arcsecond resolution astronomy images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henninen_T/0/1/0/all/0/1\">Trond R. Henninen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_D/0/1/0/all/0/1\">Debora Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erni_R/0/1/0/all/0/1\">Rolf Erni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-Based CLIP-Guided Essence Transfer. (arXiv:2110.12427v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12427","description":"<p>The conceptual blending of two signals is a semantic task that may underline\nboth creativity and intelligence. We propose to perform such blending in a way\nthat incorporates two latent spaces: that of the generator network and that of\nthe semantic network. For the first network, we employ the powerful StyleGAN\ngenerator, and for the second, the powerful image-language matching network of\nCLIP. The new method creates a blending operator that is optimized to be\nsimultaneously additive in both latent spaces. Our results demonstrate that\nthis leads to blending that is much more natural than what can be obtained in\neach space separately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chefer_H/0/1/0/all/0/1\">Hila Chefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1\">Sagie Benaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paiss_R/0/1/0/all/0/1\">Roni Paiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"hSDB-instrument: Instrument Localization Database for Laparoscopic and Robotic Surgeries. (arXiv:2110.12555v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12555","description":"<p>Automated surgical instrument localization is an important technology to\nunderstand the surgical process and in order to analyze them to provide\nmeaningful guidance during surgery or surgical index after surgery to the\nsurgeon. We introduce a new dataset that reflects the kinematic characteristics\nof surgical instruments for automated surgical instrument localization of\nsurgical videos. The hSDB(hutom Surgery DataBase)-instrument dataset consists\nof instrument localization information from 24 cases of laparoscopic\ncholecystecomy and 24 cases of robotic gastrectomy. Localization information\nfor all instruments is provided in the form of a bounding box for object\ndetection. To handle class imbalance problem between instruments, synthesized\ninstruments modeled in Unity for 3D models are included as training data.\nBesides, for 3D instrument data, a polygon annotation is provided to enable\ninstance segmentation of the tool. To reflect the kinematic characteristics of\nall instruments, they are annotated with head and body parts for laparoscopic\ninstruments, and with head, wrist, and body parts for robotic instruments\nseparately. Annotation data of assistive tools (specimen bag, needle, etc.)\nthat are frequently used for surgery are also included. Moreover, we provide\nstatistical information on the hSDB-instrument dataset and the baseline\nlocalization performances of the object detection networks trained by the\nMMDetection library and resulting analyses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jihun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_S/0/1/0/all/0/1\">Sunghwan Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hayeong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Jayeon Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chi Hyun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">SeulGi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Seungbum Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1\">Bokyung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">SungHyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyung_W/0/1/0/all/0/1\">Woo Jin Hyung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1\">Min-Kook Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local plasticity rules can learn deep representations using self-supervised contrastive predictions. (arXiv:2010.08262v5 [cs.NE] CROSS LISTED)","link":"http://arxiv.org/abs/2010.08262","description":"<p>Learning in the brain is poorly understood and learning rules that respect\nbiological constraints, yet yield deep hierarchical representations, are still\nunknown. Here, we propose a learning rule that takes inspiration from\nneuroscience and recent advances in self-supervised deep learning. Learning\nminimizes a simple layer-specific loss function and does not need to\nback-propagate error signals within or between layers. Instead, weight updates\nfollow a local, Hebbian, learning rule that only depends on pre- and\npost-synaptic neuronal activity, predictive dendritic input and widely\nbroadcasted modulation factors which are identical for large groups of neurons.\nThe learning rule applies contrastive predictive learning to a causal,\nbiological setting using saccades (i.e. rapid shifts in gaze direction). We\nfind that networks trained with this self-supervised and local rule build deep\nhierarchical representations of images, speech and video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Illing_B/0/1/0/all/0/1\">Bernd Illing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventura_J/0/1/0/all/0/1\">Jean Ventura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellec_G/0/1/0/all/0/1\">Guillaume Bellec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1\">Wulfram Gerstner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks. (arXiv:2106.01862v2 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2106.01862","description":"<p>The field of neuromorphic computing promises extremely low-power and\nlow-latency sensing and processing. Challenges in transferring learning\nalgorithms from traditional artificial neural networks (ANNs) to spiking neural\nnetworks (SNNs) have so far prevented their application to large-scale, complex\nregression tasks. Furthermore, realizing a truly asynchronous and fully\nneuromorphic pipeline that maximally attains the abovementioned benefits\ninvolves rethinking the way in which this pipeline takes in and accumulates\ninformation. In the case of perception, spikes would be passed as-is and\none-by-one between an event camera and an SNN, meaning all temporal integration\nof information must happen inside the network. In this article, we tackle these\ntwo problems. We focus on the complex task of learning to estimate optical flow\nfrom event-based camera inputs in a self-supervised manner, and modify the\nstate-of-the-art ANN training pipeline to encode minimal temporal information\nin its inputs. Moreover, we reformulate the self-supervised loss function for\nevent-based optical flow to improve its convexity. We perform experiments with\nvarious types of recurrent ANNs and SNNs using the proposed pipeline.\nConcerning SNNs, we investigate the effects of elements such as parameter\ninitialization and optimization, surrogate gradient shape, and adaptive\nneuronal mechanisms. We find that initialization and surrogate gradient width\nplay a crucial part in enabling learning with sparse inputs, while the\ninclusion of adaptivity and learnable neuronal parameters can improve\nperformance. We show that the performance of the proposed ANNs and SNNs are on\npar with that of the current state-of-the-art ANNs trained in a self-supervised\nmanner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagenaars_J/0/1/0/all/0/1\">Jesse Hagenaars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paredes_Valles_F/0/1/0/all/0/1\">Federico Paredes-Vall&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1\">Guido de Croon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}