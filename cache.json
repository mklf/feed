{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.6","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-11-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"What makes us curious? analysis of a corpus of open-domain questions. (arXiv:2110.15409v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15409","description":"<p>Every day people ask short questions through smart devices or online forums\nto seek answers to all kinds of queries. With the increasing number of\nquestions collected it becomes difficult to provide answers to each of them,\nwhich is one of the reasons behind the growing interest in automated question\nanswering. Some questions are similar to existing ones that have already been\nanswered, while others could be answered by an external knowledge source such\nas Wikipedia. An important question is what can be revealed by analysing a\nlarge set of questions. In 2017, \"We the Curious\" science centre in Bristol\nstarted a project to capture the curiosity of Bristolians: the project\ncollected more than 10,000 questions on various topics. As no rules were given\nduring collection, the questions are truly open-domain, and ranged across a\nvariety of topics. One important aim for the science centre was to understand\nwhat concerns its visitors had beyond science, particularly on societal and\ncultural issues. We addressed this question by developing an Artificial\nIntelligence tool that can be used to perform various processing tasks:\ndetection of equivalence between questions; detection of topic and type; and\nanswering of the question. As we focused on the creation of a \"generalist\"\ntool, we trained it with labelled data from different datasets. We called the\nresulting model QBERT. This paper describes what information we extracted from\nthe automated analysis of the WTC corpus of open-domain questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhaozhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howarth_A/0/1/0/all/0/1\">Amelia Howarth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briggs_N/0/1/0/all/0/1\">Nicole Briggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristianini_N/0/1/0/all/0/1\">Nello Cristianini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RadBERT-CL: Factually-Aware Contrastive Learning For Radiology Report Classification. (arXiv:2110.15426v1 [cs.LG])","link":"http://arxiv.org/abs/2110.15426","description":"<p>Radiology reports are unstructured and contain the imaging findings and\ncorresponding diagnoses transcribed by radiologists which include clinical\nfacts and negated and/or uncertain statements. Extracting pathologic findings\nand diagnoses from radiology reports is important for quality control,\npopulation health, and monitoring of disease progress. Existing works,\nprimarily rely either on rule-based systems or transformer-based pre-trained\nmodel fine-tuning, but could not take the factual and uncertain information\ninto consideration, and therefore generate false-positive outputs. In this\nwork, we introduce three sedulous augmentation techniques which retain factual\nand critical information while generating augmentations for contrastive\nlearning. We introduce RadBERT-CL, which fuses these information into BlueBert\nvia a self-supervised contrastive loss. Our experiments on MIMIC-CXR show\nsuperior performance of RadBERT-CL on fine-tuning for multi-class, multi-label\nreport classification. We illustrate that when few labeled data are available,\nRadBERT-CL outperforms conventional SOTA transformers (BERT/BlueBert) by\nsignificantly larger margins (6-11%). We also show that the representations\nlearned by RadBERT-CL can capture critical medical information in the latent\nspace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Ajay Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liyan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_M/0/1/0/all/0/1\">Meheli Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rousseau_J/0/1/0/all/0/1\">Justin Rousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Noise Robustness of Contrastive Speech Representation Learning with Speech Reconstruction. (arXiv:2110.15430v1 [cs.SD])","link":"http://arxiv.org/abs/2110.15430","description":"<p>Noise robustness is essential for deploying automatic speech recognition\n(ASR) systems in real-world environments. One way to reduce the effect of noise\ninterference is to employ a preprocessing module that conducts speech\nenhancement, and then feed the enhanced speech to an ASR backend. In this work,\ninstead of suppressing background noise with a conventional cascaded pipeline,\nwe employ a noise-robust representation learned by a refined self-supervised\nframework for noisy speech recognition. We propose to combine a reconstruction\nmodule with contrastive learning and perform multi-task continual pre-training\non noisy data. The reconstruction module is used for auxiliary learning to\nimprove the noise robustness of the learned representation and thus is not\nrequired during inference. Experiments demonstrate the effectiveness of our\nproposed method. Our model substantially reduces the word error rate (WER) for\nthe synthesized noisy LibriSpeech test sets, and yields around 4.1/7.5% WER\nreduction on noisy clean/other test sets compared to data augmentation. For the\nreal-world noisy speech from the CHiME-4 challenge (1-channel track), we have\nobtained the state of the art ASR performance without any denoising front-end.\nMoreover, we achieve comparable performance to the best supervised approach\nreported with only 16% of labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">DeLiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Text Analytics for Health to Get Meaningful Insights from a Corpus of COVID Scientific Papers. (arXiv:2110.15453v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15453","description":"<p>Since the beginning of COVID pandemic, there have been around 700000\nscientific papers published on the subject. A human researcher cannot possibly\nget acquainted with such a huge text corpus -- and therefore developing\nAI-based tools to help navigating this corpus and deriving some useful insights\nfrom it is highly needed. In this paper, we will use Text Analytics for Health\npre-trained service together with some cloud tools to extract some knowledge\nfrom scientific papers, gain insights, and build a tool to help researcher\nnavigate the paper collection in a meaningful way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soshnikov_D/0/1/0/all/0/1\">Dmitry Soshnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soshnikova_V/0/1/0/all/0/1\">Vickie Soshnikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Personal Food Preferences via Food Logs Embedding. (arXiv:2110.15498v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15498","description":"<p>Diet management is key to managing chronic diseases such as diabetes.\nAutomated food recommender systems may be able to assist by providing meal\nrecommendations that conform to a user's nutrition goals and food preferences.\nCurrent recommendation systems suffer from a lack of accuracy that is in part\ndue to a lack of knowledge of food preferences, namely foods users like to and\nare able to eat frequently. In this work, we propose a method for learning food\npreferences from food logs, a comprehensive but noisy source of information\nabout users' dietary habits. We also introduce accompanying metrics. The method\ngenerates and compares word embeddings to identify the parent food category of\neach food entry and then calculates the most popular. Our proposed approach\nidentifies 82% of a user's ten most frequently eaten foods. Our method is\npublicly available on (https://github.com/aametwally/LearningFoodPreferences)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Metwally_A/0/1/0/all/0/1\">Ahmed A. Metwally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_A/0/1/0/all/0/1\">Ariel K. Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_A/0/1/0/all/0/1\">Aman Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagarjuna_A/0/1/0/all/0/1\">Anvith Nagarjuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perelman_D/0/1/0/all/0/1\">Dalia Perelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snyder_M/0/1/0/all/0/1\">Michael Snyder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training Co-evolutionary Protein Representation via A Pairwise Masked Language Model. (arXiv:2110.15527v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15527","description":"<p>Understanding protein sequences is vital and urgent for biology, healthcare,\nand medicine. Labeling approaches are expensive yet time-consuming, while the\namount of unlabeled data is increasing quite faster than that of the labeled\ndata due to low-cost, high-throughput sequencing methods. In order to extract\nknowledge from these unlabeled data, representation learning is of significant\nvalue for protein-related tasks and has great potential for helping us learn\nmore about protein functions and structures. The key problem in the protein\nsequence representation learning is to capture the co-evolutionary information\nreflected by the inter-residue co-variation in the sequences. Instead of\nleveraging multiple sequence alignment as is usually done, we propose a novel\nmethod to capture this information directly by pre-training via a dedicated\nlanguage model, i.e., Pairwise Masked Language Model (PMLM). In a conventional\nmasked language model, the masked tokens are modeled by conditioning on the\nunmasked tokens only, but processed independently to each other. However, our\nproposed PMLM takes the dependency among masked tokens into consideration,\ni.e., the probability of a token pair is not equal to the product of the\nprobability of the two tokens. By applying this model, the pre-trained encoder\nis able to generate a better representation for protein sequences. Our result\nshows that the proposed method can effectively capture the inter-residue\ncorrelations and improves the performance of contact prediction by up to 9%\ncompared to the MLM baseline under the same setting. The proposed model also\nsignificantly outperforms the MSA baseline by more than 7% on the TAPE contact\nprediction benchmark when pre-trained on a subset of the sequence database\nwhich the MSA is generated from, revealing the potential of the sequence\npre-training method to surpass MSA based methods in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shizhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Huanhuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_F/0/1/0/all/0/1\">Fusong Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_P/0/1/0/all/0/1\">Pan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_B/0/1/0/all/0/1\">Bin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing. (arXiv:2110.15534v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15534","description":"<p>Predicting linearized Abstract Meaning Representation (AMR) graphs using\npre-trained sequence-to-sequence Transformer models has recently led to large\nimprovements on AMR parsing benchmarks. These parsers are simple and avoid\nexplicit modeling of structure but lack desirable properties such as graph\nwell-formedness guarantees or built-in graph-sentence alignments. In this work\nwe explore the integration of general pre-trained sequence-to-sequence language\nmodels and a structure-aware transition-based approach. We depart from a\npointer-based transition system and propose a simplified transition set,\ndesigned to better exploit pre-trained language models for structured\nfine-tuning. We also explore modeling the parser state within the pre-trained\nencoder-decoder architecture and different vocabulary strategies for the same\npurpose. We provide a detailed comparison with recent progress in AMR parsing\nand show that the proposed parser retains the desirable properties of previous\ntransition-based approaches, while being simpler and reaching the new parsing\nstate of the art for AMR 2.0, without the need for graph re-categorization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiawei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1\">Tahira Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Ram&#xf3;n Fernandez Astudillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Young-Suk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1\">Salim Roukos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Hand Sign Recognition: Identify Unusuality through Latent Cognizance. (arXiv:2110.15542v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15542","description":"<p>Sign language is a main communication channel among hearing disability\ncommunity. Automatic sign language transcription could facilitate better\ncommunication and understanding between hearing disability community and\nhearing majority. As a recent work in automatic sign language transcription has\ndiscussed, effectively handling or identifying a non-sign posture is one of the\nkey issues. A non-sign posture is a posture unintended for sign reading and\ndoes not belong to any valid sign. A non-sign posture may arise during sign\ntransition or simply from an unaware posture. Confidence ratio has been\nproposed to mitigate the issue. Confidence ratio is simple to compute and\nreadily available without extra training. However, confidence ratio is reported\nto only partially address the problem. In addition, confidence ratio\nformulation is susceptible to computational instability. This article proposes\nalternative formulations to confidence ratio, investigates an issue of non-sign\nidentification for Thai Finger Spelling recognition, explores potential\nsolutions and has found a promising direction. Not only does this finding\naddress the issue of non-sign identification, it also provide some insight\nbehind a well-learned inference machine, revealing hidden meaning and new\ninterpretation of the underlying mechanism. Our proposed methods are evaluated\nand shown to be effective for non-sign detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakjai_P/0/1/0/all/0/1\">Pisit Nakjai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katanyukul_T/0/1/0/all/0/1\">Tatpong Katanyukul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handshakes AI Research at CASE 2021 Task 1: Exploring different approaches for multilingual tasks. (arXiv:2110.15599v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15599","description":"<p>The aim of the CASE 2021 Shared Task 1 (H\\\"urriyeto\\u{g}lu et al., 2021) was\nto detect and classify socio-political and crisis event information at\ndocument, sentence, cross-sentence, and token levels in a multilingual setting,\nwith each of these subtasks being evaluated separately in each test language.\nOur submission contained entries in all of the subtasks, and the scores\nobtained validated our research finding: That the multilingual aspect of the\ntasks should be embraced, so that modeling and training regimes use the\nmultilingual nature of the tasks to their mutual benefit, rather than trying to\ntackle the different languages separately. Our code is available at\nhttps://github.com/HandshakesByDC/case2021/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_V/0/1/0/all/0/1\">Vivek Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Paul Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shaun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_M/0/1/0/all/0/1\">Martin Andrews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare. (arXiv:2110.15621v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15621","description":"<p>Mental health is a critical issue in modern society, and mental disorders\ncould sometimes turn to suicidal ideation without adequate treatment. Early\ndetection of mental disorders and suicidal ideation from social content\nprovides a potential way for effective social intervention. Recent advances in\npretrained contextualized language representations have promoted the\ndevelopment of several domain-specific pretrained models and facilitated\nseveral downstream applications. However, there are no existing pretrained\nlanguage models for mental healthcare. This paper trains and release two\npretrained masked language models, i.e., MentalBERT and MentalRoBERTa, to\nbenefit machine learning for the mental healthcare research community. Besides,\nwe evaluate our trained domain-specific models and several variants of\npretrained language models on several mental disorder detection benchmarks and\ndemonstrate that language representations pretrained in the target domain\nimprove the performance of mental health detection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ansari_L/0/1/0/all/0/1\">Luna Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1\">Prayag Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Path-Enhanced Multi-Relational Question Answering with Knowledge Graph Embeddings. (arXiv:2110.15622v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15622","description":"<p>The multi-relational Knowledge Base Question Answering (KBQA) system performs\nmulti-hop reasoning over the knowledge graph (KG) to achieve the answer. Recent\napproaches attempt to introduce the knowledge graph embedding (KGE) technique\nto handle the KG incompleteness but only consider the triple facts and neglect\nthe significant semantic correlation between paths and multi-relational\nquestions. In this paper, we propose a Path and Knowledge Embedding-Enhanced\nmulti-relational Question Answering model (PKEEQA), which leverages multi-hop\npaths between entities in the KG to evaluate the ambipolar correlation between\na path embedding and a multi-relational question embedding via a customizable\npath representation mechanism, benefiting for achieving more accurate answers\nfrom the perspective of both the triple facts and the extra paths. Experimental\nresults illustrate that PKEEQA improves KBQA models' performance for\nmulti-relational question answering with explainability to some extent derived\nfrom paths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Guanglin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chengguang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhongkai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shibin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Amendable Generation for Dialogue State Tracking. (arXiv:2110.15659v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15659","description":"<p>In task-oriented dialogue systems, recent dialogue state tracking methods\ntend to perform one-pass generation of the dialogue state based on the previous\ndialogue state. The mistakes of these models made at the current turn are prone\nto be carried over to the next turn, causing error propagation. In this paper,\nwe propose a novel Amendable Generation for Dialogue State Tracking (AG-DST),\nwhich contains a two-pass generation process: (1) generating a primitive\ndialogue state based on the dialogue of the current turn and the previous\ndialogue state, and (2) amending the primitive dialogue state from the first\npass. With the additional amending generation pass, our model is tasked to\nlearn more robust dialogue state tracking by amending the errors that still\nexist in the primitive dialogue state, which plays the role of reviser in the\ndouble-checking process and alleviates unnecessary error propagation.\nExperimental results show that AG-DST significantly outperforms previous works\nin two active DST datasets (MultiWOZ 2.2 and WOZ 2.0), achieving new\nstate-of-the-art performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liankai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Siqi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuqi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of ADoBo 2021: Automatic Detection of Unassimilated Borrowings in the Spanish Press. (arXiv:2110.15682v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15682","description":"<p>This paper summarizes the main findings of the ADoBo 2021 shared task,\nproposed in the context of IberLef 2021. In this task, we invited participants\nto detect lexical borrowings (coming mostly from English) in Spanish newswire\ntexts. This task was framed as a sequence classification problem using BIO\nencoding. We provided participants with an annotated corpus of lexical\nborrowings which we split into training, development and test splits. We\nreceived submissions from 4 teams with 9 different system runs overall. The\nresults, which range from F1 scores of 37 to 85, suggest that this is a\nchallenging task, especially when out-of-domain or OOV words are considered,\nand that traditional methods informed with lexicographic information would\nbenefit from taking advantage of current NLP trends.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mellado_E/0/1/0/all/0/1\">Elena &#xc1;lvarez Mellado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anke_L/0/1/0/all/0/1\">Luis Espinosa Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arroyo_J/0/1/0/all/0/1\">Julio Gonzalo Arroyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamorano_J/0/1/0/all/0/1\">Jordi Porta Zamorano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing ASR Outputs in Joint Training for Speech Emotion Recognition. (arXiv:2110.15684v1 [eess.AS])","link":"http://arxiv.org/abs/2110.15684","description":"<p>Alongside acoustic information, linguistic features based on speech\ntranscripts have been proven useful in Speech Emotion Recognition (SER).\nHowever, due to the scarcity of emotion labelled data and the difficulty of\nrecognizing emotional speech, it is hard to obtain reliable linguistic features\nand models in this research area. In this paper, we propose to fuse Automatic\nSpeech Recognition (ASR) outputs into the pipeline for joint training SER. The\nrelationship between ASR and SER is understudied, and it is unclear what and\nhow ASR features benefit SER. By examining various ASR outputs and fusion\nmethods, our experiments show that in joint ASR-SER training, incorporating\nboth ASR hidden and text output using a hierarchical co-attention fusion\napproach improves the SER performance the most. On the IEMOCAP corpus, our\napproach achieves 63.4% weighted accuracy, which is close to the baseline\nresults achieved by combining ground-truth transcripts. In addition, we also\npresent novel word error rate analysis on IEMOCAP and layer-difference analysis\nof the Wav2vec 2.0 model to better understand the relationship between ASR and\nSER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigating the Kaleidoscope of COVID-19 Misinformation Using Deep Learning. (arXiv:2110.15703v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15703","description":"<p>Irrespective of the success of the deep learning-based mixed-domain transfer\nlearning approach for solving various Natural Language Processing tasks, it\ndoes not lend a generalizable solution for detecting misinformation from\nCOVID-19 social media data. Due to the inherent complexity of this type of\ndata, caused by its dynamic (context evolves rapidly), nuanced (misinformation\ntypes are often ambiguous), and diverse (skewed, fine-grained, and overlapping\ncategories) nature, it is imperative for an effective model to capture both the\nlocal and global context of the target domain. By conducting a systematic\ninvestigation, we show that: (i) the deep Transformer-based pre-trained models,\nutilized via the mixed-domain transfer learning, are only good at capturing the\nlocal context, thus exhibits poor generalization, and (ii) a combination of\nshallow network-based domain-specific models and convolutional neural networks\ncan efficiently extract local as well as global context directly from the\ntarget data in a hierarchical fashion, enabling it to offer a more\ngeneralizable solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanzhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mohammad Rashedul Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influence of ASR and Language Model on Alzheimer's Disease Detection. (arXiv:2110.15704v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15704","description":"<p>Alzheimer's Disease is the most common form of dementia. Automatic detection\nfrom speech could help to identify symptoms at early stages, so that preventive\nactions can be carried out. This research is a contribution to the ADReSSo\nChallenge, we analyze the usage of a SotA ASR system to transcribe\nparticipant's spoken descriptions from a picture. We analyse the loss of\nperformance regarding the use of human transcriptions (measured using\ntranscriptions from the 2020 ADReSS Challenge). Furthermore, we study the\ninfluence of a language model -- which tends to correct non-standard sequences\nof words -- with the lack of language model to decode the hypothesis from the\nASR. This aims at studying the language bias and get more meaningful\ntranscriptions based only on the acoustic information from patients. The\nproposed system combines acoustic -- based on prosody and voice quality -- and\nlexical features based on the first occurrence of the most common words. The\nreported results show the effect of using automatic transcripts with or without\nlanguage model. The best fully automatic system achieves up to 76.06 % of\naccuracy (without language model), significantly higher, 3 % above, than a\nsystem employing word transcriptions decoded using general purpose language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Codina_Filba_J/0/1/0/all/0/1\">Joan Codina-Filb&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambara_G/0/1/0/all/0/1\">Guillermo C&#xe1;mbara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luque_J/0/1/0/all/0/1\">Jordi Luque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farrus_M/0/1/0/all/0/1\">Mireia Farr&#xfa;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Relation Embeddings from Pre-trained Language Models. (arXiv:2110.15705v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15705","description":"<p>Pre-trained language models have been found to capture a surprisingly rich\namount of lexical knowledge, ranging from commonsense properties of everyday\nconcepts to detailed factual knowledge about named entities. Among others, this\nmakes it possible to distill high-quality word vectors from pre-trained\nlanguage models. However, it is currently unclear to what extent it is possible\nto distill relation embeddings, i.e. vectors that characterize the relationship\nbetween two words. Such relation embeddings are appealing because they can, in\nprinciple, encode relational knowledge in a more fine-grained way than is\npossible with knowledge graphs. To obtain relation embeddings from a\npre-trained language model, we encode word pairs using a (manually or\nautomatically generated) prompt, and we fine-tune the language model such that\nrelationally similar word pairs yield similar output vectors. We find that the\nresulting relation embeddings are highly competitive on analogy (unsupervised)\nand relation classification (supervised) benchmarks, even without any\ntask-specific fine-tuning. Source code to reproduce our experimental results\nand the model checkpoints are available in the following repository:\nhttps://github.com/asahi417/relbert\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1\">Asahi Ushio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1\">Steven Schockaert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Deep Event-Level and Script-Level Information for Script Event Prediction. (arXiv:2110.15706v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15706","description":"<p>Scripts are structured sequences of events together with the participants,\nwhich are extracted from the texts.Script event prediction aims to predict the\nsubsequent event given the historical events in the script. Two kinds of\ninformation facilitate this task, namely, the event-level information and the\nscript-level information. At the event level, existing studies view an event as\na verb with its participants, while neglecting other useful properties, such as\nthe state of the participants. At the script level, most existing studies only\nconsider a single event sequence corresponding to one common protagonist. In\nthis paper, we propose a Transformer-based model, called MCPredictor, which\nintegrates deep event-level and script-level information for script event\nprediction. At the event level, MCPredictor utilizes the rich information in\nthe text to obtain more comprehensive event semantic representations. At the\nscript-level, it considers multiple event sequences corresponding to different\nparticipants of the subsequent event. The experimental results on the\nwidely-used New York Times corpus demonstrate the effectiveness and superiority\nof the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Long Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Saiping Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaolong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hidden Markov Based Mathematical Model dedicated to Extract Ingredients from Recipe Text. (arXiv:2110.15707v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15707","description":"<p>Natural Language Processing (NLP) is a branch of artificial intelligence that\ngives machines the ability to decode human languages. Partof-speech tagging\n(POS tagging) is a pre-processing task that requires an annotated corpus.\nRule-based and stochastic methods showed remarkable results for POS tag\nprediction. On this work, I performed a mathematical model based on Hidden\nMarkov structures and I obtained a high-level accuracy of ingredients extracted\nfrom text recipe with performances greater than what traditional methods could\nmake without unknown words consideration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baklouti_Z/0/1/0/all/0/1\">Zied Baklouti</a> (UP, ENIT)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural sentence embedding models for semantic similarity estimation in the biomedical domain. (arXiv:2110.15708v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15708","description":"<p>BACKGROUND: In this study, we investigated the efficacy of current\nstate-of-the-art neural sentence embedding models for semantic similarity\nestimation of sentences from biomedical literature. We trained different neural\nembedding models on 1.7 million articles from the PubMed Open Access dataset,\nand evaluated them based on a biomedical benchmark set containing 100 sentence\npairs annotated by human experts and a smaller contradiction subset derived\nfrom the original benchmark set.\n</p>\n<p>RESULTS: With a Pearson correlation of 0.819, our best unsupervised model\nbased on the Paragraph Vector Distributed Memory algorithm outperforms previous\nstate-of-the-art results achieved on the BIOSSES biomedical benchmark set.\nMoreover, our proposed supervised model that combines different string-based\nsimilarity metrics with a neural embedding model surpasses previous\nontology-dependent supervised state-of-the-art approaches in terms of Pearson's\nr (r=0.871) on the biomedical benchmark set. In contrast to the promising\nresults for the original benchmark, we found our best models' performance on\nthe smaller contradiction subset to be poor.\n</p>\n<p>CONCLUSIONS: In this study we highlighted the value of neural network-based\nmodels for semantic similarity estimation in the biomedical domain by showing\nthat they can keep up with and even surpass previous state-of-the-art\napproaches for semantic similarity estimation that depend on the availability\nof laboriously curated ontologies when evaluated on a biomedical benchmark set.\nCapturing contradictions and negations in biomedical sentences, however,\nemerged as an essential area for further work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blagec_K/0/1/0/all/0/1\">Kathrin Blagec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agibetov_A/0/1/0/all/0/1\">Asan Agibetov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LegalNLP -- Natural Language Processing methods for the Brazilian Legal Language. (arXiv:2110.15709v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15709","description":"<p>We present and make available pre-trained language models (Phraser, Word2Vec,\nDoc2Vec, FastText, and BERT) for the Brazilian legal language, a Python package\nwith functions to facilitate their use, and a set of demonstrations/tutorials\ncontaining some applications involving them. Given that our material is built\nupon legal texts coming from several Brazilian courts, this initiative is\nextremely helpful for the Brazilian legal field, which lacks other open and\nspecific tools and language models. Our main objective is to catalyze the use\nof natural language processing tools for legal texts analysis by the Brazilian\nindustry, government, and academia, providing the necessary tools and\naccessible material.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polo_F/0/1/0/all/0/1\">Felipe Maia Polo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendonca_G/0/1/0/all/0/1\">Gabriel Caiaffa Floriano Mendon&#xe7;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parreira_K/0/1/0/all/0/1\">Kau&#xea; Capellato J. Parreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gianvechio_L/0/1/0/all/0/1\">Lucka Gianvechio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordeiro_P/0/1/0/all/0/1\">Peterson Cordeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_J/0/1/0/all/0/1\">Jonathan Batista Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lima_L/0/1/0/all/0/1\">Leticia Maria Paz de Lima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maia_A/0/1/0/all/0/1\">Ant&#xf4;nio Carlos do Amaral Maia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicente_R/0/1/0/all/0/1\">Renato Vicente</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of hierarchical text using geometric deep learning: the case of clinical trials corpus. (arXiv:2110.15710v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15710","description":"<p>We consider the hierarchical representation of documents as graphs and use\ngeometric deep learning to classify them into different categories. While graph\nneural networks can efficiently handle the variable structure of hierarchical\ndocuments using the permutation invariant message passing operations, we show\nthat we can gain extra performance improvements using our proposed selective\ngraph pooling operation that arises from the fact that some parts of the\nhierarchy are invariable across different documents. We applied our model to\nclassify clinical trial (CT) protocols into completed and terminated\ncategories. We use bag-of-words based, as well as pre-trained transformer-based\nembeddings to featurize the graph nodes, achieving f1-scores around 0.85 on a\npublicly available large scale CT registry of around 360K protocols. We further\ndemonstrate how the selective pooling can add insights into the CT termination\nstatus prediction. We make the source code and dataset splits accessible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferdowsi_S/0/1/0/all/0/1\">Sohrab Ferdowsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borissov_N/0/1/0/all/0/1\">Nikolay Borissov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knafou_J/0/1/0/all/0/1\">Julien Knafou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_P/0/1/0/all/0/1\">Poorya Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodoro_D/0/1/0/all/0/1\">Douglas Teodoro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysing the Effect of Masking Length Distribution of MLM: An Evaluation Framework and Case Study on Chinese MRC Datasets. (arXiv:2110.15712v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15712","description":"<p>Machine reading comprehension (MRC) is a challenging natural language\nprocessing (NLP) task. Recently, the emergence of pre-trained models (PTM) has\nbrought this research field into a new era, in which the training objective\nplays a key role. The masked language model (MLM) is a self-supervised training\nobjective that widely used in various PTMs. With the development of training\nobjectives, many variants of MLM have been proposed, such as whole word\nmasking, entity masking, phrase masking, span masking, and so on. In different\nMLM, the length of the masked tokens is different. Similarly, in different\nmachine reading comprehension tasks, the length of the answer is also\ndifferent, and the answer is often a word, phrase, or sentence. Thus, in MRC\ntasks with different answer lengths, whether the length of MLM is related to\nperformance is a question worth studying. If this hypothesis is true, it can\nguide us how to pre-train the MLM model with a relatively suitable mask length\ndistribution for MRC task. In this paper, we try to uncover how much of MLM's\nsuccess in the machine reading comprehension tasks comes from the correlation\nbetween masking length distribution and answer length in MRC dataset. In order\nto address this issue, herein, (1) we propose four MRC tasks with different\nanswer length distributions, namely short span extraction task, long span\nextraction task, short multiple-choice cloze task, long multiple-choice cloze\ntask; (2) four Chinese MRC datasets are created for these tasks; (3) we also\nhave pre-trained four masked language models according to the answer length\ndistributions of these datasets; (4) ablation experiments are conducted on the\ndatasets to verify our hypothesis. The experimental results demonstrate that\nour hypothesis is true.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Changchang. Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaobo. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building the Language Resource for a Cebuano-Filipino Neural Machine Translation System. (arXiv:2110.15716v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15716","description":"<p>Parallel corpus is a critical resource in machine learning-based translation.\nThe task of collecting, extracting, and aligning texts in order to build an\nacceptable corpus for doing the translation is very tedious most especially for\nlow-resource languages. In this paper, we present the efforts made to build a\nparallel corpus for Cebuano and Filipino from two different domains: biblical\ntexts and the web. For the biblical resource, subword unit translation for\nverbs and copy-able approach for nouns were applied to correct inconsistencies\nin the translation. This correction mechanism was applied as a preprocessing\ntechnique. On the other hand, for Wikipedia being the main web resource,\ncommonly occurring topic segments were extracted from both the source and the\ntarget languages. These observed topic segments are unique in 4 different\ncategories. The identification of these topic segments may be used for the\nautomatic extraction of sentences. A Recurrent Neural Network was used to\nimplement the translation using OpenNMT sequence modeling tool in TensorFlow.\nThe two different corpora were then evaluated by using them as two separate\ninputs in the neural network. Results have shown a difference in BLEU scores in\nboth corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adlaon_K/0/1/0/all/0/1\">Kristine Mae Adlaon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcos_N/0/1/0/all/0/1\">Nelson Marcos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LIDSNet: A Lightweight on-device Intent Detection model using Deep Siamese Network. (arXiv:2110.15717v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15717","description":"<p>Intent detection is a crucial task in any Natural Language Understanding\n(NLU) system and forms the foundation of a task-oriented dialogue system. To\nbuild high-quality real-world conversational solutions for edge devices, there\nis a need for deploying intent detection model on device. This necessitates a\nlight-weight, fast, and accurate model that can perform efficiently in a\nresource-constrained environment. To this end, we propose LIDSNet, a novel\nlightweight on-device intent detection model, which accurately predicts the\nmessage intent by utilizing a Deep Siamese Network for learning better sentence\nrepresentations. We use character-level features to enrich the sentence-level\nrepresentations and empirically demonstrate the advantage of transfer learning\nby utilizing pre-trained embeddings. Furthermore, to investigate the efficacy\nof the modules in our architecture, we conduct an ablation study and arrive at\nour optimal model. Experimental results prove that LIDSNet achieves\nstate-of-the-art competitive accuracy of 98.00% and 95.97% on SNIPS and ATIS\npublic datasets respectively, with under 0.59M parameters. We further benchmark\nLIDSNet against fine-tuned BERTs and show that our model is at least 41x\nlighter and 30x faster during inference than MobileBERT on Samsung Galaxy S20\ndevice, justifying its efficiency on resource-constrained edge devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1\">Vibhav Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shivnikar_S/0/1/0/all/0/1\">Sudeep Deepak Shivnikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sourav Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_H/0/1/0/all/0/1\">Himanshu Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_Y/0/1/0/all/0/1\">Yashwant Saini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep convolutional forest: a dynamic deep ensemble approach for spam detection in text. (arXiv:2110.15718v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15718","description":"<p>The increase in people's use of mobile messaging services has led to the\nspread of social engineering attacks like phishing, considering that spam text\nis one of the main factors in the dissemination of phishing attacks to steal\nsensitive data such as credit cards and passwords. In addition, rumors and\nincorrect medical information regarding the COVID-19 pandemic are widely shared\non social media leading to people's fear and confusion. Thus, filtering spam\ncontent is vital to reduce risks and threats. Previous studies relied on\nmachine learning and deep learning approaches for spam classification, but\nthese approaches have two limitations. Machine learning models require manual\nfeature engineering, whereas deep neural networks require a high computational\ncost. This paper introduces a dynamic deep ensemble model for spam detection\nthat adjusts its complexity and extracts features automatically. The proposed\nmodel utilizes convolutional and pooling layers for feature extraction along\nwith base classifiers such as random forests and extremely randomized trees for\nclassifying texts into spam or legitimate ones. Moreover, the model employs\nensemble learning procedures like boosting and bagging. As a result, the model\nachieved high precision, recall, f1-score and accuracy of 98.38%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaaban_M/0/1/0/all/0/1\">Mai A. Shaaban</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_Y/0/1/0/all/0/1\">Yasser F. Hassan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Guirguis_S/0/1/0/all/0/1\">Shawkat K. Guirguis</a> (3) ((1) Department of Mathematics and Computer Science, Faculty of Science, Alexandria University, Alexandria, Egypt, (2) Faculty of Computers and Data Science, Alexandria University, Alexandria, Egypt, (3) Institute of Graduate Studies and Research, Alexandria University, Alexandria, Egypt)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Concept Map Generation through Task-Guided Graph Translation. (arXiv:2110.15720v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15720","description":"<p>Recent years have witnessed the rapid development of concept map generation\ntechniques due to their advantages in providing well-structured summarization\nof knowledge from free texts. Traditional unsupervised methods do not generate\ntask-oriented concept maps, whereas deep generative models require large\namounts of training data. In this work, we present GT-D2G (Graph Translation\nbased Document-To-Graph), an automatic concept map generation framework that\nleverages generalized NLP pipelines to derive semantic-rich initial graphs, and\ntranslates them into more concise structures under the weak supervision of\ndocument labels. The quality and interpretability of such concept maps are\nvalidated through human evaluation on three real-world corpora, and their\nutility in the downstream task is further demonstrated in the controlled\nexperiments with scarce document labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiaying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiangjue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl J. Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paperswithtopic: Topic Identification from Paper Title Only. (arXiv:2110.15721v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15721","description":"<p>The deep learning field is growing rapidly as witnessed by the exponential\ngrowth of papers submitted to journals, conferences, and pre-print servers. To\ncope with the sheer number of papers, several text mining tools from natural\nlanguage processing (NLP) have been proposed that enable researchers to keep\ntrack of recent findings. In this context, our paper makes two main\ncontributions: first, we collected and annotated a dataset of papers paired by\ntitle and sub-field from the field of artificial intelligence (AI), and,\nsecond, we present results on how to predict a paper's AI sub-field from a\ngiven paper title only. Importantly, for the latter, short-text classification\ntask we compare several algorithms from conventional machine learning all the\nway up to recent, larger transformer architectures. Finally, for the\ntransformer models, we also present gradient-based, attention visualizations to\nfurther explain the model's classification process. All code can be found at\n\\url{https://github.com/1pha/paperswithtopic}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_D/0/1/0/all/0/1\">Daehyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallraven_C/0/1/0/all/0/1\">Christian Wallraven</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Sequence Tagging Framework for Consumer Event-Cause Extraction. (arXiv:2110.15722v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15722","description":"<p>Consumer Event-Cause Extraction, the task aimed at extracting the potential\ncauses behind certain events in the text, has gained much attention in recent\nyears due to its wide applications. The ICDM 2020 conference sets up an\nevaluation competition that aims to extract events and the causes of the\nextracted events with a specified subject (a brand or product). In this task,\nwe mainly focus on how to construct an end-to-end model, and extract multiple\nevent types and event-causes simultaneously. To this end, we introduce a fresh\nperspective to revisit the relational event-cause extraction task and propose a\nnovel sequence tagging framework, instead of extracting event types and\nevents-causes separately. Experiments show our framework outperforms baseline\nmethods even when its encoder module uses an initialized pre-trained BERT\nencoder, showing the power of the new tagging framework. In this competition,\nour team achieved 1st place in the first stage leaderboard, and 3rd place in\nthe final stage leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Congqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yukun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SP-GPT2: Semantics Improvement in Vietnamese Poetry Generation. (arXiv:2110.15723v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15723","description":"<p>Automatic text generation has garnered growing attention in recent years as\nan essential step towards computer creativity. Generative Pretraining\nTransformer 2 (GPT2) is one of the state of the art approaches that have\nexcellent successes. In this paper, we took the first step to investigate the\npower of GPT2 in traditional Vietnamese poetry generation. In the earlier time,\nour experiment with base GPT2 was quite good at generating the poem in the\nproper template. Though it can learn the patterns, including rhyme and tone\nrules, from the training data, like almost all other text generation\napproaches, the poems generated still has a topic drift and semantic\ninconsistency. To improve the cohesion within the poems, we proposed a new\nmodel SP-GPT2 (semantic poem GPT2) which was built on the top GPT2 model and an\nadditional loss to constrain context throughout the entire poem. For better\nevaluation, we examined the methods by both automatic quantitative evaluation\nand human evaluation. Both automatic and human evaluation demonstrated that our\napproach can generate poems that have better cohesion without losing the\nquality due to additional loss. At the same time, we are the pioneers of this\ntopic. We released the first computational scoring module for poems generated\nin the template containing the style rule dictionary. Additionally, we are the\nfirst to publish a Luc-Bat dataset, including 87609 Luc Bat poems, which is\nequivalent to about 2.6 million sentences, combined with about 83579 poems in\nother styles was also published for further exploration. The code is available\nat https://github.com/fsoft-ailab/Poem-Generator\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hanh Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Truong Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_D/0/1/0/all/0/1\">Duc Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phong Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Learn End-to-End Goal-Oriented Dialog From Related Dialog Tasks. (arXiv:2110.15724v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15724","description":"<p>For each goal-oriented dialog task of interest, large amounts of data need to\nbe collected for end-to-end learning of a neural dialog system. Collecting that\ndata is a costly and time-consuming process. Instead, we show that we can use\nonly a small amount of data, supplemented with data from a related dialog task.\nNaively learning from related data fails to improve performance as the related\ndata can be inconsistent with the target task. We describe a meta-learning\nbased method that selectively learns from the related dialog task data. Our\napproach leads to significant accuracy improvements in an example dialog task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajendran_J/0/1/0/all/0/1\">Janarthanan Rajendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1\">Jonathan K. Kummerfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satinder Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Batch-Softmax Contrastive Loss for Pairwise Sentence Scoring Tasks. (arXiv:2110.15725v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15725","description":"<p>The use of contrastive loss for representation learning has become prominent\nin computer vision, and it is now getting attention in Natural Language\nProcessing (NLP). Here, we explore the idea of using a batch-softmax\ncontrastive loss when fine-tuning large-scale pre-trained transformer models to\nlearn better task-specific sentence embeddings for pairwise sentence scoring\ntasks. We introduce and study a number of variations in the calculation of the\nloss as well as in the overall training procedure; in particular, we find that\ndata shuffling can be quite important. Our experimental results show sizable\nimprovements on a number of datasets and pairwise sentence scoring tasks\nincluding classification, ranking, and regression. Finally, we offer detailed\nanalysis and discussion, which should be useful for researchers aiming to\nexplore the utility of contrastive loss in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1\">Anton Chernyavskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilvovsky_D/0/1/0/all/0/1\">Dmitry Ilvovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinin_P/0/1/0/all/0/1\">Pavel Kalinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Media Reveals Urban-Rural Differences in Stress across China. (arXiv:2110.15726v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15726","description":"<p>Modeling differential stress expressions in urban and rural regions in China\ncan provide a better understanding of the effects of urbanization on\npsychological well-being in a country that has rapidly grown economically in\nthe last two decades. This paper studies linguistic differences in the\nexperiences and expressions of stress in urban-rural China from Weibo posts\nfrom over 65,000 users across 329 counties using hierarchical mixed-effects\nmodels. We analyzed phrases, topical themes, and psycho-linguistic word choices\nin Weibo posts mentioning stress to better understand appraisal differences\nsurrounding psychological stress in urban and rural communities in China; we\nthen compared them with large-scale polls from Gallup. After controlling for\nsocioeconomic and gender differences, we found that rural communities tend to\nexpress stress in emotional and personal themes such as relationships, health,\nand opportunity while users in urban areas express stress using relative,\ntemporal, and external themes such as work, politics, and economics. These\ndifferences exist beyond controlling for GDP and urbanization, indicating a\nfundamentally different lifestyle between rural and urban residents in very\nspecific environments, arguably having different sources of stress. We found\ncorroborative trends in physical, financial, and social wellness with\nurbanization in Gallup polls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jesse Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_D/0/1/0/all/0/1\">Dandan Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaidka_K/0/1/0/all/0/1\">Kokil Jaidka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherman_G/0/1/0/all/0/1\">Garrick Sherman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakhetiya_V/0/1/0/all/0/1\">Vinit Jakhetiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guntuku_S/0/1/0/all/0/1\">Sharath Chandra Guntuku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calling to CNN-LSTM for Rumor Detection: A Deep Multi-channel Model for Message Veracity Classification in Microblogs. (arXiv:2110.15727v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15727","description":"<p>Reputed by their low-cost, easy-access, real-time and valuable information,\nsocial media also wildly spread unverified or fake news. Rumors can notably\ncause severe damage on individuals and the society. Therefore, rumor detection\non social media has recently attracted tremendous attention. Most rumor\ndetection approaches focus on rumor feature analysis and social features, i.e.,\nmetadata in social media. Unfortunately, these features are data-specific and\nmay not always be available, e.g., when the rumor has just popped up and not\nyet propagated. In contrast, post contents (including images or videos) play an\nimportant role and can indicate the diffusion purpose of a rumor. Furthermore,\nrumor classification is also closely related to opinion mining and sentiment\nanalysis. Yet, to the best of our knowledge, exploiting images and sentiments\nis little investigated.Considering the available multimodal features from\nmicroblogs, notably, we propose in this paper an end-to-end model called\ndeepMONITOR that is based on deep neural networks and allows quite accurate\nautomated rumor verification, by utilizing all three characteristics: post\ntextual and image contents, as well as sentiment. deepMONITOR concatenates\nimage features with the joint text and sentiment features to produce a\nreliable, fused classification. We conduct extensive experiments on two\nlarge-scale, real-world datasets. The results show that deepMONITOR achieves a\nhigher accuracy than state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azri_A/0/1/0/all/0/1\">Abderrazek Azri</a> (ERIC), <a href=\"http://arxiv.org/find/cs/1/au:+Favre_C/0/1/0/all/0/1\">C&#xe9;cile Favre</a> (ERIC), <a href=\"http://arxiv.org/find/cs/1/au:+Harbi_N/0/1/0/all/0/1\">Nouria Harbi</a> (ERIC), <a href=\"http://arxiv.org/find/cs/1/au:+Darmont_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Darmont</a> (ERIC), <a href=\"http://arxiv.org/find/cs/1/au:+Nous_C/0/1/0/all/0/1\">Camille No&#xfb;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Bias Detection: From Inception to Deployment. (arXiv:2110.15728v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15728","description":"<p>To create a more inclusive workplace, enterprises are actively investing in\nidentifying and eliminating unconscious bias (e.g., gender, race, age,\ndisability, elitism and religion) across their various functions. We propose a\ndeep learning model with a transfer learning based language model to learn from\nmanually tagged documents for automatically identifying bias in enterprise\ncontent. We first pretrain a deep learning-based language-model using\nWikipedia, then fine tune the model with a large unlabelled data set related\nwith various types of enterprise content. Finally, a linear layer followed by\nsoftmax layer is added at the end of the language model and the model is\ntrained on a labelled bias dataset consisting of enterprise content. The\ntrained model is thoroughly evaluated on independent datasets to ensure a\ngeneral application. We present the proposed method and its deployment detail\nin a real-world application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bashar_M/0/1/0/all/0/1\">Md Abul Bashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_R/0/1/0/all/0/1\">Richi Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothare_A/0/1/0/all/0/1\">Anjor Kothare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1\">Vishal Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandadai_K/0/1/0/all/0/1\">Kesavan Kandadai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decision Attentive Regularization to Improve Simultaneous Speech Translation Systems. (arXiv:2110.15729v1 [cs.SD])","link":"http://arxiv.org/abs/2110.15729","description":"<p>Simultaneous Speech-to-text Translation (SimulST) systems translate source\nspeech in tandem with the speaker using partial input. Recent works have tried\nto leverage the text translation task to improve the performance of Speech\nTranslation (ST) in the offline domain. Motivated by these improvements, we\npropose to add Decision Attentive Regularization (DAR) to Monotonic Multihead\nAttention (MMA) based SimulST systems. DAR improves the read/write decisions\nfor speech using the Simultaneous text Translation (SimulMT) task. We also\nextend several techniques from the offline domain to the SimulST task. Our\nproposed system achieves significant performance improvements for the MuST-C\nEnglish-German (EnDe) SimulST task, where we provide an average BLUE score\nimprovement of around 4.57 points or 34.17% across different latencies.\nFurther, the latency-quality tradeoffs establish that the proposed model\nachieves better results compared to the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_M/0/1/0/all/0/1\">Mohd Abbas Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Beomseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakumarapu_N/0/1/0/all/0/1\">Nikhil Kumar Lakumarapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chanwoo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E-Commerce Dispute Resolution Prediction. (arXiv:2110.15730v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15730","description":"<p>E-Commerce marketplaces support millions of daily transactions, and some\ndisagreements between buyers and sellers are unavoidable. Resolving disputes in\nan accurate, fast, and fair manner is of great importance for maintaining a\ntrustworthy platform. Simple cases can be automated, but intricate cases are\nnot sufficiently addressed by hard-coded rules, and therefore most disputes are\ncurrently resolved by people. In this work we take a first step towards\nautomatically assisting human agents in dispute resolution at scale. We\nconstruct a large dataset of disputes from the eBay online marketplace, and\nidentify several interesting behavioral and linguistic patterns. We then train\nclassifiers to predict dispute outcomes with high accuracy. We explore the\nmodel and the dataset, reporting interesting correlations, important features,\nand insights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsurel_D/0/1/0/all/0/1\">David Tsurel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doron_M/0/1/0/all/0/1\">Michael Doron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nus_A/0/1/0/all/0/1\">Alexander Nus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_A/0/1/0/all/0/1\">Arnon Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_I/0/1/0/all/0/1\">Ido Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1\">Dafna Shahaf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CORAA: a large corpus of spontaneous and prepared speech manually validated for speech recognition in Brazilian Portuguese. (arXiv:2110.15731v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15731","description":"<p>Automatic Speech recognition (ASR) is a complex and challenging task. In\nrecent years, there have been significant advances in the area. In particular,\nfor the Brazilian Portuguese (BP) language, there were about 376 hours public\navailable for ASR task until the second half of 2020. With the release of new\ndatasets in early 2021, this number increased to 574 hours. The existing\nresources, however, are composed of audios containing only read and prepared\nspeech. There is a lack of datasets including spontaneous speech, which are\nessential in different ASR applications. This paper presents CORAA (Corpus of\nAnnotated Audios) v1. with 291 hours, a publicly available dataset for ASR in\nBP containing validated pairs (audio-transcription). CORAA also contains\nEuropean Portuguese audios (4.69 hours). We also present two public ASR models\nbased on Wav2Vec 2.0 XLSR-53 and fine-tuned over CORAA. Our best model achieved\na Word Error Rate of 27.35% on CORAA test set and 16.01% on Common Voice test\nset. When measuring the Character Error Rate, we obtained 14.26% and 5.45% for\nCORAA and Common Voice, respectively. CORAA corpora were assembled to both\nimprove ASR models in BP with phenomena from spontaneous speech and motivate\nyoung researchers to start their studies on ASR for Portuguese. All the corpora\nare publicly available at https://github.com/nilc-nlp/CORAA under the CC\nBY-NC-ND 4.0 license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1\">Anderson Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1\">Frederico Santos de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1\">Lucas Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_R/0/1/0/all/0/1\">Ricardo Corso Fernandes Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_D/0/1/0/all/0/1\">Daniel Peixoto Pinto da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fayet_F/0/1/0/all/0/1\">Fernando Gorgulho Fayet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlotto_B/0/1/0/all/0/1\">Bruno Baldissera Carlotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gris_L/0/1/0/all/0/1\">Lucas Rafael Stefanel Gris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aluisio_S/0/1/0/all/0/1\">Sandra Maria Alu&#xed;sio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity Recognition in Unstructured Medical Text Documents. (arXiv:2110.15732v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15732","description":"<p>Physicians provide expert opinion to legal courts on the medical state of\npatients, including determining if a patient is likely to have permanent or\nnon-permanent injuries or ailments. An independent medical examination (IME)\nreport summarizes a physicians medical opinion about a patients health status\nbased on the physicians expertise. IME reports contain private and sensitive\ninformation (Personally Identifiable Information or PII) that needs to be\nremoved or randomly encoded before further research work can be conducted. In\nour study the IME is an orthopedic surgeon from a private practice in the\nUnited States. The goal of this research is to perform named entity recognition\n(NER) to identify and subsequently remove/encode PII information from IME\nreports prepared by the physician. We apply the NER toolkits of OpenNLP and\nspaCy, two freely available natural language processing platforms, and compare\ntheir precision, recall, and f-measure performance at identifying five\ncategories of PII across trials of randomly selected IME reports using each\nmodels common default parameters. We find that both platforms achieve high\nperformance (f-measure &gt; 0.9) at de-identification and that a spaCy model\ntrained with a 70-30 train-test data split is most performant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pearson_C/0/1/0/all/0/1\">Cole Pearson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seliya_N/0/1/0/all/0/1\">Naeem Seliya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_R/0/1/0/all/0/1\">Rushit Dave</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Gender Bias in Transformer-based Models: A Case Study on BERT. (arXiv:2110.15733v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15733","description":"<p>In this paper, we propose a novel gender bias detection method by utilizing\nattention map for transformer-based models. We 1) give an intuitive gender bias\njudgement method by comparing the different relation degree between the genders\nand the occupation according to the attention scores, 2) design a gender bias\ndetector by modifying the attention module, 3) insert the gender bias detector\ninto different positions of the model to present the internal gender bias flow,\nand 4) draw the consistent gender bias conclusion by scanning the entire\nWikipedia, a BERT pretraining dataset. We observe that 1) the attention\nmatrices, Wq and Wk introduce much more gender bias than other modules\n(including the embedding layer) and 2) the bias degree changes periodically\ninside of the model (attention matrix Q, K, V, and the remaining part of the\nattention layer (including the fully-connected layer, the residual connection,\nand the layer normalization module) enhance the gender bias while the averaged\nattentions reduces the bias).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingbing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hongwu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainju_R/0/1/0/all/0/1\">Rajat Sainju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junhuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yueying Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weiwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Leverage Multimodal EHR Data for Better Medical Predictions?. (arXiv:2110.15763v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15763","description":"<p>Healthcare is becoming a more and more important research topic recently.\nWith the growing data in the healthcare domain, it offers a great opportunity\nfor deep learning to improve the quality of medical service. However, the\ncomplexity of electronic health records (EHR) data is a challenge for the\napplication of deep learning. Specifically, the data produced in the hospital\nadmissions are monitored by the EHR system, which includes structured data like\ndaily body temperature, and unstructured data like free text and laboratory\nmeasurements. Although there are some preprocessing frameworks proposed for\nspecific EHR data, the clinical notes that contain significant clinical value\nare beyond the realm of their consideration. Besides, whether these different\ndata from various views are all beneficial to the medical tasks and how to best\nutilize these data remain unclear. Therefore, in this paper, we first extract\nthe accompanying clinical notes from EHR and propose a method to integrate\nthese data, we also comprehensively study the different models and the data\nleverage methods for better medical task prediction. The results on two medical\nprediction tasks show that our fused model with different data outperforms the\nstate-of-the-art method that without clinical notes, which illustrates the\nimportance of our fusion method and the value of clinical note features. Our\ncode is available at https: //github.com/emnlp-mimic/mimic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NxMTransformer: Semi-Structured Sparsification for Natural Language Understanding via ADMM. (arXiv:2110.15766v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15766","description":"<p>Natural Language Processing (NLP) has recently achieved success by using huge\npre-trained Transformer networks. However, these models often contain hundreds\nof millions or even billions of parameters, bringing challenges to online\ndeployment due to latency constraints. Recently, hardware manufacturers have\nintroduced dedicated hardware for NxM sparsity to provide the flexibility of\nunstructured pruning with the runtime efficiency of structured approaches. NxM\nsparsity permits arbitrarily selecting M parameters to retain from a contiguous\ngroup of N in the dense representation. However, due to the extremely high\ncomplexity of pre-trained models, the standard sparse fine-tuning techniques\noften fail to generalize well on downstream tasks, which have limited data\nresources. To address such an issue in a principled manner, we introduce a new\nlearning framework, called NxMTransformer, to induce NxM semi-structured\nsparsity on pretrained language models for natural language understanding to\nobtain better performance. In particular, we propose to formulate the NxM\nsparsity as a constrained optimization problem and use Alternating Direction\nMethod of Multipliers (ADMM) to optimize the downstream tasks while taking the\nunderlying hardware constraints into consideration. ADMM decomposes the NxM\nsparsification problem into two sub-problems that can be solved sequentially,\ngenerating sparsified Transformer networks that achieve high accuracy while\nbeing able to effectively execute on newly released hardware. We apply our\napproach to a wide range of NLP tasks, and our proposed method is able to\nachieve 1.7 points higher accuracy in GLUE score than current practices.\nMoreover, we perform detailed analysis on our approach and shed light on how\nADMM affects fine-tuning accuracy for downstream tasks. Finally, we illustrate\nhow NxMTransformer achieves performance improvement with knowledge\ndistillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holmes_C/0/1/0/all/0/1\">Connor Holmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Machine Learning-Centered Approaches for Forecasting Language Patterns During Frustration in Early Childhood. (arXiv:2110.15778v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15778","description":"<p>When faced with self-regulation challenges, children have been known the use\ntheir language to inhibit their emotions and behaviors. Yet, to date, there has\nbeen a critical lack of evidence regarding what patterns in their speech\nchildren use during these moments of frustration. In this paper, eXtreme\nGradient Boosting, Random Forest, Long Short-Term Memory Recurrent Neural\nNetworks, and Elastic Net Regression, have all been used to forecast these\nlanguage patterns in children. Based on the results of a comparative analysis\nbetween these methods, the study reveals that when dealing with\nhigh-dimensional and dense data, with very irregular and abnormal\ndistributions, as is the case with self-regulation patterns in children,\ndecision tree-based algorithms are able to outperform traditional regression\nand neural network methods in their shortcomings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhakta_A/0/1/0/all/0/1\">Arnav Bhakta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yeunjoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_P/0/1/0/all/0/1\">Pamela Cole</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Feasibility of Predicting Questions being Forgotten in Stack Overflow. (arXiv:2110.15789v1 [cs.IR])","link":"http://arxiv.org/abs/2110.15789","description":"<p>For their attractiveness, comprehensiveness and dynamic coverage of relevant\ntopics, community-based question answering sites such as Stack Overflow heavily\nrely on the engagement of their communities: Questions on new technologies,\ntechnology features as well as technology versions come up and have to be\nanswered as technology evolves (and as community members gather experience with\nit). At the same time, other questions cease in importance over time, finally\nbecoming irrelevant to users. Beyond filtering low-quality questions,\n\"forgetting\" questions, which have become redundant, is an important step for\nkeeping the Stack Overflow content concise and useful. In this work, we study\nthis managed forgetting task for Stack Overflow. Our work is based on data from\nmore than a decade (2008 - 2019) - covering 18.1M questions, that are made\npublicly available by the site itself. For establishing a deeper understanding,\nwe first analyze and characterize the set of questions about to be forgotten,\ni.e., questions that get a considerable number of views in the current period\nbut become unattractive in the near future. Subsequently, we examine the\ncapability of a wide range of features in predicting such forgotten questions\nin different categories. We find some categories in which those questions are\nmore predictable. We also discover that the text-based features are\nsurprisingly not helpful in this prediction task, while the meta information is\nmuch more predictive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thi Huyen Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Tuan-Anh Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niederee_C/0/1/0/all/0/1\">Claudia Nieder&#xe9;e</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLAUSEREC: A Clause Recommendation Framework for AI-aided Contract Authoring. (arXiv:2110.15794v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15794","description":"<p>Contracts are a common type of legal document that frequent in several\nday-to-day business workflows. However, there has been very limited NLP\nresearch in processing such documents, and even lesser in generating them.\nThese contracts are made up of clauses, and the unique nature of these clauses\ncalls for specific methods to understand and generate such documents. In this\npaper, we introduce the task of clause recommendation, asa first step to aid\nand accelerate the author-ing of contract documents. We propose a two-staged\npipeline to first predict if a specific clause type is relevant to be added in\na contract, and then recommend the top clauses for the given type based on the\ncontract context. We pretrain BERT on an existing library of clauses with two\nadditional tasks and use it for our prediction and recommendation. We\nexperiment with classification methods and similarity-based heuristics for\nclause relevance prediction, and generation-based methods for clause\nrecommendation, and evaluate the results from various methods on several clause\ntypes. We provide analyses on the results, and further outline the advantages\nand limitations of the various methods for this line of research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1\">Vinay Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_A/0/1/0/all/0/1\">Aparna Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_B/0/1/0/all/0/1\">Balaji Vasan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_A/0/1/0/all/0/1\">Anandhavelu N</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rajiv Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Non-monotonic Autoregressive Orderings with Variational Inference. (arXiv:2110.15797v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15797","description":"<p>The predominant approach for language modeling is to process sequences from\nleft to right, but this eliminates a source of information: the order by which\nthe sequence was generated. One strategy to recover this information is to\ndecode both the content and ordering of tokens. Existing approaches supervise\ncontent and ordering by designing problem-specific loss functions and\npre-training with an ordering pre-selected. Other recent works use iterative\nsearch to discover problem-specific orderings for training, but suffer from\nhigh time complexity and cannot be efficiently parallelized. We address these\nlimitations with an unsupervised parallelizable learner that discovers\nhigh-quality generation orders purely from training data -- no domain knowledge\nrequired. The learner contains an encoder network and decoder language model\nthat perform variational inference with autoregressive orders (represented as\npermutation matrices) as latent variables. The corresponding ELBO is not\ndifferentiable, so we develop a practical algorithm for end-to-end optimization\nusing policy gradients. We implement the encoder as a Transformer with\nnon-causal attention that outputs permutations in one forward pass.\nPermutations then serve as target generation orders for training an\ninsertion-based Transformer language model. Empirical results in language\nmodeling tasks demonstrate that our method is context-aware and discovers\norderings that are competitive with or even better than fixed orders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trabucco_B/0/1/0/all/0/1\">Brandon Trabucco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dong Huk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Michael Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guided Policy Search for Parameterized Skills using Adverbs. (arXiv:2110.15799v1 [cs.AI])","link":"http://arxiv.org/abs/2110.15799","description":"<p>We present a method for using adverb phrases to adjust skill parameters via\nlearned adverb-skill groundings. These groundings allow an agent to use adverb\nfeedback provided by a human to directly update a skill policy, in a manner\nsimilar to traditional local policy search methods. We show that our method can\nbe used as a drop-in replacement for these policy search methods when dense\nreward from the environment is not available but human language feedback is. We\ndemonstrate improved sample efficiency over modern policy search methods in two\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spiegel_B/0/1/0/all/0/1\">Benjamin A. Spiegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1\">George Konidaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of the Multi-label Residual Convolutional Neural Network text classifier using Content-Based Routing process. (arXiv:2110.15801v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15801","description":"<p>In this article, we will present an NLP application in text classifying\nprocess using the content-based router. The ultimate goal throughout this\narticle is to predict the event described by a legal ad from the plain text of\nthe ad. This problem is purely a supervised problem that will involve the use\nof NLP techniques and conventional modeling methodologies through the use of\nthe Multi-label Residual Convolutional Neural Network for text classification.\nWe will explain the approach put in place to solve the problem of classified\nads, the difficulties encountered and the experimental results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Achraf_T/0/1/0/all/0/1\">Tounsi Achraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safa_E/0/1/0/all/0/1\">Elkefi Safa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERMo: What can BERT learn from ELMo?. (arXiv:2110.15802v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15802","description":"<p>We propose BERMo, an architectural modification to BERT, which makes\npredictions based on a hierarchy of surface, syntactic and semantic language\nfeatures. We use linear combination scheme proposed in Embeddings from Language\nModels (ELMo) to combine the scaled internal representations from different\nnetwork depths. Our approach has two-fold benefits: (1) improved gradient flow\nfor the downstream task as every layer has a direct connection to the gradients\nof the loss function and (2) increased representative power as the model no\nlonger needs to copy the features learned in the shallower layer which are\nnecessary for the downstream task. Further, our model has a negligible\nparameter overhead as there is a single scalar parameter associated with each\nlayer in the network. Experiments on the probing task from SentEval dataset\nshow that our model performs up to $4.65\\%$ better in accuracy than the\nbaseline with an average improvement of $2.67\\%$ on the semantic tasks. When\nsubject to compression techniques, we find that our model enables stable\npruning for compressing small datasets like SST-2, where the BERT model\ncommonly diverges. We observe that our approach converges $1.67\\times$ and\n$1.15\\times$ faster than the baseline on MNLI and QQP tasks from GLUE dataset.\nMoreover, our results show that our approach can obtain better parameter\nefficiency for penalty based pruning approaches on QQP task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kodge_S/0/1/0/all/0/1\">Sangamesh Kodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing for Smart Healthcare. (arXiv:2110.15803v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15803","description":"<p>Smart healthcare has achieved significant progress in recent years. Emerging\nartificial intelligence (AI) technologies enable various smart applications\nacross various healthcare scenarios. As an essential technology powered by AI,\nnatural language processing (NLP) plays a key role in smart healthcare due to\nits capability of analysing and understanding human language. In this work we\nreview existing studies that concern NLP for smart healthcare from the\nperspectives of technique and application. We focus on feature extraction and\nmodelling for various NLP tasks encountered in smart healthcare from a\ntechnical point of view. In the context of smart healthcare applications\nemploying NLP techniques, the elaboration largely attends to representative\nsmart healthcare scenarios, including clinical practice, hospital management,\npersonal care, public health, and drug development. We further discuss the\nlimitations of current works and identify the directions for future works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Binggui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanghua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shaodan Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Unsupervised and Text Augmented Semi-Supervised Learning for Low Resourced Autoregressive Speech Recognition. (arXiv:2110.15836v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15836","description":"<p>Recent advances in unsupervised representation learning have demonstrated the\nimpact of pretraining on large amounts of read speech. We adapt these\ntechniques for domain adaptation in low-resource -- both in terms of data and\ncompute -- conversational and broadcast domains. Moving beyond CTC, we pretrain\nstate-of-the-art Conformer models in an unsupervised manner. While the\nunsupervised approach outperforms traditional semi-supervised training, the\ntechniques are complementary. Combining the techniques is a 5% absolute\nimprovement in WER, averaged over all conditions, compared to semi-supervised\ntraining alone. Additional text data is incorporated through external language\nmodels. By using CTC-based decoding, we are better able to take advantage of\nthe additional text data. When used as a transcription model, it allows the\nConformer model to better incorporate the knowledge from the language model\nthrough semi-supervised training than shallow fusion. Final performance is an\nadditional 2% better absolute when using CTC-based decoding for semi-supervised\ntraining compared to shallow fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chak-Fai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keith_F/0/1/0/all/0/1\">Francis Keith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_W/0/1/0/all/0/1\">William Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snover_M/0/1/0/all/0/1\">Matthew Snover</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Theories on Styles to their Transfer in Text: Bridging the Gap with a Hierarchical Survey. (arXiv:2110.15871v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15871","description":"<p>Humans are naturally endowed with the ability to write in a particular style.\nThey can, for instance, rephrase a formal letter in an informal way, convey a\nliteral message with the use of figures of speech, edit a novel mimicking the\nstyle of some well-known authors. Automating this form of creativity\nconstitutes the goal of style transfer. As a natural language generation task,\nstyle transfer aims at re-writing existing texts, and specifically, it creates\nparaphrases that exhibit some desired stylistic attributes. From a practical\nperspective, it envisions beneficial applications, like chat-bots that modulate\ntheir communicative style to appear empathetic, or systems that automatically\nsimplify technical articles for a non-expert audience.\n</p>\n<p>Style transfer has been dedicated several style-aware paraphrasing methods. A\nhandful of surveys give a methodological overview of the field, but they do not\nsupport researchers to focus on specific styles. With this paper, we aim at\nproviding a comprehensive discussion of the styles that have received attention\nin the transfer task. We organize them into a hierarchy, highlighting the\nchallenges for the definition of each of them, and pointing out gaps in the\ncurrent research landscape. The hierarchy comprises two main groups. One\nencompasses styles that people modulate arbitrarily, along the lines of\nregisters and genres. The other group corresponds to unintentionally expressed\nstyles, due to an author's personal characteristics. Hence, our review shows\nhow the groups relate to one another, and where specific styles, including some\nthat have never been explored, belong in the hierarchy. Moreover, we summarize\nthe methods employed for different stylistic families, hinting researchers\ntowards those that would be the most fitting for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Troiano_E/0/1/0/all/0/1\">Enrica Troiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velutharambath_A/0/1/0/all/0/1\">Aswathy Velutharambath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_a/0/1/0/all/0/1\">and Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Ensembles for Sexism Detection. (arXiv:2110.15905v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15905","description":"<p>This document presents in detail the work done for the sexism detection task\nat EXIST2021 workshop. Our methodology is built on ensembles of\nTransformer-based models which are trained on different background and corpora\nand fine-tuned on the provided dataset from the EXIST2021 workshop. We report\naccuracy of 0.767 for the binary classification task (task1), and f1 score\n0.766, and for the multi-class task (task2) accuracy 0.623 and f1-score 0.535.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davies_L/0/1/0/all/0/1\">Lily Davies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldracchi_M/0/1/0/all/0/1\">Marta Baldracchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borella_C/0/1/0/all/0/1\">Carlo Alessandro Borella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perifanos_K/0/1/0/all/0/1\">Konstantinos Perifanos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Full Constituency Parsing with Neighboring Distribution Divergence. (arXiv:2110.15931v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15931","description":"<p>Unsupervised constituency parsing has been explored much but is still far\nfrom being solved. Conventional unsupervised constituency parser is only able\nto capture the unlabeled structure of sentences. Towards unsupervised full\nconstituency parsing, we propose an unsupervised and training-free labeling\nprocedure by exploiting the property of a recently introduced metric,\nNeighboring Distribution Divergence (NDD), which evaluates semantic similarity\nbetween sentences before and after editions. For implementation, we develop NDD\ninto Dual POS-NDD (DP-NDD) and build \"molds\" to detect constituents and their\nlabels in sentences. We show that DP-NDD not only labels constituents precisely\nbut also inducts more accurate unlabeled constituency trees than all previous\nunsupervised methods with simpler rules. With two frameworks for labeled\nconstituency trees inference, we set both the new state-of-the-art for\nunlabeled F1 and strong baselines for labeled F1. In contrast with the\nconventional predicting-and-evaluating scenario, our method acts as an\nplausible example to inversely apply evaluating metrics for prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaICL: Learning to Learn In Context. (arXiv:2110.15943v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15943","description":"<p>We introduce MetaICL (Meta-training for In-Context Learning), a new\nmeta-training framework for few-shot learning where a pretrained language model\nis tuned to do in-context learn-ing on a large set of training tasks. This\nmeta-training enables the model to more effectively learn a new task in context\nat test time, by simply conditioning on a few training examples with no\nparameter updates or task-specific templates. We experiment on a large, diverse\ncollection of tasks consisting of 142 NLP datasets including classification,\nquestion answering, natural language inference, paraphrase detection and more,\nacross seven different meta-training/target splits. MetaICL outperforms a range\nof baselines including in-context learning without meta-training and multi-task\nlearning followed by zero-shot transfer. We find that the gains are\nparticularly significant for target tasks that have domain shifts from the\nmeta-training tasks, and that using a diverse set of the meta-training tasks is\nkey to improvements. We also show that MetaICL approaches (and sometimes beats)\nthe performance of models fully finetuned on the target task training data, and\noutperforms much bigger models with nearly 8x parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Keyword Spotting with Attention. (arXiv:2110.15957v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15957","description":"<p>In this paper, we consider the task of spotting spoken keywords in silent\nvideo sequences -- also known as visual keyword spotting. To this end, we\ninvestigate Transformer-based models that ingest two streams, a visual encoding\nof the video and a phonetic encoding of the keyword, and output the temporal\nlocation of the keyword if present. Our contributions are as follows: (1) We\npropose a novel architecture, the Transpotter, that uses full cross-modal\nattention between the visual and phonetic streams; (2) We show through\nextensive evaluations that our model outperforms the prior state-of-the-art\nvisual keyword spotting and lip reading methods on the challenging LRW, LRS2,\nLRS3 datasets by a large margin; (3) We demonstrate the ability of our model to\nspot words under the extreme conditions of isolated mouthings in sign language\nvideos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prajwal_K/0/1/0/all/0/1\">K R Prajwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momeni_L/0/1/0/all/0/1\">Liliane Momeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1\">Triantafyllos Afouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Daily Dosage from Medication Instructions in EHRs: An Automated Approach and Lessons Learned. (arXiv:2005.10899v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.10899","description":"<p>Medication timelines have been shown to be effective in helping physicians\nvisualize complex patient medication information. A key feature in many such\ndesigns is a longitudinal representation of a medication's daily dosage and its\nchanges over time. However, daily dosage as a discrete value is generally not\nprovided and needs to be derived from free text instructions (Sig). Existing\nworks in daily dosage extraction are narrow in scope, targeting dosage\nextraction for a single drug from clinical notes. Here, we present an automated\napproach to calculate daily dosage for all medications, combining deep\nlearning-based named entity extractor with lexicon dictionaries and regular\nexpressions, achieving 0.98 precision and 0.95 recall on an expert-generated\ndataset of 1,000 Sigs. We also analyze our expert-generated dataset, discuss\nthe challenges in understanding the complex information contained in Sigs, and\nprovide insights to guide future work in the general-purpose daily dosage\ncalculation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Diwakar Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jennifer J. Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsou_C/0/1/0/all/0/1\">Ching-Huei Tsou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FAME: Feature-Based Adversarial Meta-Embeddings for Robust Input Representations. (arXiv:2010.12305v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12305","description":"<p>Combining several embeddings typically improves performance in downstream\ntasks as different embeddings encode different information. It has been shown\nthat even models using embeddings from transformers still benefit from the\ninclusion of standard word embeddings. However, the combination of embeddings\nof different types and dimensions is challenging. As an alternative to\nattention-based meta-embeddings, we propose feature-based adversarial\nmeta-embeddings (FAME) with an attention function that is guided by features\nreflecting word-specific properties, such as shape and frequency, and show that\nthis is beneficial to handle subword-based embeddings. In addition, FAME uses\nadversarial training to optimize the mappings of differently-sized embeddings\nto the same space. We demonstrate that FAME works effectively across languages\nand domains for sequence labeling and sentence classification, in particular in\nlow-resource settings. FAME sets the new state of the art for POS tagging in 27\nlanguages, various NER settings and question classification in different\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lange_L/0/1/0/all/0/1\">Lukas Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strotgen_J/0/1/0/all/0/1\">Jannik Str&#xf6;tgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answering Open-Domain Questions of Varying Reasoning Steps from Text. (arXiv:2010.12527v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12527","description":"<p>We develop a unified system to answer directly from text open-domain\nquestions that may require a varying number of retrieval steps. We employ a\nsingle multi-task transformer model to perform all the necessary subtasks --\nretrieving supporting facts, reranking them, and predicting the answer from all\nretrieved documents -- in an iterative fashion. We avoid crucial assumptions of\nprevious work that do not transfer well to real-world settings, including\nexploiting knowledge of the fixed number of retrieval steps required to answer\neach question or using structured metadata like knowledge bases or web links\nthat have limited availability. Instead, we design a system that can answer\nopen-domain questions on any text collection without prior knowledge of\nreasoning complexity. To emulate this setting, we construct a new benchmark,\ncalled BeerQA, by combining existing one- and two-step datasets with a new\ncollection of 530 questions that require three Wikipedia pages to answer,\nunifying Wikipedia corpora versions in the process. We show that our model\ndemonstrates competitive performance on both existing benchmarks and this new\nbenchmark. We make the new benchmark available at https://beerqa.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1\">Peng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Haejun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sido_O/0/1/0/all/0/1\">Oghenetegiri &quot;TG&quot; Sido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-to-text Generation by Splicing Together Nearest Neighbors. (arXiv:2101.08248v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.08248","description":"<p>We propose to tackle data-to-text generation tasks by directly splicing\ntogether retrieved segments of text from \"neighbor\" source-target pairs. Unlike\nrecent work that conditions on retrieved neighbors but generates text\ntoken-by-token, left-to-right, we learn a policy that directly manipulates\nsegments of neighbor text, by inserting or replacing them in partially\nconstructed generations. Standard techniques for training such a policy require\nan oracle derivation for each generation, and we prove that finding the\nshortest such derivation can be reduced to parsing under a particular weighted\ncontext-free grammar. We find that policies learned in this way perform on par\nwith strong baselines in terms of automatic and human evaluation, but allow for\nmore interpretable and controllable generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1\">Sam Wiseman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1\">Arturs Backurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stratos_K/0/1/0/all/0/1\">Karl Stratos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Share or not to Share: Predicting Sets of Sources for Model Transfer Learning. (arXiv:2104.08078v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08078","description":"<p>In low-resource settings, model transfer can help to overcome a lack of\nlabeled data for many tasks and domains. However, predicting useful transfer\nsources is a challenging problem, as even the most similar sources might lead\nto unexpected negative transfer results. Thus, ranking methods based on task\nand text similarity -- as suggested in prior work -- may not be sufficient to\nidentify promising sources. To tackle this problem, we propose a new approach\nto automatically determine which and how many sources should be exploited. For\nthis, we study the effects of model transfer on sequence labeling across\nvarious domains and tasks and show that our methods based on model similarity\nand support vector machines are able to predict promising sources, resulting in\nperformance increases of up to 24 F1 points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lange_L/0/1/0/all/0/1\">Lukas Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strotgen_J/0/1/0/all/0/1\">Jannik Str&#xf6;tgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09151","description":"<p>Describing images using natural language is widely known as image captioning,\nwhich has made consistent progress due to the development of computer vision\nand natural language generation techniques. Though conventional captioning\nmodels achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and\nSPICE, the ability of captions to distinguish the target image from other\nsimilar images is under-explored. To generate distinctive captions, a few\npioneers employ contrastive learning or re-weighted the ground-truth captions,\nwhich focuses on one single input image. However, the relationships between\nobjects in a similar image group (e.g., items or properties within the same\nalbum or fine-grained events) are neglected. In this paper, we improve the\ndistinctiveness of image captions using a Group-based Distinctive Captioning\nModel (GdisCap), which compares each image with other images in one similar\ngroup and highlights the uniqueness of each image. In particular, we propose a\ngroup-based memory attention (GMA) module, which stores object features that\nare unique among the image group (i.e., with low similarity to objects in other\nimages). These unique object features are highlighted when generating captions,\nresulting in more distinctive captions. Furthermore, the distinctive words in\nthe ground-truth captions are selected to supervise the language decoder and\nGMA. Finally, we propose a new evaluation metric, distinctive word rate\n(DisWordRate) to measure the distinctiveness of captions. Quantitative results\nindicate that the proposed method significantly improves the distinctiveness of\nseveral baseline models, and achieves the state-of-the-art performance on both\naccuracy and distinctiveness. Results of a user study agree with the\nquantitative evaluation and demonstrate the rationality of the new metric\nDisWordRate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiuniu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Learning with Sentiment, Emotion, and Target Detection to Recognize Hate Speech and Offensive Language. (arXiv:2109.10255v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10255","description":"<p>The recognition of hate speech and offensive language (HOF) is commonly\nformulated as a classification task to decide if a text contains HOF. We\ninvestigate whether HOF detection can profit by taking into account the\nrelationships between HOF and similar concepts: (a) HOF is related to sentiment\nanalysis because hate speech is typically a negative statement and expresses a\nnegative opinion; (b) it is related to emotion analysis, as expressed hate\npoints to the author experiencing (or pretending to experience) anger while the\naddressees experience (or are intended to experience) fear. (c) Finally, one\nconstituting element of HOF is the mention of a targeted person or group. On\nthis basis, we hypothesize that HOF detection shows improvements when being\nmodeled jointly with these concepts, in a multi-task learning setup. We base\nour experiments on existing data sets for each of these concepts (sentiment,\nemotion, target of HOF) and evaluate our models as a participant (as team\nIMS-SINAI) in the HASOC FIRE 2021 English Subtask 1A. Based on model-selection\nexperiments in which we consider multiple available resources and submissions\nto the shared task, we find that the combination of the CrowdFlower emotion\ncorpus, the SemEval 2016 Sentiment Corpus, and the OffensEval 2019 target\ndetection data leads to an F1 =.79 in a multi-head multi-task learning model\nbased on BERT, in comparison to .7895 of plain BERT. On the HASOC 2019 test\ndata, this result is more substantial with an increase by 2pp in F1 and a\nconsiderable increase in recall. Across both data sets (2019, 2021), the recall\nis particularly increased for the class of HOF (6pp for the 2019 data and 3pp\nfor the 2021 data), showing that MTL with emotion, sentiment, and target\nidentification is an appropriate approach for early warning systems that might\nbe deployed in social media platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plaza_del_Arco_F/0/1/0/all/0/1\">Flor Miriam Plaza-del-Arco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halat_S/0/1/0/all/0/1\">Sercan Halat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Retriever-Ranker for dense text retrieval. (arXiv:2110.03611v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03611","description":"<p>Current dense text retrieval models face two typical challenges. First, it\nadopts a siamese dual-encoder architecture to encode query and document\nindependently for fast indexing and searching, whereas neglecting the\nfiner-grained term-wise interactions. This results in a sub-optimal recall\nperformance. Second, it highly relies on a negative sampling technique to build\nup the negative documents in its contrastive loss. To address these challenges,\nwe present Adversarial Retriever-Ranker (AR2), which consists of a dual-encoder\nretriever plus a cross-encoder ranker. The two models are jointly optimized\naccording to a minimax adversarial objective: the retriever learns to retrieve\nnegative documents to cheat the ranker, while the ranker learns to rank a\ncollection of candidates including both the ground-truth and the retrieved\nones, as well as providing progressive direct feedback to the dual-encoder\nretriever. Through this adversarial game, the retriever gradually produces\nharder negative documents to train a better ranker, whereas the cross-encoder\nranker provides progressive feedback to improve retriever. We evaluate AR2 on\nthree benchmarks. Experimental results show that AR2 consistently and\nsignificantly outperforms existing dense retriever methods and achieves new\nstate-of-the-art results on all of them. This includes the improvements on\nNatural Questions R@5 to 77.9%(+2.1%), TriviaQA R@5 to 78.2%(+1.4), and\nMS-MARCO MRR@10 to 39.5%(+1.3%). We will make our code, models, and data\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing. (arXiv:2110.13900v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.13900","description":"<p>Self-supervised learning (SSL) achieves great success in speech recognition,\nwhile limited exploration has been attempted for other speech processing tasks.\nAs speech signal contains multi-faceted information including speaker identity,\nparalinguistics, spoken content, etc., learning universal representations for\nall speech tasks is challenging. In this paper, we propose a new pre-trained\nmodel, WavLM, to solve full-stack downstream speech tasks. WavLM is built based\non the HuBERT framework, with an emphasis on both spoken content modeling and\nspeaker identity preservation. We first equip the Transformer structure with\ngated relative position bias to improve its capability on recognition tasks.\nFor better speaker discrimination, we propose an utterance mixing training\nstrategy, where additional overlapped utterances are created unsupervisely and\nincorporated during model training. Lastly, we scale up the training dataset\nfrom 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance\non the SUPERB benchmark, and brings significant improvements for various speech\nprocessing tasks on their representative benchmarks. The code and pretrained\nmodels are available at https://aka.ms/wavlm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yanmin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cognitive network science quantifies feelings expressed in suicide letters and Reddit mental health communities. (arXiv:2110.15269v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15269","description":"<p>Writing messages is key to expressing feelings. This study adopts cognitive\nnetwork science to reconstruct how individuals report their feelings in\nclinical narratives like suicide notes or mental health posts. We achieve this\nby reconstructing syntactic/semantic associations between conceptsin texts as\nco-occurrences enriched with affective data. We transform 142 suicide notes and\n77,000 Reddit posts from the r/anxiety, r/depression, r/schizophrenia, and\nr/do-it-your-own (r/DIY) forums into 5 cognitive networks, each one expressing\nmeanings and emotions as reported by authors. These networks reconstruct the\nsemantic frames surrounding 'feel', enabling a quantification of prominent\nassociations and emotions focused around feelings. We find strong feelings of\nsadness across all clinical Reddit boards, added to fear r/depression, and\nreplaced by joy/anticipation in r/DIY. Semantic communities and topic modelling\nboth highlight key narrative topics of 'regret', 'unhealthy lifestyle' and 'low\nmental well-being'. Importantly, negative associations and emotions co-existed\nwith trustful/positive language, focused on 'getting better'. This emotional\npolarisation provides quantitative evidence that online clinical boards possess\na complex structure, where users mix both positive and negative outlooks. This\ndichotomy is absent in the r/DIY reference board and in suicide notes, where\nnegative emotional associations about regret and pain persist but are\noverwhelmed by positive jargon addressing loved ones. Our quantitative\ncomparisons provide strong evidence that suicide notes encapsulate different\nways of expressing feelings compared to online Reddit boards, the latter acting\nmore like personal diaries and relief valve. Our findings provide an\ninterpretable, quantitative aid for supporting psychological inquiries of human\nfeelings in digital and clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_S/0/1/0/all/0/1\">Simmi Marina Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Citraro_S/0/1/0/all/0/1\">Salvatore Citraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morini_V/0/1/0/all/0/1\">Virginia Morini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossetti_G/0/1/0/all/0/1\">Giulio Rossetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stella_M/0/1/0/all/0/1\">Massimo Stella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Extraction of Causal Relations from Natural Language Text. (arXiv:2101.06426v1 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/2101.06426","description":"<p>As an essential component of human cognition, cause-effect relations appear\nfrequently in text, and curating cause-effect relations from text helps in\nbuilding causal networks for predictive tasks. Existing causality extraction\ntechniques include knowledge-based, statistical machine learning(ML)-based, and\ndeep learning-based approaches. Each method has its advantages and weaknesses.\nFor example, knowledge-based methods are understandable but require extensive\nmanual domain knowledge and have poor cross-domain applicability. Statistical\nmachine learning methods are more automated because of natural language\nprocessing (NLP) toolkits. However, feature engineering is labor-intensive, and\ntoolkits may lead to error propagation. In the past few years, deep learning\ntechniques attract substantial attention from NLP researchers because of its'\npowerful representation learning ability and the rapid increase in\ncomputational resources. Their limitations include high computational costs and\na lack of adequate annotated training data. In this paper, we conduct a\ncomprehensive survey of causality extraction. We initially introduce primary\nforms existing in the causality extraction: explicit intra-sentential\ncausality, implicit causality, and inter-sentential causality. Next, we list\nbenchmark datasets and modeling assessment methods for causal relation\nextraction. Then, we present a structured overview of the three techniques with\ntheir representative systems. Lastly, we highlight existing open challenges\nwith their potential directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Neural Disparity Refinement for Arbitrary Resolution Stereo. (arXiv:2110.15367v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15367","description":"<p>We introduce a novel architecture for neural disparity refinement aimed at\nfacilitating deployment of 3D computer vision on cheap and widespread consumer\ndevices, such as mobile phones. Our approach relies on a continuous formulation\nthat enables to estimate a refined disparity map at any arbitrary output\nresolution. Thereby, it can handle effectively the unbalanced camera setup\ntypical of nowadays mobile phones, which feature both high and low resolution\nRGB sensors within the same device. Moreover, our neural network can process\nseamlessly the output of a variety of stereo methods and, by refining the\ndisparity maps computed by a traditional matching algorithm like SGM, it can\nachieve unpaired zero-shot generalization performance compared to\nstate-of-the-art end-to-end stereo models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aleotti_F/0/1/0/all/0/1\">Filippo Aleotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tosi_F/0/1/0/all/0/1\">Fabio Tosi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_P/0/1/0/all/0/1\">Pierluigi Zama Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poggi_M/0/1/0/all/0/1\">Matteo Poggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salti_S/0/1/0/all/0/1\">Samuele Salti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattoccia_S/0/1/0/all/0/1\">Stefano Mattoccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefano_L/0/1/0/all/0/1\">Luigi Di Stefano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New SAR target recognition based on YOLO and very deep multi-canonical correlation analysis. (arXiv:2110.15383v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15383","description":"<p>Synthetic Aperture Radar (SAR) images are prone to be contaminated by noise,\nwhich makes it very difficult to perform target recognition in SAR images.\nInspired by great success of very deep convolutional neural networks (CNNs),\nthis paper proposes a robust feature extraction method for SAR image target\nclassification by adaptively fusing effective features from different CNN\nlayers. First, YOLOv4 network is fine-tuned to detect the targets from the\nrespective MF SAR target images. Second, a very deep CNN is trained from\nscratch on the moving and stationary target acquisition and recognition (MSTAR)\ndatabase by using small filters throughout the whole net to reduce the speckle\nnoise. Besides, using small-size convolution filters decreases the number of\nparameters in each layer and, therefore, reduces computation cost as the CNN\ngoes deeper. The resulting CNN model is capable of extracting very deep\nfeatures from the target images without performing any noise filtering or\npre-processing techniques. Third, our approach proposes to use the\nmulti-canonical correlation analysis (MCCA) to adaptively learn CNN features\nfrom different layers such that the resulting representations are highly\nlinearly correlated and therefore can achieve better classification accuracy\neven if a simple linear support vector machine is used. Experimental results on\nthe MSTAR dataset demonstrate that the proposed method outperforms the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amrani_M/0/1/0/all/0/1\">Moussa Amrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bey_A/0/1/0/all/0/1\">Abdelatif Bey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amamra_A/0/1/0/all/0/1\">Abdenour Amamra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Translation of Rebar Information from GPR Data into As-Built BIM: A Deep Learning-based Approach. (arXiv:2110.15448v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15448","description":"<p>Building Information Modeling (BIM) is increasingly used in the construction\nindustry, but existing studies often ignore embedded rebars. Ground Penetrating\nRadar (GPR) provides a potential solution to develop as-built BIM with surface\nelements and rebars. However, automatically translating rebars from GPR into\nBIM is challenging since GPR cannot provide any information about the scanned\nelement. Thus, we propose an approach to link GPR data and BIM according to\nFaster R-CNN. A label is attached to each element scanned by GPR for capturing\nthe labeled images, which are used with other images to build a 3D model.\nMeanwhile, Faster R-CNN is introduced to identify the labels, and the\nprojection relationship between images and the model is used to localize the\nscanned elements in the 3D model. Two concrete buildings is selected to\nevaluate the proposed approach, and the results reveal that our method could\naccurately translate the rebars from GPR data into corresponding elements in\nBIM with correct distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhongming Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_G/0/1/0/all/0/1\">Ge Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashidi_A/0/1/0/all/0/1\">Abbas Rashidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Foreground Extraction via Deep Region Competition. (arXiv:2110.15497v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15497","description":"<p>We present Deep Region Competition (DRC), an algorithm designed to extract\nforeground objects from images in a fully unsupervised manner. Foreground\nextraction can be viewed as a special case of generic image segmentation that\nfocuses on identifying and disentangling objects from the background. In this\nwork, we rethink the foreground extraction by reconciling energy-based prior\nwith generative image modeling in the form of Mixture of Experts (MoE), where\nwe further introduce the learned pixel re-assignment as the essential inductive\nbias to capture the regularities of background regions. With this modeling, the\nforeground-background partition can be naturally found through\nExpectation-Maximization (EM). We show that the proposed method effectively\nexploits the interaction between the mixture components during the partitioning\nprocess, which closely connects to region competition, a seminal approach for\ngeneric image segmentation. Experiments demonstrate that DRC exhibits more\ncompetitive performances on complex real-world data and challenging\nmulti-object scenes compared with prior methods. Moreover, we show empirically\nthat DRC can potentially generalize to novel foreground objects even from\ncategories unseen during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peiyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sirui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UDIS: Unsupervised Discovery of Bias in Deep Visual Recognition Models. (arXiv:2110.15499v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15499","description":"<p>Deep learning models have been shown to learn spurious correlations from data\nthat sometimes lead to systematic failures for certain subpopulations. Prior\nwork has typically diagnosed this by crowdsourcing annotations for various\nprotected attributes and measuring performance, which is both expensive to\nacquire and difficult to scale. In this work, we propose UDIS, an unsupervised\nalgorithm for surfacing and analyzing such failure modes. UDIS identifies\nsubpopulations via hierarchical clustering of dataset embeddings and surfaces\nsystematic failure modes by visualizing low performing clusters along with\ntheir gradient-weighted class-activation maps. We show the effectiveness of\nUDIS in identifying failure modes in models trained for image classification on\nthe CelebA and MSCOCO datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishnakumar_A/0/1/0/all/0/1\">Arvindkumar Krishnakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1\">Viraj Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudhakar_S/0/1/0/all/0/1\">Sruthi Sudhakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PEDENet: Image Anomaly Localization via Patch Embedding and Density Estimation. (arXiv:2110.15525v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15525","description":"<p>A neural network targeting at unsupervised image anomaly localization, called\nthe PEDENet, is proposed in this work. PEDENet contains a patch embedding (PE)\nnetwork, a density estimation (DE) network, and an auxiliary network called the\nlocation prediction (LP) network. The PE network takes local image patches as\ninput and performs dimension reduction to get low-dimensional patch embeddings\nvia a deep encoder structure. Being inspired by the Gaussian Mixture Model\n(GMM), the DE network takes those patch embeddings and then predicts the\ncluster membership of an embedded patch. The sum of membership probabilities is\nused as a loss term to guide the learning process. The LP network is a\nMulti-layer Perception (MLP), which takes embeddings from two neighboring\npatches as input and predicts their relative location. The performance of the\nproposed PEDENet is evaluated extensively and benchmarked with that of\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaitai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Fusion of Heterogeneous Neural Networks via Cross-Layer Alignment. (arXiv:2110.15538v1 [cs.LG])","link":"http://arxiv.org/abs/2110.15538","description":"<p>Layer-wise model fusion via optimal transport, named OTFusion, applies soft\nneuron association for unifying different pre-trained networks to save\ncomputational resources. While enjoying its success, OTFusion requires the\ninput networks to have the same number of layers. To address this issue, we\npropose a novel model fusion framework, named CLAFusion, to fuse neural\nnetworks with a different number of layers, which we refer to as heterogeneous\nneural networks, via cross-layer alignment. The cross-layer alignment problem,\nwhich is an unbalanced assignment problem, can be solved efficiently using\ndynamic programming. Based on the cross-layer alignment, our framework balances\nthe number of layers of neural networks before applying layer-wise model\nfusion. Our synthetic experiments indicate that the fused network from\nCLAFusion achieves a more favorable performance compared to the individual\nnetworks trained on heterogeneous data without the need for any retraining.\nWith an extra fine-tuning process, it improves the accuracy of residual\nnetworks on the CIFAR10 dataset. Finally, we explore its application for model\ncompression and knowledge distillation when applying to the teacher-student\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_H/0/1/0/all/0/1\">Hung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Cognizance: What Machine Really Learns. (arXiv:2110.15548v1 [cs.LG])","link":"http://arxiv.org/abs/2110.15548","description":"<p>Despite overwhelming achievements in recognition accuracy, extending an\nopen-set capability -- ability to identify when the question is out of scope --\nremains greatly challenging in a scalable machine learning inference. A recent\nresearch has discovered Latent Cognizance (LC) -- an insight on a recognition\nmechanism based on a new probabilistic interpretation, Bayesian theorem, and an\nanalysis of an internal structure of a commonly-used recognition inference\nstructure. The new interpretation emphasizes a latent assumption of an\noverlooked probabilistic condition on a learned inference model. Viability of\nLC has been shown on a task of sign language recognition, but its potential and\nimplication can reach far beyond a specific domain and can move object\nrecognition toward a scalable open-set recognition. However, LC new\nprobabilistic interpretation has not been directly investigated. This article\ninvestigates the new interpretation under a traceable context. Our findings\nsupport the rationale on which LC is based and reveal a hidden mechanism\nunderlying the learning classification inference. The ramification of these\nfindings could lead to a simple yet effective solution to an open-set\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakjai_P/0/1/0/all/0/1\">Pisit Nakjai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponsawat_J/0/1/0/all/0/1\">Jiradej Ponsawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katanyukul_T/0/1/0/all/0/1\">Tatpong Katanyukul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-Powered Semantic Segmentation and Fluid Volume Calculation of Lung CT images in Covid-19 Patients. (arXiv:2110.15558v1 [eess.IV])","link":"http://arxiv.org/abs/2110.15558","description":"<p>COVID-19 pandemic is a deadly disease spreading very fast. People with the\nconfronted immune system are susceptible to many health conditions. A highly\nsignificant condition is pneumonia, which is found to be the cause of death in\nthe majority of patients. The main purpose of this study is to find the volume\nof GGO and consolidation of a covid-19 patient so that the physicians can\nprioritize the patients. Here we used transfer learning techniques for\nsegmentation of lung CTs with the latest libraries and techniques which reduces\ntraining time and increases the accuracy of the AI Model. This system is\ntrained with DeepLabV3+ network architecture and model Resnet50 with Imagenet\nweights. We used different augmentation techniques like Gaussian Noise,\nHorizontal shift, color variation, etc to get to the result. Intersection over\nUnion(IoU) is used as the performance metrics. The IoU of lung masks is\npredicted as 99.78% and that of infected masks is as 89.01%. Our work\neffectively measures the volume of infected region by calculating the volume of\ninfected and lung mask region of the patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+P_S/0/1/0/all/0/1\">Sabeerali K.P</a>, <a href=\"http://arxiv.org/find/eess/1/au:+S_S/0/1/0/all/0/1\">Saleena T.S</a>, <a href=\"http://arxiv.org/find/eess/1/au:+P_D/0/1/0/all/0/1\">Dr.Muhamed Ilyas P</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohan_D/0/1/0/all/0/1\">Dr. Neha Mohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposing Deepfake with Pixel-wise AR and PPG Correlation from Faint Signals. (arXiv:2110.15561v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15561","description":"<p>Deepfake poses a serious threat to the reliability of judicial evidence and\nintellectual property protection. In spite of an urgent need for Deepfake\nidentification, existing pixel-level detection methods are increasingly unable\nto resist the growing realism of fake videos and lack generalization. In this\npaper, we propose a scheme to expose Deepfake through faint signals hidden in\nface videos. This scheme extracts two types of minute information hidden\nbetween face pixels-photoplethysmography (PPG) features and auto-regressive\n(AR) features, which are used as the basis for forensics in the temporal and\nspatial domains, respectively. According to the principle of PPG, tracking the\nabsorption of light by blood cells allows remote estimation of the temporal\ndomains heart rate (HR) of face video, and irregular HR fluctuations can be\nseen as traces of tampering. On the other hand, AR coefficients are able to\nreflect the inter-pixel correlation, and can also reflect the traces of\nsmoothing caused by up-sampling in the process of generating fake faces.\nFurthermore, the scheme combines asymmetric convolution block (ACBlock)-based\nimproved densely connected networks (DenseNets) to achieve face video\nauthenticity forensics. Its asymmetric convolutional structure enhances the\nrobustness of network to the input feature image upside-down and left-right\nflipping, so that the sequence of feature stitching does not affect detection\nresults. Simulation results show that our proposed scheme provides more\naccurate authenticity detection results on multiple deep forgery datasets and\nhas better generalization compared to the benchmark strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Maoyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jun Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised PET Reconstruction from a Bayesian Perspective. (arXiv:2110.15568v1 [eess.IV])","link":"http://arxiv.org/abs/2110.15568","description":"<p>Positron emission tomography (PET) reconstruction has become an ill-posed\ninverse problem due to low-count projection data, and a robust algorithm is\nurgently required to improve imaging quality. Recently, the deep image prior\n(DIP) has drawn much attention and has been successfully applied in several\nimage restoration tasks, such as denoising and inpainting, since it does not\nneed any labels (reference image). However, overfitting is a vital defect of\nthis framework. Hence, many methods have been proposed to mitigate this\nproblem, and DeepRED is a typical representation that combines DIP and\nregularization by denoising (RED). In this article, we leverage DeepRED from a\nBayesian perspective to reconstruct PET images from a single corrupted sinogram\nwithout any supervised or auxiliary information. In contrast to the\nconventional denoisers customarily used in RED, a DnCNN-like denoiser, which\ncan add an adaptive constraint to DIP and facilitate the computation of\nderivation, is employed. Moreover, to further enhance the regularization,\nGaussian noise is injected into the gradient updates, deriving a Markov chain\nMonte Carlo (MCMC) sampler. Experimental studies on brain and whole-body\ndatasets demonstrate that our proposed method can achieve better performance in\nterms of qualitative and quantitative results compared to several classic and\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1\">Chenyu Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_W/0/1/0/all/0/1\">Wenjun Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_H/0/1/0/all/0/1\">Hongwei Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_M/0/1/0/all/0/1\">Mingzheng Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1\">Jiliu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novel View Synthesis from a Single Image via Unsupervised learning. (arXiv:2110.15569v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15569","description":"<p>View synthesis aims to generate novel views from one or more given source\nviews. Although existing methods have achieved promising performance, they\nusually require paired views of different poses to learn a pixel\ntransformation. This paper proposes an unsupervised network to learn such a\npixel transformation from a single source viewpoint. In particular, the network\nconsists of a token transformation module (TTM) that facilities the\ntransformation of the features extracted from a source viewpoint image into an\nintrinsic representation with respect to a pre-defined reference pose and a\nview generation module (VGM) that synthesizes an arbitrary view from the\nrepresentation. The learned transformation allows us to synthesize a novel view\nfrom any single source viewpoint image of unknown pose. Experiments on the\nwidely used view synthesis datasets have demonstrated that the proposed network\nis able to produce comparable results to the state-of-the-art methods despite\nthe fact that learning is unsupervised and only a single source viewpoint image\nis required for generating a novel view. The code will be available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingzheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jianjun Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chuanbo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_N/0/1/0/all/0/1\">Nam Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ST-ABN: Visual Explanation Taking into Account Spatio-temporal Information for Video Recognition. (arXiv:2110.15574v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15574","description":"<p>It is difficult for people to interpret the decision-making in the inference\nprocess of deep neural networks. Visual explanation is one method for\ninterpreting the decision-making of deep learning. It analyzes the\ndecision-making of 2D CNNs by visualizing an attention map that highlights\ndiscriminative regions. Visual explanation for interpreting the decision-making\nprocess in video recognition is more difficult because it is necessary to\nconsider not only spatial but also temporal information, which is different\nfrom the case of still images. In this paper, we propose a visual explanation\nmethod called spatio-temporal attention branch network (ST-ABN) for video\nrecognition. It enables visual explanation for both spatial and temporal\ninformation. ST-ABN acquires the importance of spatial and temporal information\nduring network inference and applies it to recognition processing to improve\nrecognition performance and visual explainability. Experimental results with\nSomething-Something datasets V1 \\&amp; V2 demonstrated that ST-ABN enables visual\nexplanation that takes into account spatial and temporal information\nsimultaneously and improves recognition performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitsuhara_M/0/1/0/all/0/1\">Masahiro Mitsuhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirakawa_T/0/1/0/all/0/1\">Tsubasa Hirakawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_T/0/1/0/all/0/1\">Takayoshi Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujiyoshi_H/0/1/0/all/0/1\">Hironobu Fujiyoshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Whole Brain Segmentation with Full Volume Neural Network. (arXiv:2110.15601v1 [eess.IV])","link":"http://arxiv.org/abs/2110.15601","description":"<p>Whole brain segmentation is an important neuroimaging task that segments the\nwhole brain volume into anatomically labeled regions-of-interest. Convolutional\nneural networks have demonstrated good performance in this task. Existing\nsolutions, usually segment the brain image by classifying the voxels, or\nlabeling the slices or the sub-volumes separately. Their representation\nlearning is based on parts of the whole volume whereas their labeling result is\nproduced by aggregation of partial segmentation. Learning and inference with\nincomplete information could lead to sub-optimal final segmentation result. To\naddress these issues, we propose to adopt a full volume framework, which feeds\nthe full volume brain image into the segmentation network and directly outputs\nthe segmentation result for the whole brain volume. The framework makes use of\ncomplete information in each volume and can be implemented easily. An effective\ninstance in this framework is given subsequently. We adopt the $3$D\nhigh-resolution network (HRNet) for learning spatially fine-grained\nrepresentations and the mixed precision training scheme for memory-efficient\ntraining. Extensive experiment results on a publicly available $3$D MRI brain\ndataset show that our proposed model advances the state-of-the-art methods in\nterms of segmentation performance. Source code is publicly available at\nhttps://github.com/microsoft/VoxHRNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yeshu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_J/0/1/0/all/0/1\">Jonathan Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheng_Y/0/1/0/all/0/1\">Yilun Sheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_X/0/1/0/all/0/1\">Xiao Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_E/0/1/0/all/0/1\">Eric I-Chao Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Camouflaged Object Detection with the Uncertainty of Pseudo-edge Labels. (arXiv:2110.15606v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15606","description":"<p>This paper focuses on camouflaged object detection (COD), which is a task to\ndetect objects hidden in the background. Most of the current COD models aim to\nhighlight the target object directly while outputting ambiguous camouflaged\nboundaries. On the other hand, the performance of the models considering edge\ninformation is not yet satisfactory. To this end, we propose a new framework\nthat makes full use of multiple visual cues, i.e., saliency as well as edges,\nto refine the predicted camouflaged map. This framework consists of three key\ncomponents, i.e., a pseudo-edge generator, a pseudo-map generator, and an\nuncertainty-aware refinement module. In particular, the pseudo-edge generator\nestimates the boundary that outputs the pseudo-edge label, and the conventional\nCOD method serves as the pseudo-map generator that outputs the pseudo-map\nlabel. Then, we propose an uncertainty-based module to reduce the uncertainty\nand noise of such two pseudo labels, which takes both pseudo labels as input\nand outputs an edge-accurate camouflaged map. Experiments on various COD\ndatasets demonstrate the effectiveness of our method with superior performance\nto the existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kajiura_N/0/1/0/all/0/1\">Nobukatsu Kajiura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_S/0/1/0/all/0/1\">Shin&#x27;ichi Satoh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Spatio-temporal Relation-enhanced Network for Cross-modal Text-Video Retrieval. (arXiv:2110.15609v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15609","description":"<p>The task of cross-modal retrieval between texts and videos aims to understand\nthe correspondence between vision and language. Existing studies follow a trend\nof measuring text-video similarity on the basis of textual and video\nembeddings. In common practice, video representation is constructed by feeding\nvideo frames into 2D/3D-CNN for global visual feature extraction or only\nlearning simple semantic relations by using local-level fine-grained frame\nregions via graph convolutional network. However, these video representations\ndo not fully exploit spatio-temporal relation among visual components in\nlearning video representations, resulting in their inability to distinguish\nvideos with the same visual components but with different relations. To solve\nthis problem, we propose a Visual Spatio-temporal Relation-enhanced Network\n(VSR-Net), a novel cross-modal retrieval framework that enhances visual\nrepresentation with spatio-temporal relations among components. Specifically,\nvisual spatio-temporal relations are encoded using a multi-layer\nspatio-temporal transformer to learn visual relational features. We combine\nfine-grained local relation and global features in bridging text-video\nmodalities. Extensive experimental are conducted on both MSR-VTT and MSVD\ndatasets. The results demonstrate the effectiveness of our proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_N/0/1/0/all/0/1\">Ning Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Guangyi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yawen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chuhao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Person Re-Identification with Wireless Positioning under Weak Scene Labeling. (arXiv:2110.15610v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15610","description":"<p>Existing unsupervised person re-identification methods only rely on visual\nclues to match pedestrians under different cameras. Since visual data is\nessentially susceptible to occlusion, blur, clothing changes, etc., a promising\nsolution is to introduce heterogeneous data to make up for the defect of visual\ndata. Some works based on full-scene labeling introduce wireless positioning to\nassist cross-domain person re-identification, but their GPS labeling of entire\nmonitoring scenes is laborious. To this end, we propose to explore unsupervised\nperson re-identification with both visual data and wireless positioning\ntrajectories under weak scene labeling, in which we only need to know the\nlocations of the cameras. Specifically, we propose a novel unsupervised\nmultimodal training framework (UMTF), which models the complementarity of\nvisual data and wireless information. Our UMTF contains a multimodal data\nassociation strategy (MMDA) and a multimodal graph neural network (MMGN). MMDA\nexplores potential data associations in unlabeled multimodal data, while MMGN\npropagates multimodal messages in the video graph based on the adjacency matrix\nlearned from histogram statistics of wireless data. Thanks to the robustness of\nthe wireless data to visual noise and the collaboration of various modules,\nUMTF is capable of learning a model free of the human label on data. Extensive\nexperimental results conducted on two challenging datasets, i.e., WP-ReID and\nDukeMTMC-VideoReID demonstrate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qiaokang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attacking Video Recognition Models with Bullet-Screen Comments. (arXiv:2110.15629v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15629","description":"<p>Recent research has demonstrated that Deep Neural Networks (DNNs) are\nvulnerable to adversarial patches which introducing perceptible but localized\nchanges to the input. Nevertheless, existing approaches have focused on\ngenerating adversarial patches on images, their counterparts in videos have\nbeen less explored. Compared with images, attacking videos is much more\nchallenging as it needs to consider not only spatial cues but also temporal\ncues. To close this gap, we introduce a novel adversarial attack in this paper,\nthe bullet-screen comment (BSC) attack, which attacks video recognition models\nwith BSCs. Specifically, adversarial BSCs are generated with a Reinforcement\nLearning (RL) framework, where the environment is set as the target model and\nthe agent plays the role of selecting the position and transparency of each\nBSC. By continuously querying the target models and receiving feedback, the\nagent gradually adjusts its selection strategies in order to achieve a high\nfooling rate with non-overlapping BSCs. As BSCs can be regarded as a kind of\nmeaningful patch, adding it to a clean video will not affect people' s\nunderstanding of the video content, nor will arouse people' s suspicion. We\nconduct extensive experiments to verify the effectiveness of the proposed\nmethod. On both UCF-101 and HMDB-51 datasets, our BSC attack method can achieve\nabout 90\\% fooling rate when attack three mainstream video recognition models,\nwhile only occluding \\textless 8\\% areas in the video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhipeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task and Multi-Modal Learning for RGB Dynamic Gesture Recognition. (arXiv:2110.15639v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15639","description":"<p>Gesture recognition is getting more and more popular due to various\napplication possibilities in human-machine interaction. Existing multi-modal\ngesture recognition systems take multi-modal data as input to improve accuracy,\nbut such methods require more modality sensors, which will greatly limit their\napplication scenarios. Therefore we propose an end-to-end multi-task learning\nframework in training 2D convolutional neural networks. The framework can use\nthe depth modality to improve accuracy during training and save costs by using\nonly RGB modality during inference. Our framework is trained to learn a\nrepresentation for multi-task learning: gesture segmentation and gesture\nrecognition. Depth modality contains the prior information for the location of\nthe gesture. Therefore it can be used as the supervision for gesture\nsegmentation. A plug-and-play module named Multi-Scale-Decoder is designed to\nrealize gesture segmentation, which contains two sub-decoder. It is used in the\nlower stage and higher stage respectively, and can help the network pay\nattention to key target areas, ignore irrelevant information, and extract more\ndiscriminant features. Additionally, the MSD module and depth modality are only\nused in the training stage to improve gesture recognition performance. Only RGB\nmodality and network without MSD are required during inference. Experimental\nresults on three public gesture recognition datasets show that our proposed\nmethod provides superior performance compared with existing gesture recognition\nframeworks. Moreover, using the proposed plug-and-play MSD in other 2D\nCNN-based frameworks also get an excellent accuracy improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Dinghao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hengjie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shugong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gabor filter incorporated CNN for compression. (arXiv:2110.15644v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15644","description":"<p>Convolutional neural networks (CNNs) are remarkably successful in many\ncomputer vision tasks. However, the high cost of inference is problematic for\nembedded and real-time systems, so there are many studies on compressing the\nnetworks. On the other hand, recent advances in self-attention models showed\nthat convolution filters are preferable to self-attention in the earlier\nlayers, which indicates that stronger inductive biases are better in the\nearlier layers. As shown in convolutional filters, strong biases can train\nspecific filters and construct unnecessarily filters to zero. This is analogous\nto classical image processing tasks, where choosing the suitable filters makes\na compact dictionary to represent features. We follow this idea and incorporate\nGabor filters in the earlier layers of CNNs for compression. The parameters of\nGabor filters are learned through backpropagation, so the features are\nrestricted to Gabor filters. We show that the first layer of VGG-16 for\nCIFAR-10 has 192 kernels/features, but learning Gabor filters requires an\naverage of 29.4 kernels. Also, using Gabor filters, an average of 83% and 94%\nof kernels in the first and the second layer, respectively, can be removed on\nthe altered ResNet-20, where the first five layers are exchanged with two\nlayers of larger kernels for CIFAR-10.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imamura_A/0/1/0/all/0/1\">Akihiro Imamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arizumi_N/0/1/0/all/0/1\">Nana Arizumi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-Aware Dynamic Network for Continuous-Scale Super-Resolution. (arXiv:2110.15655v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15655","description":"<p>Single-image super-resolution (SR) with fixed and discrete scale factors has\nachieved great progress due to the development of deep learning technology.\nHowever, the continuous-scale SR, which aims to use a single model to process\narbitrary (integer or non-integer) scale factors, is still a challenging task.\nThe existing SR models generally adopt static convolution to extract features,\nand thus unable to effectively perceive the change of scale factor, resulting\nin limited generalization performance on multi-scale SR tasks. Moreover, the\nexisting continuous-scale upsampling modules do not make full use of\nmulti-scale features and face problems such as checkerboard artifacts in the SR\nresults and high computational complexity. To address the above problems, we\npropose a scale-aware dynamic network (SADN) for continuous-scale SR. First, we\npropose a scale-aware dynamic convolutional (SAD-Conv) layer for the feature\nlearning of multiple SR tasks with various scales. The SAD-Conv layer can\nadaptively adjust the attention weights of multiple convolution kernels based\non the scale factor, which enhances the expressive power of the model with a\nnegligible extra computational cost. Second, we devise a continuous-scale\nupsampling module (CSUM) with the multi-bilinear local implicit function\n(MBLIF) for any-scale upsampling. The CSUM constructs multiple feature spaces\nwith gradually increasing scales to approximate the continuous feature\nrepresentation of an image, and then the MBLIF makes full use of multi-scale\nfeatures to map arbitrary coordinates to RGB values in high-resolution space.\nWe evaluate our SADN using various benchmarks. The experimental results show\nthat the CSUM can replace the previous fixed-scale upsampling layers and obtain\na continuous-scale SR network while maintaining performance. Our SADN uses much\nfewer parameters and outperforms the state-of-the-art SR methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanlin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_N/0/1/0/all/0/1\">Ning Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-OOCS: Learning Prostate Segmentation with Inductive Bias. (arXiv:2110.15664v1 [eess.IV])","link":"http://arxiv.org/abs/2110.15664","description":"<p>Despite the great success of convolutional neural networks (CNN) in 3D\nmedical image segmentation tasks, the methods currently in use are still not\nrobust enough to the different protocols utilized by different scanners, and to\nthe variety of image properties or artefacts they produce. To this end, we\nintroduce OOCS-enhanced networks, a novel architecture inspired by the innate\nnature of visual processing in the vertebrates. With different 3D U-Net\nvariants as the base, we add two 3D residual components to the second encoder\nblocks: on and off center-surround (OOCS). They generalise the ganglion\npathways in the retina to a 3D setting. The use of 2D-OOCS in any standard CNN\nnetwork complements the feedforward framework with sharp edge-detection\ninductive biases. The use of 3D-OOCS also helps 3D U-Nets to scrutinise and\ndelineate anatomical structures present in 3D images with increased accuracy.We\ncompared the state-of-the-art 3D U-Nets with their 3D-OOCS extensions and\nshowed the superior accuracy and robustness of the latter in automatic prostate\nsegmentation from 3D Magnetic Resonance Images (MRIs). For a fair comparison,\nwe trained and tested all the investigated 3D U-Nets with the same pipeline,\nincluding automatic hyperparameter optimisation and data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhandary_S/0/1/0/all/0/1\">Shrajan Bhandary</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Babaiee_Z/0/1/0/all/0/1\">Zahra Babaiee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kostyszyn_D/0/1/0/all/0/1\">Dejan Kostyszyn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fechter_T/0/1/0/all/0/1\">Tobias Fechter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zamboglou_C/0/1/0/all/0/1\">Constantinos Zamboglou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grosu_A/0/1/0/all/0/1\">Anca Grosu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grosu_R/0/1/0/all/0/1\">Radu Grosu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-target tracking for video surveillance using deep affinity network: a brief review. (arXiv:2110.15674v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15674","description":"<p>Deep learning models are known to function like the human brain. Due to their\nfunctional mechanism, they are frequently utilized to accomplish tasks that\nrequire human intelligence. Multi-target tracking (MTT) for video surveillance\nis one of the important and challenging tasks, which has attracted the\nresearcher's attention due to its potential applications in various domains.\nMulti-target tracking tasks require locating the objects individually in each\nframe, which remains a huge challenge as there are immediate changes in\nappearances and extreme occlusions of objects. In addition to that, the\nMultitarget tracking framework requires multiple tasks to perform i.e. target\ndetection, estimating trajectory, associations between frame, and\nre-identification. Various methods have been suggested, and some assumptions\nare made to constrain the problem in the context of a particular problem. In\nthis paper, the state-of-the-art MTT models, which leverage from deep learning\nrepresentational power are reviewed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mangi_S/0/1/0/all/0/1\">Sanam Nisar Mangi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis. (arXiv:2110.15678v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15678","description":"<p>The advancement of generative radiance fields has pushed the boundary of\n3D-aware image synthesis. Motivated by the observation that a 3D object should\nlook realistic from multiple viewpoints, these methods introduce a multi-view\nconstraint as regularization to learn valid 3D radiance fields from 2D images.\nDespite the progress, they often fall short of capturing accurate 3D shapes due\nto the shape-color ambiguity, limiting their applicability in downstream tasks.\nIn this work, we address this ambiguity by proposing a novel shading-guided\ngenerative implicit model that is able to learn a starkly improved shape\nrepresentation. Our key insight is that an accurate 3D shape should also yield\na realistic rendering under different lighting conditions. This multi-lighting\nconstraint is realized by modeling illumination explicitly and performing\nshading with various lighting conditions. Gradients are derived by feeding the\nsynthesized images to a discriminator. To compensate for the additional\ncomputational burden of calculating surface normals, we further devise an\nefficient volume rendering strategy via surface tracking, reducing the training\nand inference time by 24% and 48%, respectively. Our experiments on multiple\ndatasets show that the proposed approach achieves photorealistic 3D-aware image\nsynthesis while capturing accurate underlying 3D shapes. We demonstrate\nimproved performance of our approach on 3D shape reconstruction against\nexisting methods, and show its applicability on image relighting. Our code will\nbe released at https://github.com/XingangPan/ShadeGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xudong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"False Positive Detection and Prediction Quality Estimation for LiDAR Point Cloud Segmentation. (arXiv:2110.15681v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15681","description":"<p>We present a novel post-processing tool for semantic segmentation of LiDAR\npoint cloud data, called LidarMetaSeg, which estimates the prediction quality\nsegmentwise. For this purpose we compute dispersion measures based on network\nprobability outputs as well as feature measures based on point cloud input\nfeatures and aggregate them on segment level. These aggregated measures are\nused to train a meta classification model to predict whether a predicted\nsegment is a false positive or not and a meta regression model to predict the\nsegmentwise intersection over union. Both models can then be applied to\nsemantic segmentation inferences without knowing the ground truth. In our\nexperiments we use different LiDAR segmentation models and datasets and analyze\nthe power of our method. We show that our results outperform other standard\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colling_P/0/1/0/all/0/1\">Pascal Colling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roese_Koerner_L/0/1/0/all/0/1\">Lutz Roese-Koerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Effective Image Restorer: Denoising and Luminance Adjustment for Low-photon-count Imaging. (arXiv:2110.15715v1 [eess.IV])","link":"http://arxiv.org/abs/2110.15715","description":"<p>Imaging under photon-scarce situations introduces challenges to many\napplications as the captured images are with low signal-to-noise ratio and poor\nluminance. In this paper, we investigate the raw image restoration under\nlow-photon-count conditions by simulating the imaging of quanta image sensor\n(QIS). We develop a lightweight framework, which consists of a multi-level\npyramid denoising network (MPDNet) and a luminance adjustment (LA) module to\nachieve separate denoising and luminance enhancement. The main component of our\nframework is the multi-skip attention residual block (MARB), which integrates\nmulti-scale feature fusion and attention mechanism for better feature\nrepresentation. Our MPDNet adopts the idea of Laplacian pyramid to learn the\nsmall-scale noise map and larger-scale high-frequency details at different\nlevels, and feature extractions are conducted on the multi-scale input images\nto encode richer contextual information. Our LA module enhances the luminance\nof the denoised image by estimating its illumination, which can better avoid\ncolor distortion. Extensive experimental results have demonstrated that our\nimage restorer can achieve superior performance on the degraded images with\nvarious photon levels by suppressing noise and recovering luminance and color\neffectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shansi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lam_E/0/1/0/all/0/1\">Edmund Y. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generational Frameshifts in Technology: Computer Science and Neurosurgery, The VR Use Case. (arXiv:2110.15719v1 [cs.HC])","link":"http://arxiv.org/abs/2110.15719","description":"<p>We are at a unique moment in history where there is a confluence of\ntechnologies which will synergistically come together to transform the practice\nof neurosurgery. These technological transformations will be all-encompassing,\nincluding improved tools and methods for intraoperative performance of\nneurosurgery, scalable solutions for asynchronous neurosurgical training and\nsimulation, as well as broad aggregation of operative data allowing fundamental\nchanges in quality assessment, billing, outcome measures, and dissemination of\nsurgical best practices. The ability to perform surgery more safely and more\nefficiently while capturing the operative details and parsing each component of\nthe operation will open an entirely new epoch advancing our field and all\nsurgical specialties. The digitization of all components within the operating\nroom will allow us to leverage the various fields within computer and\ncomputational science to obtain new insights that will improve care and\ndelivery of the highest quality neurosurgery regardless of location. The\ndemocratization of neurosurgery is at hand and will be driven by our\ndevelopment, extraction, and adoption of these tools of the modern world.\nVirtual reality provides a good example of how consumer-facing technologies are\nfinding a clear role in industry and medicine and serves as a notable example\nof the confluence of various computer science technologies creating a novel\nparadigm for scaling human ability and interactions. The authors describe the\ntechnology ecosystem that has come and highlight a myriad of computational and\ndata sciences that will be necessary to enable the operating room of the near\nfuture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Browd_S/0/1/0/all/0/1\">Samuel R. Browd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Maya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_C/0/1/0/all/0/1\">Chetan Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CVAD: A generic medical anomaly detector based on Cascade VAE. (arXiv:2110.15811v1 [eess.IV])","link":"http://arxiv.org/abs/2110.15811","description":"<p>Detecting out-of-distribution (OOD) samples in medical imaging plays an\nimportant role for downstream medical diagnosis. However, existing OOD\ndetectors are demonstrated on natural images composed of inter-classes and have\ndifficulty generalizing to medical images. The key issue is the granularity of\nOOD data in the medical domain, where intra-class OOD samples are predominant.\nWe focus on the generalizability of OOD detection for medical images and\npropose a self-supervised Cascade Variational autoencoder-based Anomaly\nDetector (CVAD). We use a variational autoencoders' cascade architecture, which\ncombines latent representation at multiple scales, before being fed to a\ndiscriminator to distinguish the OOD data from the in-distribution (ID) data.\nFinally, both the reconstruction error and the OOD probability predicted by the\nbinary discriminator are used to determine the anomalies. We compare the\nperformance with the state-of-the-art deep learning models to demonstrate our\nmodel's efficacy on various open-access medical imaging datasets for both\nintra- and inter-class OOD. Further extensive results on datasets including\ncommon natural datasets show our model's effectiveness and generalizability.\nThe code is available at https://github.com/XiaoyuanGuo/CVAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyuan Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1\">Judy Wawira Gichoya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Purkayastha_S/0/1/0/all/0/1\">Saptarshi Purkayastha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Banerjee_I/0/1/0/all/0/1\">Imon Banerjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A GIS Data Realistic Road Generation Approach for Traffic Simulation. (arXiv:2110.15814v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15814","description":"<p>Road networks exist in the form of polylines with attributes within the GIS\ndatabases. Such a representation renders the geographic data impracticable for\n3D road traffic simulation. In this work, we propose a method to transform raw\nGIS data into a realistic, operational model for real-time road traffic\nsimulation. For instance, the proposed raw to simulation ready data\ntransformation is achieved through several curvature estimation,\ninterpolation/approximation, and clustering schemes. The obtained results show\nthe performance of our approach and prove its adequacy to real traffic\nsimulation scenario as can be seen in this video 1 .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amara_Y/0/1/0/all/0/1\">Yacine Amara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amamra_A/0/1/0/all/0/1\">Abdenour Amamra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daheur_Y/0/1/0/all/0/1\">Yasmine Daheur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saichi_L/0/1/0/all/0/1\">Lamia Saichi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time multiview data fusion for object tracking with RGBD sensors. (arXiv:2110.15815v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15815","description":"<p>This paper presents a new approach to accurately track a moving vehicle with\na multiview setup of red-green-blue depth (RGBD) cameras. We first propose a\ncorrection method to eliminate a shift, which occurs in depth sensors when they\nbecome worn. This issue could not be otherwise corrected with the ordinary\ncalibration procedure. Next, we present a sensor-wise filtering system to\ncorrect for an unknown vehicle motion. A data fusion algorithm is then used to\noptimally merge the sensor-wise estimated trajectories. We implement most parts\nof our solution in the graphic processor. Hence, the whole system is able to\noperate at up to 25 frames per second with a configuration of five cameras.\nTest results show the accuracy we achieved and the robustness of our solution\nto overcome uncertainties in the measurements and the modelling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amamra_A/0/1/0/all/0/1\">Abdenour Amamra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1\">Nabil Aouf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C-MADA: Unsupervised Cross-Modality Adversarial Domain Adaptation framework for medical Image Segmentation. (arXiv:2110.15823v1 [eess.IV])","link":"http://arxiv.org/abs/2110.15823","description":"<p>Deep learning models have obtained state-of-the-art results for medical image\nanalysis. However, when these models are tested on an unseen domain there is a\nsignificant performance degradation. In this work, we present an unsupervised\nCross-Modality Adversarial Domain Adaptation (C-MADA) framework for medical\nimage segmentation. C-MADA implements an image- and feature-level adaptation\nmethod in a sequential manner. First, images from the source domain are\ntranslated to the target domain through an un-paired image-to-image adversarial\ntranslation with cycle-consistency loss. Then, a U-Net network is trained with\nthe mapped source domain images and target domain images in an adversarial\nmanner to learn domain-invariant feature representations. Furthermore, to\nimprove the networks segmentation performance, information about the shape,\ntexture, and con-tour of the predicted segmentation is included during the\nadversarial train-ing. C-MADA is tested on the task of brain MRI segmentation,\nobtaining competitive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baldeon_Calisto_M/0/1/0/all/0/1\">Maria Baldeon-Calisto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_Yuen_S/0/1/0/all/0/1\">Susana K. Lai-Yuen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of 2-D Convolutional Neural Networks for Damage Detection in Steel Frame Structures. (arXiv:2110.15895v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15895","description":"<p>In this paper, we present an application of 2-D convolutional neural networks\n(2-D CNNs) designed to perform both feature extraction and classification\nstages as a single organism to solve the highlighted problems. The method uses\na network of lighted CNNs instead of deep and takes raw acceleration signals as\ninput. Using lighted CNNs, in which every one of them is optimized for a\nspecific element, increases the accuracy and makes the network faster to\nperform. Also, a new framework is proposed for decreasing the data required in\nthe training phase. We verified our method on Qatar University Grandstand\nSimulator (QUGS) benchmark data provided by Structural Dynamics Team. The\nresults showed improved accuracy over other methods, and running time was\nadequate for real-time applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghazvineh_S/0/1/0/all/0/1\">Shahin Ghazvineh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouri_G/0/1/0/all/0/1\">Gholamreza Nouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavassani_S/0/1/0/all/0/1\">Seyed Hossein Hosseini Lavassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharehbaghi_V/0/1/0/all/0/1\">Vahidreza Gharehbaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Andy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Co-segmentation by Segment Swapping for Retrieval and Discovery. (arXiv:2110.15904v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15904","description":"<p>The goal of this work is to efficiently identify visually similar patterns\nfrom a pair of images, e.g. identifying an artwork detail copied between an\nengraving and an oil painting, or matching a night-time photograph with its\ndaytime counterpart. Lack of training data is a key challenge for this\nco-segmentation task. We present a simple yet surprisingly effective approach\nto overcome this difficulty: we generate synthetic training pairs by selecting\nobject segments in an image and copy-pasting them into another image. We then\nlearn to predict the repeated object masks. We find that it is crucial to\npredict the correspondences as an auxiliary task and to use Poisson blending\nand style transfer on the training pairs to generalize on real data. We analyse\nresults with two deep architectures relevant to our joint image analysis task:\na transformer-based architecture and Sparse Nc-Net, a recent network designed\nto predict coarse correspondences using 4D convolutions.\n</p>\n<p>We show our approach provides clear improvements for artwork details\nretrieval on the Brueghel dataset and achieves competitive performance on two\nplace recognition benchmarks, Tokyo247 and Pitts30K. We then demonstrate the\npotential of our approach by performing object discovery on the Internet object\ndiscovery dataset and the Brueghel dataset. Our code and data are available at\n<a href=\"http://imagine.enpc.fr/~shenx/SegSwap/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1\">Mathieu Aubry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the use of uncertainty in classifying Aedes Albopictus mosquitoes. (arXiv:2110.15912v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15912","description":"<p>The re-emergence of mosquito-borne diseases (MBDs), which kill hundreds of\nthousands of people each year, has been attributed to increased human\npopulation, migration, and environmental changes. Convolutional neural networks\n(CNNs) have been used by several studies to recognise mosquitoes in images\nprovided by projects such as Mosquito Alert to assist entomologists in\nidentifying, monitoring, and managing MBD. Nonetheless, utilising CNNs to\nautomatically label input samples could involve incorrect predictions, which\nmay mislead future epidemiological studies. Furthermore, CNNs require large\nnumbers of manually annotated data. In order to address the mentioned issues,\nthis paper proposes using the Monte Carlo Dropout method to estimate the\nuncertainty scores in order to rank the classified samples to reduce the need\nfor human supervision in recognising Aedes albopictus mosquitoes. The estimated\nuncertainty was also used in an active learning framework, where just a portion\nof the data from large training sets was manually labelled. The experimental\nresults show that the proposed classification method with rejection outperforms\nthe competing methods by improving overall performance and reducing\nentomologist annotation workload. We also provide explainable visualisations of\nthe different regions that contribute to a set of samples' uncertainty\nassessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adhane_G/0/1/0/all/0/1\">Gereziher Adhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehshibi_M/0/1/0/all/0/1\">Mohammad Mahdi Dehshibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masip_D/0/1/0/all/0/1\">David Masip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating and Maximizing Mutual Information for Knowledge Distillation. (arXiv:2110.15946v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15946","description":"<p>Knowledge distillation is a widely used general technique to transfer\nknowledge from a teacher network to a student network. In this work, we propose\nMutual Information Maximization Knowledge Distillation (MIMKD). Our method uses\na contrastive objective to simultaneously estimate and maximize a lower bound\non the mutual information between intermediate and global feature\nrepresentations from the teacher and the student networks. Our method is\nflexible, as the proposed mutual information maximization does not impose\nsignificant constraints on the structure of the intermediate features of the\nnetworks. As such, we can distill knowledge from arbitrary teachers to\narbitrary students. Our empirical results show that our method outperforms\ncompeting approaches across a wide range of student-teacher pairs with\ndifferent capacities, with different architectures, and when student networks\nare with extremely low capacity. We are able to obtain 74.55% accuracy on\nCIFAR100 with a ShufflenetV2 from a baseline accuracy of 69.8% by distilling\nknowledge from ResNet50.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Aman Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A deep convolutional neural network for classification of Aedes albopictus mosquitoes. (arXiv:2110.15956v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15956","description":"<p>Monitoring the spread of disease-carrying mosquitoes is a first and necessary\nstep to control severe diseases such as dengue, chikungunya, Zika or yellow\nfever. Previous citizen science projects have been able to obtain large image\ndatasets with linked geo-tracking information. As the number of international\ncollaborators grows, the manual annotation by expert entomologists of the large\namount of data gathered by these users becomes too time demanding and\nunscalable, posing a strong need for automated classification of mosquito\nspecies from images. We introduce the application of two Deep Convolutional\nNeural Networks in a comparative study to automate this classification task. We\nuse the transfer learning principle to train two state-of-the-art architectures\non the data provided by the Mosquito Alert project, obtaining testing accuracy\nof 94%. In addition, we applied explainable models based on the Grad-CAM\nalgorithm to visualise the most discriminant regions of the classified images,\nwhich coincide with the white band stripes located at the legs, abdomen, and\nthorax of mosquitoes of the Aedes albopictus species. The model allows us to\nfurther analyse the classification errors. Visual Grad-CAM models show that\nthey are linked to poor acquisition conditions and strong image occlusions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adhane_G/0/1/0/all/0/1\">Gereziher Adhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehshibi_M/0/1/0/all/0/1\">Mohammad Mahdi Dehshibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masip_D/0/1/0/all/0/1\">David Masip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Keyword Spotting with Attention. (arXiv:2110.15957v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15957","description":"<p>In this paper, we consider the task of spotting spoken keywords in silent\nvideo sequences -- also known as visual keyword spotting. To this end, we\ninvestigate Transformer-based models that ingest two streams, a visual encoding\nof the video and a phonetic encoding of the keyword, and output the temporal\nlocation of the keyword if present. Our contributions are as follows: (1) We\npropose a novel architecture, the Transpotter, that uses full cross-modal\nattention between the visual and phonetic streams; (2) We show through\nextensive evaluations that our model outperforms the prior state-of-the-art\nvisual keyword spotting and lip reading methods on the challenging LRW, LRS2,\nLRS3 datasets by a large margin; (3) We demonstrate the ability of our model to\nspot words under the extreme conditions of isolated mouthings in sign language\nvideos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prajwal_K/0/1/0/all/0/1\">K R Prajwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momeni_L/0/1/0/all/0/1\">Liliane Momeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1\">Triantafyllos Afouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parabolic Approximation Line Search for DNNs. (arXiv:1903.11991v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1903.11991","description":"<p>A major challenge in current optimization research for deep learning is to\nautomatically find optimal step sizes for each update step. The optimal step\nsize is closely related to the shape of the loss in the update step direction.\nHowever, this shape has not yet been examined in detail. This work shows\nempirically that the batch loss over lines in negative gradient direction is\nmostly convex locally and well suited for one-dimensional parabolic\napproximations. By exploiting this parabolic property we introduce a simple and\nrobust line search approach, which performs loss-shape dependent update steps.\nOur approach combines well-known methods such as parabolic approximation, line\nsearch and conjugate gradient, to perform efficiently. It surpasses other step\nsize estimating methods and competes with common optimization methods on a\nlarge variety of experiments without the need of hand-designed step size\nschedules. Thus, it is of interest for objectives where step-size schedules are\nunknown or do not perform well. Our extensive evaluation includes multiple\ncomprehensive hyperparameter grid searches on several datasets and\narchitectures. Finally, we provide a general investigation of exact line\nsearches in the context of batch losses and exact losses, including their\nrelation to our line search approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mutschler_M/0/1/0/all/0/1\">Maximus Mutschler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Histogram Layers for Texture Analysis. (arXiv:2001.00215v11 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2001.00215","description":"<p>An essential aspect of texture analysis is the extraction of features that\ndescribe the distribution of values in local, spatial regions. We present a\nlocalized histogram layer for artificial neural networks. Instead of computing\nglobal histograms as done previously, the proposed histogram layer directly\ncomputes the local, spatial distribution of features for texture analysis and\nparameters for the layer are estimated during backpropagation. We compare our\nmethod with state-of-the-art texture encoding methods such as the Deep Encoding\nNetwork Pooling, Deep Texture Encoding Network, Fisher Vector convolutional\nneural network, and Multi-level Texture Encoding and Representation on three\nmaterial/texture datasets: (1) the Describable Texture Dataset; (2) an\nextension of the ground terrain in outdoor scenes; (3) and a subset of the\nMaterials in Context dataset. Results indicate that the inclusion of the\nproposed histogram layer improves performance. The source code for the\nhistogram layer is publicly available:\nhttps://github.com/GatorSense/Histogram_Layer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peeples_J/0/1/0/all/0/1\">Joshua Peeples</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weihuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zare_A/0/1/0/all/0/1\">Alina Zare</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Renet: An improvement method for remote object detection based on Darknet. (arXiv:2002.03729v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.03729","description":"<p>Recently, when we used this method to identify aircraft targets in remote\nsensing images, we found that there are some defects in our own YOLOv2 and\nDarknet-19 network. Characteristic in the images we identified are not very\nclear,thats why we couldn't get some much more good results. Then we replaced\nthe maxpooling in the yolov3 network as the global maxpooling.Under the same\ntest conditions, we got a higher It achieves the processing speed of a single\nimage is only 0.023 s on a GTX1050TI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengquan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image-generation Enhanced Adaptation for Object Detection in Thermal images. (arXiv:2002.06770v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.06770","description":"<p>Object detection in thermal images is an important computer vision task and\nhas many applications such as unmanned vehicles, robotics, surveillance and\nnight vision. Deep learning based detectors have achieved major progress, which\nusually need large amount of labelled training data. However, labelled data for\nobject detection in thermal images is scarce and expensive to collect. How to\ntake advantage of the large number labelled visible images and adapt them into\nthermal image domain, is expected to solve. This paper proposes an unsupervised\nimage-generation enhanced adaptation method for object detection in thermal\nimages. To reduce the gap between visible domain and thermal domain, the\nproposed method manages to generate simulated fake thermal images that are\nsimilar to the target images, and preserves the annotation information of the\nvisible source domain. The image generation includes a CycleGAN based\nimage-to-image translation and an intensity inversion transformation. Generated\nfake thermal images are used as renewed source domain. And then the\noff-the-shelf Domain Adaptive Faster RCNN is utilized to reduce the gap between\ngenerated intermediate domain and the thermal target domain. Experiments\ndemonstrate the effectiveness and superiority of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fuyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanyi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Efficient Performance Estimators of Neural Architectures. (arXiv:2008.03064v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.03064","description":"<p>Conducting efficient performance estimations of neural architectures is a\nmajor challenge in neural architecture search (NAS). To reduce the architecture\ntraining costs in NAS, one-shot estimators (OSEs) amortize the architecture\ntraining costs by sharing the parameters of one \"supernet\" between all\narchitectures. Recently, zero-shot estimators (ZSEs) that involve no training\nare proposed to further reduce the architecture evaluation cost. Despite the\nhigh efficiency of these estimators, the quality of such estimations has not\nbeen thoroughly studied. In this paper, we conduct an extensive and organized\nassessment of OSEs and ZSEs on five NAS benchmarks: NAS-Bench-101/201/301, and\nNDS ResNet/ResNeXt-A. Specifically, we employ a set of NAS-oriented criteria to\nstudy the behavior of OSEs and ZSEs and reveal that they have certain biases\nand variances. After analyzing how and why the OSE estimations are\nunsatisfying, we explore how to mitigate the correlation gap of OSEs from\nseveral perspectives. Through our analysis, we give out suggestions for future\napplication and development of efficient architecture performance estimators.\nFurthermore, the analysis framework proposed in our work could be utilized in\nfuture research to give a more comprehensive understanding of newly designed\narchitecture performance estimators. All codes are available at\nhttps://github.com/walkerning/aw_nas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xuefei Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Changcheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenshuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zixuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shuang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huazhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Explanation is Not Enough: Structured Attention Graphs for Image Classification. (arXiv:2011.06733v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.06733","description":"<p>Attention maps are a popular way of explaining the decisions of convolutional\nnetworks for image classification. Typically, for each image of interest, a\nsingle attention map is produced, which assigns weights to pixels based on\ntheir importance to the classification. A single attention map, however,\nprovides an incomplete understanding since there are often many other maps that\nexplain a classification equally well. In this paper, we introduce structured\nattention graphs (SAGs), which compactly represent sets of attention maps for\nan image by capturing how different combinations of image regions impact a\nclassifier's confidence. We propose an approach to compute SAGs and a\nvisualization for SAGs so that deeper insight can be gained into a classifier's\ndecisions. We conduct a user study comparing the use of SAGs to traditional\nattention maps for answering counterfactual questions about image\nclassifications. Our results show that the users are more correct when\nanswering comparative counterfactual questions based on SAGs compared to the\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shitole_V/0/1/0/all/0/1\">Vivswan Shitole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuxin_L/0/1/0/all/0/1\">Li Fuxin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahng_M/0/1/0/all/0/1\">Minsuk Kahng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadepalli_P/0/1/0/all/0/1\">Prasad Tadepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1\">Alan Fern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large-Scale Database for Graph Representation Learning. (arXiv:2011.07682v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.07682","description":"<p>With the rapid emergence of graph representation learning, the construction\nof new large-scale datasets is necessary to distinguish model capabilities and\naccurately assess the strengths and weaknesses of each technique. By carefully\nanalyzing existing graph databases, we identify 3 critical components important\nfor advancing the field of graph representation learning: (1) large graphs, (2)\nmany graphs, and (3) class diversity. To date, no single graph database offers\nall these desired properties. We introduce MalNet, the largest public graph\ndatabase ever constructed, representing a large-scale ontology of malicious\nsoftware function call graphs. MalNet contains over 1.2 million graphs,\naveraging over 15k nodes and 35k edges per graph, across a hierarchy of 47\ntypes and 696 families. Compared to the popular REDDIT-12K database, MalNet\noffers 105x more graphs, 39x larger graphs on average, and 63x more classes. We\nprovide a detailed analysis of MalNet, discussing its properties and\nprovenance, along with the evaluation of state-of-the-art machine learning and\ngraph neural network techniques. The unprecedented scale and diversity of\nMalNet offers exciting opportunities to advance the frontiers of graph\nrepresentation learning--enabling new discoveries and research into imbalanced\nclassification, explainability and the impact of class hardness. The database\nis publicly available at www.mal-net.org.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Freitas_S/0/1/0/all/0/1\">Scott Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neil_J/0/1/0/all/0/1\">Joshua Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ax-BxP: Approximate Blocked Computation for Precision-Reconfigurable Deep Neural Network Acceleration. (arXiv:2011.13000v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.13000","description":"<p>Precision scaling has emerged as a popular technique to optimize the compute\nand storage requirements of Deep Neural Networks (DNNs). Efforts toward\ncreating ultra-low-precision (sub-8-bit) DNNs suggest that the minimum\nprecision required to achieve a given network-level accuracy varies\nconsiderably across networks, and even across layers within a network,\nrequiring support for variable precision in DNN hardware. Previous proposals\nsuch as bit-serial hardware incur high overheads, significantly diminishing the\nbenefits of lower precision. To efficiently support precision\nre-configurability in DNN accelerators, we introduce an approximate computing\nmethod wherein DNN computations are performed block-wise (a block is a group of\nbits) and re-configurability is supported at the granularity of blocks. Results\nof block-wise computations are composed in an approximate manner to enable\nefficient re-configurability. We design a DNN accelerator that embodies\napproximate blocked computation and propose a method to determine a suitable\napproximation configuration for a given DNN. By varying the approximation\nconfigurations across DNNs, we achieve 1.17x-1.73x and 1.02x-2.04x improvement\nin system energy and performance respectively, over an 8-bit fixed-point (FxP8)\nbaseline, with negligible loss in classification accuracy. Further, by varying\nthe approximation configurations across layers and data-structures within DNNs,\nwe achieve 1.25x-2.42x and 1.07x-2.95x improvement in system energy and\nperformance respectively, with negligible accuracy loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elangovan_R/0/1/0/all/0/1\">Reena Elangovan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shubham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Anand Raghunathan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-based Motion Segmentation with Spatio-Temporal Graph Cuts. (arXiv:2012.08730v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.08730","description":"<p>Identifying independently moving objects is an essential task for dynamic\nscene understanding. However, traditional cameras used in dynamic scenes may\nsuffer from motion blur or exposure artifacts due to their sampling principle.\nBy contrast, event-based cameras are novel bio-inspired sensors that offer\nadvantages to overcome such limitations. They report pixelwise intensity\nchanges asynchronously, which enables them to acquire visual information at\nexactly the same rate as the scene dynamics. We develop a method to identify\nindependently moving objects acquired with an event-based camera, i.e., to\nsolve the event-based motion segmentation problem. We cast the problem as an\nenergy minimization one involving the fitting of multiple motion models. We\njointly solve two subproblems, namely event cluster assignment (labeling) and\nmotion model fitting, in an iterative manner by exploiting the structure of the\ninput event data in the form of a spatio-temporal graph. Experiments on\navailable datasets demonstrate the versatility of the method in scenes with\ndifferent motion patterns and number of moving objects. The evaluation shows\nstate-of-the-art results without having to predetermine the number of expected\nmoving objects. We release the software and dataset under an open source\nlicence to foster research in the emerging topic of event-based motion\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Guillermo Gallego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiuyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shaojie Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Object Association and Pose Updating for Semantic SLAM. (arXiv:2012.11368v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.11368","description":"<p>Nowadays in the field of semantic SLAM, how to correctly use semantic\ninformation for data association is still a problem worthy of study. The key to\nsolving this problem is to correctly associate multiple object measurements of\none object landmark, and refine the pose of object landmark. However, different\nobjects locating closely are prone to be associated as one object landmark, and\nit is difficult to pick up a best pose from multiple object measurements\nassociated with one object landmark. To tackle these problems, we propose a\nhierarchical object association strategy by means of multiple object tracking,\nthrough which closing objects will be correctly associated to different object\nlandmarks, and an approach to refine the pose of object landmark from multiple\nobject measurements. The proposed method is evaluated on a simulated sequence\nand several sequences in the Kitti dataset. Experimental results show a very\nimpressive improvement with respect to the traditional SLAM and the\nstate-of-the-art semantic SLAM method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaiqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OBoW: Online Bag-of-Visual-Words Generation for Self-Supervised Learning. (arXiv:2012.11552v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.11552","description":"<p>Learning image representations without human supervision is an important and\nactive research field. Several recent approaches have successfully leveraged\nthe idea of making such a representation invariant under different types of\nperturbations, especially via contrastive-based instance discrimination\ntraining. Although effective visual representations should indeed exhibit such\ninvariances, there are other important characteristics, such as encoding\ncontextual reasoning skills, for which alternative reconstruction-based\napproaches might be better suited.\n</p>\n<p>With this in mind, we propose a teacher-student scheme to learn\nrepresentations by training a convolutional net to reconstruct a\nbag-of-visual-words (BoW) representation of an image, given as input a\nperturbed version of that same image. Our strategy performs an online training\nof both the teacher network (whose role is to generate the BoW targets) and the\nstudent network (whose role is to learn representations), along with an online\nupdate of the visual-words vocabulary (used for the BoW targets). This idea\neffectively enables fully online BoW-guided unsupervised learning. Extensive\nexperiments demonstrate the interest of our BoW-based strategy which surpasses\nprevious state-of-the-art methods (including contrastive-based ones) in several\napplications. For instance, in downstream tasks such Pascal object detection,\nPascal classification and Places205 classification, our method improves over\nall prior unsupervised approaches, thus establishing new state-of-the-art\nresults that are also significantly better even than those of supervised\npre-training. We provide the implementation code at\nhttps://github.com/valeoai/obow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gidaris_S/0/1/0/all/0/1\">Spyros Gidaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1\">Andrei Bursuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1\">Gilles Puy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komodakis_N/0/1/0/all/0/1\">Nikos Komodakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape, Appearance, and Pose. (arXiv:2102.06199v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.06199","description":"<p>While deep learning reshaped the classical motion capture pipeline with\nfeed-forward networks, generative models are required to recover fine alignment\nvia iterative refinement. Unfortunately, the existing models are usually\nhand-crafted or learned in controlled conditions, only applicable to limited\ndomains. We propose a method to learn a generative neural body model from\nunlabelled monocular videos by extending Neural Radiance Fields (NeRFs). We\nequip them with a skeleton to apply to time-varying and articulated motion. A\nkey insight is that implicit models require the inverse of the forward\nkinematics used in explicit surface models. Our reparameterization defines\nspatial latent variables relative to the pose of body parts and thereby\novercomes ill-posed inverse operations with an overparameterization. This\nenables learning volumetric body shape and appearance from scratch while\njointly refining the articulated pose; all without ground truth labels for\nappearance, pose, or 3D shape on the input videos. When used for\nnovel-view-synthesis and motion capture, our neural model improves accuracy on\ndiverse datasets. Project website: https://lemonatsu.github.io/anerf/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shih-Yang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Frank Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hard-Attention for Scalable Image Classification. (arXiv:2102.10212v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.10212","description":"<p>Can we leverage high-resolution information without the unsustainable\nquadratic complexity to input scale? We propose Traversal Network (TNet), a\nnovel multi-scale hard-attention architecture, which traverses image\nscale-space in a top-down fashion, visiting only the most informative image\nregions along the way. TNet offers an adjustable trade-off between accuracy and\ncomplexity, by changing the number of attended image locations. We compare our\nmodel against hard-attention baselines on ImageNet, achieving higher accuracy\nwith less resources (FLOPs, processing time and memory). We further test our\nmodel on fMoW dataset, where we process satellite images of size up to $896\n\\times 896$ px, getting up to $2.5$x faster processing compared to baselines\noperating on the same resolution, while achieving higher accuracy as well. TNet\nis modular, meaning that most classification models could be adopted as its\nbackbone for feature extraction, making the reported performance gains\northogonal to benefits offered by existing optimized deep models. Finally,\nhard-attention guarantees a degree of interpretability to our model's\npredictions, without any extra cost beyond inference. Code is available at\n$\\href{https://github.com/Tpap/TNet}{github.com/Tpap/TNet}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_A/0/1/0/all/0/1\">Athanasios Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korus_P/0/1/0/all/0/1\">Pawe&#x142; Korus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Memon_N/0/1/0/all/0/1\">Nasir Memon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Self-supervised Neural Architecture Search. (arXiv:2102.10557v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.10557","description":"<p>This paper proposes a novel cell-based neural architecture search algorithm\n(NAS), which completely alleviates the expensive costs of data labeling\ninherited from supervised learning. Our algorithm capitalizes on the\neffectiveness of self-supervised learning for image representations, which is\nan increasingly crucial topic of computer vision. First, using only a small\namount of unlabeled train data under contrastive self-supervised learning allow\nus to search on a more extensive search space, discovering better neural\narchitectures without surging the computational resources. Second, we entirely\nrelieve the cost for labeled data (by contrastive loss) in the search stage\nwithout compromising architectures' final performance in the evaluation phase.\nFinally, we tackle the inherent discrete search space of the NAS problem by\nsequential model-based optimization via the tree-parzen estimator (SMBO-TPE),\nenabling us to reduce the computational expense response surface significantly.\nAn extensive number of experiments empirically show that our search algorithm\ncan achieve state-of-the-art results with better efficiency in data labeling\ncost, searching time, and accuracy in final validation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">J. Morris Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cycle Self-Training for Domain Adaptation. (arXiv:2103.03571v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.03571","description":"<p>Mainstream approaches for unsupervised domain adaptation (UDA) learn\ndomain-invariant representations to narrow the domain shift. Recently,\nself-training has been gaining momentum in UDA, which exploits unlabeled target\ndata by training with target pseudo-labels. However, as corroborated in this\nwork, under distributional shift in UDA, the pseudo-labels can be unreliable in\nterms of their large discrepancy from target ground truth. Thereby, we propose\nCycle Self-Training (CST), a principled self-training algorithm that explicitly\nenforces pseudo-labels to generalize across domains. CST cycles between a\nforward step and a reverse step until convergence. In the forward step, CST\ngenerates target pseudo-labels with a source-trained classifier. In the reverse\nstep, CST trains a target classifier using target pseudo-labels, and then\nupdates the shared representations to make the target classifier perform well\non the source data. We introduce the Tsallis entropy as a confidence-friendly\nregularization to improve the quality of target pseudo-labels. We analyze CST\ntheoretically under realistic assumptions, and provide hard cases where CST\nrecovers target ground truth, while both invariant feature learning and vanilla\nself-training fail. Empirical results indicate that CST significantly improves\nover the state-of-the-arts on visual recognition and sentiment analysis\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Morphological and Histogram based Text Line Segmentation in the OCR Context. (arXiv:2103.08922v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08922","description":"<p>Text line segmentation is one of the pre-stages of modern optical character\nrecognition systems. The algorithmic approach proposed by this paper has been\ndesigned for this exact purpose. Its main characteristic is the combination of\ntwo different techniques, morphological image operations and horizontal\nhistogram projections. The method was developed to be applied on a historic\ndata collection that commonly features quality issues, such as degraded paper,\nblurred text, or presence of noise. For that reason, the segmenter in question\ncould be of particular interest for cultural institutions, that want access to\nrobust line bounding boxes for a given historic document. Because of the\npromising segmentation results that are joined by low computational cost, the\nalgorithm was incorporated into the OCR pipeline of the National Library of\nLuxembourg, in the context of the initiative of reprocessing their historic\nnewspaper collection. The general contribution of this paper is to outline the\napproach and to evaluate the gains in terms of accuracy and speed, comparing it\nto the segmentation algorithm bundled with the used open source OCR software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pit Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuctionNet-1Billion: A Large-Scale Benchmark for Suction Grasping. (arXiv:2103.12311v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.12311","description":"<p>Suction is an important solution for the longstanding robotic grasping\nproblem. Compared with other kinds of grasping, suction grasping is easier to\nrepresent and often more reliable in practice. Though preferred in many\nscenarios, it is not fully investigated and lacks sufficient training data and\nevaluation benchmarks. To address that, firstly, we propose a new physical\nmodel to analytically evaluate seal formation and wrench resistance of a\nsuction grasping, which are two key aspects of grasp success. Secondly, a\ntwo-step methodology is adopted to generate annotations on a large-scale\ndataset collected in real-world cluttered scenarios. Thirdly, a standard online\nevaluation system is proposed to evaluate suction poses in continuous operation\nspace, which can benchmark different algorithms fairly without the need of\nexhaustive labeling. Real-robot experiments are conducted to show that our\nannotations align well with real world. Meanwhile, we propose a method to\npredict numerous suction poses from an RGB-D image of a cluttered scene and\ndemonstrate our superiority against several previous methods. Result analyses\nare further provided to help readers better understand the challenges in this\narea. Data and source code are publicly available at www.graspnet.net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hanwen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Hao-Shu Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DA4Event: towards bridging the Sim-to-Real Gap for Event Cameras using Domain Adaptation. (arXiv:2103.12768v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12768","description":"<p>Event cameras are novel bio-inspired sensors, which asynchronously capture\npixel-level intensity changes in the form of \"events\". The innovative way they\nacquire data presents several advantages over standard devices, especially in\npoor lighting and high-speed motion conditions. However, the novelty of these\nsensors results in the lack of a large amount of training data capable of fully\nunlocking their potential. The most common approach implemented by researchers\nto address this issue is to leverage simulated event data. Yet, this approach\ncomes with an open research question: how well simulated data generalize to\nreal data? To answer this, we propose to exploit, in the event-based context,\nrecent Domain Adaptation (DA) advances in traditional computer vision, showing\nthat DA techniques applied to event data help reduce the sim-to-real gap. To\nthis purpose, we propose a novel architecture, which we call Multi-View DA4E\n(MV-DA4E), that better exploits the peculiarities of frame-based event\nrepresentations while also promoting domain invariant characteristics in\nfeatures. Through extensive experiments, we prove the effectiveness of DA\nmethods and MV-DA4E on N-Caltech101. Moreover, we validate their soundness in a\nreal-world scenario through a cross-domain analysis on the popular RGB-D Object\nDataset (ROD), which we extended to the event modality (RGB-E).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Planamente_M/0/1/0/all/0/1\">Mirco Planamente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plizzari_C/0/1/0/all/0/1\">Chiara Plizzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cannici_M/0/1/0/all/0/1\">Marco Cannici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1\">Marco Ciccone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strada_F/0/1/0/all/0/1\">Francesco Strada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bottino_A/0/1/0/all/0/1\">Andrea Bottino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1\">Matteo Matteucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Knowledge Expansion. (arXiv:2103.14431v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14431","description":"<p>The popularity of multimodal sensors and the accessibility of the Internet\nhave brought us a massive amount of unlabeled multimodal data. Since existing\ndatasets and well-trained models are primarily unimodal, the modality gap\nbetween a unimodal network and unlabeled multimodal data poses an interesting\nproblem: how to transfer a pre-trained unimodal network to perform the same\ntask on unlabeled multimodal data? In this work, we propose multimodal\nknowledge expansion (MKE), a knowledge distillation-based framework to\neffectively utilize multimodal data without requiring labels. Opposite to\ntraditional knowledge distillation, where the student is designed to be\nlightweight and inferior to the teacher, we observe that a multimodal student\nmodel consistently denoises pseudo labels and generalizes better than its\nteacher. Extensive experiments on four tasks and different modalities verify\nthis finding. Furthermore, we connect the mechanism of MKE to semi-supervised\nlearning and offer both empirical and theoretical explanations to understand\nthe denoising capability of a multimodal student.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zihui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhengqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Fidelity End-to-End Video Encoder Pre-training for Temporal Action Localization. (arXiv:2103.15233v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15233","description":"<p>Temporal action localization (TAL) is a fundamental yet challenging task in\nvideo understanding. Existing TAL methods rely on pre-training a video encoder\nthrough action classification supervision. This results in a task discrepancy\nproblem for the video encoder -- trained for action classification, but used\nfor TAL. Intuitively, end-to-end model optimization is a good solution.\nHowever, this is not operable for TAL subject to the GPU memory constraints,\ndue to the prohibitive computational cost in processing long untrimmed videos.\nIn this paper, we resolve this challenge by introducing a novel low-fidelity\nend-to-end (LoFi) video encoder pre-training method. Instead of always using\nthe full training configurations for TAL learning, we propose to reduce the\nmini-batch composition in terms of temporal, spatial or spatio-temporal\nresolution so that end-to-end optimization for the video encoder becomes\noperable under the memory conditions of a mid-range hardware budget. Crucially,\nthis enables the gradient to flow backward through the video encoder from a TAL\nloss supervision, favourably solving the task discrepancy problem and providing\nmore effective feature representations. Extensive experiments show that the\nproposed LoFi pre-training approach can significantly enhance the performance\nof existing TAL methods. Encouragingly, even with a lightweight ResNet18 based\nvideo encoder in a single RGB stream, our method surpasses two-stream ResNet50\nbased alternatives with expensive optical flow, often by a good margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Rua_J/0/1/0/all/0/1\">Juan-Manuel Perez-Rua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1\">Brais Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-based Thermal-Inertial SLAM with Probabilistic Neural Networks. (arXiv:2104.07196v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07196","description":"<p>Simultaneous Localization and Mapping (SLAM) system typically employ\nvision-based sensors to observe the surrounding environment. However, the\nperformance of such systems highly depends on the ambient illumination\nconditions. In scenarios with adverse visibility or in the presence of airborne\nparticulates (e.g. smoke, dust, etc.), alternative modalities such as those\nbased on thermal imaging and inertial sensors are more promising. In this\npaper, we propose the first complete thermal-inertial SLAM system which\ncombines neural abstraction in the SLAM front end with robust pose graph\noptimization in the SLAM back end. We model the sensor abstraction in the front\nend by employing probabilistic deep learning parameterized by Mixture Density\nNetworks (MDN). Our key strategies to successfully model this encoding from\nthermal imagery are the usage of normalized 14-bit radiometric data, the\nincorporation of hallucinated visual (RGB) features, and the inclusion of\nfeature selection to estimate the MDN parameters. To enable a full SLAM system,\nwe also design an efficient global image descriptor which is able to detect\nloop closures from thermal embedding vectors. We performed extensive\nexperiments and analysis using three datasets, namely self-collected ground\nrobot and handheld data taken in indoor environment, and one public dataset\n(SubT-tunnel) collected in underground tunnel. Finally, we demonstrate that an\naccurate thermal-inertial SLAM system can be realized in conditions of both\nbenign and adverse visibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saputra_M/0/1/0/all/0/1\">Muhamad Risqi U. Saputra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gusmao_P/0/1/0/all/0/1\">Pedro P. B. de Gusmao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-paced Resistance Learning against Overfitting on Noisy Labels. (arXiv:2105.03059v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03059","description":"<p>Noisy labels composed of correct and corrupted ones are pervasive in\npractice. They might significantly deteriorate the performance of convolutional\nneural networks (CNNs), because CNNs are easily overfitted on corrupted labels.\nTo address this issue, inspired by an observation, deep neural networks might\nfirst memorize the probably correct-label data and then corrupt-label samples,\nwe propose a novel yet simple self-paced resistance framework to resist\ncorrupted labels, without using any clean validation data. The proposed\nframework first utilizes the memorization effect of CNNs to learn a curriculum,\nwhich contains confident samples and provides meaningful supervision for other\ntraining samples. Then it adopts selected confident samples and a proposed\nresistance loss to update model parameters; the resistance loss tends to smooth\nmodel parameters' update or attain equivalent prediction over each class,\nthereby resisting model overfitting on corrupted labels. Finally, we unify\nthese two modules into a single loss function and optimize it in an alternative\nlearning. Extensive experiments demonstrate the significantly superior\nperformance of the proposed framework over recent state-of-the-art methods on\nnoisy-label data. Source codes of the proposed method are available on\nhttps://github.com/xsshi2015/Self-paced-Resistance-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaoshuang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaofeng Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Jensen-Shannon Divergence Loss for Learning with Noisy Labels. (arXiv:2105.04522v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.04522","description":"<p>Prior works have found it beneficial to combine provably noise-robust loss\nfunctions e.g., mean absolute error (MAE) with standard categorical loss\nfunction e.g. cross entropy (CE) to improve their learnability. Here, we\npropose to use Jensen-Shannon divergence as a noise-robust loss function and\nshow that it interestingly interpolate between CE and MAE with a controllable\nmixing parameter. Furthermore, we make a crucial observation that CE exhibit\nlower consistency around noisy data points. Based on this observation, we adopt\na generalized version of the Jensen-Shannon divergence for multiple\ndistributions to encourage consistency around data points. Using this loss\nfunction, we show state-of-the-art results on both synthetic (CIFAR), and\nreal-world (e.g., WebVision) noise with varying noise rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Englesson_E/0/1/0/all/0/1\">Erik Englesson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1\">Hossein Azizpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neighborhood-Aware Neural Architecture Search. (arXiv:2105.06369v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.06369","description":"<p>Existing neural architecture search (NAS) methods often return an\narchitecture with good search performance but generalizes poorly to the test\nsetting. To achieve better generalization, we propose a novel\nneighborhood-aware NAS formulation to identify flat-minima architectures in the\nsearch space, with the assumption that flat minima generalize better than sharp\nminima. The phrase ``flat-minima architecture'' refers to architectures whose\nperformance is stable under small perturbations in the architecture (e.g.,\nreplacing a convolution with a skip connection). Our formulation takes the\n``flatness'' of an architecture into account by aggregating the performance\nover the neighborhood of this architecture. We demonstrate a principled way to\napply our formulation to existing search algorithms, including sampling-based\nalgorithms and gradient-based algorithms. To facilitate the application to\ngradient-based algorithms, we also propose a differentiable representation for\nthe neighborhood of architectures. Based on our formulation, we propose\nneighborhood-aware random search (NA-RS) and neighborhood-aware differentiable\narchitecture search (NA-DARTS). Notably, by simply augmenting DARTS with our\nformulation, NA-DARTS outperforms DARTS and achieves state-of-the-art\nperformance on established benchmarks, including CIFAR-10, CIFAR-100 and\nImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shengcao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengtian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DOCTOR: A Simple Method for Detecting Misclassification Errors. (arXiv:2106.02395v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02395","description":"<p>Deep neural networks (DNNs) have shown to perform very well on large scale\nobject recognition problems and lead to widespread use for real-world\napplications, including situations where DNN are implemented as \"black boxes\".\nA promising approach to secure their use is to accept decisions that are likely\nto be correct while discarding the others. In this work, we propose DOCTOR, a\nsimple method that aims to identify whether the prediction of a DNN classifier\nshould (or should not) be trusted so that, consequently, it would be possible\nto accept it or to reject it. Two scenarios are investigated: Totally Black Box\n(TBB) where only the soft-predictions are available and Partially Black Box\n(PBB) where gradient-propagation to perform input pre-processing is allowed.\nEmpirically, we show that DOCTOR outperforms all state-of-the-art methods on\nvarious well-known images and sentiment analysis datasets. In particular, we\nobserve a reduction of up to $4\\%$ of the false rejection rate (FRR) in the PBB\nscenario. DOCTOR can be applied to any pre-trained model, it does not require\nprior information about the underlying dataset and is as simple as the simplest\navailable methods in the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Granese_F/0/1/0/all/0/1\">Federica Granese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanelli_M/0/1/0/all/0/1\">Marco Romanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorla_D/0/1/0/all/0/1\">Daniele Gorla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1\">Catuscia Palamidessi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manifold Topology Divergence: a Framework for Comparing Data Manifolds. (arXiv:2106.04024v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.04024","description":"<p>We develop a framework for comparing data manifolds, aimed, in particular,\ntowards the evaluation of deep generative models. We describe a novel tool,\nCross-Barcode(P,Q), that, given a pair of distributions in a high-dimensional\nspace, tracks multiscale topology spacial discrepancies between manifolds on\nwhich the distributions are concentrated. Based on the Cross-Barcode, we\nintroduce the Manifold Topology Divergence score (MTop-Divergence) and apply it\nto assess the performance of deep generative models in various domains: images,\n3D-shapes, time-series, and on different datasets: MNIST, Fashion MNIST, SVHN,\nCIFAR10, FFHQ, chest X-ray images, market stock data, ShapeNet. We demonstrate\nthat the MTop-Divergence accurately detects various degrees of mode-dropping,\nintra-mode collapse, mode invention, and image disturbance. Our algorithm\nscales well (essentially linearly) with the increase of the dimension of the\nambient high-dimensional space. It is one of the first TDA-based practical\nmethodologies that can be applied universally to datasets of different sizes\nand dimensions, including the ones on which the most recent GANs in the visual\ndomain are trained. The proposed method is domain agnostic and does not rely on\npre-trained networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1\">Ilya Trofimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotnikov_G/0/1/0/all/0/1\">Grigorii Sotnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trimbach_E/0/1/0/all/0/1\">Ekaterina Trimbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1\">Alexander Korotin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filippov_A/0/1/0/all/0/1\">Alexander Filippov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style. (arXiv:2106.04619v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2106.04619","description":"<p>Self-supervised representation learning has shown remarkable success in a\nnumber of domains. A common practice is to perform data augmentation via\nhand-crafted transformations intended to leave the semantics of the data\ninvariant. We seek to understand the empirical success of this approach from a\ntheoretical perspective. We formulate the augmentation process as a latent\nvariable model by postulating a partition of the latent representation into a\ncontent component, which is assumed invariant to augmentation, and a style\ncomponent, which is allowed to change. Unlike prior work on disentanglement and\nindependent component analysis, we allow for both nontrivial statistical and\ncausal dependencies in the latent space. We study the identifiability of the\nlatent representation based on pairs of views of the observations and prove\nsufficient conditions that allow us to identify the invariant content partition\nup to an invertible mapping in both generative and discriminative settings. We\nfind numerical simulations with dependent latent variables are consistent with\nour theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional,\nvisually complex images with rich causal dependencies, which we use to study\nthe effect of data augmentations performed in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gresele_L/0/1/0/all/0/1\">Luigi Gresele</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Besserve_M/0/1/0/all/0/1\">Michel Besserve</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Potato Crop Stress Identification in Aerial Images using Deep Learning-based Object Detection. (arXiv:2106.07770v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07770","description":"<p>Recent research on the application of remote sensing and deep learning-based\nanalysis in precision agriculture demonstrated a potential for improved crop\nmanagement and reduced environmental impacts of agricultural production.\nDespite the promising results, the practical relevance of these technologies\nfor field deployment requires novel algorithms that are customized for analysis\nof agricultural images and robust to implementation on natural field imagery.\nThe paper presents an approach for analyzing aerial images of a potato (Solanum\ntuberosum L.) crop using deep neural networks. The main objective is to\ndemonstrate automated spatial recognition of healthy vs. stressed crop at a\nplant level. Specifically, we examine premature plant senescence resulting in\ndrought stress on Russet Burbank potato plants. We propose a novel deep\nlearning (DL) model for detecting crop stress, named Retina-UNet-Ag. The\nproposed architecture is a variant of Retina-UNet and includes connections from\nlow-level semantic representation maps to the feature pyramid network. The\npaper also introduces a dataset of aerial field images acquired with a Parrot\nSequoia camera. The dataset includes manually annotated bounding boxes of\nhealthy and stressed plant regions. Experimental validation demonstrated the\nability for distinguishing healthy and stressed plants in field images,\nachieving an average dice score coefficient (DSC) of 0.74. A comparison to\nrelated state-of-the-art DL models for object detection revealed that the\npresented approach is effective for this task. The proposed method is conducive\ntoward the assessment and recognition of potato crop stress in aerial field\nimages collected under natural conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Butte_S/0/1/0/all/0/1\">Sujata Butte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakanski_A/0/1/0/all/0/1\">Aleksandar Vakanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duellman_K/0/1/0/all/0/1\">Kasia Duellman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirkouei_A/0/1/0/all/0/1\">Amin Mirkouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-Time Personalization with a Transformer for Human Pose Estimation. (arXiv:2107.02133v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02133","description":"<p>We propose to personalize a human pose estimator given a set of test images\nof a person without using any manual annotations. While there is a significant\nadvancement in human pose estimation, it is still very challenging for a model\nto generalize to different unknown environments and unseen persons. Instead of\nusing a fixed model for every test case, we adapt our pose estimator during\ntest time to exploit person-specific information. We first train our model on\ndiverse data with both a supervised and a self-supervised pose estimation\nobjectives jointly. We use a Transformer model to build a transformation\nbetween the self-supervised keypoints and the supervised keypoints. During test\ntime, we personalize and adapt our model by fine-tuning with the\nself-supervised objective. The pose is then improved by transforming the\nupdated self-supervised keypoints. We experiment with multiple datasets and\nshow significant improvements on pose estimations with our self-supervised\npersonalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_M/0/1/0/all/0/1\">Miao Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_Z/0/1/0/all/0/1\">Zonglin Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gundavarapu_N/0/1/0/all/0/1\">Nitesh B. Gundavarapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Short-Term Transformer for Online Action Detection. (arXiv:2107.03377v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03377","description":"<p>We present Long Short-term TRansformer (LSTR), a temporal modeling algorithm\nfor online action detection, which employs a long- and short-term memory\nmechanism to model prolonged sequence data. It consists of an LSTR encoder that\ndynamically leverages coarse-scale historical information from an extended\ntemporal window (e.g., 2048 frames spanning of up to 8 minutes), together with\nan LSTR decoder that focuses on a short time window (e.g., 32 frames spanning 8\nseconds) to model the fine-scale characteristics of the data. Compared to prior\nwork, LSTR provides an effective and efficient method to model long videos with\nfewer heuristics, which is validated by extensive empirical analysis. LSTR\nachieves state-of-the-art performance on three standard online action detection\nbenchmarks, THUMOS'14, TVSeries, and HACS Segment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingze Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Layers Susceptible to Adversarial Attacks. (arXiv:2107.04827v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.04827","description":"<p>In this paper, we investigate the use of pretraining with adversarial\nnetworks, with the objective of discovering the relationship between network\ndepth and robustness. For this purpose, we selectively retrain different\nportions of VGG and ResNet architectures on CIFAR-10, Imagenette, and ImageNet\nusing non-adversarial and adversarial data. Experimental results show that\nsusceptibility to adversarial samples is associated with low-level feature\nextraction layers. Therefore, retraining of high-level layers is insufficient\nfor achieving robustness. Furthermore, adversarial attacks yield outputs from\nearly layers that differ statistically from features for non-adversarial\nsamples and do not permit consistent classification by subsequent layers. This\nsupports common hypotheses regarding the association of robustness with the\nfeature extractor, insufficiency of deeper layers in providing robustness, and\nlarge differences in adversarial and non-adversarial feature vectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1\">Shoaib Ahmed Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1\">Thomas Breuel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Multi-modal Video Temporal Grounding. (arXiv:2107.05624v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05624","description":"<p>We address the problem of text-guided video temporal grounding, which aims to\nidentify the time interval of a certain event based on a natural language\ndescription. Different from most existing methods that only consider RGB images\nas visual features, we propose a multi-modal framework to extract complementary\ninformation from videos. Specifically, we adopt RGB images for appearance,\noptical flow for motion, and depth maps for image structure. While RGB images\nprovide abundant visual cues of certain events, the performance may be affected\nby background clutters. Therefore, we use optical flow to focus on large motion\nand depth maps to infer the scene configuration when the action is related to\nobjects recognizable with their shapes. To integrate the three modalities more\neffectively and enable inter-modal learning, we design a dynamic fusion scheme\nwith transformers to model the interactions between modalities. Furthermore, we\napply intra-modal self-supervised learning to enhance feature representations\nacross videos for each modality, which also facilitates multi-modal learning.\nWe conduct extensive experiments on the Charades-STA and ActivityNet Captions\ndatasets, and show that the proposed method performs favorably against\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Hsuan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09151","description":"<p>Describing images using natural language is widely known as image captioning,\nwhich has made consistent progress due to the development of computer vision\nand natural language generation techniques. Though conventional captioning\nmodels achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and\nSPICE, the ability of captions to distinguish the target image from other\nsimilar images is under-explored. To generate distinctive captions, a few\npioneers employ contrastive learning or re-weighted the ground-truth captions,\nwhich focuses on one single input image. However, the relationships between\nobjects in a similar image group (e.g., items or properties within the same\nalbum or fine-grained events) are neglected. In this paper, we improve the\ndistinctiveness of image captions using a Group-based Distinctive Captioning\nModel (GdisCap), which compares each image with other images in one similar\ngroup and highlights the uniqueness of each image. In particular, we propose a\ngroup-based memory attention (GMA) module, which stores object features that\nare unique among the image group (i.e., with low similarity to objects in other\nimages). These unique object features are highlighted when generating captions,\nresulting in more distinctive captions. Furthermore, the distinctive words in\nthe ground-truth captions are selected to supervise the language decoder and\nGMA. Finally, we propose a new evaluation metric, distinctive word rate\n(DisWordRate) to measure the distinctiveness of captions. Quantitative results\nindicate that the proposed method significantly improves the distinctiveness of\nseveral baseline models, and achieves the state-of-the-art performance on both\naccuracy and distinctiveness. Results of a user study agree with the\nquantitative evaluation and demonstrate the rationality of the new metric\nDisWordRate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiuniu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifted Chunk Transformer for Spatio-Temporal Representational Learning. (arXiv:2108.11575v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11575","description":"<p>Spatio-temporal representational learning has been widely adopted in various\nfields such as action recognition, video object segmentation, and action\nanticipation. Previous spatio-temporal representational learning approaches\nprimarily employ ConvNets or sequential models,e.g., LSTM, to learn the\nintra-frame and inter-frame features. Recently, Transformer models have\nsuccessfully dominated the study of natural language processing (NLP), image\nclassification, etc. However, the pure-Transformer based spatio-temporal\nlearning can be prohibitively costly on memory and computation to extract\nfine-grained features from a tiny patch. To tackle the training difficulty and\nenhance the spatio-temporal learning, we construct a shifted chunk Transformer\nwith pure self-attention blocks. Leveraging the recent efficient Transformer\ndesign in NLP, this shifted chunk Transformer can learn hierarchical\nspatio-temporal features from a local tiny patch to a global video clip. Our\nshifted self-attention can also effectively model complicated inter-frame\nvariances. Furthermore, we build a clip encoder based on Transformer to model\nlong-term temporal dependencies. We conduct thorough ablation studies to\nvalidate each component and hyper-parameters in our shifted chunk Transformer,\nand it outperforms previous state-of-the-art approaches on Kinetics-400,\nKinetics-600, UCF101, and HMDB51.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_X/0/1/0/all/0/1\">Xuefan Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tingxun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAENet: A Progressive Attention-Enhanced Network for 3D to 2D Retinal Vessel Segmentation. (arXiv:2108.11695v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.11695","description":"<p>3D to 2D retinal vessel segmentation is a challenging problem in Optical\nCoherence Tomography Angiography (OCTA) images. Accurate retinal vessel\nsegmentation is important for the diagnosis and prevention of ophthalmic\ndiseases. However, making full use of the 3D data of OCTA volumes is a vital\nfactor for obtaining satisfactory segmentation results. In this paper, we\npropose a Progressive Attention-Enhanced Network (PAENet) based on attention\nmechanisms to extract rich feature representation. Specifically, the framework\nconsists of two main parts, the three-dimensional feature learning path and the\ntwo-dimensional segmentation path. In the three-dimensional feature learning\npath, we design a novel Adaptive Pooling Module (APM) and propose a new\nQuadruple Attention Module (QAM). The APM captures dependencies along the\nprojection direction of volumes and learns a series of pooling coefficients for\nfeature fusion, which efficiently reduces feature dimension. In addition, the\nQAM reweights the features by capturing four-group cross-dimension\ndependencies, which makes maximum use of 4D feature tensors. In the\ntwo-dimensional segmentation path, to acquire more detailed information, we\npropose a Feature Fusion Module (FFM) to inject 3D information into the 2D\npath. Meanwhile, we adopt the Polarized Self-Attention (PSA) block to model the\nsemantic interdependencies in spatial and channel dimensions respectively.\nExperimentally, our extensive experiments on the OCTA-500 dataset show that our\nproposed algorithm achieves state-of-the-art performance compared with previous\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1\">Zhuojie Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognition Awareness: An Application of Latent Cognizance to Open-Set Recognition. (arXiv:2108.12115v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12115","description":"<p>This study investigates an application of a new probabilistic interpretation\nof a softmax output to Open-Set Recognition (OSR). Softmax is a mechanism\nwildly used in classification and object recognition.\n</p>\n<p>However, a softmax mechanism forces a model to operate under a closed-set\nparadigm, i.e., to predict an object class out of a set of pre-defined labels.\n</p>\n<p>This characteristic contributes to efficacy in classification, but poses a\nrisk of non-sense prediction in object recognition.\n</p>\n<p>Object recognition is often operated under a dynamic and diverse condition.\n</p>\n<p>A foreign object -- an object of any unprepared class -- can be encountered\nat any time.\n</p>\n<p>OSR is intended to address an issue of identifying a foreign object in object\nrecognition.\n</p>\n<p>Based on Bayes theorem and the emphasis of conditioning on the context,\nsoftmax inference has been re-interpreted.\n</p>\n<p>This re-interpretation has led to a new approach to OSR, called Latent\nCognizance (LC). Our investigation employs various scenarios, using Imagenet\n2012 dataset as well as fooling and open-set images. The findings support LC\nhypothesis and show its effectiveness on OSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katanyukul_T/0/1/0/all/0/1\">Tatpong Katanyukul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakjai_P/0/1/0/all/0/1\">Pisit Nakjai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR. (arXiv:2109.09628v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09628","description":"<p>Self-supervised monocular depth prediction provides a cost-effective solution\nto obtain the 3D location of each pixel. However, the existing approaches\nusually lead to unsatisfactory accuracy, which is critical for autonomous\nrobots. In this paper, we propose a novel two-stage network to advance the\nself-supervised monocular dense depth learning by leveraging low-cost sparse\n(e.g. 4-beam) LiDAR. Unlike the existing methods that use sparse LiDAR mainly\nin a manner of time-consuming iterative post-processing, our model fuses\nmonocular image features and sparse LiDAR features to predict initial depth\nmaps. Then, an efficient feed-forward refine network is further designed to\ncorrect the errors in these initial depth maps in pseudo-3D space with\nreal-time performance. Extensive experiments show that our proposed model\nsignificantly outperforms all the state-of-the-art self-supervised methods, as\nwell as the sparse-LiDAR-based methods on both self-supervised monocular depth\nprediction and completion tasks. With the accurate dense depth prediction, our\nmodel outperforms the state-of-the-art sparse-LiDAR-based method\n(Pseudo-LiDAR++) by more than 68% for the downstream task monocular 3D object\ndetection on the KITTI Leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Ziyue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Longlong Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Peng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingli Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Context-Aware Network for Abdominal Multi-organ Segmentation. (arXiv:2109.10601v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.10601","description":"<p>The contextual information, presented in abdominal CT scan, is relative\nconsistent. In order to make full use of the overall 3D context, we develop a\nwhole-volume-based coarse-to-fine framework for efficient and effective\nabdominal multi-organ segmentation. We propose a new efficientSegNet network,\nwhich is composed of basic encoder, slim decoder and efficient context block.\nFor the decoder module, anisotropic convolution with a k*k*1 intra-slice\nconvolution and a 1*1*k inter-slice convolution, is designed to reduce the\ncomputation burden. For the context block, we propose strip pooling module to\ncapture anisotropic and long-range contextual information, which exists in\nabdominal scene. Quantitative evaluation on the FLARE2021 validation cases,\nthis method achieves the average dice similarity coefficient (DSC) of 0.895 and\naverage normalized surface distance (NSD) of 0.775. This method won the 1st\nplace on the 2021-MICCAI-FLARE challenge. Codes and models are available at\nhttps://github.com/Shanghai-Aitrox-Technology/EfficientSegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Hua Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Network Pruning Through Constrained Reinforcement Learning. (arXiv:2110.08558v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08558","description":"<p>Network pruning reduces the size of neural networks by removing (pruning)\nneurons such that the performance drop is minimal. Traditional pruning\napproaches focus on designing metrics to quantify the usefulness of a neuron\nwhich is often quite tedious and sub-optimal. More recent approaches have\ninstead focused on training auxiliary networks to automatically learn how\nuseful each neuron is however, they often do not take computational limitations\ninto account. In this work, we propose a general methodology for pruning neural\nnetworks. Our proposed methodology can prune neural networks to respect\npre-defined computational budgets on arbitrary, possibly non-differentiable,\nfunctions. Furthermore, we only assume the ability to be able to evaluate these\nfunctions for different inputs, and hence they do not need to be fully\nspecified beforehand. We achieve this by proposing a novel pruning strategy via\nconstrained reinforcement learning algorithms. We prove the effectiveness of\nour approach via comparison with state-of-the-art methods on standard image\nclassification datasets. Specifically, we reduce 83-92.90 of total parameters\non various variants of VGG while achieving comparable or better performance\nthan that of original networks. We also achieved 75.09 reduction in parameters\non ResNet18 without incurring any loss in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malik_S/0/1/0/all/0/1\">Shehryar Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haider_M/0/1/0/all/0/1\">Muhammad Umair Haider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_O/0/1/0/all/0/1\">Omer Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taj_M/0/1/0/all/0/1\">Murtaza Taj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransFusion: Cross-view Fusion with Transformer for 3D Human Pose Estimation. (arXiv:2110.09554v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09554","description":"<p>Estimating the 2D human poses in each view is typically the first step in\ncalibrated multi-view 3D pose estimation. But the performance of 2D pose\ndetectors suffers from challenging situations such as occlusions and oblique\nviewing angles. To address these challenges, previous works derive\npoint-to-point correspondences between different views from epipolar geometry\nand utilize the correspondences to merge prediction heatmaps or feature\nrepresentations. Instead of post-prediction merge/calibration, here we\nintroduce a transformer framework for multi-view 3D pose estimation, aiming at\ndirectly improving individual 2D predictors by integrating information from\ndifferent views. Inspired by previous multi-modal transformers, we design a\nunified transformer architecture, named TransFusion, to fuse cues from both\ncurrent views and neighboring views. Moreover, we propose the concept of\nepipolar field to encode 3D positional information into the transformer model.\nThe 3D position encoding guided by the epipolar field provides an efficient way\nof encoding correspondences between pixels of different views. Experiments on\nHuman 3.6M and Ski-Pose show that our method is more efficient and has\nconsistent improvements compared to other fusion methods. Specifically, we\nachieve 25.8 mm MPJPE on Human 3.6M with only 5M parameters on 256 x 256\nresolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deying Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiangyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yusheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shih-Yao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohui Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOFT: Softmax-free Transformer with Linear Complexity. (arXiv:2110.11945v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11945","description":"<p>Vision transformers (ViTs) have pushed the state-of-the-art for various\nvisual recognition tasks by patch-wise image tokenization followed by\nself-attention. However, the employment of self-attention modules results in a\nquadratic complexity in both computation and memory usage. Various attempts on\napproximating the self-attention computation with linear complexity have been\nmade in Natural Language Processing. However, an in-depth analysis in this work\nshows that they are either theoretically flawed or empirically ineffective for\nvisual recognition. We further identify that their limitations are rooted in\nkeeping the softmax self-attention during approximations. Specifically,\nconventional self-attention is computed by normalizing the scaled dot-product\nbetween token feature vectors. Keeping this softmax operation challenges any\nsubsequent linearization efforts. Based on this insight, for the first time, a\nsoftmax-free transformer or SOFT is proposed. To remove softmax in\nself-attention, Gaussian kernel function is used to replace the dot-product\nsimilarity without further normalization. This enables a full self-attention\nmatrix to be approximated via a low-rank matrix decomposition. The robustness\nof the approximation is achieved by calculating its Moore-Penrose inverse using\na Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT\nsignificantly improves the computational efficiency of existing ViT variants.\nCrucially, with a linear complexity, much longer token sequences are permitted\nin SOFT, resulting in superior trade-off between accuracy and complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiachen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jinghan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Weiguo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MisConv: Convolutional Neural Networks for Missing Data. (arXiv:2110.14010v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.14010","description":"<p>Processing of missing data by modern neural networks, such as CNNs, remains a\nfundamental, yet unsolved challenge, which naturally arises in many practical\napplications, like image inpainting or autonomous vehicles and robots. While\nimputation-based techniques are still one of the most popular solutions, they\nfrequently introduce unreliable information to the data and do not take into\naccount the uncertainty of estimation, which may be destructive for a machine\nlearning model. In this paper, we present MisConv, a general mechanism, for\nadapting various CNN architectures to process incomplete images. By modeling\nthe distribution of missing values by the Mixture of Factor Analyzers, we cover\nthe spectrum of possible replacements and find an analytical formula for the\nexpected value of convolution operator applied to the incomplete image. The\nwhole framework is realized by matrix operations, which makes MisConv extremely\nefficient in practice. Experiments performed on various image processing tasks\ndemonstrate that MisConv achieves superior or comparable performance to the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Przewiezlikowski_M/0/1/0/all/0/1\">Marcin Przewi&#x119;&#x17a;likowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1\">Marek &#x15a;mieja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struski_L/0/1/0/all/0/1\">&#x141;ukasz Struski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1\">Jacek Tabor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand gesture detection in tests performed by older adults. (arXiv:2110.14461v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14461","description":"<p>Our team are developing a new online test that analyses hand movement\nfeatures associated with ageing that can be completed remotely from the\nresearch centre. To obtain hand movement features, participants will be asked\nto perform a variety of hand gestures using their own computer cameras.\nHowever, it is challenging to collect high quality hand movement video data,\nespecially for older participants, many of whom have no IT background. During\nthe data collection process, one of the key steps is to detect whether the\nparticipants are following the test instructions correctly and also to detect\nsimilar gestures from different devices. Furthermore, we need this process to\nbe automated and accurate as we expect many thousands of participants to\ncomplete the test. We have implemented a hand gesture detector to detect the\ngestures in the hand movement tests and our detection mAP is 0.782 which is\nbetter than the state-of-the-art. In this research, we have processed 20,000\nimages collected from hand movement tests and labelled 6,450 images to detect\ndifferent hand gestures in the hand movement tests. This paper has the\nfollowing three contributions. Firstly, we compared and analysed the\nperformance of different network structures for hand gesture detection.\nSecondly, we have made many attempts to improve the accuracy of the model and\nhave succeeded in improving the classification accuracy for similar gestures by\nimplementing attention layers. Thirdly, we have created two datasets and\nincluded 20 percent of blurred images in the dataset to investigate how\ndifferent network structures were impacted by noisy data, our experiments have\nalso shown our network has better performance on the noisy dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son N. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Quan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alty_J/0/1/0/all/0/1\">Jane Alty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Arbitrary Scale Super-Resolution Approach for 3-Dimensional Magnetic Resonance Image using Implicit Neural Representation. (arXiv:2110.14476v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.14476","description":"<p>High Resolution (HR) medical images provide rich anatomical structure details\nto facilitate early and accurate diagnosis. In MRI, restricted by hardware\ncapacity, scan time, and patient cooperation ability, isotropic 3D HR image\nacquisition typically requests long scan time and, results in small spatial\ncoverage and low SNR. Recent studies showed that, with deep convolutional\nneural networks, isotropic HR MR images could be recovered from low-resolution\n(LR) input via single image super-resolution (SISR) algorithms. However, most\nexisting SISR methods tend to approach a scale-specific projection between LR\nand HR images, thus these methods can only deal with a fixed up-sampling rate.\nFor achieving different up-sampling rates, multiple SR networks have to be\nbuilt up respectively, which is very time-consuming and resource-intensive. In\nthis paper, we propose ArSSR, an Arbitrary Scale Super-Resolution approach for\nrecovering 3D HR MR images. In the ArSSR model, the reconstruction of HR images\nwith different up-scaling rates is defined as learning a continuous implicit\nvoxel function from the observed LR images. Then the SR task is converted to\nrepresent the implicit voxel function via deep neural networks from a set of\npaired HR-LR training examples. The ArSSR model consists of an encoder network\nand a decoder network. Specifically, the convolutional encoder network is to\nextract feature maps from the LR input images and the fully-connected decoder\nnetwork is to approximate the implicit voxel function. Due to the continuity of\nthe learned function, a single ArSSR model can achieve arbitrary up-sampling\nrate reconstruction of HR images from any input LR image after training.\nExperimental results on three datasets show that the ArSSR model can achieve\nstate-of-the-art SR performance for 3D HR MR image reconstruction while using a\nsingle trained model to achieve arbitrary up-sampling scales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1\">Qing Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yawen Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_H/0/1/0/all/0/1\">Hongjiang Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches. (arXiv:2110.14908v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2110.14908","description":"<p>What makes speeches effective has long been a subject for debate, and until\ntoday there is broad controversy among public speaking experts about what\nfactors make a speech effective as well as the roles of these factors in\nspeeches. Moreover, there is a lack of quantitative analysis methods to help\nunderstand effective speaking strategies. In this paper, we propose E-ffective,\na visual analytic system allowing speaking experts and novices to analyze both\nthe role of speech factors and their contribution in effective speeches. From\ninterviews with domain experts and investigating existing literature, we\nidentified important factors to consider in inspirational speeches. We obtained\nthe generated factors from multi-modal data that were then related to\neffectiveness data. Our system supports rapid understanding of critical factors\nin inspirational speeches, including the influence of emotions by means of\nnovel visualization methods and interaction. Two novel visualizations include\nE-spiral (that shows the emotional shifts in speeches in a visually compact\nway) and E-script (that connects speech content with key speech delivery\ninformation). In our evaluation we studied the influence of our system on\nexperts' domain knowledge about speech factors. We further studied the\nusability of the system by speaking novices and experts on assisting analysis\nof inspirational speech effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maher_K/0/1/0/all/0/1\">Kevin Maher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zeyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiancheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaoming Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Cuixia Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning Disentangled Group Representation as Feature. (arXiv:2110.15255v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.15255","description":"<p>A good visual representation is an inference map from observations (images)\nto features (vectors) that faithfully reflects the hidden modularized\ngenerative factors (semantics). In this paper, we formulate the notion of\n\"good\" representation from a group-theoretic view using Higgins' definition of\ndisentangled representation, and show that existing Self-Supervised Learning\n(SSL) only disentangles simple augmentation features such as rotation and\ncolorization, thus unable to modularize the remaining semantics. To break the\nlimitation, we propose an iterative SSL algorithm: Iterative Partition-based\nInvariant Risk Minimization (IP-IRM), which successfully grounds the abstract\nsemantics and the group acting on them into concrete contrastive learning. At\neach iteration, IP-IRM first partitions the training samples into two subsets\nthat correspond to an entangled group element. Then, it minimizes a\nsubset-invariant contrastive loss, where the invariance guarantees to\ndisentangle the group element. We prove that IP-IRM converges to a fully\ndisentangled representation and show its effectiveness on various benchmarks.\nCodes are available at https://github.com/Wangt-CN/IP-IRM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhongqi Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qianru Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}