{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Brazilian Court Documents Clustered by Similarity Together Using Natural Language Processing Approaches with Transformers. (arXiv:2204.07182v1 [cs.AI])","link":"http://arxiv.org/abs/2204.07182","description":"<p>Recent advances in Artificial intelligence (AI) have leveraged promising\nresults in solving complex problems in the area of Natural Language Processing\n(NLP), being an important tool to help in the expeditious resolution of\njudicial proceedings in the legal area. In this context, this work targets the\nproblem of detecting the degree of similarity between judicial documents that\ncan be achieved in the inference group, by applying six NLP techniques based on\ntransformers, namely BERT, GPT-2 and RoBERTa pre-trained in the Brazilian\nPortuguese language and the same specialized using 210,000 legal proceedings.\nDocuments were pre-processed and had their content transformed into a vector\nrepresentation using these NLP techniques. Unsupervised learning was used to\ncluster the lawsuits, calculating the quality of the model based on the cosine\nof the distance between the elements of the group to its centroid. We noticed\nthat models based on transformers present better performance when compared to\nprevious research, highlighting the RoBERTa model specialized in the Brazilian\nPortuguese language, making it possible to advance in the current state of the\nart in the area of NLP applied to the legal sector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_R/0/1/0/all/0/1\">Raphael Souza de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_E/0/1/0/all/0/1\">Erick Giovani Sperandio Nascimento</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying Feature Underspecified Lexicon Phonological Features in Multilingual Text-to-Speech. (arXiv:2204.07228v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07228","description":"<p>This study investigates whether the phonological features derived from the\nFeaturally Underspecified Lexicon model can be applied in text-to-speech\nsystems to generate native and non-native speech in English and Mandarin. We\npresent a mapping of ARPABET/pinyin to SAMPA/SAMPA-SC and then to phonological\nfeatures. This mapping was tested for whether it could lead to the successful\ngeneration of native, non-native, and code-switched speech in the two\nlanguages. We ran two experiments, one with a small dataset and one with a\nlarger dataset. The results supported that phonological features could be used\nas a feasible input system for languages in or not in the train data, although\nfurther investigation is needed to improve model performance. The results lend\nsupport to FUL by presenting successfully synthesised output, and by having the\noutput carrying a source-language accent when synthesising a language not in\nthe training data. The TTS process stimulated human second language acquisition\nprocess and thus also confirm FUL's ability to account for acquisition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huinan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiewen Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Fake News Detection: Are current models \"fact-checking\" or \"gut-checking\"?. (arXiv:2204.07229v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07229","description":"<p>Automatic fake news detection models are ostensibly based on logic, where the\ntruth of a claim made in a headline can be determined by supporting or refuting\nevidence found in a resulting web query. These models are believed to be\nreasoning in some way; however, it has been shown that these same results, or\nbetter, can be achieved without considering the claim at all -- only the\nevidence. This implies that other signals are contained within the examined\nevidence, and could be based on manipulable factors such as emotion, sentiment,\nor part-of-speech (POS) frequencies, which are vulnerable to adversarial\ninputs. We neutralize some of these signals through multiple forms of both\nneural and non-neural pre-processing and style transfer, and find that this\nflattening of extraneous indicators can induce the models to actually require\nboth claims and evidence to perform well. We conclude with the construction of\na model using emotion vectors built off a lexicon and passed through an\n\"emotional attention\" mechanism to appropriately weight certain emotions. We\nprovide quantifiable results that prove our hypothesis that manipulable\nfeatures are being used for fact-checking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kelk_I/0/1/0/all/0/1\">Ian Kelk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basseri_B/0/1/0/all/0/1\">Benjamin Basseri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wee Yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Richard Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanner_C/0/1/0/all/0/1\">Chris Tanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Different are Pre-trained Transformers for Text Ranking?. (arXiv:2204.07233v1 [cs.IR])","link":"http://arxiv.org/abs/2204.07233","description":"<p>In recent years, large pre-trained transformers have led to substantial gains\nin performance over traditional retrieval models and feedback approaches.\nHowever, these results are primarily based on the MS Marco/TREC Deep Learning\nTrack setup, with its very particular setup, and our understanding of why and\nhow these models work better is fragmented at best. We analyze effective\nBERT-based cross-encoders versus traditional BM25 ranking for the passage\nretrieval task where the largest gains have been observed, and investigate two\nmain questions. On the one hand, what is similar? To what extent does the\nneural ranker already encompass the capacity of traditional rankers? Is the\ngain in performance due to a better ranking of the same documents (prioritizing\nprecision)? On the other hand, what is different? Can it retrieve effectively\ndocuments missed by traditional systems (prioritizing recall)? We discover\nsubstantial differences in the notion of relevance identifying strengths and\nweaknesses of BERT that may inspire research for future improvement. Our\nresults contribute to our understanding of (black-box) neural rankers relative\nto (well-understood) traditional rankers, help understand the particular\nexperimental setting of MS-Marco-based test collections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rau_D/0/1/0/all/0/1\">David Rau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamps_J/0/1/0/all/0/1\">Jaap Kamps</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A* shortest string decoding for non-idempotent semirings. (arXiv:2204.07236v1 [cs.FL])","link":"http://arxiv.org/abs/2204.07236","description":"<p>The single shortest path algorithm is undefined for weighted finite-state\nautomata over non-idempotent semirings because such semirings do not guarantee\nthe existence of a shortest path. However, in non-idempotent semirings\nadmitting an order satisfying a monotonicity condition (such as the plus-times\nor log semirings), the notion of shortest string is well-defined. We describe\nan algorithm which finds the shortest string for a weighted non-deterministic\nautomaton over such semirings using the backwards shortest distance of an\nequivalent deterministic automaton (DFA) as a heuristic for A* search performed\nover a companion idempotent semiring, which is proven to return the shortest\nstring. While there may be exponentially more states in the DFA, this algorithm\nneeds to visit only a small fraction of them if determinization is performed\n\"on the fly\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gorman_K/0/1/0/all/0/1\">Kyle Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allauzen_C/0/1/0/all/0/1\">Cyril Allauzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers. (arXiv:2204.07237v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07237","description":"<p>This paper presents the first multi-objective transformer model for\nconstructing open cloze tests that exploits generation and discrimination\ncapabilities to improve performance. Our model is further enhanced by tweaking\nits loss function and applying a post-processing re-ranking algorithm that\nimproves overall test structure. Experiments using automatic and human\nevaluation show that our approach can achieve up to 82% accuracy according to\nexperts, outperforming previous work and baselines. We also release a\ncollection of high-quality open cloze tests along with sample system output and\nhuman annotations that can serve as a future benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Felice_M/0/1/0/all/0/1\">Mariano Felice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taslimipoor_S/0/1/0/all/0/1\">Shiva Taslimipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttery_P/0/1/0/all/0/1\">Paula Buttery</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Art of Prompting: Event Detection based on Type Specific Prompts. (arXiv:2204.07241v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07241","description":"<p>We compare various forms of prompts to represent event types and develop a\nunified framework to incorporate the event type specific prompts for\nsupervised, few-shot, and zero-shot event detection. The experimental results\ndemonstrate that a well-defined and comprehensive event type prompt can\nsignificantly improve the performance of event detection, especially when the\nannotated data is scarce (few-shot event detection) or not available (zero-shot\nevent detection). By leveraging the semantics of event types, our unified\nframework shows up to 24.3\\% F-score gain over the previous state-of-the-art\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sijia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated speech tools for helping communities process restricted-access corpora for language revival efforts. (arXiv:2204.07272v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07272","description":"<p>Many archival recordings of speech from endangered languages remain\nunannotated and inaccessible to community members and language learning\nprograms. One bottleneck is the time-intensive nature of annotation. An even\nnarrower bottleneck occurs for recordings with access constraints, such as\nlanguage that must be vetted or filtered by authorised community members before\nannotation can begin. We propose a privacy-preserving workflow to widen both\nbottlenecks for recordings where speech in the endangered language is\nintermixed with a more widely-used language such as English for meta-linguistic\ncommentary and questions (e.g. What is the word for 'tree'?). We integrate\nvoice activity detection (VAD), spoken language identification (SLI), and\nautomatic speech recognition (ASR) to transcribe the metalinguistic content,\nwhich an authorised person can quickly scan to triage recordings that can be\nannotated by people with lower levels of access. We report work-in-progress\nprocessing 136 hours archival audio containing a mix of English and Muruwari.\nOur collaborative work with the Muruwari custodian of the archival materials\nshow that this workflow reduces metalanguage transcription time by 20% even\ngiven only minimal amounts of annotated training data: 10 utterances per\nlanguage for SLI and 39 minutes of the English for ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+San_N/0/1/0/all/0/1\">Nay San</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogunr%5Cdemi_T/0/1/0/all/0/1\">Tol&#xfa;l\\d{o}p\\d&#xe9; &#xd2;g&#xfa;nr\\d&#xe8;m&#xed;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mount_A/0/1/0/all/0/1\">Alison Mount</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_R/0/1/0/all/0/1\">Ruben Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higgins_M/0/1/0/all/0/1\">Michael Higgins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barker_R/0/1/0/all/0/1\">Roy Barker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1\">Jane Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Prompting: Episodic Memory Prompt for Lifelong Event Detection. (arXiv:2204.07275v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07275","description":"<p>Lifelong event detection aims to incrementally update a model with new event\ntypes and data while retaining the capability on previously learned old types.\nOne critical challenge is that the model would catastrophically forget old\ntypes when continually trained on new data. In this paper, we introduce\nEpisodic Memory Prompts (EMP) to explicitly preserve the learned task-specific\nknowledge. Our method adopts continuous prompt for each task and they are\noptimized to instruct the model prediction and learn event-specific\nrepresentation. The EMPs learned in previous tasks are carried along with the\nmodel in subsequent tasks, and can serve as a memory module that keeps the old\nknowledge and transferring to new tasks. Experiment results demonstrate the\neffectiveness of our method. Furthermore, we also conduct a comprehensive\nanalysis of the new and old event types in lifelong learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minqian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context NLP Models. (arXiv:2204.07288v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07288","description":"<p>With many real-world applications of Natural Language Processing (NLP)\ncomprising of long texts, there has been a rise in NLP benchmarks that measure\nthe accuracy of models that can handle longer input sequences. However, these\nbenchmarks do not consider the trade-offs between accuracy, speed, and power\nconsumption as input sizes or model sizes are varied. In this work, we perform\na systematic study of this accuracy vs. efficiency trade-off on two widely used\nlong-sequence models - Longformer-Encoder-Decoder (LED) and Big Bird - during\nfine-tuning and inference on four datasets from the SCROLLS benchmark. To study\nhow this trade-off differs across hyperparameter settings, we compare the\nmodels across four sequence lengths (1024, 2048, 3072, 4096) and two model\nsizes (base and large) under a fixed resource budget. We find that LED\nconsistently achieves better accuracy at lower energy costs than Big Bird. For\nsummarization, we find that increasing model size is more energy efficient than\nincreasing sequence length for higher accuracy. However, this comes at the cost\nof a large drop in inference speed. For question answering, we find that\nsmaller models are both more efficient and more accurate due to the larger\ntraining batch sizes possible under a fixed resource budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ang_P/0/1/0/all/0/1\">Phyllis Ang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wills_L/0/1/0/all/0/1\">Lisa Wu Wills</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying and Measuring Token-Level Sentiment Bias in Pre-trained Language Models with Prompts. (arXiv:2204.07289v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07289","description":"<p>Due to the superior performance, large-scale pre-trained language models\n(PLMs) have been widely adopted in many aspects of human society. However, we\nstill lack effective tools to understand the potential bias embedded in the\nblack-box models. Recent advances in prompt tuning show the possibility to\nexplore the internal mechanism of the PLMs. In this work, we propose two\ntoken-level sentiment tests: Sentiment Association Test (SAT) and Sentiment\nShift Test (SST) which utilize the prompt as a probe to detect the latent bias\nin the PLMs. Our experiments on the collection of sentiment datasets show that\nboth SAT and SST can identify sentiment bias in PLMs and SST is able to\nquantify the bias. The results also suggest that fine-tuning can possibly\naugment the existing bias in PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Apoorv Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_D/0/1/0/all/0/1\">Deval Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where to Go for the Holidays: Towards Mixed-Type Dialogs for Clarification of User Goals. (arXiv:2204.07299v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07299","description":"<p>Most dialog systems posit that users have figured out clear and specific\ngoals before starting an interaction. For example, users have determined the\ndeparture, the destination, and the travel time for booking a flight. However,\nin many scenarios, limited by experience and knowledge, users may know what\nthey need, but still struggle to figure out clear and specific goals by\ndetermining all the necessary slots.\n</p>\n<p>In this paper, we identify this challenge and make a step forward by\ncollecting a new human-to-human mixed-type dialog corpus. It contains 5k dialog\nsessions and 168k utterances for 4 dialog types and 5 domains. Within each\nsession, an agent first provides user-goal-related knowledge to help figure out\nclear and specific goals, and then help achieve them.\n</p>\n<p>Furthermore, we propose a mixed-type dialog model with a novel Prompt-based\ncontinual learning mechanism. Specifically, the mechanism enables the model to\ncontinually strengthen its ability on any specific type by utilizing existing\ndialog corpora effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zeyang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zheng-Yu Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Cross-Modal Understanding in Visual Dialog via Contrastive Learning. (arXiv:2204.07302v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07302","description":"<p>Visual Dialog is a challenging vision-language task since the visual dialog\nagent needs to answer a series of questions after reasoning over both the image\ncontent and dialog history. Though existing methods try to deal with the\ncross-modal understanding in visual dialog, they are still not enough in\nranking candidate answers based on their understanding of visual and textual\ncontexts. In this paper, we analyze the cross-modal understanding in visual\ndialog based on the vision-language pre-training model VD-BERT and propose a\nnovel approach to improve the cross-modal understanding for visual dialog,\nnamed ICMU. ICMU enhances cross-modal understanding by distinguishing different\npulled inputs (i.e. pulled images, questions or answers) based on four-way\ncontrastive learning. In addition, ICMU exploits the single-turn visual\nquestion answering to enhance the visual dialog model's cross-modal\nunderstanding to handle a multi-turn visually-grounded conversation.\nExperiments show that the proposed approach improves the visual dialog model's\ncross-modal understanding and brings satisfactory gain to the VisDial dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saga: A Platform for Continuous Construction and Serving of Knowledge At Scale. (arXiv:2204.07309v1 [cs.DB])","link":"http://arxiv.org/abs/2204.07309","description":"<p>We introduce Saga, a next-generation knowledge construction and serving\nplatform for powering knowledge-based applications at industrial scale. Saga\nfollows a hybrid batch-incremental design to continuously integrate billions of\nfacts about real-world entities and construct a central knowledge graph that\nsupports multiple production use cases with diverse requirements around data\nfreshness, accuracy, and availability. In this paper, we discuss the unique\nchallenges associated with knowledge graph construction at industrial scale,\nand review the main components of Saga and how they address these challenges.\nFinally, we share lessons-learned from a wide array of production use cases\npowered by Saga.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilyas_I/0/1/0/all/0/1\">Ihab F. Ilyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1\">Theodoros Rekatsinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konda_V/0/1/0/all/0/1\">Vishnu Konda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pound_J/0/1/0/all/0/1\">Jeffrey Pound</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaoguang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soliman_M/0/1/0/all/0/1\">Mohamed Soliman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding. (arXiv:2204.07316v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07316","description":"<p>Transformer-based models are widely used in natural language understanding\n(NLU) tasks, and multimodal transformers have been effective in visual-language\ntasks. This study explores distilling visual information from pretrained\nmultimodal transformers to pretrained language encoders. Our framework is\ninspired by cross-modal encoders' success in visual-language tasks while we\nalter the learning objective to cater to the language-heavy characteristics of\nNLU. After training with a small number of extra adapting steps and finetuned,\nthe proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in\ngeneral language understanding evaluation (GLUE), situations with adversarial\ngenerations (SWAG) benchmarks, and readability benchmarks. We analyze the\nperformance of XDBERT on GLUE to show that the improvement is likely visually\ngrounded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chan-Jan Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Entire-Space Models for Target-oriented Opinion Words Extraction. (arXiv:2204.07337v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07337","description":"<p>Target-oriented opinion words extraction (TOWE) is a subtask of aspect-based\nsentiment analysis (ABSA). Given a sentence and an aspect term occurring in the\nsentence, TOWE extracts the corresponding opinion words for the aspect term.\nTOWE has two types of instance. In the first type, aspect terms are associated\nwith at least one opinion word, while in the second type, aspect terms do not\nhave corresponding opinion words. However, previous researches trained and\nevaluated their models with only the first type of instance, resulting in a\nsample selection bias problem. Specifically, TOWE models were trained with only\nthe first type of instance, while these models would be utilized to make\ninference on the entire space with both the first type of instance and the\nsecond type of instance. Thus, the generalization performance will be hurt.\nMoreover, the performance of these models on the first type of instance cannot\nreflect their performance on entire space. To validate the sample selection\nbias problem, four popular TOWE datasets containing only aspect terms\nassociated with at least one opinion word are extended and additionally include\naspect terms without corresponding opinion words. Experimental results on these\ndatasets show that training TOWE models on entire space will significantly\nimprove model performance and evaluating TOWE models only on the first type of\ninstance will overestimate model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuncong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sheng-Hua Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaMemo: Language Modeling with Look-Ahead Memory. (arXiv:2204.07341v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07341","description":"<p>Although Transformers with fully connected self-attentions are powerful to\nmodel long-term dependencies, they are struggling to scale to long texts with\nthousands of words in language modeling. One of the solutions is to equip the\nmodel with a recurrence memory. However, existing approaches directly reuse\nhidden states from the previous segment that encodes contexts in a\nuni-directional way. As a result, this prohibits the memory to dynamically\ninteract with the current context that provides up-to-date information for\ntoken prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo)\nthat enhances the recurrence memory by incrementally attending to the\nright-side tokens, and interpolating with the old memory states to maintain\nlong-term information in the history. LaMemo embraces bi-directional attention\nand segment recurrence with an additional computation overhead only linearly\nproportional to the memory length. Experiments on widely used language modeling\nbenchmarks demonstrate its superiority over the baselines equipped with\ndifferent types of memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Haozhe Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07356","description":"<p>Pretrained models have produced great success in both Computer Vision (CV)\nand Natural Language Processing (NLP). This progress leads to learning joint\nrepresentations of vision and language pretraining by feeding visual and\nlinguistic contents into a multi-layer transformer, Visual-Language Pretrained\nModels (VLPMs). In this paper, we present an overview of the major advances\nachieved in VLPMs for producing joint representations of vision and language.\nAs the preliminaries, we briefly describe the general task definition and\ngenetic architecture of VLPMs. We first discuss the language and vision data\nencoding methods and then present the mainstream VLPM structure as the core\ncontent. We further summarise several essential pretraining and fine-tuning\nstrategies. Finally, we highlight three future directions for both CV and NLP\nresearchers to provide insightful guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_F/0/1/0/all/0/1\">Feiqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqing Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Revision by On-the-Fly Representation Optimization. (arXiv:2204.07359v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07359","description":"<p>Text revision refers to a family of natural language generation tasks, where\nthe source and target sequences share moderate resemblance in surface form but\ndifferentiate in attributes, such as text formality and simplicity. Current\nstate-of-the-art methods formulate these tasks as sequence-to-sequence learning\nproblems, which rely on large-scale parallel training corpus. In this paper, we\npresent an iterative in-place editing approach for text revision, which\nrequires no parallel data. In this approach, we simply fine-tune a pre-trained\nTransformer with masked language modeling and attribute classification. During\ninference, the editing at each iteration is realized by two-step span\nreplacement. At the first step, the distributed representation of the text\noptimizes on the fly towards an attribute function. At the second step, a text\nspan is masked and another new one is proposed conditioned on the optimized\nrepresentation. The empirical experiments on two typical and important text\nrevision tasks, text formalization and text simplification, show the\neffectiveness of our approach. It achieves competitive and even better\nperformance than state-of-the-art supervised methods on text simplification,\nand gains better performance than strong unsupervised methods on text\nformalization \\footnote{Code and model are available at\n\\url{https://github.com/jingjingli01/OREO}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Surprisal in Issue Trackers Actionable?. (arXiv:2204.07363v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07363","description":"<p>Background. From information theory, surprisal is a measurement of how\nunexpected an event is. Statistical language models provide a probabilistic\napproximation of natural languages, and because surprisal is constructed with\nthe probability of an event occuring, it is therefore possible to determine the\nsurprisal associated with English sentences. The issues and pull requests of\nsoftware repository issue trackers give insight into the development process\nand likely contain the surprising events of this process.\n</p>\n<p>Objective. Prior works have identified that unusual events in software\nrepositories are of interest to developers, and use simple code metrics-based\nmethods for detecting them. In this study we will propose a new method for\nunusual event detection in software repositories using surprisal. With the\nability to find surprising issues and pull requests, we intend to further\nanalyse them to determine if they actually hold importance in a repository, or\nif they pose a significant challenge to address. If it is possible to find bad\nsurprises early, or before they cause additional troubles, it is plausible that\neffort, cost and time will be saved as a result.\n</p>\n<p>Method. After extracting the issues and pull requests from 5000 of the most\npopular software repositories on GitHub, we will train a language model to\nrepresent these issues. We will measure their perceived importance in the\nrepository, measure their resolution difficulty using several analogues,\nmeasure the surprisal of each, and finally generate inferential statistics to\ndescribe any correlations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caddy_J/0/1/0/all/0/1\">James Caddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1\">Markus Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treude_C/0/1/0/all/0/1\">Christoph Treude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barr_E/0/1/0/all/0/1\">Earl T. Barr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allamanis_M/0/1/0/all/0/1\">Miltiadis Allamanis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Role of Pre-trained Language Models in Word Ordering: A Case Study with BART. (arXiv:2204.07367v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07367","description":"<p>Word ordering is a constrained language generation task taking unordered\nwords as input. Existing work uses linear models and neural networks for the\ntask, yet pre-trained language models have not been studied in word ordering,\nlet alone why they help. We use BART as an instance and show its effectiveness\nin the task. To explain why BART helps word ordering, we extend analysis with\nprobing and empirically identify that syntactic dependency knowledge in BART is\na reliable explanation. We also report performance gains with BART in the\nrelated partial tree linearization task, which readily extends our analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zebin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Building a Personalized Dialogue Generator via Implicit User Persona Detection. (arXiv:2204.07372v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07372","description":"<p>Current works in the generation of personalized dialogue primarily contribute\nto the agent avoiding contradictory persona and driving the response more\ninformative. However, we found that the generated responses from these models\nare mostly self-centered with little care for the other party since they ignore\nthe user's persona. Moreover, we consider high-quality transmission is\nessentially built based on apprehending the persona of the other party.\nMotivated by this, we propose a novel personalized dialogue generator by\ndetecting implicit user persona. Because it's difficult to collect a large\nnumber of personas for each user, we attempt to model the user's potential\npersona and its representation from the dialogue absence of any external\ninformation. Perception variable and fader variable are conceived utilizing\nConditional Variational Inference. The two latent variables simulate the\nprocess of people being aware of the other party's persona and producing the\ncorresponding expression in conversation. Finally, Posterior-discriminated\nRegularization is presented to enhance the training procedure. Empirical\nstudies demonstrate that compared with the state-of-the-art methods, ours is\nmore concerned with the user's persona and outperforms in evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_I/0/1/0/all/0/1\">Itsugun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_R/0/1/0/all/0/1\">Ryota Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1\">Hiroaki Saito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Email Spam Detection Using Hierarchical Attention Hybrid Deep Learning Method. (arXiv:2204.07390v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07390","description":"<p>Email is one of the most widely used ways to communicate, with millions of\npeople and businesses relying on it to communicate and share knowledge and\ninformation on a daily basis. Nevertheless, the rise in email users has\noccurred a dramatic increase in spam emails in recent years. Processing and\nmanaging emails properly for individuals and companies are getting increasingly\ndifficult. This article proposes a novel technique for email spam detection\nthat is based on a combination of convolutional neural networks, gated\nrecurrent units, and attention mechanisms. During system training, the network\nis selectively focused on necessary parts of the email text. The usage of\nconvolution layers to extract more meaningful, abstract, and generalizable\nfeatures by hierarchical representation is the major contribution of this\nstudy. Additionally, this contribution incorporates cross-dataset evaluation,\nwhich enables the generation of more independent performance results from the\nmodel's training dataset. According to cross-dataset evaluation results, the\nproposed technique advances the results of the present attention-based\ntechniques by utilizing temporal convolutions, which give us more flexible\nreceptive field sizes are utilized. The suggested technique's findings are\ncompared to those of state-of-the-art models and show that our approach\noutperforms them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zavrak_S/0/1/0/all/0/1\">Sultan Zavrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_S/0/1/0/all/0/1\">Seyhmus Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fine-grained Causal Reasoning and QA. (arXiv:2204.07408v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07408","description":"<p>Understanding causality is key to the success of NLP applications, especially\nin high-stakes domains. Causality comes in various perspectives such as enable\nand prevent that, despite their importance, have been largely ignored in the\nliterature. This paper introduces a novel fine-grained causal reasoning dataset\nand presents a series of novel predictive tasks in NLP, such as causality\ndetection, event causality extraction, and Causal QA. Our dataset contains\nhuman annotations of 25K cause-effect event pairs and 24K question-answering\npairs within multi-sentence samples, where each can have multiple causal\nrelationships. Through extensive experiments and analysis, we show that the\ncomplex relations in our dataset bring unique challenges to state-of-the-art\nmethods across all three tasks and highlight potential research opportunities,\nespecially in developing \"causal-thinking\" methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ML_LTU at SemEval-2022 Task 4: T5 Towards Identifying Patronizing and Condescending Language. (arXiv:2204.07432v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07432","description":"<p>This paper describes the system used by the Machine Learning Group of LTU in\nsubtask 1 of the SemEval-2022 Task 4: Patronizing and Condescending Language\n(PCL) Detection. Our system consists of finetuning a pretrained\nText-to-Text-Transfer Transformer (T5) and innovatively reducing its\nout-of-class predictions. The main contributions of this paper are 1) the\ndescription of the implementation details of the T5 model we used, 2) analysis\nof the successes &amp; struggles of the model in this task, and 3) ablation studies\nbeyond the official submission to ascertain the relative importance of data\nsplit. Our model achieves an F1 score of 0.5452 on the official test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhaled_L/0/1/0/all/0/1\">Lama Alkhaled</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhaled_H/0/1/0/all/0/1\">Hamam Alkhaled</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_F/0/1/0/all/0/1\">Foteini Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification. (arXiv:2204.07434v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07434","description":"<p>Document-level Event Causality Identification (DECI) aims to identify causal\nrelations between event pairs in a document. It poses a great challenge of\nacross-sentence reasoning without clear causal indicators. In this paper, we\npropose a novel Event Relational Graph TransfOrmer (ERGO) framework for DECI,\nwhich improves existing state-of-the-art (SOTA) methods upon two aspects.\nFirst, we formulate DECI as a node classification problem by constructing an\nevent relational graph, without the needs of prior knowledge or tools. Second,\nERGO seamlessly integrates event-pair relation classification and global\ninference, which leverages a Relational Graph Transformer (RGT) to capture the\npotential causal chain. Besides, we introduce edge-building strategies and\nadaptive focal loss to deal with the massive false positives caused by common\nspurious correlation. Extensive experiments on two benchmark datasets show that\nERGO significantly outperforms previous SOTA methods (13.1% F1 gains on\naverage). We have conducted extensive quantitative analysis and case studies to\nprovide insights for future research directions (Section 4.8).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meiqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kunquan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Political Communities on Twitter: Case Study of the 2022 French Presidential Election. (arXiv:2204.07436v1 [cs.SI])","link":"http://arxiv.org/abs/2204.07436","description":"<p>With the significant increase in users on social media platforms, a new means\nof political campaigning has appeared. Twitter and Facebook are now notable\ncampaigning tools during elections. Indeed, the candidates and their parties\nnow take to the internet to interact and spread their ideas. In this paper, we\naim to identify political communities formed on Twitter during the 2022 French\npresidential election and analyze each respective community. We create a\nlarge-scale Twitter dataset containing 1.2 million users and 62.6 million\ntweets that mention keywords relevant to the election. We perform community\ndetection on a retweet graph of users and propose an in-depth analysis of the\nstance of each community. Finally, we attempt to detect offensive tweets and\nautomatic bots, comparing across communities in order to gain insight into each\ncandidate's supporter demographics and online campaign strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdine_H/0/1/0/all/0/1\">Hadi Abdine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanzhu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rennard_V/0/1/0/all/0/1\">Virgile Rennard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval. (arXiv:2204.07441v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07441","description":"<p>Large-scale single-stream pre-training has shown dramatic performance in\nimage-text retrieval. Regrettably, it faces low inference efficiency due to\nheavy attention layers. Recently, two-stream methods like CLIP and ALIGN with\nhigh inference efficiency have also shown promising performance, however, they\nonly consider instance-level alignment between the two streams (thus there is\nstill room for improvement). To overcome these limitations, we propose a novel\nCOllaborative Two-Stream vision-language pretraining model termed COTS for\nimage-text retrieval by enhancing cross-modal interaction. In addition to\ninstance level alignment via momentum contrastive learning, we leverage two\nextra levels of cross-modal interactions in our COTS: (1) Token-level\ninteraction - a masked visionlanguage modeling (MVLM) learning objective is\ndevised without using a cross-stream network module, where variational\nautoencoder is imposed on the visual encoder to generate visual tokens for each\nimage. (2) Task-level interaction - a KL-alignment learning objective is\ndevised between text-to-image and image-to-text retrieval tasks, where the\nprobability distribution per task is computed with the negative queues in\nmomentum contrastive learning. Under a fair comparison setting, our COTS\nachieves the highest performance among all two-stream methods and comparable\nperformance (but with 10,800X faster in inference) w.r.t. the latest\nsingle-stream methods. Importantly, our COTS is also applicable to\ntext-to-video retrieval, yielding new state-ofthe-art on the widely-used\nMSR-VTT dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haoyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_N/0/1/0/all/0/1\">Nanyi Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuqi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yizhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiwu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stretching Sentence-pair NLI Models to Reason over Long Documents and Clusters. (arXiv:2204.07447v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07447","description":"<p>Natural Language Inference (NLI) has been extensively studied by the NLP\ncommunity as a framework for estimating the semantic relation between sentence\npairs. While early work identified certain biases in NLI models, recent\nadvancements in modeling and datasets demonstrated promising performance. In\nthis work, we further explore the direct zero-shot applicability of NLI models\nto real applications, beyond the sentence-pair setting they were trained on.\nFirst, we analyze the robustness of these models to longer and out-of-domain\ninputs. Then, we develop new aggregation methods to allow operating over full\ndocuments, reaching state-of-the-art performance on the ContractNLI dataset.\nInterestingly, we find NLI scores to provide strong retrieval signals, leading\nto more relevant evidence extractions compared to common similarity-based\nmethods. Finally, we go further and investigate whole document clusters to\nidentify both discrepancies and consensus among sources. In a test case, we\nfind real inconsistencies between Wikipedia pages in different languages about\nthe same topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buthpitiya_S/0/1/0/all/0/1\">Senaka Buthpitiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabrikant_A/0/1/0/all/0/1\">Alex Fabrikant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Qtrade AI at SemEval-2022 Task 11: An Unified Framework for Multilingual NER Task. (arXiv:2204.07459v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07459","description":"<p>This paper describes our system, which placed third in the Multilingual Track\n(subtask 11), fourth in the Code-Mixed Track (subtask 12), and seventh in the\nChinese Track (subtask 9) in the SemEval 2022 Task 11: MultiCoNER Multilingual\nComplex Named Entity Recognition. Our system's key contributions are as\nfollows: 1) For multilingual NER tasks, we offer an unified framework with\nwhich one can easily execute single-language or multilingual NER tasks, 2) for\nlow-resource code-mixed NER task, one can easily enhance his or her dataset\nthrough implementing several simple data augmentation methods and 3) for\nChinese tasks, we propose a model that can capture Chinese lexical semantic,\nlexical border, and lexical graph structural information. Finally, our system\nachieves macro-f1 scores of 77.66, 84.35, and 74.00 on subtasks 11, 12, and 9,\nrespectively, during the testing phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Weichao Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanping Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guangbo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qian Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Pre-trained Language Models with Syntactic Dependency Prediction Task for Chinese Semantic Error Recognition. (arXiv:2204.07464v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07464","description":"<p>Existing Chinese text error detection mainly focuses on spelling and simple\ngrammatical errors. These errors have been studied extensively and are\nrelatively simple for humans. On the contrary, Chinese semantic errors are\nunderstudied and more complex that humans cannot easily recognize. The task of\nthis paper is Chinese Semantic Error Recognition (CSER), a binary\nclassification task to determine whether a sentence contains semantic errors.\nThe current research has no effective method to solve this task. In this paper,\nwe inherit the model structure of BERT and design several syntax-related\npre-training tasks so that the model can learn syntactic knowledge. Our\npre-training tasks consider both the directionality of the dependency structure\nand the diversity of the dependency relationship. Due to the lack of a\npublished dataset for CSER, we build a high-quality dataset for CSER for the\nfirst time named Corpus of Chinese Linguistic Semantic Acceptability (CoCLSA).\nThe experimental results on the CoCLSA show that our methods outperform\nuniversal pre-trained models and syntax-infused models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dayong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixture of Experts for Biomedical Question Answering. (arXiv:2204.07469v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07469","description":"<p>Biomedical Question Answering (BQA) has attracted increasing attention in\nrecent years due to its promising application prospect. It is a challenging\ntask because the biomedical questions are professional and usually vary widely.\nExisting question answering methods answer all questions with a homogeneous\nmodel, leading to various types of questions competing for the shared\nparameters, which will confuse the model decision for each single type of\nquestions. In this paper, in order to alleviate the parameter competition\nproblem, we propose a Mixture-of-Expert (MoE) based question answering method\ncalled MoEBQA that decouples the computation for different types of questions\nby sparse routing. To be specific, we split a pretrained Transformer model into\nbottom and top blocks. The bottom blocks are shared by all the examples, aiming\nto capture the general features. The top blocks are extended to an MoE version\nthat consists of a series of independent experts, where each example is\nassigned to a few experts according to its underlying question type. MoEBQA\nautomatically learns the routing strategy in an end-to-end manner so that each\nexpert tends to deal with the question types it is expert in. We evaluate\nMoEBQA on three BQA datasets constructed based on real examinations. The\nresults show that our MoE extension significantly boosts the performance of\nquestion answering models and achieves new state-of-the-art performance. In\naddition, we elaborately analyze our MoE modules to reveal how MoEBQA works and\nfind that it can automatically group the questions into human-readable\nclusters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenbin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Weihua Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yajuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yong Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polling Latent Opinions: A Method for Computational Sociolinguistics Using Transformer Language Models. (arXiv:2204.07483v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07483","description":"<p>Text analysis of social media for sentiment, topic analysis, and other\nanalysis depends initially on the selection of keywords and phrases that will\nbe used to create the research corpora. However, keywords that researchers\nchoose may occur infrequently, leading to errors that arise from using small\nsamples. In this paper, we use the capacity for memorization, interpolation,\nand extrapolation of Transformer Language Models such as the GPT series to\nlearn the linguistic behaviors of a subgroup within larger corpora of Yelp\nreviews. We then use prompt-based queries to generate synthetic text that can\nbe analyzed to produce insights into specific opinions held by the populations\nthat the models were trained on. Once learned, more specific sentiment queries\ncan be made of the model with high levels of accuracy when compared to\ntraditional keyword searches. We show that even in cases where a specific\nkeyphrase is limited or not present at all in the training corpora, the GPT is\nable to accurately generate large volumes of text that have the correct\nsentiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dant_P/0/1/0/all/0/1\">Philip Feldman. Aaron Dant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foulds_J/0/1/0/all/0/1\">James R. Foulds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shemei Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Passage Retrieval with Zero-Shot Question Generation. (arXiv:2204.07496v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07496","description":"<p>We propose a simple and effective re-ranking method for improving passage\nretrieval in open question answering. The re-ranker re-scores retrieved\npassages with a zero-shot question generation model, which uses a pre-trained\nlanguage model to compute the probability of the input question conditioned on\na retrieved passage. This approach can be applied on top of any retrieval\nmethod (e.g. neural or keyword-based), does not require any domain- or\ntask-specific training (and therefore is expected to generalize better to data\ndistribution shifts), and provides rich cross-attention between query and\npassage (i.e. it must explain every token in the question). When evaluated on a\nnumber of open-domain retrieval datasets, our re-ranker improves strong\nunsupervised retrieval models by 6%-18% absolute and strong supervised models\nby up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new\nstate-of-the-art results on full open-domain question answering by simply\nadding the new re-ranker to existing models with no further changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1\">Devendra Singh Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Game-Playing Agents with Natural Language Annotations. (arXiv:2204.07531v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07531","description":"<p>We present a new dataset containing 10K human-annotated games of Go and show\nhow these natural language annotations can be used as a tool for model\ninterpretability. Given a board state and its associated comment, our approach\nuses linear probing to predict mentions of domain-specific terms (e.g., ko,\natari) from the intermediate state representations of game-playing agents like\nAlphaGo Zero. We find these game concepts are nontrivially encoded in two\ndistinct policy networks, one trained via imitation learning and another\ntrained via reinforcement learning. Furthermore, mentions of domain-specific\nterms are most easily predicted from the later layers of both models,\nsuggesting that these policy networks encode high-level abstractions similar to\nthose used in the natural language annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomlin_N/0/1/0/all/0/1\">Nicholas Tomlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_A/0/1/0/all/0/1\">Andre He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Judgement as a Compass to Navigate Automatic Metrics for Formality Transfer. (arXiv:2204.07549v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07549","description":"<p>Although text style transfer has witnessed rapid development in recent years,\nthere is as yet no established standard for evaluation, which is performed\nusing several automatic metrics, lacking the possibility of always resorting to\nhuman judgement. We focus on the task of formality transfer, and on the three\naspects that are usually evaluated: style strength, content preservation, and\nfluency. To cast light on how such aspects are assessed by common and new\nmetrics, we run a human-based evaluation and perform a rich correlation\nanalysis. We are then able to offer some recommendations on the use of such\nmetrics in formality transfer, also with an eye to their generalisability (or\nnot) to related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huiyuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiali Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarization with Graphical Elements. (arXiv:2204.07551v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07551","description":"<p>Automatic text summarization has experienced substantial progress in recent\nyears. With this progress, the question has arisen whether the types of\nsummaries that are typically generated by automatic summarization models align\nwith users' needs. Ter Hoeve et al (2020) answer this question negatively.\nAmongst others, they recommend focusing on generating summaries with more\ngraphical elements. This is in line with what we know from the\npsycholinguistics literature about how humans process text. Motivated from\nthese two angles, we propose a new task: summarization with graphical elements,\nand we verify that these summaries are helpful for a critical mass of people.\nWe collect a high quality human labeled dataset to support research into the\ntask. We present a number of baseline methods that show that the task is\ninteresting and challenging. Hence, with this work we hope to inspire a new\nline of research within the automatic summarization community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1\">Maartje ter Hoeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Rare Word Recognition with LM-aware MWER Training. (arXiv:2204.07553v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07553","description":"<p>Language models (LMs) significantly improve the recognition accuracy of\nend-to-end (E2E) models on words rarely seen during training, when used in\neither the shallow fusion or the rescoring setups. In this work, we introduce\nLMs in the learning of hybrid autoregressive transducer (HAT) models in the\ndiscriminative training framework, to mitigate the training versus inference\ngap regarding the use of LMs. For the shallow fusion setup, we use LMs during\nboth hypotheses generation and loss computation, and the LM-aware MWER-trained\nmodel achieves 10\\% relative improvement over the model trained with standard\nMWER on voice search test sets containing rare words. For the rescoring setup,\nwe learn a small neural module to generate per-token fusion weights in a\ndata-dependent manner. This model achieves the same rescoring WER as regular\nMWER-trained model, but without the need for sweeping fusion weights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tongzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Variani_E/0/1/0/all/0/1\">Ehsan Variani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_N/0/1/0/all/0/1\">Neeraj Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavandadi_S/0/1/0/all/0/1\">Sepand Mavandadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyser_C/0/1/0/all/0/1\">Cal Peyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybach_D/0/1/0/all/0/1\">David Rybach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Idiom Paraphrasing. (arXiv:2204.07555v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07555","description":"<p>Idioms, are a kind of idiomatic expression in Chinese, most of which consist\nof four Chinese characters. Due to the properties of non-compositionality and\nmetaphorical meaning, Chinese Idioms are hard to be understood by children and\nnon-native speakers. This study proposes a novel task, denoted as Chinese Idiom\nParaphrasing (CIP). CIP aims to rephrase idioms-included sentences to\nnon-idiomatic ones under the premise of preserving the original sentence's\nmeaning. Since the sentences without idioms are easier handled by Chinese NLP\nsystems, CIP can be used to pre-process Chinese datasets, thereby facilitating\nand improving the performance of Chinese NLP tasks, e.g., machine translation\nsystem, Chinese idiom cloze, and Chinese idiom embeddings. In this study, CIP\ntask is treated as a special paraphrase generation task. To circumvent\ndifficulties in acquiring annotations, we first establish a large-scale CIP\ndataset based on human and machine collaboration, which consists of 115,530\nsentence pairs. We further deploy three baselines and two novel CIP approaches\nto deal with CIP problems. The results show that the proposed methods have\nbetter performances than the baselines based on the established CIP dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiang_J/0/1/0/all/0/1\">Jipeng Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaowei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunhao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xindong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Align-Refine for Non-autoregressive Deliberation. (arXiv:2204.07556v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07556","description":"<p>We propose a streaming non-autoregressive (non-AR) decoding algorithm to\ndeliberate the hypothesis alignment of a streaming RNN-T model. Our algorithm\nfacilitates a simple greedy decoding procedure, and at the same time is capable\nof producing the decoding result at each frame with limited right context, thus\nenjoying both high efficiency and low latency. These advantages are achieved by\nconverting the offline Align-Refine algorithm to be streaming-compatible, with\na novel transformer decoder architecture that performs local self-attentions\nfor both text and audio, and a time-aligned cross-attention at each layer.\nFurthermore, we perform discriminative training of our model with the minimum\nword error rate (MWER) criterion, which has not been done in the non-AR\ndecoding literature. Experiments on voice search datasets and Librispeech show\nthat with reasonable right context, our streaming model performs as well as the\noffline counterpart, and discriminative training leads to further WER gain when\nthe first-pass model has small capacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Factuality in Text Simplification. (arXiv:2204.07562v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07562","description":"<p>Automated simplification models aim to make input texts more readable. Such\nmethods have the potential to make complex information accessible to a wider\naudience, e.g., providing access to recent medical literature which might\notherwise be impenetrable for a lay reader. However, such models risk\nintroducing errors into automatically simplified texts, for instance by\ninserting statements unsupported by the corresponding original text, or by\nomitting key information. Providing more readable but inaccurate versions of\ntexts may in many cases be worse than providing no such access at all. The\nproblem of factual accuracy (and the lack thereof) has received heightened\nattention in the context of summarization models, but the factuality of\nautomatically simplified texts has not been investigated. We introduce a\ntaxonomy of errors that we use to analyze both references drawn from standard\nsimplification datasets and state-of-the-art model outputs. We find that errors\noften appear in both that are not captured by existing evaluation metrics,\nmotivating a need for research into ensuring the factual accuracy of automated\nsimplification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Devaraj_A/0/1/0/all/0/1\">Ashwin Devaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheffield_W/0/1/0/all/0/1\">William Sheffield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation Benchmarks for Spanish Sentence Representations. (arXiv:2204.07571v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07571","description":"<p>Due to the success of pre-trained language models, versions of languages\nother than English have been released in recent years. This fact implies the\nneed for resources to evaluate these models. In the case of Spanish, there are\nfew ways to systematically assess the models' quality. In this paper, we narrow\nthe gap by building two evaluation benchmarks. Inspired by previous work\n(Conneau and Kiela, 2018; Chen et al., 2019), we introduce Spanish SentEval and\nSpanish DiscoEval, aiming to assess the capabilities of stand-alone and\ndiscourse-aware sentence representations, respectively. Our benchmarks include\nconsiderable pre-existing and newly constructed datasets that address different\ntasks from various domains. In addition, we evaluate and analyze the most\nrecent pre-trained Spanish language models to exhibit their capabilities and\nlimitations. As an example, we discover that for the case of discourse\nevaluation tasks, mBERT, a language model trained on multiple languages,\nusually provides a richer latent representation than models trained only with\ndocuments in Spanish. We hope our contribution will motivate a fairer, more\ncomparable, and less cumbersome way to evaluate future Spanish language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Vladimir Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1\">Andr&#xe9;s Carvallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Souvik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canete_J/0/1/0/all/0/1\">Jos&#xe9; Ca&#xf1;ete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_M/0/1/0/all/0/1\">Marcelo Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercer_R/0/1/0/all/0/1\">Robert E. Mercer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bravo_Marquez_F/0/1/0/all/0/1\">Felipe Bravo-Marquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Alvaro Soto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consecutive Decoding for Speech-to-text Translation. (arXiv:2009.09737v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.09737","description":"<p>Speech-to-text translation (ST), which directly translates the source\nlanguage speech to the target language text, has attracted intensive attention\nrecently. However, the combination of speech recognition and machine\ntranslation in a single model poses a heavy burden on the direct cross-modal\ncross-lingual mapping. To reduce the learning difficulty, we propose\nCOnSecutive Transcription and Translation (COSTT), an integral approach for\nspeech-to-text translation. The key idea is to generate source transcript and\ntarget translation text with a single decoder. It benefits the model training\nso that additional large parallel text corpus can be fully exploited to enhance\nthe speech translation training. Our method is verified on three mainstream\ndatasets, including Augmented LibriSpeech English-French dataset, IWSLT2018\nEnglish-German dataset, and TED English-Chinese dataset. Experiments show that\nour proposed COSTT outperforms or on par with the previous state-of-the-art\nmethods on the three datasets. We have released our code at\n\\url{https://github.com/dqqcasia/st}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qianqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QAConv: Question Answering on Informative Conversations. (arXiv:2105.06912v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06912","description":"<p>This paper introduces QAConv, a new question answering (QA) dataset that uses\nconversations as a knowledge source. We focus on informative conversations,\nincluding business emails, panel discussions, and work channels. Unlike\nopen-domain and task-oriented dialogues, these conversations are usually long,\ncomplex, asynchronous, and involve strong domain knowledge. In total, we\ncollect 34,608 QA pairs from 10,259 selected conversations with both\nhuman-written and machine-generated questions. We use a question generator and\na dialogue summarizer as auxiliary tools to collect and recommend questions.\nThe dataset has two testing scenarios: chunk mode and full mode, depending on\nwhether the grounded partial conversation is provided or retrieved.\nExperimental results show that state-of-the-art pretrained QA systems have\nlimited zero-shot performance and tend to predict our questions as\nunanswerable. Our dataset provides a new training and evaluation testbed to\nfacilitate QA on conversations research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieve-then-extract Based Knowledge Graph Querying Using Graph Neural Networks. (arXiv:2111.10541v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10541","description":"<p>The abstract of Retrieve-then-extract Based Knowledge Graph Querying Using\nGraph Neural Networks will be updated here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hanning Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Po Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking Outside the Box to Ground Language in 3D Scenes. (arXiv:2112.08879v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08879","description":"<p>Most language grounding models learn to select the referred object from a\npool of object proposals provided by a pre-trained detector. This object\nproposal bottleneck is limiting because an utterance may refer to visual\nentities at various levels of granularity, such as the chair, the leg of a\nchair, or the tip of the front leg of a chair, which may be missed by the\ndetector. Recently, MDETR introduced a language grounding model for 2D images\nthat do not have such a box proposal bottleneck; instead of selecting objects\nfrom a proposal pool, it instead decodes the referenced object boxes directly\nfrom image and language features and achieves big leaps in performance. We\npropose a language grounding model for 3D scenes built on MDETR, which we call\nBEAUTY-DETR, from bottom-up and top-down DETR. BEAUTY-DETR attends on an\nadditional object proposal pool computed bottom-up from a pre-trained detector.\nYet it decodes referenced objects without selecting them from the pool. In this\nway, it uses powerful object detectors to help ground language without being\nrestricted by their misses. Second, BEAUTY-DETR augments supervision from\nlanguage grounding annotations by configuring object detection annotations as\nlanguage prompts to be grounded in images. The proposed model sets a new\nstate-of-the-art across popular 3D language grounding benchmarks with\nsignificant performance gains over previous 3D approaches (12.6% on SR3D, 11.6%\non NR3D and 6.3% on ScanRefer). It outperforms a straightforward MDETR for the\n3D point clouds method we implemented by 6.7% on SR3D, 11.8% on NR3D and 5% on\nthe ScanRefer benchmark. When applied to language grounding in 2D images, it\nperforms on par with MDETR. We ablate each of the design choices of the model\nand quantify their contribution to performance. Code and checkpoints are\navailable at https://github.com/nickgkan/beauty_detr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1\">Nikolaos Gkanatsios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mediratta_I/0/1/0/all/0/1\">Ishita Mediratta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences. (arXiv:2201.11838v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11838","description":"<p>Transformers-based models, such as BERT, have dramatically improved the\nperformance for various natural language processing tasks. The clinical\nknowledge enriched model, namely ClinicalBERT, also achieved state-of-the-art\nresults when performed on clinical named entity recognition and natural\nlanguage inference tasks. One of the core limitations of these transformers is\nthe substantial memory consumption due to their full self-attention mechanism.\nTo overcome this, long sequence transformer models, e.g. Longformer and\nBigBird, were proposed with the idea of sparse attention mechanism to reduce\nthe memory usage from quadratic to the sequence length to a linear scale. These\nmodels extended the maximum input sequence length from 512 to 4096, which\nenhanced the ability of modeling long-term dependency and consequently achieved\noptimal results in a variety of tasks. Inspired by the success of these long\nsequence transformer models, we introduce two domain enriched language models,\nnamely Clinical-Longformer and Clinical-BigBird, which are pre-trained from\nlarge-scale clinical corpora. We evaluate both pre-trained models using 10\nbaseline tasks including named entity recognition, question answering, and\ndocument classification tasks. The results demonstrate that Clinical-Longformer\nand Clinical-BigBird consistently and significantly outperform ClinicalBERT as\nwell as other short-sequence transformers in all downstream tasks. We have made\nour source code available at\n[https://github.com/luoyuanlab/Clinical-Longformer] the pre-trained models\navailable for public download at:\n[https://huggingface.co/yikuan8/Clinical-Longformer].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehbe_R/0/1/0/all/0/1\">Ramsey M. Wehbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_F/0/1/0/all/0/1\">Faraz S. Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-constraint Optimal Transport for Entity Alignment with Dangling Cases. (arXiv:2203.05744v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05744","description":"<p>Entity alignment (EA) merges knowledge graphs (KGs) by identifying the\nequivalent entities in different graphs, which can effectively enrich knowledge\nrepresentations of KGs. However, in practice, different KGs often include\ndangling entities whose counterparts cannot be found in the other graph, which\nlimits the performance of EA methods. To improve EA with dangling entities, we\npropose an unsupervised method called Semi-constraint Optimal Transport for\nEntity Alignment in Dangling cases (SoTead). Our main idea is to model the\nentity alignment between two KGs as an optimal transport problem from one KG's\nentities to the others. First, we set pseudo entity pairs between KGs based on\npretrained word embeddings. Then, we conduct contrastive metric learning to\nobtain the transport cost between each entity pair. Finally, we introduce a\nvirtual entity for each KG to \"align\" the dangling entities from the other KGs,\nwhich relaxes the optimization constraints and leads to a semi-constraint\noptimal transport. In the experimental part, we first show the superiority of\nSoTead on a commonly-used entity alignment dataset. Besides, to analyze the\nability for dangling entity detection with other baselines, we construct a\nmedical cross-lingual knowledge graph dataset, MedED, where our SoTead also\nreaches state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengxuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Text-to-Program for Question Answering on Structured Electronic Health Records. (arXiv:2203.06918v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06918","description":"<p>Question Answering on Electronic Health Records (EHR-QA) has a significant\nimpact on the healthcare domain, and it is being actively studied. Previous\nresearch on structured EHR-QA focuses on converting natural language queries\ninto query language such as SQL or SPARQL (NLQ2Query), so the problem scope is\nlimited to pre-defined data types by the specific query language. In order to\nexpand the EHR-QA task beyond this limitation to handle multi-modal medical\ndata and solve complex inference in the future, more primitive systemic\nlanguage is needed. In this paper, we design the program-based model\n(NLQ2Program) for EHR-QA as the first step towards the future direction. We\ntackle MIMICSPARQL*, the graph-based EHR-QA dataset, via a program-based\napproach in a semi-supervised manner in order to overcome the absence of gold\nprograms. Without the gold program, our proposed model shows comparable\nperformance to the previous state-of-the-art model, which is an NLQ2Query model\n(0.9% gain). In addition, for a reliable EHR-QA model, we apply the uncertainty\ndecomposition method to measure the ambiguity in the input question. We\nempirically confirmed data uncertainty is most indicative of the ambiguity in\nthe input question.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Seongsu Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale Bilingual Language-Image Contrastive Learning. (arXiv:2203.14463v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14463","description":"<p>This paper is a technical report to share our experience and findings\nbuilding a Korean and English bilingual multimodal model. While many of the\nmultimodal datasets focus on English and multilingual multimodal research uses\nmachine-translated texts, employing such machine-translated texts is limited to\ndescribing unique expressions, cultural information, and proper noun in\nlanguages other than English. In this work, we collect 1.1 billion image-text\npairs (708 million Korean and 476 million English) and train a bilingual\nmultimodal model named KELIP. We introduce simple yet effective training\nschemes, including MAE pre-training and multi-crop augmentation. Extensive\nexperiments demonstrate that a model trained with such training schemes shows\ncompetitive performance in both languages. Moreover, we discuss\nmultimodal-related research questions: 1) strong augmentation-based methods can\ndistract the model from learning proper multimodal relations; 2) training\nmultimodal model without cross-lingual relation can learn the relation via\nvisual semantics; 3) our bilingual KELIP can capture cultural differences of\nvisual semantics for the same meaning of words; 4) a large-scale multimodal\nmodel can be used for multimodal feature analogy. We hope that this work will\nprovide helpful experience and findings for future research. We provide an\nopen-source pre-trained KELIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1\">Byungsoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1\">Geonmo Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Attention through Gradient-Based Learned Runtime Pruning. (arXiv:2204.03227v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03227","description":"<p>Self-attention is a key enabler of state-of-art accuracy for various\ntransformer-based Natural Language Processing models. This attention mechanism\ncalculates a correlation score for each word with respect to the other words in\na sentence. Commonly, only a small subset of words highly correlates with the\nword under attention, which is only determined at runtime. As such, a\nsignificant amount of computation is inconsequential due to low attention\nscores and can potentially be pruned. The main challenge is finding the\nthreshold for the scores below which subsequent computation will be\ninconsequential. Although such a threshold is discrete, this paper formulates\nits search through a soft differentiable regularizer integrated into the loss\nfunction of the training. This formulation piggy backs on the back-propagation\ntraining to analytically co-optimize the threshold and the weights\nsimultaneously, striking a formally optimal balance between accuracy and\ncomputation pruning. To best utilize this mathematical innovation, we devise a\nbit-serial architecture, dubbed LeOPArd, for transformer language models with\nbit-level early termination microarchitectural mechanism. We evaluate our\ndesign across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision\ntransformer models. Post-layout results show that, on average, LeOPArd yields\n1.9x and 3.9x speedup and energy reduction, respectively, while keeping the\naverage accuracy virtually intact (&lt;0.2% degradation)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodrati_S/0/1/0/all/0/1\">Soroush Ghodrati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdanbakhsh_A/0/1/0/all/0/1\">Amir Yazdanbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilzadeh_H/0/1/0/all/0/1\">Hadi Esmaeilzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Mingu Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Schema Graph Fusion Network for Multi-Domain Dialogue State Tracking. (arXiv:2204.06677v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06677","description":"<p>Dialogue State Tracking (DST) aims to keep track of users' intentions during\nthe course of a conversation. In DST, modelling the relations among domains and\nslots is still an under-studied problem. Existing approaches that have\nconsidered such relations generally fall short in: (1) fusing prior slot-domain\nmembership relations and dialogue-aware dynamic slot relations explicitly, and\n(2) generalizing to unseen domains. To address these issues, we propose a novel\n\\textbf{D}ynamic \\textbf{S}chema \\textbf{G}raph \\textbf{F}usion\n\\textbf{Net}work (\\textbf{DSGFNet}), which generates a dynamic schema graph to\nexplicitly fuse the prior slot-domain membership relations and dialogue-aware\ndynamic slot relations. It also uses the schemata to facilitate knowledge\ntransfer to new domains. DSGFNet consists of a dialogue utterance encoder, a\nschema graph encoder, a dialogue-aware schema graph evolving network, and a\nschema graph enhanced dialogue state decoder. Empirical results on benchmark\ndatasets (i.e., SGD, MultiWOZ2.1, and MultiWOZ2.2), show that DSGFNet\noutperforms existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fanghua Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges for Open-domain Targeted Sentiment Analysis. (arXiv:2204.06893v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06893","description":"<p>Since previous studies on open-domain targeted sentiment analysis are limited\nin dataset domain variety and sentence level, we propose a novel dataset\nconsisting of 6,013 human-labeled data to extend the data domains in topics of\ninterest and document level. Furthermore, we offer a nested target annotation\nschema to extract the complete sentiment information in documents, boosting the\npracticality and effectiveness of open-domain targeted sentiment analysis.\nMoreover, we leverage the pre-trained model BART in a sequence-to-sequence\ngeneration method for the task. Benchmark results show that there exists large\nroom for improvement of open-domain targeted sentiment analysis. Meanwhile,\nexperiments have shown that challenges remain in the effective use of\nopen-domain data, long documents, the complexity of target structure, and\ndomain variances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongjie Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yanxia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Interactive Object Segmentation in 3D Point Clouds. (arXiv:2204.07183v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07183","description":"<p>Deep learning depends on large amounts of labeled training data. Manual\nlabeling is expensive and represents a bottleneck, especially for tasks such as\nsegmentation, where labels must be assigned down to the level of individual\npoints. That challenge is even more daunting for 3D data: 3D point clouds\ncontain millions of points per scene, and their accurate annotation is markedly\nmore time-consuming. The situation is further aggravated by the added\ncomplexity of user interfaces for 3D point clouds, which slows down annotation\neven more. For the case of 2D image segmentation, interactive techniques have\nbecome common, where user feedback in the form of a few clicks guides a\nsegmentation algorithm -- nowadays usually a neural network -- to achieve an\naccurate labeling with minimal effort. Surprisingly, interactive segmentation\nof 3D scenes has not been explored much. Previous work has attempted to obtain\naccurate 3D segmentation masks using human feedback from the 2D domain, which\nis only possible if correctly aligned images are available together with the 3D\npoint cloud, and it involves switching between the 2D and 3D domains. Here, we\npresent an interactive 3D object segmentation method in which the user\ninteracts directly with the 3D point cloud. Importantly, our model does not\nrequire training data from the target domain: when trained on ScanNet, it\nperforms well on several other datasets with different data characteristics as\nwell as different object classes. Moreover, our method is orthogonal to\nsupervised (instance) segmentation methods and can be combined with them to\nrefine automatic segmentations with minimal human effort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kontogianni_T/0/1/0/all/0/1\">Theodora Kontogianni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikkan_E/0/1/0/all/0/1\">Ekin Celikkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Compositional Consistency for Video Question Answering. (arXiv:2204.07190v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07190","description":"<p>Recent video question answering benchmarks indicate that state-of-the-art\nmodels struggle to answer compositional questions. However, it remains unclear\nwhich types of compositional reasoning cause models to mispredict. Furthermore,\nit is difficult to discern whether models arrive at answers using compositional\nreasoning or by leveraging data biases. In this paper, we develop a question\ndecomposition engine that programmatically deconstructs a compositional\nquestion into a directed acyclic graph of sub-questions. The graph is designed\nsuch that each parent question is a composition of its children. We present\nAGQA-Decomp, a benchmark containing $2.3M$ question graphs, with an average of\n$11.49$ sub-questions per graph, and $4.55M$ total new sub-questions. Using\nquestion graphs, we evaluate three state-of-the-art models with a suite of\nnovel compositional consistency metrics. We find that models either cannot\nreason correctly through most compositions or are reliant on incorrect\nreasoning to reach answers, frequently contradicting themselves or achieving\nhigh accuracies when failing at intermediate reasoning steps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1\">Mona Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gul_M/0/1/0/all/0/1\">Mustafa Omer Gul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_E/0/1/0/all/0/1\">Eva Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grunde_McLaughlin_M/0/1/0/all/0/1\">Madeleine Grunde-McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawala_M/0/1/0/all/0/1\">Maneesh Agrawala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLGAN: Generative Adversarial Networks for Power-Line Segmentation in Aerial Images. (arXiv:2204.07243v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07243","description":"<p>Accurate segmentation of power lines in various aerial images is very\nimportant for UAV flight safety. The complex background and very thin\nstructures of power lines, however, make it an inherently difficult task in\ncomputer vision. This paper presents PLGAN, a simple yet effective method based\non generative adversarial networks, to segment power lines from aerial images\nwith different backgrounds. Instead of directly using the adversarial networks\nto generate the segmentation, we take their certain decoding features and embed\nthem into another semantic segmentation network by considering more context,\ngeometry, and appearance information of power lines. We further exploit the\nappropriate form of the generated images for high-quality feature embedding and\ndefine a new loss function in the Hough-transform parameter space to enhance\nthe segmentation of very thin power lines. Extensive experiments and\ncomprehensive analysis demonstrate that our proposed PLGAN outperforms the\nprior state-of-the-art methods for semantic segmentation and line detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelfattah_R/0/1/0/all/0/1\">Rabab Abdelfattah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robotic and Generative Adversarial Attacks in Offline Writer-independent Signature Verification. (arXiv:2204.07246v1 [cs.RO])","link":"http://arxiv.org/abs/2204.07246","description":"<p>This study explores how robots and generative approaches can be used to mount\nsuccessful false-acceptance adversarial attacks on signature verification\nsystems. Initially, a convolutional neural network topology and data\naugmentation strategy are explored and tuned, producing an 87.12% accurate\nmodel for the verification of 2,640 human signatures. Two robots are then\ntasked with forging 50 signatures, where 25 are used for the verification\nattack, and the remaining 25 are used for tuning of the model to defend against\nthem. Adversarial attacks on the system show that there exists an information\nsecurity risk; the Line-us robotic arm can fool the system 24% of the time and\nthe iDraw 2.0 robot 32% of the time. A conditional GAN finds similar success,\nwith around 30% forged signatures misclassified as genuine. Following fine-tune\ntransfer learning of robotic and generative data, adversarial attacks are\nreduced below the model threshold by both robots and the GAN. It is observed\nthat tuning the model reduces the risk of attack by robots to 8% and 12%, and\nthat conditional generative adversarial attacks can be reduced to 4% when 25\nimages are presented and 5% when 1000 images are presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bird_J/0/1/0/all/0/1\">Jordan J. Bird</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early Myocardial Infarction Detection with One-Class Classification over Multi-view Echocardiography. (arXiv:2204.07253v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07253","description":"<p>Myocardial infarction (MI) is the leading cause of mortality and morbidity in\nthe world. Early therapeutics of MI can ensure the prevention of further\nmyocardial necrosis. Echocardiography is the fundamental imaging technique that\ncan reveal the earliest sign of MI. However, the scarcity of echocardiographic\ndatasets for the MI detection is the major issue for training data-driven\nclassification algorithms. In this study, we propose a framework for early\ndetection of MI over multi-view echocardiography that leverages one-class\nclassification (OCC) techniques. The OCC techniques are used to train a model\nfor detecting a specific target class using instances from that particular\ncategory only. We investigated the usage of uni-modal and multi-modal one-class\nclassification techniques in the proposed framework using the HMC-QU dataset\nthat includes apical 4-chamber (A4C) and apical 2-chamber (A2C) views in a\ntotal of 260 echocardiography recordings. Experimental results show that the\nmulti-modal approach achieves a sensitivity level of 85.23% and F1-Score of\n80.21%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Degerli_A/0/1/0/all/0/1\">Aysen Degerli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sohrab_F/0/1/0/all/0/1\">Fahad Sohrab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imposing Consistency for Optical Flow Estimation. (arXiv:2204.07262v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07262","description":"<p>Imposing consistency through proxy tasks has been shown to enhance\ndata-driven learning and enable self-supervision in various tasks. This paper\nintroduces novel and effective consistency strategies for optical flow\nestimation, a problem where labels from real-world data are very challenging to\nderive. More specifically, we propose occlusion consistency and zero forcing in\nthe forms of self-supervised learning and transformation consistency in the\nform of semi-supervised learning. We apply these consistency techniques in a\nway that the network model learns to describe pixel-level motions better while\nrequiring no additional annotations. We demonstrate that our consistency\nstrategies applied to a strong baseline network model using the original\ndatasets and labels provide further improvements, attaining the\nstate-of-the-art results on the KITTI-2015 scene flow benchmark in the\nnon-stereo category. Our method achieves the best foreground accuracy (4.33% in\nFl-all) over both the stereo and non-stereo categories, even though using only\nmonocular image inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisoo Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jamie Menjay Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Spatially Varying Pixel Exposures for Motion Deblurring. (arXiv:2204.07267v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07267","description":"<p>Computationally removing the motion blur introduced by camera shake or object\nmotion in a captured image remains a challenging task in computational\nphotography. Deblurring methods are often limited by the fixed global exposure\ntime of the image capture process. The post-processing algorithm either must\ndeblur a longer exposure that contains relatively little noise or denoise a\nshort exposure that intentionally removes the opportunity for blur at the cost\nof increased noise. We present a novel approach of leveraging spatially varying\npixel exposures for motion deblurring using next-generation focal-plane\nsensor--processors along with an end-to-end design of these exposures and a\nmachine learning--based motion-deblurring framework. We demonstrate in\nsimulation and a physical prototype that learned spatially varying pixel\nexposures (L-SVPE) can successfully deblur scenes while recovering high\nfrequency detail. Our work illustrates the promising role that focal-plane\nsensor--processors can play in the future of computational imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_C/0/1/0/all/0/1\">Cindy M. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martel_J/0/1/0/all/0/1\">Julien N.P. Martel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-agnostic Multi-Domain Learning with Domain-Specific Adapters for Action Recognition. (arXiv:2204.07270v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07270","description":"<p>In this paper, we propose a multi-domain learning model for action\nrecognition. The proposed method inserts domain-specific adapters between\nlayers of domain-independent layers of a backbone network. Unlike a multi-head\nnetwork that switches classification heads only, our model switches not only\nthe heads, but also the adapters for facilitating to learn feature\nrepresentations universal to multiple domains. Unlike prior works, the proposed\nmethod is model-agnostic and doesn't assume model structures unlike prior\nworks. Experimental results on three popular action recognition datasets\n(HMDB51, UCF101, and Kinetics-400) demonstrate that the proposed method is more\neffective than a multi-head architecture and more efficient than separately\ntraining models for each domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Omi_K/0/1/0/all/0/1\">Kazuki Omi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamaki_T/0/1/0/all/0/1\">Toru Tamaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invisible-to-Visible: Privacy-Aware Human Instance Segmentation using Airborne Ultrasound via Collaborative Learning Variational Autoencoder. (arXiv:2204.07280v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07280","description":"<p>In action understanding in indoor, we have to recognize human pose and action\nconsidering privacy. Although camera images can be used for highly accurate\nhuman action recognition, camera images do not preserve privacy. Therefore, we\npropose a new task for human instance segmentation from invisible information,\nespecially airborne ultrasound, for action recognition. To perform instance\nsegmentation from invisible information, we first convert sound waves to\nreflected sound directional images (sound images). Although the sound images\ncan roughly identify the location of a person, the detailed shape is ambiguous.\nTo address this problem, we propose a collaborative learning variational\nautoencoder (CL-VAE) that simultaneously uses sound and RGB images during\ntraining. In inference, it is possible to obtain instance segmentation results\nonly from sound images. As a result of performance verification, CL-VAE could\nestimate human instance segmentations more accurately than conventional\nvariational autoencoder and some other models. Since this method can obtain\nhuman segmentations individually, it could be applied to human action\nrecognition tasks with privacy protection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanigawa_R/0/1/0/all/0/1\">Risako Tanigawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_Y/0/1/0/all/0/1\">Yasunori Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozuka_K/0/1/0/all/0/1\">Kazuki Kozuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_T/0/1/0/all/0/1\">Takayoshi Yamashita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guided Co-Modulated GAN for 360{\\deg} Field of View Extrapolation. (arXiv:2204.07286v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07286","description":"<p>We propose a method to extrapolate a 360{\\deg} field of view from a single\nimage that allows for user-controlled synthesis of the out-painted content. To\ndo so, we propose improvements to an existing GAN-based in-painting\narchitecture for out-painting panoramic image representation. Our method\nobtains state-of-the-art results and outperforms previous methods on standard\nimage quality metrics. To allow controlled synthesis of out-painting, we\nintroduce a novel guided co-modulation framework, which drives the image\ngeneration process with a common pretrained discriminative model. Doing so\nmaintains the high visual quality of generated panoramas while enabling\nuser-controlled semantic content in the extrapolated field of view. We\ndemonstrate the state-of-the-art results of our method on field of view\nextrapolation both qualitatively and quantitatively, providing thorough\nanalysis of our novel editing capabilities. Finally, we demonstrate that our\napproach benefits the photorealistic virtual insertion of highly glossy objects\nin photographs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dastjerdi_M/0/1/0/all/0/1\">Mohammad Reza Karimi Dastjerdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hold_Geoffroy_Y/0/1/0/all/0/1\">Yannick Hold-Geoffroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenmann_J/0/1/0/all/0/1\">Jonathan Eisenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodadadeh_S/0/1/0/all/0/1\">Siavash Khodadadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Lalonde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Learning based Semi-Supervised Object Detection. (arXiv:2204.07300v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07300","description":"<p>Semi-supervised object detection (SSOD) aims to facilitate the training and\ndeployment of object detectors with the help of a large amount of unlabeled\ndata. Though various self-training based and consistency-regularization based\nSSOD methods have been proposed, most of them are anchor-based detectors,\nignoring the fact that in many real-world applications anchor-free detectors\nare more demanded. In this paper, we intend to bridge this gap and propose a\nDenSe Learning (DSL) based anchor-free SSOD algorithm. Specifically, we achieve\nthis goal by introducing several novel techniques, including an Adaptive\nFiltering strategy for assigning multi-level and accurate dense pixel-wise\npseudo-labels, an Aggregated Teacher for producing stable and precise\npseudo-labels, and an uncertainty-consistency-regularization term among scales\nand shuffled patches for improving the generalization capability of the\ndetector. Extensive experiments are conducted on MS-COCO and PASCAL-VOC, and\nthe results show that our proposed DSL method records new state-of-the-art SSOD\nperformance, surpassing existing methods by a large margin. Codes can be found\nat \\textcolor{blue}{https://github.com/chenbinghui1/DSL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Binghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Biao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Cross-Modal Understanding in Visual Dialog via Contrastive Learning. (arXiv:2204.07302v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07302","description":"<p>Visual Dialog is a challenging vision-language task since the visual dialog\nagent needs to answer a series of questions after reasoning over both the image\ncontent and dialog history. Though existing methods try to deal with the\ncross-modal understanding in visual dialog, they are still not enough in\nranking candidate answers based on their understanding of visual and textual\ncontexts. In this paper, we analyze the cross-modal understanding in visual\ndialog based on the vision-language pre-training model VD-BERT and propose a\nnovel approach to improve the cross-modal understanding for visual dialog,\nnamed ICMU. ICMU enhances cross-modal understanding by distinguishing different\npulled inputs (i.e. pulled images, questions or answers) based on four-way\ncontrastive learning. In addition, ICMU exploits the single-turn visual\nquestion answering to enhance the visual dialog model's cross-modal\nunderstanding to handle a multi-turn visually-grounded conversation.\nExperiments show that the proposed approach improves the visual dialog model's\ncross-modal understanding and brings satisfactory gain to the VisDial dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference. (arXiv:2204.07305v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07305","description":"<p>Few-shot learning (FSL) is an important and topical problem in computer\nvision that has motivated extensive research into numerous methods spanning\nfrom sophisticated meta-learning methods to simple transfer learning baselines.\nWe seek to push the limits of a simple-but-effective pipeline for more\nrealistic and practical settings of few-shot image classification. To this end,\nwe explore few-shot learning from the perspective of neural network\narchitecture, as well as a three stage pipeline of network updates under\ndifferent data supplies, where unsupervised external data is considered for\npre-training, base categories are used to simulate few-shot tasks for\nmeta-training, and the scarcely labelled data of an novel task is taken for\nfine-tuning. We investigate questions such as: (1) How pre-training on external\ndata benefits FSL? (2) How state-of-the-art transformer architectures can be\nexploited? and (3) How fine-tuning mitigates domain shift? Ultimately, we show\nthat a simple transformer-based pipeline yields surprisingly good performance\non standard benchmarks such as Mini-ImageNet, CIFAR-FS, CDFSL and Meta-Dataset.\nOur code and demo are available at https://hushell.github.io/pmf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shell Xu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuhmer_J/0/1/0/all/0/1\">Jan St&#xfc;hmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy M. Hospedales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaSets: Meta-Learning on Point Sets for Generalizable Representations. (arXiv:2204.07311v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07311","description":"<p>Deep learning techniques for point clouds have achieved strong performance on\na range of 3D vision tasks. However, it is costly to annotate large-scale point\nsets, making it critical to learn generalizable representations that can\ntransfer well across different point sets. In this paper, we study a new\nproblem of 3D Domain Generalization (3DDG) with the goal to generalize the\nmodel to other unseen domains of point clouds without any access to them in the\ntraining process. It is a challenging problem due to the substantial geometry\nshift from simulated to real data, such that most existing 3D models\nunderperform due to overfitting the complete geometries in the source domain.\nWe propose to tackle this problem via MetaSets, which meta-learns point cloud\nrepresentations from a group of classification tasks on carefully-designed\ntransformed point sets containing specific geometry priors. The learned\nrepresentations are more generalizable to various unseen domains of different\ngeometries. We design two benchmarks for Sim-to-Real transfer of 3D point\nclouds. Experimental results show that MetaSets outperforms existing 3D deep\nlearning methods by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhangjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Compression for Rate Constrained Object Detection on the Edge. (arXiv:2204.07314v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07314","description":"<p>Recent advances in computer vision has led to a growth of interest in\ndeploying visual analytics model on mobile devices. However, most mobile\ndevices have limited computing power, which prohibits them from running large\nscale visual analytics neural networks. An emerging approach to solve this\nproblem is to offload the computation of these neural networks to computing\nresources at an edge server. Efficient computation offloading requires\noptimizing the trade-off between multiple objectives including compressed data\nrate, analytics performance, and computation speed. In this work, we consider a\n\"split computation\" system to offload a part of the computation of the YOLO\nobject detection model. We propose a learnable feature compression approach to\ncompress the intermediate YOLO features with light-weight computation. We train\nthe feature compression and decompression module together with the YOLO model\nto optimize the object detection accuracy under a rate constraint. Compared to\nbaseline methods that apply either standard image compression or learned image\ncompression at the mobile and perform image decompression and YOLO at the edge,\nthe proposed system achieves higher detection accuracy at the low to medium\nrate range. Furthermore, the proposed system requires substantially lower\ncomputation time on the mobile device with CPU only.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhongzheng Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rawlekar_S/0/1/0/all/0/1\">Samyak Rawlekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garg_S/0/1/0/all/0/1\">Siddharth Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Erkip_E/0/1/0/all/0/1\">Elza Erkip</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Keypoint-based Global Association Network for Lane Detection. (arXiv:2204.07335v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07335","description":"<p>Lane detection is a challenging task that requires predicting complex\ntopology shapes of lane lines and distinguishing different types of lanes\nsimultaneously. Earlier works follow a top-down roadmap to regress predefined\nanchors into various shapes of lane lines, which lacks enough flexibility to\nfit complex shapes of lanes due to the fixed anchor shapes. Lately, some works\npropose to formulate lane detection as a keypoint estimation problem to\ndescribe the shapes of lane lines more flexibly and gradually group adjacent\nkeypoints belonging to the same lane line in a point-by-point manner, which is\ninefficient and time-consuming during postprocessing. In this paper, we propose\na Global Association Network (GANet) to formulate the lane detection problem\nfrom a new perspective, where each keypoint is directly regressed to the\nstarting point of the lane line instead of point-by-point extension.\nConcretely, the association of keypoints to their belonged lane line is\nconducted by predicting their offsets to the corresponding starting points of\nlanes globally without dependence on each other, which could be done in\nparallel to greatly improve efficiency. In addition, we further propose a\nLane-aware Feature Aggregator (LFA), which adaptively captures the local\ncorrelations between adjacent keypoints to supplement local information to the\nglobal association. Extensive experiments on two popular lane detection\nbenchmarks show that our method outperforms previous methods with F1 score of\n79.63% on CULane and 97.71% on Tusimple dataset with high FPS. The code will be\nreleased at https://github.com/Wolfwjs/GANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinsheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yinchao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaofei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_T/0/1/0/all/0/1\">Tianrui Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianzhu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAiD: Context-Aware Instance Discrimination for Self-supervised Learning in Medical Imaging. (arXiv:2204.07344v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07344","description":"<p>Recently, self-supervised instance discrimination methods have achieved\nsignificant success in learning visual representations from unlabeled\nphotographic images. However, given the marked differences between photographic\nand medical images, the efficacy of instance-based objectives, focusing on\nlearning the most discriminative global features in the image (i.e., wheels in\nbicycle), remains unknown in medical imaging. Our preliminary analysis showed\nthat high global similarity of medical images in terms of anatomy hampers\ninstance discrimination methods for capturing a set of distinct features,\nnegatively impacting their performance on medical downstream tasks. To\nalleviate this limitation, we have developed a simple yet effective\nself-supervised framework, called Context-Aware instance Discrimination (CAiD).\nCAiD aims to improve instance discrimination learning by providing finer and\nmore discriminative information encoded from a diverse local context of\nunlabeled medical images. We conduct a systematic analysis to investigate the\nutility of the learned features from a three-pronged perspective: (i)\ngeneralizability and transferability, (ii) separability in the embedding space,\nand (iii) reusability. Our extensive experiments demonstrate that CAiD (1)\nenriches representations learned from existing instance discrimination methods;\n(2) delivers more discriminative features by adequately capturing finer\ncontextual information from individual medial images; and (3) improves\nreusability of low/mid-level features compared to standard instance\ndiscriminative methods. As open science, all codes and pre-trained models are\navailable on our GitHub page: https://github.com/JLiangLab/CAiD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Taher_M/0/1/0/all/0/1\">Mohammad Reza Hosseinzadeh Taher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haghighi_F/0/1/0/all/0/1\">Fatemeh Haghighi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gotway_M/0/1/0/all/0/1\">Michael B. Gotway</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1\">Jianming Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVSTER: Epipolar Transformer for Efficient Multi-View Stereo. (arXiv:2204.07346v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07346","description":"<p>Learning-based Multi-View Stereo (MVS) methods warp source images into the\nreference camera frustum to form 3D volumes, which are fused as a cost volume\nto be regularized by subsequent networks. The fusing step plays a vital role in\nbridging 2D semantics and 3D spatial associations. However, previous methods\nutilize extra networks to learn 2D information as fusing cues, underusing 3D\nspatial correlations and bringing additional computation costs. Therefore, we\npresent MVSTER, which leverages the proposed epipolar Transformer to learn both\n2D semantics and 3D spatial associations efficiently. Specifically, the\nepipolar Transformer utilizes a detachable monocular depth estimator to enhance\n2D semantics and uses cross-attention to construct data-dependent 3D\nassociations along epipolar line. Additionally, MVSTER is built in a cascade\nstructure, where entropy-regularized optimal transport is leveraged to\npropagate finer depth estimations in each stage. Extensive experiments show\nMVSTER achieves state-of-the-art reconstruction performance with significantly\nhigher efficiency: Compared with MVSNet and CasMVSNet, our MVSTER achieves 34%\nand 14% relative improvements on the DTU benchmark, with 80% and 51% relative\nreductions in running time. MVSTER also ranks first on Tanks&amp;Temples-Advanced\namong all published works. Code is released at https://github.com/JeffWang987.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1\">Fangbo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yun Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_X/0/1/0/all/0/1\">Xu Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yijia He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crowd counting with crowd attention convolutional neural network. (arXiv:2204.07347v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07347","description":"<p>Crowd counting is a challenging problem due to the scene complexity and scale\nvariation. Although deep learning has achieved great improvement in crowd\ncounting, scene complexity affects the judgement of these methods and they\nusually regard some objects as people mistakenly; causing potentially enormous\nerrors in the crowd counting result. To address the problem, we propose a novel\nend-to-end model called Crowd Attention Convolutional Neural Network (CAT-CNN).\nOur CAT-CNN can adaptively assess the importance of a human head at each pixel\nlocation by automatically encoding a confidence map. With the guidance of the\nconfidence map, the position of human head in estimated density map gets more\nattention to encode the final density map, which can avoid enormous\nmisjudgements effectively. The crowd count can be obtained by integrating the\nfinal density map. To encode a highly refined density map, the total crowd\ncount of each image is classified in a designed classification task and we\nfirst explicitly map the prior of the population-level category to feature\nmaps. To verify the efficiency of our proposed method, extensive experiments\nare conducted on three highly challenging datasets. Results establish the\nsuperiority of our method over many state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Wen Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zengfu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Condition-Invariant and Compact Visual Place Description by Convolutional Autoencoder. (arXiv:2204.07350v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07350","description":"<p>Visual place recognition (VPR) in condition-varying environments is still an\nopen problem. Popular solutions are CNN-based image descriptors, which have\nbeen shown to outperform traditional image descriptors based on hand-crafted\nvisual features. However, there are two drawbacks of current CNN-based\ndescriptors: a) their high dimension and b) lack of generalization, leading to\nlow efficiency and poor performance in applications. In this paper, we propose\nto use a convolutional autoencoder (CAE) to tackle this problem. We employ a\nhigh-level layer of a pre-trained CNN to generate features, and train a CAE to\nmap the features to a low-dimensional space to improve the condition invariance\nproperty of the descriptor and reduce its dimension at the same time. We verify\nour method in three challenging datasets involving significant illumination\nchanges, and our method is shown to be superior to the state-of-the-art. For\nthe benefit of the community, we make public the source code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hanjing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weinan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingwen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Li He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yisheng Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07356","description":"<p>Pretrained models have produced great success in both Computer Vision (CV)\nand Natural Language Processing (NLP). This progress leads to learning joint\nrepresentations of vision and language pretraining by feeding visual and\nlinguistic contents into a multi-layer transformer, Visual-Language Pretrained\nModels (VLPMs). In this paper, we present an overview of the major advances\nachieved in VLPMs for producing joint representations of vision and language.\nAs the preliminaries, we briefly describe the general task definition and\ngenetic architecture of VLPMs. We first discuss the language and vision data\nencoding methods and then present the mainstream VLPM structure as the core\ncontent. We further summarise several essential pretraining and fine-tuning\nstrategies. Finally, we highlight three future directions for both CV and NLP\nresearchers to provide insightful guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_F/0/1/0/all/0/1\">Feiqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqing Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ResT V2: Simpler, Faster and Stronger. (arXiv:2204.07366v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07366","description":"<p>This paper proposes ResTv2, a simpler, faster, and stronger multi-scale\nvision Transformer for visual recognition. ResTv2 simplifies the EMSA structure\nin ResTv1 (i.e., eliminating the multi-head interaction part) and employs an\nupsample operation to reconstruct the lost medium- and high-frequency\ninformation caused by the downsampling operation. In addition, we explore\ndifferent techniques for better apply ResTv2 backbones to downstream tasks. We\nfound that although combining EMSAv2 and window attention can greatly reduce\nthe theoretical matrix multiply FLOPs, it may significantly decrease the\ncomputation density, thus causing lower actual speed. We comprehensively\nvalidate ResTv2 on ImageNet classification, COCO detection, and ADE20K semantic\nsegmentation. Experimental results show that the proposed ResTv2 can outperform\nthe recently state-of-the-art backbones by a large margin, demonstrating the\npotential of ResTv2 as solid backbones. The code and models will be made\npublicly available at \\url{https://github.com/wofmanaf/ResT}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing-Long Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu-Bin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2D Human Pose Estimation: A Survey. (arXiv:2204.07370v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07370","description":"<p>Human pose estimation aims at localizing human anatomical keypoints or body\nparts in the input data (e.g., images, videos, or signals). It forms a crucial\ncomponent in enabling machines to have an insightful understanding of the\nbehaviors of humans, and has become a salient problem in computer vision and\nrelated fields. Deep learning techniques allow learning feature representations\ndirectly from the data, significantly pushing the performance boundary of human\npose estimation. In this paper, we reap the recent achievements of 2D human\npose estimation methods and present a comprehensive survey. Briefly, existing\napproaches put their efforts in three directions, namely network architecture\ndesign, network training refinement, and post processing. Network architecture\ndesign looks at the architecture of human pose estimation models, extracting\nmore robust features for keypoint recognition and localization. Network\ntraining refinement tap into the training of neural networks and aims to\nimprove the representational ability of models. Post processing further\nincorporates model-agnostic polishing strategies to improve the performance of\nkeypoint detection. More than 200 research contributions are involved in this\nsurvey, covering methodological frameworks, common benchmark datasets,\nevaluation metrics, and performance comparisons. We seek to provide researchers\nwith a more comprehensive and systematic review on human pose estimation,\nallowing them to acquire a grand panorama and better identify future\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Runyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sifan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fengcheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Adversarial Robustness-Accuracy Tradeoff in Robot Learning. (arXiv:2204.07373v1 [cs.RO])","link":"http://arxiv.org/abs/2204.07373","description":"<p>Adversarial training (i.e., training on adversarially perturbed input data)\nis a well-studied method for making neural networks robust to potential\nadversarial attacks during inference. However, the improved robustness does not\ncome for free but rather is accompanied by a decrease in overall model accuracy\nand performance. Recent work has shown that, in practical robot learning\napplications, the effects of adversarial training do not pose a fair trade-off\nbut inflict a net loss when measured in holistic robot performance. This work\nrevisits the robustness-accuracy trade-off in robot learning by systematically\nanalyzing if recent advances in robust training methods and theory in\nconjunction with adversarial robot learning can make adversarial training\nsuitable for real-world robot applications. We evaluate a wide variety of robot\nlearning tasks ranging from autonomous driving in a high-fidelity environment\namenable to sim-to-real deployment, to mobile robot gesture recognition. Our\nresults demonstrate that, while these techniques make incremental improvements\non the trade-off on a relative scale, the negative side-effects caused by\nadversarial training still outweigh the improvements by an order of magnitude.\nWe conclude that more substantial advances in robust learning methods are\nnecessary before they can benefit robot learning tasks in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1\">Mathias Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1\">Thomas A. Henzinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Captioning In the Transformer Age. (arXiv:2204.07374v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07374","description":"<p>Image Captioning (IC) has achieved astonishing developments by incorporating\nvarious techniques into the CNN-RNN encoder-decoder architecture. However,\nsince CNN and RNN do not share the basic network component, such a\nheterogeneous pipeline is hard to be trained end-to-end where the visual\nencoder will not learn anything from the caption supervision. This drawback\ninspires the researchers to develop a homogeneous architecture that facilitates\nend-to-end training, for which Transformer is the perfect one that has proven\nits huge potential in both vision and language domains and thus can be used as\nthe basic component of the visual encoder and language decoder in an IC\npipeline. Meantime, self-supervised learning releases the power of the\nTransformer architecture that a pre-trained large-scale one can be generalized\nto various tasks including IC. The success of these large-scale models seems to\nweaken the importance of the single IC task. However, we demonstrate that IC\nstill has its specific significance in this age by analyzing the connections\nbetween IC with some popular self-supervised learning paradigms. Due to the\npage limitation, we only refer to highly important papers in this short survey\nand more related works can be found at\nhttps://github.com/SjokerLily/awesome-image-captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crowd counting with segmentation attention convolutional neural network. (arXiv:2204.07380v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07380","description":"<p>Deep learning occupies an undisputed dominance in crowd counting. In this\npaper, we propose a novel convolutional neural network (CNN) architecture\ncalled SegCrowdNet. Despite the complex background in crowd scenes, the\nproposeSegCrowdNet still adaptively highlights the human head region and\nsuppresses the non-head region by segmentation. With the guidance of an\nattention mechanism, the proposed SegCrowdNet pays more attention to the human\nhead region and automatically encodes the highly refined density map. The crowd\ncount can be obtained by integrating the density map. To adapt the variation of\ncrowd counts, SegCrowdNet intelligently classifies the crowd count of each\nimage into several groups. In addition, the multi-scale features are learned\nand extracted in the proposed SegCrowdNet to overcome the scale variations of\nthe crowd. To verify the effectiveness of our proposed method, extensive\nexperiments are conducted on four challenging datasets. The results demonstrate\nthat our proposed SegCrowdNet achieves excellent performance compared with the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zengfu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FasterVideo: Efficient Online Joint Object Detection And Tracking. (arXiv:2204.07394v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07394","description":"<p>Object detection and tracking in videos represent essential and\ncomputationally demanding building blocks for current and future visual\nperception systems. In order to reduce the efficiency gap between available\nmethods and computational requirements of real-world applications, we propose\nto re-think one of the most successful methods for image object detection,\nFaster R-CNN, and extend it to the video domain. Specifically, we extend the\ndetection framework to learn instance-level embeddings which prove beneficial\nfor data association and re-identification purposes. Focusing on the\ncomputational aspects of detection and tracking, our proposed method reaches a\nvery high computational efficiency necessary for relevant applications, while\nstill managing to compete with recent and state-of-the-art methods as shown in\nthe experiments we conduct on standard object tracking benchmarks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mouawad_I/0/1/0/all/0/1\">Issa Mouawad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odone_F/0/1/0/all/0/1\">Francesca Odone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSR-HEF: Crowd Counting with Multi-Scale Semantic Refining and Hard Example Focusing. (arXiv:2204.07406v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07406","description":"<p>Crowd counting based on density maps is generally regarded as a regression\ntask.Deep learning is used to learn the mapping between image content and crowd\ndensity distribution. Although great success has been achieved, some\npedestrians far away from the camera are difficult to be detected. And the\nnumber of hard examples is often larger. Existing methods with simple Euclidean\ndistance algorithm indiscriminately optimize the hard and easy examples so that\nthe densities of hard examples are usually incorrectly predicted to be lower or\neven zero, which results in large counting errors. To address this problem, we\nare the first to propose the Hard Example Focusing(HEF) algorithm for the\nregression task of crowd counting. The HEF algorithm makes our model rapidly\nfocus on hard examples by attenuating the contribution of easy examples.Then\nhigher importance will be given to the hard examples with wrong estimations.\nMoreover, the scale variations in crowd scenes are large, and the scale\nannotations are labor-intensive and expensive. By proposing a multi-Scale\nSemantic Refining (SSR) strategy, lower layers of our model can break through\nthe limitation of deep learning to capture semantic features of different\nscales to sufficiently deal with the scale variation. We perform extensive\nexperiments on six benchmark datasets to verify the proposed method. Results\nindicate the superiority of our proposed method over the state-of-the-art\nmethods. Moreover, our designed model is smaller and faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kewei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Wen Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zengfu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Sensitivity-Based Filter Pruning. (arXiv:2204.07412v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07412","description":"<p>In this paper, we present a novel sensitivity-based filter pruning algorithm\n(SbF-Pruner) to learn the importance scores of filters of each layer\nend-to-end. Our method learns the scores from the filter weights, enabling it\nto account for the correlations between the filters of each layer. Moreover, by\ntraining the pruning scores of all layers simultaneously our method can account\nfor layer interdependencies, which is essential to find a performant sparse\nsub-network. Our proposed method can train and generate a pruned network from\nscratch in a straightforward, one-stage training process without requiring a\npretrained network. Ultimately, we do not need layer-specific hyperparameters\nand pre-defined layer budgets, since SbF-Pruner can implicitly determine the\nappropriate number of channels in each layer. Our experimental results on\ndifferent network architectures suggest that SbF-Pruner outperforms advanced\npruning methods. Notably, on CIFAR-10, without requiring a pretrained baseline\nnetwork, we obtain 1.02% and 1.19% accuracy gain on ResNet56 and ResNet110,\ncompared to the baseline reported for state-of-the-art pruning algorithms. This\nis while SbF-Pruner reduces parameter-count by 52.3% (for ResNet56) and 54%\n(for ResNet101), which is better than the state-of-the-art pruning algorithms\nwith a high margin of 9.5% and 6.6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Babaiee_Z/0/1/0/all/0/1\">Zahra Babaiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1\">Lucas Liebenwein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1\">Radu Grosu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOTVerse: A User-defined Task Space of Single Object Tracking. (arXiv:2204.07414v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07414","description":"<p>Single object tracking (SOT) research falls into a cycle - trackers perform\nwell on most benchmarks but quickly fail in challenging scenarios, causing\nresearchers to doubt the insufficient data content and take more effort\nconstructing larger datasets with more challenging situations. However,\nisolated experimental environments and limited evaluation methods more\nseriously hinder the SOT research. The former causes existing datasets can not\nbe exploited comprehensively, while the latter neglects challenging factors in\nthe evaluation process. In this article, we systematize the representative\nbenchmarks and form a single object tracking metaverse (SOTVerse) - a\nuser-defined SOT task space to break through the bottleneck. We first propose a\n3E Paradigm to describe tasks by three components (i.e., environment,\nevaluation, and executor). Then, we summarize task characteristics, clarify the\norganization standards, and construct SOTVerse with 12.56 million frames.\nSpecifically, SOTVerse automatically labels challenging factors per frame,\nallowing users to generate user-defined spaces efficiently via construction\nrules. Besides, SOTVerse provides two mechanisms with new indicators and\nsuccessfully evaluates trackers under various subtasks. Consequently, SOTVerse\nfirstly provides a strategy to improve resource utilization in the computer\nvision area, making research more standardized and scientific. The SOTVerse,\ntoolkit, evaluation server, and results are available at\n<a href=\"http://metaverse.aitestunion.com.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shiyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiqi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep CardioSound: An Ensembled Deep Learning Model for Heart Sound MultiLabelling. (arXiv:2204.07420v1 [cs.SD])","link":"http://arxiv.org/abs/2204.07420","description":"<p>Heart sound diagnosis and classification play an essential role in detecting\ncardiovascular disorders, especially when the remote diagnosis becomes standard\nclinical practice. Most of the current work is designed for single category\nbased heard sound classification tasks. To further extend the landscape of the\nautomatic heart sound diagnosis landscape, this work proposes a deep multilabel\nlearning model that can automatically annotate heart sound recordings with\nlabels from different label groups, including murmur's timing, pitch, grading,\nquality, and shape. Our experiment results show that the proposed method has\nachieved outstanding performance on the holdout data for the multi-labelling\ntask with sensitivity=0.990, specificity=0.999, F1=0.990 at the segments level,\nand an overall accuracy=0.969 at the patient's recording level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Li Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davenport_S/0/1/0/all/0/1\">Steven Davenport</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yonghong Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning for Instance Segmentation of Waste Bottles using Mask R-CNN Algorithm. (arXiv:2204.07437v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07437","description":"<p>This paper proposes a methodological approach with a transfer learning scheme\nfor plastic waste bottle detection and instance segmentation using the\n\\textit{mask region proposal convolutional neural network} (Mask R-CNN).\nPlastic bottles constitute one of the major pollutants posing a serious threat\nto the environment both in oceans and on land. The automated identification and\nsegregation of bottles can facilitate plastic waste recycling. We prepare a\ncustom-made dataset of 192 bottle images with pixel-by pixel-polygon annotation\nfor the automatic segmentation task. The proposed transfer learning scheme\nmakes use of a Mask R-CNN model pre-trained on the Microsoft COCO dataset. We\npresent a comprehensive scheme for fine-tuning the base pre-trained Mask-RCNN\nmodel on our custom dataset. Our final fine-tuned model has achieved 59.4\n\\textit{mean average precision} (mAP), which corresponds to the MS COCO metric.\nThe results indicate a promising application of deep learning for detecting\nwaste bottles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaikumar_P/0/1/0/all/0/1\">Punitha Jaikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandaele_R/0/1/0/all/0/1\">Remy Vandaele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojha_V/0/1/0/all/0/1\">Varun Ojha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold. (arXiv:2204.07439v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07439","description":"<p>Binary Neural Networks (BNNs) have emerged as a promising solution for\nreducing the memory footprint and compute costs of deep neural networks. BNNs,\non the other hand, suffer from information loss because binary activations are\nlimited to only two values, resulting in reduced accuracy. To improve the\naccuracy, previous studies have attempted to control the distribution of binary\nactivation by manually shifting the threshold of the activation function or\nmaking the shift amount trainable. During the process, they usually depended on\nstatistical information computed from a batch. We argue that using statistical\ndata from a batch fails to capture the crucial information for each input\ninstance in BNN computations, and the differences between statistical\ninformation computed from each instance need to be considered when determining\nthe binary activation threshold of each instance. Based on the concept, we\npropose the Binary Neural Network with INSTAnce-aware threshold (INSTA-BNN),\nwhich decides the activation threshold value considering the difference between\nstatistical data computed from a batch and each instance. The proposed\nINSTA-BNN outperforms the baseline by 2.5% and 2.3% on the ImageNet\nclassification task with comparable computing cost, achieving 68.0% and 71.7%\ntop-1 accuracy on ResNet-18 and MobileNetV1 based models, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Changhun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyungjun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1\">Eunhyeok Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jae-Joon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval. (arXiv:2204.07441v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07441","description":"<p>Large-scale single-stream pre-training has shown dramatic performance in\nimage-text retrieval. Regrettably, it faces low inference efficiency due to\nheavy attention layers. Recently, two-stream methods like CLIP and ALIGN with\nhigh inference efficiency have also shown promising performance, however, they\nonly consider instance-level alignment between the two streams (thus there is\nstill room for improvement). To overcome these limitations, we propose a novel\nCOllaborative Two-Stream vision-language pretraining model termed COTS for\nimage-text retrieval by enhancing cross-modal interaction. In addition to\ninstance level alignment via momentum contrastive learning, we leverage two\nextra levels of cross-modal interactions in our COTS: (1) Token-level\ninteraction - a masked visionlanguage modeling (MVLM) learning objective is\ndevised without using a cross-stream network module, where variational\nautoencoder is imposed on the visual encoder to generate visual tokens for each\nimage. (2) Task-level interaction - a KL-alignment learning objective is\ndevised between text-to-image and image-to-text retrieval tasks, where the\nprobability distribution per task is computed with the negative queues in\nmomentum contrastive learning. Under a fair comparison setting, our COTS\nachieves the highest performance among all two-stream methods and comparable\nperformance (but with 10,800X faster in inference) w.r.t. the latest\nsingle-stream methods. Importantly, our COTS is also applicable to\ntext-to-video retrieval, yielding new state-ofthe-art on the widely-used\nMSR-VTT dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haoyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_N/0/1/0/all/0/1\">Nanyi Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuqi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yizhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiwu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable and Real-time Multi-Camera Vehicle Detection, Re-Identification, and Tracking. (arXiv:2204.07442v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07442","description":"<p>Multi-camera vehicle tracking is one of the most complicated tasks in\nComputer Vision as it involves distinct tasks including Vehicle Detection,\nTracking, and Re-identification. Despite the challenges, multi-camera vehicle\ntracking has immense potential in transportation applications including speed,\nvolume, origin-destination (O-D), and routing data generation. Several recent\nworks have addressed the multi-camera tracking problem. However, most of the\neffort has gone towards improving accuracy on high-quality benchmark datasets\nwhile disregarding lower camera resolutions, compression artifacts and the\noverwhelming amount of computational power and time needed to carry out this\ntask on its edge and thus making it prohibitive for large-scale and real-time\ndeployment. Therefore, in this work we shed light on practical issues that\nshould be addressed for the design of a multi-camera tracking system to provide\nactionable and timely insights. Moreover, we propose a real-time city-scale\nmulti-camera vehicle tracking system that compares favorably to computationally\nintensive alternatives and handles real-world, low-resolution CCTV instead of\nidealized and curated video streams. To show its effectiveness, in addition to\nintegration into the Regional Integrated Transportation Information System\n(RITIS), we participated in the 2021 NVIDIA AI City multi-camera tracking\nchallenge and our method is ranked among the top five performers on the public\nleaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khorramshahi_P/0/1/0/all/0/1\">Pirazh Khorramshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_V/0/1/0/all/0/1\">Vineet Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pack_M/0/1/0/all/0/1\">Michael Pack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Violence in Video Based on Deep Features Fusion Technique. (arXiv:2204.07443v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07443","description":"<p>With the rapid growth of surveillance cameras in many public places to\nmon-itor human activities such as in malls, streets, schools and, prisons,\nthere is a strong demand for such systems to detect violence events\nautomatically. Au-tomatic analysis of video to detect violence is significant\nfor law enforce-ment. Moreover, it helps to avoid any social, economic and\nenvironmental damages. Mostly, all systems today require manual human\nsupervisors to de-tect violence scenes in the video which is inefficient and\ninaccurate. in this work, we interest in physical violence that involved two\npersons or more. This work proposed a novel method to detect violence using a\nfusion tech-nique of two significantly different convolutional neural networks\n(CNNs) which are AlexNet and SqueezeNet networks. Each network followed by\nseparate Convolution Long Short Term memory (ConvLSTM) to extract ro-bust and\nricher features from a video in the final hidden state. Then, making a fusion\nof these two obtained states and fed to the max-pooling layer. Final-ly,\nfeatures were classified using a series of fully connected layers and soft-max\nclassifier. The performance of the proposed method is evaluated using three\nstandard benchmark datasets in terms of detection accuracy: Hockey Fight\ndataset, Movie dataset and Violent Flow dataset. The results show an accuracy\nof 97%, 100%, and 96% respectively. A comparison of the results with the state\nof the art techniques revealed the promising capability of the proposed method\nin recognizing violent videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahlan_H/0/1/0/all/0/1\">Heyam M. Bin Jahlan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elrefaei_L/0/1/0/all/0/1\">Lamiaa A. Elrefaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Aware Pretraining for Dense Video Captioning. (arXiv:2204.07449v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07449","description":"<p>This report describes the details of our approach for the event\ndense-captioning task in ActivityNet Challenge 2021. We present a\nsemantic-aware pretraining method for dense video captioning, which empowers\nthe learned features to recognize high-level semantic concepts. Diverse video\nfeatures of different modalities are fed into an event captioning module to\ngenerate accurate and meaningful sentences. Our final ensemble model achieves a\n10.00 METEOR score on the test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ran Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ORCNet: A context-based network to simultaneously segment the ocular region components. (arXiv:2204.07456v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07456","description":"<p>Accurate extraction of the Region of Interest is critical for successful\nocular region-based biometrics. In this direction, we propose a new\ncontext-based segmentation approach, entitled Ocular Region Context Network\n(ORCNet), introducing a specific loss function, i.e., he Punish Context Loss\n(PC-Loss). The PC-Loss punishes the segmentation losses of a network by using a\npercentage difference value between the ground truth and the segmented masks.\nWe obtain the percentage difference by taking into account Biederman's semantic\nrelationship concepts, in which we use three contexts (semantic, spatial, and\nscale) to evaluate the relationships of the objects in an image. Our proposal\nachieved promising results in the evaluated scenarios: iris, sclera, and ALL\n(iris + sclera) segmentations, utperforming the literature baseline techniques.\nThe ORCNet with ResNet-152 outperforms the best baseline (EncNet with\nResNet-152) on average by 2.27%, 28.26% and 6.43% in terms of F-Score, Error\nRate and Intersection Over Union, respectively. We also provide (for research\npurposes) 3,191 manually labeled masks for the MICHE-I database, as another\ncontribution of our work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lucio_D/0/1/0/all/0/1\">Diego Rafael Lucio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanlorensi_L/0/1/0/all/0/1\">Luiz A. Zanlorensi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_Y/0/1/0/all/0/1\">Yandre Maldonado e Gomes da Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensitivity of sparse codes to image distortions. (arXiv:2204.07466v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07466","description":"<p>Sparse coding has been proposed as a theory of visual cortex and as an\nunsupervised algorithm for learning representations. We show empirically with\nthe MNIST dataset that sparse codes can be very sensitive to image distortions,\na behavior that may hinder invariant object recognition. A locally linear\nanalysis suggests that the sensitivity is due to the existence of linear\ncombinations of active dictionary elements with high cancellation. A nearest\nneighbor classifier is shown to perform worse on sparse codes than original\nimages. For a linear classifier with a sufficiently large number of labeled\nexamples, sparse codes are shown to yield higher accuracy than original images,\nbut no higher than a representation computed by a random feedforward net.\nSensitivity to distortions seems to be a basic property of sparse codes, and\none should be aware of this property when applying sparse codes to invariant\nobject recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luther_K/0/1/0/all/0/1\">Kyle Luther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seung_H/0/1/0/all/0/1\">H. Sebastian Seung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Attention using Partial-Order Relationships for Image Captioning. (arXiv:2204.07476v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07476","description":"<p>The use of attention models for automated image captioning has enabled many\nsystems to produce accurate and meaningful descriptions for images. Over the\nyears, many novel approaches have been proposed to enhance the attention\nprocess using different feature representations. In this paper, we extend this\napproach by creating a guided attention network mechanism, that exploits the\nrelationship between the visual scene and text-descriptions using spatial\nfeatures from the image, high-level information from the topics, and temporal\ncontext from caption generation, which are embedded together in an ordered\nembedding space. A pairwise ranking objective is used for training this\nembedding space which allows similar images, topics and captions in the shared\nsemantic space to maintain a partial order in the visual-semantic hierarchy and\nhence, helps the model to produce more visually accurate captions. The\nexperimental results based on MSCOCO dataset shows the competitiveness of our\napproach, with many state-of-the-art models on various evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popattia_M/0/1/0/all/0/1\">Murad Popattia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafi_M/0/1/0/all/0/1\">Muhammad Rafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qureshi_R/0/1/0/all/0/1\">Rizwan Qureshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nawaz_S/0/1/0/all/0/1\">Shah Nawaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards PAC Multi-Object Detection and Tracking. (arXiv:2204.07482v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07482","description":"<p>Accurately detecting and tracking multi-objects is important for\nsafety-critical applications such as autonomous navigation. However, it remains\nchallenging to provide guarantees on the performance of state-of-the-art\ntechniques based on deep learning. We consider a strategy known as conformal\nprediction, which predicts sets of labels instead of a single label; in the\nclassification and regression settings, these algorithms can guarantee that the\ntrue label lies within the prediction set with high probability. Building on\nthese ideas, we propose multi-object detection and tracking algorithms that\ncome with probably approximately correct (PAC) guarantees. They do so by\nconstructing both a prediction set around each object detection as well as\naround the set of edge transitions; given an object, the detection prediction\nset contains its true bounding box with high probability, and the edge\nprediction set contains its true transition across frames with high\nprobability. We empirically demonstrate that our method can detect and track\nobjects with PAC guarantees on the COCO and MOT-17 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sangdon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiayan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Insup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1\">Osbert Bastani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-wise Contrastive Style Learning for Instagram Filter Removal. (arXiv:2204.07486v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07486","description":"<p>Image-level corruptions and perturbations degrade the performance of CNNs on\ndifferent downstream vision tasks. Social media filters are one of the most\ncommon resources of various corruptions and perturbations for real-world visual\nanalysis applications. The negative effects of these distractive factors can be\nalleviated by recovering the original images with their pure style for the\ninference of the downstream vision tasks. Assuming these filters substantially\ninject a piece of additional style information to the social media images, we\ncan formulate the problem of recovering the original versions as a reverse\nstyle transfer problem. We introduce Contrastive Instagram Filter Removal\nNetwork (CIFR), which enhances this idea for Instagram filter removal by\nemploying a novel multi-layer patch-wise contrastive style learning mechanism.\nExperiments show our proposed strategy produces better qualitative and\nquantitative results than the previous studies. Moreover, we present the\nresults of our additional experiments for proposed architecture within\ndifferent settings. Finally, we present the inference outputs and quantitative\ncomparison of filtered and recovered images on localization and segmentation\ntasks to encourage the main motivation for this problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kinli_F/0/1/0/all/0/1\">Furkan K&#x131;nl&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozcan_B/0/1/0/all/0/1\">Bar&#x131;&#x15f; &#xd6;zcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirac_F/0/1/0/all/0/1\">Furkan K&#x131;ra&#xe7;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthesizing Informative Training Samples with GAN. (arXiv:2204.07513v1 [cs.LG])","link":"http://arxiv.org/abs/2204.07513","description":"<p>Remarkable progress has been achieved in synthesizing photo-realistic images\nwith generative adversarial neural networks (GANs). Recently, GANs are utilized\nas the training sample generator when obtaining or storing real training data\nis expensive even infeasible. However, traditional GANs generated images are\nnot as informative as the real training samples when being used to train deep\nneural networks. In this paper, we propose a novel method to synthesize\nInformative Training samples with GAN (IT-GAN). Specifically, we freeze a\npre-trained GAN model and learn the informative latent vectors that corresponds\nto informative training samples. The synthesized images are required to\npreserve information for training deep neural networks rather than visual\nreality or fidelity. Experiments verify that the deep neural networks can learn\nfaster and achieve better performance when being trained with our IT-GAN\ngenerated images. We also show that our method is a promising solution to\ndataset condensation problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unconditional Image-Text Pair Generation with Multimodal Cross Quantizer. (arXiv:2204.07537v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07537","description":"<p>Though deep generative models have gained a lot of attention, most of the\nexisting works are designed for the unimodal generation task. In this paper, we\nexplore a new method for unconditional image-text pair generation. We propose\nMXQ-VAE, a vector quantization method for multimodal image-text representation.\nMXQ-VAE accepts a paired image and text as input, and learns a joint quantized\nrepresentation space, so that the image-text pair can be converted to a\nsequence of unified indices. Then we can use autoregressive generative models\nto model the joint image-text representation, and even perform unconditional\nimage-text pair generation. Extensive experimental results demonstrate that our\napproach effectively generates semantically consistent image-text pair and also\nenhances meaningful alignment between image and text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised atmospheric component learning in low-light image problem. (arXiv:2204.07546v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07546","description":"<p>Ambient lighting conditions play a crucial role in determining the perceptual\nquality of images from photographic devices. In general, inadequate\ntransmission light and undesired atmospheric conditions jointly degrade the\nimage quality. If we know the desired ambient factors associated with the given\nlow-light image, we can recover the enhanced image easily \\cite{b1}. Typical\ndeep networks perform enhancement mappings without investigating the light\ndistribution and color formulation properties. This leads to a lack of image\ninstance-adaptive performance in practice. On the other hand, physical\nmodel-driven schemes suffer from the need for inherent decompositions and\nmultiple objective minimizations. Moreover, the above approaches are rarely\ndata efficient or free of postprediction tuning. Influenced by the above\nissues, this study presents a semisupervised training method using no-reference\nimage quality metrics for low-light image restoration. We incorporate the\nclassical haze distribution model \\cite{b2} to explore the physical properties\nof the given image in order to learn the effect of atmospheric components and\nminimize a single objective for restoration. We validate the performance of our\nnetwork for six widely used low-light datasets. The experiments show that the\nproposed study achieves state-of-the-art or comparable performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fahim_M/0/1/0/all/0/1\">Masud An Nur Islam Fahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqib_N/0/1/0/all/0/1\">Nazmus Saqib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yub_J/0/1/0/all/0/1\">Jung Ho Yub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation. (arXiv:2204.07548v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07548","description":"<p>Recent works on 3D semantic segmentation propose to exploit the synergy\nbetween images and point clouds by processing each modality with a dedicated\nnetwork and projecting learned 2D features onto 3D points. Merging large-scale\npoint clouds and images raises several challenges, such as constructing a\nmapping between points and pixels, and aggregating features between multiple\nviews. Current methods require mesh reconstruction or specialized sensors to\nrecover occlusions, and use heuristics to select and aggregate available\nimages. In contrast, we propose an end-to-end trainable multi-view aggregation\nmodel leveraging the viewing conditions of 3D points to merge features from\nimages taken at arbitrary positions. Our method can combine standard 2D and 3D\nnetworks and outperforms both 3D models operating on colorized point clouds and\nhybrid 2D/3D networks without requiring colorization, meshing, or true depth\nmaps. We set a new state-of-the-art for large-scale indoor/outdoor semantic\nsegmentation on S3DIS (74.7 mIoU 6-Fold) and on KITTI-360 (58.3 mIoU). Our full\npipeline is accessible at https://github.com/drprojects/DeepViewAgg, and only\nrequires raw 3D scans and a set of images and poses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robert_D/0/1/0/all/0/1\">Damien Robert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallet_B/0/1/0/all/0/1\">Bruno Vallet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PL-VINS: Real-Time Monocular Visual-Inertial SLAM with Point and Line Features. (arXiv:2009.07462v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2009.07462","description":"<p>Leveraging line features to improve localization accuracy of point-based\nvisual-inertial SLAM (VINS) is gaining interest as they provide additional\nconstraints on scene structure. However, real-time performance when\nincorporating line features in VINS has not been addressed. This paper presents\nPL-VINS, a real-time optimization-based monocular VINS method with point and\nline features, developed based on the state-of-the-art point-based VINS-Mono\n\\cite{vins}. We observe that current works use the LSD \\cite{lsd} algorithm to\nextract line features; however, LSD is designed for scene shape representation\ninstead of the pose estimation problem, which becomes the bottleneck for the\nreal-time performance due to its high computational cost. In this paper, a\nmodified LSD algorithm is presented by studying a hidden parameter tuning and\nlength rejection strategy. The modified LSD can run at least three times as\nfast as LSD. Further, by representing space lines with the Pl\\\"{u}cker\ncoordinates, the residual error in line estimation is modeled in terms of the\npoint-to-line distance, which is then minimized by iteratively updating the\nminimum four-parameter orthonormal representation of the Pl\\\"{u}cker\ncoordinates. Experiments in a public benchmark dataset show that the\nlocalization error of our method is 12-16\\% less than that of VINS-Mono at the\nsame pose update frequency. %For the benefit of the community, The source code\nof our method is available at: https://github.com/cnqiangfu/PL-VINS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongshan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_I/0/1/0/all/0/1\">Islam Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Feng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yijia He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can You Spot the Chameleon? Adversarially Camouflaging Images from Co-Salient Object Detection. (arXiv:2009.09258v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.09258","description":"<p>Co-salient object detection (CoSOD) has recently achieved significant\nprogress and played a key role in retrieval-related tasks. However, it\ninevitably poses an entirely new safety and security issue, i.e., highly\npersonal and sensitive content can potentially be extracting by powerful CoSOD\nmethods. In this paper, we address this problem from the perspective of\nadversarial attacks and identify a novel task: adversarial co-saliency attack.\nSpecially, given an image selected from a group of images containing some\ncommon and salient objects, we aim to generate an adversarial version that can\nmislead CoSOD methods to predict incorrect co-salient regions. Note that,\ncompared with general white-box adversarial attacks for classification, this\nnew task faces two additional challenges: (1) low success rate due to the\ndiverse appearance of images in the group; (2) low transferability across CoSOD\nmethods due to the considerable difference between CoSOD pipelines. To address\nthese challenges, we propose the very first black-box joint adversarial\nexposure and noise attack (Jadena), where we jointly and locally tune the\nexposure and additive perturbations of the image according to a newly designed\nhigh-feature-level contrast-sensitive loss function. Our method, without any\ninformation on the state-of-the-art CoSOD methods, leads to significant\nperformance degradation on various co-saliency detection datasets and makes the\nco-salient objects undetectable. This can have strong practical benefits in\nproperly securing the large number of personal photos currently shared on the\nInternet. Moreover, our method is potential to be utilized as a metric for\nevaluating the robustness of CoSOD methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruijun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TubeR: Tubelet Transformer for Video Action Detection. (arXiv:2104.00969v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00969","description":"<p>We propose TubeR: a simple solution for spatio-temporal video action\ndetection. Different from existing methods that depend on either an off-line\nactor detector or hand-designed actor-positional hypotheses like proposals or\nanchors, we propose to directly detect an action tubelet in a video by\nsimultaneously performing action localization and recognition from a single\nrepresentation. TubeR learns a set of tubelet-queries and utilizes a\ntubelet-attention module to model the dynamic spatio-temporal nature of a video\nclip, which effectively reinforces the model capacity compared to using\nactor-positional hypotheses in the spatio-temporal space. For videos containing\ntransitional states or scene changes, we propose a context aware classification\nhead to utilize short-term and long-term context to strengthen action\nclassification, and an action switch regression head for detecting the precise\ntemporal action extent. TubeR directly produces action tubelets with variable\nlengths and even maintains good results for long video clips. TubeR outperforms\nthe previous state-of-the-art on commonly used action detection datasets AVA,\nUCF101-24 and JHMDB51-21.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiaojiao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_S/0/1/0/all/0/1\">Shuai Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingze Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunhui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_K/0/1/0/all/0/1\">Kaustav Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marsic_I/0/1/0/all/0/1\">Ivan Marsic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G.M. Snoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DHNet: Double MPEG-4 Compression Detection via Multiple DCT Histograms. (arXiv:2107.08939v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2107.08939","description":"<p>In this article, we aim to detect the double compression of MPEG-4, a\nuniversal video codec that is built into surveillance systems and shooting\ndevices. Double compression is accompanied by various types of video\nmanipulation, and its traces can be exploited to determine whether a video is a\nforgery. To this end, we present a neural network-based approach with\ndiscriminant features for capturing peculiar artifacts in the discrete cosine\ntransform (DCT) domain caused by double MPEG-4 compression. By analyzing the\nintra-coding process of MPEG-4, which performs block-DCT-based quantization, we\nexploit multiple DCT histograms as features to focus on the statistical\nproperties of DCT coefficients on multiresolution blocks. Furthermore, we\nimprove detection performance using a vectorized feature of the quantization\ntable on dense layers as auxiliary information. Compared with neural\nnetwork-based approaches suitable for exploring subtle manipulations, the\nexperimental results reveal that this work achieves high performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1\">Seung-Hun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_W/0/1/0/all/0/1\">Wonhyuk Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1\">Myung-Joon Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jihyeon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_I/0/1/0/all/0/1\">In-Jae Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigation of condominium building collapse in Surfside, Florida: A video feature tracking approach. (arXiv:2109.06629v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06629","description":"<p>On June 24, 2021, a 12-story condominium building (Champlain Towers South) in\nSurfside, Florida partially collapsed, resulting in one of the deadliest\nbuilding collapses in United States history with 98 people confirmed deceased.\nIn this work, we analyze the collapse event using a video clip that is publicly\navailable from social media. In our analysis, we apply computer vision\nalgorithms to corroborate new information from the video clip that may not be\nreadily interpreted by human eyes. By comparing the differential features\nagainst different video frames, our proposed method is used to quantify the\nfalling structural components by mapping the directions and magnitudes of their\nmovements. We demonstrate the potential of this video processing methodology in\ninvestigations of catastrophic structural failures and hope our approach may\nserve as a basis for further investigations into structure collapse events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangxiong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smyl_D/0/1/0/all/0/1\">Danny Smyl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Coherence for Text-to-Image Retrieval. (arXiv:2109.11047v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11047","description":"<p>Common image-text joint understanding techniques presume that images and the\nassociated text can universally be characterized by a single implicit model.\nHowever, co-occurring images and text can be related in qualitatively different\nways, and explicitly modeling it could improve the performance of current joint\nunderstanding models. In this paper, we train a Cross-Modal Coherence Modelfor\ntext-to-image retrieval task. Our analysis shows that models trained with\nimage--text coherence relations can retrieve images originally paired with\ntarget text more often than coherence-agnostic models. We also show via human\nevaluation that images retrieved by the proposed coherence-aware model are\npreferred over a coherence-agnostic baseline by a huge margin. Our findings\nprovide insights into the ways that different modalities communicate and the\nrole of coherence relations in capturing commonsense inferences in text and\nimagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fangda Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_H/0/1/0/all/0/1\">Hareesh Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1\">Mubbasir Kapadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1\">Vladimir Pavlovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Matthew Stone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Instance Segmentation with Automotive Radar Detection Points. (arXiv:2110.01775v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01775","description":"<p>Automotive radar provides reliable environmental perception in all-weather\nconditions with affordable cost, but it hardly supplies semantic and geometry\ninformation due to the sparsity of radar detection points. With the development\nof automotive radar technologies in recent years, instance segmentation becomes\npossible by using automotive radar. Its data contain contexts such as radar\ncross section and micro-Doppler effects, and sometimes can provide detection\nwhen the field of view is obscured. The outcome from instance segmentation\ncould be potentially used as the input of trackers for tracking targets. The\nexisting methods often utilize a clustering-based classification framework,\nwhich fits the need of real-time processing but has limited performance due to\nminimum information provided by sparse radar detection points. In this paper,\nwe propose an efficient method based on clustering of estimated semantic\ninformation to achieve instance segmentation for the sparse radar detection\npoints. In addition, we show that the performance of the proposed approach can\nbe further enhanced by incorporating the visual multi-layer perceptron. The\neffectiveness of the proposed method is verified by experimental results on the\npopular RadarScenes dataset, achieving 89.53% mean coverage and 86.97% mean\naverage precision with the IoU threshold of 0.5, which is superior to other\napproaches in the literature. More significantly, the consumed memory is around\n1MB, and the inference time is less than 40ms, indicating that our proposed\nalgorithm is storage and time efficient. These two criteria ensure the\npracticality of the proposed method in real-world systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Weiyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Liping Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Importance of Firth Bias Reduction in Few-Shot Classification. (arXiv:2110.02529v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02529","description":"<p>Learning accurate classifiers for novel categories from very few examples,\nknown as few-shot image classification, is a challenging task in statistical\nmachine learning and computer vision. The performance in few-shot\nclassification suffers from the bias in the estimation of classifier\nparameters; however, an effective underlying bias reduction technique that\ncould alleviate this issue in training few-shot classifiers has been\noverlooked. In this work, we demonstrate the effectiveness of Firth bias\nreduction in few-shot classification. Theoretically, Firth bias reduction\nremoves the $O(N^{-1})$ first order term from the small-sample bias of the\nMaximum Likelihood Estimator. Here we show that the general Firth bias\nreduction technique simplifies to encouraging uniform class assignment\nprobabilities for multinomial logistic classification, and almost has the same\neffect in cosine classifiers. We derive an easy-to-implement optimization\nobjective for Firth penalized multinomial logistic and cosine classifiers,\nwhich is equivalent to penalizing the cross-entropy loss with a KL-divergence\nbetween the uniform label distribution and the predictions. Then, we\nempirically evaluate that it is consistently effective across the board for\nfew-shot image classification, regardless of (1) the feature representations\nfrom different backbones, (2) the number of samples per class, and (3) the\nnumber of classes. Finally, we show the robustness of Firth bias reduction, in\nthe case of imbalanced data distribution. Our implementation is available at\nhttps://github.com/ehsansaleh/firth_bias_reduction\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_S/0/1/0/all/0/1\">Saba Ghaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleh_E/0/1/0/all/0/1\">Ehsan Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-xiong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple but Effective: CLIP Embeddings for Embodied AI. (arXiv:2111.09888v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09888","description":"<p>Contrastive language image pretraining (CLIP) encoders have been shown to be\nbeneficial for a range of visual tasks from classification and detection to\ncaptioning and image manipulation. We investigate the effectiveness of CLIP\nvisual backbones for Embodied AI tasks. We build incredibly simple baselines,\nnamed EmbCLIP, with no task specific architectures, inductive biases (such as\nthe use of semantic maps), auxiliary tasks during training, or depth maps --\nyet we find that our improved baselines perform very well across a range of\ntasks and simulators. EmbCLIP tops the RoboTHOR ObjectNav leaderboard by a huge\nmargin of 20 pts (Success Rate). It tops the iTHOR 1-Phase Rearrangement\nleaderboard, beating the next best submission, which employs Active Neural\nMapping, and more than doubling the % Fixed Strict metric (0.08 to 0.17). It\nalso beats the winners of the 2021 Habitat ObjectNav Challenge, which employ\nauxiliary tasks, depth maps, and human demonstrations, and those of the 2019\nHabitat PointNav Challenge. We evaluate the ability of CLIP's visual\nrepresentations at capturing semantic information about input observations --\nprimitives that are useful for navigation-heavy embodied tasks -- and find that\nCLIP's representations encode these primitives more effectively than\nImageNet-pretrained backbones. Finally, we extend one of our baselines,\nproducing an agent capable of zero-shot object navigation that can navigate to\nobjects that were not used as targets during training. Our code and models are\navailable at https://github.com/allenai/embodied-clip\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Apoorv Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weihs_L/0/1/0/all/0/1\">Luca Weihs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nanorobot queue: Cooperative treatment of cancer based on team member communication and image processing. (arXiv:2111.11236v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2111.11236","description":"<p>Although nanorobots have been used as clinical prescriptions for work such as\ngastroscopy, and even photoacoustic tomography technology has been proposed to\ncontrol nanorobots to deliver drugs at designated delivery points in real time,\nand there are cases of eliminating \"superbacteria\" in blood through nanorobots,\nmost technologies are immature, either with low efficiency or low accuracy,\nEither it can not be mass produced, so the most effective way to treat cancer\ndiseases at this stage is through chemotherapy and radiotherapy. Patients are\nsuffering and can not be cured. Therefore, this paper proposes an ideal model\nof a treatment method that can completely cure cancer, a cooperative treatment\nmethod based on nano robot queue through team member communication and computer\nvision image classification (target detection).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Compositional Zero-shot Learning with DeCompositional Consensus. (arXiv:2111.14673v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14673","description":"<p>Parts represent a basic unit of geometric and semantic similarity across\ndifferent objects. We argue that part knowledge should be composable beyond the\nobserved object classes. Towards this, we present 3D Compositional Zero-shot\nLearning as a problem of part generalization from seen to unseen object classes\nfor semantic segmentation. We provide a structured study through benchmarking\nthe task with the proposed Compositional-PartNet dataset. This dataset is\ncreated by processing the original PartNet to maximize part overlap across\ndifferent objects. The existing point cloud part segmentation methods fail to\ngeneralize to unseen object classes in this setting. As a solution, we propose\nDeCompositional Consensus, which combines a part segmentation network with a\npart scoring network. The key intuition to our approach is that a segmentation\nmask over some parts should have a consensus with its part scores when each\npart is taken apart. The two networks reason over different part combinations\ndefined in a per-object part prior to generate the most suitable segmentation\nmask. We demonstrate that our method allows compositional zero-shot\nsegmentation and generalized zero-shot classification, and establishes the\nstate of the art on both tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naeem_M/0/1/0/all/0/1\">Muhammad Ferjad Naeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ornek_E/0/1/0/all/0/1\">Evin P&#x131;nar &#xd6;rnek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yongqin Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-domain Integrative Swin Transformer network for Sparse-View Tomographic Reconstruction. (arXiv:2111.14831v7 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.14831","description":"<p>Decreasing projection views to lower X-ray radiation dose usually leads to\nsevere streak artifacts. To improve image quality from sparse-view data, a\nMulti-domain Integrative Swin Transformer network (MIST-net) was developed in\nthis article. First, MIST-net incorporated lavish domain features from data,\nresidual-data, image, and residual-image using flexible network architectures,\nwhere residual-data and residual-image sub-network was considered as data\nconsistency module to eliminate interpolation and reconstruction errors.\nSecond, a trainable edge enhancement filter was incorporated to detect and\nprotect image edges. Third, a high-quality reconstruction Swin transformer\n(i.e., Recformer) was designed to capture image global features. The experiment\nresults on numerical and real cardiac clinical datasets with 48-views\ndemonstrated that our proposed MIST-net provided better image quality with more\nsmall features and sharp edges than other competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1\">Jiayi Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Heye Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Weifei Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Weiwen Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIVE: Evaluating the Human Interpretability of Visual Explanations. (arXiv:2112.03184v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03184","description":"<p>As machine learning is increasingly applied to high-impact, high-risk\ndomains, there have been a number of new methods aimed at making AI models more\nhuman interpretable. Despite the recent growth of interpretability work, there\nis a lack of systematic evaluation of proposed techniques. In this work, we\npropose HIVE (Human Interpretability of Visual Explanations), a novel human\nevaluation framework for visual interpretability methods that allows for\nfalsifiable hypothesis testing, cross-method comparison, and human-centered\nevaluation. To the best of our knowledge, this is the first work of its kind.\nUsing HIVE, we conduct IRB-approved human studies with nearly 1000 participants\nand evaluate four methods that represent the diversity of computer vision\ninterpretability works: GradCAM, BagNet, ProtoPNet, and ProtoTree. Our results\nsuggest that explanations engender human trust, even for incorrect predictions,\nyet are not distinct enough for users to distinguish between correct and\nincorrect predictions. We open-source HIVE to enable future studies and to\nencourage more human-centered approaches to interpretability research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunnie S. Y. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_N/0/1/0/all/0/1\">Nicole Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaswamy_V/0/1/0/all/0/1\">Vikram V. Ramaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_R/0/1/0/all/0/1\">Ruth Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Control for Free! Image Synthesis with Semantic Diffusion Guidance. (arXiv:2112.05744v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05744","description":"<p>Controllable image synthesis models allow creation of diverse images based on\ntext instructions or guidance from a reference image. Recently, denoising\ndiffusion probabilistic models have been shown to generate more realistic\nimagery than prior methods, and have been successfully demonstrated in\nunconditional and class-conditional settings. We investigate fine-grained,\ncontinuous control of this model class, and introduce a novel unified framework\nfor semantic diffusion guidance, which allows either language or image\nguidance, or both. Guidance is injected into a pretrained unconditional\ndiffusion model using the gradient of image-text or image matching scores. We\nexplore CLIP-based language guidance as well as both content and style-based\nimage guidance in a unified framework. Our text-guided synthesis approach can\nbe applied to datasets without associated text annotations. We conduct\nexperiments on FFHQ and LSUN datasets, and show results on fine-grained\ntext-guided image synthesis, synthesis of images related to a style or content\nreference image, and examples with both textual and image guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xihui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dong Huk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azadi_S/0/1/0/all/0/1\">Samaneh Azadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chopikyan_A/0/1/0/all/0/1\">Arman Chopikyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuxiao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video as Conditional Graph Hierarchy for Multi-Granular Question Answering. (arXiv:2112.06197v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06197","description":"<p>Video question answering requires the models to understand and reason about\nboth the complex video and language data to correctly derive the answers.\nExisting efforts have been focused on designing sophisticated cross-modal\ninteractions to fuse the information from two modalities, while encoding the\nvideo and question holistically as frame and word sequences. Despite their\nsuccess, these methods are essentially revolving around the sequential nature\nof video- and question-contents, providing little insight to the problem of\nquestion-answering and lacking interpretability as well. In this work, we argue\nthat while video is presented in frame sequence, the visual elements (e.g.,\nobjects, actions, activities and events) are not sequential but rather\nhierarchical in semantic space. To align with the multi-granular essence of\nlinguistic concepts in language queries, we propose to model video as a\nconditional graph hierarchy which weaves together visual facts of different\ngranularity in a level-wise manner, with the guidance of corresponding textual\ncues. Despite the simplicity, our extensive experiments demonstrate the\nsuperiority of such conditional hierarchical graph architecture, with clear\nperformance improvements over prior methods and also better generalization\nacross different type of questions. Further analyses also demonstrate the\nmodel's reliability as it shows meaningful visual-textual evidences for the\npredicted answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Junbin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yicong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupling Zero-Shot Semantic Segmentation. (arXiv:2112.07910v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07910","description":"<p>Zero-shot semantic segmentation (ZS3) aims to segment the novel categories\nthat have not been seen in the training. Existing works formulate ZS3 as a\npixel-level zeroshot classification problem, and transfer semantic knowledge\nfrom seen classes to unseen ones with the help of language models pre-trained\nonly with texts. While simple, the pixel-level ZS3 formulation shows the\nlimited capability to integrate vision-language models that are often\npre-trained with image-text pairs and currently demonstrate great potential for\nvision tasks. Inspired by the observation that humans often perform\nsegment-level semantic labeling, we propose to decouple the ZS3 into two\nsub-tasks: 1) a classagnostic grouping task to group the pixels into segments.\n2) a zero-shot classification task on segments. The former task does not\ninvolve category information and can be directly transferred to group pixels\nfor unseen classes. The latter task performs at segment-level and provides a\nnatural way to leverage large-scale vision-language models pre-trained with\nimage-text pairs (e.g. CLIP) for ZS3. Based on the decoupling formulation, we\npropose a simple and effective zero-shot semantic segmentation model, called\nZegFormer, which outperforms the previous methods on ZS3 standard benchmarks by\nlarge margins, e.g., 22 points on the PASCAL VOC and 3 points on the COCO-Stuff\nin terms of mIoU for unseen classes. Code will be released at\nhttps://github.com/dingjiansw101/ZegFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking Outside the Box to Ground Language in 3D Scenes. (arXiv:2112.08879v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08879","description":"<p>Most language grounding models learn to select the referred object from a\npool of object proposals provided by a pre-trained detector. This object\nproposal bottleneck is limiting because an utterance may refer to visual\nentities at various levels of granularity, such as the chair, the leg of a\nchair, or the tip of the front leg of a chair, which may be missed by the\ndetector. Recently, MDETR introduced a language grounding model for 2D images\nthat do not have such a box proposal bottleneck; instead of selecting objects\nfrom a proposal pool, it instead decodes the referenced object boxes directly\nfrom image and language features and achieves big leaps in performance. We\npropose a language grounding model for 3D scenes built on MDETR, which we call\nBEAUTY-DETR, from bottom-up and top-down DETR. BEAUTY-DETR attends on an\nadditional object proposal pool computed bottom-up from a pre-trained detector.\nYet it decodes referenced objects without selecting them from the pool. In this\nway, it uses powerful object detectors to help ground language without being\nrestricted by their misses. Second, BEAUTY-DETR augments supervision from\nlanguage grounding annotations by configuring object detection annotations as\nlanguage prompts to be grounded in images. The proposed model sets a new\nstate-of-the-art across popular 3D language grounding benchmarks with\nsignificant performance gains over previous 3D approaches (12.6% on SR3D, 11.6%\non NR3D and 6.3% on ScanRefer). It outperforms a straightforward MDETR for the\n3D point clouds method we implemented by 6.7% on SR3D, 11.8% on NR3D and 5% on\nthe ScanRefer benchmark. When applied to language grounding in 2D images, it\nperforms on par with MDETR. We ablate each of the design choices of the model\nand quantify their contribution to performance. Code and checkpoints are\navailable at https://github.com/nickgkan/beauty_detr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1\">Nikolaos Gkanatsios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mediratta_I/0/1/0/all/0/1\">Ishita Mediratta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Network-Aware 5G Edge Computing for Object Detection: Augmenting Wearables to \"See\" More, Farther and Faster. (arXiv:2112.13194v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.13194","description":"<p>Advanced wearable devices are increasingly incorporating high-resolution\nmulti-camera systems. As state-of-the-art neural networks for processing the\nresulting image data are computationally demanding, there has been growing\ninterest in leveraging fifth generation (5G) wireless connectivity and mobile\nedge computing for offloading this processing to the cloud. To assess this\npossibility, this paper presents a detailed simulation and evaluation of 5G\nwireless offloading for object detection within a powerful, new smart wearable\ncalled VIS4ION, for the Blind-and-Visually Impaired (BVI). The current VIS4ION\nsystem is an instrumented book-bag with high-resolution cameras, vision\nprocessing and haptic and audio feedback. The paper considers uploading the\ncamera data to a mobile edge cloud to perform real-time object detection and\ntransmitting the detection results back to the wearable. To determine the video\nrequirements, the paper evaluates the impact of video bit rate and resolution\non object detection accuracy and range. A new street scene dataset with labeled\nobjects relevant to BVI navigation is leveraged for analysis. The vision\nevaluation is combined with a detailed full-stack wireless network simulation\nto determine the distribution of throughputs and delays with real navigation\npaths and ray-tracing from new high-resolution 3D models in an urban\nenvironment. For comparison, the wireless simulation considers both a standard\n4G-Long Term Evolution (LTE) carrier and high-rate 5G millimeter-wave (mmWave)\ncarrier. The work thus provides a thorough and realistic assessment of edge\ncomputing with mmWave connectivity in an application with both high bandwidth\nand low latency requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhongzheng Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azzino_T/0/1/0/all/0/1\">Tommy Azzino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hao_Y/0/1/0/all/0/1\">Yu Hao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyu_Y/0/1/0/all/0/1\">Yixuan Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pei_H/0/1/0/all/0/1\">Haoyang Pei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boldini_A/0/1/0/all/0/1\">Alain Boldini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mezzavilla_M/0/1/0/all/0/1\">Marco Mezzavilla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beheshti_M/0/1/0/all/0/1\">Mahya Beheshti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Porfiri_M/0/1/0/all/0/1\">Maurizio Porfiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hudson_T/0/1/0/all/0/1\">Todd Hudson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seiple_W/0/1/0/all/0/1\">William Seiple</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rangan_S/0/1/0/all/0/1\">Sundeep Rangan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rizzo_J/0/1/0/all/0/1\">J.R. Rizzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation. (arXiv:2203.06558v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06558","description":"<p>Training a generalizable 3D part segmentation network is quite challenging\nbut of great importance in real-world applications. To tackle this problem,\nsome works design task-specific solutions by translating human understanding of\nthe task to machine's learning process, which faces the risk of missing the\noptimal strategy since machines do not necessarily understand in the exact\nhuman way. Others try to use conventional task-agnostic approaches designed for\ndomain generalization problems with no task prior knowledge considered. To\nsolve the above issues, we propose AutoGPart, a generic method enabling\ntraining generalizable 3D part segmentation networks with the task prior\nconsidered. AutoGPart builds a supervision space with geometric prior knowledge\nencoded, and lets the machine to search for the optimal supervisions from the\nspace for a specific segmentation task automatically. Extensive experiments on\nthree generalizable 3D part segmentation tasks are conducted to demonstrate the\neffectiveness and versatility of AutoGPart. We demonstrate that the performance\nof segmentation networks using simple backbones can be significantly improved\nwhen trained with supervisions searched by our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xueyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaomeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anyi Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-Gait: Automatic Ataxia Risk Assessment with Computer Vision on Gait Task Videos. (arXiv:2203.08215v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08215","description":"<p>In this paper, we investigated whether we can 1) detect participants with\nataxia-specific gait characteristics (risk-prediction), and 2) assess severity\nof ataxia from gait (severity-assessment) using computer vision. We created a\ndataset of 155 videos from 89 participants, 24 controls and 65 diagnosed with\n(or are pre-manifest) spinocerebellar ataxias (SCAs), performing the gait task\nof the Scale for the Assessment and Rating of Ataxia (SARA) from 11 medical\nsites located in 8 different states across the United States. We develop a\ncomputer vision pipeline to detect, track, and separate out the participants\nfrom their surroundings and construct several features from their body pose\ncoordinates to capture gait characteristics like step width, step length,\nswing, stability, speed, etc. Our risk-prediction model achieves 83.06%\naccuracy and an 80.23% F1 score. Similarly, our severity-assessment model\nachieves a mean absolute error (MAE) score of 0.6225 and a Pearson's\ncorrelation coefficient score of 0.7268. Our models still performed\ncompetitively when evaluated on data from sites not used during training.\nFurthermore, through feature importance analysis, we found that our models\nassociate wider steps, decreased walking speed, and increased instability with\ngreater ataxia severity, which is consistent with previously established\nclinical knowledge. Our models create possibilities for remote ataxia\nassessment in non-clinical settings in the future, which could significantly\nimprove accessibility of ataxia care. Furthermore, our underlying dataset was\nassembled from a geographically diverse cohort, highlighting its potential to\nfurther increase equity. The code used in this study is open to the public, and\nthe anonymized body pose landmark dataset is also available upon request.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_W/0/1/0/all/0/1\">Wasifur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Masum Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olubajo_T/0/1/0/all/0/1\">Titilayo Olubajo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thaker_J/0/1/0/all/0/1\">Jeet Thaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelkader_A/0/1/0/all/0/1\">Abdelrahman Abdelkader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Phillip Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashizawa_T/0/1/0/all/0/1\">Tetsuo Ashizawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Ehsan Hoque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale Bilingual Language-Image Contrastive Learning. (arXiv:2203.14463v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14463","description":"<p>This paper is a technical report to share our experience and findings\nbuilding a Korean and English bilingual multimodal model. While many of the\nmultimodal datasets focus on English and multilingual multimodal research uses\nmachine-translated texts, employing such machine-translated texts is limited to\ndescribing unique expressions, cultural information, and proper noun in\nlanguages other than English. In this work, we collect 1.1 billion image-text\npairs (708 million Korean and 476 million English) and train a bilingual\nmultimodal model named KELIP. We introduce simple yet effective training\nschemes, including MAE pre-training and multi-crop augmentation. Extensive\nexperiments demonstrate that a model trained with such training schemes shows\ncompetitive performance in both languages. Moreover, we discuss\nmultimodal-related research questions: 1) strong augmentation-based methods can\ndistract the model from learning proper multimodal relations; 2) training\nmultimodal model without cross-lingual relation can learn the relation via\nvisual semantics; 3) our bilingual KELIP can capture cultural differences of\nvisual semantics for the same meaning of words; 4) a large-scale multimodal\nmodel can be used for multimodal feature analogy. We hope that this work will\nprovide helpful experience and findings for future research. We provide an\nopen-source pre-trained KELIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1\">Byungsoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1\">Geonmo Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exemplar-based Pattern Synthesis with Implicit Periodic Field Network. (arXiv:2204.01671v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01671","description":"<p>Synthesis of ergodic, stationary visual patterns is widely applicable in\ntexturing, shape modeling, and digital content creation. The wide applicability\nof this technique thus requires the pattern synthesis approaches to be\nscalable, diverse, and authentic. In this paper, we propose an exemplar-based\nvisual pattern synthesis framework that aims to model the inner statistics of\nvisual patterns and generate new, versatile patterns that meet the\naforementioned requirements. To this end, we propose an implicit network based\non generative adversarial network (GAN) and periodic encoding, thus calling our\nnetwork the Implicit Periodic Field Network (IPFN). The design of IPFN ensures\nscalability: the implicit formulation directly maps the input coordinates to\nfeatures, which enables synthesis of arbitrary size and is computationally\nefficient for 3D shape synthesis. Learning with a periodic encoding scheme\nencourages diversity: the network is constrained to model the inner statistics\nof the exemplar based on spatial latent codes in a periodic field. Coupled with\ncontinuously designed GAN training procedures, IPFN is shown to synthesize\ntileable patterns with smooth transitions and local variations. Last but not\nleast, thanks to both the adversarial training technique and the encoded\nFourier features, IPFN learns high-frequency functions that produce authentic,\nhigh-quality results. To validate our approach, we present novel experimental\nresults on various applications in 2D texture synthesis and 3D shape synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weikai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shichen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yajie Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Optical Flow-Based Line Feature Tracking. (arXiv:2204.03331v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03331","description":"<p>In this paper we propose a novel sparse optical flow (SOF)-based line feature\ntracking method for the camera pose estimation problem. This method is inspired\nby the point-based SOF algorithm and developed based on an observation that two\nadjacent images in time-varying image sequences satisfy brightness invariant.\nBased on this observation, we re-define the goal of line feature tracking:\ntrack two endpoints of a line feature instead of the entire line based on gray\nvalue matching instead of descriptor matching. To achieve this goal, an\nefficient two endpoint tracking (TET) method is presented: first, describe a\ngiven line feature with its two endpoints; next, track the two endpoints based\non SOF to obtain two new tracked endpoints by minimizing a pixel-level\ngrayscale residual function; finally, connect the two tracked endpoints to\ngenerate a new line feature. The correspondence is established between the\ngiven and the new line feature. Compared with current descriptor-based methods,\nour TET method needs not to compute descriptors and detect line features\nrepeatedly. Naturally, it has an obvious advantage over computation.\nExperiments in several public benchmark datasets show our method yields highly\ncompetitive accuracy with an obvious advantage over speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongshan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_I/0/1/0/all/0/1\">Islam Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmenting across places: The need for fair transfer learning with satellite imagery. (arXiv:2204.04358v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04358","description":"<p>The increasing availability of high-resolution satellite imagery has enabled\nthe use of machine learning to support land-cover measurement and inform\npolicy-making. However, labelling satellite images is expensive and is\navailable for only some locations. This prompts the use of transfer learning to\nadapt models from data-rich locations to others. Given the potential for\nhigh-impact applications of satellite imagery across geographies, a systematic\nassessment of transfer learning implications is warranted. In this work, we\nconsider the task of land-cover segmentation and study the fairness\nimplications of transferring models across locations. We leverage a large\nsatellite image segmentation benchmark with 5987 images from 18 districts (9\nurban and 9 rural). Via fairness metrics we quantify disparities in model\nperformance along two axes -- across urban-rural locations and across\nland-cover classes. Findings show that state-of-the-art models have better\noverall accuracy in rural areas compared to urban areas, through unsupervised\ndomain adaptation methods transfer learning better to urban versus rural areas\nand enlarge fairness gaps. In analysis of reasons for these findings, we show\nthat raw satellite images are overall more dissimilar between source and target\ndistricts for rural than for urban locations. This work highlights the need to\nconduct fairness analysis for satellite imagery segmentation models and\nmotivates the development of methods for fair transfer learning in order not to\nintroduce disparities between places, particularly urban and rural locations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Harvineet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chok_L/0/1/0/all/0/1\">Lazarus Chok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chunara_R/0/1/0/all/0/1\">Rumi Chunara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Machine Learning Model Evaluation in Pathology. (arXiv:2204.05205v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.05205","description":"<p>Machine Learning has been applied to pathology images in research and\nclinical practice with promising outcomes. However, standard ML models often\nlack the rigorous evaluation required for clinical decisions. Machine learning\ntechniques for natural images are ill-equipped to deal with pathology images\nthat are significantly large and noisy, require expensive labeling, are hard to\ninterpret, and are susceptible to spurious correlations. We propose a set of\npractical guidelines for ML evaluation in pathology that address the above\nconcerns. The paper includes measures for setting up the evaluation framework,\neffectively dealing with variability in labels, and a recommended suite of\ntests to address issues related to domain shift, robustness, and confounding\nvariables. We hope that the proposed framework will bridge the gap between ML\nresearchers and domain experts, leading to wider adoption of ML techniques in\npathology and improving patient outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Javed_S/0/1/0/all/0/1\">Syed Ashar Javed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Juyal_D/0/1/0/all/0/1\">Dinkar Juyal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shanis_Z/0/1/0/all/0/1\">Zahil Shanis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakraborty_S/0/1/0/all/0/1\">Shreya Chakraborty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pokkalla_H/0/1/0/all/0/1\">Harsha Pokkalla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prakash_A/0/1/0/all/0/1\">Aaditya Prakash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06718","description":"<p>Convolutional neural network (CNN) achieves impressive success in the field\nof computer vision during the past few decades. As the core of CNNs, image\nconvolution operation helps CNNs to achieve good performance on image-related\ntasks. However, image convolution is hard to be implemented and parallelized.\nIn this paper, we propose a novel neural network model, namely CEMNet, that can\nbe trained in frequency domain. The most important motivation of this research\nis that we can use the very simple element-wise multiplication operation to\nreplace the image convolution in frequency domain based on Cross-Correlation\nTheorem. We further introduce Weight Fixation Mechanism to alleviate\nover-fitting, and analyze the working behavior of Batch Normalization, Leaky\nReLU and Dropout in frequency domain to design their counterparts for CEMNet.\nAlso, to deal with complex inputs brought by DFT, we design two branch network\nstructure for CEMNet. Experimental results imply that CEMNet works well in\nfrequency domain, and achieve good performance on MNIST and CIFAR-10 databases.\nTo our knowledge, CEMNet is the first model trained in Fourier Domain that\nachieves more than 70\\% validation accuracy on CIFAR-10 database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hengyue Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenbo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}