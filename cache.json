{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ScAN: Suicide Attempt and Ideation Events Dataset. (arXiv:2205.07872v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07872","description":"<p>Suicide is an important public health concern and one of the leading causes\nof death worldwide. Suicidal behaviors, including suicide attempts (SA) and\nsuicide ideations (SI), are leading risk factors for death by suicide.\nInformation related to patients' previous and current SA and SI are frequently\ndocumented in the electronic health record (EHR) notes. Accurate detection of\nsuch documentation may help improve surveillance and predictions of patients'\nsuicidal behaviors and alert medical professionals for suicide prevention\nefforts. In this study, we first built Suicide Attempt and Ideation Events\n(ScAN) dataset, a subset of the publicly available MIMIC III dataset spanning\nover 12k+ EHR notes with 19k+ annotated SA and SI events information. The\nannotations also contain attributes such as method of suicide attempt. We also\nprovide a strong baseline model ScANER (Suicide Attempt and Ideation Events\nRetriever), a multi-task RoBERTa-based model with a retrieval module to extract\nall the relevant suicidal behavioral evidences from EHR notes of an\nhospital-stay and, and a prediction module to identify the type of suicidal\nbehavior (SA and SI) concluded during the patient's stay at the hospital.\nScANER achieved a macro-weighted F1-score of 0.83 for identifying suicidal\nbehavioral evidences and a macro F1-score of 0.78 and 0.60 for classification\nof SA and SI for the patient's hospital-stay, respectively. ScAN and ScANER are\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rawat_B/0/1/0/all/0/1\">Bhanu Pratap Singh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovaly_S/0/1/0/all/0/1\">Samuel Kovaly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pigeon_W/0/1/0/all/0/1\">Wilfred R. Pigeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Diversity of Argument-Making in the Wild: from Assumptions and Definitions to Causation and Anecdote in Reddit's \"Change My View\". (arXiv:2205.07938v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07938","description":"<p>What kinds of arguments do people make, and what effect do they have on\nothers? Normative constraints on argument-making are as old as philosophy\nitself, but little is known about the diversity of arguments made in practice.\nWe use NLP tools to extract patterns of argument-making from the Reddit site\n\"Change My View\" (r/CMV). This reveals six distinct argument patterns: not just\nthe familiar deductive and inductive forms, but also arguments about\ndefinitions, relevance, possibility and cause, and personal experience. Data\nfrom r/CMV also reveal differences in efficacy: personal experience and, to a\nlesser extent, arguments about causation and examples, are most likely to shift\na person's view, while arguments about relevance are the least. Finally, our\nmethods reveal a gradient of argument-making preferences among users: a\ntwo-axis model, of \"personal--impersonal\" and \"concrete--abstract\", can account\nfor nearly 80% of the strategy variance between individuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Na_R/0/1/0/all/0/1\">Robin W. Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeDeo_S/0/1/0/all/0/1\">Simon DeDeo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta AI at Arabic Hate Speech 2022: MultiTask Learning with Self-Correction for Hate Speech Classification. (arXiv:2205.07960v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07960","description":"<p>In this paper, we tackle the Arabic Fine-Grained Hate Speech Detection shared\ntask and demonstrate significant improvements over reported baselines for its\nthree subtasks. The tasks are to predict if a tweet contains (1) Offensive\nlanguage; and whether it is considered (2) Hate Speech or not and if so, then\npredict the (3) Fine-Grained Hate Speech label from one of six categories. Our\nfinal solution is an ensemble of models that employs multitask learning and a\nself-consistency correction method yielding 82.7% on the hate speech subtask --\nreflecting a 3.4% relative improvement compared to previous work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AlKhamissi_B/0/1/0/all/0/1\">Badr AlKhamissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Budge programming language. (arXiv:2205.07979v1 [cs.PL])","link":"http://arxiv.org/abs/2205.07979","description":"<p>We present a simple, esoteric programming language based on G\\\"odel numbering\nand prime factorization, enhanced with explicit, scoped loops, allowing for\neasy program composition. We will show the syntax and semantics and then\nprovide a few example programs and their evaluation. We will also provide a few\ninterpreter implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sitnikovski_B/0/1/0/all/0/1\">Boro Sitnikovski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Debiasing Translation Artifacts. (arXiv:2205.08001v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08001","description":"<p>Cross-lingual natural language processing relies on translation, either by\nhumans or machines, at different levels, from translating training data to\ntranslating test sets. However, compared to original texts in the same\nlanguage, translations possess distinct qualities referred to as\ntranslationese. Previous research has shown that these translation artifacts\ninfluence the performance of a variety of cross-lingual tasks. In this work, we\npropose a novel approach to reducing translationese by extending an established\nbias-removal technique. We use the Iterative Null-space Projection (INLP)\nalgorithm, and show by measuring classification accuracy before and after\ndebiasing, that translationese is reduced at both sentence and word level. We\nevaluate the utility of debiasing translationese on a natural language\ninference (NLI) task, and show that by reducing this bias, NLI accuracy\nimproves. To the best of our knowledge, this is the first study to debias\ntranslationese as represented in latent embedding space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_K/0/1/0/all/0/1\">Koel Dutta Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalota_R/0/1/0/all/0/1\">Rricha Jalota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espa&#xf1;a-Bonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction. (arXiv:2205.08012v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08012","description":"<p>Knowledge graph (KG) link prediction is a fundamental task in artificial\nintelligence, with applications in natural language processing, information\nretrieval, and biomedicine. Recently, promising results have been achieved by\nleveraging cross-modal information in KGs, using ensembles that combine\nknowledge graph embeddings (KGEs) and contextual language models (LMs).\nHowever, existing ensembles are either (1) not consistently effective in terms\nof ranking accuracy gains or (2) impractically inefficient on larger datasets\ndue to the combinatorial explosion problem of pairwise ranking with deep\nlanguage models. In this paper, we propose a novel tiered ranking architecture\nCascadER to maintain the ranking accuracy of full ensembling while improving\nefficiency considerably. CascadER uses LMs to rerank the outputs of more\nefficient base KGEs, relying on an adaptive subset selection scheme aimed at\ninvoking the LMs minimally while maximizing accuracy gain over the KGE.\nExtensive experiments demonstrate that CascadER improves MRR by up to 9 points\nover KGE baselines, setting new state-of-the-art performance on four benchmarks\nwhile improving efficiency by one or more orders of magnitude over competitive\ncross-modal baselines. Our empirical analyses reveal that diversity of models\nacross modalities and preservation of individual models' confidence signals\nhelp explain the effectiveness of CascadER, and suggest promising directions\nfor cross-modal cascaded architectures. Code and pretrained models are\navailable at https://github.com/tsafavi/cascader.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safavi_T/0/1/0/all/0/1\">Tara Safavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing Multilingual Resources to Question Answering in Arabic. (arXiv:2205.08024v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08024","description":"<p>The goal of the paper is to predict answers to questions given a passage of\nQur'an. The answers are always found in the passage, so the task of the model\nis to predict where an answer starts and where it ends. As the initial data set\nis rather small for training, we make use of multilingual BERT so that we can\naugment the training data by using data available for languages other than\nArabic. Furthermore, we crawl a large Arabic corpus that is domain specific to\nreligious discourse. Our approach consists of two steps, first we train a BERT\nmodel to predict a set of possible answers in a passage. Finally, we use\nanother BERT based model to rank the candidate answers produced by the first\nBERT model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"What makes a question inquisitive?\" A Study on Type-Controlled Inquisitive Question Generation. (arXiv:2205.08056v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08056","description":"<p>We propose a type-controlled framework for inquisitive question generation.\nWe annotate an inquisitive question dataset with question types, train question\ntype classifiers, and finetune models for type-controlled question generation.\nEmpirical results demonstrate that we can generate a variety of questions that\nadhere to specific types while drawing from the source texts. We also\ninvestigate strategies for selecting a single question from a generated set,\nconsidering both an informative vs.~inquisitive question classifier and a\npairwise ranker trained from a small set of expert annotations. Question\nselection using the pairwise ranker yields strong results in automatic and\nmanual evaluation. Our human evaluation assesses multiple aspects of the\ngenerated questions, finding that the ranker chooses questions with the best\nsyntax (4.59), semantics (4.37), and inquisitiveness (3.92) on a scale of 1-5,\neven rivaling the performance of human-written questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lingyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Debanjan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unbiased Math Word Problems Benchmark for Mitigating Solving Bias. (arXiv:2205.08108v1 [cs.AI])","link":"http://arxiv.org/abs/2205.08108","description":"<p>In this paper, we revisit the solving bias when evaluating models on current\nMath Word Problem (MWP) benchmarks. However, current solvers exist solving bias\nwhich consists of data bias and learning bias due to biased dataset and\nimproper training strategy. Our experiments verify MWP solvers are easy to be\nbiased by the biased training datasets which do not cover diverse questions for\neach problem narrative of all MWPs, thus a solver can only learn shallow\nheuristics rather than deep semantics for understanding problems. Besides, an\nMWP can be naturally solved by multiple equivalent equations while current\ndatasets take only one of the equivalent equations as ground truth, forcing the\nmodel to match the labeled ground truth and ignoring other equivalent\nequations. Here, we first introduce a novel MWP dataset named UnbiasedMWP which\nis constructed by varying the grounded expressions in our collected data and\nannotating them with corresponding multiple new questions manually. Then, to\nfurther mitigate learning bias, we propose a Dynamic Target Selection (DTS)\nStrategy to dynamically select more suitable target expressions according to\nthe longest prefix match between the current model output and candidate\nequivalent equations which are obtained by applying commutative law during\ntraining. The results show that our UnbiasedMWP has significantly fewer biases\nthan its original data and other datasets, posing a promising benchmark for\nfairly evaluating the solvers' reasoning skills rather than matching nearest\nneighbors. And the solvers trained with our DTS achieve higher accuracies on\nmultiple MWP benchmarks. The source code is available at\nhttps://github.com/yangzhch6/UnbiasedMWP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhicheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jinghui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning. (arXiv:2205.08124v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08124","description":"<p>Transfer learning (TL) in natural language processing (NLP) has seen a surge\nof interest in recent years, as pre-trained models have shown an impressive\nability to transfer to novel tasks. Three main strategies have emerged for\nmaking use of multiple supervised datasets during fine-tuning: training on an\nintermediate task before training on the target task (STILTs), using multi-task\nlearning (MTL) to train jointly on a supplementary task and the target task\n(pairwise MTL), or simply using MTL to train jointly on all available datasets\n(MTL-ALL). In this work, we compare all three TL methods in a comprehensive\nanalysis on the GLUE dataset suite. We find that there is a simple heuristic\nfor when to use one of these techniques over the other: pairwise MTL is better\nthan STILTs when the target task has fewer instances than the supporting task\nand vice versa. We show that this holds true in more than 92% of applicable\ncases on the GLUE dataset and validate this hypothesis with experiments varying\ndataset size. The simplicity and effectiveness of this heuristic is surprising\nand warrants additional exploration by the TL community. Furthermore, we find\nthat MTL-ALL is worse than the pairwise methods in almost every case. We hope\nthis study will aid others as they choose between TL methods for NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weller_O/0/1/0/all/0/1\">Orion Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seppi_K/0/1/0/all/0/1\">Kevin Seppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEMI-FND: Stacked Ensemble Based Multimodal Inference For Faster Fake News Detection. (arXiv:2205.08159v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08159","description":"<p>Fake News Detection (FND) is an essential field in natural language\nprocessing that aims to identify and check the truthfulness of major claims in\na news article to decide the news veracity. FND finds its uses in preventing\nsocial, political and national damage caused due to misrepresentation of facts\nwhich may harm a certain section of society. Further, with the explosive rise\nin fake news dissemination over social media, including images and text, it has\nbecome imperative to identify fake news faster and more accurately. To solve\nthis problem, this work investigates a novel multimodal stacked ensemble-based\napproach (SEMIFND) to fake news detection. Focus is also kept on ensuring\nfaster performance with fewer parameters. Moreover, to improve multimodal\nperformance, a deep unimodal analysis is done on the image modality to identify\nNasNet Mobile as the most appropriate model for the task. For text, an ensemble\nof BERT and ELECTRA is used. The approach was evaluated on two datasets:\nTwitter MediaEval and Weibo Corpus. The suggested framework offered accuracies\nof 85.80% and 86.83% on the Twitter and Weibo datasets respectively. These\nreported metrics are found to be superior when compared to similar recent\nworks. Further, we also report a reduction in the number of parameters used in\ntraining when compared to recent relevant works. SEMI-FND offers an overall\nparameter reduction of at least 20% with unimodal parametric reduction on text\nbeing 60%. Therefore, based on the investigations presented, it is concluded\nthat the application of a stacked ensembling significantly improves FND over\nother approaches while also improving speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prabhav Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_R/0/1/0/all/0/1\">Ridam Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_K/0/1/0/all/0/1\">K.P.S. Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vineet Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level Cross-Lingual Speech Representation. (arXiv:2205.08180v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08180","description":"<p>We propose the SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level\nCross-Lingual Speech Representation learning framework. Unlike previous works\non speech representation learning, which learns multilingual contextual speech\nembedding at the resolution of an acoustic frame (10-20ms), this work focuses\non learning multimodal (speech-text) multilingual speech embedding at the\nresolution of a sentence (5-10s) such that the embedding vector space is\nsemantically aligned across different languages. We combine state-of-the-art\nmultilingual acoustic frame-level speech representation learning model XLS-R\nwith the Language Agnostic BERT Sentence Embedding (LaBSE) model to create an\nutterance-level multimodal multilingual speech encoder SAMU-XLSR. Although we\ntrain SAMU-XLSR with only multilingual transcribed speech data, cross-lingual\nspeech-text and speech-speech associations emerge in its learned representation\nspace. To substantiate our claims, we use SAMU-XLSR speech encoder in\ncombination with a pre-trained LaBSE text sentence encoder for cross-lingual\nspeech-to-text translation retrieval, and SAMU-XLSR alone for cross-lingual\nspeech-to-speech translation retrieval. We highlight these applications by\nperforming several cross-lingual text and speech translation retrieval tasks\nacross several datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1\">Sameer Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_A/0/1/0/all/0/1\">Antoine Laurent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SKILL: Structured Knowledge Infusion for Large Language Models. (arXiv:2205.08184v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08184","description":"<p>Large language models (LLMs) have demonstrated human-level performance on a\nvast spectrum of natural language tasks. However, it is largely unexplored\nwhether they can better internalize knowledge from a structured data, such as a\nknowledge graph, or from text. In this work, we propose a method to infuse\nstructured knowledge into LLMs, by directly training T5 models on factual\ntriples of knowledge graphs (KGs). We show that models pre-trained on Wikidata\nKG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as\nwell as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The\nmodels pre-trained on factual triples compare competitively with the ones on\nnatural language sentences that contain the same knowledge. Trained on a\nsmaller size KG, WikiMovies, we saw 3x improvement of exact match score on\nMetaQA task compared to T5 baseline. The proposed method has an advantage that\nno alignment between the knowledge graph and text corpus is required in\ncurating training data. This makes our method particularly useful when working\nwith industry-scale knowledge graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moiseev_F/0/1/0/all/0/1\">Fedor Moiseev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhe Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfonseca_E/0/1/0/all/0/1\">Enrique Alfonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Unsupervised Sentence Compression by Fine-tuning Transformers with Reinforcement Learning. (arXiv:2205.08221v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08221","description":"<p>Sentence compression reduces the length of text by removing non-essential\ncontent while preserving important facts and grammaticality. Unsupervised\nobjective driven methods for sentence compression can be used to create\ncustomized models without the need for ground-truth training data, while\nallowing flexibility in the objective function(s) that are used for learning\nand inference. Recent unsupervised sentence compression approaches use custom\nobjectives to guide discrete search; however, guided search is expensive at\ninference time. In this work, we explore the use of reinforcement learning to\ntrain effective sentence compression models that are also fast when generating\npredictions. In particular, we cast the task as binary sequence labelling and\nfine-tune a pre-trained transformer using a simple policy gradient approach.\nOur approach outperforms other unsupervised models while also being more\nefficient at inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghalandari_D/0/1/0/all/0/1\">Demian Gholipour Ghalandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hokamp_C/0/1/0/all/0/1\">Chris Hokamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ifrim_G/0/1/0/all/0/1\">Georgiana Ifrim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning. (arXiv:2205.08232v1 [cs.AI])","link":"http://arxiv.org/abs/2205.08232","description":"<p>Recently, deep learning models have made great progress in MWP solving on\nanswer accuracy. However, they are uninterpretable since they mainly rely on\nshallow heuristics to achieve high performance without understanding and\nreasoning the grounded math logic. To address this issue and make a step\ntowards interpretable MWP solving, we first construct a high-quality MWP\ndataset named InterMWP which consists of 11,495 MWPs and annotates\ninterpretable logical formulas based on algebraic knowledge as the grounded\nlinguistic logic of each solution equation. Different from existing MWP\ndatasets, our InterMWP benchmark asks for a solver to not only output the\nsolution expressions but also predict the corresponding logical formulas. We\nfurther propose a novel approach with logical prompt and interpretation\ngeneration, called LogicSolver. For each MWP, our LogicSolver first retrieves\nsome highly-correlated algebraic knowledge and then passes them to the backbone\nmodel as prompts to improve the semantic representations of MWPs. With these\nimproved semantic representations, our LogicSolver generates corresponding\nsolution expressions and interpretable knowledge formulas in accord with the\ngenerated solution expressions, simultaneously. Experimental results show that\nour LogicSolver has stronger logical formula-based interpretability than\nbaselines while achieving higher answer accuracy with the help of logical\nprompts, simultaneously.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhicheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jinghui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Letters From the Past: Modeling Historical Sound Change Through Diachronic Character Embeddings. (arXiv:2205.08256v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08256","description":"<p>While a great deal of work has been done on NLP approaches to lexical\nsemantic change detection, other aspects of language change have received less\nattention from the NLP community. In this paper, we address the detection of\nsound change through historical spelling. We propose that a sound change can be\ncaptured by comparing the relative distance through time between their\ndistributions using PPMI character embeddings. We verify this hypothesis in\nsynthetic data and then test the method's ability to trace the well-known\nhistorical change of lenition of plosives in Danish historical sources. We show\nthat the models are able to identify several of the changes under consideration\nand to uncover meaningful contexts in which they appeared. The methodology has\nthe potential to contribute to the study of open questions such as the relative\nchronology of sound shifts and their geographical distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boldsen_S/0/1/0/all/0/1\">Sidsel Boldsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paggio_P/0/1/0/all/0/1\">Patrizia Paggio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling Math Word Problems with Fine-to-Coarse Abstracting and Reasoning. (arXiv:2205.08274v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08274","description":"<p>Math Word Problems (MWP) is an important task that requires the ability of\nunderstanding and reasoning over mathematical text. Existing approaches mostly\nformalize it as a generation task by adopting Seq2Seq or Seq2Tree models to\nencode an input math problem in natural language as a global representation and\ngenerate the output mathematical expression. Such approaches only learn shallow\nheuristics and fail to capture fine-grained variations in inputs. In this\npaper, we propose to model a math word problem in a fine-to-coarse manner to\ncapture both the local fine-grained information and the global logical\nstructure of it. Instead of generating a complete equation sequence or\nexpression tree from the global features, we iteratively combine low-level\noperands to predict a higher-level operator, abstracting the problem and\nreasoning about the solving operators from bottom to up. Our model is naturally\nmore sensitive to local variations and can better generalize to unseen problem\ntypes. Extensive evaluations on Math23k and SVAMP datasets demonstrate the\naccuracy and robustness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ailisi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xueyao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Alignment Bias in Neural Seq2Seq Semantic Parsers. (arXiv:2205.08288v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08288","description":"<p>Prior to deep learning the semantic parsing community has been interested in\nunderstanding and modeling the range of possible word alignments between\nnatural language sentences and their corresponding meaning representations.\nSequence-to-sequence models changed the research landscape suggesting that we\nno longer need to worry about alignments since they can be learned\nautomatically by means of an attention mechanism. More recently, researchers\nhave started to question such premise. In this work we investigate whether\nseq2seq models can handle both simple and complex alignments. To answer this\nquestion we augment the popular Geo semantic parsing dataset with alignment\nannotations and create Geo-Aligned. We then study the performance of standard\nseq2seq models on the examples that can be aligned monotonically versus\nexamples that require more complex alignments. Our empirical study shows that\nperformance is significantly better over monotonic alignments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Locatelli_D/0/1/0/all/0/1\">Davide Locatelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quattoni_A/0/1/0/all/0/1\">Ariadna Quattoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias and Fairness on Multimodal Emotion Detection Algorithms. (arXiv:2205.08383v1 [cs.LG])","link":"http://arxiv.org/abs/2205.08383","description":"<p>Numerous studies have shown that machine learning algorithms can latch onto\nprotected attributes such as race and gender and generate predictions that\nsystematically discriminate against one or more groups. To date the majority of\nbias and fairness research has been on unimodal models. In this work, we\nexplore the biases that exist in emotion recognition systems in relationship to\nthe modalities utilized, and study how multimodal approaches affect system bias\nand fairness. We consider audio, text, and video modalities, as well as all\npossible multimodal combinations of those, and find that text alone has the\nleast bias, and accounts for the majority of the models' performances, raising\ndoubts about the worthiness of multimodal emotion recognition systems when bias\nand fairness are desired alongside model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmitz_M/0/1/0/all/0/1\">Matheus Schmitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_R/0/1/0/all/0/1\">Rehan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jimi Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Evaluation Framework for Legal Document Summarization. (arXiv:2205.08478v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08478","description":"<p>A law practitioner has to go through numerous lengthy legal case proceedings\nfor their practices of various categories, such as land dispute, corruption,\netc. Hence, it is important to summarize these documents, and ensure that\nsummaries contain phrases with intent matching the category of the case. To the\nbest of our knowledge, there is no evaluation metric that evaluates a summary\nbased on its intent. We propose an automated intent-based summarization metric,\nwhich shows a better agreement with human evaluation as compared to other\nautomated metrics like BLEU, ROUGE-L etc. in terms of human satisfaction. We\nalso curate a dataset by annotating intent phrases in legal documents, and show\na proof of concept as to how this system can be automated. Additionally, all\nthe code and data to generate reproducible results is available on Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mullick_A/0/1/0/all/0/1\">Ankan Mullick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandy_A/0/1/0/all/0/1\">Abhilash Nandy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapadnis_M/0/1/0/all/0/1\">Manav Nitin Kapadnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patnaik_S/0/1/0/all/0/1\">Sohan Patnaik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghav_R/0/1/0/all/0/1\">R Raghav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_R/0/1/0/all/0/1\">Roshni Kar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Aggregation in Zero-Shot Cross-Lingual Transfer Using Multilingual BERT. (arXiv:2205.08497v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08497","description":"<p>Multilingual BERT (mBERT), a language model pre-trained on large multilingual\ncorpora, has impressive zero-shot cross-lingual transfer capabilities and\nperforms surprisingly well on zero-shot POS tagging and Named Entity\nRecognition (NER), as well as on cross-lingual model transfer. At present, the\nmainstream methods to solve the cross-lingual downstream tasks are always using\nthe last transformer layer's output of mBERT as the representation of\nlinguistic information. In this work, we explore the complementary property of\nlower layers to the last transformer layer of mBERT. A feature aggregation\nmodule based on an attention mechanism is proposed to fuse the information\ncontained in different layers of mBERT. The experiments are conducted on four\nzero-shot cross-lingual transfer datasets, and the proposed method obtains\nperformance improvements on key multilingual benchmark tasks XNLI (+1.5 %),\nPAWS-X (+2.4 %), NER (+1.2 F1), and POS (+1.5 F1). Through the analysis of the\nexperimental results, we prove that the layers before the last layer of mBERT\ncan provide extra useful information for cross-lingual downstream tasks and\nexplore the interpretability of mBERT empirically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beiduo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_K/0/1/0/all/0/1\">Kun Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recovering Private Text in Federated Learning of Language Models. (arXiv:2205.08514v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08514","description":"<p>Federated learning allows distributed users to collaboratively train a model\nwhile keeping each user's data private. Recently, a growing body of work has\ndemonstrated that an eavesdropping attacker can effectively recover image data\nfrom gradients transmitted during federated learning. However, little progress\nhas been made in recovering text data. In this paper, we present a novel attack\nmethod FILM for federated learning of language models -- for the first time, we\nshow the feasibility of recovering text from large batch sizes of up to 128\nsentences. Different from image-recovery methods which are optimized to match\ngradients, we take a distinct approach that first identifies a set of words\nfrom gradients and then directly reconstructs sentences based on beam search\nand a prior-based reordering strategy. The key insight of our attack is to\nleverage either prior knowledge in pre-trained language models or memorization\nduring training. Despite its simplicity, we demonstrate that FILM can work well\nwith several large-scale datasets -- it can extract single sentences with high\nfidelity even for large batch sizes and recover multiple sentences from the\nbatch successfully if the attack is applied iteratively. We hope our results\ncan motivate future work in developing stronger attacks as well as new defense\nmethods for training language models in federated learning. Our code is\npublicly available at https://github.com/Princeton-SysML/FILM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Samyak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yangsibo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Plagiarism in Introductory Programming Course Assignments. (arXiv:2205.08520v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08520","description":"<p>Measuring plagiarism in programming assignments is an essential task to the\neducational procedure. This paper discusses the methods of plagiarism and its\ndetection in introductory programming course assignments written in C++. A\nsmall corpus of assignments is made publically available. A general framework\nto compute the similarity between a solution pair is developed that uses the\nthree token-based similarity methods as features and predicts if the solution\nis plagiarized. The importance of each feature is also measured, which in\nreturn ranks the effectiveness of each method in use. Finally, the artificially\ngenerated dataset improves the results compared to the original data. We\nachieved an F1 score of 0.955 and 0.971 on original and synthetic datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humayoun_M/0/1/0/all/0/1\">Muhammad Humayoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashmi_M/0/1/0/all/0/1\">Muhammad Adnan Hashmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Ali Hanzala Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Human Evaluation of Machine Translation across Language Pairs. (arXiv:2205.08533v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08533","description":"<p>Obtaining meaningful quality scores for machine translation systems through\nhuman evaluation remains a challenge given the high variability between human\nevaluators, partly due to subjective expectations for translation quality for\ndifferent language pairs. We propose a new metric called XSTS that is more\nfocused on semantic equivalence and a cross-lingual calibration method that\nenables more consistent assessment. We demonstrate the effectiveness of these\nnovel contributions in large scale evaluation studies across up to 14 language\npairs, with translation both into and out of English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Licht_D/0/1/0/all/0/1\">Daniel Licht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cynthia Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_J/0/1/0/all/0/1\">Janice Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distantly-Supervised Long-Tailed Relation Extraction Using Constraint Graphs. (arXiv:2105.11225v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.11225","description":"<p>Label noise and long-tailed distributions are two major challenges in\ndistantly supervised relation extraction. Recent studies have shown great\nprogress on denoising, but paid little attention to the problem of long-tailed\nrelations. In this paper, we introduce a constraint graph to model the\ndependencies between relation labels. On top of that, we further propose a\nnovel constraint graph-based relation extraction framework(CGRE) to handle the\ntwo challenges simultaneously. CGRE employs graph convolution networks to\npropagate information from data-rich relation nodes to data-poor relation\nnodes, and thus boosts the representation learning of long-tailed relations. To\nfurther improve the noise immunity, a constraint-aware attention module is\ndesigned in CGRE to integrate the constraint information. Extensive\nexperimental results indicate that CGRE achieves significant improvements over\nthe previous methods for both denoising and long-tailed relation extraction.\nThe pre-processed datasets and source code are publicly available at\nhttps://github.com/tmliang/CGRE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tianming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1\">Gaurav Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Maozu Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KGAP: Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.03861","description":"<p>Identifying political perspectives in news media has become an important task\ndue to the rapid growth of political commentary and the increasingly polarized\npolitical ideologies. Previous approaches focus on textual content and leave\nout the rich social and political context that is essential in the perspective\ndetection process. To address this limitation, we propose KGAP, a political\nperspective detection method that incorporates external domain knowledge.\nSpecifically, we construct a political knowledge graph to serve as\ndomain-specific external knowledge. We then construct heterogeneous information\nnetworks to represent news documents, which jointly model news text and\nexternal knowledge. Finally, we adopt relational graph neural networks and\nconduct political perspective detection as graph-level classification.\nExtensive experiments demonstrate that our method consistently achieves the\nbest performance on two real-world perspective detection benchmarks. Ablation\nstudies further bear out the necessity of external knowledge and the\neffectiveness of our graph-based approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding. (arXiv:2109.06838v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06838","description":"<p>While large language models have shown exciting progress on several NLP\nbenchmarks, evaluating their ability for complex analogical reasoning remains\nunder-explored. Here, we introduce a high-quality crowdsourced dataset of\nnarratives for employing proverbs in context as a benchmark for abstract\nlanguage understanding. The dataset provides fine-grained annotation of aligned\nspans between proverbs and narratives, and contains minimal lexical overlaps\nbetween narratives and proverbs, ensuring that models need to go beyond\nsurface-level reasoning to succeed. We explore three tasks: (1) proverb\nrecommendation and alignment prediction, (2) narrative generation for a given\nproverb and topic, and (3) identifying narratives with similar motifs. Our\nexperiments show that neural language models struggle on these tasks compared\nto humans, and these tasks pose multiple learning challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents. (arXiv:2109.10341v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10341","description":"<p>Document-level neural machine translation (DocNMT) achieves coherent\ntranslations by incorporating cross-sentence context. However, for most\nlanguage pairs there's a shortage of parallel documents, although parallel\nsentences are readily available. In this paper, we study whether and how\ncontextual modeling in DocNMT is transferable via multilingual modeling. We\nfocus on the scenario of zero-shot transfer from teacher languages with\ndocument level data to student languages with no documents but sentence level\ndata, and for the first time treat document-level translation as a transfer\nlearning problem. Using simple concatenation-based DocNMT, we explore the\neffect of 3 factors on the transfer: the number of teacher languages with\ndocument level data, the balance between document and sentence level data at\ntraining, and the data condition of parallel documents (genuine vs.\nbacktranslated). Our experiments on Europarl-7 and IWSLT-10 show the\nfeasibility of multilingual transfer for DocNMT, particularly on\ndocument-specific metrics. We observe that more teacher languages and adequate\ndata balance both contribute to better transfer quality. Surprisingly, the\ntransfer is less sensitive to the data condition, where multilingual DocNMT\ndelivers decent performance with either backtranslated or genuine document\npairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabirmoghaddam_A/0/1/0/all/0/1\">Ali Dabirmoghaddam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arivazhagan_N/0/1/0/all/0/1\">Naveen Arivazhagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Retrieve Passages without Supervision. (arXiv:2112.07708v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07708","description":"<p>Dense retrievers for open-domain question answering (ODQA) have been shown to\nachieve impressive performance by training on large datasets of\nquestion-passage pairs. In this work we ask whether this dependence on labeled\ndata can be reduced via unsupervised pretraining that is geared towards ODQA.\nWe show this is in fact possible, via a novel pretraining scheme designed for\nretrieval. Our \"recurring span retrieval\" approach uses recurring spans across\npassages in a document to create pseudo examples for contrastive learning. Our\npretraining scheme directly controls for term overlap across pseudo queries and\nrelevant passages, thus allowing to model both lexical and semantic relations\nbetween them. The resulting model, named Spider, performs surprisingly well\nwithout any labeled training examples on a wide range of ODQA datasets.\nSpecifically, it significantly outperforms all other pretrained baselines in a\nzero-shot setting, and is competitive with BM25, a strong sparse baseline.\nMoreover, a hybrid retriever over Spider and BM25 improves over both, and is\noften competitive with DPR models, which are trained on tens of thousands of\nexamples. Last, notable gains are observed when using Spider as an\ninitialization for supervised training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shachaf_G/0/1/0/all/0/1\">Gal Shachaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants. (arXiv:2112.09062v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09062","description":"<p>In Dynamic Adversarial Data Collection (DADC), human annotators are tasked\nwith finding examples that models struggle to predict correctly. Models trained\non DADC-collected training data have been shown to be more robust in\nadversarial and out-of-domain settings, and are considerably harder for humans\nto fool. However, DADC is more time-consuming than traditional data collection\nand thus more costly per annotated example. In this work, we examine whether we\ncan maintain the advantages of DADC, without incurring the additional cost. To\nthat end, we introduce Generative Annotation Assistants (GAAs),\ngenerator-in-the-loop models that provide real-time suggestions that annotators\ncan either approve, modify, or reject entirely. We collect training datasets in\ntwenty experimental settings and perform a detailed analysis of this approach\nfor the task of extractive question answering (QA) for both standard and\nadversarial data collection. We demonstrate that GAAs provide significant\nefficiency benefits with over a 30% annotation speed-up, while leading to over\na 5x improvement in model fooling rates. In addition, we find that using\nGAA-assisted training data leads to higher downstream model performance on a\nvariety of question answering tasks over adversarial data collection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1\">Max Bartolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Black-Box Tuning for Language-Model-as-a-Service. (arXiv:2201.03514v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03514","description":"<p>Extremely large pre-trained language models (PTMs) such as GPT-3 are usually\nreleased as a service. It allows users to design task-specific prompts to query\nthe PTMs through some black-box APIs. In such a scenario, which we call\nLanguage-Model-as-a-Service (LMaaS), the gradients of PTMs are usually\nunavailable. Can we optimize the task prompts by only accessing the model\ninference APIs? This paper proposes the black-box tuning framework to optimize\nthe continuous prompt prepended to the input text via derivative-free\noptimization. Instead of optimizing in the original high-dimensional prompt\nspace, which is intractable for traditional derivative-free optimization, we\nperform optimization in a randomly generated subspace due to the low intrinsic\ndimensionality of large PTMs. The experimental results show that the black-box\ntuning with RoBERTa on a few labeled samples not only significantly outperforms\nmanual prompt and GPT-3's in-context learning, but also surpasses the\ngradient-based counterparts, i.e., prompt tuning and full model tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hong Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models. (arXiv:2202.13392v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13392","description":"<p>Pre-trained language models (PLMs) cannot well recall rich factual knowledge\nof entities exhibited in large-scale corpora, especially those rare entities.\nIn this paper, we propose to build a simple but effective Pluggable Entity\nLookup Table (PELT) on demand by aggregating the entity's output\nrepresentations of multiple occurrences in the corpora. PELT can be compatibly\nplugged as inputs to infuse supplemental entity knowledge into PLMs. Compared\nto previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation\nwith capability of acquiring knowledge from out-of-domain corpora for domain\nadaptation scenario. The experiments on knowledge-related tasks demonstrate\nthat our method, PELT, can flexibly and effectively transfer entity knowledge\nfrom related corpora into PLMs with different architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diagonal State Spaces are as Effective as Structured State Spaces. (arXiv:2203.14343v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.14343","description":"<p>Modeling long range dependencies in sequential data is a fundamental step\ntowards attaining human-level performance in many modalities such as text,\nvision, audio and video. While attention-based models are a popular and\neffective choice in modeling short-range interactions, their performance on\ntasks requiring long range reasoning has been largely inadequate. In an\nexciting result, Gu et al. (ICLR 2022) proposed the $\\textit{Structured State\nSpace}$ (S4) architecture delivering large gains over state-of-the-art models\non several long-range tasks across various modalities. The core proposition of\nS4 is the parameterization of state matrices via a diagonal plus low rank\nstructure, allowing efficient computation. In this work, we show that one can\nmatch the performance of S4 even without the low rank correction and thus\nassuming the state matrices to be diagonal. Our $\\textit{Diagonal State Space}$\n(DSS) model matches the performance of S4 on Long Range Arena tasks, speech\nclassification on Speech Commands dataset, while being conceptually simpler and\nstraightforward to implement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1\">Albert Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Gender Debiasing Affects Internal Model Representations, and Why It Matters. (arXiv:2204.06827v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06827","description":"<p>Common studies of gender bias in NLP focus either on extrinsic bias measured\nby model performance on a downstream task or on intrinsic bias found in models'\ninternal representations. However, the relationship between extrinsic and\nintrinsic bias is relatively unknown. In this work, we illuminate this\nrelationship by measuring both quantities together: we debias a model during\ndownstream fine-tuning, which reduces extrinsic bias, and measure the effect on\nintrinsic bias, which is operationalized as bias extractability with\ninformation-theoretic probing. Through experiments on two tasks and multiple\nbias metrics, we show that our intrinsic bias metric is a better indicator of\ndebiasing than (a contextual adaptation of) the standard WEAT metric, and can\nalso expose cases of superficial debiasing. Our framework provides a\ncomprehensive perspective on bias in NLP models, which can be applied to deploy\nNLP systems in a more informed manner. Our code and model checkpoints are\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orgad_H/0/1/0/all/0/1\">Hadas Orgad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldfarb_Tarrant_S/0/1/0/all/0/1\">Seraphina Goldfarb-Tarrant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Few-Shot Learning with FASL. (arXiv:2204.09347v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09347","description":"<p>Recent advances in natural language processing (NLP) have led to strong text\nclassification models for many tasks. However, still often thousands of\nexamples are needed to train models with good quality. This makes it\nchallenging to quickly develop and deploy new models for real world problems\nand business needs. Few-shot learning and active learning are two lines of\nresearch, aimed at tackling this problem. In this work, we combine both lines\ninto FASL, a platform that allows training text classification models using an\niterative and fast process. We investigate which active learning methods work\nbest in our few-shot setup. Additionally, we develop a model to predict when to\nstop annotating. This is relevant as in a few-shot setup we do not have access\nto a large validation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Torro_G/0/1/0/all/0/1\">Guillermo P&#xe9;rez-Torr&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_A/0/1/0/all/0/1\">Angelo Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_Salvador_M/0/1/0/all/0/1\">Marc Franco-Salvador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09817","description":"<p>Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision-language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1\">Stephanie Hyland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1\">Maria Wetscherek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero and Few-shot Learning for Author Profiling. (arXiv:2204.10543v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10543","description":"<p>Author profiling classifies author characteristics by analyzing how language\nis shared among people. In this work, we study that task from a low-resource\nviewpoint: using little or no training data. We explore different zero and\nfew-shot models based on entailment and evaluate our systems on several\nprofiling tasks in Spanish and English. In addition, we study the effect of\nboth the entailment hypothesis and the size of the few-shot training sample. We\nfind that entailment-based models out-perform supervised text classifiers based\non roberta-XLM and that we can reach 80% of the accuracy of previous approaches\nusing less than 50\\% of the training data on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chinea_Rios_M/0/1/0/all/0/1\">Mara Chinea-Rios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarracen_G/0/1/0/all/0/1\">Gretel Liz De la Pe&#xf1;a Sarrac&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangel_F/0/1/0/all/0/1\">Francisco Rangel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_Salvador_M/0/1/0/all/0/1\">Marc Franco-Salvador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heroes, Villains, and Victims, and GPT-3: Automated Extraction of Character Roles Without Training Data. (arXiv:2205.07557v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.07557","description":"<p>This paper shows how to use large-scale pre-trained language models to\nextract character roles from narrative texts without training data. Queried\nwith a zero-shot question-answering prompt, GPT-3 can identify the hero,\nvillain, and victim in diverse domains: newspaper articles, movie plot\nsummaries, and political speeches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stammbach_D/0/1/0/all/0/1\">Dominik Stammbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniak_M/0/1/0/all/0/1\">Maria Antoniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ash_E/0/1/0/all/0/1\">Elliott Ash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CQR-SQL: Conversational Question Reformulation Enhanced Context-Dependent Text-to-SQL Parsers. (arXiv:2205.07686v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.07686","description":"<p>Context-dependent text-to-SQL is the task of translating multi-turn questions\ninto database-related SQL queries. Existing methods typically focus on making\nfull use of history context or previously predicted SQL for currently SQL\nparsing, while neglecting to explicitly comprehend the schema and\nconversational dependency, such as co-reference, ellipsis and user focus\nchange. In this paper, we propose CQR-SQL, which uses auxiliary Conversational\nQuestion Reformulation (CQR) learning to explicitly exploit schema and decouple\ncontextual dependency for SQL parsing. Specifically, we first present a schema\nenhanced recursive CQR method to produce domain-relevant self-contained\nquestions. Secondly, we train CQR-SQL models to map the semantics of multi-turn\nquestions and auxiliary self-contained questions into the same latent space\nthrough schema grounding consistency task and tree-structured SQL parsing\nconsistency task, which enhances the abilities of SQL parsing by adequately\ncontextual understanding. At the time of writing, our CQR-SQL achieves new\nstate-of-the-art results on two context-dependent text-to-SQL benchmarks SParC\nand CoSQL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1\">Dongling Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1\">Linzheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian-Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Functional2Structural: Cross-Modality Brain Networks Representation Learning. (arXiv:2205.07854v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07854","description":"<p>MRI-based modeling of brain networks has been widely used to understand\nfunctional and structural interactions and connections among brain regions, and\nfactors that affect them, such as brain development and disease. Graph mining\non brain networks may facilitate the discovery of novel biomarkers for clinical\nphenotypes and neurodegenerative diseases. Since brain networks derived from\nfunctional and structural MRI describe the brain topology from different\nperspectives, exploring a representation that combines these cross-modality\nbrain networks is non-trivial. Most current studies aim to extract a fused\nrepresentation of the two types of brain network by projecting the structural\nnetwork to the functional counterpart. Since the functional network is dynamic\nand the structural network is static, mapping a static object to a dynamic\nobject is suboptimal. However, mapping in the opposite direction is not\nfeasible due to the non-negativity requirement of current graph learning\ntechniques. Here, we propose a novel graph learning framework, known as Deep\nSigned Brain Networks (DSBN), with a signed graph encoder that, from an\nopposite perspective, learns the cross-modality representations by projecting\nthe functional network to the structural counterpart. We validate our framework\non clinical phenotype and neurodegenerative disease prediction tasks using two\nindependent, publicly available datasets (HCP and OASIS). The experimental\nresults clearly demonstrate the advantages of our model compared to several\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoteng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiyao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yalin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackin_S/0/1/0/all/0/1\">Scott Mackin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajilore_O/0/1/0/all/0/1\">Olusola Ajilore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leow_A/0/1/0/all/0/1\">Alex Leow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_P/0/1/0/all/0/1\">Paul Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Liang Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy Enhancement for Cloud-Based Few-Shot Learning. (arXiv:2205.07864v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07864","description":"<p>Requiring less data for accurate models, few-shot learning has shown\nrobustness and generality in many application domains. However, deploying\nfew-shot models in untrusted environments may inflict privacy concerns, e.g.,\nattacks or adversaries that may breach the privacy of user-supplied data. This\npaper studies the privacy enhancement for the few-shot learning in an untrusted\nenvironment, e.g., the cloud, by establishing a novel privacy-preserved\nembedding space that preserves the privacy of data and maintains the accuracy\nof the model. We examine the impact of various image privacy methods such as\nblurring, pixelization, Gaussian noise, and differentially private pixelization\n(DP-Pix) on few-shot image classification and propose a method that learns\nprivacy-preserved representation through the joint loss. The empirical results\nshow how privacy-performance trade-off can be negotiated for privacy-enhanced\nfew-shot learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parnami_A/0/1/0/all/0/1\">Archit Parnami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usama_M/0/1/0/all/0/1\">Muhammad Usama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Liyue Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minwoo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Primal-Dual UNet for Sparse View Cone Beam Computed Tomography Volume Reconstruction. (arXiv:2205.07866v1 [eess.IV])","link":"http://arxiv.org/abs/2205.07866","description":"<p>In this paper, the Primal-Dual UNet for sparse view CT reconstruction is\nmodified to be applicable to cone beam projections and perform reconstructions\nof entire volumes instead of slices. Experiments show that the PSNR of the\nproposed method is increased by 10dB compared to the direct FDK reconstruction\nand almost 3dB compared to the modified original Primal-Dual Network when using\nonly 23 projections. The presented network is not optimized wrt. memory\nconsumption or hyperparameters but merely serves as a proof of concept and is\nlimited to low resolution projections and volumes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ernst_P/0/1/0/all/0/1\">Philipp Ernst</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumick Chatterjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rose_G/0/1/0/all/0/1\">Georg Rose</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Updates of a Pre-trained Model for Few-shot Learning. (arXiv:2205.07874v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07874","description":"<p>Most of the recent few-shot learning algorithms are based on transfer\nlearning, where a model is pre-trained using a large amount of source data, and\nthe pre-trained model is updated using a small amount of target data afterward.\nIn transfer-based few-shot learning, sophisticated pre-training methods have\nbeen widely studied for universal and improved representation. However, there\nis little study on updating pre-trained models for few-shot learning. In this\npaper, we compare the two popular updating methods, fine-tuning (i.e., updating\nthe entire network) and linear probing (i.e., updating only the linear\nclassifier), considering the distribution shift between the source and target\ndata. We find that fine-tuning is better than linear probing as the number of\nsamples increases, regardless of distribution shift. Next, we investigate the\neffectiveness and ineffectiveness of data augmentation when pre-trained models\nare fine-tuned. Our fundamental analyses demonstrate that careful\nconsiderations of the details about updating pre-trained models are required\nfor better few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yujin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jaehoon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungnyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Driven Interpolation for Super-Scarce X-Ray Computed Tomography. (arXiv:2205.07888v1 [eess.IV])","link":"http://arxiv.org/abs/2205.07888","description":"<p>We address the problem of reconstructing X-Ray tomographic images from scarce\nmeasurements by interpolating missing acquisitions using a self-supervised\napproach. To do so, we train shallow neural networks to combine two\nneighbouring acquisitions into an estimated measurement at an intermediate\nangle. This procedure yields an enhanced sequence of measurements that can be\nreconstructed using standard methods, or further enhanced using regularisation\napproaches.\n</p>\n<p>Unlike methods that improve the sequence of acquisitions using an initial\ndeterministic interpolation followed by machine-learning enhancement, we focus\non inferring one measurement at once. This allows the method to scale to 3D,\nthe computation to be faster and crucially, the interpolation to be\nsignificantly better than the current methods, when they exist. We also\nestablish that a sequence of measurements must be processed as such, rather\nthan as an image or a volume. We do so by comparing interpolation and\nup-sampling methods, and find that the latter significantly under-perform.\n</p>\n<p>We compare the performance of the proposed method against deterministic\ninterpolation and up-sampling procedures and find that it outperforms them,\neven when used jointly with a state-of-the-art projection-data enhancement\napproach using machine-learning. These results are obtained for 2D and 3D\nimaging, on large biomedical datasets, in both projection space and image\nspace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Valat_E/0/1/0/all/0/1\">Emilien Valat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farrahi_K/0/1/0/all/0/1\">Katayoun Farrahi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blumensath_T/0/1/0/all/0/1\">Thomas Blumensath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Apprenticeship Learning for Playing Games. (arXiv:2205.07959v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07959","description":"<p>In the last decade, deep learning has achieved great success in machine\nlearning tasks where the input data is represented with different levels of\nabstractions. Driven by the recent research in reinforcement learning using\ndeep neural networks, we explore the feasibility of designing a learning model\nbased on expert behaviour for complex, multidimensional tasks where reward\nfunction is not available. We propose a novel method for apprenticeship\nlearning based on the previous research on supervised learning techniques in\nreinforcement learning. Our method is applied to video frames from Atari games\nin order to teach an artificial agent to play those games. Even though the\nreported results are not comparable with the state-of-the-art results in\nreinforcement learning, we demonstrate that such an approach has the potential\nto achieve strong performance in the future and is worthwhile for further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Markovikj_D/0/1/0/all/0/1\">Dejan Markovikj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Visual Counterfactual Explanations in Image Space. (arXiv:2205.07972v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07972","description":"<p>Visual counterfactual explanations (VCEs) in image space are an important\ntool to understand decisions of image classifiers as they show under which\nchanges of the image the decision of the classifier would change. Their\ngeneration in image space is challenging and requires robust models due to the\nproblem of adversarial examples. Existing techniques to generate VCEs in image\nspace suffer from spurious changes in the background. Our novel perturbation\nmodel for VCEs together with its efficient optimization via our novel\nAuto-Frank-Wolfe scheme yields sparse VCEs which are significantly more\nobject-centric. Moreover, we show that VCEs can be used to detect undesired\nbehavior of ImageNet classifiers due to spurious features in the ImageNet\ndataset and discuss how estimates of the data-generating distribution can be\nused for VCEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boreiko_V/0/1/0/all/0/1\">Valentyn Boreiko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augustin_M/0/1/0/all/0/1\">Maximilian Augustin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1\">Francesco Croce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berens_P/0/1/0/all/0/1\">Philipp Berens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1\">Matthias Hein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TOCH: Spatio-Temporal Object Correspondence to Hand for Motion Refinement. (arXiv:2205.07982v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07982","description":"<p>We present TOCH, a method for refining incorrect 3D hand-object interaction\nsequences using a data prior. Existing hand trackers, especially those that\nrely on very few cameras, often produce visually unrealistic results with\nhand-object intersection or missing contacts. Although correcting such errors\nrequires reasoning about temporal aspects of interaction, most previous work\nfocus on static grasps and contacts. The core of our method are TOCH fields, a\nnovel spatio-temporal representation for modeling correspondences between hands\nand objects during interaction. The key component is a point-wise\nobject-centric representation which encodes the hand position relative to the\nobject. Leveraging this novel representation, we learn a latent manifold of\nplausible TOCH fields with a temporal denoising auto-encoder. Experiments\ndemonstrate that TOCH outperforms state-of-the-art (SOTA) 3D hand-object\ninteraction models, which are limited to static grasps and contacts. More\nimportantly, our method produces smooth interactions even before and after\ncontact. Using a single trained TOCH model, we quantitatively and qualitatively\ndemonstrate its usefulness for 1) correcting erroneous reconstruction results\nfrom off-the-shelf RGB/RGB-D hand-object reconstruction methods, 2) de-noising,\nand 3) grasp transfer across objects. We will release our code and trained\nmodel on our project page at <a href=\"http://virtualhumans.mpi-inf.mpg.de/toch/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Keyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar_B/0/1/0/all/0/1\">Bharat Bhatnagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenssen_J/0/1/0/all/0/1\">Jan Eric Lenssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-Time Adaptation with Shape Moments for Image Segmentation. (arXiv:2205.07983v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07983","description":"<p>Supervised learning is well-known to fail at generalization under\ndistribution shifts. In typical clinical settings, the source data is\ninaccessible and the target distribution is represented with a handful of\nsamples: adaptation can only happen at test time on a few or even a single\nsubject(s). We investigate test-time single-subject adaptation for\nsegmentation, and propose a Shape-guided Entropy Minimization objective for\ntackling this task. During inference for a single testing subject, our loss is\nminimized with respect to the batch normalization's scale and bias parameters.\nWe show the potential of integrating various shape priors to guide adaptation\nto plausible solutions, and validate our method in two challenging scenarios:\nMRI-to-CT adaptation of cardiac segmentation and cross-site adaptation of\nprostate segmentation. Our approach exhibits substantially better performances\nthan the existing test-time adaptation methods. Even more surprisingly, it\nfares better than state-of-the-art domain adaptation methods, although it\nforgoes training on additional target data during adaptation. Our results\nquestion the usefulness of training on target data in segmentation adaptation,\nand points to the substantial effect of shape priors on test-time inference.\nOur framework can be readily used for integrating various priors and for\nadapting any segmentation network, and our code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bateson_M/0/1/0/all/0/1\">Mathilde Bateson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombaert_H/0/1/0/all/0/1\">Herv&#xe9; Lombaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lost in Compression: the Impact of Lossy Image Compression on Variable Size Object Detection within Infrared Imagery. (arXiv:2205.08002v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08002","description":"<p>Lossy image compression strategies allow for more efficient storage and\ntransmission of data by encoding data to a reduced form. This is essential\nenable training with larger datasets on less storage-equipped environments.\nHowever, such compression can cause severe decline in performance of deep\nConvolution Neural Network (CNN) architectures even when mild compression is\napplied and the resulting compressed imagery is visually identical. In this\nwork, we apply the lossy JPEG compression method with six discrete levels of\nincreasing compression {95, 75, 50, 15, 10, 5} to infrared band (thermal)\nimagery. Our study quantitatively evaluates the affect that increasing levels\nof lossy compression has upon the performance of characteristically diverse\nobject detection architectures (Cascade-RCNN, FSAF and Deformable DETR) with\nrespect to varying sizes of objects present in the dataset. When training and\nevaluating on uncompressed data as a baseline, we achieve maximal mean Average\nPrecision (mAP) of 0.823 with Cascade R-CNN across the FLIR dataset,\noutperforming prior work. The impact of the lossy compression is more extreme\nat higher compression levels (15, 10, 5) across all three CNN architectures.\nHowever, re-training models on lossy compressed imagery notably ameliorated\nperformances for all three CNN models with an average increment of ~76% (at\nhigher compression level 5). Additionally, we demonstrate the relative\nsensitivity of differing object areas {tiny, small, medium, large} with respect\nto the compression level. We show that tiny and small objects are more\nsensitive to compression than medium and large objects. Overall, Cascade R-CNN\nattains the maximal mAP across most of the object area categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhowmik_N/0/1/0/all/0/1\">Neelanjan Bhowmik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barker_J/0/1/0/all/0/1\">Jack W. Barker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaus_Y/0/1/0/all/0/1\">Yona Falinie A. Gaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1\">Toby P. Breckon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual learning on 3D point clouds with random compressed rehearsal. (arXiv:2205.08013v1 [cs.LG])","link":"http://arxiv.org/abs/2205.08013","description":"<p>Contemporary deep neural networks offer state-of-the-art results when applied\nto visual reasoning, e.g., in the context of 3D point cloud data. Point clouds\nare important datatype for precise modeling of three-dimensional environments,\nbut effective processing of this type of data proves to be challenging. In the\nworld of large, heavily-parameterized network architectures and\ncontinuously-streamed data, there is an increasing need for machine learning\nmodels that can be trained on additional data. Unfortunately, currently\navailable models cannot fully leverage training on additional data without\nlosing their past knowledge. Combating this phenomenon, called catastrophic\nforgetting, is one of the main objectives of continual learning. Continual\nlearning for deep neural networks has been an active field of research,\nprimarily in 2D computer vision, natural language processing, reinforcement\nlearning, and robotics. However, in 3D computer vision, there are hardly any\ncontinual learning solutions specifically designed to take advantage of point\ncloud structure. This work proposes a novel neural network architecture capable\nof continual learning on 3D point cloud data. We utilize point cloud structure\nproperties for preserving a heavily compressed set of past data. By using\nrehearsal and reconstruction as regularization methods of the learning process,\nour approach achieves a significant decrease of catastrophic forgetting\ncompared to the existing solutions on several most popular point cloud datasets\nconsidering two continual learning settings: when a task is known beforehand,\nand in the challenging scenario of when task information is unknown to the\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zamorski_M/0/1/0/all/0/1\">Maciej Zamorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stypulkowski_M/0/1/0/all/0/1\">Micha&#x142; Stypu&#x142;kowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanowski_K/0/1/0/all/0/1\">Konrad Karanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zieba_M/0/1/0/all/0/1\">Maciej Zi&#x119;ba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection and Physical Interaction with Deformable Linear Objects. (arXiv:2205.08041v1 [cs.RO])","link":"http://arxiv.org/abs/2205.08041","description":"<p>Deformable linear objects (e.g., cables, ropes, and threads) commonly appear\nin our everyday lives. However, perception of these objects and the study of\nphysical interaction with them is still a growing area. There have already been\nsuccessful methods to model and track deformable linear objects. However, the\nnumber of methods that can automatically extract the initial conditions in\nnon-trivial situations for these methods has been limited, and they have been\nintroduced to the community only recently. On the other hand, while physical\ninteraction with these objects has been done with ground manipulators, there\nhave not been any studies on physical interaction and manipulation of the\ndeformable linear object with aerial robots.\n</p>\n<p>This workshop describes our recent work on detecting deformable linear\nobjects, which uses the segmentation output of the existing methods to provide\nthe initialization required by the tracking methods automatically. It works\nwith crossings and can fill the gaps and occlusions in the segmentation and\noutput the model desirable for physical interaction and simulation. Then we\npresent our work on using the method for tasks such as routing and manipulation\nwith the ground and aerial robots. We discuss our feasibility analysis on\nextending the physical interaction with these objects to aerial manipulation\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keipour_A/0/1/0/all/0/1\">Azarakhsh Keipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousaei_M/0/1/0/all/0/1\">Mohammadreza Mousaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandari_M/0/1/0/all/0/1\">Maryam Bandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaal_S/0/1/0/all/0/1\">Stefan Schaal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Attention Memory Network for Video Object Segmentation. (arXiv:2205.08075v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08075","description":"<p>Semi-supervised video object segmentation is a fundamental yet Challenging\ntask in computer vision. Embedding matching based CFBI series networks have\nachieved promising results by foreground-background integration approach.\nDespite its superior performance, these works exhibit distinct shortcomings,\nespecially the false predictions caused by little appearance instances in first\nframe, even they could easily be recognized by previous frame. Moreover, they\nsuffer from object's occlusion and error drifts. In order to overcome the\nshortcomings , we propose Collaborative Attention Memory Network with an\nenhanced segmentation head. We introduce a object context scheme that\nexplicitly enhances the object information, which aims at only gathering the\npixels that belong to the same category as a given pixel as its context.\nAdditionally, a segmentation head with Feature Pyramid Attention(FPA) module is\nadopted to perform spatial pyramid attention structure on high-level output.\nFurthermore, we propose an ensemble network to combine STM network with all\nthese new refined CFBI network. Finally, we evaluated our approach on the 2021\nYoutube-VOS challenge where we obtain 6th place with an overall score of\n83.5\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhixing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_J/0/1/0/all/0/1\">Junli Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Fei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yuandong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jinpeng Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers. (arXiv:2205.08078v1 [cs.LG])","link":"http://arxiv.org/abs/2205.08078","description":"<p>Vision transformers using self-attention or its proposed alternatives have\ndemonstrated promising results in many image related tasks. However, the\nunderpinning inductive bias of attention is not well understood. To address\nthis issue, this paper analyzes attention through the lens of convex duality.\nFor the non-linear dot-product self-attention, and alternative mechanisms such\nas MLP-mixer and Fourier Neural Operator (FNO), we derive equivalent\nfinite-dimensional convex problems that are interpretable and solvable to\nglobal optimality. The convex programs lead to {\\it block nuclear-norm\nregularization} that promotes low rank in the latent feature and token\ndimensions. In particular, we show how self-attention networks implicitly\nclusters the tokens, based on their latent similarity. We conduct experiments\nfor transferring a pre-trained transformer backbone for CIFAR-100\nclassification by fine-tuning a variety of convex attention heads. The results\nindicate the merits of the bias induced by attention compared with the existing\nMLP or linear heads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahiner_A/0/1/0/all/0/1\">Arda Sahiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ergen_T/0/1/0/all/0/1\">Tolga Ergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozturkler_B/0/1/0/all/0/1\">Batu Ozturkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1\">John Pauly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardani_M/0/1/0/all/0/1\">Morteza Mardani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-Aware Metric Learning for Open World Semantic Segmentation via Meta-Channel Aggregation. (arXiv:2205.08083v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08083","description":"<p>As one of the most challenging and practical segmentation tasks, open-world\nsemantic segmentation requires the model to segment the anomaly regions in the\nimages and incrementally learn to segment out-of-distribution (OOD) objects,\nespecially under a few-shot condition. The current state-of-the-art (SOTA)\nmethod, Deep Metric Learning Network (DMLNet), relies on pixel-level metric\nlearning, with which the identification of similar regions having different\nsemantics is difficult. Therefore, we propose a method called region-aware\nmetric learning (RAML), which first separates the regions of the images and\ngenerates region-aware features for further metric learning. RAML improves the\nintegrity of the segmented anomaly regions. Moreover, we propose a novel\nmeta-channel aggregation (MCA) module to further separate anomaly regions,\nforming high-quality sub-region candidates and thereby improving the model\nperformance for OOD objects. To evaluate the proposed RAML, we have conducted\nextensive experiments and ablation studies on Lost And Found and Road Anomaly\ndatasets for anomaly segmentation and the CityScapes dataset for incremental\nfew-shot learning. The results show that the proposed RAML achieves SOTA\nperformance in both stages of open world segmentation. Our code and appendix\nare available at https://github.com/czifan/RAML.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hexin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingze Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yutong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Stereo Depth Estimation for Pseudo LiDAR: A Self-Supervised Approach Based on Multi-Input ResNet Encoder. (arXiv:2205.08089v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08089","description":"<p>Perception and localization are essential for autonomous delivery vehicles,\nmostly estimated from 3D LiDAR sensors due to their precise distance\nmeasurement capability. This paper presents a strategy to obtain the real-time\npseudo point cloud instead of the laser sensor from the image sensor. We\npropose an approach to use different depth estimators to obtain pseudo point\nclouds like LiDAR to obtain better performance. Moreover, the training and\nvalidating strategy of the depth estimator has adopted stereo imagery data to\nestimate more accurate depth estimation as well as point cloud results. Our\napproach to generating depth maps outperforms on KITTI benchmark while yielding\npoint clouds significantly faster than other approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1\">Sabir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xianke Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Linear Comb Filter for Event Flicker Removal. (arXiv:2205.08090v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08090","description":"<p>Event cameras are bio-inspired sensors that capture per-pixel asynchronous\nintensity change rather than the synchronous absolute intensity frames captured\nby a classical camera sensor. Such cameras are ideal for robotics applications\nsince they have high temporal resolution, high dynamic range and low latency.\nHowever, due to their high temporal resolution, event cameras are particularly\nsensitive to flicker such as from fluorescent or LED lights. During every cycle\nfrom bright to dark, pixels that image a flickering light source generate many\nevents that provide little or no useful information for a robot, swamping the\nuseful data in the scene. In this paper, we propose a novel linear filter to\npreprocess event data to remove unwanted flicker events from an event stream.\nThe proposed algorithm achieves over 4.6 times relative improvement in the\nsignal-to-noise ratio when compared to the raw event stream due to the\neffective removal of flicker from fluorescent lighting. Thus, it is ideally\nsuited to robotics applications that operate in indoor settings or scenes\nilluminated by flickering light sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_D/0/1/0/all/0/1\">Dingran Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1\">Yonhon Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahony_R/0/1/0/all/0/1\">Robert Mahony</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MATrIX -- Modality-Aware Transformer for Information eXtraction. (arXiv:2205.08094v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08094","description":"<p>We present MATrIX - a Modality-Aware Transformer for Information eXtraction\nin the Visual Document Understanding (VDU) domain. VDU covers information\nextraction from visually rich documents such as forms, invoices, receipts,\ntables, graphs, presentations, or advertisements. In these, text semantics and\nvisual information supplement each other to provide a global understanding of\nthe document. MATrIX is pre-trained in an unsupervised way with specifically\ndesigned tasks that require the use of multi-modal information (spatial,\nvisual, or textual). We consider the spatial and text modalities all at once in\na single token set. To make the attention more flexible, we use a learned\nmodality-aware relative bias in the attention mechanism to modulate the\nattention between the tokens of different modalities. We evaluate MATrIX on 3\ndifferent datasets each with strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delteil_T/0/1/0/all/0/1\">Thomas Delteil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belval_E/0/1/0/all/0/1\">Edouard Belval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_L/0/1/0/all/0/1\">Luis Goncalves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_V/0/1/0/all/0/1\">Vijay Mahadevan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey. (arXiv:2205.08099v1 [cs.LG])","link":"http://arxiv.org/abs/2205.08099","description":"<p>State-of-the-art deep learning models have a parameter count that reaches\ninto the billions. Training, storing and transferring such models is energy and\ntime consuming, thus costly. A big part of these costs is caused by training\nthe network. Model compression lowers storage and transfer costs, and can\nfurther make training more efficient by decreasing the number of computations\nin the forward and/or backward pass. Thus, compressing networks also at\ntraining time while maintaining a high performance is an important research\ntopic. This work is a survey on methods which reduce the number of trained\nweights in deep learning models throughout the training. Most of the introduced\nmethods set network parameters to zero which is called pruning. The presented\npruning approaches are categorized into pruning at initialization, lottery\ntickets and dynamic sparse training. Moreover, we discuss methods that freeze\nparts of a network at its random initialization. By freezing weights, the\nnumber of trainable parameters is shrunken which reduces gradient computations\nand the dimensionality of the model's optimization space. In this survey we\nfirst propose dimensionality reduced training as an underlying mathematical\nmodel that covers pruning and freezing during training. Afterwards, we present\nand discuss different dimensionality reduced training methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wimmer_P/0/1/0/all/0/1\">Paul Wimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehnert_J/0/1/0/all/0/1\">Jens Mehnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1\">Alexandru Paul Condurache</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computerized Tomography Pulmonary Angiography Image Simulation using Cycle Generative Adversarial Network from Chest CT imaging in Pulmonary Embolism Patients. (arXiv:2205.08106v1 [eess.IV])","link":"http://arxiv.org/abs/2205.08106","description":"<p>The purpose of this research is to develop a system that generates simulated\ncomputed tomography pulmonary angiography (CTPA) images clinically for\npulmonary embolism diagnoses. Nowadays, CTPA images are the gold standard\ncomputerized detection method to determine and identify the symptoms of\npulmonary embolism (PE), although performing CTPA is harmful for patients and\nalso expensive. Therefore, we aim to detect possible PE patients through CT\nimages. The system will simulate CTPA images with deep learning models for the\nidentification of PE patients' symptoms, providing physicians with another\nreference for determining PE patients. In this study, the simulated CTPA image\ngeneration system uses a generative antagonistic network to enhance the\nfeatures of pulmonary vessels in the CT images to strengthen the reference\nvalue of the images and provide a basis for hospitals to judge PE patients. We\nused the CT images of 22 patients from National Cheng Kung University Hospital\nand the corresponding CTPA images as the training data for the task of\nsimulating CTPA images and generated them using two sets of generative\ncountermeasure networks. This study is expected to propose a new approach to\nthe clinical diagnosis of pulmonary embolism, in which a deep learning network\nis used to assist in the complex screening process and to review the generated\nsimulated CTPA images, allowing physicians to assess whether a patient needs to\nundergo detailed testing for CTPA, improving the speed of detection of\npulmonary embolism and significantly reducing the number of undetected\npatients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Chia-Hung Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun-Chien Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">Chin Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using artificial intelligence to detect chest X-rays with no significant findings in a primary health care setting in Oulu, Finland. (arXiv:2205.08123v1 [eess.IV])","link":"http://arxiv.org/abs/2205.08123","description":"<p>Objectives: To assess the use of artificial intelligence-based software in\nruling out chest X-ray cases, with no significant findings in a primary health\ncare setting.\n</p>\n<p>Methods: In this retrospective study, a commercially available artificial\nintelligence (AI) software was used to analyse 10 000 chest X-rays of Finnish\nprimary health care patients. In studies with a mismatch between an AI normal\nreport and the original radiologist report, a consensus read by two\nboard-certified radiologists was conducted to make the final diagnosis.\n</p>\n<p>Results: After the exclusion of cases not meeting the study criteria, 9579\ncases were analysed by AI. Of these cases, 4451 were considered normal in the\noriginal radiologist report and 4644 after the consensus reading. The number of\ncases correctly found nonsignificant by AI was 1692 (17.7% of all studies and\n36.4% of studies with no significant findings). After the consensus read, there\nwere nine confirmed false-negative studies. These studies included four cases\nof slightly enlarged heart size, four cases of slightly increased pulmonary\nopacification and one case with a small unilateral pleural effusion. This gives\nthe AI a sensitivity of 99.8% (95% CI= 99.65-99.92) and specificity of 36.4 %\n(95% CI= 35.05-37.84) for recognising significant pathology on a chest X-ray.\n</p>\n<p>Conclusions: AI was able to correctly rule out 36.4% of chest X-rays with no\nsignificant findings of primary health care patients, with a minimal number of\nfalse negatives that would lead to effectively no compromise on patient safety.\nNo critical findings were missed by the software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Keski_Filppula_T/0/1/0/all/0/1\">Tommi Keski-Filppula</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nikki_M/0/1/0/all/0/1\">Marko Nikki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haapea_M/0/1/0/all/0/1\">Marianne Haapea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramanauskas_N/0/1/0/all/0/1\">Naglis Ramanauskas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tervonen_O/0/1/0/all/0/1\">Osmo Tervonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space. (arXiv:2205.08129v1 [cs.RO])","link":"http://arxiv.org/abs/2205.08129","description":"<p>General-purpose robots require diverse repertoires of behaviors to complete\nchallenging tasks in real-world unstructured environments. To address this\nissue, goal-conditioned reinforcement learning aims to acquire policies that\ncan reach configurable goals for a wide range of tasks on command. However,\nsuch goal-conditioned policies are notoriously difficult and time-consuming to\ntrain from scratch. In this paper, we propose Planning to Practice (PTP), a\nmethod that makes it practical to train goal-conditioned policies for\nlong-horizon tasks that require multiple distinct types of interactions to\nsolve. Our approach is based on two key ideas. First, we decompose the\ngoal-reaching problem hierarchically, with a high-level planner that sets\nintermediate subgoals using conditional subgoal generators in the latent space\nfor a low-level model-free policy. Second, we propose a hybrid approach which\nfirst pre-trains both the conditional subgoal generator and the policy on\npreviously collected data through offline reinforcement learning, and then\nfine-tunes the policy via online exploration. This fine-tuning process is\nitself facilitated by the planned subgoals, which breaks down the original\ntarget task into short-horizon goal-reaching tasks that are significantly\neasier to learn. We conduct experiments in both the simulation and real world,\nin which the policy is pre-trained on demonstrations of short primitive\nbehaviors and fine-tuned for temporally extended tasks that are unseen in the\noffline data. Our experimental results show that PTP can generate feasible\nsequences of subgoals that enable the policy to efficiently solve the target\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1\">Kuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Patrick Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Ashvin Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brachial Plexus Nerve Trunk Segmentation Using Deep Learning: A Comparative Study with Doctors' Manual Segmentation. (arXiv:2205.08143v1 [eess.IV])","link":"http://arxiv.org/abs/2205.08143","description":"<p>Ultrasound-guided nerve block anesthesia (UGNB) is a high-tech visual nerve\nblock anesthesia method that can observe the target nerve and its surrounding\nstructures, the puncture needle's advancement, and local anesthetics spread in\nreal-time. The key in UGNB is nerve identification. With the help of deep\nlearning methods, the automatic identification or segmentation of nerves can be\nrealized, assisting doctors in completing nerve block anesthesia accurately and\nefficiently. Here, we establish a public dataset containing 320 ultrasound\nimages of brachial plexus (BP). Three experienced doctors jointly produce the\nBP segmentation ground truth and label brachial plexus trunks. We design a\nbrachial plexus segmentation system (BPSegSys) based on deep learning. BPSegSys\nachieves experienced-doctor-level nerve identification performance in various\nexperiments. We evaluate BPSegSys' performance in terms of\nintersection-over-union (IoU), a commonly used performance measure for\nsegmentation experiments. Considering three dataset groups in our established\npublic dataset, the IoU of BPSegSys are 0.5238, 0.4715, and 0.5029,\nrespectively, which exceed the IoU 0.5205, 0.4704, and 0.4979 of experienced\ndoctors. In addition, we show that BPSegSys can help doctors identify brachial\nplexus trunks more accurately, with IoU improvement up to 27%, which has\nsignificant clinical application value.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_B/0/1/0/all/0/1\">Binbin Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_L/0/1/0/all/0/1\">Lingsi Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jianlin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_B/0/1/0/all/0/1\">Bin Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jianhua Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_D/0/1/0/all/0/1\">Dingcheng Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pairwise Comparison Network for Remote Sensing Scene Classification. (arXiv:2205.08147v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08147","description":"<p>Remote sensing scene classification aims to assign a specific semantic label\nto a remote sensing image. Recently, convolutional neural networks have greatly\nimproved the performance of remote sensing scene classification. However, some\nconfused images may be easily recognized as the incorrect category, which\ngenerally degrade the performance. The differences between image pairs can be\nused to distinguish image categories. This paper proposed a pairwise comparison\nnetwork, which contains two main steps: pairwise selection and pairwise\nrepresentation. The proposed network first selects similar image pairs, and\nthen represents the image pairs with pairwise representations. The\nself-representation is introduced to highlight the informative parts of each\nimage itself, while the mutual-representation is proposed to capture the subtle\ndifferences between image pairs. Comprehensive experimental results on two\nchallenging datasets (AID, NWPU-RESISC45) demonstrate the effectiveness of the\nproposed network. The code are provided in\nhttps://github.com/spectralpublic/PCNet.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiangtao_Z/0/1/0/all/0/1\">Zheng Xiangtao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiaoqiang_L/0/1/0/all/0/1\">Lu Xiaoqiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender and Racial Bias in Visual Question Answering Datasets. (arXiv:2205.08148v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08148","description":"<p>Vision-and-language tasks have increasingly drawn more attention as a means\nto evaluate human-like reasoning in machine learning models. A popular task in\nthe field is visual question answering (VQA), which aims to answer questions\nabout images. However, VQA models have been shown to exploit language bias by\nlearning the statistical correlations between questions and answers without\nlooking into the image content: e.g., questions about the color of a banana are\nanswered with yellow, even if the banana in the image is green. If societal\nbias (e.g., sexism, racism, ableism, etc.) is present in the training data,\nthis problem may be causing VQA models to learn harmful stereotypes. For this\nreason, we investigate gender and racial bias in five VQA datasets. In our\nanalysis, we find that the distribution of answers is highly different between\nquestions about women and men, as well as the existence of detrimental\ngender-stereotypical samples. Likewise, we identify that specific race-related\nattributes are underrepresented, whereas potentially discriminatory samples\nappear in the analyzed datasets. Our findings suggest that there are dangers\nassociated to using VQA datasets without considering and dealing with the\npotentially harmful stereotypes. We conclude the paper by proposing solutions\nto alleviate the problem before, during, and after the dataset collection\nprocess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hirota_Y/0/1/0/all/0/1\">Yusuke Hirota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1\">Yuta Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Noa Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnPWC-SVDLO: Multi-SVD on PointPWC for Unsupervised Lidar Odometry. (arXiv:2205.08150v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08150","description":"<p>High-precision lidar odomety is an essential part of autonomous driving. In\nrecent years, deep learning methods have been widely used in lidar odomety\ntasks, but most of the current methods only extract the global features of the\npoint clouds. It is impossible to obtain more detailed point-level features in\nthis way. In addition, only the fully connected layer is used to estimate the\npose. The fully connected layer has achieved obvious results in the\nclassification task, but the changes in pose are a continuous rather than\ndiscrete process, high-precision pose estimation can not be obtained only by\nusing the fully connected layer. Our method avoids problems mentioned above. We\nuse PointPWC as our backbone network. PointPWC is originally used for scene\nflow estimation. The scene flow estimation task has a strong correlation with\nlidar odomety. Traget point clouds can be obtained by adding the scene flow and\nsource point clouds. We can achieve the pose directly through ICP algorithm\nsolved by SVD, and the fully connected layer is no longer used. PointPWC\nextracts point-level features from point clouds with different sampling levels,\nwhich solves the problem of too rough feature extraction. We conduct\nexperiments on KITTI, Ford Campus Vision and Lidar DataSe and Apollo-SouthBay\nDataset. Our result is comparable with the state-of-the-art unsupervised deep\nlearing method SelfVoxeLO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1\">Yiming Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-based Network for Few-shot Image Classification. (arXiv:2205.08157v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08157","description":"<p>The transductive inference is an effective technique in the few-shot learning\ntask, where query sets update prototypes to improve themselves. However, these\nmethods optimize the model by considering only the classification scores of the\nquery instances as confidence while ignoring the uncertainty of these\nclassification scores. In this paper, we propose a novel method called\nUncertainty-Based Network, which models the uncertainty of classification\nresults with the help of mutual information. Specifically, we first data\naugment and classify the query instance and calculate the mutual information of\nthese classification scores. Then, mutual information is used as uncertainty to\nassign weights to classification scores, and the iterative update strategy\nbased on classification scores and uncertainties assigns the optimal weights to\nquery instances in prototype optimization. Extensive results on four benchmarks\nshow that Uncertainty-Based Network achieves comparable performance in\nclassification accuracy compared to state-of-the-art method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Minglei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chunhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yin-Dong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CellTypeGraph: A New Geometric Computer Vision Benchmark. (arXiv:2205.08166v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08166","description":"<p>Classifying all cells in an organ is a relevant and difficult problem from\nplant developmental biology. We here abstract the problem into a new benchmark\nfor node classification in a geo-referenced graph. Solving it requires learning\nthe spatial layout of the organ including symmetries. To allow the convenient\ntesting of new geometrical learning methods, the benchmark of Arabidopsis\nthaliana ovules is made available as a PyTorch data loader, along with a large\nnumber of precomputed features. Finally, we benchmark eight recent graph neural\nnetwork architectures, finding that DeeperGCN currently works best on this\nproblem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cerrone_L/0/1/0/all/0/1\">Lorenzo Cerrone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayan_A/0/1/0/all/0/1\">Athul Vijayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mody_T/0/1/0/all/0/1\">Tejasvinee Mody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneitz_K/0/1/0/all/0/1\">Kay Schneitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamprecht_F/0/1/0/all/0/1\">Fred A. Hamprecht</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DynPL-SVO: A New Method Using Point and Line Features for Stereo Visual Odometry in Dynamic Scenes. (arXiv:2205.08207v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08207","description":"<p>Stereo visual odometry is widely used where a robot tracks its position and\norientation using stereo cameras. Most of the approaches recovered mobile\nrobotics motion based on the matching and tracking of point features along a\nsequence of stereo images. But in low-textured and dynamic scenes, there are no\nsufficient robust static point features for motion estimation, causing lots of\nprevious work to fail to reconstruct the robotic motion. However, line features\ncan be detected in such low-textured and dynamic scenes. In this paper, we\nproposed DynPL-SVO, a stereo visual odometry with the $dynamic$ $grid$\nalgorithm and the cost function containing both vertical and horizontal\ninformation of the line features. Stereo camera motion was obtained through\nLevenberg-Marquard minimization of re-projection error of point and line\nfeatures. The experimental results on the KITTI and EuRoC MAV datasets showed\nthat the DynPL-SVO had a competitive performance when compared to other\nstate-of-the-art systems by producing more robust and accurate motion\nestimation, especially in low-textured and dynamic scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hong-Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chunbo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"blob loss: instance imbalance aware loss functions for semantic segmentation. (arXiv:2205.08209v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08209","description":"<p>Deep convolutional neural networks have proven to be remarkably effective in\nsemantic segmentation tasks. Most popular loss functions were introduced\ntargeting improved volumetric scores, such as the Sorensen Dice coefficient. By\ndesign, DSC can tackle class imbalance; however, it does not recognize instance\nimbalance within a class. As a result, a large foreground instance can dominate\nminor instances and still produce a satisfactory Sorensen Dice coefficient.\nNevertheless, missing out on instances will lead to poor detection performance.\nThis represents a critical issue in applications such as disease progression\nmonitoring. For example, it is imperative to locate and surveil small-scale\nlesions in the follow-up of multiple sclerosis patients. We propose a novel\nfamily of loss functions, nicknamed blob loss, primarily aimed at maximizing\ninstance-level detection metrics, such as F1 score and sensitivity. Blob loss\nis designed for semantic segmentation problems in which the instances are the\nconnected components within a class. We extensively evaluate a DSC-based blob\nloss in five complex 3D semantic segmentation tasks featuring pronounced\ninstance heterogeneity in terms of texture and morphology. Compared to soft\nDice loss, we achieve 5 percent improvement for MS lesions, 3 percent\nimprovement for liver tumor, and an average 2 percent improvement for\nMicroscopy segmentation tasks considering F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kofler_F/0/1/0/all/0/1\">Florian Kofler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shit_S/0/1/0/all/0/1\">Suprosanna Shit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1\">Ivan Ezhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvath_I/0/1/0/all/0/1\">Izabela Horvath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Maskari_R/0/1/0/all/0/1\">Rami Al-Maskari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1\">Harsharan Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loehr_T/0/1/0/all/0/1\">Timo Loehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piraud_M/0/1/0/all/0/1\">Marie Piraud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erturk_A/0/1/0/all/0/1\">Ali Erturk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1\">Jan Kirschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peeken_J/0/1/0/all/0/1\">Jan Peeken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_C/0/1/0/all/0/1\">Claus Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiestler_B/0/1/0/all/0/1\">Benedikt Wiestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAS-Net: Conditional Atlas Generation and Brain Segmentation for Fetal MRI. (arXiv:2205.08239v1 [eess.IV])","link":"http://arxiv.org/abs/2205.08239","description":"<p>Fetal Magnetic Resonance Imaging (MRI) is used in prenatal diagnosis and to\nassess early brain development. Accurate segmentation of the different brain\ntissues is a vital step in several brain analysis tasks, such as cortical\nsurface reconstruction and tissue thickness measurements. Fetal MRI scans,\nhowever, are prone to motion artifacts that can affect the correctness of both\nmanual and automatic segmentation techniques. In this paper, we propose a novel\nnetwork structure that can simultaneously generate conditional atlases and\npredict brain tissue segmentation, called CAS-Net. The conditional atlases\nprovide anatomical priors that can constrain the segmentation connectivity,\ndespite the heterogeneity of intensity values caused by motion or partial\nvolume effects. The proposed method is trained and evaluated on 253 subjects\nfrom the developing Human Connectome Project (dHCP). The results demonstrate\nthat the proposed method can generate conditional age-specific atlas with sharp\nboundary and shape variance. It also segment multi-category brain tissues for\nfetal MRI with a high overall Dice similarity coefficient (DSC) of $85.2\\%$ for\nthe selected 9 tissue labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Liu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_Q/0/1/0/all/0/1\">Qiang Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinclair_M/0/1/0/all/0/1\">Matthew Sinclair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makropoulos_A/0/1/0/all/0/1\">Antonios Makropoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hajnal_J/0/1/0/all/0/1\">Joseph Hajnal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Edwards_A/0/1/0/all/0/1\">A. David Edwards</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alansary_A/0/1/0/all/0/1\">Amir Alansary</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Optimal Sequential Grouping for Video Scene Detection. (arXiv:2205.08249v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08249","description":"<p>Video scene detection is the task of dividing videos into temporal semantic\nchapters. This is an important preliminary step before attempting to analyze\nheterogeneous video content. Recently, Optimal Sequential Grouping (OSG) was\nproposed as a powerful unsupervised solution to solve a formulation of the\nvideo scene detection problem. In this work, we extend the capabilities of OSG\nto the learning regime. By giving the capability to both learn from examples\nand leverage a robust optimization formulation, we can boost performance and\nenhance the versatility of the technology. We present a comprehensive analysis\nof incorporating OSG into deep learning neural networks under various\nconfigurations. These configurations include learning an embedding in a\nstraight-forward manner, a tailored loss designed to guide the solution of OSG,\nand an integrated model where the learning is performed through the OSG\npipeline. With thorough evaluation and analysis, we assess the benefits and\nbehavior of the various configurations, and show that our learnable OSG\napproach exhibits desirable behavior and enhanced performance compared to the\nstate of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rotman_D/0/1/0/all/0/1\">Daniel Rotman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaroker_Y/0/1/0/all/0/1\">Yevgeny Yaroker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amrani_E/0/1/0/all/0/1\">Elad Amrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzelay_U/0/1/0/all/0/1\">Udi Barzelay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1\">Rami Ben-Ari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection Masking for Improved OCR on Noisy Documents. (arXiv:2205.08257v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08257","description":"<p>Optical Character Recognition (OCR), the task of extracting textual\ninformation from scanned documents is a vital and broadly used technology for\ndigitizing and indexing physical documents. Existing technologies perform well\nfor clean documents, but when the document is visually degraded, or when there\nare non-textual elements, OCR quality can be greatly impacted, specifically due\nto erroneous detections. In this paper we present an improved detection network\nwith a masking system to improve the quality of OCR performed on documents. By\nfiltering non-textual elements from the image we can utilize document-level OCR\nto incorporate contextual information to improve OCR results. We perform a\nunified evaluation on a publicly available dataset demonstrating the usefulness\nand broad applicability of our method. Additionally, we present and make\npublicly available our synthetic dataset with a unique hard-negative component\nspecifically tuned to improve detection results, and evaluate the benefits that\ncan be gained from its usage\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rotman_D/0/1/0/all/0/1\">Daniel Rotman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azulai_O/0/1/0/all/0/1\">Ophir Azulai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapira_I/0/1/0/all/0/1\">Inbar Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burshtein_Y/0/1/0/all/0/1\">Yevgeny Burshtein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzelay_U/0/1/0/all/0/1\">Udi Barzelay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MulT: An End-to-End Multitask Learning Transformer. (arXiv:2205.08303v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08303","description":"<p>We propose an end-to-end Multitask Learning Transformer framework, named\nMulT, to simultaneously learn multiple high-level vision tasks, including depth\nestimation, semantic segmentation, reshading, surface normal estimation, 2D\nkeypoint detection, and edge detection. Based on the Swin transformer model,\nour framework encodes the input image into a shared representation and makes\npredictions for each vision task using task-specific transformer-based decoder\nheads. At the heart of our approach is a shared attention mechanism modeling\nthe dependencies across the tasks. We evaluate our model on several multitask\nbenchmarks, showing that our MulT framework outperforms both the state-of-the\nart multitask convolutional neural network models and all the respective single\ntask transformer models. Our experiments further highlight the benefits of\nsharing attention across all the tasks, and demonstrate that our MulT model is\nrobust and generalizes well to new domains. Our project website is at\nhttps://ivrl.github.io/MulT/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_D/0/1/0/all/0/1\">Deblina Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1\">Sabine S&#xfc;sstrunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning of Multi-Object Keypoints for Robotic Manipulation. (arXiv:2205.08316v1 [cs.RO])","link":"http://arxiv.org/abs/2205.08316","description":"<p>In recent years, policy learning methods using either reinforcement or\nimitation have made significant progress. However, both techniques still suffer\nfrom being computationally expensive and requiring large amounts of training\ndata. This problem is especially prevalent in real-world robotic manipulation\ntasks, where access to ground truth scene features is not available and\npolicies are instead learned from raw camera observations. In this paper, we\ndemonstrate the efficacy of learning image keypoints via the Dense\nCorrespondence pretext task for downstream policy learning. Extending prior\nwork to challenging multi-object scenes, we show that our model can be trained\nto deal with important problems in representation learning, primarily\nscale-invariance and occlusion. We evaluate our approach on diverse robot\nmanipulation tasks, compare it to other visual representation learning\napproaches, and demonstrate its flexibility and effectiveness for\nsample-efficient policy learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartz_J/0/1/0/all/0/1\">Jan Ole von Hartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chisari_E/0/1/0/all/0/1\">Eugenio Chisari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welschehold_T/0/1/0/all/0/1\">Tim Welschehold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Interactive Image Matting. (arXiv:2205.08324v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08324","description":"<p>Recent image matting studies are developing towards proposing trimap-free or\ninteractive methods for complete complex image matting tasks. Although avoiding\nthe extensive labors of trimap annotation, existing methods still suffer from\ntwo limitations: (1) For the single image with multiple objects, it is\nessential to provide extra interaction information to help determining the\nmatting target; (2) For transparent objects, the accurate regression of alpha\nmatte from RGB image is much more difficult compared with the opaque ones. In\nthis work, we propose a Unified Interactive image Matting method, named UIM,\nwhich solves the limitations and achieves satisfying matting results for any\nscenario. Specifically, UIM leverages multiple types of user interaction to\navoid the ambiguity of multiple matting targets, and we compare the pros and\ncons of different annotation types in detail. To unify the matting performance\nfor transparent and opaque objects, we decouple image matting into two stages,\ni.e., foreground segmentation and transparency prediction. Moreover, we design\na multi-scale attentive fusion module to alleviate the vagueness in the\nboundary region. Experimental results demonstrate that UIM achieves\nstate-of-the-art performance on the Composition-1K test set and a synthetic\nunified dataset. Our code and models will be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Stephen.D.H Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">YiQi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Conghui He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphMapper: Efficient Visual Navigation by Scene Graph Generation. (arXiv:2205.08325v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08325","description":"<p>Understanding the geometric relationships between objects in a scene is a\ncore capability in enabling both humans and autonomous agents to navigate in\nnew environments. A sparse, unified representation of the scene topology will\nallow agents to act efficiently to move through their environment, communicate\nthe environment state with others, and utilize the representation for diverse\ndownstream tasks. To this end, we propose a method to train an autonomous agent\nto learn to accumulate a 3D scene graph representation of its environment by\nsimultaneously learning to navigate through said environment. We demonstrate\nthat our approach, GraphMapper, enables the learning of effective navigation\npolicies through fewer interactions with the environment than vision-based\nsystems alone. Further, we show that GraphMapper can act as a modular scene\nencoder to operate alongside existing Learning-based solutions to not only\nincrease navigational efficiency but also generate intermediate scene\nrepresentations that are useful for other future tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seymour_Z/0/1/0/all/0/1\">Zachary Seymour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1\">Niluthpol Chowdhury Mithun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Han-Pang Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samarasekera_S/0/1/0/all/0/1\">Supun Samarasekera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rakesh Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Supervised Information Bottleneck Hashing for Cross-modal Retrieval based Computer-aided Diagnosis. (arXiv:2205.08365v1 [cs.LG])","link":"http://arxiv.org/abs/2205.08365","description":"<p>Mapping X-ray images, radiology reports, and other medical data as binary\ncodes in the common space, which can assist clinicians to retrieve\npathology-related data from heterogeneous modalities (i.e., hashing-based\ncross-modal medical data retrieval), provides a new view to promot\ncomputeraided diagnosis. Nevertheless, there remains a barrier to boost medical\nretrieval accuracy: how to reveal the ambiguous semantics of medical data\nwithout the distraction of superfluous information. To circumvent this\ndrawback, we propose Deep Supervised Information Bottleneck Hashing (DSIBH),\nwhich effectively strengthens the discriminability of hash codes. Specifically,\nthe Deep Deterministic Information Bottleneck (Yu, Yu, and Principe 2021) for\nsingle modality is extended to the cross-modal scenario. Benefiting from this,\nthe superfluous information is reduced, which facilitates the discriminability\nof hash codes. Experimental results demonstrate the superior accuracy of the\nproposed DSIBH compared with state-of-the-arts in cross-modal medical data\nretrieval tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yufeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuhuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1\">Weihua Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Velocity Picking Using Unsupervised Ensemble Learning. (arXiv:2205.08372v1 [cs.LG])","link":"http://arxiv.org/abs/2205.08372","description":"<p>In seismic data processing, accurate and efficient automatic velocity picking\nalgorithms can significantly accelerate the processing, and the main branch is\nto use velocity spectra for velocity pickup. Recently, machine learning\nalgorithms have been widely used in automatic spectrum picking. Even though\ndeep learning methods can address the problem well in supervised cases, they\nare often accompanied by expensive computational costs and low\ninterpretability. On the contrast, unsupervised learning methods based on the\nphysical knowledge have great potential to efficiently resolve the task. In\nthis paper, we propose an unsupervised ensemble learning (UEL) method to pick\nthe root mean square (RMS) velocities on the spectrum. In particular, UEL\nutilizes the information of nearby velocity spectra and the nearest seed\nvelocity curve to assist the selection of effective and reasonable velocity\npoints. To increase the coherence of energy peaks, an information gain method\nis developed by local normalization. In addition, we designed the attention\nscale-space filter (ASSF) clustering method to incorporate the coherence\ninformation into the picking process. Experiments on three datasets demonstrate\nthat compared to traditional clustering methods, UEL can recognize energy\nclusters better, especially with smaller blobs. Moreover, the injection of\nnearby spectra and interval velocity constraint in UEL significantly improves\nthe robustness and accuracy of picking results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">H.T. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">J.S. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">C.X. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Z.X. Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">C.L. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Z.Y. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_W/0/1/0/all/0/1\">W.F. Geng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias and Fairness on Multimodal Emotion Detection Algorithms. (arXiv:2205.08383v1 [cs.LG])","link":"http://arxiv.org/abs/2205.08383","description":"<p>Numerous studies have shown that machine learning algorithms can latch onto\nprotected attributes such as race and gender and generate predictions that\nsystematically discriminate against one or more groups. To date the majority of\nbias and fairness research has been on unimodal models. In this work, we\nexplore the biases that exist in emotion recognition systems in relationship to\nthe modalities utilized, and study how multimodal approaches affect system bias\nand fairness. We consider audio, text, and video modalities, as well as all\npossible multimodal combinations of those, and find that text alone has the\nleast bias, and accounts for the majority of the models' performances, raising\ndoubts about the worthiness of multimodal emotion recognition systems when bias\nand fairness are desired alongside model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmitz_M/0/1/0/all/0/1\">Matheus Schmitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_R/0/1/0/all/0/1\">Rehan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jimi Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images. (arXiv:2205.08390v1 [eess.IV])","link":"http://arxiv.org/abs/2205.08390","description":"<p>Ultrasonography is an important routine examination for breast cancer\ndiagnosis, due to its non-invasive, radiation-free and low-cost properties.\nHowever, it is still not the first-line screening test for breast cancer due to\nits inherent limitations. It would be a tremendous success if we can precisely\ndiagnose breast cancer by breast ultrasound images (BUS). Many learning-based\ncomputer-aided diagnostic methods have been proposed to achieve breast cancer\ndiagnosis/lesion classification. However, most of them require a pre-define ROI\nand then classify the lesion inside the ROI. Conventional classification\nbackbones, such as VGG16 and ResNet50, can achieve promising classification\nresults with no ROI requirement. But these models lack interpretability, thus\nrestricting their use in clinical practice. In this study, we propose a novel\nROI-free model for breast cancer diagnosis in ultrasound images with\ninterpretable feature representations. We leverage the anatomical prior\nknowledge that malignant and benign tumors have different spatial relationships\nbetween different tissue layers, and propose a HoVer-Transformer to formulate\nthis prior knowledge. The proposed HoVer-Trans block extracts the inter- and\nintra-layer spatial information horizontally and vertically. We conduct and\nrelease an open dataset GDPH&amp;GYFYY for breast cancer diagnosis in BUS. The\nproposed model is evaluated in three datasets by comparing with four CNN-based\nmodels and two vision transformer models via a five-fold cross validation. It\nachieves state-of-the-art classification performance with the best model\ninterpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mo_Y/0/1/0/all/0/1\">Yuhao Mo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1\">Chu Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1\">Min Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jiatai Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchao Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chunwang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_B/0/1/0/all/0/1\">Bingjiang Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_Y/0/1/0/all/0/1\">Yanfen Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_L/0/1/0/all/0/1\">Lei Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xipeng Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zeyan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xiaomei Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zaiyi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_C/0/1/0/all/0/1\">Changhong Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Building Footprint Generation with Feature and Output Consistency Training. (arXiv:2205.08416v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08416","description":"<p>Accurate and reliable building footprint maps are vital to urban planning and\nmonitoring, and most existing approaches fall back on convolutional neural\nnetworks (CNNs) for building footprint generation. However, one limitation of\nthese methods is that they require strong supervisory information from massive\nannotated samples for network learning. State-of-the-art semi-supervised\nsemantic segmentation networks with consistency training can help to deal with\nthis issue by leveraging a large amount of unlabeled data, which encourages the\nconsistency of model output on data perturbation. Considering that rich\ninformation is also encoded in feature maps, we propose to integrate the\nconsistency of both features and outputs in the end-to-end network training of\nunlabeled samples, enabling to impose additional constraints. Prior\nsemi-supervised semantic segmentation networks have established the cluster\nassumption, in which the decision boundary should lie in the vicinity of low\nsample density. In this work, we observe that for building footprint\ngeneration, the low-density regions are more apparent at the intermediate\nfeature representations within the encoder than the encoder's input or output.\nTherefore, we propose an instruction to assign the perturbation to the\nintermediate feature representations within the encoder, which considers the\nspatial resolution of input remote sensing imagery and the mean size of\nindividual buildings in the study area. The proposed method is evaluated on\nthree datasets with different resolutions: Planet dataset (3 m/pixel),\nMassachusetts dataset (1 m/pixel), and Inria dataset (0.3 m/pixel).\nExperimental results show that the proposed approach can well extract more\ncomplete building structures and alleviate omission errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yilei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Visual Servoing for Multi-Step Tasks. (arXiv:2205.08441v1 [cs.RO])","link":"http://arxiv.org/abs/2205.08441","description":"<p>Visual Servoing has been effectively used to move a robot into specific\ntarget locations or to track a recorded demonstration. It does not require\nmanual programming, but it is typically limited to settings where one\ndemonstration maps to one environment state. We propose a modular approach to\nextend visual servoing to scenarios with multiple demonstration sequences. We\ncall this conditional servoing, as we choose the next demonstration conditioned\non the observation of the robot. This method presents an appealing strategy to\ntackle multi-step problems, as individual demonstrations can be combined\nflexibly into a control policy. We propose different selection functions and\ncompare them on a shape-sorting task in simulation. With the reprojection error\nyielding the best overall results, we implement this selection function on a\nreal robot and show the efficacy of the proposed conditional servoing. For\nvideos of our experiments, please check out our project page:\nhttps://lmb.informatik.uni-freiburg.de/projects/conditional_servoing/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izquierdo_S/0/1/0/all/0/1\">Sergio Izquierdo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argus_M/0/1/0/all/0/1\">Max Argus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of Graph Based Features in Computer Aided Diagnosis for Histopathological Image Classification of Gastric Cancer. (arXiv:2205.08467v1 [eess.IV])","link":"http://arxiv.org/abs/2205.08467","description":"<p>The gold standard for gastric cancer detection is gastric histopathological\nimage analysis, but there are certain drawbacks in the existing\nhistopathological detection and diagnosis. In this paper, based on the study of\ncomputer aided diagnosis system, graph based features are applied to gastric\ncancer histopathology microscopic image analysis, and a classifier is used to\nclassify gastric cancer cells from benign cells. Firstly, image segmentation is\nperformed, and after finding the region, cell nuclei are extracted using the\nk-means method, the minimum spanning tree (MST) is drawn, and graph based\nfeatures of the MST are extracted. The graph based features are then put into\nthe classifier for classification. In this study, different segmentation\nmethods are compared in the tissue segmentation stage, among which are\nLevel-Set, Otsu thresholding, watershed, SegNet, U-Net and Trans-U-Net\nsegmentation; Graph based features, Red, Green, Blue features, Grey-Level\nCo-occurrence Matrix features, Histograms of Oriented Gradient features and\nLocal Binary Patterns features are compared in the feature extraction stage;\nRadial Basis Function (RBF) Support Vector Machine (SVM), Linear SVM,\nArtificial Neural Network, Random Forests, k-NearestNeighbor, VGG16, and\nInception-V3 are compared in the classifier stage. It is found that using U-Net\nto segment tissue areas, then extracting graph based features, and finally\nusing RBF SVM classifier gives the optimal results with 94.29%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Haiqing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ai_S/0/1/0/all/0/1\">Shiliang Ai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuchao Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yixin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xinyu Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ColonFormer: An Efficient Transformer based Method for Colon Polyp Segmentation. (arXiv:2205.08473v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08473","description":"<p>Identifying polyps is a challenging problem for automatic analysis of\nendoscopic images in computer-aided clinical support systems. Models based on\nconvolutional networks (CNN), transformers, and combinations of them have been\nproposed to segment polyps with promising results. However, those approaches\nhave limitations either in modeling the local appearance of the polyps only or\nlack of multi-level features for spatial dependency in the decoding process.\nThis paper proposes a novel network, namely ColonFormer, to address these\nlimitations. ColonFormer is an encoder-decoder architecture with the capability\nof modeling long-range semantic information at both encoder and decoder\nbranches. The encoder is a lightweight architecture based on transformers for\nmodeling global semantic relations at multi scales. The decoder is a\nhierarchical network structure designed for learning multi-level features to\nenrich feature representation. Besides, a refinement module is added with a new\nskip connection technique to refine the boundary of polyp objects in the global\nmap for accurate segmentation. Extensive experiments have been conducted on\nfive popular benchmark datasets for polyp segmentation, including Kvasir,\nCVC-Clinic DB, CVCColonDB, EndoScene, and ETIS. Experimental results show that\nour ColonFormer achieve state-of-the-art performance on all benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duc_N/0/1/0/all/0/1\">Nguyen Thanh Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oanh_N/0/1/0/all/0/1\">Nguyen Thi Oanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thuy_N/0/1/0/all/0/1\">Nguyen Thi Thuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triet_T/0/1/0/all/0/1\">Tran Minh Triet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_D/0/1/0/all/0/1\">Dinh Viet Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A CLIP-Hitchhiker's Guide to Long Video Retrieval. (arXiv:2205.08508v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08508","description":"<p>Our goal in this paper is the adaptation of image-text models for long video\nretrieval. Recent works have demonstrated state-of-the-art performance in video\nretrieval by adopting CLIP, effectively hitchhiking on the image-text\nrepresentation for video tasks. However, there has been limited success in\nlearning temporal aggregation that outperform mean-pooling the image-level\nrepresentations extracted per frame by CLIP. We find that the simple yet\neffective baseline of weighted-mean of frame embeddings via query-scoring is a\nsignificant improvement above all prior temporal modelling attempts and\nmean-pooling. In doing so, we provide an improved baseline for others to\ncompare to and demonstrate state-of-the-art performance of this simple baseline\non a suite of long video retrieval benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bain_M/0/1/0/all/0/1\">Max Bain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Segmentation in Real-World Images via Spelke Object Inference. (arXiv:2205.08515v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08515","description":"<p>Self-supervised category-agnostic segmentation of real-world images into\nobjects is a challenging open problem in computer vision. Here, we show how to\nlearn static grouping priors from motion self-supervision, building on the\ncognitive science notion of Spelke Objects: groupings of stuff that move\ntogether. We introduce Excitatory-Inhibitory Segment Extraction Network\n(EISEN), which learns from optical flow estimates to extract pairwise affinity\ngraphs for static scenes. EISEN then produces segments from affinities using a\nnovel graph propagation and competition mechanism. Correlations between\nindependent sources of motion (e.g. robot arms) and objects they move are\nresolved into separate segments through a bootstrapping training process. We\nshow that EISEN achieves a substantial improvement in the state of the art for\nself-supervised segmentation on challenging synthetic and real-world robotic\nimage datasets. We also present an ablation analysis illustrating the\nimportance of each element of the EISEN architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Honglin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_R/0/1/0/all/0/1\">Rahul Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_Y/0/1/0/all/0/1\">Yoni Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1\">Daniel L. K. Yamins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bear_D/0/1/0/all/0/1\">Daniel M. Bear</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Neural Networks Compress Manifolds Optimally?. (arXiv:2205.08518v1 [cs.IT])","link":"http://arxiv.org/abs/2205.08518","description":"<p>Artificial Neural-Network-based (ANN-based) lossy compressors have recently\nobtained striking results on several sources. Their success may be ascribed to\nan ability to identify the structure of low-dimensional manifolds in\nhigh-dimensional ambient spaces. Indeed, prior work has shown that ANN-based\ncompressors can achieve the optimal entropy-distortion curve for some such\nsources. In contrast, we determine the optimal entropy-distortion tradeoffs for\ntwo low-dimensional manifolds with circular structure and show that\nstate-of-the-art ANN-based compressors fail to optimally compress the sources,\nespecially at high rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhadane_S/0/1/0/all/0/1\">Sourbh Bhadane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1\">Aaron B. Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balle_J/0/1/0/all/0/1\">Johannes Ball&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Neural Articulated Shape and Appearance Models. (arXiv:2205.08525v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08525","description":"<p>Learning geometry, motion, and appearance priors of object classes is\nimportant for the solution of a large variety of computer vision problems.\nWhile the majority of approaches has focused on static objects, dynamic\nobjects, especially with controllable articulation, are less explored. We\npropose a novel approach for learning a representation of the geometry,\nappearance, and motion of a class of articulated objects given only a set of\ncolor images as input. In a self-supervised manner, our novel representation\nlearns shape, appearance, and articulation codes that enable independent\ncontrol of these semantic dimensions. Our model is trained end-to-end without\nrequiring any articulation annotations. Experiments show that our approach\nperforms well for different joint types, such as revolute and prismatic joints,\nas well as different combinations of these joints. Compared to state of the art\nthat uses direct 3D supervision and does not output appearance, we recover more\nfaithful geometry and appearance from 2D observations only. In addition, our\nrepresentation enables a large variety of applications, such as few-shot\nreconstruction, the generation of novel articulations, and novel\nview-synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chabra_R/0/1/0/all/0/1\">Rohan Chabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lingni Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassner_C/0/1/0/all/0/1\">Christoph Lassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1\">Michael Zollh&#xf6;fer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusinkiewicz_S/0/1/0/all/0/1\">Szymon Rusinkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sweeney_C/0/1/0/all/0/1\">Chris Sweeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newcombe_R/0/1/0/all/0/1\">Richard Newcombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slavcheva_M/0/1/0/all/0/1\">Mira Slavcheva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer Adapter for Dense Predictions. (arXiv:2205.08534v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08534","description":"<p>This work investigates a simple yet powerful adapter for Vision Transformer\n(ViT). Unlike recent visual transformers that introduce vision-specific\ninductive biases into their architectures, ViT achieves inferior performance on\ndense prediction tasks due to lacking prior information of images. To solve\nthis issue, we propose a Vision Transformer Adapter (ViT-Adapter), which can\nremedy the defects of ViT and achieve comparable performance to vision-specific\nmodels by introducing inductive biases via an additional architecture.\nSpecifically, the backbone in our framework is a vanilla transformer that can\nbe pre-trained with multi-modal data. When fine-tuning on downstream tasks, a\nmodality-specific adapter is used to introduce the data and tasks' prior\ninformation into the model, making it suitable for these tasks. We verify the\neffectiveness of our ViT-Adapter on multiple downstream tasks, including object\ndetection, instance segmentation, and semantic segmentation. Notably, when\nusing HTC++, our ViT-Adapter-L yields 60.1 box AP and 52.1 mask AP on COCO\ntest-dev, surpassing Swin-L by 1.4 box AP and 1.0 mask AP. For semantic\nsegmentation, our ViT-Adapter-L establishes a new state-of-the-art of 60.5 mIoU\non ADE20K val, 0.6 points higher than SwinV2-G. We hope that the proposed\nViT-Adapter could serve as an alternative for vision-specific transformers and\nfacilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yuchen Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars. (arXiv:2205.08535v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08535","description":"<p>3D avatar creation plays a crucial role in the digital age. However, the\nwhole production process is prohibitively time-consuming and labor-intensive.\nTo democratize this technology to a larger audience, we propose AvatarCLIP, a\nzero-shot text-driven framework for 3D avatar generation and animation. Unlike\nprofessional software that requires expert knowledge, AvatarCLIP empowers\nlayman users to customize a 3D avatar with the desired shape and texture, and\ndrive the avatar with the described motions using solely natural languages. Our\nkey insight is to take advantage of the powerful vision-language model CLIP for\nsupervising neural human generation, in terms of 3D geometry, texture and\nanimation. Specifically, driven by natural language descriptions, we initialize\n3D human geometry generation with a shape VAE network. Based on the generated\n3D human shapes, a volume rendering model is utilized to further facilitate\ngeometry sculpting and texture generation. Moreover, by leveraging the priors\nlearned in the motion VAE, a CLIP-guided reference-based motion synthesis\nmethod is proposed for the animation of the generated 3D avatar. Extensive\nqualitative and quantitative experiments validate the effectiveness and\ngeneralizability of AvatarCLIP on a wide range of avatars. Remarkably,\nAvatarCLIP can generate unseen 3D avatars with novel animations, achieving\nsuperior zero-shot capability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1\">Fangzhou Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Visual Embeddings for Attributes and Objects. (arXiv:2205.08536v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08536","description":"<p>We study the problem of compositional zero-shot learning for object-attribute\nrecognition. Prior works use visual features extracted with a backbone network,\npre-trained for object classification and thus do not capture the subtly\ndistinct features associated with attributes. To overcome this challenge, these\nstudies employ supervision from the linguistic space, and use pre-trained word\nembeddings to better separate and compose attribute-object pairs for\nrecognition. Analogous to linguistic embedding space, which already has unique\nand agnostic embeddings for object and attribute, we shift the focus back to\nthe visual space and propose a novel architecture that can disentangle\nattribute and object features in the visual space. We use visual decomposed\nfeatures to hallucinate embeddings that are representative for the seen and\nnovel compositions to better regularize the learning of our model. Extensive\nexperiments show that our method outperforms existing work with significant\nmargin on three datasets: MIT-States, UT-Zappos, and a new benchmark created\nbased on VAW. The code, models, and dataset splits are publicly available at\nhttps://github.com/nirat1606/OADis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saini_N/0/1/0/all/0/1\">Nirat Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_K/0/1/0/all/0/1\">Khoi Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signal2Image Modules in Deep Neural Networks for EEG Classification. (arXiv:1904.13216v8 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/1904.13216","description":"<p>Deep learning has revolutionized computer vision utilizing the increased\navailability of big data and the power of parallel computational units such as\ngraphical processing units. The vast majority of deep learning research is\nconducted using images as training data, however the biomedical domain is rich\nin physiological signals that are used for diagnosis and prediction problems.\nIt is still an open research question how to best utilize signals to train deep\nneural networks.\n</p>\n<p>In this paper we define the term Signal2Image (S2Is) as trainable or\nnon-trainable prefix modules that convert signals, such as\nElectroencephalography (EEG), to image-like representations making them\nsuitable for training image-based deep neural networks defined as `base\nmodels'. We compare the accuracy and time performance of four S2Is (`signal as\nimage', spectrogram, one and two layer Convolutional Neural Networks (CNNs))\ncombined with a set of `base models' (LeNet, AlexNet, VGGnet, ResNet, DenseNet)\nalong with the depth-wise and 1D variations of the latter. We also provide\nempirical evidence that the one layer CNN S2I performs better in eleven out of\nfifteen tested models than non-trainable S2Is for classifying EEG signals and\nwe present visual comparisons of the outputs of the S2Is.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lambrou_G/0/1/0/all/0/1\">George I Lambrou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koutsouris_D/0/1/0/all/0/1\">Dimitrios Koutsouris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsely Activated Networks. (arXiv:1907.06592v9 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1907.06592","description":"<p>Previous literature on unsupervised learning focused on designing structural\npriors with the aim of learning meaningful features. However, this was done\nwithout considering the description length of the learned representations which\nis a direct and unbiased measure of the model complexity. In this paper, first\nwe introduce the $\\varphi$ metric that evaluates unsupervised models based on\ntheir reconstruction accuracy and the degree of compression of their internal\nrepresentations. We then present and define two activation functions (Identity,\nReLU) as base of reference and three sparse activation functions (top-k\nabsolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize\nthe previously defined $\\varphi$. We lastly present Sparsely Activated Networks\n(SANs) that consist of kernels with shared weights that, during encoding, are\nconvolved with the input and then passed through a sparse activation function.\nDuring decoding, the same weights are convolved with the sparse activation map\nand subsequently the partial reconstructions from each weight are summed to\nreconstruct the input. We compare SANs using the five previously defined\nactivation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,\nFMNIST) and show that models that are selected using $\\varphi$ have small\ndescription representation length and consist of interpretable kernels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1\">Dimitrios Koutsouris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Occluded Video Instance Segmentation: A Benchmark. (arXiv:2102.01558v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.01558","description":"<p>Can our video understanding systems perceive objects when a heavy occlusion\nexists in a scene?\n</p>\n<p>To answer this question, we collect a large-scale dataset called OVIS for\noccluded video instance segmentation, that is, to simultaneously detect,\nsegment, and track instances in occluded scenes. OVIS consists of 296k\nhigh-quality instance masks from 25 semantic categories, where object\nocclusions usually occur. While our human vision systems can understand those\noccluded instances by contextual reasoning and association, our experiments\nsuggest that current video understanding systems cannot. On the OVIS dataset,\nthe highest AP achieved by state-of-the-art algorithms is only 16.3, which\nreveals that we are still at a nascent stage for understanding objects,\ninstances, and videos in a real-world scenario. We also present a simple\nplug-and-play module that performs temporal feature calibration to complement\nmissing object cues caused by occlusion. Built upon MaskTrack R-CNN and\nSipMask, we obtain a remarkable AP improvement on the OVIS dataset. The OVIS\ndataset and project code are available at <a href=\"http://songbai.site/ovis\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiyang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Projection: A Mechanism for Human-like Reasoning in Artificial Intelligence. (arXiv:2103.13512v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2103.13512","description":"<p>Artificial Intelligence systems cannot yet match human abilities to apply\nknowledge to situations that vary from what they have been programmed for, or\ntrained for. In visual object recognition methods of inference exploiting\ntop-down information (from a model) have been shown to be effective for\nrecognising entities in difficult conditions. Here this type of inference,\ncalled `projection', is shown to be a key mechanism to solve the problem of\napplying knowledge to varied or challenging situations, across a range of AI\ndomains, such as vision, robotics, or language. Finally the relevance of\nprojection to tackling the commonsense knowledge problem is discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised 3D Human Pose Estimation with Cross-view U-shaped Graph Convolutional Network. (arXiv:2105.10882v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10882","description":"<p>Although monocular 3D human pose estimation methods have made significant\nprogress, it is far from being solved due to the inherent depth ambiguity.\nInstead, exploiting multi-view information is a practical way to achieve\nabsolute 3D human pose estimation. In this paper, we propose a simple yet\neffective pipeline for weakly-supervised cross-view 3D human pose estimation.\nBy only using two camera views, our method can achieve state-of-the-art\nperformance in a weakly-supervised manner, requiring no 3D ground truth but\nonly 2D annotations. Specifically, our method contains two steps: triangulation\nand refinement. First, given the 2D keypoints that can be obtained through any\nclassic 2D detection methods, triangulation is performed across two views to\nlift the 2D keypoints into coarse 3D poses. Then, a novel cross-view U-shaped\ngraph convolutional network (CV-UGCN), which can explore the spatial\nconfigurations and cross-view correlations, is designed to refine the coarse 3D\nposes. In particular, the refinement progress is achieved through\nweakly-supervised learning, in which geometric and structure-aware consistency\nchecks are performed. We evaluate our method on the standard benchmark dataset,\nHuman3.6M. The Mean Per Joint Position Error on the benchmark dataset is 27.4\nmm, which outperforms existing state-of-the-art methods remarkably (27.4 mm vs\n30.2 mm).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Guoliang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Runwei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Face Anti-Spoofing: A Survey. (arXiv:2106.14948v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14948","description":"<p>Face anti-spoofing (FAS) has lately attracted increasing attention due to its\nvital role in securing face recognition systems from presentation attacks\n(PAs). As more and more realistic PAs with novel types spring up, traditional\nFAS methods based on handcrafted features become unreliable due to their\nlimited representation capacity. With the emergence of large-scale academic\ndatasets in the recent decade, deep learning based FAS achieves remarkable\nperformance and dominates this area. However, existing reviews in this field\nmainly focus on the handcrafted features, which are outdated and uninspiring\nfor the progress of FAS community. In this paper, to stimulate future research,\nwe present the first comprehensive review of recent advances in deep learning\nbased FAS. It covers several novel and insightful components: 1) besides\nsupervision with binary label (e.g., '0' for bonafide vs. '1' for PAs), we also\ninvestigate recent methods with pixel-wise supervision (e.g., pseudo depth\nmap); 2) in addition to traditional intra-dataset evaluation, we collect and\nanalyze the latest methods specially designed for domain generalization and\nopen-set FAS; and 3) besides commercial RGB camera, we summarize the deep\nlearning applications under multi-modal (e.g., depth and infrared) or\nspecialized (e.g., light field and flash) sensors. We conclude this survey by\nemphasizing current open issues and highlighting potential prospects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yunxiao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenxu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-Free Domain Adaptation for Image Segmentation. (arXiv:2108.03152v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03152","description":"<p>Domain adaptation (DA) has drawn high interest for its capacity to adapt a\nmodel trained on labeled source data to perform well on unlabeled or weakly\nlabeled target data from a different domain. Most common DA techniques require\nconcurrent access to the input images of both the source and target domains.\nHowever, in practice, privacy concerns often impede the availability of source\nimages in the adaptation phase. This is a very frequent DA scenario in medical\nimaging, where, for instance, the source and target images could come from\ndifferent clinical sites. We introduce a source-free domain adaptation for\nimage segmentation. Our formulation is based on minimizing a label-free entropy\nloss defined over target-domain data, which we further guide with a\ndomain-invariant prior on the segmentation regions. Many priors can be derived\nfrom anatomical information. Here, a class ratio prior is estimated from\nanatomical knowledge and integrated in the form of a Kullback Leibler (KL)\ndivergence in our overall loss function. Furthermore, we motivate our overall\nloss with an interesting link to maximizing the mutual information between the\ntarget images and their label predictions. We show the effectiveness of our\nprior aware entropy minimization in a variety of domain-adaptation scenarios,\nwith different modalities and applications, including spine, prostate, and\ncardiac segmentation. Our method yields comparable results to several state of\nthe art adaptation techniques, despite having access to much less information,\nas the source images are entirely absent in our adaptation phase. Our\nstraightforward adaptation strategy uses only one network, contrary to popular\nadversarial techniques, which are not applicable to a source-free DA setting.\nOur framework can be readily used in a breadth of segmentation problems, and\nour code is publicly available: https://github.com/mathilde-b/SFDA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bateson_M/0/1/0/all/0/1\">Mathilde Bateson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kervadec_H/0/1/0/all/0/1\">Hoel Kervadec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombaert_H/0/1/0/all/0/1\">Herv&#xe9; Lombaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Meta Pattern for Face Anti-Spoofing. (arXiv:2110.06753v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06753","description":"<p>Face Anti-Spoofing (FAS) is essential to secure face recognition systems and\nhas been extensively studied in recent years. Although deep neural networks\n(DNNs) for the FAS task have achieved promising results in intra-dataset\nexperiments with similar distributions of training and testing data, the DNNs'\ngeneralization ability is limited under the cross-domain scenarios with\ndifferent distributions of training and testing data. To improve the\ngeneralization ability, recent hybrid methods have been explored to extract\ntask-aware handcrafted features (e.g., Local Binary Pattern) as discriminative\ninformation for the input of DNNs. However, the handcrafted feature extraction\nrelies on experts' domain knowledge, and how to choose appropriate handcrafted\nfeatures is underexplored. To this end, we propose a learnable network to\nextract Meta Pattern (MP) in our learning-to-learn framework. By replacing\nhandcrafted features with the MP, the discriminative information from MP is\ncapable of learning a more generalized model. Moreover, we devise a two-stream\nnetwork to hierarchically fuse the input RGB image and the extracted MP by\nusing our proposed Hierarchical Fusion Module (HFM). We conduct comprehensive\nexperiments and show that our MP outperforms the compared handcrafted features.\nAlso, our proposed method with HFM and the MP can achieve state-of-the-art\nperformance on two different domain generalization evaluation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Rizhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1\">Renjie Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yongjian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex Chichung Kot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Strong Gravitational Lenses Through Self-Attention. (arXiv:2110.09202v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09202","description":"<p>The upcoming large scale surveys like LSST are expected to find approximately\n$10^5$ strong gravitational lenses by analysing data of many orders of\nmagnitude larger than those in contemporary astronomical surveys. In this case,\nnon-automated techniques will be highly challenging and time-consuming, even if\nthey are possible at all. We propose a new automated architecture based on the\nprinciple of self-attention to find strong gravitational lenses. The advantages\nof self-attention-based encoder models over convolution neural networks are\ninvestigated, and ways to optimise the outcome of encoder models are analysed.\nWe constructed and trained 21 self-attention based encoder models and five\nconvolution neural networks to identify gravitational lenses from the Bologna\nLens Challenge. Each model was trained separately using 18,000 simulated\nimages, cross-validated using 2,000 images, and then applied to a test set with\n100,000 images. We used four different metrics for evaluation: classification\naccuracy, area under the receiver operating characteristic curve (AUROC), the\nTPR$_0$ score and the TPR$_{10}$ score. The performances of\nself-attention-based encoder models and CNNs participating in the challenge are\ncompared. They were able to surpass the CNN models that participated in the\nBologna Lens Challenge by a high margin for the $TPR_0$ and $TPR_${10}$.\nSelf-Attention based models have clear advantages compared to simpler CNNs.\nThey have highly competing performance in comparison to the currently used\nresidual neural networks. Compared to CNNs, self-attention based models can\nidentify highly confident lensing candidates and will be able to filter out\npotential candidates from real data. Moreover, introducing the encoder layers\ncan also tackle the over-fitting problem present in the CNNs by acting as\neffective filters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thuruthipilly_H/0/1/0/all/0/1\">Hareesh Thuruthipilly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_A/0/1/0/all/0/1\">Adam Zadrozny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollo_A/0/1/0/all/0/1\">Agnieszka Pollo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biesiada_M/0/1/0/all/0/1\">Marek Biesiada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-agnostic Object Detection with Multi-modal Transformer. (arXiv:2111.11430v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11430","description":"<p>What constitutes an object? This has been a long-standing question in\ncomputer vision. Towards this goal, numerous learning-free and learning-based\napproaches have been developed to score objectness. However, they generally do\nnot scale well across new domains and novel objects. In this paper, we advocate\nthat existing methods lack a top-down supervision signal governed by\nhuman-understandable semantics. For the first time in literature, we\ndemonstrate that Multi-modal Vision Transformers (MViT) trained with aligned\nimage-text pairs can effectively bridge this gap. Our extensive experiments\nacross various domains and novel objects show the state-of-the-art performance\nof MViTs to localize generic objects in images. Based on the observation that\nexisting MViTs do not include multi-scale feature processing and usually\nrequire longer training schedules, we develop an efficient MViT architecture\nusing multi-scale deformable attention and late vision-language fusion. We show\nthe significance of MViT proposals in a diverse range of applications including\nopen-world object detection, salient and camouflage object detection,\nsupervised and self-supervised detection tasks. Further, MViTs can adaptively\ngenerate proposals given a specific language query and thus offer enhanced\ninteractability. Code: \\url{https://git.io/J1HPY}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maaz_M/0/1/0/all/0/1\">Muhammad Maaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_H/0/1/0/all/0/1\">Hanoona Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1\">Rao Muhammad Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Attack on Facial Soft-biometric Privacy Enhancement. (arXiv:2111.12405v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12405","description":"<p>In the recent past, different researchers have proposed privacy-enhancing\nface recognition systems designed to conceal soft-biometric attributes at\nfeature level. These works have reported impressive results, but generally did\nnot consider specific attacks in their analysis of privacy protection. We\nintroduce an attack on said schemes based on two observations: (1) highly\nsimilar facial representations usually originate from face images with similar\nsoft-biometric attributes; (2) to achieve high recognition accuracy, robustness\nagainst intra-class variations within facial representations has to be retained\nin their privacy-enhanced versions. The presented attack only requires the\nprivacy-enhancing algorithm as a black-box and a relatively small database of\nface images with annotated soft-biometric attributes. Firstly, an intercepted\nprivacy-enhanced face representation is compared against the attacker's\ndatabase. Subsequently, the unknown attribute is inferred from the attributes\nassociated with the highest obtained similarity scores. In the experiments, the\nattack is applied against two state-of-the-art approaches. The attack is shown\nto circumvent the privacy enhancement to a considerable degree and is able to\ncorrectly classify gender with an accuracy of up to approximately 90%. Future\nworks on privacy-enhancing face recognition are encouraged to include the\nproposed attack in evaluations on the privacy protection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Osorio_Roig_D/0/1/0/all/0/1\">Dail&#xe9; Osorio-Roig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terhorst_P/0/1/0/all/0/1\">Philipp Terh&#xf6;rst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struc_V/0/1/0/all/0/1\">Vitomir &#x160;truc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning. (arXiv:2111.13196v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13196","description":"<p>The canonical approach to video captioning dictates a caption generation\nmodel to learn from offline-extracted dense video features. These feature\nextractors usually operate on video frames sampled at a fixed frame rate and\nare often trained on image/video understanding tasks, without adaption to video\ncaptioning data. In this work, we present SwinBERT, an end-to-end\ntransformer-based model for video captioning, which takes video frame patches\ndirectly as inputs, and outputs a natural language description. Instead of\nleveraging multiple 2D/3D feature extractors, our method adopts a video\ntransformer to encode spatial-temporal representations that can adapt to\nvariable lengths of video input without dedicated design for different frame\nrates. Based on this model architecture, we show that video captioning can\nbenefit significantly from more densely sampled video frames as opposed to\nprevious successes with sparsely sampled video frames for video-and-language\nunderstanding tasks (e.g., video question answering). Moreover, to avoid the\ninherent redundancy in consecutive video frames, we propose adaptively learning\na sparse attention mask and optimizing it for task-specific performance\nimprovement through better long-range video sequence modeling. Through\nextensive experiments on 5 video captioning datasets, we show that SwinBERT\nachieves across-the-board performance improvements over previous methods, often\nby a large margin. The learned sparse attention masks in addition push the\nlimit to new state of the arts, and can be transferred between different video\nlengths and between different datasets. Code is available at\nhttps://github.com/microsoft/SwinBERT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chung-Ching Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Faisal Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yumao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Generalization by Learning a Bridge Across Domains. (arXiv:2112.02300v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02300","description":"<p>The ability to generalize learned representations across significantly\ndifferent visual domains, such as between real photos, clipart, paintings, and\nsketches, is a fundamental capacity of the human visual system. In this paper,\ndifferent from most cross-domain works that utilize some (or full) source\ndomain supervision, we approach a relatively new and very practical\nUnsupervised Domain Generalization (UDG) setup of having no training\nsupervision in neither source nor target domains. Our approach is based on\nself-supervised learning of a Bridge Across Domains (BrAD) - an auxiliary\nbridge domain accompanied by a set of semantics preserving visual\n(image-to-image) mappings to BrAD from each of the training domains. The BrAD\nand mappings to it are learned jointly (end-to-end) with a contrastive\nself-supervised representation model that semantically aligns each of the\ndomains to its BrAD-projection, and hence implicitly drives all the domains\n(seen or unseen) to semantically align to each other. In this work, we show how\nusing an edge-regularized BrAD our approach achieves significant gains across\nmultiple benchmarks and a range of tasks, including UDG, Few-shot UDA, and\nunsupervised generalization across multi-domain datasets (including\ngeneralization to unseen domains and classes).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harary_S/0/1/0/all/0/1\">Sivan Harary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_E/0/1/0/all/0/1\">Eli Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelle_A/0/1/0/all/0/1\">Assaf Arbelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staar_P/0/1/0/all/0/1\">Peter Staar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Hussein_S/0/1/0/all/0/1\">Shady Abu-Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amrani_E/0/1/0/all/0/1\">Elad Amrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1\">Roei Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfassy_A/0/1/0/all/0/1\">Amit Alfassy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1\">Dina Katabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Unsupervised Stain-To-Stain Translation using Self-Supervision and Meta-Learning. (arXiv:2112.08837v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.08837","description":"<p>In digital pathology, many image analysis tasks are challenged by the need\nfor large and time-consuming manual data annotations to cope with various\nsources of variability in the image domain. Unsupervised domain adaptation\nbased on image-to-image translation is gaining importance in this field by\naddressing variabilities without the manual overhead. Here, we tackle the\nvariation of different histological stains by unsupervised stain-to-stain\ntranslation to enable a stain-independent applicability of a deep learning\nsegmentation model. We use CycleGANs for stain-to-stain translation in kidney\nhistopathology, and propose two novel approaches to improve translational\neffectivity. First, we integrate a prior segmentation network into the CycleGAN\nfor a self-supervised, application-oriented optimization of translation through\nsemantic guidance, and second, we incorporate extra channels to the translation\noutput to implicitly separate artificial meta-information otherwise encoded for\ntackling underdetermined reconstructions. The latter showed partially superior\nperformances to the unmodified CycleGAN, but the former performed best in all\nstains providing instance-level Dice scores ranging between 78% and 92% for\nmost kidney structures, such as glomeruli, tubules, and veins. However,\nCycleGANs showed only limited performance in the translation of other\nstructures, e.g. arteries. Our study also found somewhat lower performance for\nall structures in all stains when compared to segmentation in the original\nstain. Our study suggests that with current unsupervised technologies, it seems\nunlikely to produce generally applicable simulated stains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bouteldja_N/0/1/0/all/0/1\">Nassim Bouteldja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klinkhammer_B/0/1/0/all/0/1\">Barbara Mara Klinkhammer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schlaich_T/0/1/0/all/0/1\">Tarek Schlaich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boor_P/0/1/0/all/0/1\">Peter Boor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Merhof_D/0/1/0/all/0/1\">Dorit Merhof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"V-LinkNet: Learning Contextual Inpainting Across Latent Space of Generative Adversarial Network. (arXiv:2201.00323v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00323","description":"<p>Image inpainting is a key technique in image processing task to predict the\nmissing regions and generate realistic images. Given the advancement of\nexisting generative inpainting models with feature extraction, propagation and\nreconstruction capabilities, there is lack of high-quality feature extraction\nand transfer mechanisms in deeper layers to tackle persistent aberrations on\nthe generated inpainted regions. Our method, V-LinkNet, develops high-level\nfeature transference to deep level textural context of inpainted regions our\nwork, proposes a novel technique of combining encoders learning through a\nrecursive residual transition layer (RSTL). The RSTL layer easily adapts dual\nencoders by increasing the unique semantic information through direct\ncommunication. By collaborating the dual encoders structure with contextualised\nfeature representation loss function, our system gains the ability to inpaint\nwith high-level features. To reduce biases from random mask-image pairing, we\nintroduce a standard protocol with paired mask-image on the testing set of\nCelebA-HQ, Paris Street View and Places2 datasets. Our results show V-LinkNet\nperformed better on CelebA-HQ and Paris Street View using this standard\nprotocol. We will share the standard protocol and our codes with the research\ncommunity upon acceptance of this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jam_J/0/1/0/all/0/1\">Jireh Jam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kendrick_C/0/1/0/all/0/1\">Connah Kendrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drouard_V/0/1/0/all/0/1\">Vincent Drouard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_K/0/1/0/all/0/1\">Kevin Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_M/0/1/0/all/0/1\">Moi Hoon Yap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Quantum-Classical Algorithm for Robust Fitting. (arXiv:2201.10110v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10110","description":"<p>Fitting geometric models onto outlier contaminated data is provably\nintractable. Many computer vision systems rely on random sampling heuristics to\nsolve robust fitting, which do not provide optimality guarantees and error\nbounds. It is therefore critical to develop novel approaches that can bridge\nthe gap between exact solutions that are costly, and fast heuristics that offer\nno quality assurances. In this paper, we propose a hybrid quantum-classical\nalgorithm for robust fitting. Our core contribution is a novel robust fitting\nformulation that solves a sequence of integer programs and terminates with a\nglobal solution or an error bound. The combinatorial subproblems are amenable\nto a quantum annealer, which helps to tighten the bound efficiently. While our\nusage of quantum computing does not surmount the fundamental intractability of\nrobust fitting, by providing error bounds our algorithm is a practical\nimprovement over randomised heuristics. Moreover, our work represents a\nconcrete application of quantum computing in computer vision. We present\nresults obtained using an actual quantum computer (D-Wave Advantage) and via\nsimulation. Source code: https://github.com/dadung/HQC-robust-fitting\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doan_A/0/1/0/all/0/1\">Anh-Dzung Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasdelli_M/0/1/0/all/0/1\">Michele Sasdelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1\">David Suter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransBTSV2: Towards Better and More Efficient Volumetric Segmentation of Medical Images. (arXiv:2201.12785v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.12785","description":"<p>Transformer, benefiting from global (long-range) information modeling using\nself-attention mechanism, has been successful in natural language processing\nand computer vision recently. Convolutional Neural Networks, capable of\ncapturing local features, are difficult to model explicit long-distance\ndependencies from global feature space. However, both local and global features\nare crucial for dense prediction tasks, especially for 3D medical image\nsegmentation. In this paper, we present the further attempt to exploit\nTransformer in 3D CNN for 3D medical image volumetric segmentation and propose\na novel network named TransBTSV2 based on the encoder-decoder structure.\nDifferent from TransBTS, the proposed TransBTSV2 is not limited to brain tumor\nsegmentation (BTS) but focuses on general medical image segmentation, providing\na stronger and more efficient 3D baseline for volumetric segmentation of\nmedical images. As a hybrid CNN-Transformer architecture, TransBTSV2 can\nachieve accurate segmentation of medical images without any pre-training,\npossessing the strong inductive bias as CNNs and powerful global context\nmodeling ability as Transformer. With the proposed insight to redesign the\ninternal structure of Transformer block and the introduced Deformable\nBottleneck Module to capture shape-aware local details, a highly efficient\narchitecture is achieved with superior performance. Extensive experimental\nresults on four medical image datasets (BraTS 2019, BraTS 2020, LiTS 2017 and\nKiTS 2019) demonstrate that TransBTSV2 achieves comparable or better results\ncompared to the state-of-the-art methods for the segmentation of brain tumor,\nliver tumor as well as kidney tumor. Code will be publicly available at\nhttps://github.com/Wenxuan-1119/TransBTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangyun Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zha_S/0/1/0/all/0/1\">Sen Zha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Developmentally-Inspired Examination of Shape versus Texture Bias in Machines. (arXiv:2202.08340v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08340","description":"<p>Early in development, children learn to extend novel category labels to\nobjects with the same shape, a phenomenon known as the shape bias. Inspired by\nthese findings, Geirhos et al. (2019) examined whether deep neural networks\nshow a shape or texture bias by constructing images with conflicting shape and\ntexture cues. They found that convolutional neural networks strongly preferred\nto classify familiar objects based on texture as opposed to shape, suggesting a\ntexture bias. However, there are a number of differences between how the\nnetworks were tested in this study versus how children are typically tested. In\nthis work, we re-examine the inductive biases of neural networks by adapting\nthe stimuli and procedure from Geirhos et al. (2019) to more closely follow the\ndevelopmental paradigm and test on a wide range of pre-trained neural networks.\nAcross three experiments, we find that deep neural networks exhibit a\npreference for shape rather than texture when tested under conditions that more\nclosely replicate the developmental procedure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tartaglini_A/0/1/0/all/0/1\">Alexa R. Tartaglini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vong_W/0/1/0/all/0/1\">Wai Keen Vong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1\">Brenden M. Lake</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Roto-Translation Equivariant Super-Resolution of Two-Dimensional Flows Using Convolutional Neural Networks. (arXiv:2202.11099v3 [physics.flu-dyn] UPDATED)","link":"http://arxiv.org/abs/2202.11099","description":"<p>Convolutional neural networks (CNNs) often apparently process vectors as\nquantities that have no direction such as colors in images. This study\ninvestigates the effect of considering vectors as geometric objects in terms of\nsuper-resolution of velocity on two-dimensional fluids. Vector is distinguished\nfrom scalar by the transformation law associated with a change in basis, which\ncan be incorporated as prior knowledge using the equivariant deep learning. The\nexisting CNNs are converted into equivariant ones by rendering each layer\nequivariant with respect to rotation and translation. The training data in the\nhigh- and low-resolution are generated with the fluid simulation and\ndownsampling, respectively. The inference of the equivariant CNNs is not highly\naccurate or robust, compared with the conventional CNNs. The conventional CNNs\ncan learn the equivariance and recognize vector directions, adapting to the\nsymmetry of data. In contrast, the equivariant CNNs do not have this\nflexibility and their inference can be sensitive to the method of data\ngeneration. The main advantage of equivariant CNNs is the trainability with a\nsmaller size of data due to the reduction in the parameters. The conclusion of\nthis paper is negative toward the use of equivariant CNNs in super-resolution\ntasks, which is in contrast to previous studies that apply equivariant neural\nnetworks to other fluid mechanics tasks. The effect of incorporating the\ngeometric equivariance in neural networks has yet to be sufficiently explored.\nIt will be necessary to conduct super-resolution with more general\nconfigurations such as flows on irregular grids.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yasuda_Y/0/1/0/all/0/1\">Yuki Yasuda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoVISQ: Reduction of Video Service Quality via Adversarial Attacks on Deep Learning-based Video Compression. (arXiv:2203.10183v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10183","description":"<p>Video compression plays a crucial role in video streaming and classification\nsystems by maximizing the end-user quality of experience (QoE) at a given\nbandwidth budget. In this paper, we conduct the first systematic study for\nadversarial attacks on deep learning-based video compression and downstream\nclassification systems. Our attack framework, dubbed RoVISQ, manipulates the\nRate-Distortion (R-D) relationship of a video compression model to achieve one\nor both of the following goals: (1) increasing the network bandwidth, (2)\ndegrading the video quality for end-users. We further devise new objectives for\ntargeted and untargeted attacks to a downstream video classification service.\nFinally, we design an input-invariant perturbation that universally disrupts\nvideo compression and classification systems in real time. Unlike previously\nproposed attacks on video classification, our adversarial perturbations are the\nfirst to withstand compression. We empirically show the resilience of RoVISQ\nattacks against various defenses, i.e., adversarial training, video denoising,\nand JPEG compression. Our extensive experimental results on various video\ndatasets show RoVISQ attacks deteriorate peak signal-to-noise ratio by up to\n5.6dB and the bit-rate by up to 2.4 times while achieving over 90% attack\nsuccess rate on a downstream classifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jung-Woo Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javaheripi_M/0/1/0/all/0/1\">Mojan Javaheripi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidano_S/0/1/0/all/0/1\">Seira Hidano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1\">Farinaz Koushanfar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modality-Balanced Embedding for Video Retrieval. (arXiv:2204.08182v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08182","description":"<p>Video search has become the main routine for users to discover videos\nrelevant to a text query on large short-video sharing platforms. During\ntraining a query-video bi-encoder model using online search logs, we identify a\nmodality bias phenomenon that the video encoder almost entirely relies on text\nmatching, neglecting other modalities of the videos such as vision, audio. This\nmodality imbalanceresults from a) modality gap: the relevance between a query\nand a video text is much easier to learn as the query is also a piece of text,\nwith the same modality as the video text; b) data bias: most training samples\ncan be solved solely by text matching. Here we share our practices to improve\nthe first retrieval stage including our solution for the modality imbalance\nissue. We propose MBVR (short for Modality Balanced Video Retrieval) with two\nkey components: manually generated modality-shuffled (MS) samples and a dynamic\nmargin (DM) based on visual relevance. They can encourage the video encoder to\npay balanced attentions to each modality. Through extensive experiments on a\nreal world dataset, we show empirically that our method is both effective and\nefficient in solving modality bias problem. We have also deployed our MBVR in a\nlarge video platform and observed statistically significant boost over a highly\noptimized baseline in an A/B test and manual GSB evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_B/0/1/0/all/0/1\">Bingqing Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1\">Qiushi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Cheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09817","description":"<p>Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision-language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1\">Stephanie Hyland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1\">Maria Wetscherek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The 6th AI City Challenge. (arXiv:2204.10380v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10380","description":"<p>The 6th edition of the AI City Challenge specifically focuses on problems in\ntwo domains where there is tremendous unlocked potential at the intersection of\ncomputer vision and artificial intelligence: Intelligent Traffic Systems (ITS),\nand brick and mortar retail businesses. The four challenge tracks of the 2022\nAI City Challenge received participation requests from 254 teams across 27\ncountries. Track 1 addressed city-scale multi-target multi-camera (MTMC)\nvehicle tracking. Track 2 addressed natural-language-based vehicle track\nretrieval. Track 3 was a brand new track for naturalistic driving analysis,\nwhere the data were captured by several cameras mounted inside the vehicle\nfocusing on driver safety, and the task was to classify driver actions. Track 4\nwas another new track aiming to achieve retail store automated checkout using\nonly a single view camera. We released two leader boards for submissions based\non different methods, including a public leader board for the contest, where no\nuse of external data is allowed, and a general leader board for all submitted\nresults. The top performance of participating teams established strong\nbaselines and even outperformed the state-of-the-art in the proposed challenge\ntracks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naphade_M/0/1/0/all/0/1\">Milind Naphade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasiu_D/0/1/0/all/0/1\">David C. Anastasiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yue Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mohammed Shaiqur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatachalapathy_A/0/1/0/all/0/1\">Archana Venkatachalapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuj Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ablavsky_V/0/1/0/all/0/1\">Vitaly Ablavsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pranamesh Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alice Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shangru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequencer: Deep LSTM for Image Classification. (arXiv:2205.01972v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01972","description":"<p>In recent computer vision research, the advent of the Vision Transformer\n(ViT) has rapidly revolutionized various architectural design efforts: ViT\nachieved state-of-the-art image classification performance using self-attention\nfound in natural language processing, and MLP-Mixer achieved competitive\nperformance using simple multi-layer perceptrons. In contrast, several studies\nhave also suggested that carefully redesigned convolutional neural networks\n(CNNs) can achieve advanced performance comparable to ViT without resorting to\nthese new ideas. Against this background, there is growing interest in what\ninductive bias is suitable for computer vision. Here we propose Sequencer, a\nnovel and competitive architecture alternative to ViT that provides a new\nperspective on these issues. Unlike ViTs, Sequencer models long-range\ndependencies using LSTMs rather than self-attention layers. We also propose a\ntwo-dimensional version of Sequencer module, where an LSTM is decomposed into\nvertical and horizontal LSTMs to enhance performance. Despite its simplicity,\nseveral experiments demonstrate that Sequencer performs impressively well:\nSequencer2D-L, with 54M parameters, realizes 84.6% top-1 accuracy on only\nImageNet-1K. Not only that, we show that it has good transferability and the\nrobust resolution adaptability on double resolution-band.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1\">Yuki Tatsunami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1\">Masato Taki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating the Training of Video Super-Resolution Models. (arXiv:2205.05069v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05069","description":"<p>Despite that convolution neural networks (CNN) have recently demonstrated\nhigh-quality reconstruction for video super-resolution (VSR), efficiently\ntraining competitive VSR models remains a challenging problem. It usually takes\nan order of magnitude more time than training their counterpart image models,\nleading to long research cycles. Existing VSR methods typically train models\nwith fixed spatial and temporal sizes from beginning to end. The fixed sizes\nare usually set to large values for good performance, resulting to slow\ntraining. However, is such a rigid training strategy necessary for VSR? In this\nwork, we show that it is possible to gradually train video models from small to\nlarge spatial/temporal sizes, i.e., in an easy-to-hard manner. In particular,\nthe whole training is divided into several stages and the earlier stage has\nsmaller training spatial shape. Inside each stage, the temporal size also\nvaries from short to long while the spatial size remains unchanged. Training is\naccelerated by such a multigrid training strategy, as most of computation is\nperformed on smaller spatial and shorter temporal shapes. For further\nacceleration with GPU parallelization, we also investigate the large minibatch\ntraining without the loss in accuracy. Extensive experiments demonstrate that\nour method is capable of largely speeding up training (up to $6.2\\times$\nspeedup in wall-clock training time) without performance drop for various VSR\nmodels. The code is available at\nhttps://github.com/TencentARC/Efficient-VSR-Training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lijian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhongang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Depth Completion: A Survey. (arXiv:2205.05335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05335","description":"<p>Depth completion aims at predicting dense pixel-wise depth from a sparse map\ncaptured from a depth sensor. It plays an essential role in various\napplications such as autonomous driving, 3D reconstruction, augmented reality,\nand robot navigation. Recent successes on the task have been demonstrated and\ndominated by deep learning based solutions. In this article, for the first\ntime, we provide a comprehensive literature review that helps readers better\ngrasp the research trends and clearly understand the current advances. We\ninvestigate the related studies from the design aspects of network\narchitectures, loss functions, benchmark datasets, and learning strategies with\na proposal of a novel taxonomy that categorizes existing methods. Besides, we\npresent a quantitative comparison of model performance on two widely used\nbenchmark datasets, including an indoor and an outdoor dataset. Finally, we\ndiscuss the challenges of prior works and provide readers with some insights\nfor future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1\">Chenyu Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozay_M/0/1/0/all/0/1\">Mete Ozay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chenyou Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Honghai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning. (arXiv:2205.06401v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2205.06401","description":"<p>Contrastive learning pre-trains an image encoder using a large amount of\nunlabeled data such that the image encoder can be used as a general-purpose\nfeature extractor for various downstream tasks. In this work, we propose\nPoisonedEncoder, a data poisoning attack to contrastive learning. In\nparticular, an attacker injects carefully crafted poisoning inputs into the\nunlabeled pre-training data, such that the downstream classifiers built based\non the poisoned encoder for multiple target downstream tasks simultaneously\nclassify attacker-chosen, arbitrary clean inputs as attacker-chosen, arbitrary\nclasses. We formulate our data poisoning attack as a bilevel optimization\nproblem, whose solution is the set of poisoning inputs; and we propose a\ncontrastive-learning-tailored method to approximately solve it. Our evaluation\non multiple datasets shows that PoisonedEncoder achieves high attack success\nrates while maintaining the testing accuracy of the downstream classifiers\nbuilt upon the poisoned encoder for non-attacker-chosen inputs. We also\nevaluate five defenses against PoisonedEncoder, including one pre-processing,\nthree in-processing, and one post-processing defenses. Our results show that\nthese defenses can decrease the attack success rate of PoisonedEncoder, but\nthey also sacrifice the utility of the encoder or require a large clean\npre-training dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinyuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Effective Transformer-based Solution for RSNA Intracranial Hemorrhage Detection Competition. (arXiv:2205.07556v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.07556","description":"<p>We present an effective method for Intracranial Hemorrhage Detection (IHD)\nwhich exceeds the performance of the winner solution in RSNA-IHD competition\n(2019). Meanwhile, our model only takes quarter parameters and ten percent\nFLOPs compared to the winner's solution. The IHD task needs to predict the\nhemorrhage category of each slice for the input brain CT. We review the top-5\nsolutions for the IHD competition held by the Radiological Society of North\nAmerica(RSNA) in 2019. Nearly all the top solutions rely on 2D convolutional\nnetworks and sequential models (Bidirectional GRU or LSTM) to extract\nintra-slice and inter-slice features, respectively. All the top solutions\nenhance the performance by leveraging the model ensemble, and the model number\nvaries from 7 to 31. In the past years, since much progress has been made in\nthe computer vision regime especially Transformer-based models, we introduce\nthe Transformer-based techniques to extract the features in both intra-slice\nand inter-slice views for IHD tasks. Additionally, a semi-supervised method is\nembedded into our workflow to further improve the performance. The code is\navailable in the manuscript.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fangxin Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}