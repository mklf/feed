{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Leaf Clinical Trials Corpus: a new resource for query generation from clinical trial eligibility criteria. (arXiv:2207.13757v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13757","description":"<p>Identifying cohorts of patients based on eligibility criteria such as medical\nconditions, procedures, and medication use is critical to recruitment for\nclinical trials. Such criteria are often most naturally described in free-text,\nusing language familiar to clinicians and researchers. In order to identify\npotential participants at scale, these criteria must first be translated into\nqueries on clinical databases, which can be labor-intensive and error-prone.\nNatural language processing (NLP) methods offer a potential means of such\nconversion into database queries automatically. However they must first be\ntrained and evaluated using corpora which capture clinical trials criteria in\nsufficient detail. In this paper, we introduce the Leaf Clinical Trials (LCT)\ncorpus, a human-annotated corpus of over 1,000 clinical trial eligibility\ncriteria descriptions using highly granular structured labels capturing a range\nof biomedical phenomena. We provide details of our schema, annotation process,\ncorpus quality, and statistics. Additionally, we present baseline information\nextraction results on this corpus as benchmarks for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dobbins_N/0/1/0/all/0/1\">Nicholas J Dobbins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullen_T/0/1/0/all/0/1\">Tony Mullen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzuner_O/0/1/0/all/0/1\">Ozlem Uzuner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CompText: Visualizing, Comparing & Understanding Text Corpus. (arXiv:2207.13771v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13771","description":"<p>A common practice in Natural Language Processing (NLP) is to visualize the\ntext corpus without reading through the entire literature, still grasping the\ncentral idea and key points described. For a long time, researchers focused on\nextracting topics from the text and visualizing them based on their relative\nsignificance in the corpus. However, recently, researchers started coming up\nwith more complex systems that not only expose the topics of the corpus but\nalso word closely related to the topic to give users a holistic view. These\ndetailed visualizations spawned research on comparing text corpora based on\ntheir visualization. Topics are often compared to idealize the difference\nbetween corpora. However, to capture greater semantics from different corpora,\nresearchers have started to compare texts based on the sentiment of the topics\nrelated to the text. Comparing the words carrying the most weightage, we can\nget an idea about the important topics for corpus. There are multiple existing\ntexts comparing methods present that compare topics rather than sentiments but\nwe feel that focusing on sentiment-carrying words would better compare the two\ncorpora. Since only sentiments can explain the real feeling of the text and not\njust the topic, topics without sentiments are just nouns. We aim to\ndifferentiate the corpus with a focus on sentiment, as opposed to comparing all\nthe words appearing in the two corpora. The rationale behind this is, that the\ntwo corpora do not many have identical words for side-by-side comparison, so\ncomparing the sentiment words gives us an idea of how the corpora are appealing\nto the emotions of the reader. We can argue that the entropy or the\nunexpectedness and divergence of topics should also be of importance and help\nus to identify key pivot points and the importance of certain topics in the\ncorpus alongside relative sentiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_S/0/1/0/all/0/1\">Suvi Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jas_D/0/1/0/all/0/1\">Divjeet Singh Jas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persona-Knowledge Dialogue Multi-Context Retrieval and Enhanced Decoding Methods. (arXiv:2207.13919v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13919","description":"<p>Persona and Knowledge dual context open-domain chat is a novel dialogue\ngeneration task introduced recently. While Persona and Knowledge is each\ninteresting context of open-domain dialogue, the combination of both has not\nbeen well studied. We tackle Persona-Knowledge identification and response\ngeneration tasks in this paper. We design an informed data augmentation\nstrategy that is compatible with neural Q&amp;A retrieval models. With the\naugmented data, we perform permutative Persona-Knowledge evaluation and\nsuccessive Persona search fine-tuning. Furthermore, we perform dialogue\ngeneration with various decoding techniques and illustrate crucial elements. We\nachieve SOTA across official metrics with 93.99% Grounding accuracy average and\n23.62 SacreBLEU score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Min Sik Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Min Sang Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLRIP: Pre-training a military language representation model with informative factual knowledge and professional knowledge base. (arXiv:2207.13929v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13929","description":"<p>Incorporating prior knowledge into pre-trained language models has proven to\nbe effective for knowledge-driven NLP tasks, such as entity typing and relation\nextraction. Current pre-training procedures usually inject external knowledge\ninto models by using knowledge masking, knowledge fusion and knowledge\nreplacement. However, factual information contained in the input sentences have\nnot been fully mined, and the external knowledge for injecting have not been\nstrictly checked. As a result, the context information cannot be fully\nexploited and extra noise will be introduced or the amount of knowledge\ninjected is limited. To address these issues, we propose MLRIP, which modifies\nthe knowledge masking strategies proposed by ERNIE-Baidu, and introduce a\ntwo-stage entity replacement strategy. Extensive experiments with comprehensive\nanalyses illustrate the superiority of MLRIP over BERT-based models in military\nknowledge-driven NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuekang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiping Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Interpretability Evaluation Benchmark for Pre-trained Language Models. (arXiv:2207.13948v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13948","description":"<p>While pre-trained language models (LMs) have brought great improvements in\nmany NLP tasks, there is increasing attention to explore capabilities of LMs\nand interpret their predictions. However, existing works usually focus only on\na certain capability with some downstream tasks. There is a lack of datasets\nfor directly evaluating the masked word prediction performance and the\ninterpretability of pre-trained LMs. To fill in the gap, we propose a novel\nevaluation benchmark providing with both English and Chinese annotated data. It\ntests LMs abilities in multiple dimensions, i.e., grammar, semantics,\nknowledge, reasoning and computation. In addition, it provides carefully\nannotated token-level rationales that satisfy sufficiency and compactness. It\ncontains perturbed instances for each original instance, so as to use the\nrationale consistency under perturbations as the metric for faithfulness, a\nperspective of interpretability. We conduct experiments on several widely-used\npre-trained LMs. The results show that they perform very poorly on the\ndimensions of knowledge and computation. And their plausibility in all\ndimensions is far from satisfactory, especially when the rationale is short. In\naddition, the pre-trained LMs we evaluated are not robust on syntax-aware data.\nWe will release this evaluation benchmark at \\url{<a href=\"http://xyz\">this http URL</a>}, and hope it can\nfacilitate the research progress of pre-trained LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yaozong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Architecture Search on Efficient Transformers and Beyond. (arXiv:2207.13955v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13955","description":"<p>Recently, numerous efficient Transformers have been proposed to reduce the\nquadratic computational complexity of standard Transformers caused by the\nSoftmax attention. However, most of them simply swap Softmax with an efficient\nattention mechanism without considering the customized architectures specially\nfor the efficient attention. In this paper, we argue that the handcrafted\nvanilla Transformer architectures for Softmax attention may not be suitable for\nefficient Transformers. To address this issue, we propose a new framework to\nfind optimal architectures for efficient Transformers with the neural\narchitecture search (NAS) technique. The proposed method is validated on\npopular machine translation and image classification tasks. We observe that the\noptimal architecture of the efficient Transformer has the reduced computation\ncompared with that of the standard Transformer, but the general accuracy is\nless comparable. It indicates that the Softmax attention and efficient\nattention have their own distinctions but neither of them can simultaneously\nbalance the accuracy and efficiency well. This motivates us to mix the two\ntypes of attention to reduce the performance imbalance. Besides the search\nspaces that commonly used in existing NAS Transformer approaches, we propose a\nnew search space that allows the NAS algorithm to automatically search the\nattention variants along with architectures. Extensive experiments on WMT' 14\nEn-De and CIFAR-10 demonstrate that our searched architecture maintains\ncomparable accuracy to the standard Transformer with notably improved\ncomputational efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zexiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kaiyue Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weixuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PHEMEPlus: Enriching Social Media Rumour Verification with External Evidence. (arXiv:2207.13970v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13970","description":"<p>Work on social media rumour verification utilises signals from posts, their\npropagation and users involved. Other lines of work target identifying and\nfact-checking claims based on information from Wikipedia, or trustworthy news\narticles without considering social media context. However works combining the\ninformation from social media with external evidence from the wider web are\nlacking. To facilitate research in this direction, we release a novel dataset,\nPHEMEPlus, an extension of the PHEME benchmark, which contains social media\nconversations as well as relevant external evidence for each rumour. We\ndemonstrate the effectiveness of incorporating such evidence in improving\nrumour verification models. Additionally, as part of the evidence collection,\nwe evaluate various ways of query formulation to identify the most effective\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dougrez_Lewis_J/0/1/0/all/0/1\">John Dougrez-Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochkina_E/0/1/0/all/0/1\">Elena Kochkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arana_Catania_M/0/1/0/all/0/1\">M. Arana-Catania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1\">Maria Liakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowing Where and What: Unified Word Block Pretraining for Document Understanding. (arXiv:2207.13979v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13979","description":"<p>Due to the complex layouts of documents, it is challenging to extract\ninformation for documents. Most previous studies develop multimodal pre-trained\nmodels in a self-supervised way. In this paper, we focus on the embedding\nlearning of word blocks containing text and layout information, and propose\nUTel, a language model with Unified TExt and Layout pre-training. Specifically,\nwe propose two pre-training tasks: Surrounding Word Prediction (SWP) for the\nlayout learning, and Contrastive learning of Word Embeddings (CWE) for\nidentifying different word blocks. Moreover, we replace the commonly used 1D\nposition embedding with a 1D clipped relative position embedding. In this way,\nthe joint training of Masked Layout-Language Modeling (MLLM) and two newly\nproposed tasks enables the interaction between semantic and spatial features in\na unified way. Additionally, the proposed UTel can process arbitrary-length\nsequences by removing the 1D position embedding, while maintaining competitive\nperformance. Extensive experimental results show UTel learns better joint\nrepresentations and achieves superior performance than previous methods on\nvarious downstream tasks, though requiring no image modality. Code is available\nat \\url{https://github.com/taosong2019/UTel}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Song Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1\">Tiantian Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Canjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Can Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence to sequence pretraining for a less-resourced Slovenian language. (arXiv:2207.13988v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13988","description":"<p>Large pretrained language models have recently conquered the area of natural\nlanguage processing. As an alternative to predominant masked language modelling\nintroduced in BERT, the T5 model has introduced a more general training\nobjective, namely sequence to sequence transformation, which includes masked\nlanguage model but more naturally fits text generation tasks such as machine\ntranslation, summarization, open-domain question answering, text\nsimplification, dialogue systems, etc. The monolingual variants of T5 models\nhave been limited to well-resourced languages, while the massively multilingual\nT5 model supports 101 languages. In contrast, we trained two different sized\nT5-type sequence to sequence models for morphologically rich Slovene language\nwith much less resources and analyzed their behavior. Concerning classification\ntasks, the SloT5 models mostly lag behind the monolingual Slovene SloBERTa\nmodel but are to be considered for the generative tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ulcar_M/0/1/0/all/0/1\">Matej Ul&#x10d;ar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation. (arXiv:2207.14000v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14000","description":"<p>Combining deep learning with symbolic logic reasoning aims to capitalize on\nthe success of both fields and is drawing increasing attention. Inspired by\nDeepLogic, an end-to-end model trained to perform inference on logic programs,\nwe introduce IMA-GloVe-GA, an iterative neural inference network for multi-step\nreasoning expressed in natural language. In our model, reasoning is performed\nusing an iterative memory neural network based on RNN with a gate attention\nmechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES\nV1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention\ncan achieve higher test accuracy than DeepLogic and other RNN baseline models.\nOur model achieves better out-of-distribution generalisation than RoBERTa-Large\nwhen the rules have been shuffled. Furthermore, to address the issue of\nunbalanced distribution of reasoning depths in the current multi-step reasoning\ndatasets, we develop PARARULE-Plus, a large dataset with more examples that\nrequire deeper reasoning steps. Experimental results show that the addition of\nPARARULE-Plus can increase the model's performance on examples requiring deeper\nreasoning depths. The source code and data are available at\nhttps://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1\">Qiming Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1\">Alex Yuxuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartill_T/0/1/0/all/0/1\">Tim Hartill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_N/0/1/0/all/0/1\">Neset Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhenyun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1\">Michael Witbrock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiamou Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Raising Student Completion Rates with Adaptive Curriculum and Contextual Bandits. (arXiv:2207.14003v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14003","description":"<p>We present an adaptive learning Intelligent Tutoring System, which uses\nmodel-based reinforcement learning in the form of contextual bandits to assign\nlearning activities to students. The model is trained on the trajectories of\nthousands of students in order to maximize their exercise completion rates and\ncontinues to learn online, automatically adjusting itself to new activities. A\nrandomized controlled trial with students shows that our model leads to\nsuperior completion rates and significantly improved student engagement when\ncompared to other approaches. Our approach is fully-automated unlocking new\nopportunities for learning experience personalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belfer_R/0/1/0/all/0/1\">Robert Belfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochmar_E/0/1/0/all/0/1\">Ekaterina Kochmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serban_I/0/1/0/all/0/1\">Iulian Vlad Serban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CubeMLP: A MLP-based Model for Multimodal Sentiment Analysis and Depression Estimation. (arXiv:2207.14087v1 [cs.MM])","link":"http://arxiv.org/abs/2207.14087","description":"<p>Multimodal sentiment analysis and depression estimation are two important\nresearch topics that aim to predict human mental states using multimodal data.\nPrevious research has focused on developing effective fusion strategies for\nexchanging and integrating mind-related information from different modalities.\nSome MLP-based techniques have recently achieved considerable success in a\nvariety of computer vision tasks. Inspired by this, we explore multimodal\napproaches with a feature-mixing perspective in this study. To this end, we\nintroduce CubeMLP, a multimodal feature processing framework based entirely on\nMLP. CubeMLP consists of three independent MLP units, each of which has two\naffine transformations. CubeMLP accepts all relevant modality features as input\nand mixes them across three axes. After extracting the characteristics using\nCubeMLP, the mixed multimodal features are flattened for task predictions. Our\nexperiments are conducted on sentiment analysis datasets: CMU-MOSI and\nCMU-MOSEI, and depression estimation dataset: AVEC2019. The results show that\nCubeMLP can achieve state-of-the-art performance with a much lower computing\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Type Prediction Leveraging Graph Walks and Entity Descriptions. (arXiv:2207.14094v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14094","description":"<p>The entity type information in Knowledge Graphs (KGs) such as DBpedia,\nFreebase, etc. is often incomplete due to automated generation or human\ncuration. Entity typing is the task of assigning or inferring the semantic type\nof an entity in a KG. This paper presents \\textit{GRAND}, a novel approach for\nentity typing leveraging different graph walk strategies in RDF2vec together\nwith textual entity descriptions. RDF2vec first generates graph walks and then\nuses a language model to obtain embeddings for each node in the graph. This\nstudy shows that the walk generation strategy and the embedding model have a\nsignificant effect on the performance of the entity typing task. The proposed\napproach outperforms the baseline approaches on the benchmark datasets DBpedia\nand FIGER for entity typing in KGs for both fine-grained and coarse-grained\nclasses. The results show that the combination of order-aware RDF2vec variants\ntogether with the contextual embeddings of the textual entity descriptions\nachieve the best results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_R/0/1/0/all/0/1\">Russa Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portisch_J/0/1/0/all/0/1\">Jan Portisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sack_H/0/1/0/all/0/1\">Harald Sack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mehwish Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14116","description":"<p>We present Claim-Dissector: a novel latent variable model for fact-checking\nand fact-analysis, which given a claim and a set of retrieved provenances\nallows learning jointly: (i) what are the relevant provenances to this claim\n(ii) what is the veracity of this claim. We propose to disentangle the\nper-provenance relevance probability and its contribution to the final veracity\nprobability in an interpretable way - the final veracity probability is\nproportional to a linear ensemble of per-provenance relevance probabilities.\nThis way, it can be clearly identified the relevance of which sources\ncontributes to what extent towards the final probability. We show that our\nsystem achieves state-of-the-art results on FEVER dataset comparable to\ntwo-stage systems typically used in traditional fact-checking pipelines, while\nit often uses significantly less parameters and computation.\n</p>\n<p>Our analysis shows that proposed approach further allows to learn not just\nwhich provenances are relevant, but also which provenances lead to supporting\nand which toward denying the claim, without direct supervision. This not only\nadds interpretability, but also allows to detect claims with conflicting\nevidence automatically. Furthermore, we study whether our model can learn\nfine-grained relevance cues while using coarse-grained supervision. We show\nthat our model can achieve competitive sentence-recall while using only\nparagraph-level relevance supervision. Finally, traversing towards the finest\ngranularity of relevance, we show that our framework is capable of identifying\nrelevance at the token-level. To do this, we present a new benchmark focusing\non token-level interpretability - humans annotate tokens in relevant\nprovenances they considered essential when making their judgement. Then we\nmeasure how similar are these annotations to tokens our model is focusing on.\nOur code, and dataset will be released online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Causal Effects of Data Statistics on Language Model's `Factual' Predictions. (arXiv:2207.14251v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14251","description":"<p>Large amounts of training data are one of the major reasons for the high\nperformance of state-of-the-art NLP models. But what exactly in the training\ndata causes a model to make a certain prediction? We seek to answer this\nquestion by providing a language for describing how training data influences\npredictions, through a causal framework. Importantly, our framework bypasses\nthe need to retrain expensive models and allows us to estimate causal effects\nbased on observational data alone. Addressing the problem of extracting factual\nknowledge from pretrained language models (PLMs), we focus on simple data\nstatistics such as co-occurrence counts and show that these statistics do\ninfluence the predictions of PLMs, suggesting that such models rely on shallow\nheuristics. Our causal framework and our results demonstrate the importance of\nstudying datasets and the benefits of causality for understanding NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kassner_N/0/1/0/all/0/1\">Nora Kassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichander_A/0/1/0/all/0/1\">Abhilasha Ravichander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1\">Marius Mosbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Training of Language Models to Fill in the Middle. (arXiv:2207.14255v1 [cs.CL])","link":"http://arxiv.org/abs/2207.14255","description":"<p>We show that autoregressive language models can learn to infill text after we\napply a straightforward transformation to the dataset, which simply moves a\nspan of text from the middle of a document to its end. While this data\naugmentation has garnered much interest in recent years, we provide extensive\nevidence that training models with a large fraction of data transformed in this\nway does not harm the original left-to-right generative capability, as measured\nby perplexity and sampling evaluations across a wide range of scales. Given the\nusefulness, simplicity, and efficiency of training models to fill-in-the-middle\n(FIM), we suggest that future autoregressive language models be trained with\nFIM by default. To this end, we run a series of ablations on key\nhyperparameters, such as the data transformation frequency, the structure of\nthe transformation, and the method of selecting the infill span. We use these\nablations to prescribe strong default settings and best practices to train FIM\nmodels. We have released our best infilling model trained with best practices\nin our API, and release our infilling benchmarks to aid future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bavarian_M/0/1/0/all/0/1\">Mohammad Bavarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_H/0/1/0/all/0/1\">Heewoo Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tezak_N/0/1/0/all/0/1\">Nikolas Tezak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLeavey_C/0/1/0/all/0/1\">Christine McLeavey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tworek_J/0/1/0/all/0/1\">Jerry Tworek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mark Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings. (arXiv:2109.08449v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08449","description":"<p>Large pretrained language models (PreLMs) are revolutionizing natural\nlanguage processing across all benchmarks. However, their sheer size is\nprohibitive for small laboratories or for deployment on mobile devices.\nApproaches like pruning and distillation reduce the model size but typically\nretain the same model architecture. In contrast, we explore distilling PreLMs\ninto a different, more efficient architecture, Continual Multiplication of\nWords (CMOW), which embeds each word as a matrix and uses matrix multiplication\nto encode sequences. We extend the CMOW architecture and its CMOW/CBOW-Hybrid\nvariant with a bidirectional component for more expressive power, per-token\nrepresentations for a general (task-agnostic) distillation during pretraining,\nand a two-sequence encoding scheme that facilitates downstream tasks on\nsentence pairs, such as sentence similarity and natural language inference. Our\nmatrix-based bidirectional CMOW/CBOW-Hybrid model is competitive to DistilBERT\non question similarity and recognizing textual entailment, but uses only half\nof the number of parameters and is three times faster in terms of inference\nspeed. We match or exceed the scores of ELMo for all tasks of the GLUE\nbenchmark except for the sentiment analysis task SST-2 and the linguistic\nacceptability task CoLA. However, compared to previous cross-architecture\ndistillation approaches, we demonstrate a doubling of the scores on detecting\nlinguistic acceptability. This shows that matrix-based embeddings can be used\nto distill large PreLM into competitive models and motivates further research\nin this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuber_I/0/1/0/all/0/1\">Isabelle Cuber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1\">Christoph Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nolscher_H/0/1/0/all/0/1\">Henrik Ferdinand N&#xf6;lscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonderecker_A/0/1/0/all/0/1\">Angelina Sonderecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Entity Tagging with Multimodal Knowledge Base. (arXiv:2201.00693v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2201.00693","description":"<p>To enhance research on multimodal knowledge base and multimodal information\nprocessing, we propose a new task called multimodal entity tagging (MET) with a\nmultimodal knowledge base (MKB). We also develop a dataset for the problem\nusing an existing MKB. In an MKB, there are entities and their associated texts\nand images. In MET, given a text-image pair, one uses the information in the\nMKB to automatically identify the related entity in the text-image pair. We\nsolve the task by using the information retrieval paradigm and implement\nseveral baselines using state-of-the-art methods in NLP and CV. We conduct\nextensive experiments and make analyses on the experimental results. The\nresults show that the task is challenging, but current technologies can achieve\nrelatively high performance. We will release the dataset, code, and models for\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1\">Chao Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Klexikon: A German Dataset for Joint Summarization and Simplification. (arXiv:2201.07198v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.07198","description":"<p>Traditionally, Text Simplification is treated as a monolingual translation\ntask where sentences between source texts and their simplified counterparts are\naligned for training. However, especially for longer input documents,\nsummarizing the text (or dropping less relevant content altogether) plays an\nimportant role in the simplification process, which is currently not reflected\nin existing datasets. Simultaneously, resources for non-English languages are\nscarce in general and prohibitive for training new solutions. To tackle this\nproblem, we pose core requirements for a system that can jointly summarize and\nsimplify long source documents. We further describe the creation of a new\ndataset for joint Text Simplification and Summarization based on German\nWikipedia and the German children's lexicon \"Klexikon\", consisting of almost\n2900 documents. We release a document-aligned version that particularly\nhighlights the summarization aspect, and provide statistical evidence that this\nresource is well suited to simplification as well. Code and data are available\non Github: https://github.com/dennlinger/klexikon\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aumiller_D/0/1/0/all/0/1\">Dennis Aumiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gertz_M/0/1/0/all/0/1\">Michael Gertz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Cascades. (arXiv:2207.10342v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.10342","description":"<p>Prompted models have demonstrated impressive few-shot learning abilities.\nRepeated interactions at test-time with a single model, or the composition of\nmultiple models together, further expands capabilities. These compositions are\nprobabilistic models, and may be expressed in the language of graphical models\nwith random variables whose values are complex data types such as strings.\nCases with control flow and dynamic structure require techniques from\nprobabilistic programming, which allow implementing disparate model structures\nand inference strategies in a unified language. We formalize several existing\ntechniques from this perspective, including scratchpads / chain of thought,\nverifiers, STaR, selection-inference, and tool use. We refer to the resulting\nprograms as language model cascades.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Winnie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewkowycz_A/0/1/0/all/0/1\">Aitor Lewkowycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieber_D/0/1/0/all/0/1\">David Bieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopes_R/0/1/0/all/0/1\">Raphael Gontijo Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saurous_R/0/1/0/all/0/1\">Rif A. Saurous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohl_dickstein_J/0/1/0/all/0/1\">Jascha Sohl-dickstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1\">Kevin Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\mu\\text{KG}$: A Library for Multi-source Knowledge Graph Embeddings and Applications. (arXiv:2207.11442v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.11442","description":"<p>This paper presents $\\mu\\text{KG}$, an open-source Python library for\nrepresentation learning over knowledge graphs. $\\mu\\text{KG}$ supports joint\nrepresentation learning over multi-source knowledge graphs (and also a single\nknowledge graph), multiple deep learning libraries (PyTorch and TensorFlow2),\nmultiple embedding tasks (link prediction, entity alignment, entity typing, and\nmulti-source link prediction), and multiple parallel computing modes\n(multi-process and multi-GPU computing). It currently implements 26 popular\nknowledge graph embedding models and supports 16 benchmark datasets.\n$\\mu\\text{KG}$ provides advanced implementations of embedding techniques with\nsimplified pipelines of different tasks. It also comes with high-quality\ndocumentation for ease of use. $\\mu\\text{KG}$ is more comprehensive than\nexisting knowledge graph embedding libraries. It is useful for a thorough\ncomparison and analysis of various embedding models and tasks. We show that the\njointly learned embeddings can greatly help knowledge-powered downstream tasks,\nsuch as multi-hop knowledge graph question answering. We will stay abreast of\nthe latest developments in the related fields and incorporate them into\n$\\mu\\text{KG}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xindi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zequn Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.13243","description":"<p>The last decade of machine learning has seen drastic increases in scale and\ncapabilities, and deep neural networks (DNNs) are increasingly being deployed\nacross a wide range of domains. However, the inner workings of DNNs are\ngenerally difficult to understand, raising concerns about the safety of using\nthese systems without a rigorous understanding of how they function. In this\nsurvey, we review literature on techniques for interpreting the inner\ncomponents of DNNs, which we call \"inner\" interpretability methods.\nSpecifically, we review methods for interpreting weights, neurons, subnetworks,\nand latent representations with a focus on how these techniques relate to the\ngoal of designing safer, more trustworthy AI systems. We also highlight\nconnections between interpretability and work in modularity, adversarial\nrobustness, continual learning, network compression, and studying the human\nvisual system. Finally, we discuss key challenges and argue for future work in\ninterpretability for AI safety that focuses on diagnostics, benchmarking, and\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rauker_T/0/1/0/all/0/1\">Tilman R&#xe4;uker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_A/0/1/0/all/0/1\">Anson Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Break and Make: Interactive Structural Understanding Using LEGO Bricks. (arXiv:2207.13738v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13738","description":"<p>Visual understanding of geometric structures with complex spatial\nrelationships is a fundamental component of human intelligence. As children, we\nlearn how to reason about structure not only from observation, but also by\ninteracting with the world around us -- by taking things apart and putting them\nback together again. The ability to reason about structure and compositionality\nallows us to not only build things, but also understand and reverse-engineer\ncomplex systems. In order to advance research in interactive reasoning for\npart-based geometric understanding, we propose a challenging new assembly\nproblem using LEGO bricks that we call Break and Make. In this problem an agent\nis given a LEGO model and attempts to understand its structure by interactively\ninspecting and disassembling it. After this inspection period, the agent must\nthen prove its understanding by rebuilding the model from scratch using\nlow-level action primitives. In order to facilitate research on this problem we\nhave built LTRON, a fully interactive 3D simulator that allows learning agents\nto assemble, disassemble and manipulate LEGO models. We pair this simulator\nwith a new dataset of fan-made LEGO creations that have been uploaded to the\ninternet in order to provide complex scenes containing over a thousand unique\nbrick shapes. We take a first step towards solving this problem using\nsequence-to-sequence models that provide guidance for how to make progress on\nthis challenging problem. Our simulator and data are available at\ngithub.com/aaronwalsman/ltron. Additional training code and PyTorch examples\nare available at github.com/aaronwalsman/ltron-torch-eccv22.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walsman_A/0/1/0/all/0/1\">Aaron Walsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Muru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1\">Klemen Kotar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1\">Karthik Desingh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lighting (In)consistency of Paint by Text. (arXiv:2207.13744v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13744","description":"<p>Whereas generative adversarial networks are capable of synthesizing highly\nrealistic images of faces, cats, landscapes, or almost any other single\ncategory, paint-by-text synthesis engines can -- from a single text prompt --\nsynthesize realistic images of seemingly endless categories with arbitrary\nconfigurations and combinations. This powerful technology poses new challenges\nto the photo-forensic community. Motivated by the fact that paint by text is\nnot based on explicit geometric or physical models, and the human visual\nsystem's general insensitivity to lighting inconsistencies, we provide an\ninitial exploration of the lighting consistency of DALL-E-2 synthesized images\nto determine if physics-based forensic analyses will prove fruitful in\ndetecting this new breed of synthetic media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farid_H/0/1/0/all/0/1\">Hany Farid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAUDI: A Neural Architect for Immersive 3D Scene Generation. (arXiv:2207.13751v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13751","description":"<p>We introduce GAUDI, a generative model capable of capturing the distribution\nof complex and realistic 3D scenes that can be rendered immersively from a\nmoving camera. We tackle this challenging problem with a scalable yet powerful\napproach, where we first optimize a latent representation that disentangles\nradiance fields and camera poses. This latent representation is then used to\nlearn a generative model that enables both unconditional and conditional\ngeneration of 3D scenes. Our model generalizes previous works that focus on\nsingle objects by removing the assumption that the camera pose distribution can\nbe shared across samples. We show that GAUDI obtains state-of-the-art\nperformance in the unconditional generative setting across multiple datasets\nand allows for conditional generation of 3D scenes given conditioning variables\nlike sparse image observations or text that describes the scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bautista_M/0/1/0/all/0/1\">Miguel Angel Bautista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengsheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abnar_S/0/1/0/all/0/1\">Samira Abnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbott_W/0/1/0/all/0/1\">Walter Talbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1\">Alexander Toshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_L/0/1/0/all/0/1\">Laurent Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shuangfei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1\">Hanlin Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulbricht_D/0/1/0/all/0/1\">Daniel Ulbricht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghan_A/0/1/0/all/0/1\">Afshin Dehghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Josh Susskind</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Classification of Thyroid Nodules on Ultrasound: Validation on an Independent Dataset. (arXiv:2207.13765v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13765","description":"<p>Objectives: The purpose is to apply a previously validated deep learning\nalgorithm to a new thyroid nodule ultrasound image dataset and compare its\nperformances with radiologists. Methods: Prior study presented an algorithm\nwhich is able to detect thyroid nodules and then make malignancy\nclassifications with two ultrasound images. A multi-task deep convolutional\nneural network was trained from 1278 nodules and originally tested with 99\nseparate nodules. The results were comparable with that of radiologists. The\nalgorithm was further tested with 378 nodules imaged with ultrasound machines\nfrom different manufacturers and product types than the training cases. Four\nexperienced radiologists were requested to evaluate the nodules for comparison\nwith deep learning. Results: The Area Under Curve (AUC) of the deep learning\nalgorithm and four radiologists were calculated with parametric, binormal\nestimation. For the deep learning algorithm, the AUC was 0.70 (95% CI: 0.64 -\n0.75). The AUC of radiologists were 0.66 (95% CI: 0.61 - 0.71), 0.67 (95%\nCI:0.62 - 0.73), 0.68 (95% CI: 0.63 - 0.73), and 0.66 (95%CI: 0.61 - 0.71).\nConclusion: In the new testing dataset, the deep learning algorithm achieved\nsimilar performances with all four radiologists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Weng_J/0/1/0/all/0/1\">Jingxi Weng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wildman_Tobriner_B/0/1/0/all/0/1\">Benjamin Wildman-Tobriner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buda_M/0/1/0/all/0/1\">Mateusz Buda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jichen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ho_L/0/1/0/all/0/1\">Lisa M. Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Allen_B/0/1/0/all/0/1\">Brian C. Allen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ehieli_W/0/1/0/all/0/1\">Wendy L. Ehieli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miller_C/0/1/0/all/0/1\">Chad M. Miller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jikai Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazurowski_M/0/1/0/all/0/1\">Maciej A. Mazurowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion Sensing. (arXiv:2207.13784v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13784","description":"<p>Today's Mixed Reality head-mounted displays track the user's head pose in\nworld space as well as the user's hands for interaction in both Augmented\nReality and Virtual Reality scenarios. While this is adequate to support user\ninput, it unfortunately limits users' virtual representations to just their\nupper bodies. Current systems thus resort to floating avatars, whose limitation\nis particularly evident in collaborative settings. To estimate full-body poses\nfrom the sparse input sources, prior work has incorporated additional trackers\nand sensors at the pelvis or lower body, which increases setup complexity and\nlimits practical application in mobile settings. In this paper, we present\nAvatarPoser, the first learning-based method that predicts full-body poses in\nworld coordinates using only motion input from the user's head and hands. Our\nmethod builds on a Transformer encoder to extract deep features from the input\nsignals and decouples global motion from the learned local joint orientations\nto guide pose estimation. To obtain accurate full-body motions that resemble\nmotion capture animations, we refine the arm joints' positions using an\noptimization routine with inverse kinematics to match the original tracking\ninput. In our evaluation, AvatarPoser achieved new state-of-the-art results in\nevaluations on large motion capture datasets (AMASS). At the same time, our\nmethod's inference speed supports real-time operation, providing a practical\ninterface to support holistic avatar control and representation for Metaverse\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiaxi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Streli_P/0/1/0/all/0/1\">Paul Streli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Huajian Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fender_A/0/1/0/all/0/1\">Andreas Fender</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laich_L/0/1/0/all/0/1\">Larissa Laich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snape_P/0/1/0/all/0/1\">Patrick Snape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holz_C/0/1/0/all/0/1\">Christian Holz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Assess Danger from Movies for Cooperative Escape Planning in Hazardous Environments. (arXiv:2207.13791v1 [cs.RO])","link":"http://arxiv.org/abs/2207.13791","description":"<p>There has been a plethora of work towards improving robot perception and\nnavigation, yet their application in hazardous environments, like during a fire\nor an earthquake, is still at a nascent stage. We hypothesize two key\nchallenges here: first, it is difficult to replicate such scenarios in the real\nworld, which is necessary for training and testing purposes. Second, current\nsystems are not fully able to take advantage of the rich multi-modal data\navailable in such hazardous environments. To address the first challenge, we\npropose to harness the enormous amount of visual content available in the form\nof movies and TV shows, and develop a dataset that can represent hazardous\nenvironments encountered in the real world. The data is annotated with\nhigh-level danger ratings for realistic disaster images, and corresponding\nkeywords are provided that summarize the content of the scene. In response to\nthe second challenge, we propose a multi-modal danger estimation pipeline for\ncollaborative human-robot escape scenarios. Our Bayesian framework improves\ndanger estimation by fusing information from robot's camera sensor and language\ninputs from the human. Furthermore, we augment the estimation module with a\nrisk-aware planner that helps in identifying safer paths out of the dangerous\nenvironment. Through extensive simulations, we exhibit the advantages of our\nmulti-modal perception framework that gets translated into tangible benefits\nsuch as higher success rate in a collaborative human-robot mission.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shree_V/0/1/0/all/0/1\">Vikram Shree</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_S/0/1/0/all/0/1\">Sarah Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asfora_B/0/1/0/all/0/1\">Beatriz Asfora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banfi_J/0/1/0/all/0/1\">Jacopo Banfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1\">Mark Campbell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look at Adjacent Frames: Video Anomaly Detection without Offline Training. (arXiv:2207.13798v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13798","description":"<p>We propose a solution to detect anomalous events in videos without the need\nto train a model offline. Specifically, our solution is based on a\nrandomly-initialized multilayer perceptron that is optimized online to\nreconstruct video frames, pixel-by-pixel, from their frequency information.\nBased on the information shifts between adjacent frames, an incremental learner\nis used to update parameters of the multilayer perceptron after observing each\nframe, thus allowing to detect anomalous events along the video stream.\nTraditional solutions that require no offline training are limited to operating\non videos with only a few abnormal frames. Our solution breaks this limit and\nachieves strong performance on benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yuqi Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1\">Guodong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1\">Victor Sanchez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose-NDF: Modeling Human Pose Manifolds with Neural Distance Fields. (arXiv:2207.13807v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13807","description":"<p>We present Pose-NDF, a continuous model for plausible human poses based on\nneural distance fields (NDFs). Pose or motion priors are important for\ngenerating realistic new poses and for reconstructing accurate poses from noisy\nor partial observations. Pose-NDF learns a manifold of plausible poses as the\nzero level set of a neural implicit function, extending the idea of modeling\nimplicit surfaces in 3D to the high-dimensional domain SO(3)^K, where a human\npose is defined by a single data point, represented by K quaternions. The\nresulting high-dimensional implicit function can be differentiated with respect\nto the input poses and thus can be used to project arbitrary poses onto the\nmanifold by using gradient descent on the set of 3-dimensional hyperspheres. In\ncontrast to previous VAE-based human pose priors, which transform the pose\nspace into a Gaussian distribution, we model the actual pose manifold,\npreserving the distances between poses. We demonstrate that PoseNDF outperforms\nexisting state-of-the-art methods as a prior in various downstream tasks,\nranging from denoising real-world human mocap data, pose recovery from occluded\ndata to 3D pose reconstruction from images. Furthermore, we show that it can be\nused to generate more diverse poses by random sampling and projection than\nVAE-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1\">Garvita Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antic_D/0/1/0/all/0/1\">Dimitrije Antic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenssen_J/0/1/0/all/0/1\">Jan Eric Lenssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarafianos_N/0/1/0/all/0/1\">Nikolaos Sarafianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers. (arXiv:2207.13820v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13820","description":"<p>Transformer encoder architectures have recently achieved state-of-the-art\nresults on monocular 3D human mesh reconstruction, but they require a\nsubstantial number of parameters and expensive computations. Due to the large\nmemory overhead and slow inference speed, it is difficult to deploy such models\nfor practical use. In this paper, we propose a novel transformer\nencoder-decoder architecture for 3D human mesh reconstruction from a single\nimage, called FastMETRO. We identify the performance bottleneck in the\nencoder-based transformers is caused by the token design which introduces high\ncomplexity interactions among input tokens. We disentangle the interactions via\nan encoder-decoder architecture, which allows our model to demand much fewer\nparameters and shorter inference time. In addition, we impose the prior\nknowledge of human body's morphological relationship via attention masking and\nmesh upsampling operations, which leads to faster convergence with higher\naccuracy. Our FastMETRO improves the Pareto-front of accuracy and efficiency,\nand clearly outperforms image-based methods on Human3.6M and 3DPW. Furthermore,\nwe validate its generalizability on FreiHAND.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junhyeong Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youwang_K/0/1/0/all/0/1\">Kim Youwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-Morphomics, Morphological Features on CT scans for lung nodule malignancy diagnosis. (arXiv:2207.13830v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13830","description":"<p>Pathologies systematically induce morphological changes, thus providing a\nmajor but yet insufficiently quantified source of observables for diagnosis.\nThe study develops a predictive model of the pathological states based on\nmorphological features (3D-morphomics) on Computed Tomography (CT) volumes. A\ncomplete workflow for mesh extraction and simplification of an organ's surface\nis developed, and coupled with an automatic extraction of morphological\nfeatures given by the distribution of mean curvature and mesh energy. An\nXGBoost supervised classifier is then trained and tested on the 3D-morphomics\nto predict the pathological states. This framework is applied to the prediction\nof the malignancy of lung's nodules. On a subset of NLST database with\nmalignancy confirmed biopsy, using 3D-morphomics only, the classification model\nof lung nodules into malignant vs. benign achieves 0.964 of AUC. Three other\nsets of classical features are trained and tested, (1) clinical relevant\nfeatures gives an AUC of 0.58, (2) 111 radiomics gives an AUC of 0.976, (3)\nradiologist ground truth (GT) containing the nodule size, attenuation and\nspiculation qualitative annotations gives an AUC of 0.979. We also test the\nBrock model and obtain an AUC of 0.826. Combining 3D-morphomics and radiomics\nfeatures achieves state-of-the-art results with an AUC of 0.978 where the\n3D-morphomics have some of the highest predictive powers. As a validation on a\npublic independent cohort, models are applied to the LIDC dataset, the\n3D-morphomics achieves an AUC of 0.906 and the 3D-morphomics+radiomics achieves\nan AUC of 0.958, which ranks second in the challenge among deep models. It\nestablishes the curvature distributions as efficient features for predicting\nlung nodule malignancy and a new method that can be applied directly to\narbitrary computer aided diagnosis task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Munoz_E/0/1/0/all/0/1\">Elias Munoz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baudot_P/0/1/0/all/0/1\">Pierre Baudot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Le_V/0/1/0/all/0/1\">Van-Khoa Le</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Voyton_C/0/1/0/all/0/1\">Charles Voyton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Renoust_B/0/1/0/all/0/1\">Benjamin Renoust</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Francis_D/0/1/0/all/0/1\">Danny Francis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Groza_V/0/1/0/all/0/1\">Vladimir Groza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brisset_J/0/1/0/all/0/1\">Jean-Christophe Brisset</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geremia_E/0/1/0/all/0/1\">Ezequiel Geremia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iannessi_A/0/1/0/all/0/1\">Antoine Iannessi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huet_B/0/1/0/all/0/1\">Benoit Huet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extraction of Coronary Vessels in Fluoroscopic X-Ray Sequences Using Vessel Correspondence Optimization. (arXiv:2207.13837v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13837","description":"<p>We present a method to extract coronary vessels from fluoroscopic x-ray\nsequences. Given the vessel structure for the source frame, vessel\ncorrespondence candidates in the subsequent frame are generated by a novel\nhierarchical search scheme to overcome the aperture problem. Optimal\ncorrespondences are determined within a Markov random field optimization\nframework. Post-processing is performed to extract vessel branches newly\nvisible due to the inflow of contrast agent. Quantitative and qualitative\nevaluation conducted on a dataset of 18 sequences demonstrates the\neffectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shin_S/0/1/0/all/0/1\">Seung Yeon Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Soochahn Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noh_K/0/1/0/all/0/1\">Kyoung Jin Noh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yun_I/0/1/0/all/0/1\">Il Dong Yun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EEG2Mel: Reconstructing Sound from Brain Responses to Music. (arXiv:2207.13845v1 [cs.SD])","link":"http://arxiv.org/abs/2207.13845","description":"<p>Information retrieval from brain responses to auditory and visual stimuli has\nshown success through classification of song names and image classes presented\nto participants while recording EEG signals. Information retrieval in the form\nof reconstructing auditory stimuli has also shown some success, but here we\nimprove on previous methods by reconstructing music stimuli well enough to be\nperceived and identified independently. Furthermore, deep learning models were\ntrained on time-aligned music stimuli spectrum for each corresponding\none-second window of EEG recording, which greatly reduces feature extraction\nsteps needed when compared to prior studies. The NMED-Tempo and NMED-Hindi\ndatasets of participants passively listening to full length songs were used to\ntrain and validate Convolutional Neural Network (CNN) regressors. The efficacy\nof raw voltage versus power spectrum inputs and linear versus mel spectrogram\noutputs were tested, and all inputs and outputs were converted into 2D images.\nThe quality of reconstructed spectrograms was assessed by training classifiers\nwhich showed 81% accuracy for mel-spectrograms and 72% for linear spectrograms\n(10% chance accuracy). Lastly, reconstructions of auditory music stimuli were\ndiscriminated by listeners at an 85% success rate (50% chance) in a\ntwo-alternative match-to-sample task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_Aristizabal_A/0/1/0/all/0/1\">Adolfo G. Ramirez-Aristizabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kello_C/0/1/0/all/0/1\">Chris Kello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DnSwin: Toward Real-World Denoising via Continuous Wavelet Sliding-Transformer. (arXiv:2207.13861v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13861","description":"<p>Real-world image denoising is a practical image restoration problem that aims\nto obtain clean images from in-the-wild noisy input. Recently, Vision\nTransformer (ViT) exhibits a strong ability to capture long-range dependencies\nand many researchers attempt to apply ViT to image denoising tasks. However,\nreal-world image is an isolated frame that makes the ViT build the long-range\ndependencies on the internal patches, which divides images into patches and\ndisarranges the noise pattern and gradient continuity. In this article, we\npropose to resolve this issue by using a continuous Wavelet Sliding-Transformer\nthat builds frequency correspondence under real-world scenes, called DnSwin.\nSpecifically, we first extract the bottom features from noisy input images by\nusing a CNN encoder. The key to DnSwin is to separate high-frequency and\nlow-frequency information from the features and build frequency dependencies.\nTo this end, we propose Wavelet Sliding-Window Transformer that utilizes\ndiscrete wavelet transform, self-attention and inverse discrete wavelet\ntransform to extract deep features. Finally, we reconstruct the deep features\ninto denoised images using a CNN decoder. Both quantitative and qualitative\nevaluations on real-world denoising benchmarks demonstrate that the proposed\nDnSwin performs favorably against the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhijing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xiaobin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziying Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yukai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinshan Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MKANet: A Lightweight Network with Sobel Boundary Loss for Efficient Land-cover Classification of Satellite Remote Sensing Imagery. (arXiv:2207.13866v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13866","description":"<p>Land cover classification is a multi-class segmentation task to classify each\npixel into a certain natural or man-made category of the earth surface, such as\nwater, soil, natural vegetation, crops, and human infrastructure. Limited by\nhardware computational resources and memory capacity, most existing studies\npreprocessed original remote sensing images by down sampling or cropping them\ninto small patches less than 512*512 pixels before sending them to a deep\nneural network. However, down sampling images incurs spatial detail loss,\nrenders small segments hard to discriminate, and reverses the spatial\nresolution progress obtained by decades of years of efforts. Cropping images\ninto small patches causes a loss of long-range context information, and\nrestoring the predicted results to their original size brings extra latency. In\nresponse to the above weaknesses, we present an efficient lightweight semantic\nsegmentation network termed MKANet. Aimed at the characteristics of top view\nhigh-resolution remote sensing imagery, MKANet utilizes sharing kernels to\nsimultaneously and equally handle ground segments of inconsistent scales, and\nalso employs parallel and shallow architecture to boost inference speed and\nfriendly support image patches more than 10X larger. To enhance boundary and\nsmall segments discrimination, we also propose a method that captures category\nimpurity areas, exploits boundary information and exerts an extra penalty on\nboundaries and small segment misjudgment. Both visual interpretations and\nquantitative metrics of extensive experiments demonstrate that MKANet acquires\nstate-of-the-art accuracy on two land-cover classification datasets and infers\n2X faster than other competitive lightweight networks. All these merits\nhighlight the potential of MKANet in practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jinshan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guangqi Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Steganography Network. (arXiv:2207.13867v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13867","description":"<p>Steganography usually modifies cover media to embed secret data. A new\nsteganographic approach called generative steganography (GS) has emerged\nrecently, in which stego images (images containing secret data) are generated\nfrom secret data directly without cover media. However, existing GS schemes are\noften criticized for their poor performances. In this paper, we propose an\nadvanced generative steganography network (GSN) that can generate realistic\nstego images without using cover images, in which mutual information is firstly\nintroduced in stego image generation. Our model contains four sub-networks,\ni.e., an image generator ($G$), a discriminator ($D$), a steganalyzer ($S$),\nand a data extractor ($E$). $D$ and $S$ act as two adversarial discriminators\nto ensure the visual and statistical imperceptibility of generated stego\nimages. $E$ is to extract the hidden secret from generated stego images. The\ngenerator $G$ is flexibly constructed to synthesize either cover or stego\nimages with different inputs. It facilitates covert communication by hiding the\nfunction of generating stego images in a normal image generator. A module named\nsecret block is designed delicately to conceal secret data in the feature maps\nduring image generation, with which high hiding capacity and image fidelity are\nachieved. In addition, a novel hierarchical gradient decay skill is developed\nto resist steganalysis detection. Experiments demonstrate the superiority of\nour work over existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Ping Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Ge Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qing Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extraction of Vascular Wall in Carotid Ultrasound via a Novel Boundary-Delineation Network. (arXiv:2207.13868v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13868","description":"<p>Ultrasound imaging plays an important role in the diagnosis of vascular\nlesions. Accurate segmentation of the vascular wall is important for the\nprevention, diagnosis and treatment of vascular diseases. However, existing\nmethods have inaccurate localization of the vascular wall boundary.\nSegmentation errors occur in discontinuous vascular wall boundaries and dark\nboundaries. To overcome these problems, we propose a new boundary-delineation\nnetwork (BDNet). We use the boundary refinement module to re-delineate the\nboundary of the vascular wall to obtain the correct boundary location. We\ndesigned the feature extraction module to extract and fuse multi-scale features\nand different receptive field features to solve the problem of dark boundaries\nand discontinuous boundaries. We use a new loss function to optimize the model.\nThe interference of class imbalance on model optimization is prevented to\nobtain finer and smoother boundaries. Finally, to facilitate clinical\napplications, we design the model to be lightweight. Experimental results show\nthat our model achieves the best segmentation results and significantly reduces\nmemory consumption compared to existing models for the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1\">Qinghua Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_L/0/1/0/all/0/1\">Lizhi Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_G/0/1/0/all/0/1\">Guanqing Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chunying Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Repulsive Force Unit for Garment Collision Handling in Neural Networks. (arXiv:2207.13871v1 [cs.GR])","link":"http://arxiv.org/abs/2207.13871","description":"<p>Despite recent success, deep learning-based methods for predicting 3D garment\ndeformation under body motion suffer from interpenetration problems between the\ngarment and the body. To address this problem, we propose a novel collision\nhandling neural network layer called Repulsive Force Unit (ReFU). Based on the\nsigned distance function (SDF) of the underlying body and the current garment\nvertex positions, ReFU predicts the per-vertex offsets that push any\ninterpenetrating vertex to a collision-free configuration while preserving the\nfine geometric details. We show that ReFU is differentiable with trainable\nparameters and can be integrated into different network backbones that predict\n3D garment deformations. Our experiments show that ReFU significantly reduces\nthe number of collisions between the body and the garment and better preserves\ngeometric details compared to prior methods based on collision loss or\npost-processing optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1\">Qingyang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tuanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1\">Duygu Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real Image Restoration via Structure-preserving Complementarity Attention. (arXiv:2207.13879v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13879","description":"<p>Since convolutional neural networks perform well in learning generalizable\nimage priors from large-scale data, these models have been widely used in image\ndenoising tasks. However, the computational complexity increases dramatically\nas well on complex model. In this paper, We propose a novel lightweight\nComplementary Attention Module, which includes a density module and a sparse\nmodule, which can cooperatively mine dense and sparse features for feature\ncomplementary learning to build an efficient lightweight architecture.\nMoreover, to reduce the loss of details caused by denoising, this paper\nconstructs a gradient-based structure-preserving branch. We utilize\ngradient-based branches to obtain additional structural priors for denoising,\nand make the model pay more attention to image geometric details through\ngradient loss optimization.Based on the above, we propose an efficiently Unet\nstructured network with dual branch, the visual results show that can\neffectively preserve the structural details of the original image, we evaluate\nbenchmarks including SIDD and DND, where SCANet achieves state-of-the-art\nperformance in PSNR and SSIM while significantly reducing computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanfan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Gen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuperVessel: Segmenting High-resolution Vessel from Low-resolution Retinal Image. (arXiv:2207.13882v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13882","description":"<p>Vascular segmentation extracts blood vessels from images and serves as the\nbasis for diagnosing various diseases, like ophthalmic diseases.\nOphthalmologists often require high-resolution segmentation results for\nanalysis, which leads to super-computational load by most existing methods. If\nbased on low-resolution input, they easily ignore tiny vessels or cause\ndiscontinuity of segmented vessels. To solve these problems, the paper proposes\nan algorithm named SuperVessel, which gives out high-resolution and accurate\nvessel segmentation using low-resolution images as input. We first take\nsuper-resolution as our auxiliary branch to provide potential high-resolution\ndetail features, which can be deleted in the test phase. Secondly, we propose\ntwo modules to enhance the features of the interested segmentation region,\nincluding an upsampling with feature decomposition (UFD) module and a feature\ninteraction module (FIM) with a constraining loss to focus on the interested\nfeatures. Extensive experiments on three publicly available datasets\ndemonstrate that our proposed SuperVessel can segment more tiny vessels with\nhigher segmentation accuracy IoU over 6%, compared with other state-of-the-art\nalgorithms. Besides, the stability of SuperVessel is also stronger than other\nalgorithms. We will release the code after the paper is published.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhongxi Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_D/0/1/0/all/0/1\">Dan Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Accuracy Is Not Enough: The Need for Consistency in Object Detection. (arXiv:2207.13890v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13890","description":"<p>Object detectors are vital to many modern computer vision applications.\nHowever, even state-of-the-art object detectors are not perfect. On two images\nthat look similar to human eyes, the same detector can make different\npredictions because of small image distortions like camera sensor noise and\nlighting changes. This problem is called inconsistency. Existing accuracy\nmetrics do not properly account for inconsistency, and similar work in this\narea only targets improvements on artificial image distortions. Therefore, we\npropose a method to use non-artificial video frames to measure object detection\nconsistency over time, across frames. Using this method, we show that the\nconsistency of modern object detectors ranges from 83.2% to 97.1% on different\nvideo datasets from the Multiple Object Tracking Challenge. We conclude by\nshowing that applying image distortion corrections like .WEBP Image Compression\nand Unsharp Masking can improve consistency by as much as 5.1%, with no loss in\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tung_C/0/1/0/all/0/1\">Caleb Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Abhinav Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordwell_F/0/1/0/all/0/1\">Fischer Bordwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eliopoulos_N/0/1/0/all/0/1\">Nick Eliopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1\">George K. Thiruvathukal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yung-Hsiang Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Data Augmentation Technique for Out-of-Distribution Sample Detection using Compounded Corruptions. (arXiv:2207.13916v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13916","description":"<p>Modern deep neural network models are known to erroneously classify\nout-of-distribution (OOD) test data into one of the in-distribution (ID)\ntraining classes with high confidence. This can have disastrous consequences\nfor safety-critical applications. A popular mitigation strategy is to train a\nseparate classifier that can detect such OOD samples at the test time. In most\npractical settings OOD examples are not known at the train time, and hence a\nkey question is: how to augment the ID data with synthetic OOD samples for\ntraining such an OOD detector? In this paper, we propose a novel Compounded\nCorruption technique for the OOD data augmentation termed CnC. One of the major\nadvantages of CnC is that it does not require any hold-out data apart from the\ntraining set. Further, unlike current state-of-the-art (SOTA) techniques, CnC\ndoes not require backpropagation or ensembling at the test time, making our\nmethod much faster at inference. Our extensive comparison with 20 methods from\nthe major conferences in last 4 years show that a model trained using CnC based\ndata augmentation, significantly outperforms SOTA, both in terms of OOD\ndetection accuracy as well as inference time. We include a detailed post-hoc\nanalysis to investigate the reasons for the success of our method and identify\nhigher relative entropy and diversity of CnC samples as probable causes. We\nalso provide theoretical insights via a piece-wise decomposition analysis on a\ntwo-dimensional dataset to reveal (visually and quantitatively) that our\napproach leads to a tighter boundary around ID classes, leading to better\ndetection of OOD samples. Source code link: https://github.com/cnc-ood\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hebbalaguppe_R/0/1/0/all/0/1\">Ramya S. Hebbalaguppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goshal_S/0/1/0/all/0/1\">Soumya Suvra Goshal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_J/0/1/0/all/0/1\">Jatin Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadilkar_H/0/1/0/all/0/1\">Harshad Khadilkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chetan Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning based Degradation Representation for Blind Super-Resolution. (arXiv:2207.13963v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13963","description":"<p>The most of CNN based super-resolution (SR) methods assume that the\ndegradation is known (\\eg, bicubic). These methods will suffer a severe\nperformance drop when the degradation is different from their assumption.\nTherefore, some approaches attempt to train SR networks with the complex\ncombination of multiple degradations to cover the real degradation space. To\nadapt to multiple unknown degradations, introducing an explicit degradation\nestimator can actually facilitate SR performance. However, previous explicit\ndegradation estimation methods usually predict Gaussian blur with the\nsupervision of groundtruth blur kernels, and estimation errors may lead to SR\nfailure. Thus, it is necessary to design a method that can extract implicit\ndiscriminative degradation representation. To this end, we propose a\nMeta-Learning based Region Degradation Aware SR Network (MRDA), including\nMeta-Learning Network (MLN), Degradation Extraction Network (DEN), and Region\nDegradation Aware SR Network (RDAN). To handle the lack of groundtruth\ndegradation, we use the MLN to rapidly adapt to the specific complex\ndegradation after several iterations and extract implicit degradation\ninformation. Subsequently, a teacher network MRDA$_{T}$ is designed to further\nutilize the degradation information extracted by MLN for SR. However, MLN\nrequires iterating on paired low-resolution (LR) and corresponding\nhigh-resolution (HR) images, which is unavailable in the inference phase.\nTherefore, we adopt knowledge distillation (KD) to make the student network\nlearn to directly extract the same implicit degradation representation (IDR) as\nthe teacher from LR images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Bin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yapeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_Y/0/1/0/all/0/1\">Yucheng Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qingmin Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Effects of Different Types of Label Noise in Multi-Label Remote Sensing Image Classification. (arXiv:2207.13975v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13975","description":"<p>The development of accurate methods for multi-label classification (MLC) of\nremote sensing (RS) images is one of the most important research topics in RS.\nTo address MLC problems, the use of deep neural networks that require a high\nnumber of reliable training images annotated by multiple land-cover class\nlabels (multi-labels) have been found popular in RS. However, collecting such\nannotations is time-consuming and costly. A common procedure to obtain\nannotations at zero labeling cost is to rely on thematic products or\ncrowdsourced labels. As a drawback, these procedures come with the risk of\nlabel noise that can distort the learning process of the MLC algorithms. In the\nliterature, most label noise robust methods are designed for single label\nclassification (SLC) problems in computer vision (CV), where each image is\nannotated by a single label. Unlike SLC, label noise in MLC can be associated\nwith: 1) subtractive label-noise (a land cover class label is not assigned to\nan image while that class is present in the image); 2) additive label-noise (a\nland cover class label is assigned to an image although that class is not\npresent in the given image); and 3) mixed label-noise (a combination of both).\nIn this paper, we investigate three different noise robust CV SLC methods and\nadapt them to be robust for multi-label noise scenarios in RS. During\nexperiments we study the effects of different types of multi-label noise and\nevaluate the adapted methods rigorously. To this end, we also introduce a\nsynthetic multi-label noise injection strategy that is more adequate to\nsimulate operational scenarios compared to the uniform label noise injection\nstrategy, in which the labels of absent and present classes are flipped at\nuniform probability. Further, we study the relevance of different evaluation\nmetrics in MLC problems under noisy multi-labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burgert_T/0/1/0/all/0/1\">Tom Burgert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravanbakhsh_M/0/1/0/all/0/1\">Mahdyar Ravanbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Mask Transfiner for High-Quality Video Instance Segmentation. (arXiv:2207.14012v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14012","description":"<p>While Video Instance Segmentation (VIS) has seen rapid progress, current\napproaches struggle to predict high-quality masks with accurate boundary\ndetails. Moreover, the predicted segmentations often fluctuate over time,\nsuggesting that temporal consistency cues are neglected or not fully utilized.\nIn this paper, we set out to tackle these issues, with the aim of achieving\nhighly detailed and more temporally stable mask predictions for VIS. We first\npropose the Video Mask Transfiner (VMT) method, capable of leveraging\nfine-grained high-resolution features thanks to a highly efficient video\ntransformer structure. Our VMT detects and groups sparse error-prone\nspatio-temporal regions of each tracklet in the video segment, which are then\nrefined using both local and instance-level cues. Second, we identify that the\ncoarse boundary annotations of the popular YouTube-VIS dataset constitute a\nmajor limiting factor. Based on our VMT architecture, we therefore design an\nautomated annotation refinement approach by iterative training and\nself-correction. To benchmark high-quality mask predictions for VIS, we\nintroduce the HQ-YTVIS dataset, consisting of a manually re-annotated test set\nand our automatically refined training data. We compare VMT with the most\nrecent state-of-the-art methods on the HQ-YTVIS, as well as the Youtube-VIS,\nOVIS and BDD100K MOTS benchmarks. Experimental results clearly demonstrate the\nefficacy and effectiveness of our method on segmenting complex and dynamic\nobjects, by capturing precise details.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Lei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer. (arXiv:2207.14024v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14024","description":"<p>Large-scale deployment of autonomous vehicles has been continually delayed\ndue to safety concerns. On the one hand, comprehensive scene understanding is\nindispensable, a lack of which would result in vulnerability to rare but\ncomplex traffic situations, such as the sudden emergence of unknown objects.\nHowever, reasoning from a global context requires access to sensors of multiple\ntypes and adequate fusion of multi-modal sensor signals, which is difficult to\nachieve. On the other hand, the lack of interpretability in learning models\nalso hampers the safety with unverifiable failure causes. In this paper, we\npropose a safety-enhanced autonomous driving framework, named Interpretable\nSensor Fusion Transformer(InterFuser), to fully process and fuse information\nfrom multi-modal multi-view sensors for achieving comprehensive scene\nunderstanding and adversarial event detection. Besides, intermediate\ninterpretable features are generated from our framework, which provide more\nsemantics and are exploited to better constrain actions to be within the safe\nsets. We conducted extensive experiments on CARLA benchmarks, where our model\noutperforms prior methods, ranking the first on the public CARLA Leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Hao Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">LeTian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">RuoBing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separable Quaternion Matrix Factorization for Polarization Images. (arXiv:2207.14039v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14039","description":"<p>Polarization is a unique characteristic of transverse wave and is represented\nby Stokes parameters. Analysis of polarization states can reveal valuable\ninformation about the sources. In this paper, we propose a separable low-rank\nquaternion linear mixing model to polarized signals: we assume each column of\nthe source factor matrix equals a column of polarized data matrix and refer to\nthe corresponding problem as separable quaternion matrix factorization (SQMF).\nWe discuss some properties of the matrix that can be decomposed by SQMF. To\ndetermine the source factor matrix in quaternion space, we propose a heuristic\nalgorithm called quaternion successive projection algorithm (QSPA) inspired by\nthe successive projection algorithm. To guarantee the effectiveness of QSPA, a\nnew normalization operator is proposed for the quaternion matrix. We use a\nblock coordinate descent algorithm to compute nonnegative factor activation\nmatrix in real number space. We test our method on the applications of\npolarization image representation and spectro-polarimetric imaging unmixing to\nverify its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junjun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael K. Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Self-Tuning Data Association for Geo-Referencing Using Lane Markings. (arXiv:2207.14042v1 [cs.RO])","link":"http://arxiv.org/abs/2207.14042","description":"<p>Localization in aerial imagery-based maps offers many advantages, such as\nglobal consistency, geo-referenced maps, and the availability of publicly\naccessible data. However, the landmarks that can be observed from both aerial\nimagery and on-board sensors is limited. This leads to ambiguities or aliasing\nduring the data association.\n</p>\n<p>Building upon a highly informative representation (that allows efficient data\nassociation), this paper presents a complete pipeline for resolving these\nambiguities. Its core is a robust self-tuning data association that adapts the\nsearch area depending on the entropy of the measurements. Additionally, to\nsmooth the final result, we adjust the information matrix for the associated\ndata as a function of the relative transform produced by the data association\nprocess.\n</p>\n<p>We evaluate our method on real data from urban and rural scenarios around the\ncity of Karlsruhe in Germany. We compare state-of-the-art outlier mitigation\nmethods with our self-tuning approach, demonstrating a considerable\nimprovement, especially for outer-urban scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Banon_M/0/1/0/all/0/1\">Miguel &#xc1;ngel Mu&#xf1;oz-Ba&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauls_J/0/1/0/all/0/1\">Jan-Hendrik Pauls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haohao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Candelas_F/0/1/0/all/0/1\">Francisco A. Candelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_F/0/1/0/all/0/1\">Fernando Torres</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Strands: Learning Hair Geometry and Appearance from Multi-View Images. (arXiv:2207.14067v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14067","description":"<p>We present Neural Strands, a novel learning framework for modeling accurate\nhair geometry and appearance from multi-view image inputs. The learned hair\nmodel can be rendered in real-time from any viewpoint with high-fidelity\nview-dependent effects. Our model achieves intuitive shape and style control\nunlike volumetric counterparts. To enable these properties, we propose a novel\nhair representation based on a neural scalp texture that encodes the geometry\nand appearance of individual strands at each texel location. Furthermore, we\nintroduce a novel neural rendering framework based on rasterization of the\nlearned hair strands. Our neural rendering is strand-accurate and anti-aliased,\nmaking the rendering view-consistent and photorealistic. Combining appearance\nwith a multi-view geometric prior, we enable, for the first time, the joint\nlearning of appearance and explicit hair geometry from a multi-view setup. We\ndemonstrate the efficacy of our approach in terms of fidelity and efficiency\nfor various hairstyles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosu_R/0/1/0/all/0/1\">Radu Alexandru Rosu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1\">Giljoo Nam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PEA: Improving the Performance of ReLU Networks for Free by Using Progressive Ensemble Activations. (arXiv:2207.14074v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14074","description":"<p>In recent years novel activation functions have been proposed to improve the\nperformance of neural networks, and they show superior performance compared to\nthe ReLU counterpart. However, there are environments, where the availability\nof complex activations is limited, and usually only the ReLU is supported. In\nthis paper we propose methods that can be used to improve the performance of\nReLU networks by using these efficient novel activations during model training.\nMore specifically, we propose ensemble activations that are composed of the\nReLU and one of these novel activations. Furthermore, the coefficients of the\nensemble are neither fixed nor learned, but are progressively updated during\nthe training process in a way that by the end of the training only the ReLU\nactivations remain active in the network and the other activations can be\nremoved. This means that in inference time the network contains ReLU\nactivations only. We perform extensive evaluations on the ImageNet\nclassification task using various compact network architectures and various\nnovel activation functions. Results show 0.2-0.8% top-1 accuracy gain, which\nconfirms the applicability of the proposed methods. Furthermore, we demonstrate\nthe proposed methods on semantic segmentation and we boost the performance of a\ncompact segmentation network by 0.34% mIOU on the Cityscapes dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Utasi_A/0/1/0/all/0/1\">&#xc1;kos Utasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topological Analysis of Ensembles of Hydrodynamic Turbulent Flows -- An Experimental Study. (arXiv:2207.14080v1 [physics.flu-dyn])","link":"http://arxiv.org/abs/2207.14080","description":"<p>This application paper presents a comprehensive experimental evaluation of\nthe suitability of Topological Data Analysis (TDA) for the quantitative\ncomparison of turbulent flows. Specifically, our study documents the usage of\nthe persistence diagram of the maxima of flow enstrophy (an established\nvorticity indicator), for the topological representation of 180 ensemble\nmembers, generated by a coarse sampling of the parameter space of five\nnumerical solvers. We document five main hypotheses reported by domain experts,\ndescribing their expectations regarding the variability of the flows generated\nby the distinct solver configurations. We contribute three evaluation protocols\nto assess the validation of the above hypotheses by two comparison measures:\n(i) a standard distance used in scientific imaging (the L2 norm) and (ii) an\nestablished topological distance between persistence diagrams (the\nL2-Wasserstein metric). Extensive experiments on the input ensemble demonstrate\nthe superiority of the topological distance (ii) to report as close to each\nother flows which are expected to be similar by domain experts, due to the\nconfiguration of their vortices. Overall, the insights reported by our study\nbring an experimental evidence of the suitability of TDA for representing and\ncomparing turbulent flows, thereby providing to the fluid dynamics community\nconfidence for its usage in future work. Also, our flow data and evaluation\nprotocols provide to the TDA community an application-approved benchmark for\nthe evaluation and design of further topological distances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Nauleau_F/0/1/0/all/0/1\">Florent Nauleau</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Vivodtzev_F/0/1/0/all/0/1\">Fabien Vivodtzev</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bridel_Bertomeu_T/0/1/0/all/0/1\">Thibault Bridel-Bertomeu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Beaugendre_H/0/1/0/all/0/1\">Heloise Beaugendre</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tierny_J/0/1/0/all/0/1\">Julien Tierny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Camouflaged Object Detection with Scribble Annotations. (arXiv:2207.14083v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14083","description":"<p>Existing camouflaged object detection (COD) methods rely heavily on\nlarge-scale datasets with pixel-wise annotations. However, due to the ambiguous\nboundary, it is very time-consuming and labor-intensive to annotate camouflage\nobjects pixel-wisely (which takes ~ 60 minutes per image). In this paper, we\npropose the first weakly-supervised camouflaged object detection (COD) method,\nusing scribble annotations as supervision. To achieve this, we first construct\na scribble-based camouflaged object dataset with 4,040 images and corresponding\nscribble annotations. It is worth noting that annotating the scribbles used in\nour dataset takes only ~ 10 seconds per image, which is 360 times faster than\nper-pixel annotations. However, the network directly using scribble annotations\nfor supervision will fail to localize the boundary of camouflaged objects and\ntend to have inconsistent predictions since scribble annotations only describe\nthe primary structure of objects without details. To tackle this problem, we\npropose a novel consistency loss composed of two parts: a reliable cross-view\nloss to attain reliable consistency over different images, and a soft\ninside-view loss to maintain consistency inside a single prediction map.\nBesides, we observe that humans use semantic information to segment regions\nnear boundaries of camouflaged objects. Therefore, we design a feature-guided\nloss, which includes visual features directly extracted from images and\nsemantically significant features captured by models. Moreover, we propose a\nnovel network that detects camouflaged objects by scribble learning on\nstructural information and semantic relations. Experimental results show that\nour model outperforms relevant state-of-the-art methods on three COD benchmarks\nwith an average improvement of 11.0% on MAE, 3.2% on S-measure, 2.5% on\nE-measure and 4.4% on weighted F-measure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruozhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qihua Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiaying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CubeMLP: A MLP-based Model for Multimodal Sentiment Analysis and Depression Estimation. (arXiv:2207.14087v1 [cs.MM])","link":"http://arxiv.org/abs/2207.14087","description":"<p>Multimodal sentiment analysis and depression estimation are two important\nresearch topics that aim to predict human mental states using multimodal data.\nPrevious research has focused on developing effective fusion strategies for\nexchanging and integrating mind-related information from different modalities.\nSome MLP-based techniques have recently achieved considerable success in a\nvariety of computer vision tasks. Inspired by this, we explore multimodal\napproaches with a feature-mixing perspective in this study. To this end, we\nintroduce CubeMLP, a multimodal feature processing framework based entirely on\nMLP. CubeMLP consists of three independent MLP units, each of which has two\naffine transformations. CubeMLP accepts all relevant modality features as input\nand mixes them across three axes. After extracting the characteristics using\nCubeMLP, the mixed multimodal features are flattened for task predictions. Our\nexperiments are conducted on sentiment analysis datasets: CMU-MOSI and\nCMU-MOSEI, and depression estimation dataset: AVEC2019. The results show that\nCubeMLP can achieve state-of-the-art performance with a much lower computing\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Large-Scale Small Object Detection: Survey and Benchmarks. (arXiv:2207.14096v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14096","description":"<p>With the rise of deep convolutional neural networks, object detection has\nachieved prominent advances in past years. However, such prosperity could not\ncamouflage the unsatisfactory situation of Small Object Detection (SOD), one of\nthe notoriously challenging tasks in computer vision, owing to the poor visual\nappearance and noisy representation caused by the intrinsic structure of small\ntargets. In addition, large-scale dataset for benchmarking small object\ndetection methods remains a bottleneck. In this paper, we first conduct a\nthorough review of small object detection. Then, to catalyze the development of\nSOD, we construct two large-scale Small Object Detection dAtasets (SODA),\nSODA-D and SODA-A, which focus on the Driving and Aerial scenarios\nrespectively. SODA-D includes 24704 high-quality traffic images and 277596\ninstances of 9 categories. For SODA-A, we harvest 2510 high-resolution aerial\nimages and annotate 800203 instances over 9 classes. The proposed datasets, as\nwe know, are the first-ever attempt to large-scale benchmarks with a vast\ncollection of exhaustively annotated instances tailored for multi-category SOD.\nFinally, we evaluate the performance of mainstream methods on SODA. We expect\nthe released benchmarks could facilitate the development of SOD and spawn more\nbreakthroughs in this field. Datasets and codes will be available soon at:\n\\url{https://shaunyuan22.github.io/SODA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xiwen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Kebing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qinghua Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RHA-Net: An Encoder-Decoder Network with Residual Blocks and Hybrid Attention Mechanisms for Pavement Crack Segmentation. (arXiv:2207.14166v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14166","description":"<p>The acquisition and evaluation of pavement surface data play an essential\nrole in pavement condition evaluation. In this paper, an efficient and\neffective end-to-end network for automatic pavement crack segmentation, called\nRHA-Net, is proposed to improve the pavement crack segmentation accuracy. The\nRHA-Net is built by integrating residual blocks (ResBlocks) and hybrid\nattention blocks into the encoder-decoder architecture. The ResBlocks are used\nto improve the ability of RHA-Net to extract high-level abstract features. The\nhybrid attention blocks are designed to fuse both low-level features and\nhigh-level features to help the model focus on correct channels and areas of\ncracks, thereby improving the feature presentation ability of RHA-Net. An image\ndata set containing 789 pavement crack images collected by a self-designed\nmobile robot is constructed and used for training and evaluating the proposed\nmodel. Compared with other state-of-the-art networks, the proposed model\nachieves better performance and the functionalities of adding residual blocks\nand hybrid attention mechanisms are validated in a comprehensive ablation\nstudy. Additionally, a light-weighted version of the model generated by\nintroducing depthwise separable convolution achieves better a performance and a\nmuch faster processing speed with 1/30 of the number of U-Net parameters. The\ndeveloped system can segment pavement crack in real-time on an embedded device\nJetson TX2 (25 FPS). The video taken in real-time experiments is released at\nhttps://youtu.be/3XIogk0fiG4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Guijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiacheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_D/0/1/0/all/0/1\">Duan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Peili Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_W/0/1/0/all/0/1\">Weihua Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kelvin C. P. Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content-oriented learned image compression. (arXiv:2207.14168v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14168","description":"<p>In recent years, with the development of deep neural networks, end-to-end\noptimized image compression has made significant progress and exceeded the\nclassic methods in terms of rate-distortion performance. However, most\nlearning-based image compression methods are unlabeled and do not consider\nimage semantics or content when optimizing the model. In fact, human eyes have\ndifferent sensitivities to different content, so the image content also needs\nto be considered. In this paper, we propose a content-oriented image\ncompression method, which handles different kinds of image contents with\ndifferent strategies. Extensive experiments show that the proposed method\nachieves competitive subjective results compared with state-of-the-art\nend-to-end learned image compression methods or classic methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shangyin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yihui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yibo Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Aligned Matching for Enhanced DETR Convergence and Multi-Scale Feature Fusion. (arXiv:2207.14172v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14172","description":"<p>The recently proposed DEtection TRansformer (DETR) has established a fully\nend-to-end paradigm for object detection. However, DETR suffers from slow\ntraining convergence, which hinders its applicability to various detection\ntasks. We observe that DETR's slow convergence is largely attributed to the\ndifficulty in matching object queries to relevant regions due to the unaligned\nsemantics between object queries and encoded image features. With this\nobservation, we design Semantic-Aligned-Matching DETR++ (SAM-DETR++) to\naccelerate DETR's convergence and improve detection performance. The core of\nSAM-DETR++ is a plug-and-play module that projects object queries and encoded\nimage features into the same feature embedding space, where each object query\ncan be easily matched to relevant regions with similar semantics. Besides,\nSAM-DETR++ searches for multiple representative keypoints and exploits their\nfeatures for semantic-aligned matching with enhanced representation capacity.\nFurthermore, SAM-DETR++ can effectively fuse multi-scale features in a\ncoarse-to-fine manner on the basis of the designed semantic-aligned matching.\nExtensive experiments show that the proposed SAM-DETR++ achieves superior\nconvergence speed and competitive detection accuracy. Additionally, as a\nplug-and-play method, SAM-DETR++ can complement existing DETR convergence\nsolutions with even better performance, achieving 44.8% AP with merely 12\ntraining epochs and 49.1% AP with 50 training epochs on COCO val2017 with\nResNet-50. Codes are available at https://github.com/ZhangGongjie/SAM-DETR .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gongjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kaiwen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Limited Annotations: A Survey on Deep Semi-Supervised Learning for Medical Image Segmentation. (arXiv:2207.14191v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14191","description":"<p>Medical image segmentation is a fundamental and critical step in many\nimage-guided clinical approaches. Recent success of deep learning-based\nsegmentation methods usually relies on a large amount of labeled data, which is\nparticularly difficult and costly to obtain especially in the medical imaging\ndomain where only experts can provide reliable and accurate annotations.\nSemi-supervised learning has emerged as an appealing strategy and been widely\napplied to medical image segmentation tasks to train deep models with limited\nannotations. In this paper, we present a comprehensive review of recently\nproposed semi-supervised learning methods for medical image segmentation and\nsummarized both the technical novelties and empirical results. Furthermore, we\nanalyze and discuss the limitations and several unsolved problems of existing\napproaches. We hope this review could inspire the research community to explore\nsolutions for this challenge and further promote the developments in medical\nimage segmentation field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_R/0/1/0/all/0/1\">Rushi Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Le Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Rong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jicong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Cross-Person Cues for Body-Part Interactiveness Learning in HOI Detection. (arXiv:2207.14192v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14192","description":"<p>Human-Object Interaction (HOI) detection plays a crucial role in activity\nunderstanding. Though significant progress has been made, interactiveness\nlearning remains a challenging problem in HOI detection: existing methods\nusually generate redundant negative H-O pair proposals and fail to effectively\nextract interactive pairs. Though interactiveness has been studied in both\nwhole body- and part- level and facilitates the H-O pairing, previous works\nonly focus on the target person once (i.e., in a local perspective) and\noverlook the information of the other persons. In this paper, we argue that\ncomparing body-parts of multi-person simultaneously can afford us more useful\nand supplementary interactiveness cues. That said, to learn body-part\ninteractiveness from a global perspective: when classifying a target person's\nbody-part interactiveness, visual cues are explored not only from\nherself/himself but also from other persons in the image. We construct\nbody-part saliency maps based on self-attention to mine cross-person\ninformative cues and learn the holistic relationships between all the\nbody-parts. We evaluate the proposed method on widely-used benchmarks HICO-DET\nand V-COCO. With our new perspective, the holistic global-local body-part\ninteractiveness learning achieves significant improvements over\nstate-of-the-art. Our code is available at\nhttps://github.com/enlighten0707/Body-Part-Map-for-Interactiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoqian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong-Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinpeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuzhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Voronoi Diagram Subdivision: Towards A Holistic Geometric Framework for Exemplar-free Class-Incremental Learning. (arXiv:2207.14202v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14202","description":"<p>Exemplar-free Class-incremental Learning (CIL) is a challenging problem\nbecause rehearsing data from previous phases is strictly prohibited, causing\ncatastrophic forgetting of Deep Neural Networks (DNNs). In this paper, we\npresent iVoro, a holistic framework for CIL, derived from computational\ngeometry. We found Voronoi Diagram (VD), a classical model for space\nsubdivision, is especially powerful for solving the CIL problem, because VD\nitself can be constructed favorably in an incremental manner -- the newly added\nsites (classes) will only affect the proximate classes, making the\nnon-contiguous classes hardly forgettable. Further, in order to find a better\nset of centers for VD construction, we colligate DNN with VD using Power\nDiagram and show that the VD structure can be optimized by integrating local\nDNN models using a divide-and-conquer algorithm. Moreover, our VD construction\nis not restricted to the deep feature space, but is also applicable to multiple\nintermediate feature spaces, promoting VD to be multi-centered VD (CIVD) that\nefficiently captures multi-grained features from DNN. Importantly, iVoro is\nalso capable of handling uncertainty-aware test-time Voronoi cell assignment\nand has exhibited high correlations between geometric uncertainty and\npredictive accuracy (up to ~0.9). Putting everything together, iVoro achieves\nup to 25.26%, 37.09%, and 33.21% improvements on CIFAR-100, TinyImageNet, and\nImageNet-Subset, respectively, compared to the state-of-the-art non-exemplar\nCIL approaches. In conclusion, iVoro enables highly accurate,\nprivacy-preserving, and geometrically interpretable CIL that is particularly\nuseful when cross-phase data sharing is forbidden, e.g. in medical\napplications. Our code is available at https://machunwei.github.io/ivoro.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chunwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinhui Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Humans disagree with the IoU for measuring object detector localization error. (arXiv:2207.14221v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14221","description":"<p>The localization quality of automatic object detectors is typically evaluated\nby the Intersection over Union (IoU) score. In this work, we show that humans\nhave a different view on localization quality. To evaluate this, we conduct a\nsurvey with more than 70 participants. Results show that for localization\nerrors with the exact same IoU score, humans might not consider that these\nerrors are equal, and express a preference. Our work is the first to evaluate\nIoU with humans and makes it clear that relying on IoU scores alone to evaluate\nlocalization errors might not be sufficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strafforello_O/0/1/0/all/0/1\">Ombretta Strafforello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekart_V/0/1/0/all/0/1\">Vanathi Rajasekart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kayhan_O/0/1/0/all/0/1\">Osman S. Kayhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inel_O/0/1/0/all/0/1\">Oana Inel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan van Gemert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Electricity Price Forecasting Model based on Gated Recurrent Units. (arXiv:2207.14225v1 [cs.LG])","link":"http://arxiv.org/abs/2207.14225","description":"<p>The participation of consumers and producers in demand response programs has\nincreased in smart grids, which reduces investment and operation costs of power\nsystems. Also, with the advent of renewable energy sources, the electricity\nmarket is becoming more complex and unpredictable. To effectively implement\ndemand response programs, forecasting the future price of electricity is very\ncrucial for producers in the electricity market. Electricity prices are very\nvolatile and change under the influence of various factors such as temperature,\nwind speed, rainfall, intensity of commercial and daily activities, etc.\nTherefore, considering the influencing factors as dependent variables can\nincrease the accuracy of the forecast. In this paper, a model for electricity\nprice forecasting is presented based on Gated Recurrent Units. The electrical\nload consumption is considered as an input variable in this model. Noise in\nelectricity price seriously reduces the efficiency and effectiveness of\nanalysis. Therefore, an adaptive noise reducer is integrated into the model for\nnoise reduction. The SAEs are then used to extract features from the de-noised\nelectricity price. Finally, the de-noised features are fed into the GRU to\ntrain predictor. Results on real dataset shows that the proposed methodology\ncan perform effectively in prediction of electricity price.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_N/0/1/0/all/0/1\">Nafise Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajabi_R/0/1/0/all/0/1\">Roozbeh Rajabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estebsari_A/0/1/0/all/0/1\">Abouzar Estebsari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Recognition by Request. (arXiv:2207.14227v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14227","description":"<p>In this paper, we present a novel protocol of annotation and evaluation for\nvisual recognition. Different from traditional settings, the protocol does not\nrequire the labeler/algorithm to annotate/recognize all targets (objects,\nparts, etc.) at once, but instead raises a number of recognition instructions\nand the algorithm recognizes targets by request. This mechanism brings two\nbeneficial properties to reduce the burden of annotation, namely, (i) variable\ngranularity: different scenarios can have different levels of annotation, in\nparticular, object parts can be labeled only in large and clear instances, (ii)\nbeing open-domain: new concepts can be added to the database in minimal costs.\nTo deal with the proposed setting, we maintain a knowledge base and design a\nquery-based visual recognition framework that constructs queries on-the-fly\nbased on the requests. We evaluate the recognition system on two\nmixed-annotated datasets, CPP and ADE20K, and demonstrate its promising ability\nof learning from partially labeled data as well as adapting to new concepts\nwith only text labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chufeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction. (arXiv:2207.14238v1 [eess.IV])","link":"http://arxiv.org/abs/2207.14238","description":"<p>The LIDC-IDRI database is the most popular benchmark for lung cancer\nprediction. However, with subjective assessment from radiologists, nodules in\nLIDC may have entirely different malignancy annotations from the pathological\nground truth, introducing label assignment errors and subsequent supervision\nbias during training. The LIDC database thus requires more objective labels for\nlearning-based cancer prediction. Based on an extra small dataset containing\n180 nodules diagnosed by pathological examination, we propose to re-label LIDC\ndata to mitigate the effect of original annotation bias verified on this robust\nbenchmark. We demonstrate in this paper that providing new labels by similar\nnodule retrieval based on metric learning would be an effective re-labeling\nstrategy. Training on these re-labeled LIDC nodules leads to improved model\nperformance, which is enhanced when new labels of uncertain nodules are added.\nWe further infer that re-labeling LIDC is current an expedient way for robust\nlung cancer prediction while building a large pathological-proven nodule\ndatabase provides the long-term solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hanxiao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_W/0/1/0/all/0/1\">Weihao Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhexin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_F/0/1/0/all/0/1\">Feng Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1\">Yun Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang-Zhong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining human parsing with analytical feature extraction and ranking schemes for high-generalization person reidentification. (arXiv:2207.14243v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14243","description":"<p>Person reidentification (re-ID) has been receiving increasing attention in\nrecent years due to its importance for both science and society. Machine\nlearning and particularly Deep Learning (DL) has become the main re-id tool\nthat allowed researches to achieve unprecedented accuracy levels on benchmark\ndatasets. However, there is a known problem of poor generalization of DL\nmodels. That is, models trained to achieve high accuracy on one dataset perform\npoorly on other ones and require re-training. To address this issue, we present\na model without trainable parameters which shows great potential for high\ngeneralization. It combines a fully analytical feature extraction and\nsimilarity ranking scheme with DL-based human parsing used to obtain the\ninitial subregion classification. We show that such combination to a high\nextent eliminates the drawbacks of existing analytical methods. We use\ninterpretable color and texture features which have human-readable similarity\nmeasures associated with them. To verify the proposed method we conduct\nexperiments on Market1501 and CUHK03 datasets achieving competitive rank-1\naccuracy comparable with that of DL-models. Most importantly we show that our\nmethod achieves 63.9% and 93.5% rank-1 cross-domain accuracy when applied to\ntransfer learning tasks. It is significantly higher than previously reported\n30-50% transfer accuracy. We discuss the potential ways of adding new features\nto further improve the model. We also show the advantage of interpretable\nfeatures for constructing human-generated queries from verbal description to\nconduct search without a query image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabdullin_N/0/1/0/all/0/1\">Nikita Gabdullin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonteBoxFinder: Detecting and Filtering Primitives to Fit a Noisy Point Cloud. (arXiv:2207.14268v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14268","description":"<p>We present MonteBoxFinder, a method that, given a noisy input point cloud,\nfits cuboids to the input scene. Our primary contribution is a discrete\noptimization algorithm that, from a dense set of initially detected cuboids, is\nable to efficiently filter good boxes from the noisy ones. Inspired by recent\napplications of MCTS to scene understanding problems, we develop a stochastic\nalgorithm that is, by design, more efficient for our task. Indeed, the quality\nof a fit for a cuboid arrangement is invariant to the order in which the\ncuboids are added into the scene. We develop several search baselines for our\nproblem and demonstrate, on the ScanNet dataset, that our approach is more\nefficient and precise. Finally, we strongly believe that our core algorithm is\nvery general and that it could be extended to many other problems in 3D scene\nunderstanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjisoa_M/0/1/0/all/0/1\">Micha&#xeb;l Ramamonjisoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stekovic_S/0/1/0/all/0/1\">Sinisa Stekovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CuDi: Curve Distillation for Efficient and Controllable Exposure Adjustment. (arXiv:2207.14273v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14273","description":"<p>We present Curve Distillation, CuDi, for efficient and controllable exposure\nadjustment without the requirement of paired or unpaired data during training.\nOur method inherits the zero-reference learning and curve-based framework from\nan effective low-light image enhancement method, Zero-DCE, with further speed\nup in its inference speed, reduction in its model size, and extension to\ncontrollable exposure adjustment. The improved inference speed and lightweight\nmodel are achieved through novel curve distillation that approximates the\ntime-consuming iterative operation in the conventional curve-based framework by\nhigh-order curve's tangent line. The controllable exposure adjustment is made\npossible with a new self-supervised spatial exposure control loss that\nconstrains the exposure levels of different spatial regions of the output to be\nclose to the brightness distribution of an exposure map serving as an input\ncondition. Different from most existing methods that can only correct either\nunderexposed or overexposed photos, our approach corrects both underexposed and\noverexposed photos with a single model. Notably, our approach can additionally\nadjust the exposure levels of a photo globally or locally with the guidance of\nan input condition exposure map, which can be pre-defined or manually set in\nthe inference stage. Through extensive experiments, we show that our method is\nappealing for its fast, robust, and flexible performance, outperforming\nstate-of-the-art methods in real scenes. Project page:\nhttps://li-chongyi.github.io/CuDi_files/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chunle Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruicheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shangchen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The One Where They Reconstructed 3D Humans and Environments in TV Shows. (arXiv:2207.14279v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14279","description":"<p>TV shows depict a wide variety of human behaviors and have been studied\nextensively for their potential to be a rich source of data for many\napplications. However, the majority of the existing work focuses on 2D\nrecognition tasks. In this paper, we make the observation that there is a\ncertain persistence in TV shows, i.e., repetition of the environments and the\nhumans, which makes possible the 3D reconstruction of this content. Building on\nthis insight, we propose an automatic approach that operates on an entire\nseason of a TV show and aggregates information in 3D; we build a 3D model of\nthe environment, compute camera information, static 3D scene structure and body\nscale information. Then, we demonstrate how this information acts as rich 3D\ncontext that can guide and improve the recovery of 3D human pose and position\nin these environments. Moreover, we show that reasoning about humans and their\nenvironment in 3D enables a broad range of downstream applications:\nre-identification, gaze estimation, cinematography and image editing. We apply\nour approach on environments from seven iconic TV shows and perform an\nextensive evaluation of the proposed system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavlakos_G/0/1/0/all/0/1\">Georgios Pavlakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_E/0/1/0/all/0/1\">Ethan Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tancik_M/0/1/0/all/0/1\">Matthew Tancik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions. (arXiv:2207.14284v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14284","description":"<p>Recent progress in vision Transformers exhibits great success in various\ntasks driven by the new spatial modeling mechanism based on dot-product\nself-attention. In this paper, we show that the key ingredients behind the\nvision Transformers, namely input-adaptive, long-range and high-order spatial\ninteractions, can also be efficiently implemented with a convolution-based\nframework. We present the Recursive Gated Convolution\n($\\textit{g}^\\textit{n}$Conv) that performs high-order spatial interactions\nwith gated convolutions and recursive designs. The new operation is highly\nflexible and customizable, which is compatible with various variants of\nconvolution and extends the two-order interactions in self-attention to\narbitrary orders without introducing significant extra computation.\n$\\textit{g}^\\textit{n}$Conv can serve as a plug-and-play module to improve\nvarious vision Transformers and convolution-based models. Based on the\noperation, we construct a new family of generic vision backbones named HorNet.\nExtensive experiments on ImageNet classification, COCO object detection and\nADE20K semantic segmentation show HorNet outperform Swin Transformers and\nConvNeXt by a significant margin with similar overall architecture and training\nconfigurations. HorNet also shows favorable scalability to more training data\nand a larger model size. Apart from the effectiveness in visual encoders, we\nalso show $\\textit{g}^\\textit{n}$Conv can be applied to task-specific decoders\nand consistently improve dense prediction performance with less computation.\nOur results demonstrate that $\\textit{g}^\\textit{n}$Conv can be a new basic\nmodule for visual modeling that effectively combines the merits of both vision\nTransformers and CNNs. Code is available at\nhttps://github.com/raoyongming/HorNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yansong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth Field Networks for Generalizable Multi-view Scene Representation. (arXiv:2207.14287v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14287","description":"<p>Modern 3D computer vision leverages learning to boost geometric reasoning,\nmapping image data to classical structures such as cost volumes or epipolar\nconstraints to improve matching. These architectures are specialized according\nto the particular problem, and thus require significant task-specific tuning,\noften leading to poor domain generalization performance. Recently, generalist\nTransformer architectures have achieved impressive results in tasks such as\noptical flow and depth estimation by encoding geometric priors as inputs rather\nthan as enforced constraints. In this paper, we extend this idea and propose to\nlearn an implicit, multi-view consistent scene representation, introducing a\nseries of 3D data augmentation techniques as a geometric inductive prior to\nincrease view diversity. We also show that introducing view synthesis as an\nauxiliary task further improves depth estimation. Our Depth Field Networks\n(DeFiNe) achieve state-of-the-art results in stereo and video depth estimation\nwithout explicit geometric constraints, and improve on zero-shot domain\ngeneralization by a wide margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1\">Vitor Guizilini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasiljevic_I/0/1/0/all/0/1\">Igor Vasiljevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiading Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1\">Greg Shakhnarovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walter_M/0/1/0/all/0/1\">Matthew Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rewriting Geometric Rules of a GAN. (arXiv:2207.14288v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14288","description":"<p>Deep generative models make visual content creation more accessible to novice\nusers by automating the synthesis of diverse, realistic content based on a\ncollected dataset. However, the current machine learning approaches miss a key\nelement of the creative process -- the ability to synthesize things that go far\nbeyond the data distribution and everyday experience. To begin to address this\nissue, we enable a user to \"warp\" a given model by editing just a handful of\noriginal model outputs with desired geometric changes. Our method applies a\nlow-rank update to a single model layer to reconstruct edited examples.\nFurthermore, to combat overfitting, we propose a latent space augmentation\nmethod based on style-mixing. Our method allows a user to create a model that\nsynthesizes endless objects with defined geometric changes, enabling the\ncreation of a new generative model without the burden of curating a large-scale\ndataset. We also demonstrate that edited models can be composed to achieve\naggregated effects, and we present an interactive interface to enable users to\ncreate new models through composition. Empirical measurements on multiple test\ncases suggest the advantage of our method against recent GAN fine-tuning\nmethods. Finally, we showcase several applications using the edited models,\nincluding latent space interpolation and image editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng-Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Initialization and Alignment for Adversarial Texture Optimization. (arXiv:2207.14289v1 [cs.CV])","link":"http://arxiv.org/abs/2207.14289","description":"<p>While recovery of geometry from image and video data has received a lot of\nattention in computer vision, methods to capture the texture for a given\ngeometry are less mature. Specifically, classical methods for texture\ngeneration often assume clean geometry and reasonably well-aligned image data.\nWhile very recent methods, e.g., adversarial texture optimization, better\nhandle lower-quality data obtained from hand-held devices, we find them to\nstill struggle frequently. To improve robustness, particularly of recent\nadversarial texture optimization, we develop an explicit initialization and an\nalignment procedure. It deals with complex geometry due to a robust mapping of\nthe geometry to the texture map and a hard-assignment-based initialization. It\ndeals with misalignment of geometry and images by integrating fast\nimage-alignment into the texture refinement optimization. We demonstrate\nefficacy of our texture generation on a dataset of 11 scenes with a total of\n2807 frames, observing 7.8% and 11.1% relative improvements regarding\nperceptual and sharpness measurements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhizhen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander G. Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta. (arXiv:2009.08753v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.08753","description":"<p>Learning to generate new images for a novel category based on only a few\nimages, named as few-shot image generation, has attracted increasing research\ninterest. Several state-of-the-art works have yielded impressive results, but\nthe diversity is still limited. In this work, we propose a novel Delta\nGenerative Adversarial Network (DeltaGAN), which consists of a reconstruction\nsubnetwork and a generation subnetwork. The reconstruction subnetwork captures\nintra-category transformation, i.e., \"delta\", between same-category pairs. The\ngeneration subnetwork generates sample-specific \"delta\" for an input image,\nwhich is combined with this input image to generate a new image within the same\ncategory. Besides, an adversarial delta matching loss is designed to link the\nabove two subnetworks together. Extensive experiments on five few-shot image\ndatasets demonstrate the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianfu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Deep Morphological Networks with Neural Architecture Search. (arXiv:2106.07714v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07714","description":"<p>Deep Neural Networks (DNNs) are generated by sequentially performing linear\nand non-linear processes. Using a combination of linear and non-linear\nprocedures is critical for generating a sufficiently deep feature space. The\nmajority of non-linear operators are derivations of activation functions or\npooling functions. Mathematical morphology is a branch of mathematics that\nprovides non-linear operators for a variety of image processing problems. We\ninvestigate the utility of integrating these operations in an end-to-end deep\nlearning framework in this paper. DNNs are designed to acquire a realistic\nrepresentation for a particular job. Morphological operators give topological\ndescriptors that convey salient information about the shapes of objects\ndepicted in images. We propose a method based on meta-learning to incorporate\nmorphological operators into DNNs. The learned architecture demonstrates how\nour novel morphological operations significantly increase DNN performance on\nvarious tasks, including picture classification and edge detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yufei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belkhir_N/0/1/0/all/0/1\">Nacim Belkhir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angulo_J/0/1/0/all/0/1\">Jesus Angulo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALLNet: A Hybrid Convolutional Neural Network to Improve Diagnosis of Acute Lymphocytic Leukemia (ALL) in White Blood Cells. (arXiv:2108.08195v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08195","description":"<p>Due to morphological similarity at the microscopic level, making an accurate\nand time-sensitive distinction between blood cells affected by Acute\nLymphocytic Leukemia (ALL) and their healthy counterparts calls for the usage\nof machine learning architectures. However, three of the most common models,\nVGG, ResNet, and Inception, each come with their own set of flaws with room for\nimprovement which demands the need for a superior model. ALLNet, the proposed\nhybrid convolutional neural network architecture, consists of a combination of\nthe VGG, ResNet, and Inception models. The ALL Challenge dataset of ISBI 2019\n(available here) contains 10,691 images of white blood cells which were used to\ntrain and test the models. 7,272 of the images in the dataset are of cells with\nALL and 3,419 of them are of healthy cells. Of the images, 60% were used to\ntrain the model, 20% were used for the cross-validation set, and 20% were used\nfor the test set. ALLNet outperformed the VGG, ResNet, and the Inception models\nacross the board, achieving an accuracy of 92.6567%, a sensitivity of 95.5304%,\na specificity of 85.9155%, an AUC score of 0.966347, and an F1 score of 0.94803\nin the cross-validation set. In the test set, ALLNet achieved an accuracy of\n92.0991%, a sensitivity of 96.5446%, a specificity of 82.8035%, an AUC score of\n0.959972, and an F1 score of 0.942963. The utilization of ALLNet in the\nclinical workspace can better treat the thousands of people suffering from ALL\nacross the world, many of whom are children.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mattapalli_S/0/1/0/all/0/1\">Sai Mattapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athavale_R/0/1/0/all/0/1\">Rishi Athavale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised learning for joint SAR and multispectral land cover classification. (arXiv:2108.09075v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.09075","description":"<p>Semi-supervised learning techniques are gaining popularity due to their\ncapability of building models that are effective, even when scarce amounts of\nlabeled data are available. In this paper, we present a framework and specific\ntasks for self-supervised pretraining of \\textit{multichannel} models, such as\nthe fusion of multispectral and synthetic aperture radar images. We show that\nthe proposed self-supervised approach is highly effective at learning features\nthat correlate with the labels for land cover classification. This is enabled\nby an explicit design of pretraining tasks which promotes bridging the gaps\nbetween sensing modalities and exploiting the spectral characteristics of the\ninput. In a semi-supervised setting, when limited labels are available, using\nthe proposed self-supervised pretraining, followed by supervised finetuning for\nland cover classification with SAR and multispectral data, outperforms\nconventional approaches such as purely supervised learning, initialization from\ntraining on ImageNet and other recent self-supervised approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Montanaro_A/0/1/0/all/0/1\">Antonio Montanaro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valsesia_D/0/1/0/all/0/1\">Diego Valsesia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fracastoro_G/0/1/0/all/0/1\">Giulia Fracastoro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Magli_E/0/1/0/all/0/1\">Enrico Magli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TACS: Taxonomy Adaptive Cross-Domain Semantic Segmentation. (arXiv:2109.04813v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04813","description":"<p>Traditional domain adaptive semantic segmentation addresses the task of\nadapting a model to a novel target domain under limited or no additional\nsupervision. While tackling the input domain gap, the standard domain\nadaptation settings assume no domain change in the output space. In semantic\nprediction tasks, different datasets are often labeled according to different\nsemantic taxonomies. In many real-world settings, the target domain task\nrequires a different taxonomy than the one imposed by the source domain. We\ntherefore introduce the more general taxonomy adaptive cross-domain semantic\nsegmentation (TACS) problem, allowing for inconsistent taxonomies between the\ntwo domains. We further propose an approach that jointly addresses the\nimage-level and label-level domain adaptation. On the label-level, we employ a\nbilateral mixed sampling strategy to augment the target domain, and a\nrelabelling method to unify and align the label spaces. We address the\nimage-level domain gap by proposing an uncertainty-rectified contrastive\nlearning method, leading to more domain-invariant and class-discriminative\nfeatures. We extensively evaluate the effectiveness of our framework under\ndifferent TACS settings: open taxonomy, coarse-to-fine taxonomy, and\nimplicitly-overlapping taxonomy. Our approach outperforms the previous\nstate-of-the-art by a large margin, while being capable of adapting to target\ntaxonomies. Our implementation is publicly available at\nhttps://github.com/ETHRuiGong/TADA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Rui Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1\">Ajad Chhatkuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFNet: Multi-class Few-shot Segmentation Network with Pixel-wise Metric Learning. (arXiv:2111.00232v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00232","description":"<p>In visual recognition tasks, few-shot learning requires the ability to learn\nobject categories with few support examples. Its re-popularity in light of the\ndeep learning development is mainly in image classification. This work focuses\non few-shot semantic segmentation, which is still a largely unexplored field. A\nfew recent advances are often restricted to single-class few-shot segmentation.\nIn this paper, we first present a novel multi-way (class) encoding and decoding\narchitecture which effectively fuses multi-scale query information and\nmulti-class support information into one query-support embedding. Multi-class\nsegmentation is directly decoded upon this embedding. For better feature\nfusion, a multi-level attention mechanism is proposed within the architecture,\nwhich includes the attention for support feature modulation and attention for\nmulti-scale combination. Last, to enhance the embedding space learning, an\nadditional pixel-wise metric learning module is introduced with triplet loss\nformulated on the pixel-level embedding of the input image. Extensive\nexperiments on standard benchmarks PASCAL-5i and COCO-20i show clear benefits\nof our method over the state of the art in few-shot segmentation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions. (arXiv:2112.00246v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00246","description":"<p>Perceiving and interacting with 3D articulated objects, such as cabinets,\ndoors, and faucets, pose particular challenges for future home-assistant robots\nperforming daily tasks in human environments. Besides parsing the articulated\nparts and joint parameters, researchers recently advocate learning manipulation\naffordance over the input shape geometry which is more task-aware and\ngeometrically fine-grained. However, taking only passive observations as\ninputs, these methods ignore many hidden but important kinematic constraints\n(e.g., joint location and limits) and dynamic factors (e.g., joint friction and\nrestitution), therefore losing significant accuracy for test cases with such\nuncertainties. In this paper, we propose a novel framework, named AdaAfford,\nthat learns to perform very few test-time interactions for quickly adapting the\naffordance priors to more accurate instance-specific posteriors. We conduct\nlarge-scale experiments using the PartNet-Mobility dataset and prove that our\nsystem performs better than baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruihai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1\">Jiaqi Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Entity Tagging with Multimodal Knowledge Base. (arXiv:2201.00693v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2201.00693","description":"<p>To enhance research on multimodal knowledge base and multimodal information\nprocessing, we propose a new task called multimodal entity tagging (MET) with a\nmultimodal knowledge base (MKB). We also develop a dataset for the problem\nusing an existing MKB. In an MKB, there are entities and their associated texts\nand images. In MET, given a text-image pair, one uses the information in the\nMKB to automatically identify the related entity in the text-image pair. We\nsolve the task by using the information retrieval paradigm and implement\nseveral baselines using state-of-the-art methods in NLP and CV. We conduct\nextensive experiments and make analyses on the experimental results. The\nresults show that the task is challenging, but current technologies can achieve\nrelatively high performance. We will release the dataset, code, and models for\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1\">Chao Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Snapshot Spectral Compressive Imaging Reconstruction Using Convolution and Contextual Transformer. (arXiv:2201.05768v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.05768","description":"<p>Spectral compressive imaging (SCI) is able to encode the high-dimensional\nhyperspectral image to a 2D measurement, and then uses algorithms to\nreconstruct the spatio-spectral data-cube. At present, the main bottleneck of\nSCI is the reconstruction algorithm, and the state-of-the-art (SOTA)\nreconstruction methods generally face the problem of long reconstruction time\nand/or poor detail recovery. In this paper, we propose a novel hybrid network\nmodule, namely CCoT (Convolution and Contextual Transformer) block, which can\nacquire the inductive bias ability of convolution and the powerful modeling\nability of transformer simultaneously,and is conducive to improving the quality\nof reconstruction to restore fine details. We integrate the proposed CCoT block\ninto deep unfolding framework based on the generalized alternating projection\nalgorithm, and further propose the GAP-CCoT network. Through the experiments of\nextensive synthetic and real data, our proposed model achieves higher\nreconstruction quality ($&gt;$2dB in PSNR on simulated benchmark datasets) and\nshorter running time than existing SOTA algorithms by a large margin. The code\nand models are publicly available at https://github.com/ucaswangls/GAP-CCoT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lishun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1\">Zongliang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_Y/0/1/0/all/0/1\">Yong Zhong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransDARC: Transformer-based Driver Activity Recognition with Latent Space Feature Calibration. (arXiv:2203.00927v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00927","description":"<p>Traditional video-based human activity recognition has experienced remarkable\nprogress linked to the rise of deep learning, but this effect was slower as it\ncomes to the downstream task of driver behavior understanding. Understanding\nthe situation inside the vehicle cabin is essential for Advanced Driving\nAssistant System (ADAS) as it enables identifying distraction, predicting\ndriver's intent and leads to more convenient human-vehicle interaction. At the\nsame time, driver observation systems face substantial obstacles as they need\nto capture different granularities of driver states, while the complexity of\nsuch secondary activities grows with the rising automation and increased driver\nfreedom. Furthermore, a model is rarely deployed under conditions identical to\nthe ones in the training set, as sensor placements and types vary from vehicle\nto vehicle, constituting a substantial obstacle for real-life deployment of\ndata-driven models. In this work, we present a novel vision-based framework for\nrecognizing secondary driver behaviours based on visual transformers and an\nadditional augmented feature distribution calibration module. This module\noperates in the latent feature-space enriching and diversifying the training\nset at feature-level in order to improve generalization to novel data\nappearances, (e.g., sensor changes) and general feature quality. Our framework\nconsistently leads to better recognition rates, surpassing previous\nstate-of-the-art results of the public Drive&amp;Act benchmark on all granularity\nlevels. Our code is publicly available at\nhttps://github.com/KPeng9510/TransDARC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Familiarity Hypothesis: Explaining the Behavior of Deep Open Set Methods. (arXiv:2203.02486v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02486","description":"<p>In many object recognition applications, the set of possible categories is an\nopen set, and the deployed recognition system will encounter novel objects\nbelonging to categories unseen during training. Detecting such \"novel category\"\nobjects is usually formulated as an anomaly detection problem. Anomaly\ndetection algorithms for feature-vector data identify anomalies as outliers,\nbut outlier detection has not worked well in deep learning. Instead, methods\nbased on the computed logits of visual object classifiers give state-of-the-art\nperformance. This paper proposes the Familiarity Hypothesis that these methods\nsucceed because they are detecting the absence of familiar learned features\nrather than the presence of novelty. This distinction is important, because\nfamiliarity-based detection will fail in many situations where novelty is\npresent. For example when an image contains both a novel object and a familiar\none, the familiarity score will be high, so the novel object will not be\nnoticed. The paper reviews evidence from the literature and presents additional\nevidence from our own experiments that provide strong support for this\nhypothesis. The paper concludes with a discussion of whether familiarity-based\ndetection is an inevitable consequence of representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dietterich_T/0/1/0/all/0/1\">Thomas G. Dietterich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyer_A/0/1/0/all/0/1\">Alexander Guyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Stream Multi-Level Alignment for Vision-Language Pretraining. (arXiv:2203.14395v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14395","description":"<p>Self-supervised vision-language pretraining from pure images and text with a\ncontrastive loss is effective, but ignores fine-grained alignment due to a\ndual-stream architecture that aligns image and text representations only on a\nglobal level. Earlier, supervised, non-contrastive methods were capable of\nfiner-grained alignment, but required dense annotations that were not scalable.\nWe propose a single stream architecture that aligns images and language at\nmultiple levels: global, fine-grained patch-token, and conceptual/semantic,\nusing two novel tasks: symmetric cross-modality reconstruction (XMM) and a\npseudo-labeled key word prediction (PSL). In XMM, we mask input tokens from one\nmodality and use cross-modal information to reconstruct the masked token, thus\nimproving fine-grained alignment between the two modalities. In PSL, we use\nattention to select keywords in a caption, use a momentum encoder to recommend\nother important keywords that are missing from the caption but represented in\nthe image, and then train the visual encoder to predict the presence of those\nkeywords, helping it learn semantic concepts that are essential for grounding a\ntextual token to an image region. We demonstrate competitive performance and\nimproved data efficiency on image-text retrieval, grounding, visual question\nanswering/reasoning against larger models and models trained on more data. Code\nand models available at zaidkhan.me/SIMLA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zaid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+BG_V/0/1/0/all/0/1\">Vijay Kumar BG</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulter_S/0/1/0/all/0/1\">Samuel Schulter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mc-BEiT: Multi-choice Discretization for Image BERT Pre-training. (arXiv:2203.15371v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15371","description":"<p>Image BERT pre-training with masked image modeling (MIM) becomes a popular\npractice to cope with self-supervised representation learning. A seminal work,\nBEiT, casts MIM as a classification task with a visual vocabulary, tokenizing\nthe continuous visual signals into discrete vision tokens using a pre-learned\ndVAE. Despite a feasible solution, the improper discretization hinders further\nimprovements of image pre-training. Since image discretization has no\nground-truth answers, we believe that the masked patch should not be assigned\nwith a unique token id even if a better tokenizer can be obtained. In this\nwork, we introduce an improved BERT-style image pre-training method, namely\nmc-BEiT, which performs MIM proxy tasks towards eased and refined multi-choice\ntraining objectives. Specifically, the multi-choice supervision for the masked\nimage patches is formed by the soft probability vectors of the discrete token\nids, which are predicted by the off-the-shelf image tokenizer and further\nrefined by high-level inter-patch perceptions resorting to the observation that\nsimilar patches should share their choices. Extensive experiments on\nclassification, segmentation, and detection tasks demonstrate the superiority\nof our method, e.g., the pre-trained ViT-B achieves 84.1% top-1 fine-tuning\naccuracy on ImageNet-1K classification, 49.2% AP^b and 44.0% AP^m of object\ndetection and instance segmentation on COCO, 50.8% mIOU on ADE20K semantic\nsegmentation, outperforming the competitive counterparts. The code will be\navailable at https://github.com/lixiaotong97/mc-BEiT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaotong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zixuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generative Deep Learning Approach to Stochastic Downscaling of Precipitation Forecasts. (arXiv:2204.02028v2 [physics.ao-ph] UPDATED)","link":"http://arxiv.org/abs/2204.02028","description":"<p>Despite continuous improvements, precipitation forecasts are still not as\naccurate and reliable as those of other meteorological variables. A major\ncontributing factor to this is that several key processes affecting\nprecipitation distribution and intensity occur below the resolved scale of\nglobal weather models. Generative adversarial networks (GANs) have been\ndemonstrated by the computer vision community to be successful at\nsuper-resolution problems, i.e., learning to add fine-scale structure to coarse\nimages. Leinonen et al. (2020) previously applied a GAN to produce ensembles of\nreconstructed high-resolution atmospheric fields, given coarsened input data.\nIn this paper, we demonstrate this approach can be extended to the more\nchallenging problem of increasing the accuracy and resolution of comparatively\nlow-resolution input from a weather forecasting model, using high-resolution\nradar measurements as a \"ground truth\". The neural network must learn to add\nresolution and structure whilst accounting for non-negligible forecast error.\nWe show that GANs and VAE-GANs can match the statistical properties of\nstate-of-the-art pointwise post-processing methods whilst creating\nhigh-resolution, spatially coherent precipitation maps. Our model compares\nfavourably to the best existing downscaling methods in both pixel-wise and\npooled CRPS scores, power spectrum information and rank histograms (used to\nassess calibration). We test our models and show that they perform in a range\nof scenarios, including heavy rainfall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Harris_L/0/1/0/all/0/1\">Lucy Harris</a>, <a href=\"http://arxiv.org/find/physics/1/au:+McRae_A/0/1/0/all/0/1\">Andrew T. T. McRae</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chantry_M/0/1/0/all/0/1\">Matthew Chantry</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dueben_P/0/1/0/all/0/1\">Peter D. Dueben</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Palmer_T/0/1/0/all/0/1\">Tim N. Palmer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demonstrate Once, Imitate Immediately (DOME): Learning Visual Servoing for One-Shot Imitation Learning. (arXiv:2204.02863v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2204.02863","description":"<p>We present DOME, a novel method for one-shot imitation learning, where a task\ncan be learned from just a single demonstration and then be deployed\nimmediately, without any further data collection or training. DOME does not\nrequire prior task or object knowledge, and can perform the task in novel\nobject configurations and with distractors. At its core, DOME uses an\nimage-conditioned object segmentation network followed by a learned visual\nservoing network, to move the robot's end-effector to the same relative pose to\nthe object as during the demonstration, after which the task can be completed\nby replaying the demonstration's end-effector velocities. We show that DOME\nachieves near 100% success rate on 7 real-world everyday tasks, and we perform\nseveral studies to thoroughly understand each individual component of DOME.\nVideos and supplementary material are available at:\nhttps://www.robot-learning.uk/dome .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1\">Eugene Valassakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papagiannis_G/0/1/0/all/0/1\">Georgios Papagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1\">Norman Di Palo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO. (arXiv:2204.03359v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03359","description":"<p>Image-Text matching (ITM) is a common task for evaluating the quality of\nVision and Language (VL) models. However, existing ITM benchmarks have a\nsignificant limitation. They have many missing correspondences, originating\nfrom the data construction process itself. For example, a caption is only\nmatched with one image although the caption can be matched with other similar\nimages and vice versa. To correct the massive false negatives, we construct the\nExtended COCO Validation (ECCV) Caption dataset by supplying the missing\nassociations with machine and human annotators. We employ five state-of-the-art\nITM models with diverse properties for our annotation process. Our dataset\nprovides x3.6 positive image-to-caption associations and x8.5 caption-to-image\nassociations compared to the original MS-COCO. We also propose to use an\ninformative ranking-based metric mAP@R, rather than the popular Recall@K (R@K).\nWe re-evaluate the existing 25 VL models on existing and proposed benchmarks.\nOur findings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K\nR@K, CxC R@1 are highly correlated with each other, while the rankings change\nwhen we shift to the ECCV mAP@R. Lastly, we delve into the effect of the bias\nintroduced by the choice of machine annotator. Source code and dataset are\navailable at https://github.com/naver-ai/eccv-caption\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wonjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Song Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Minsuk Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seong Joon Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Few-Shot Part Segmentation using Coarse Supervision. (arXiv:2204.05393v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05393","description":"<p>A significant bottleneck in training deep networks for part segmentation is\nthe cost of obtaining detailed annotations. We propose a framework to exploit\ncoarse labels such as figure-ground masks and keypoint locations that are\nreadily available for some categories to improve part segmentation models. A\nkey challenge is that these annotations were collected for different tasks and\nwith different labeling styles and cannot be readily mapped to the part labels.\nTo this end, we propose to jointly learn the dependencies between labeling\nstyles and the part segmentation model, allowing us to utilize supervision from\ndiverse labels. To evaluate our approach we develop a benchmark on the\nCaltech-UCSD birds and OID Aircraft dataset. Our approach outperforms baselines\nbased on multi-task learning, semi-supervised learning, and competitive methods\nrelying on loss functions manually designed to exploit sparse-supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_O/0/1/0/all/0/1\">Oindrila Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zezhou Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reliable Visual Question Answering: Abstain Rather Than Answer Incorrectly. (arXiv:2204.13631v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13631","description":"<p>Machine learning has advanced dramatically, narrowing the accuracy gap to\nhumans in multimodal tasks like visual question answering (VQA). However, while\nhumans can say \"I don't know\" when they are uncertain (i.e., abstain from\nanswering a question), such ability has been largely neglected in multimodal\nresearch, despite the importance of this problem to the usage of VQA in real\nsettings. In this work, we promote a problem formulation for reliable VQA,\nwhere we prefer abstention over providing an incorrect answer. We first enable\nabstention capabilities for several VQA models, and analyze both their\ncoverage, the portion of questions answered, and risk, the error on that\nportion. For that, we explore several abstention approaches. We find that\nalthough the best performing models achieve over 71% accuracy on the VQA v2\ndataset, introducing the option to abstain by directly using a model's softmax\nscores limits them to answering less than 8% of the questions to achieve a low\nrisk of error (i.e., 1%). This motivates us to utilize a multimodal selection\nfunction to directly estimate the correctness of the predicted answers, which\nwe show can increase the coverage by, for example, 2.4x from 6.8% to 16.3% at\n1% risk. While it is important to analyze both coverage and risk, these metrics\nhave a trade-off which makes comparing VQA models challenging. To address this,\nwe also propose an Effective Reliability metric for VQA that places a larger\ncost on incorrect answers compared to abstentions. This new problem\nformulation, metric, and analysis for VQA provide the groundwork for building\neffective and reliable VQA models that have the self-awareness to abstain if\nand only if they don't know the answer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1\">Spencer Whitehead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petryk_S/0/1/0/all/0/1\">Suzanne Petryk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakib_V/0/1/0/all/0/1\">Vedaad Shakib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinction Maximization Loss: Efficiently Improving Uncertainty Estimation and Out-of-Distribution Detection by Simply Replacing the Loss and Calibrating. (arXiv:2205.05874v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.05874","description":"<p>Building robust deterministic neural networks remains a challenge. On the one\nhand, some approaches improve out-of-distribution detection at the cost of\nreducing classification accuracy in some situations. On the other hand, some\nmethods simultaneously increase classification accuracy, uncertainty\nestimation, and out-of-distribution detection at the expense of reducing the\ninference efficiency. In this paper, we propose training deterministic neural\nnetworks using our DisMax loss, which works as a drop-in replacement for the\nusual SoftMax loss (i.e., the combination of the linear output layer, the\nSoftMax activation, and the cross-entropy loss). Starting from the IsoMax+\nloss, we create each logit based on the distances to all prototypes, rather\nthan just the one associated with the correct class. We also introduce a\nmechanism to combine images to construct what we call fractional probability\nregularization. Moreover, we present a fast way to calibrate the network after\ntraining. Finally, we propose a composite score to perform out-of-distribution\ndetection. Our experiments show that DisMax usually outperforms current\napproaches simultaneously in classification accuracy, uncertainty estimation,\nand out-of-distribution detection while maintaining deterministic neural\nnetwork inference efficiency. The code to reproduce the results is available at\nhttps://github.com/dlmacedo/distinction-maximization-loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localized Vision-Language Matching for Open-vocabulary Object Detection. (arXiv:2205.06160v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.06160","description":"<p>In this work, we propose an open-vocabulary object detection method that,\nbased on image-caption pairs, learns to detect novel object classes along with\na given set of known classes. It is a two-stage training approach that first\nuses a location-guided image-caption matching technique to learn class labels\nfor both novel and known classes in a weakly-supervised manner and second\nspecializes the model for the object detection task using known class\nannotations. We show that a simple language model fits better than a large\ncontextualized language model for detecting novel objects. Moreover, we\nintroduce a consistency-regularization technique to better exploit\nimage-caption pair information. Our method compares favorably to existing\nopen-vocabulary detection approaches while being data-efficient. Source code is\navailable at https://github.com/lmb-freiburg/locov .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bravo_M/0/1/0/all/0/1\">Maria A. Bravo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1\">Sudhanshu Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TBraTS: Trusted Brain Tumor Segmentation. (arXiv:2206.09309v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.09309","description":"<p>Despite recent improvements in the accuracy of brain tumor segmentation, the\nresults still exhibit low levels of confidence and robustness. Uncertainty\nestimation is one effective way to change this situation, as it provides a\nmeasure of confidence in the segmentation results. In this paper, we propose a\ntrusted brain tumor segmentation network which can generate robust segmentation\nresults and reliable uncertainty estimations without excessive computational\nburden and modification of the backbone network. In our method, uncertainty is\nmodeled explicitly using subjective logic theory, which treats the predictions\nof backbone neural network as subjective opinions by parameterizing the class\nprobabilities of the segmentation as a Dirichlet distribution. Meanwhile, the\ntrusted segmentation framework learns the function that gathers reliable\nevidence from the feature leading to the final segmentation results. Overall,\nour unified trusted segmentation framework endows the model with reliability\nand robustness to out-of-distribution samples. To evaluate the effectiveness of\nour model in robustness and reliability, qualitative and quantitative\nexperiments are conducted on the BraTS 2019 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zou_K/0/1/0/all/0/1\">Ke Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xuedong Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_X/0/1/0/all/0/1\">Xiaojing Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation with Representative Teacher Keys Based on Attention Mechanism for Image Classification Model Compression. (arXiv:2206.12788v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.12788","description":"<p>With the improvement of AI chips (e.g., GPU, TPU, and NPU) and the fast\ndevelopment of the Internet of Things (IoT), some robust deep neural networks\n(DNNs) are usually composed of millions or even hundreds of millions of\nparameters. Such a large model may not be suitable for directly deploying on\nlow computation and low capacity units (e.g., edge devices). Knowledge\ndistillation (KD) has recently been recognized as a powerful model compression\nmethod to decrease the model parameters effectively. The central concept of KD\nis to extract useful information from the feature maps of a large model (i.e.,\nteacher model) as a reference to successfully train a small model (i.e.,\nstudent model) in which the model size is much smaller than the teacher one.\nAlthough many KD methods have been proposed to utilize the information from the\nfeature maps of intermediate layers in the teacher model, most did not consider\nthe similarity of feature maps between the teacher model and the student model.\nAs a result, it may make the student model learn useless information. Inspired\nby the attention mechanism, we propose a novel KD method called representative\nteacher key (RTK) that not only considers the similarity of feature maps but\nalso filters out the useless information to improve the performance of the\ntarget student model. In the experiments, we validate our proposed method with\nseveral backbone networks (e.g., ResNet and WideResNet) and datasets (e.g.,\nCIFAR10, CIFAR100, SVHN, and CINIC10). The results show that our proposed RTK\ncan effectively improve the classification accuracy of the state-of-the-art\nattention-based KD method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jun-Teng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_S/0/1/0/all/0/1\">Sheng-Che Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Scott C.-H. Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Modelling With Inverse Heat Dissipation. (arXiv:2206.13397v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13397","description":"<p>While diffusion models have shown great success in image generation, their\nnoise-inverting generative process does not explicitly consider the structure\nof images, such as their inherent multi-scale nature. Inspired by diffusion\nmodels and the desirability of coarse-to-fine modelling, we propose a new model\nthat generates images through iteratively inverting the heat equation, a PDE\nthat locally erases fine-scale information when run over the 2D plane of the\nimage. In our novel methodology, the solution of the forward heat equation is\ninterpreted as a variational approximation in a directed graphical model. We\ndemonstrate promising image quality and point out emergent qualitative\nproperties not seen in diffusion models, such as disentanglement of overall\ncolour and shape in images and aspects of neural network interpretability.\nSpectral analysis on natural images positions our model as a type of dual to\ndiffusion models and reveals implicit inductive biases in them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rissanen_S/0/1/0/all/0/1\">Severi Rissanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinonen_M/0/1/0/all/0/1\">Markus Heinonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1\">Arno Solin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenLDN: Learning to Discover Novel Classes for Open-World Semi-Supervised Learning. (arXiv:2207.02261v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02261","description":"<p>Semi-supervised learning (SSL) is one of the dominant approaches to address\nthe annotation bottleneck of supervised learning. Recent SSL methods can\neffectively leverage a large repository of unlabeled data to improve\nperformance while relying on a small set of labeled data. One common assumption\nin most SSL methods is that the labeled and unlabeled data are from the same\ndata distribution. However, this is hardly the case in many real-world\nscenarios, which limits their applicability. In this work, instead, we attempt\nto solve the challenging open-world SSL problem that does not make such an\nassumption. In the open-world SSL problem, the objective is to recognize\nsamples of known classes, and simultaneously detect and cluster samples\nbelonging to novel classes present in unlabeled data. This work introduces\nOpenLDN that utilizes a pairwise similarity loss to discover novel classes.\nUsing a bi-level optimization rule this pairwise similarity loss exploits the\ninformation available in the labeled set to implicitly cluster novel class\nsamples, while simultaneously recognizing samples from known classes. After\ndiscovering novel classes, OpenLDN transforms the open-world SSL problem into a\nstandard SSL problem to achieve additional performance gains using existing SSL\nmethods. Our extensive experiments demonstrate that OpenLDN outperforms the\ncurrent state-of-the-art methods on multiple popular classification benchmarks\nwhile providing a better accuracy/training time trade-off.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rizve_M/0/1/0/all/0/1\">Mamshad Nayeem Rizve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1\">Navid Kardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Realistic Semi-Supervised Learning. (arXiv:2207.02269v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02269","description":"<p>Deep learning is pushing the state-of-the-art in many computer vision\napplications. However, it relies on large annotated data repositories, and\ncapturing the unconstrained nature of the real-world data is yet to be solved.\nSemi-supervised learning (SSL) complements the annotated training data with a\nlarge corpus of unlabeled data to reduce annotation cost. The standard SSL\napproach assumes unlabeled data are from the same distribution as annotated\ndata. Recently, a more realistic SSL problem, called open-world SSL, is\nintroduced, where the unannotated data might contain samples from unknown\nclasses. In this paper, we propose a novel pseudo-label based approach to\ntackle SSL in open-world setting. At the core of our method, we utilize sample\nuncertainty and incorporate prior knowledge about class distribution to\ngenerate reliable class-distribution-aware pseudo-labels for unlabeled data\nbelonging to both known and unknown classes. Our extensive experimentation\nshowcases the effectiveness of our approach on several benchmark datasets,\nwhere it substantially outperforms the existing state-of-the-art on seven\ndiverse datasets including CIFAR-100 (~17%), ImageNet-100 (~5%), and Tiny\nImageNet (~9%). We also highlight the flexibility of our approach in solving\nnovel class discovery task, demonstrate its stability in dealing with\nimbalanced data, and complement our approach with a technique to estimate the\nnumber of novel classes\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rizve_M/0/1/0/all/0/1\">Mamshad Nayeem Rizve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1\">Navid Kardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence. (arXiv:2207.04630v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2207.04630","description":"<p>Ten years into the revival of deep networks and artificial intelligence, we\npropose a theoretical framework that sheds light on understanding deep networks\nwithin a bigger picture of Intelligence in general. We introduce two\nfundamental principles, Parsimony and Self-consistency, that address two\nfundamental questions regarding Intelligence: what to learn and how to learn,\nrespectively. We believe the two principles are the cornerstones for the\nemergence of Intelligence, artificial or natural. While these two principles\nhave rich classical roots, we argue that they can be stated anew in entirely\nmeasurable and computable ways. More specifically, the two principles lead to\nan effective and efficient computational framework, compressive closed-loop\ntranscription, that unifies and explains the evolution of modern deep networks\nand many artificial intelligence practices. While we mainly use modeling of\nvisual data as an example, we believe the two principles will unify\nunderstanding of broad families of autonomous intelligent systems and provide a\nframework for understanding the brain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_D/0/1/0/all/0/1\">Doris Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Heung-Yeung Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Grand Unification of Object Tracking. (arXiv:2207.07078v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07078","description":"<p>We present a unified method, termed Unicorn, that can simultaneously solve\nfour tracking problems (SOT, MOT, VOS, MOTS) with a single network using the\nsame model parameters. Due to the fragmented definitions of the object tracking\nproblem itself, most existing trackers are developed to address a single or\npart of tasks and overspecialize on the characteristics of specific tasks. By\ncontrast, Unicorn provides a unified solution, adopting the same input,\nbackbone, embedding, and head across all tracking tasks. For the first time, we\naccomplish the great unification of the tracking network architecture and\nlearning paradigm. Unicorn performs on-par or better than its task-specific\ncounterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17,\nBDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will\nserve as a solid step towards the general vision model. Code is available at\nhttps://github.com/MasterBin-IIAU/Unicorn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Lightweight Super-Resolution with Dual Regression Learning. (arXiv:2207.07929v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07929","description":"<p>Deep neural networks have exhibited remarkable performance in image\nsuper-resolution (SR) tasks by learning a mapping from low-resolution (LR)\nimages to high-resolution (HR) images. However, the SR problem is typically an\nill-posed problem and existing methods would come with several limitations.\nFirst, the possible mapping space of SR can be extremely large since there may\nexist many different HR images that can be downsampled to the same LR image. As\na result, it is hard to directly learn a promising SR mapping from such a large\nspace. Second, it is often inevitable to develop very large models with\nextremely high computational cost to yield promising SR performance. In\npractice, one can use model compression techniques to obtain compact models by\nreducing model redundancy. Nevertheless, it is hard for existing model\ncompression methods to accurately identify the redundant components due to the\nextremely large SR mapping space. To alleviate the first challenge, we propose\na dual regression learning scheme to reduce the space of possible SR mappings.\nSpecifically, in addition to the mapping from LR to HR images, we learn an\nadditional dual regression mapping to estimate the downsampling kernel and\nreconstruct LR images. In this way, the dual mapping acts as a constraint to\nreduce the space of possible mappings. To address the second challenge, we\npropose a lightweight dual regression compression method to reduce model\nredundancy in both layer-level and channel-level based on channel pruning.\nSpecifically, we first develop a channel number search method that minimizes\nthe dual regression loss to determine the redundancy of each layer. Given the\nsearched channel numbers, we further exploit the dual regression manner to\nevaluate the importance of channels and prune the redundant ones. Extensive\nexperiments show the effectiveness of our method in obtaining accurate and\nefficient SR models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiezhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zeshuai Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OTPose: Occlusion-Aware Transformer for Pose Estimation in Sparsely-Labeled Videos. (arXiv:2207.09725v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09725","description":"<p>Although many approaches for multi-human pose estimation in videos have shown\nprofound results, they require densely annotated data which entails excessive\nman labor. Furthermore, there exists occlusion and motion blur that inevitably\nlead to poor estimation performance. To address these problems, we propose a\nmethod that leverages an attention mask for occluded joints and encodes\ntemporal dependency between frames using transformers. First, our framework\ncomposes different combinations of sparsely annotated frames that denote the\ntrack of the overall joint movement. We propose an occlusion attention mask\nfrom these combinations that enable encoding occlusion-aware heatmaps as a\nsemi-supervised task. Second, the proposed temporal encoder employs transformer\narchitecture to effectively aggregate the temporal relationship and\nkeypoint-wise attention from each time step and accurately refines the target\nframe's final pose estimation. We achieve state-of-the-art pose estimation\nresults for PoseTrack2017 and PoseTrack2018 datasets and demonstrate the\nrobustness of our approach to occlusion and motion blur in sparsely annotated\nvideo data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1\">Kyung-Min Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gun-Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta. (arXiv:2207.10271v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10271","description":"<p>Learning to generate new images for a novel category based on only a few\nimages, named as few-shot image generation, has attracted increasing research\ninterest. Several state-of-the-art works have yielded impressive results, but\nthe diversity is still limited. In this work, we propose a novel Delta\nGenerative Adversarial Network (DeltaGAN), which consists of a reconstruction\nsubnetwork and a generation subnetwork. The reconstruction subnetwork captures\nintra-category transformation, i.e., delta, between same-category pairs. The\ngeneration subnetwork generates sample-specific delta for an input image, which\nis combined with this input image to generate a new image within the same\ncategory. Besides, an adversarial delta matching loss is designed to link the\nabove two subnetworks together. Extensive experiments on six benchmark datasets\ndemonstrate the effectiveness of our proposed method. Our code is available at\nhttps://github.com/bcmi/DeltaGAN-Few-Shot-Image-Generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianfu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance Fields. (arXiv:2207.10312v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10312","description":"<p>Novel view synthesis has recently been revolutionized by learning neural\nradiance fields directly from sparse observations. However, rendering images\nwith this new paradigm is slow due to the fact that an accurate quadrature of\nthe volume rendering equation requires a large number of samples for each ray.\nPrevious work has mainly focused on speeding up the network evaluations that\nare associated with each sample point, e.g., via caching of radiance values\ninto explicit spatial data structures, but this comes at the expense of model\ncompactness. In this paper, we propose a novel dual-network architecture that\ntakes an orthogonal direction by learning how to best reduce the number of\nrequired sample points. To this end, we split our network into a sampling and\nshading network that are jointly trained. Our training scheme employs fixed\nsample positions along each ray, and incrementally introduces sparsity\nthroughout training to achieve high quality even at low sample counts. After\nfine-tuning with the target number of samples, the resulting compact neural\nrepresentation can be rendered in real-time. Our experiments demonstrate that\nour approach outperforms concurrent compact neural representations in terms of\nquality and frame rate and performs on par with highly efficient hybrid\nrepresentations. Code and supplementary material is available at\nhttps://thomasneff.github.io/adanerf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurz_A/0/1/0/all/0/1\">Andreas Kurz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neff_T/0/1/0/all/0/1\">Thomas Neff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhaoyang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1\">Michael Zollh&#xf6;fer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinberger_M/0/1/0/all/0/1\">Markus Steinberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Patch-Based Neural Rendering. (arXiv:2207.10662v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10662","description":"<p>Neural rendering has received tremendous attention since the advent of Neural\nRadiance Fields (NeRF), and has pushed the state-of-the-art on novel-view\nsynthesis considerably. The recent focus has been on models that overfit to a\nsingle scene, and the few attempts to learn models that can synthesize novel\nviews of unseen scenes mostly consist of combining deep convolutional features\nwith a NeRF-like model. We propose a different paradigm, where no deep features\nand no NeRF-like volume rendering are needed. Our method is capable of\npredicting the color of a target ray in a novel scene directly, just from a\ncollection of patches sampled from the scene. We first leverage epipolar\ngeometry to extract patches along the epipolar lines of each reference view.\nEach patch is linearly projected into a 1D feature vector and a sequence of\ntransformers process the collection. For positional encoding, we parameterize\nrays as in a light field representation, with the crucial difference that the\ncoordinates are canonicalized with respect to the target ray, which makes our\nmethod independent of the reference frame and improves generalization. We show\nthat our approach outperforms the state-of-the-art on novel view synthesis of\nunseen scenes even when being trained with considerably less data than prior\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suhail_M/0/1/0/all/0/1\">Mohammed Suhail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteves_C/0/1/0/all/0/1\">Carlos Esteves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1\">Leonid Sigal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1\">Ameesh Makadia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Object Counting and Detection. (arXiv:2207.10988v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10988","description":"<p>We tackle a new task of few-shot object counting and detection. Given a few\nexemplar bounding boxes of a target object class, we seek to count and detect\nall objects of the target class. This task shares the same supervision as the\nfew-shot object counting but additionally outputs the object bounding boxes\nalong with the total object count. To address this challenging problem, we\nintroduce a novel two-stage training strategy and a novel uncertainty-aware\nfew-shot object detector: Counting-DETR. The former is aimed at generating\npseudo ground-truth bounding boxes to train the latter. The latter leverages\nthe pseudo ground-truth provided by the former but takes the necessary steps to\naccount for the imperfection of pseudo ground-truth. To validate the\nperformance of our method on the new task, we introduce two new datasets named\nFSCD-147 and FSCD-LVIS. Both datasets contain images with complex scenes,\nmultiple object classes per image, and a huge variation in object shapes,\nsizes, and appearance. Our proposed approach outperforms very strong baselines\nadapted from few-shot object counting and few-shot object detection with a\nlarge margin in both counting and detection metrics. The code and models are\navailable at https://github.com/VinAIResearch/Counting-DETR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1\">Chau Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoai_M/0/1/0/all/0/1\">Minh Hoai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Video Captioning with Evolving Pseudo-Tokens. (arXiv:2207.11100v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.11100","description":"<p>We introduce a zero-shot video captioning method that employs two frozen\nnetworks: the GPT-2 language model and the CLIP image-text matching model. The\nmatching score is used to steer the language model toward generating a sentence\nthat has a high average matching score to a subset of the video frames. Unlike\nzero-shot image captioning methods, our work considers the entire sentence at\nonce. This is achieved by optimizing, during the generation process, part of\nthe prompt from scratch, by modifying the representation of all other tokens in\nthe prompt, and by repeating the process iteratively, gradually improving the\nspecificity and comprehensiveness of the generated sentence. Our experiments\nshow that the generated captions are coherent and display a broad range of\nreal-world knowledge. Our code is available at:\nhttps://github.com/YoadTew/zero-shot-video-to-text\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tewel_Y/0/1/0/all/0/1\">Yoad Tewel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Y/0/1/0/all/0/1\">Yoav Shalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadler_R/0/1/0/all/0/1\">Roy Nadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Riemannian Geometry Approach for Minimizing Distortion and its Applications. (arXiv:2207.12038v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.12038","description":"<p>Given an affine transformation $T$, we define its Fisher distortion\n$Dist_F(T)$. We show that the Fisher distortion has Riemannian metric structure\nand provide an algorithm for finding mean distorting transformation -- namely\n-- for a given set $\\{T_{i}\\}_{i=1}^N$ of affine transformations, find an\naffine transformation $T$ that minimize the overall distortion\n$\\sum_{i=1}^NDist_F^{2}(T^{-1}T_{i}).$ The mean distorting transformation can\nbe useful in some fields -- in particular, we apply it for rendering affine\npanoramas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozeri_D/0/1/0/all/0/1\">Dror Ozeri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLO and Mask R-CNN for Vehicle Number Plate Identification. (arXiv:2207.13165v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.13165","description":"<p>License plate scanners have grown in popularity in parking lots during the\npast few years. In order to quickly identify license plates, traditional plate\nrecognition devices used in parking lots employ a fixed source of light and\nshooting angles. For skewed angles, such as license plate images taken with\nultra-wide angle or fisheye lenses, deformation of the license plate\nrecognition plate can also be quite severe, impairing the ability of standard\nlicense plate recognition systems to identify the plate. Mask RCNN gadget that\nmay be utilised for oblique pictures and various shooting angles. The results\nof the experiments show that the suggested design will be capable of\nclassifying license plates with bevel angles larger than 0/60. Character\nrecognition using the suggested Mask R-CNN approach has advanced significantly\nas well. The proposed Mask R-CNN method has also achieved significant progress\nin character recognition, which is tilted more than 45 degrees as compared to\nthe strategy of employing the YOLOv2 model. Experiment results also suggest\nthat the methodology presented in the open data plate collecting is better than\nother techniques (known as the AOLP dataset).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganjoo_S/0/1/0/all/0/1\">Siddharth Ganjoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.13243","description":"<p>The last decade of machine learning has seen drastic increases in scale and\ncapabilities, and deep neural networks (DNNs) are increasingly being deployed\nacross a wide range of domains. However, the inner workings of DNNs are\ngenerally difficult to understand, raising concerns about the safety of using\nthese systems without a rigorous understanding of how they function. In this\nsurvey, we review literature on techniques for interpreting the inner\ncomponents of DNNs, which we call \"inner\" interpretability methods.\nSpecifically, we review methods for interpreting weights, neurons, subnetworks,\nand latent representations with a focus on how these techniques relate to the\ngoal of designing safer, more trustworthy AI systems. We also highlight\nconnections between interpretability and work in modularity, adversarial\nrobustness, continual learning, network compression, and studying the human\nvisual system. Finally, we discuss key challenges and argue for future work in\ninterpretability for AI safety that focuses on diagnostics, benchmarking, and\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rauker_T/0/1/0/all/0/1\">Tilman R&#xe4;uker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_A/0/1/0/all/0/1\">Anson Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look Closer to Your Enemy: Learning to Attack via Teacher-student Mimicking. (arXiv:2207.13381v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.13381","description":"<p>This paper aims to generate realistic attack samples of person\nre-identification, ReID, by reading the enemy's mind (VM). In this paper, we\npropose a novel inconspicuous and controllable ReID attack baseline, LCYE, to\ngenerate adversarial query images. Concretely, LCYE first distills VM's\nknowledge via teacher-student memory mimicking in the proxy task. Then this\nknowledge prior acts as an explicit cipher conveying what is essential and\nrealistic, believed by VM, for accurate adversarial misleading. Besides,\nbenefiting from the multiple opposing task framework of LCYE, we further\ninvestigate the interpretability and generalization of ReID models from the\nview of the adversarial attack, including cross-domain adaption, cross-model\nconsensus, and online learning process. Extensive experiments on four ReID\nbenchmarks show that our method outperforms other state-of-the-art attackers\nwith a large margin in white-box, black-box, and target attacks. Our code is\nnow available at https://gitfront.io/r/user-3704489/mKXusqDT4ffr/LCYE/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingejie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiqing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sirui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1\">Dingwen Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical Keystroke Synthesis for Improved Bot Detection. (arXiv:2207.13394v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.13394","description":"<p>This work proposes two statistical approaches for the synthesis of keystroke\nbiometric data based on Universal and User-dependent Models. Both approaches\nare validated on the bot detection task, using the keystroke synthetic data to\nbetter train the systems. Our experiments include a dataset with 136 million\nkeystroke events from 168,000 subjects. We have analyzed the performance of the\ntwo synthesis approaches through qualitative and quantitative experiments.\nDifferent bot detectors are considered based on two supervised classifiers\n(Support Vector Machine and Long Short-Term Memory network) and a learning\nframework including human and generated samples. Our results prove that the\nproposed statistical approaches are able to generate realistic human-like\nsynthetic keystroke samples. Also, the classification results suggest that in\nscenarios with large labeled data, these synthetic samples can be detected with\nhigh accuracy. However, in few-shot learning scenarios it represents an\nimportant challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DeAlcala_D/0/1/0/all/0/1\">Daniel DeAlcala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acien_A/0/1/0/all/0/1\">Alejandro Acien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_S/0/1/0/all/0/1\">Santiago Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_M/0/1/0/all/0/1\">Miguel A. Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Moises Diaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}