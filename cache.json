{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-17T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DeepTrust: A Reliable Financial Knowledge Retrieval Framework For Explaining Extreme Pricing Anomalies. (arXiv:2203.08144v1 [q-fin.ST])","link":"http://arxiv.org/abs/2203.08144","description":"<p>Extreme pricing anomalies may occur unexpectedly without a trivial cause, and\nequity traders typically experience a meticulous process to source disparate\ninformation and analyze its reliability before integrating it into the trusted\nknowledge base. We introduce DeepTrust, a reliable financial knowledge\nretrieval framework on Twitter to explain extreme price moves at speed, while\nensuring data veracity using state-of-the-art NLP techniques. Our proposed\nframework consists of three modules, specialized for anomaly detection,\ninformation retrieval and reliability assessment. The workflow starts with\nidentifying anomalous asset price changes using machine learning models trained\nwith historical pricing data, and retrieving correlated unstructured data from\nTwitter using enhanced queries with dynamic search conditions. DeepTrust\nextrapolates information reliability from tweet features, traces of generative\nlanguage model, argumentation structure, subjectivity and sentiment signals,\nand refine a concise collection of credible tweets for market insights. The\nframework is evaluated on two self-annotated financial anomalies, i.e., Twitter\nand Facebook stock price on 29 and 30 April 2021. The optimal setup outperforms\nthe baseline classifier by 7.75% and 15.77% on F0.5-scores, and 10.55% and\n18.88% on precision, respectively, proving its capability in screening\nunreliable information precisely. At the same time, information retrieval and\nreliability assessment modules are analyzed individually on their effectiveness\nand causes of limitations, with identified subjective and objective factors\nthat influence the performance. As a collaborative project with Refinitiv, this\nframework paves a promising path towards building a scalable commercial\nsolution that assists traders to reach investment decisions on pricing\nanomalies with authenticated knowledge from social media platforms in\nreal-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Chan_P/0/1/0/all/0/1\">Pok Wah Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Contamination: From Memorization to Exploitation. (arXiv:2203.08242v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08242","description":"<p>Pretrained language models are typically trained on massive web-based\ndatasets, which are often \"contaminated\" with downstream test sets. It is not\nclear to what extent models exploit the contaminated data for downstream tasks.\nWe present a principled method to study this question. We pretrain BERT models\non joint corpora of Wikipedia and labeled downstream datasets, and fine-tune\nthem on the relevant task. Comparing performance between samples seen and\nunseen during pretraining enables us to define and quantify levels of\nmemorization and exploitation. Experiments with two models and three downstream\ntasks show that exploitation exists in some cases, but in others the models\nmemorize the contaminated data, but do not exploit it. We show that these two\nmeasures are affected by different factors such as the number of duplications\nof the contaminated data and the model size. Our results highlight the\nimportance of analyzing massive web-scale datasets to verify that progress in\nNLP is obtained by better language understanding and not better data\nexploitation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magar_I/0/1/0/all/0/1\">Inbal Magar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Improving Attentive Neural Networks in Legal Text Processing. (arXiv:2203.08244v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08244","description":"<p>In recent years, thanks to breakthroughs in neural network techniques\nespecially attentive deep learning models, natural language processing has made\nmany impressive achievements. However, automated legal word processing is still\na difficult branch of natural language processing. Legal sentences are often\nlong and contain complicated legal terminologies. Hence, models that work well\non general documents still face challenges in dealing with legal documents. We\nhave verified the existence of this problem with our experiments in this work.\nIn this dissertation, we selectively present the main achievements in improving\nattentive neural networks in automatic legal document processing. Language\nmodels tend to grow larger and larger, though, without expert knowledge, these\nmodels can still fail in domain adaptation, especially for specialized fields\nlike law.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report Summarization. (arXiv:2203.08257v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08257","description":"<p>The IMPRESSIONS section of a radiology report about an imaging study is a\nsummary of the radiologist's reasoning and conclusions, and it also aids the\nreferring physician in confirming or excluding certain diagnoses. A cascade of\ntasks are required to automatically generate an abstractive summary of the\ntypical information-rich radiology report. These tasks include acquisition of\nsalient content from the report and generation of a concise, easily consumable\nIMPRESSIONS section. Prior research on radiology report summarization has\nfocused on single-step end-to-end models -- which subsume the task of salient\ncontent acquisition. To fully explore the cascade structure and explainability\nof radiology report summarization, we introduce two innovations. First, we\ndesign a two-step approach: extractive summarization followed by abstractive\nsummarization. Second, we additionally break down the extractive part into two\nindependent tasks: extraction of salient (1) sentences and (2) keywords.\nExperiments on a publicly available radiology report dataset show our novel\napproach leads to a more precise summary compared to single-step and to\ntwo-step-with-single-extractive-process baselines with an overall improvement\nin F1 score Of 3-4%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karn_S/0/1/0/all/0/1\">Sanjeev Kumar Karn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuetze_H/0/1/0/all/0/1\">Hinrich Schuetze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farri_O/0/1/0/all/0/1\">Oladimeji Farri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Quality Estimation for Low Resource Corpus Mining. (arXiv:2203.08259v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08259","description":"<p>Quality Estimation (QE) models have the potential to change how we evaluate\nand maybe even train machine translation models. However, these models still\nlack the robustness to achieve general adoption. We show that State-of-the-art\nQE models, when tested in a Parallel Corpus Mining (PCM) setting, perform\nunexpectedly bad due to a lack of robustness to out-of-domain examples. We\npropose a combination of multitask training, data augmentation and contrastive\nlearning to achieve better and more robust QE performance. We show that our\nmethod improves QE performance significantly in the MLQE challenge and the\nrobustness of QE models when tested in the Parallel Corpus Mining setup. We\nincrease the accuracy in PCM by more than 0.80, making it on par with\nstate-of-the-art PCM methods that use millions of sentence pairs to train their\nmodels. In comparison, we use a thousand times less data, 7K parallel sentences\nin total, and propose a novel low resource PCM method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocyigit_M/0/1/0/all/0/1\">Muhammed Yusuf Kocyigit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-neural Models Matter: A Re-evaluation of Neural Referring Expression Generation Systems. (arXiv:2203.08274v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08274","description":"<p>In recent years, neural models have often outperformed rule-based and classic\nMachine Learning approaches in NLG. These classic approaches are now often\ndisregarded, for example when new neural models are evaluated. We argue that\nthey should not be overlooked, since, for some tasks, well-designed non-neural\napproaches achieve better performance than neural ones. In this paper, the task\nof generating referring expressions in linguistic context is used as an\nexample. We examined two very different English datasets (WEBNLG and WSJ), and\nevaluated each algorithm using both automatic and human evaluations. Overall,\nthe results of these evaluations suggest that rule-based systems with simple\nrule sets achieve on-par or better performance on both datasets compared to\nstate-of-the-art neural REG systems. In the case of the more realistic dataset,\nWSJ, a machine learning-based system with well-designed linguistic features\nperformed best. We hope that our work can encourage researchers to consider\nnon-neural models in future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Same_F/0/1/0/all/0/1\">Fahime Same</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastKASSIM: A Fast Tree Kernel-Based Syntactic Similarity Metric. (arXiv:2203.08299v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08299","description":"<p>Syntax is a fundamental component of language, yet few metrics have been\nemployed to capture syntactic similarity or coherence at the utterance- and\ndocument-level. The existing standard document-level syntactic similarity\nmetric is computationally expensive and performs inconsistently when faced with\nsyntactically dissimilar documents. To address these challenges, we present\nFastKASSIM, a metric for utterance- and document-level syntactic similarity\nwhich pairs and averages the most similar dependency parse trees between a pair\nof documents based on tree kernels. FastKASSIM is more robust to syntactic\ndissimilarities and differences in length, and runs up to to 5.2 times faster\nthan our baseline method over the documents in the r/ChangeMyView corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maximillian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Caitlyn Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperdecoders: Instance-specific decoders for multi-task NLP. (arXiv:2203.08304v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08304","description":"<p>We investigate input-conditioned hypernetworks for multi-tasking in NLP,\ngenerating parameter-efficient adaptations for a decoder using a hypernetwork\nconditioned on the output of an encoder. This approach produces a unique\ndecoder for every input instance, allowing the network a larger degree of\nflexibility than prior work that specializes the decoder for each task. We\napply our method to sequence classification tasks, extractive QA, and\nsummarisation and find that it often outperforms fully finetuning the\nunderlying model and surpasses previous parameter efficient fine-tuning\nmethods. Gains are particularly large when evaluated out-of-domain on the MRQA\nbenchmark. In addition, as the pretrained model is frozen, our method\neliminates negative interference among unrelated tasks, a common failure mode\nin fully fine-tuned approaches. An analysis of the embeddings produced by our\nmodel suggests that a large benefit of our approach is allowing the encoder\nmore effective control over the decoder, allowing mapping from hidden\nrepresentations to a final text-based label without interference from other\ntasks' output formats or labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivison_H/0/1/0/all/0/1\">Hamish Ivison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Word Translation via Two-Stage Contrastive Learning. (arXiv:2203.08307v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08307","description":"<p>Word translation or bilingual lexicon induction (BLI) is a key cross-lingual\ntask, aiming to bridge the lexical gap between different languages. In this\nwork, we propose a robust and effective two-stage contrastive learning\nframework for the BLI task. At Stage C1, we propose to refine standard\ncross-lingual linear maps between static word embeddings (WEs) via a\ncontrastive learning objective; we also show how to integrate it into the\nself-learning procedure for even more refined cross-lingual maps. In Stage C2,\nwe conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word\ntranslation capability. We also show that static WEs induced from the\n`C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments\non standard BLI datasets for diverse languages and different experimental\nsetups demonstrate substantial gains achieved by our framework. While the BLI\nmethod from Stage C1 already yields substantial gains over all state-of-the-art\nBLI methods in our comparison, even stronger improvements are met with the full\ntwo-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28\nlanguage pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaoyiran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Generative Language Models for Zero-Shot Cross-Lingual Event Argument Extraction. (arXiv:2203.08308v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08308","description":"<p>We present a study on leveraging multilingual pre-trained generative language\nmodels for zero-shot cross-lingual event argument extraction (EAE). By\nformulating EAE as a language generation task, our method effectively encodes\nevent structures and captures the dependencies between arguments. We design\nlanguage-agnostic templates to represent the event argument structures, which\nare compatible with any language, hence facilitating the cross-lingual\ntransfer. Our proposed model finetunes multilingual pre-trained generative\nlanguage models to generate sentences that fill in the language-agnostic\ntemplate with arguments extracted from the input passage. The model is trained\non source languages and is then directly applied to target languages for event\nargument extraction. Experiments demonstrate that the proposed model\noutperforms the current state-of-the-art models on zero-shot cross-lingual EAE.\nComprehensive studies and error analyses are presented to better understand the\nadvantages and the current limitations of using generative language models for\nzero-shot cross-lingual transfer EAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Premkumar Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Afrocentric NLP for African Languages: Where We Are and Where We Can Go. (arXiv:2203.08351v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08351","description":"<p>Aligning with ACL 2022 special Theme on \"Language Diversity: from Low\nResource to Endangered Languages\", we discuss the major linguistic and\nsociopolitical challenges facing development of NLP technologies for African\nlanguages. Situating African languages in a typological framework, we discuss\nhow the particulars of these languages can be harnessed. To facilitate future\nresearch, we also highlight current efforts, communities, venues, datasets, and\ntools. Our main objective is to motivate and advocate for an Afrocentric\napproach to technology development. With this in mind, we recommend\n\\textit{what} technologies to build and \\textit{how} to build, evaluate, and\ndeploy them based on the needs of local African communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1\">Ife Adebara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spot the Difference: A Cooperative Object-Referring Game in Non-Perfectly Co-Observable Scene. (arXiv:2203.08362v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08362","description":"<p>Visual dialog has witnessed great progress after introducing various\nvision-oriented goals into the conversation, especially such as GuessWhich and\nGuessWhat, where the only image is visible by either and both of the questioner\nand the answerer, respectively. Researchers explore more on visual dialog tasks\nin such kind of single- or perfectly co-observable visual scene, while somewhat\nneglect the exploration on tasks of non perfectly co-observable visual scene,\nwhere the images accessed by two agents may not be exactly the same, often\noccurred in practice. Although building common ground in non-perfectly\nco-observable visual scene through conversation is significant for advanced\ndialog agents, the lack of such dialog task and corresponding large-scale\ndataset makes it impossible to carry out in-depth research. To break this\nlimitation, we propose an object-referring game in non-perfectly co-observable\nvisual scene, where the goal is to spot the difference between the similar\nvisual scenes through conversing in natural language. The task addresses\nchallenges of the dialog strategy in non-perfectly co-observable visual scene\nand the ability of categorizing objects. Correspondingly, we construct a\nlarge-scale multimodal dataset, named SpotDiff, which contains 87k Virtual\nReality images and 97k dialogs generated by self-play. Finally, we give\nbenchmark models for this task, and conduct extensive experiments to evaluate\nits performance as well as analyze its main challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Duo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_Q/0/1/0/all/0/1\">Qingyi Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hairun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fangxiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Document Representation Learning for Open-Domain Dense Retrieval. (arXiv:2203.08372v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08372","description":"<p>Dense retrieval has achieved impressive advances in first-stage retrieval\nfrom a large-scale document collection, which is built on bi-encoder\narchitecture to produce single vector representation of query and document.\nHowever, a document can usually answer multiple potential queries from\ndifferent views. So the single vector representation of a document is hard to\nmatch with multi-view queries, and faces a semantic mismatch problem. This\npaper proposes a multi-view document representation learning framework, aiming\nto produce multi-view embeddings to represent documents and enforce them to\nalign with different queries. First, we propose a simple yet effective method\nof generating multiple embeddings through viewers. Second, to prevent\nmulti-view embeddings from collapsing to the same one, we further propose a\nglobal-local loss with annealed temperature to encourage the multiple viewers\nto better align with different potential queries. Experiments show our method\noutperforms recent works and achieves state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shunyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transforming Sequence Tagging Into A Seq2Seq Task. (arXiv:2203.08378v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08378","description":"<p>Pretrained, large, generative language models (LMs) have had great success in\na wide range of sequence tagging and structured prediction tasks. Casting a\nsequence tagging task as a Seq2Seq one requires deciding the formats of the\ninput and output sequences. However, we lack a principled understanding of the\ntrade-offs associated with these formats (such as the effect on model accuracy,\nsequence length, multilingual generalization, hallucination). In this paper, we\nrigorously study different formats one could use for casting input text\nsentences and their output labels into the input and target (i.e., output) of a\nSeq2Seq model. Along the way, we introduce a new format, which we show to not\nonly be simpler but also more effective. Additionally the new format\ndemonstrates significant gains in the multilingual settings -- both zero-shot\ntransfer learning and joint training. Lastly, we find that the new format is\nmore robust and almost completely devoid of hallucination -- an issue we find\ncommon in existing formats. With well over a 1000 experiments studying 14\ndifferent formats, over 7 diverse public benchmarks -- including 3 multilingual\ndatasets spanning 7 languages -- we believe our findings provide a strong\nempirical basis in understanding how we should tackle sequence tagging tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raman_K/0/1/0/all/0/1\">Karthik Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naim_I/0/1/0/all/0/1\">Iftekhar Naim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiecao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yalasangi_K/0/1/0/all/0/1\">Kiran Yalasangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_K/0/1/0/all/0/1\">Krishna Srinivasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach. (arXiv:2203.08383v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08383","description":"<p>While Pre-trained Language Models (PLMs) internalize a great amount of world\nknowledge, they have been shown incapable of recalling these knowledge to solve\ntasks requiring complex &amp; multi-step inference procedures. Similar to how\nhumans develop a \"train of thought\" for these tasks, how can we equip PLMs with\nsuch abilities? In this work, we explore an iterative prompting framework, a\nnew prompting paradigm which progressively elicits relevant knowledge from PLMs\nfor multi-step inference tasks. We identify key limitations of existing\nprompting methods, namely they are either restricted to queries with a single\nidentifiable relation/predicate, or being agnostic to input contexts, which\nmakes it difficult to capture variabilities across different inference steps.\nWe propose an iterative context-aware prompter, which addresses these\nlimitations by learning to dynamically synthesize prompts conditioned on the\ncurrent step's contexts. Experiments on three datasets involving multi-step\ninference show the effectiveness of the iterative scheme and our proposed\nprompter design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boshi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages. (arXiv:2203.08388v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08388","description":"<p>While there has been a recent burgeoning of applications at the intersection\nof natural and programming languages, such as code generation and code\nsummarization, these applications are usually English-centric. This creates a\nbarrier for program developers who are not proficient in English. To mitigate\nthis gap in technology development across languages, we propose a multilingual\ndataset, MCoNaLa, to benchmark code generation from natural language commands\nextending beyond English. Modeled off of the methodology from the English\nCode/Natural Language Challenge (CoNaLa) dataset, we annotated a total of 896\nNL-code pairs in three languages: Spanish, Japanese, and Russian. We present a\nquantitative evaluation of performance on the MCoNaLa dataset by testing with\nstate-of-the-art code generation systems. While the difficulties vary across\nthese three languages, all systems lag significantly behind their English\ncounterparts, revealing the challenges in adapting code generation to new\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuenca_G/0/1/0/all/0/1\">Grace Cuenca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation. (arXiv:2203.08394v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08394","description":"<p>Back-translation is a critical component of Unsupervised Neural Machine\nTranslation (UNMT), which generates pseudo parallel data from target\nmonolingual data. A UNMT model is trained on the pseudo parallel data with\ntranslated source, and translates natural source sentences in inference. The\nsource discrepancy between training and inference hinders the translation\nperformance of UNMT models. By carefully designing experiments, we identify two\nrepresentative characteristics of the data gap in source: (1) style gap (i.e.,\ntranslated vs. natural text style) that leads to poor generalization\ncapability; (2) content gap that induces the model to produce hallucination\ncontent biased towards the target language. To narrow the data gap, we propose\nan online self-training approach, which simultaneously uses the pseudo parallel\ndata {natural source, translated target} to mimic the inference scenario.\nExperimental results on several widely-used language pairs show that our\napproach outperforms two strong baselines (XLM and MASS) by remedying the style\nand content gaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again. (arXiv:2203.08410v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08410","description":"<p>The strong few-shot in-context learning capability of large pre-trained\nlanguage models (PLMs) such as GPT-3 is highly appealing for biomedical\napplications where data annotation is particularly costly. In this paper, we\npresent the first systematic and comprehensive study to compare the few-shot\nperformance of GPT-3 in-context learning with fine-tuning smaller (i.e.,\nBERT-sized) PLMs on two highly representative biomedical information extraction\ntasks, named entity recognition and relation extraction. We follow the true\nfew-shot setting to avoid overestimating models' few-shot performance by model\nselection over a large validation set. We also optimize GPT-3's performance\nwith known techniques such as contextual calibration and dynamic in-context\nexample retrieval. However, our results show that GPT-3 still significantly\nunderperforms compared with simply fine-tuning a smaller PLM using the same\nsmall training set. Moreover, what is equally important for practical\napplications is that adding more labeled data would reliably yield an\nimprovement in model performance. While that is the case when fine-tuning small\nPLMs, GPT-3's performance barely improves when adding more data. In-depth\nanalyses further reveal issues of the in-context learning setting that may be\ndetrimental to information extraction tasks in general. Given the high cost of\nexperimenting with GPT-3, we hope our study provides guidance for biomedical\nresearchers and practitioners towards more promising directions such as\nfine-tuning GPT-3 or small PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_B/0/1/0/all/0/1\">Bernal Jim&#xe9;nez Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McNeal_N/0/1/0/all/0/1\">Nikolas McNeal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_C/0/1/0/all/0/1\">Clay Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">You Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction. (arXiv:2203.08411v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08411","description":"<p>Sequence modeling has demonstrated state-of-the-art performance on natural\nlanguage and document understanding tasks. However, it is challenging to\ncorrectly serialize tokens in form-like documents in practice due to their\nvariety of layout patterns. We propose FormNet, a structure-aware sequence\nmodel to mitigate the suboptimal serialization of forms. First, we design Rich\nAttention that leverages the spatial relationship between tokens in a form for\nmore precise attention score calculation. Second, we construct Super-Tokens for\neach word by embedding representations from their neighboring tokens through\ngraph convolutions. FormNet therefore explicitly recovers local syntactic\ninformation that may have been lost during serialization. In experiments,\nFormNet outperforms existing methods with a more compact model size and less\npre-training data, establishing new state-of-the-art performance on CORD, FUNSD\nand Payment benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozat_T/0/1/0/all/0/1\">Timothy Dozat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perot_V/0/1/0/all/0/1\">Vincent Perot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guolong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_N/0/1/0/all/0/1\">Nan Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure. (arXiv:2203.08430v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08430","description":"<p>Multilingual pre-trained language models, such as mBERT and XLM-R, have shown\nimpressive cross-lingual ability. Surprisingly, both of them use multilingual\nmasked language model (MLM) without any cross-lingual supervision or aligned\ndata. Despite the encouraging results, we still lack a clear understanding of\nwhy cross-lingual ability could emerge from multilingual MLM. In our work, we\nargue that cross-language ability comes from the commonality between languages.\nSpecifically, we study three language properties: constituent order,\ncomposition and word co-occurrence. First, we create an artificial language by\nmodifying property in source language. Then we study the contribution of\nmodified property through the change of cross-language transfer results on\ntarget language. We conduct experiments on six languages and two cross-lingual\nNLP tasks (textual entailment, sentence retrieval). Our main conclusion is that\nthe contribution of constituent order and word co-occurrence is limited, while\nthe composition is more crucial to the success of cross-linguistic transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yuan Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Say What You Don't Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search. (arXiv:2203.08436v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08436","description":"<p>Abstractive summarization systems today produce fluent and relevant output,\nbut often \"hallucinate\" statements not supported by the source text. We analyze\nthe connection between hallucinations and training data, and find evidence that\nmodels hallucinate because they train on target summaries that are unsupported\nby the source. Based on our findings, we present PINOCCHIO, a new decoding\nmethod that improves the consistency of a transformer-based abstractive\nsummarizer by constraining beam search to avoid hallucinations. Given the model\nstates and outputs at a given step, PINOCCHIO detects likely model\nhallucinations based on various measures of attribution to the source text.\nPINOCCHIO backtracks to find more consistent output, and can opt to produce no\nsummary at all when no consistent generation can be found. In experiments, we\nfind that PINOCCHIO improves the consistency of generation (in terms of F1) by\nan average of~67% on two abstractive summarization datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+King_D/0/1/0/all/0/1\">Daniel King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramani_N/0/1/0/all/0/1\">Nishant Subramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation. (arXiv:2203.08442v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08442","description":"<p>In this paper, we present a substantial step in better understanding the SOTA\nsequence-to-sequence (Seq2Seq) pretraining for neural machine\ntranslation~(NMT). We focus on studying the impact of the jointly pretrained\ndecoder, which is the main difference between Seq2Seq pretraining and previous\nencoder-based pretraining approaches for NMT. By carefully designing\nexperiments on three language pairs, we find that Seq2Seq pretraining is a\ndouble-edged sword: On one hand, it helps NMT models to produce more diverse\ntranslations and reduce adequacy-related translation errors. On the other hand,\nthe discrepancies between Seq2Seq pretraining and NMT finetuning limit the\ntranslation quality (i.e., domain discrepancy) and induce the over-estimation\nissue (i.e., objective discrepancy). Based on these observations, we further\npropose simple and effective strategies, named in-domain pretraining and input\nadaptation to remedy the domain and objective discrepancies, respectively.\nExperimental results on several language pairs show that our approach can\nconsistently improve both translation performance and model robustness upon\nSeq2Seq pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yongchang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets. (arXiv:2203.08445v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08445","description":"<p>A rapidly growing body of research has demonstrated the inability of NLP\nmodels to generalize compositionally and has tried to alleviate it through\nspecialized architectures, training schemes, and data augmentation, among other\napproaches. In this work, we study a different relatively under-explored\napproach: sampling diverse train sets that encourage compositional\ngeneralization. We propose a novel algorithm for sampling a structurally\ndiverse set of instances from a labeled instance pool with structured outputs.\nEvaluating on 5 semantic parsing datasets of varying complexity, we show that\nour algorithm performs competitively with or better than prior algorithms in\nnot only compositional template splits but also traditional IID splits of all\nbut the least structurally diverse datasets. In general, we find that diverse\ntrain sets lead to better generalization than random training sets of the same\nsize in 9 out of 10 dataset-split pairs, with over 10% absolute improvement in\n5, providing further evidence to their sample efficiency. Moreover, we show\nthat structural diversity also makes for more comprehensive test sets that\nrequire diverse training to succeed on. Finally, we use information theory to\nshow that reduction in spurious correlations between substructures may be one\nreason why diverse training sets improve generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Pre-trained Language Models Interpret Similes as Smart as Human?. (arXiv:2203.08452v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08452","description":"<p>Simile interpretation is a crucial task in natural language processing.\nNowadays, pre-trained language models (PLMs) have achieved state-of-the-art\nperformance on many tasks. However, it remains under-explored whether PLMs can\ninterpret similes or not. In this paper, we investigate the ability of PLMs in\nsimile interpretation by designing a novel task named Simile Property Probing,\ni.e., to let the PLMs infer the shared properties of similes. We construct our\nsimile property probing datasets from both general textual corpora and\nhuman-designed questions, containing 1,633 examples covering seven main\ncategories. Our empirical study based on the constructed datasets shows that\nPLMs can infer similes' shared properties while still underperforming humans.\nTo bridge the gap with human performance, we additionally design a\nknowledge-enhanced training objective by incorporating the simile knowledge\ninto PLMs via knowledge embedding methods. Our method results in a gain of\n8.58% in the probing task and 1.37% in the downstream task of sentiment\nclassification. The datasets and code are publicly available at\nhttps://github.com/Abbey4799/PLMs-Interpret-Simile.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Sijie Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model. (arXiv:2203.08459v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08459","description":"<p>Pre-trained language models such as BERT have been successful at tackling\nmany natural language processing tasks. However, the unsupervised sub-word\ntokenization methods commonly used in these models (e.g., byte-pair encoding -\nBPE) are sub-optimal at handling morphologically rich languages. Even given a\nmorphological analyzer, naive sequencing of morphemes into a standard BERT\narchitecture is inefficient at capturing morphological compositionality and\nexpressing word-relative syntactic regularities. We address these challenges by\nproposing a simple yet effective two-tier BERT architecture that leverages a\nmorphological analyzer and explicitly represents morphological\ncompositionality. Despite the success of BERT, most of its evaluations have\nbeen conducted on high-resource languages, obscuring its applicability on\nlow-resource languages. We evaluate our proposed method on the low-resource\nmorphologically rich Kinyarwanda language, naming the proposed model\narchitecture KinyaBERT. A robust set of experimental results reveal that\nKinyaBERT outperforms solid baselines by 2% F1 score on a named entity\nrecognition task and by 4.3% average score of a machine-translated GLUE\nbenchmark. KinyaBERT fine-tuning has better convergence and achieves more\nrobust results on multiple tasks even in the presence of translation noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nzeyimana_A/0/1/0/all/0/1\">Antoine Nzeyimana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubungo_A/0/1/0/all/0/1\">Andre Niyongabo Rubungo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning. (arXiv:2203.08480v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08480","description":"<p>The ability to recognize analogies is fundamental to human cognition.\nExisting benchmarks to test word analogy do not reveal the underneath process\nof analogical reasoning of neural models. Holding the belief that models\ncapable of reasoning should be right for the right reasons, we propose a\nfirst-of-its-kind Explainable Knowledge-intensive Analogical Reasoning\nbenchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in\nEnglish) problems sourced from the Civil Service Exams, which require intensive\nbackground knowledge to solve. More importantly, we design a free-text\nexplanation scheme to explain whether an analogy should be drawn, and manually\nannotate them for each and every question and candidate answer. Empirical\nresults suggest that this benchmark is very challenging for some\nstate-of-the-art models for both explanation generation and analogical question\nanswering tasks, which invites further research in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Ziquan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongqiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations. (arXiv:2203.08500v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08500","description":"<p>Recently, various response generation models for two-party conversations have\nachieved impressive improvements, but less effort has been paid to multi-party\nconversations (MPCs) which are more practical and complicated. Compared with a\ntwo-party conversation where a dialogue context is a sequence of utterances,\nbuilding a response generation model for MPCs is more challenging, since there\nexist complicated context structures and the generated responses heavily rely\non both interlocutors (i.e., speaker and addressee) and history utterances. To\naddress these challenges, we present HeterMPC, a heterogeneous graph-based\nneural network for response generation in MPCs which models the semantics of\nutterances and interlocutors simultaneously with two types of nodes in a graph.\nBesides, we also design six types of meta relations with\nnode-edge-type-dependent parameters to characterize the heterogeneous\ninteractions within the graph. Through multi-hop updating, HeterMPC can\nadequately utilize the structural knowledge of conversations for response\ngeneration. Experimental results on the Ubuntu Internet Relay Chat (IRC)\nchannel benchmark show that HeterMPC outperforms various baseline models for\nresponse generation in MPCs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chao-Hong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConTinTin: Continual Learning from Task Instructions. (arXiv:2203.08512v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08512","description":"<p>The mainstream machine learning paradigms for NLP often work with two\nunderlying presumptions. First, the target task is predefined and static, a\nsystem just needs to learn to solve it exclusively. Second, the supervision of\na task mainly comes from a set of labeled examples. A question arises: how to\nbuild a system that can keep learning new tasks from their instructions? This\nwork defines a new learning paradigm ConTinTin (Continual Learning from Task\nInstructions), in which a system should learn a sequence of new tasks one by\none, each task is explained by a piece of textual instruction. The system is\nrequired to (i) generate the expected outputs of a new task by learning from\nits instruction, (ii) transfer the knowledge acquired from upstream tasks to\nhelp solve downstream tasks (i.e, forward-transfer), and (iii) retain or even\nimprove the performance on earlier tasks after learning new tasks (i.e.,\nbackward-transfer). This new problem is studied on a stream of more than 60\ntasks, each equipped with an instruction. Technically, our method\nInstructionSpeak contains two strategies that make full use of task\ninstructions to improve forward-transfer and backward-transfer: one is to learn\nfrom the negative output, the other is to re-visit instructions of prior tasks.\nTo our knowledge, this is the first time to study ConTinTin in NLP. In addition\nto the problem formulation and our promising approach, this work also\ncontributes to providing rich analyses for the community to better understand\nthis novel learning problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TegTok: Augmenting Text Generation via Task-specific and Open-world Knowledge. (arXiv:2203.08517v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08517","description":"<p>Generating natural and informative texts has been a long-standing problem in\nNLP. Much effort has been dedicated into incorporating pre-trained language\nmodels (PLMs) with various open-world knowledge, such as knowledge graphs or\nwiki pages. However, their ability to access and manipulate the task-specific\nknowledge is still limited on downstream tasks, as this type of knowledge is\nusually not well covered in PLMs and is hard to acquire. To address the\nproblem, we propose augmenting TExt Generation via Task-specific and Open-world\nKnowledge (TegTok) in a unified framework. Our model selects knowledge entries\nfrom two types of knowledge sources through dense retrieval and then injects\nthem into the input encoding and output decoding stages respectively on the\nbasis of PLMs. With the help of these two types of knowledge, our model can\nlearn what and how to generate. Experiments on two text generation tasks of\ndialogue generation and question generation, and on two datasets show that our\nmethod achieves better performance than various baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chao-Hong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphological Reinflection with Multiple Arguments: An Extended Annotation schema and a Georgian Case Study. (arXiv:2203.08527v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08527","description":"<p>In recent years, a flurry of morphological datasets had emerged, most notably\nUniMorph, a multi-lingual repository of inflection tables. However, the flat\nstructure of the current morphological annotation schemas makes the treatment\nof some languages quirky, if not impossible, specifically in cases of\npolypersonal agreement. In this paper we propose a general solution for such\ncases and expand the UniMorph annotation schema to naturally address this\nphenomenon, in which verbs agree with multiple arguments using true affixes. We\napply this extended schema to one such language, Georgian, and provide a\nhuman-verified, accurate and balanced morphological dataset for Georgian verbs.\nThe dataset has 4 times more tables and 6 times more verb forms compared to the\nexisting UniMorph dataset, covering all possible variants of argument marking,\ndemonstrating the adequacy of our proposed scheme. Experiments with a standard\nreinflection model show that generalization is easy when the data is split at\nthe form level, but extremely hard when splitting along lemma lines. Expanding\nthe other languages in UniMorph to this schema is expected to improve both the\ncoverage, consistency and interpretability of this benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guriel_D/0/1/0/all/0/1\">David Guriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer. (arXiv:2203.08552v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08552","description":"<p>We exploit the pre-trained seq2seq model mBART for multilingual text style\ntransfer. Using machine translated data as well as gold aligned English\nsentences yields state-of-the-art results in the three target languages we\nconsider. Besides, in view of the general scarcity of parallel data, we propose\na modular approach for multilingual formality transfer, which consists of two\ntraining strategies that target adaptation to both language and task. Our\napproach achieves competitive performance without monolingual task-specific\nparallel data and can be applied to other style transfer tasks as well as to\nother languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huiyuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Dependency Parsing with Worst-Case Aware Automated Curriculum Learning. (arXiv:2203.08555v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08555","description":"<p>Large multilingual pretrained language models such as mBERT and XLM-RoBERTa\nhave been found to be surprisingly effective for cross-lingual transfer of\nsyntactic parsing models (Wu and Dredze 2019), but only between related\nlanguages. However, source and training languages are rarely related, when\nparsing truly low-resource languages. To close this gap, we adopt a method from\nmulti-task learning, which relies on automated curriculum learning, to\ndynamically optimize for parsing performance on outlier languages. We show that\nthis approach is significantly better than uniform and size-proportional\nsampling in the zero-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lhoneux_M/0/1/0/all/0/1\">Miryam de Lhoneux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEVEN: A Large-Scale Chinese Legal Event Detection Dataset. (arXiv:2203.08556v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08556","description":"<p>Recognizing facts is the most fundamental step in making judgments, hence\ndetecting events in the legal documents is important to legal case analysis\ntasks. However, existing Legal Event Detection (LED) datasets only concern\nincomprehensive event types and have limited annotated data, which restricts\nthe development of LED methods and their downstream applications. To alleviate\nthese issues, we present LEVEN a large-scale Chinese LEgal eVENt detection\ndataset, with 8,116 legal documents and 150,977 human-annotated event mentions\nin 108 event types. Not only charge-related events, LEVEN also covers general\nevents, which are critical for legal case understanding but neglected in\nexisting LED datasets. To our knowledge, LEVEN is the largest LED dataset and\nhas dozens of times the data scale of others, which shall significantly promote\nthe training and evaluation of LED methods. The results of extensive\nexperiments indicate that LED is challenging and needs further effort.\nMoreover, we simply utilize legal events as side information to promote\ndownstream applications. The method achieves improvements of average 2.2 points\nprecision in low-resource judgment prediction, and 1.5 points mean average\nprecision in unsupervised case retrieval, which suggests the fundamentality of\nLED. The source code and dataset can be obtained from\nhttps://github.com/thunlp/LEVEN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_F/0/1/0/all/0/1\">Feng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaojun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1\">Cunchao Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Weixing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geographic Adaptation of Pretrained Language Models. (arXiv:2203.08565v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08565","description":"<p>Geographic linguistic features are commonly used to improve the performance\nof pretrained language models (PLMs) on NLP tasks where geographic knowledge is\nintuitively beneficial (e.g., geolocation prediction and dialect feature\nprediction). Existing work, however, leverages such geographic information in\ntask-specific fine-tuning, failing to incorporate it into PLMs' geo-linguistic\nknowledge, which would make it transferable across different tasks. In this\nwork, we introduce an approach to task-agnostic geoadaptation of PLMs that\nforces the PLM to learn associations between linguistic phenomena and\ngeographic locations. More specifically, geoadaptation is an intermediate\ntraining step that couples masked language modeling and geolocation prediction\nin a dynamic multitask learning setup. In our experiments, we geoadapt BERTi\\'c\n-- a PLM for Bosnian, Croatian, Montenegrin, and Serbian (BCMS) -- using a\ncorpus of geotagged BCMS tweets. Evaluation on three different tasks, namely\nunsupervised (zero-shot) and supervised geolocation prediction and\n(unsupervised) prediction of dialect features, shows that our geoadaptation\napproach is very effective: e.g., we obtain new state-of-the-art performance in\nsupervised geolocation prediction and report massive gains over geographically\nuninformed PLMs on zero-shot geolocation prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_V/0/1/0/all/0/1\">Valentin Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ljubesic_N/0/1/0/all/0/1\">Nikola Ljube&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1\">Janet B. Pierrehumbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Learning for Few-Shot Dialogue State Tracking. (arXiv:2203.08568v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08568","description":"<p>Collecting and annotating task-oriented dialogues is time-consuming and\ncostly. Thus, few-shot learning for dialogue tasks presents an exciting\nopportunity. In this work, we propose an in-context (IC) learning framework for\nfew-shot dialogue state tracking (DST), where a large pre-trained language\nmodel (LM) takes a test instance and a few annotated examples as input, and\ndirectly decodes the dialogue states without any parameter updates. This makes\nthe LM more flexible and scalable compared to prior few-shot DST work when\nadapting to new domains and scenarios. We study ways to formulate dialogue\ncontext into prompts for LMs and propose an efficient approach to retrieve\ndialogues as exemplars given a test instance and a selection pool of few-shot\nexamples. To better leverage the pre-trained LMs, we also reformulate DST into\na text-to-SQL problem. Empirical results on MultiWOZ 2.1 and 2.4 show that our\nmethod IC-DST outperforms previous fine-tuned state-of-the-art models in\nfew-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yushi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Hsuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianbao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Whither the Priors for (Vocal) Interactivity?. (arXiv:2203.08578v1 [cs.RO])","link":"http://arxiv.org/abs/2203.08578","description":"<p>Voice-based communication is often cited as one of the most `natural' ways in\nwhich humans and robots might interact, and the recent availability of accurate\nautomatic speech recognition and intelligible speech synthesis has enabled\nresearchers to integrate advanced off-the-shelf spoken language technology\ncomponents into their robot platforms. Despite this, the resulting interactions\nare anything but `natural'. It transpires that simply giving a robot a voice\ndoesn't mean that a user will know how (or when) to talk to it, and the\nresulting `conversations' tend to be stilted, one-sided and short. On the\nsurface, these difficulties might appear to be fairly trivial consequences of\nusers' unfamiliarity with robots (and \\emph{vice versa}), and that any problems\nwould be mitigated by long-term use by the human, coupled with `deep learning'\nby the robot. However, it is argued here that such communication failures are\nindicative of a deeper malaise: a fundamental lack of basic principles --\n\\emph{priors} -- underpinning not only speech-based interaction in particular,\nbut (vocal) interactivity in general. This is evidenced not only by the fact\nthat contemporary spoken language systems already require training data sets\nthat are orders-of-magnitude greater than that experienced by a young child,\nbut also by the lack of design principles for creating effective communicative\nhuman-robot interaction. This short position paper identifies some of the key\nareas where theoretical insights might help overcome these shortfalls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moore_R/0/1/0/all/0/1\">Roger K. Moore</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Summary of Long Instructions is Better for Program Synthesis. (arXiv:2203.08597v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08597","description":"<p>Despite the success of large pre-trained language models (LMs) such as Codex,\nthey show below-par performance on the larger and more complicated programming\nrelated questions. We show that LMs benefit from the summarized version of\ncomplicated questions. Our findings show that superfluous information often\npresent in problem description such as human characters, background stories,\nnames (which are included to help humans in understanding a task) does not help\nmodels in understanding a task. To this extent, we create a meta-dataset from\nthe frequently used APPS dataset for the program synthesis task. Our\nmeta-dataset consists of human and synthesized summary of the long and\ncomplicated programming questions. Experimental results on Codex show that our\nproposed approach outperforms baseline by 8.13% on an average in terms of\nstrict accuracy. Our analysis shows that summary significantly improve\nperformance for introductory (9.86%) and interview (11.48%) related programming\nquestions. However, it shows improvement by a small margin (~2%) for\ncompetitive programming questions, implying the scope for future research\ndirection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuznia_K/0/1/0/all/0/1\">Kirby Kuznia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Networks for Multiparallel Word Alignment. (arXiv:2203.08654v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08654","description":"<p>After a period of decrease, interest in word alignments is increasing again\nfor their usefulness in domains such as typological research, cross-lingual\nannotation projection, and machine translation. Generally, alignment algorithms\nonly use bitext and do not make use of the fact that many parallel corpora are\nmultiparallel. Here, we compute high-quality word alignments between multiple\nlanguage pairs by considering all language pairs together. First, we create a\nmultiparallel word alignment graph, joining all bilingual word alignment pairs\nin one graph. Next, we use graph neural networks (GNNs) to exploit the graph\nstructure. Our GNN approach (i) utilizes information about the meaning,\nposition, and language of the input words, (ii) incorporates information from\nmultiple parallel sentences, (iii) adds and removes edges from the initial\nalignments, and (iv) yields a prediction model that can generalize beyond the\ntraining sentences. We show that community detection provides valuable\ninformation for multiparallel word alignment. Our method outperforms previous\nwork on three word-alignment datasets and on a downstream task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imani_A/0/1/0/all/0/1\">Ayyoob Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senel_L/0/1/0/all/0/1\">L&#xfc;tfi Kerem &#x15e;enel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1\">Masoud Jalili Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Feasibility Study of Answer-Unaware Question Generation for Education. (arXiv:2203.08685v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08685","description":"<p>We conduct a feasibility study into the applicability of answer-unaware\nquestion generation models to textbook passages. We show that a significant\nportion of errors in such systems arise from asking irrelevant or\nuninterpretable questions and that such errors can be ameliorated by providing\nsummarized input. We find that giving these models human-written summaries\ninstead of the original text results in a significant increase in acceptability\nof generated questions (33% $\\rightarrow$ 83%) as determined by expert\nannotators. We also find that, in the absence of human-written summaries,\nautomatic summarization can serve as a good middle ground.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dugan_L/0/1/0/all/0/1\">Liam Dugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miltsakaki_E/0/1/0/all/0/1\">Eleni Miltsakaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Shriyash Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginsberg_E/0/1/0/all/0/1\">Etan Ginsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_H/0/1/0/all/0/1\">Hannah Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dayheon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chuning Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turning Stocks into Memes: A Dataset for Understanding How Social Communities Can Drive Wall Street. (arXiv:2203.08694v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08694","description":"<p>Who actually expresses an intent to buy GameStop shares on Reddit? What\nconvinces people to buy stocks? Are people convinced to support a coordinated\nplan to adversely impact Wall Street investors? Existing literature on\nunderstanding intent has mainly relied on surveys and self reporting; however\nthere are limitations to these methodologies. Hence, in this paper, we develop\nan annotated dataset of communications centered on the GameStop phenomenon to\nanalyze the subscriber intentions behaviors within the r/WallStreetBets\ncommunity to buy (or not buy) stocks. Likewise, we curate a dataset to better\nunderstand how intent interacts with a user's general support towards the\ncoordinated actions of the community for GameStop. Overall, our dataset can\nprovide insight to social scientists on the persuasive power to buy into social\nmovements online by adopting common language and narrative. WARNING: This paper\ncontains offensive language that commonly appears on Reddit's r/WallStreetBets\nsubreddit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_R/0/1/0/all/0/1\">Richard Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_P/0/1/0/all/0/1\">Paras Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xingmeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rios_A/0/1/0/all/0/1\">Anthony Rios</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Stage Prompting for Knowledgeable Dialogue Generation. (arXiv:2203.08745v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08745","description":"<p>Existing knowledge-grounded dialogue systems typically use finetuned versions\nof a pretrained language model (LM) and large-scale knowledge bases. These\nmodels typically fail to generalize on topics outside of the knowledge base,\nand require maintaining separate potentially large checkpoints each time\nfinetuning is needed. In this paper, we aim to address these limitations by\nleveraging the inherent knowledge stored in the pretrained LM as well as its\npowerful generation ability. We propose a multi-stage prompting approach to\ngenerate knowledgeable responses from a single pretrained LM. We first prompt\nthe LM to generate knowledge based on the dialogue context. Then, we further\nprompt it to generate responses based on the dialogue context and the\npreviously generated knowledge. Results show that our knowledge generator\noutperforms the state-of-the-art retrieval-based model by 5.8% when combining\nknowledge relevance and correctness. In addition, our multi-stage prompting\noutperforms the finetuning-based dialogue model in terms of response\nknowledgeability and engagement by up to 10% and 5%, respectively. Furthermore,\nwe scale our model up to 530 billion parameters and show that larger LMs\nimprove the generation correctness score by up to 10%, and response relevance,\nknowledgeability and engagement by up to 10%. Our code is available at:\nhttps://github.com/NVIDIA/Megatron-LM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prenger_R/0/1/0/all/0/1\">Ryan Prenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sample, Translate, Recombine: Leveraging Audio Alignments for Data Augmentation in End-to-end Speech Translation. (arXiv:2203.08757v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08757","description":"<p>End-to-end speech translation relies on data that pair source-language speech\ninputs with corresponding translations into a target language. Such data are\nnotoriously scarce, making synthetic data augmentation by back-translation or\nknowledge distillation a necessary ingredient of end-to-end training. In this\npaper, we present a novel approach to data augmentation that leverages audio\nalignments, linguistic properties, and translation. First, we augment a\ntranscription by sampling from a suffix memory that stores text and audio data.\nSecond, we translate the augmented transcript. Finally, we recombine\nconcatenated audio segments and the generated translation. Besides training an\nMT-system, we only use basic off-the-shelf components without fine-tuning.\nWhile having similar resource demands as knowledge distillation, adding our\nmethod delivers consistent improvements of up to 0.9 and 1.1 BLEU points on\nfive language pairs on CoVoST 2 and on two language pairs on Europarl-ST,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tsz Kin Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schamoni_S/0/1/0/all/0/1\">Shigehiko Schamoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data. (arXiv:2203.08773v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08773","description":"<p>Retrieval-based methods have been shown to be effective in NLP tasks via\nintroducing external knowledge. However, the indexing and retrieving of\nlarge-scale corpora bring considerable computational cost. Surprisingly, we\nfound that REtrieving from the traINing datA (REINA) only can lead to\nsignificant gains on multiple NLG and NLU tasks. We retrieve the labeled\ntraining instances most similar to the input text and then concatenate them\nwith the input to feed into the model to generate the output. Experimental\nresults show that this simple method can achieve significantly better\nperformance on a variety of NLU and NLG tasks, including summarization, machine\ntranslation, language modeling, and question answering tasks. For instance, our\nproposed method achieved state-of-the-art results on XSum, BigPatent, and\nCommonsenseQA. Our code is released, https://github.com/microsoft/REINA .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Siqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUE Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals. (arXiv:2203.08774v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08774","description":"<p>We propose a framework to modularize the training of neural language models\nthat use diverse forms of sentence-external context (including metadata) by\neliminating the need to jointly train sentence-external and within-sentence\nencoders. Our approach, contextual universal embeddings (CUE), trains LMs on\none set of context, such as date and author, and adapts to novel metadata\ntypes, such as article title, or previous sentence. The model consists of a\npretrained neural sentence LM, a BERT-based context encoder, and a masked\ntransformer decoder that estimates LM probabilities using sentence-internal and\nsentence-external information. When context or metadata are unavailable, our\nmodel learns to combine contextual and sentence-internal information using\nnoisy oracle unigram embeddings as a proxy. Real contextual information can be\nintroduced later and used to adapt a small number of parameters that map\ncontextual data into the decoder's embedding space. We validate the CUE\nframework on a NYTimes text corpus with multiple metadata types, for which the\nLM perplexity can be lowered from 36.6 to 27.4 by conditioning on context.\nBootstrapping a contextual LM with only a subset of the context/metadata during\ntraining retains 85\\% of the achievable gain. Training the model initially with\nproxy context retains 67% of the perplexity gain after adapting to real\ncontext. Furthermore, we can swap one type of pretrained sentence LM for\nanother without retraining the context encoders, by only adapting the decoder\nmodel. Overall, we obtain a modular framework that allows incremental, scalable\ntraining of context-enhanced LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novotney_S/0/1/0/all/0/1\">Scott Novotney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Sreeparna Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zeeshan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Shortest Rationales the Best Explanations for Human Understanding?. (arXiv:2203.08788v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08788","description":"<p>Existing self-explaining models typically favor extracting the shortest\npossible rationales - snippets of an input text \"responsible for\" corresponding\noutput - to explain the model prediction, with the assumption that shorter\nrationales are more intuitive to humans. However, this assumption has yet to be\nvalidated. Is the shortest rationale indeed the most human-understandable? To\nanswer this question, we design a self-explaining model, LimitedInk, which\nallows users to extract rationales at any target length. Compared to existing\nbaselines, LimitedInk achieves compatible end-task performance and\nhuman-annotated rationale agreement, making it a suitable representation of the\nrecent class of self-explaining models. We use LimitedInk to conduct a user\nstudy on the impact of rationale length, where we ask human judges to predict\nthe sentiment label of documents based only on LimitedInk-generated rationales\nwith different lengths. We show rationales that are too short do not help\nhumans predict labels better than randomly masked text, suggesting the need for\nmore careful design of the best human rationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenbo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Parameter Allocation Search. (arXiv:2006.10598v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.10598","description":"<p>Training neural networks requires increasing amounts of memory. Parameter\nsharing can reduce memory and communication costs, but existing methods assume\nnetworks have many identical layers and utilize hand-crafted sharing strategies\nthat fail to generalize. We introduce Neural Parameter Allocation Search\n(NPAS), a novel task where the goal is to train a neural network given an\narbitrary, fixed parameter budget. NPAS covers both low-budget regimes, which\nproduce compact networks, as well as a novel high-budget regime, where\nadditional capacity can be added to boost performance without increasing\ninference FLOPs. To address NPAS, we introduce Shapeshifter Networks (SSNs),\nwhich automatically learn where and how to share parameters in a network to\nsupport any parameter budget without requiring any changes to the architecture\nor loss function. NPAS and SSNs provide a complete framework for addressing\ngeneralized parameter sharing, and can also be combined with prior work for\nadditional performance gains. We demonstrate the effectiveness of our approach\nusing nine network architectures across four diverse tasks, including ImageNet\nclassification and transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dryden_N/0/1/0/all/0/1\">Nikoli Dryden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frost_J/0/1/0/all/0/1\">Julius Frost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1\">Torsten Hoefler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHIELD: Defending Textual Neural Networks against Multiple Black-Box Adversarial Attacks with Stochastic Multi-Expert Patcher. (arXiv:2011.08908v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.08908","description":"<p>Even though several methods have proposed to defend textual neural network\n(NN) models against black-box adversarial attacks, they often defend against a\nspecific text perturbation strategy and/or require re-training the models from\nscratch. This leads to a lack of generalization in practice and redundant\ncomputation. In particular, the state-of-the-art transformer models (e.g.,\nBERT, RoBERTa) require great time and computation resources. By borrowing an\nidea from software engineering, in order to address these limitations, we\npropose a novel algorithm, SHIELD, which modifies and re-trains only the last\nlayer of a textual NN, and thus it \"patches\" and \"transforms\" the NN into a\nstochastic weighted ensemble of multi-expert prediction heads. Considering that\nmost of current black-box attacks rely on iterative search mechanisms to\noptimize their adversarial perturbations, SHIELD confuses the attackers by\nautomatically utilizing different weighted ensembles of predictors depending on\nthe input. In other words, SHIELD breaks a fundamental assumption of the\nattack, which is a victim NN model remains constant during an attack. By\nconducting comprehensive experiments, we demonstrate that all of CNN, RNN,\nBERT, and RoBERTa-based textual NNs, once patched by SHIELD, exhibit a relative\nenhancement of 15%--70% in accuracy on average against 14 different black-box\nattacks, outperforming 6 defensive baselines across 3 public datasets. All\ncodes are to be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thai Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Noseong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Hate Speech with GPT-3. (arXiv:2103.12407v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.12407","description":"<p>Sophisticated language models such as OpenAI's GPT-3 can generate hateful\ntext that targets marginalized groups. Given this capacity, we are interested\nin whether large language models can be used to identify hate speech and\nclassify text as sexist or racist. We use GPT-3 to identify sexist and racist\ntext passages with zero-, one-, and few-shot learning. We find that with zero-\nand one-shot learning, GPT-3 can identify sexist or racist text with an\naccuracy between 57 per cent and 68 per cent depending on the category of text\nand type of learning. With few-shot learning, the model's accuracy can be as\nhigh as 88 per cent. Large language models have a role to play in hate speech\ndetection, and with further development could eventually be used to counter\nhate speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_K/0/1/0/all/0/1\">Ke-Li Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_A/0/1/0/all/0/1\">Annie Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexander_R/0/1/0/all/0/1\">Rohan Alexander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on reinforcement learning for language processing. (arXiv:2104.05565v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05565","description":"<p>In recent years some researchers have explored the use of reinforcement\nlearning (RL) algorithms as key components in the solution of various natural\nlanguage processing tasks. For instance, some of these algorithms leveraging\ndeep neural learning have found their way into conversational systems. This\npaper reviews the state of the art of RL methods for their possible use for\ndifferent problems of natural language processing, focusing primarily on\nconversational systems, mainly due to their growing relevance. We provide\ndetailed descriptions of the problems as well as discussions of why RL is\nwell-suited to solve them. Also, we analyze the advantages and limitations of\nthese methods. Finally, we elaborate on promising research directions in\nnatural language processing that might benefit from reinforcement learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uc_Cetina_V/0/1/0/all/0/1\">Victor Uc-Cetina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_Guerrero_N/0/1/0/all/0/1\">Nicolas Navarro-Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Gonzalez_A/0/1/0/all/0/1\">Anabel Martin-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining. (arXiv:2104.07642v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07642","description":"<p>This work presents methods for learning cross-lingual sentence\nrepresentations using paired or unpaired bilingual texts. We hypothesize that\nthe cross-lingual alignment strategy is transferable, and therefore a model\ntrained to align only two languages can encode multilingually more aligned\nrepresentations. We thus introduce dual-pivot transfer: training on one\nlanguage pair and evaluating on other pairs. To study this theory, we design\nunsupervised models trained on unpaired sentences and single-pair supervised\nmodels trained on bitexts, both based on the unsupervised language model XLM-R\nwith its parameters frozen. The experiments evaluate the models as universal\nsentence encoders on the task of unsupervised bitext mining on two datasets,\nwhere the unsupervised model reaches the state of the art of unsupervised\nretrieval, and the alternative single-pair supervised model approaches the\nperformance of multilingually supervised models. The results suggest that\nbilingual training techniques as proposed can be applied to get sentence\nrepresentations with multilingual alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tien_C/0/1/0/all/0/1\">Chih-chan Tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinert_Threlkeld_S/0/1/0/all/0/1\">Shane Steinert-Threlkeld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Premise-based Multimodal Reasoning: A Human-like Cognitive Process. (arXiv:2105.07122v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.07122","description":"<p>It is a common practice for recent works in vision language cross-modal\nreasoning to adopt a binary or multi-choice classification formulation taking\nas input a set of source image(s) and textual query. In this work, we take a\nsober look at such an unconditional formulation in the sense that no prior\nknowledge is specified with respect to the source image(s). Inspired by the\ndesigns of both visual commonsense reasoning and natural language inference\ntasks, we propose a new task termed Premise-based Multi-modal Reasoning(PMR)\nwhere a textual premise is the background presumption on each source image. The\nPMR dataset contains 15,360 manually annotated samples which are created by a\nmulti-phase crowd-sourcing process. With selected high-quality movie\nscreenshots and human-curated premise templates from 6 pre-defined categories,\nwe ask crowd-source workers to write one true hypothesis and three distractors\n(4 choices) given the premise and image through a cross-check procedure.\nBesides, we generate adversarial samples to alleviate the annotation artifacts\nand double the size of PMR. We benchmark various state-of-the-art (pretrained)\nmulti-modal inference models on PMR and conduct comprehensive experimental\nanalyses to showcase the utility of our dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Ziwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Heming Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1\">Shoujie Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Haoran Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Weidong Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zuifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator. (arXiv:2105.11589v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11589","description":"<p>Interactive robots navigating photo-realistic environments need to be trained\nto effectively leverage and handle the dynamic nature of dialogue in addition\nto the challenges underlying vision-and-language navigation (VLN). In this\npaper, we present VISITRON, a multi-modal Transformer-based navigator better\nsuited to the interactive regime inherent to Cooperative Vision-and-Dialog\nNavigation (CVDN). VISITRON is trained to: i) identify and associate\nobject-level concepts and semantics between the environment and dialogue\nhistory, ii) identify when to interact vs. navigate via imitation learning of a\nbinary classification head. We perform extensive pre-training and fine-tuning\nablations with VISITRON to gain empirical insights and improve performance on\nCVDN. VISITRON's ability to identify when to interact leads to a natural\ngeneralization of the game-play mode introduced by Roman et al.\n(<a href=\"/abs/2005.00728\">arXiv:2005.00728</a>) for enabling the use of such models in different\nenvironments. VISITRON is competitive with models on the static CVDN\nleaderboard and attains state-of-the-art performance on the Success weighted by\nPath Length (SPL) metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1\">Robinson Piramuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan T&#xfc;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-T&#xfc;r</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REAM$\\sharp$: An Enhancement Approach to Reference-based Evaluation Metrics for Open-domain Dialog Generation. (arXiv:2105.14488v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14488","description":"<p>The lack of reliable automatic evaluation metrics is a major impediment to\nthe development of open-domain dialogue systems. Various reference-based\nmetrics have been proposed to calculate a score between a predicted response\nand a small set of references. However, these metrics show unsatisfactory\ncorrelations with human judgments. For a reference-based metric, its\nreliability mainly depends on two factors: its ability to measure the\nsimilarity between the predicted response and the reference response, as well\nas the reliability of the given reference set. Yet, there are few discussions\non the latter. Our work attempts to fill this vacancy. We first clarify an\nassumption on reference-based metrics that, if more high-quality references are\nadded into the reference set, the reliability of the metric will increase.\nNext, we present REAM$\\sharp$: an enhancement approach to Reference-based\nEvAluation Metrics for open-domain dialogue systems. A prediction model is\ndesigned to estimate the reliability of the given reference set. We show how\nits predicted results can be helpful to augment the reference set, and thus\nimprove the reliability of the metric. Experiments validate both the\neffectiveness of our prediction model and that the reliability of\nreference-based metrics improves with the augmented reference sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Hyperbolic Neural Networks. (arXiv:2105.14686v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14686","description":"<p>Hyperbolic neural networks have shown great potential for modeling complex\ndata. However, existing hyperbolic networks are not completely hyperbolic, as\nthey encode features in a hyperbolic space yet formalize most of their\noperations in the tangent space (a Euclidean subspace) at the origin of the\nhyperbolic space. This hybrid method greatly limits the modeling ability of\nnetworks. In this paper, we propose a fully hyperbolic framework to build\nhyperbolic networks based on the Lorentz model by adapting the Lorentz\ntransformations (including boost and rotation) to formalize essential\noperations of neural networks. Moreover, we also prove that linear\ntransformation in tangent spaces used by existing hyperbolic networks is a\nrelaxation of the Lorentz rotation and does not include the boost, implicitly\nlimiting the capabilities of existing hyperbolic networks. The experimental\nresults on four NLP tasks show that our method has better performance for\nbuilding both shallow and deep networks. Our code will be released to\nfacilitate follow-up research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hexu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues. (arXiv:2106.01006v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01006","description":"<p>Inferring social relations from dialogues is vital for building emotionally\nintelligent robots to interpret human language better and act accordingly. We\nmodel the social network as an And-or Graph, named SocAoG, for the consistency\nof relations among a group and leveraging attributes as inference cues.\nMoreover, we formulate a sequential structure prediction task, and propose an\n$\\alpha$-$\\beta$-$\\gamma$ strategy to incrementally parse SocAoG for the\ndynamic inference upon any incoming utterance: (i) an $\\alpha$ process\npredicting attributes and relations conditioned on the semantics of dialogues,\n(ii) a $\\beta$ process updating the social relations based on related\nattributes, and (iii) a $\\gamma$ process updating individual's attributes based\non interpersonal social relations. Empirical results on DialogRE and MovieGraph\nshow that our model infers social relations more accurately than the\nstate-of-the-art methods. Moreover, the ablation study shows the three\nprocesses complement each other, and the case study demonstrates the dynamic\nrelational inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expressivity of Emergent Language is a Trade-off between Contextual Complexity and Unpredictability. (arXiv:2106.03982v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.03982","description":"<p>Researchers are using deep learning models to explore the emergence of\nlanguage in various language games, where agents interact and develop an\nemergent language to solve tasks. We focus on the factors that determine the\nexpressivity of emergent languages, which reflects the amount of information\nabout input spaces those languages are capable of encoding. We measure the\nexpressivity of emergent languages based on the generalisation performance\nacross different games, and demonstrate that the expressivity of emergent\nlanguages is a trade-off between the complexity and unpredictability of the\ncontext those languages emerged from. Another contribution of this work is the\ndiscovery of message type collapse, i.e. the number of unique messages is lower\nthan that of inputs. We also show that using the contrastive loss proposed by\nChen et al. (2020) can alleviate this problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shangmin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathewson_K/0/1/0/all/0/1\">Kory Mathewson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirby_S/0/1/0/all/0/1\">Simon Kirby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1\">Stefano V. Albrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kenny Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding. (arXiv:2106.07250v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07250","description":"<p>In knowledge graph embedding, the theoretical relationship between the\nsoftmax cross-entropy and negative sampling loss functions has not been\ninvestigated. This makes it difficult to fairly compare the results of the two\ndifferent loss functions. We attempted to solve this problem by using the\nBregman divergence to provide a unified interpretation of the softmax\ncross-entropy and negative sampling loss functions. Under this interpretation,\nwe can derive theoretical findings for fair comparison. Experimental results on\nthe FB15k-237 and WN18RR datasets show that the theoretical findings are valid\nin practical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1\">Hidetaka Kamigaito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1\">Katsuhiko Hayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question Answering Infused Pre-training of General-Purpose Contextualized Representations. (arXiv:2106.08190v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08190","description":"<p>We propose a pre-training objective based on question answering (QA) for\nlearning general-purpose contextual representations, motivated by the intuition\nthat the representation of a phrase in a passage should encode all questions\nthat the phrase can answer in context. To this end, we train a bi-encoder QA\nmodel, which independently encodes passages and questions, to match the\npredictions of a more accurate cross-encoder model on 80 million synthesized QA\npairs. By encoding QA-relevant information, the bi-encoder's token-level\nrepresentations are useful for non-QA downstream tasks without extensive (or in\nsome cases, any) fine-tuning. We show large improvements over both\nRoBERTa-large and previous state-of-the-art results on zero-shot and few-shot\nparaphrase detection on four datasets, few-shot named entity recognition on two\ndatasets, and zero-shot sentiment analysis on three datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at How Fine-tuning Changes BERT. (arXiv:2106.14282v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.14282","description":"<p>Given the prevalence of pre-trained contextualized representations in today's\nNLP, there have been many efforts to understand what information they contain,\nand why they seem to be universally successful. The most common approach to use\nthese representations involves fine-tuning them for an end task. Yet, how\nfine-tuning changes the underlying embedding space is less studied. In this\nwork, we study the English BERT family and use two probing techniques to\nanalyze how fine-tuning changes the space. We hypothesize that fine-tuning\naffects classification performance by increasing the distances between examples\nassociated with different labels. We confirm this hypothesis with carefully\ndesigned experiments on five different NLP tasks. Via these experiments, we\nalso discover an exception to the prevailing wisdom that \"fine-tuning always\nimproves performance\". Finally, by comparing the representations before and\nafter fine-tuning, we discover that fine-tuning does not introduce arbitrary\nchanges to representations; instead, it adjusts the representations to\ndownstream tasks while largely preserving the original spatial structure of the\ndata points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Investigation of the (In)effectiveness of Counterfactually Augmented Data. (arXiv:2107.00753v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.00753","description":"<p>While pretrained language models achieve excellent performance on natural\nlanguage understanding benchmarks, they tend to rely on spurious correlations\nand generalize poorly to out-of-distribution (OOD) data. Recent work has\nexplored using counterfactually-augmented data (CAD) -- data generated by\nminimally perturbing examples to flip the ground-truth label -- to identify\nrobust features that are invariant under distribution shift. However, empirical\nresults using CAD for OOD generalization have been mixed. To explain this\ndiscrepancy, we draw insights from a linear Gaussian model and demonstrate the\npitfalls of CAD. Specifically, we show that (a) while CAD is effective at\nidentifying robust features, it may prevent the model from learning unperturbed\nrobust features; and (b) CAD may exacerbate existing spurious correlations in\nthe data. On two crowdsourced CAD datasets, our results show that the lack of\nperturbation diversity limits their effectiveness on OOD generalization,\ncalling for innovative crowdsourcing procedures to elicit diverse perturbation\nof examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nitish Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes. (arXiv:2107.08929v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.08929","description":"<p>We introduce MemSum (Multi-step Episodic Markov decision process extractive\nSUMmarizer), a reinforcement-learning-based extractive summarizer enriched at\neach step with information on the current extraction history. When MemSum\niteratively selects sentences into the summary, it considers a broad\ninformation set that would intuitively also be used by humans in this task: 1)\nthe text content of the sentence, 2) the global text context of the rest of the\ndocument, and 3) the extraction history consisting of the set of sentences that\nhave already been extracted. With a lightweight architecture, MemSum obtains\nstate-of-the-art test-set performance (ROUGE) in summarizing long documents\ntaken from PubMed, arXiv, and GovReport. Ablation studies demonstrate the\nimportance of local, global, and history information. A human evaluation\nconfirms the high quality and low redundancy of the generated summaries,\nstemming from MemSum's awareness of extraction history.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Nianlong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ash_E/0/1/0/all/0/1\">Elliott Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahnloser_R/0/1/0/all/0/1\">Richard H.R. Hahnloser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs. (arXiv:2107.14795v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.14795","description":"<p>A central goal of machine learning is the development of systems that can\nsolve many problems in as many data domains as possible. Current architectures,\nhowever, cannot be applied beyond a small set of stereotyped settings, as they\nbake in domain &amp; task assumptions or scale poorly to large inputs or outputs.\nIn this work, we propose Perceiver IO, a general-purpose architecture that\nhandles data from arbitrary settings while scaling linearly with the size of\ninputs and outputs. Our model augments the Perceiver with a flexible querying\nmechanism that enables outputs of various sizes and semantics, doing away with\nthe need for task-specific architecture engineering. The same architecture\nachieves strong results on tasks spanning natural language and visual\nunderstanding, multi-task and multi-modal reasoning, and StarCraft II. As\nhighlights, Perceiver IO outperforms a Transformer-based BERT baseline on the\nGLUE language benchmark despite removing input tokenization and achieves\nstate-of-the-art performance on Sintel optical flow estimation with no explicit\nmechanisms for multiscale correspondence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1\">Carl Doersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1\">Catalin Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoran_D/0/1/0/all/0/1\">Daniel Zoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matthew M. Botvinick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#x101;o Carreira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASR-GLUE: A New Multi-task Benchmark for ASR-Robust Natural Language Understanding. (arXiv:2108.13048v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13048","description":"<p>Language understanding in speech-based systems have attracted much attention\nin recent years with the growing demand for voice interface applications.\nHowever, the robustness of natural language understanding (NLU) systems to\nerrors introduced by automatic speech recognition (ASR) is under-examined. %To\nfacilitate the research on ASR-robust general language understanding, In this\npaper, we propose ASR-GLUE benchmark, a new collection of 6 different NLU tasks\nfor evaluating the performance of models under ASR error across 3 different\nlevels of background noise and 6 speakers with various voice characteristics.\nBased on the proposed benchmark, we systematically investigate the effect of\nASR error on NLU tasks in terms of noise intensity, error type and speaker\nvariants. We further purpose two ways, correction-based method and data\naugmentation-based method to improve robustness of the NLU systems. Extensive\nexperimental results and analysises show that the proposed methods are\neffective to some extent, but still far from human performance, demonstrating\nthat NLU under ASR error is still very challenging and requires further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lingyun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-to-Table: A New Way of Information Extraction. (arXiv:2109.02707v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02707","description":"<p>We study a new problem setting of information extraction (IE), referred to as\ntext-to-table. In text-to-table, given a text, one creates a table or several\ntables expressing the main content of the text, while the model is learned from\ntext-table pair data. The problem setting differs from those of the existing\nmethods for IE. First, the extraction can be carried out from long texts to\nlarge tables with complex structures. Second, the extraction is entirely\ndata-driven, and there is no need to explicitly define the schemas. As far as\nwe know, there has been no previous work that studies the problem. In this\nwork, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem.\nWe first employ a seq2seq model fine-tuned from a pre-trained language model to\nperform the task. We also develop a new method within the seq2seq approach,\nexploiting two additional techniques in table generation: table constraint and\ntable relation embeddings. We consider text-to-table as an inverse problem of\nthe well-studied table-to-text, and make use of four existing table-to-text\ndatasets in our experiments on text-to-table. Experimental results show that\nthe vanilla seq2seq model can outperform the baseline methods of using relation\nextraction and named entity extraction. The results also show that our method\ncan further boost the performances of the vanilla seq2seq model. We further\ndiscuss the main challenges of the proposed task. The code and data are\navailable at https://github.com/shirley-wu/text_to_table.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xueqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiacheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings. (arXiv:2109.03127v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03127","description":"<p>Recent studies have determined that the learned token embeddings of\nlarge-scale neural language models are degenerated to be anisotropic with a\nnarrow-cone shape. This phenomenon, called the representation degeneration\nproblem, facilitates an increase in the overall similarity between token\nembeddings that negatively affect the performance of the models. Although the\nexisting methods that address the degeneration problem based on observations of\nthe phenomenon triggered by the problem improves the performance of the text\ngeneration, the training dynamics of token embeddings behind the degeneration\nproblem are still not explored. In this study, we analyze the training dynamics\nof the token embeddings focusing on rare token embedding. We demonstrate that\nthe specific part of the gradient for rare token embeddings is the key cause of\nthe degeneration problem for all tokens during training stage. Based on the\nanalysis, we propose a novel method called, adaptive gradient gating (AGG). AGG\naddresses the degeneration problem by gating the specific part of the gradient\nfor rare token embeddings. Experimental results from language modeling, word\nsimilarity, and machine translation tasks quantitatively and qualitatively\nverify the effectiveness of AGG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sangwon Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jongyoon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heeseung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-min Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_W/0/1/0/all/0/1\">Woo-Jong Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Latent-State GPT for Semi-supervised Task-Oriented Dialog Systems. (arXiv:2109.04314v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04314","description":"<p>Recently, two approaches, fine-tuning large pre-trained language models and\nvariational training, have attracted significant interests, separately, for\nsemi-supervised end-to-end task-oriented dialog (TOD) systems. In this paper,\nwe propose Variational Latent-State GPT model (VLS-GPT), which is the first to\ncombine the strengths of the two approaches. Among many options of models, we\npropose the generative model and the inference model for variational learning\nof the end-to-end TOD system, both as auto-regressive language models based on\nGPT-2, which can be further trained over a mix of labeled and unlabeled dialog\ndata in a semi-supervised manner. Variational training of VLS-GPT is both\nstatistically and computationally more challenging than previous variational\nlearning works for sequential latent variable models, which use turn-level\nfirst-order Markovian. The inference model in VLS-GPT is non-Markovian due to\nthe use of the Transformer architecture. In this work, we establish Recursive\nMonte Carlo Approximation (RMCA) to the variational objective with\nnon-Markovian inference model and prove its unbiasedness. Further, we develop\nthe computational strategy of sampling-then-forward-computation to realize\nRMCA, which successfully overcomes the memory explosion issue of using GPT in\nvariational learning and speeds up training. Semi-supervised TOD experiments\nare conducted on two benchmark multi-domain datasets of different languages -\nMultiWOZ2.1 and CrossWOZ. VLS-GPT is shown to significantly outperform both\nsupervised-only and semi-supervised self-training baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yucheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenru Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Junlan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reframing Instructional Prompts to GPTk's Language. (arXiv:2109.07830v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07830","description":"<p>What kinds of instructional prompts are easier to follow for Language Models\n(LMs)? We study this question by conducting extensive empirical analysis that\nshed light on important features of successful instructional prompts.\nSpecifically, we study several classes of reframing techniques for manual\nreformulation of prompts into more effective ones. Some examples include\ndecomposing a complex task instruction into multiple simpler tasks or itemizing\ninstructions into sequential steps. Our experiments compare the zero-shot and\nfew-shot performance of LMs prompted with reframed instructions on 12 NLP tasks\nacross 6 categories. Compared with original instructions, our reframed\ninstructions lead to significant improvements across LMs with different sizes.\nFor example, the same reframed prompts boost few-shot performance of\nGPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all\ntasks. Furthermore, reframed instructions reduce the number of examples\nrequired to prompt LMs in the few-shot setting. We hope these\nempirically-driven techniques will pave the way towards more effective future\nprompting algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoNet: Pooling Network for Efficient Token Mixing in Long Sequences. (arXiv:2110.02442v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02442","description":"<p>Transformer-based models have achieved great success in various NLP, vision,\nand speech tasks. However, the core of Transformer, the self-attention\nmechanism, has a quadratic time and memory complexity with respect to the\nsequence length, which hinders applications of Transformer-based models to long\nsequences. Many approaches have been proposed to mitigate this problem, such as\nsparse attention mechanisms, low-rank matrix approximations and scalable\nkernels, and token mixing alternatives to self-attention. We propose a novel\nPooling Network (PoNet) for token mixing in long sequences with linear\ncomplexity. We design multi-granularity pooling and pooling fusion to capture\ndifferent levels of contextual information and combine their interactions with\ntokens. On the Long Range Arena benchmark, PoNet significantly outperforms\nTransformer and achieves competitive accuracy, while being only slightly slower\nthan the fastest model, FNet, across all sequence lengths measured on GPUs. We\nalso conduct systematic studies on the transfer learning capability of PoNet\nand observe that PoNet achieves 95.7% of the accuracy of BERT on the GLUE\nbenchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis\ndemonstrates effectiveness of the designed multi-granularity pooling and\npooling fusion for token mixing in long sequences and efficacy of the designed\npre-training tasks for PoNet to learn transferable contextualized language\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chao-Hong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Generalization in Dependency Parsing. (arXiv:2110.06843v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06843","description":"<p>Compositionality -- the ability to combine familiar units like words into\nnovel phrases and sentences -- has been the focus of intense interest in\nartificial intelligence in recent years. To test compositional generalization\nin semantic parsing, Keysers et al. (2020) introduced Compositional Freebase\nQueries (CFQ). This dataset maximizes the similarity between the test and train\ndistributions over primitive units, like words, while maximizing the compound\ndivergence: the dissimilarity between test and train distributions over larger\nstructures, like phrases. Dependency parsing, however, lacks a compositional\ngeneralization benchmark. In this work, we introduce a gold-standard set of\ndependency parses for CFQ, and use this to analyze the behavior of a\nstate-of-the art dependency parser (Qi et al., 2020) on the CFQ dataset. We\nfind that increasing compound divergence degrades dependency parsing\nperformance, although not as dramatically as semantic parsing performance.\nAdditionally, we find the performance of the dependency parser does not\nuniformly degrade relative to compound divergence, and the parser performs\ndifferently on different splits with the same compound divergence. We explore a\nnumber of hypotheses for what causes the non-uniform degradation in dependency\nparsing performance, and identify a number of syntactic structures that drive\nthe dependency parser's lower performance on the most challenging splits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goodwin_E/0/1/0/all/0/1\">Emily Goodwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_T/0/1/0/all/0/1\">Timothy J. O&#x27;Donnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracing Origins: Coreference-aware Machine Reading Comprehension. (arXiv:2110.07961v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07961","description":"<p>Machine reading comprehension is a heavily-studied research and test field\nfor evaluating new pre-trained language models (PrLMs) and fine-tuning\nstrategies, and recent studies have enriched the pre-trained language models\nwith syntactic, semantic and other linguistic information to improve the\nperformance of the models. In this paper, we imitate the human reading process\nin connecting the anaphoric expressions and explicitly leverage the coreference\ninformation of the entities to enhance the word embeddings from the pre-trained\nlanguage model, in order to highlight the coreference mentions of the entities\nthat must be identified for coreference-intensive question answering in QUOREF,\na relatively new dataset that is specifically designed to evaluate the\ncoreference-related performance of a model. We use two strategies to fine-tune\na pre-trained language model, namely, placing an additional encoder layer after\na pre-trained language model to focus on the coreference mentions or\nconstructing a relational graph convolutional network to model the coreference\nrelations. We demonstrate that the explicit incorporation of coreference\ninformation in the fine-tuning stage performs better than the incorporation of\nthe coreference information in pre-training a language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baorong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BBQ: A Hand-Built Bias Benchmark for Question Answering. (arXiv:2110.08193v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08193","description":"<p>It is well documented that NLP models learn social biases, but little work\nhas been done on how these biases manifest in model outputs for applied tasks\nlike question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a\ndataset of question sets constructed by the authors that highlight attested\nsocial biases against people belonging to protected classes along nine social\ndimensions relevant for U.S. English-speaking contexts. Our task evaluates\nmodel responses at two levels: (i) given an under-informative context, we test\nhow strongly responses reflect social biases, and (ii) given an adequately\ninformative context, we test whether the model's biases override a correct\nanswer choice. We find that models often rely on stereotypes when the context\nis under-informative, meaning the model's outputs consistently reproduce\nharmful biases in this setting. Though models are more accurate when the\ncontext provides an informative answer, they still rely on stereotypes and\naverage up to 3.4 percentage points higher accuracy when the correct answer\naligns with a social bias than when it conflicts, with this difference widening\nto over 5 points on examples targeting gender for most models tested.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1\">Alicia Parrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nangia_N/0/1/0/all/0/1\">Nikita Nangia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_J/0/1/0/all/0/1\">Jana Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Htut_P/0/1/0/all/0/1\">Phu Mon Htut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coherence boosting: When your pretrained language model is not paying enough attention. (arXiv:2110.08294v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08294","description":"<p>Long-range semantic coherence remains a challenge in automatic language\ngeneration and understanding. We demonstrate that large language models have\ninsufficiently learned the effect of distant words on next-token prediction. We\npresent coherence boosting, an inference procedure that increases a LM's focus\non a long context. We show the benefits of coherence boosting with pretrained\nmodels by distributional analyses of generated ordinary text and dialog\nresponses. It is also found that coherence boosting with state-of-the-art\nmodels for various zero-shot NLP tasks yields performance gains with no\nadditional training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malkin_N/0/1/0/all/0/1\">Nikolay Malkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1\">Nebojsa Jojic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Procedural Knowledge by Sequencing Multimodal Instructional Manuals. (arXiv:2110.08486v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08486","description":"<p>The ability to sequence unordered events is an essential skill to comprehend\nand reason about real world task procedures, which often requires thorough\nunderstanding of temporal common sense and multimodal information, as these\nprocedures are often communicated through a combination of texts and images.\nSuch capability is essential for applications such as sequential task planning\nand multi-source instruction summarization. While humans are capable of\nreasoning about and sequencing unordered multimodal procedural instructions,\nwhether current machine learning models have such essential capability is still\nan open question. In this work, we benchmark models' capability of reasoning\nover and sequencing unordered multimodal instructions by curating datasets from\npopular online instructional manuals and collecting comprehensive human\nannotations. We find models not only perform significantly worse than humans\nbut also seem incapable of efficiently utilizing the multimodal information. To\nimprove machines' performance on multimodal event sequencing, we propose\nsequentiality-aware pretraining techniques that exploit the sequential\nalignment properties of both texts and images, resulting in &gt; 5% significant\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1\">Alex Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alipoormolabashi_P/0/1/0/all/0/1\">Pegah Alipoormolabashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1\">Marjorie Freedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1\">Ralph Weischedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sharpness-Aware Minimization Improves Language Model Generalization. (arXiv:2110.08529v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08529","description":"<p>The allure of superhuman-level capabilities has led to considerable interest\nin language models like GPT-3 and T5, wherein the research has, by and large,\nrevolved around new model architectures, training tasks, and loss objectives,\nalong with substantial engineering efforts to scale up model capacity and\ndataset size. Comparatively little work has been done to improve the\ngeneralization of these models through better optimization. In this work, we\nshow that Sharpness-Aware Minimization (SAM), a recently proposed optimization\nprocedure that encourages convergence to flatter minima, can substantially\nimprove the generalization of language models without much computational\noverhead. We show that SAM is able to boost performance on SuperGLUE, GLUE, Web\nQuestions, Natural Questions, Trivia QA, and TyDiQA, with particularly large\ngains when training data for these tasks is limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobahi_H/0/1/0/all/0/1\">Hossein Mobahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Efficiency Misnomer. (arXiv:2110.12894v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.12894","description":"<p>Model efficiency is a critical aspect of developing and deploying machine\nlearning models. Inference time and latency directly affect the user\nexperience, and some applications have hard requirements. In addition to\ninference costs, model training also have direct financial and environmental\nimpacts. Although there are numerous well-established metrics (cost indicators)\nfor measuring model efficiency, researchers and practitioners often assume that\nthese metrics are correlated with each other and report only few of them. In\nthis paper, we thoroughly discuss common cost indicators, their advantages and\ndisadvantages, and how they can contradict each other. We demonstrate how\nincomplete reporting of cost indicators can lead to partial conclusions and a\nblurred or incomplete picture of the practical considerations of different\nmodels. We further present suggestions to improve reporting of efficiency\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1\">Ashish Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Cross-Lingual Machine Reading Comprehension via Inter-sentence Dependency Graph. (arXiv:2112.00503v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.00503","description":"<p>We target the task of cross-lingual Machine Reading Comprehension (MRC) in\nthe direct zero-shot setting, by incorporating syntactic features from\nUniversal Dependencies (UD), and the key features we use are the syntactic\nrelations within each sentence. While previous work has demonstrated effective\nsyntax-guided MRC models, we propose to adopt the inter-sentence syntactic\nrelations, in addition to the rudimentary intra-sentence relations, to further\nutilize the syntactic dependencies in the multi-sentence input of the MRC task.\nIn our approach, we build the Inter-Sentence Dependency Graph (ISDG) connecting\ndependency trees to form global syntactic relations across sentences. We then\npropose the ISDG encoder that encodes the global dependency graph, addressing\nthe inter-sentence relations via both one-hop and multi-hop dependency paths\nexplicitly. Experiments on three multilingual MRC datasets (XQuAD, MLQA,\nTyDiQA-GoldP) show that our encoder that is only trained on English is able to\nimprove the zero-shot performance on all 14 test sets covering 8 languages,\nwith up to 3.8 F1 / 5.2 EM improvement on-average, and 5.2 F1 / 11.2 EM on\ncertain languages. Further analysis shows the improvement can be attributed to\nthe attention on the cross-linguistically consistent syntactic path.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_B/0/1/0/all/0/1\">Bo Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingchao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sharpness-Aware Minimization with Dynamic Reweighting. (arXiv:2112.08772v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.08772","description":"<p>Deep neural networks are often overparameterized and may not easily achieve\nmodel generalization. Adversarial training has shown effectiveness in improving\ngeneralization by regularizing the change of loss on top of adversarially\nchosen perturbations. The recently proposed sharpness-aware minimization (SAM)\nalgorithm conducts adversarial weight perturbation, encouraging the model to\nconverge to a flat minima. Unfortunately, due to increased computational cost,\nadversarial weight perturbation can only be efficiently estimated per-batch\ninstead of per-instance by SAM, leading to degraded performance. In this paper,\nwe tackle this efficiency bottleneck and propose the first instance-based\nweight perturbation method: sharpness-aware minimization with dynamic\nreweighting ({\\delta}-SAM). {\\delta}-SAM dynamically reweights perturbation\nwithin each batch by estimated guardedness (i.e. unguarded instances are\nup-weighted), serving as a better approximation to per-instance perturbation.\nExperiments on various tasks demonstrate the effectiveness of {\\delta}-SAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Leakage in Elicited Natural Language Inference Datasets. (arXiv:2112.09237v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09237","description":"<p>Natural language inference (NLI) is an important task for producing useful\nmodels of human language. Unfortunately large-scale NLI dataset production\nrelies on crowdworkers who are prone to introduce biases in the sentences they\nwrite, enabling models to predict sentence pair relations from a single\nsentence in the pair, better than chance. This elicited sentence relation\nleakage property is undesirable as it enables models to cheat rather than learn\nthe desired reasoning capabilities, and hasn't gone away since its 2018\ndiscovery. We analyze this problem in 8 modern NLI datasets, using a\ncombination of previously established and novel model-based techniques, so as\nto enable ameliorating this leakage in future NLI datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Stage Episodic Control for Strategic Exploration in Text Games. (arXiv:2201.01251v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.01251","description":"<p>Text adventure games present unique challenges to reinforcement learning\nmethods due to their combinatorially large action spaces and sparse rewards.\nThe interplay of these two factors is particularly demanding because large\naction spaces require extensive exploration, while sparse rewards provide\nlimited feedback. This work proposes to tackle the explore-vs-exploit dilemma\nusing a multi-stage approach that explicitly disentangles these two strategies\nwithin each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins\neach episode using an exploitation policy that imitates a set of promising\ntrajectories from the past, and then switches over to an exploration policy\naimed at discovering novel actions that lead to unseen state spaces. This\npolicy decomposition allows us to combine global decisions about which parts of\nthe game space to return to with curiosity-based local exploration in that\nspace, motivated by how a human may approach these games. Our method\nsignificantly outperforms prior approaches by 27% and 11% average normalized\nscore over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in\nboth deterministic and stochastic settings, respectively. On the game of Zork1,\nin particular, XTX obtains a score of 103, more than a 2x improvement over\nprior methods, and pushes past several known bottlenecks in the game that have\nplagued previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tuyls_J/0/1/0/all/0/1\">Jens Tuyls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound. (arXiv:2201.02639v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02639","description":"<p>As humans, we navigate a multimodal world, building a holistic understanding\nfrom all our senses. We introduce MERLOT Reserve, a model that represents\nvideos jointly over time -- through a new training objective that learns from\naudio, subtitles, and video frames. Given a video, we replace snippets of text\nand audio with a MASK token; the model learns by choosing the correct\nmasked-out snippet. Our objective learns faster than alternatives, and performs\nwell at scale: we pretrain on 20 million YouTube videos.\n</p>\n<p>Empirical results show that MERLOT Reserve learns strong multimodal\nrepresentations. When finetuned, it sets state-of-the-art on Visual Commonsense\nReasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%,\nand 1.5% respectively. Ablations show that these tasks benefit from audio\npretraining -- even VCR, a QA task centered around images (without sound).\nMoreover, our objective enables out-of-the-box prediction, revealing strong\nmultimodal commonsense understanding. In a fully zero-shot setting, our model\nobtains competitive results on four video tasks, even outperforming supervised\napproaches on the recently proposed Situated Reasoning (STAR) benchmark.\n</p>\n<p>We analyze why audio enables better vision-language representations,\nsuggesting significant opportunities for future research. We conclude by\ndiscussing ethical and societal implications of multimodal pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-assisted prompt editing to improve GPT-3 after deployment. (arXiv:2201.06009v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06009","description":"<p>Large LMs such as GPT-3 are powerful, but can commit mistakes that are\nobvious to humans. For example, GPT-3 would mistakenly interpret \"What word is\nsimilar to good?\" to mean a homonym, while the user intended a synonym. Our\ngoal is to effectively correct such errors via user interactions with the\nsystem but without retraining, which will be prohibitively costly. We pair\nGPT-3 with a growing memory of recorded cases where the model misunderstood the\nuser's intents, along with user feedback for clarification. Such a memory\nallows our system to produce enhanced prompts for any new query based on the\nuser feedback for error correction on similar cases in the past. On four tasks\n(two lexical tasks, two advanced ethical reasoning tasks), we show how a\n(simulated) user can interactively teach a deployed GPT-3, substantially\nincreasing its accuracy over the queries with different kinds of\nmisunderstandings by the GPT-3. Our approach is a step towards the low-cost\nutility enhancement for very large pre-trained LMs. All the code and data is\navailable at https://github.com/madaan/memprompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pair-Level Supervised Contrastive Learning for Natural Language Inference. (arXiv:2201.10927v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.10927","description":"<p>Natural language inference (NLI) is an increasingly important task for\nnatural language understanding, which requires one to infer the relationship\nbetween the sentence pair (premise and hypothesis). Many recent works have used\ncontrastive learning by incorporating the relationship of the sentence pair\nfrom NLI datasets to learn sentence representation. However, these methods only\nfocus on comparisons with sentence-level representations. In this paper, we\npropose a Pair-level Supervised Contrastive Learning approach (PairSCL). We\nadopt a cross attention module to learn the joint representations of the\nsentence pairs. A contrastive learning objective is designed to distinguish the\nvaried classes of sentence pairs by pulling those in one class together and\npushing apart the pairs in other classes. We evaluate PairSCL on two public\ndatasets of NLI where the accuracy of PairSCL outperforms other methods by 2.1%\non average. Furthermore, our method outperforms the previous state-of-the-art\nmethod on seven transfer tasks of text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu&#x27;ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGPT: GPT Sentence Embeddings for Semantic Search. (arXiv:2202.08904v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08904","description":"<p>GPT transformers are the largest language models available, yet semantic\nsearch is dominated by BERT transformers. We present SGPT-BE and SGPT-CE for\napplying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric\nsearch.\n</p>\n<p>SGPT-BE produces semantically meaningful sentence embeddings by contrastive\nfine-tuning of only bias tensors and a novel pooling method. A 5.8 billion\nparameter SGPT-BE outperforms the best available sentence embeddings by 6%\nsetting a new state-of-the-art on BEIR. It outperforms the concurrently\nproposed OpenAI Embeddings of the 175B Davinci endpoint, which fine-tunes\n250,000 times more parameters.\n</p>\n<p>SGPT-CE uses log probabilities from GPT models without any fine-tuning. A 6.1\nbillion parameter SGPT-CE sets an unsupervised state-of-the-art on BEIR. It\nbeats the supervised state-of-the-art on 7 datasets, but significantly loses on\nother datasets. We show how this can be alleviated by adapting the prompt.\n</p>\n<p>SGPT-BE and SGPT-CE performance scales with model size. Yet, increased\nlatency, storage and compute costs should be considered. Code, models and\nresult files are freely available at https://github.com/Muennighoff/sgpt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Impact of Negative Samples of Contrastive Learning: A Case Study of Sentence Embedding. (arXiv:2202.13093v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13093","description":"<p>Contrastive learning is emerging as a powerful technique for extracting\nknowledge from unlabeled data. This technique requires a balanced mixture of\ntwo ingredients: positive (similar) and negative (dissimilar) samples. This is\ntypically achieved by maintaining a queue of negative samples during training.\nPrior works in the area typically uses a fixed-length negative sample queue,\nbut how the negative sample size affects the model performance remains unclear.\nThe opaque impact of the number of negative samples on performance when\nemploying contrastive learning aroused our in-depth exploration. This paper\npresents a momentum contrastive learning model with negative sample queue for\nsentence embedding, namely MoCoSE. We add the prediction layer to the online\nbranch to make the model asymmetric and together with EMA update mechanism of\nthe target branch to prevent the model from collapsing. We define a maximum\ntraceable distance metric, through which we learn to what extent the text\ncontrastive learning benefits from the historical information of negative\nsamples. Our experiments find that the best results are obtained when the\nmaximum traceable distance is at a certain range, demonstrating that there is\nan optimal range of historical information for a negative sample queue. We\nevaluate the proposed unsupervised MoCoSE on the semantic text similarity (STS)\ntask and obtain an average Spearman's correlation of $77.27\\%$. Source code is\navailable at https://github.com/xbdxwyh/mocose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuxin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Ling Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The impact of lexical and grammatical processing on generating code from natural language. (arXiv:2202.13972v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13972","description":"<p>Considering the seq2seq architecture of TranX for natural language to code\ntranslation, we identify four key components of importance: grammatical\nconstraints, lexical preprocessing, input representations, and copy mechanisms.\nTo study the impact of these components, we use a state-of-the-art architecture\nthat relies on BERT encoder and a grammar-based decoder for which a\nformalization is provided. The paper highlights the importance of the lexical\nsubstitution component in the current natural language to code systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beau_N/0/1/0/all/0/1\">Nathana&#xeb;l Beau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crabbe_B/0/1/0/all/0/1\">Beno&#xee;t Crabb&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure Extraction in Task-Oriented Dialogues with Slot Clustering. (arXiv:2203.00073v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00073","description":"<p>Extracting structure information from dialogue data can help us better\nunderstand user and system behaviors. In task-oriented dialogues, dialogue\nstructure has often been considered as transition graphs among dialogue states.\nHowever, annotating dialogue states manually is expensive and time-consuming.\nIn this paper, we propose a simple yet effective approach for structure\nextraction in task-oriented dialogues. We first detect and cluster possible\nslot tokens with a pre-trained model to approximate dialogue ontology for a\ntarget domain. Then we track the status of each identified token group and\nderive a state transition structure. Empirical results show that our approach\noutperforms unsupervised baseline models by far in dialogue structure\nextraction. In addition, we show that data augmentation based on extracted\nstructures enriches the surface formats of training data and can achieve a\nsignificant performance boost in dialogue response generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mukayese: Turkish NLP Strikes Back. (arXiv:2203.01215v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.01215","description":"<p>Having sufficient resources for language X lifts it from the under-resourced\nlanguages class, but not necessarily from the under-researched class. In this\npaper, we address the problem of the absence of organized benchmarks in the\nTurkish language. We demonstrate that languages such as Turkish are left behind\nthe state-of-the-art in NLP applications. As a solution, we present Mukayese, a\nset of NLP benchmarks for the Turkish language that contains several NLP tasks.\nWe work on one or more datasets for each benchmark and present two or more\nbaselines. Moreover, we present four new benchmarking datasets in Turkish for\nlanguage modeling, sentence segmentation, and spell checking. All datasets and\nbaselines are available under: https://github.com/alisafaya/mukayese\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safaya_A/0/1/0/all/0/1\">Ali Safaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtulus_E/0/1/0/all/0/1\">Emirhan Kurtulu&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goktogan_A/0/1/0/all/0/1\">Arda G&#xf6;kto&#x11f;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1\">Deniz Yuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Iterative Revision from Human-Written Text. (arXiv:2203.03802v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03802","description":"<p>Writing is, by nature, a strategic, adaptive, and more importantly, an\niterative process. A crucial part of writing is editing and revising the text.\nPrevious works on text revision have focused on defining edit intention\ntaxonomies within a single domain or developing computational models with a\nsingle level of edit granularity, such as sentence-level edits, which differ\nfrom human's revision cycles. This work describes IteraTeR: the first\nlarge-scale, multi-domain, edit-intention annotated corpus of iteratively\nrevised text. In particular, IteraTeR is collected based on a new framework to\ncomprehensively model the iterative text revisions that generalize to various\ndomains of formal writing, edit intentions, revision depths, and granularities.\nWhen we incorporate our annotated edit intentions, both generative and\nedit-based text revision models significantly improve automatic evaluations.\nThrough our work, we better understand the text revision process, making vital\nconnections between edit intentions and writing quality, enabling the creation\nof diverse corpora to support computational modeling of iterative text\nrevisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raheja_V/0/1/0/all/0/1\">Vipul Raheja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Dhruv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Z/0/1/0/all/0/1\">Zae Myung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_M/0/1/0/all/0/1\">Melissa Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sense Embeddings are also Biased--Evaluating Social Biases in Static and Contextualised Sense Embeddings. (arXiv:2203.07523v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07523","description":"<p>Sense embedding learning methods learn different embeddings for the different\nsenses of an ambiguous word. One sense of an ambiguous word might be socially\nbiased while its other senses remain unbiased. In comparison to the numerous\nprior work evaluating the social biases in pretrained word embeddings, the\nbiases in sense embeddings have been relatively understudied. We create a\nbenchmark dataset for evaluating the social biases in sense embeddings and\npropose novel sense-specific bias evaluation measures. We conduct an extensive\nevaluation of multiple static and contextualised sense embeddings for various\ntypes of social biases using the proposed measures. Our experimental results\nshow that even in cases where no biases are found at word-level, there still\nexist worrying levels of social biases at sense-level, which are often ignored\nby the word-level bias evaluation measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1\">Masahiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue. (arXiv:2203.07657v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07657","description":"<p>Effective human-chatbot conversations need to achieve both coherence and\nefficiency. Complex conversation settings such as persuasion involve\ncommunicating changes in attitude or behavior, so users' perspectives need to\nbe carefully considered and addressed, even when not directly related to the\ntopic. In this work, we contribute a novel modular dialogue system framework\nthat seamlessly integrates factual information and social content into\npersuasive dialogue. Our framework is generalizable to any dialogue tasks that\nhave mixed social and task contents. We conducted a study that compared user\nevaluations of our framework versus a baseline end-to-end generation model. We\nfound our model was evaluated to be more favorable in all dimensions including\ncompetence and friendliness compared to the baseline model which does not\nexplicitly handle social content or factual questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maximillian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_F/0/1/0/all/0/1\">Feifan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Ryan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Saurav Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Energy-Latency Attacks via Sponge Poisoning. (arXiv:2203.08147v1 [cs.CR])","link":"http://arxiv.org/abs/2203.08147","description":"<p>Sponge examples are test-time inputs carefully-optimized to increase energy\nconsumption and latency of neural networks when deployed on hardware\naccelerators. In this work, we demonstrate that sponge attacks can also be\nimplanted at training time, when model training is outsourced to a third party,\nvia an attack that we call sponge poisoning. This attack allows one to increase\nthe energy consumption and latency of machine-learning models indiscriminately\non each test-time input. We present a novel formalization for sponge poisoning,\novercoming the limitations related to the optimization of test-time sponge\nexamples, and show that this attack is possible even if the attacker only\ncontrols a few poisoning samples and model updates. Our extensive experimental\nanalysis, involving two deep learning architectures and three datasets, shows\nthat sponge poisoning can almost completely vanish the effect of such hardware\naccelerators. Finally, we analyze activations of the resulting sponge models,\nidentifying the module components that are more sensitive to this\nvulnerability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cina_A/0/1/0/all/0/1\">Antonio Emanuele Cin&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1\">Ambra Demontis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1\">Fabio Roli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelillo_M/0/1/0/all/0/1\">Marcello Pelillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNet Architectures in Multiplanar Volumetric Segmentation -- Validated on Three Knee MRI Cohorts. (arXiv:2203.08194v1 [eess.IV])","link":"http://arxiv.org/abs/2203.08194","description":"<p>UNet has become the gold standard method for segmenting 2D medical images\nthat any new method must be validated against. However, in recent years,\nseveral variations of the seminal UNet have been proposed with promising\nresults. However, there is no clear consensus on the generalisability of these\narchitectures, and UNet currently remains the methodological gold standard. The\npurpose of this study was to evaluate some of the most promising UNet-inspired\narchitectures for 3D segmentation. For the segmentation of 3D scans,\nUNet-inspired methods are also dominant, but there is a larger variety across\napplications. By evaluating the architectures in a different dimensionality,\nembedded in a different method, and for a different task, we aimed to evaluate\nif any of these UNet-alternatives are promising as a new gold standard that\ngeneralizes even better than UNet. Specifically, we investigated the\narchitectures as the central 2D segmentation core in the Multi-Planar Unet 3D\nsegmentation method that previously demonstrated excellent generalization in\nthe MICCAI Segmentation Decathlon. Generalisability can be demonstrated if a\npromising UNet-variant consistently outperforms UNet in this setting. For this\npurpose, we evaluated four architectures for cartilage segmentation from three\ndifferent cohorts with knee MRIs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sengara_S/0/1/0/all/0/1\">Sandeep Singh Sengara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meulengrachtb_C/0/1/0/all/0/1\">Christopher Meulengrachtb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boesenb_M/0/1/0/all/0/1\">Mikael Ploug Boesenb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Overgaardb_A/0/1/0/all/0/1\">Anders F&#xf8;hrby Overgaardb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gudbergsenb_H/0/1/0/all/0/1\">Henrik Gudbergsenb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nybingb_J/0/1/0/all/0/1\">Janus Damm Nybingb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dam_E/0/1/0/all/0/1\">Erik Bj&#xf8;rnager Dam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection. (arXiv:2203.08195v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08195","description":"<p>Lidars and cameras are critical sensors that provide complementary\ninformation for 3D detection in autonomous driving. While prevalent multi-modal\nmethods simply decorate raw lidar point clouds with camera features and feed\nthem directly to existing 3D detection models, our study shows that fusing\ncamera features with deep lidar features instead of raw points, can lead to\nbetter performance. However, as those features are often augmented and\naggregated, a key challenge in fusion is how to effectively align the\ntransformed features from two modalities. In this paper, we propose two novel\ntechniques: InverseAug that inverses geometric-related augmentations, e.g.,\nrotation, to enable accurate geometric alignment between lidar points and image\npixels, and LearnableAlign that leverages cross-attention to dynamically\ncapture the correlations between image and lidar features during fusion. Based\non InverseAug and LearnableAlign, we develop a family of generic multi-modal 3D\ndetection models named DeepFusion, which is more accurate than previous\nmethods. For example, DeepFusion improves PointPillars, CenterPoint, and 3D-MAN\nbaselines on Pedestrian detection for 6.7, 8.9, and 6.2 LEVEL_2 APH,\nrespectively. Notably, our models achieve state-of-the-art performance on Waymo\nOpen Dataset, and show strong model robustness against input corruptions and\nout-of-distribution data. Code will be publicly available at\nhttps://github.com/tensorflow/lingvo/tree/master/lingvo/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1\">Tianjian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1\">Ben Caine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngiam_J/0/1/0/all/0/1\">Jiquan Ngiam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Daiyi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Junyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yifeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SocialVAE: Human Trajectory Prediction using Timewise Latents. (arXiv:2203.08207v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08207","description":"<p>Predicting pedestrian movement is critical for human behavior analysis and\nalso for safe and efficient human-agent interactions. However, despite\nsignificant advancements, it is still challenging for existing approaches to\ncapture the uncertainty and multimodality of human navigation decision making.\nIn this paper, we propose SocialVAE, a novel approach for human trajectory\nprediction. The core of SocialVAE is a timewise variational autoencoder\narchitecture that exploits stochastic recurrent neural networks to perform\nprediction, combined with a social attention mechanism and backward posterior\napproximation to allow for better extraction of pedestrian navigation\nstrategies. We show that SocialVAE improves current state-of-the-art\nperformance on several pedestrian trajectory prediction benchmarks, including\nthe ETH/UCY benchmark, the Stanford Drone Dataset and SportVU NBA movement\ndataset. Code is available at: {\\tt https://github.com/xupei0610/SocialVAE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayet_J/0/1/0/all/0/1\">Jean-Bernard Hayet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamouzas_I/0/1/0/all/0/1\">Ioannis Karamouzas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HUMUS-Net: Hybrid unrolled multi-scale network architecture for accelerated MRI reconstruction. (arXiv:2203.08213v1 [eess.IV])","link":"http://arxiv.org/abs/2203.08213","description":"<p>In accelerated MRI reconstruction, the anatomy of a patient is recovered from\na set of under-sampled and noisy measurements. Deep learning approaches have\nbeen proven to be successful in solving this ill-posed inverse problem and are\ncapable of producing very high quality reconstructions. However, current\narchitectures heavily rely on convolutions, that are content-independent and\nhave difficulties modeling long-range dependencies in images. Recently,\nTransformers, the workhorse of contemporary natural language processing, have\nemerged as powerful building blocks for a multitude of vision tasks. These\nmodels split input images into non-overlapping patches, embed the patches into\nlower-dimensional tokens and utilize a self-attention mechanism that does not\nsuffer from the aforementioned weaknesses of convolutional architectures.\nHowever, Transformers incur extremely high compute and memory cost when 1) the\ninput image resolution is high and 2) when the image needs to be split into a\nlarge number of patches to preserve fine detail information, both of which are\ntypical in low-level vision problems such as MRI reconstruction, having a\ncompounding effect. To tackle these challenges, we propose HUMUS-Net, a hybrid\narchitecture that combines the beneficial implicit bias and efficiency of\nconvolutions with the power of Transformer blocks in an unrolled and\nmulti-scale network. HUMUS-Net extracts high-resolution features via\nconvolutional blocks and refines low-resolution features via a novel\nTransformer-based multi-scale feature extractor. Features from both levels are\nthen synthesized into a high-resolution output reconstruction. Our network\nestablishes new state of the art on the largest publicly available MRI dataset,\nthe fastMRI dataset. We further demonstrate the performance of HUMUS-Net on two\nother popular MRI datasets and perform fine-grained ablation studies to\nvalidate our design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fabian_Z/0/1/0/all/0/1\">Zalan Fabian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-Gait: Automatic Ataxia Risk Assessment with Computer Vision on Gait Task Videos. (arXiv:2203.08215v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08215","description":"<p>In this paper, we investigated whether we can 1) detect participants with\nataxia-specific gait characteristics (risk-prediction), and 2) assess severity\nof ataxia from gait (severity-assessment). We collected 155 videos from 89\nparticipants, 24 controls and 65 diagnosed with (or are pre-manifest)\nspinocerebellar ataxias (SCAs), performing the gait task of the Scale for the\nAssessment and Rating of Ataxia (SARA) from 11 medical sites located in 8\ndifferent states in the United States. We developed a method to separate the\nparticipants from their surroundings and constructed several features to\ncapture gait characteristics like step width, step length, swing, stability,\nspeed, etc. Our risk-prediction model achieves 83.06% accuracy and an 80.23% F1\nscore. Similarly, our severity-assessment model achieves a mean absolute error\n(MAE) score of 0.6225 and a Pearson's correlation coefficient score of 0.7268.\nOur models still performed competitively when evaluated on data from sites not\nused during training. Furthermore, through feature importance analysis, we\nfound that our models associate wider steps, decreased walking speed, and\nincreased instability with greater ataxia severity, which is consistent with\npreviously established clinical knowledge. Our models create possibilities for\nremote ataxia assessment in non-clinical settings in the future, which could\nsignificantly improve accessibility of ataxia care. Furthermore, our underlying\ndataset was assembled from a geographically diverse cohort, highlighting its\npotential to further increase equity. The code used in this study is open to\nthe public, and the anonymized body pose landmark dataset could be released\nupon approval from our Institutional Review Board (IRB).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_W/0/1/0/all/0/1\">Wasifur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Masum Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olubajo_T/0/1/0/all/0/1\">Titilayo Olubajo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thaker_J/0/1/0/all/0/1\">Jeet Thaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelkader_A/0/1/0/all/0/1\">Abdelrahman Abdelkader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Phillip Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashizawa_T/0/1/0/all/0/1\">Tetsuo Ashizawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Ehsan Hoque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Portrait Harmonization. (arXiv:2203.08216v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08216","description":"<p>Current image harmonization methods consider the entire background as the\nguidance for harmonization. However, this may limit the capability for user to\nchoose any specific object/person in the background to guide the harmonization.\nTo enable flexible interaction between user and harmonization, we introduce\ninteractive harmonization, a new setting where the harmonization is performed\nwith respect to a selected \\emph{region} in the reference image instead of the\nentire background. A new flexible framework that allows users to pick certain\nregions of the background image and use it to guide the harmonization is\nproposed. Inspired by professional portrait harmonization users, we also\nintroduce a new luminance matching loss to optimally match the color/luminance\nconditions between the composite foreground and select reference region. This\nframework provides more control to the image harmonization pipeline achieving\nvisually pleasing portrait edits. Furthermore, we also introduce a new dataset\ncarefully curated for validating portrait harmonization. Extensive experiments\non both synthetic and real-world datasets show that the proposed approach is\nefficient and robust compared to previous harmonization baselines, especially\nfor portraits. Project Webpage at\n\\href{https://jeya-maria-jose.github.io/IPH-web/}{https://jeya-maria-jose.github.io/IPH-web/}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echevarria_J/0/1/0/all/0/1\">Jose Echevarria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yinglan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zijun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrowdMLP: Weakly-Supervised Crowd Counting via Multi-Granularity MLP. (arXiv:2203.08219v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08219","description":"<p>Existing state-of-the-art crowd counting algorithms rely excessively on\nlocation-level annotations, which are burdensome to acquire. When only\ncount-level (weak) supervisory signals are available, it is arduous and\nerror-prone to regress total counts due to the lack of explicit spatial\nconstraints. To address this issue, a novel and efficient counter (referred to\nas CrowdMLP) is presented, which probes into modelling global dependencies of\nembeddings and regressing total counts by devising a multi-granularity MLP\nregressor. In specific, a locally-focused pre-trained frontend is cascaded to\nextract crude feature maps with intrinsic spatial cues, which prevent the model\nfrom collapsing into trivial outcomes. The crude embeddings, along with raw\ncrowd scenes, are tokenized at different granularity levels. The\nmulti-granularity MLP then proceeds to mix tokens at the dimensions of\ncardinality, channel, and spatial for mining global information. An effective\nproxy task, namely Split-Counting, is also proposed to evade the barrier of\nlimited samples and the shortage of spatial hints in a self-supervised manner.\nExtensive experiments demonstrate that CrowdMLP significantly outperforms\nexisting weakly-supervised counting algorithms and performs on par with\nstate-of-the-art location-level supervised approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Minglun Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Dive into Dataset Imbalance and Bias in Face Identification. (arXiv:2203.08235v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08235","description":"<p>As the deployment of automated face recognition (FR) systems proliferates,\nbias in these systems is not just an academic question, but a matter of public\nconcern. Media portrayals often center imbalance as the main source of bias,\ni.e., that FR models perform worse on images of non-white people or women\nbecause these demographic groups are underrepresented in training data. Recent\nacademic research paints a more nuanced picture of this relationship. However,\nprevious studies of data imbalance in FR have focused exclusively on the face\nverification setting, while the face identification setting has been largely\nignored, despite being deployed in sensitive applications such as law\nenforcement. This is an unfortunate omission, as 'imbalance' is a more complex\nmatter in identification; imbalance may arise in not only the training data,\nbut also the testing data, and furthermore may affect the proportion of\nidentities belonging to each demographic group or the number of images\nbelonging to each identity. In this work, we address this gap in the research\nby thoroughly exploring the effects of each kind of imbalance possible in face\nidentification, and discuss other factors which may impact bias in this\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cherepanova_V/0/1/0/all/0/1\">Valeriia Cherepanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_S/0/1/0/all/0/1\">Steven Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dooley_S/0/1/0/all/0/1\">Samuel Dooley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souri_H/0/1/0/all/0/1\">Hossein Souri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Visual Transformer Compression. (arXiv:2203.08243v1 [cs.LG])","link":"http://arxiv.org/abs/2203.08243","description":"<p>Vision transformers (ViTs) have gained popularity recently. Even without\ncustomized image operators such as convolutions, ViTs can yield competitive\nperformance when properly trained on massive data. However, the computational\noverhead of ViTs remains prohibitive, due to stacking multi-head self-attention\nmodules and else. Compared to the vast literature and prevailing success in\ncompressing convolutional neural networks, the study of Vision Transformer\ncompression has also just emerged, and existing works focused on one or two\naspects of compression. This paper proposes a unified ViT compression framework\nthat seamlessly assembles three effective techniques: pruning, layer skipping,\nand knowledge distillation. We formulate a budget-constrained, end-to-end\noptimization framework, targeting jointly learning model weights, layer-wise\npruning ratios/masks, and skip configurations, under a distillation loss. The\noptimization problem is then solved using the primal-dual algorithm.\nExperiments are conducted with several ViT variants, e.g. DeiT and T2T-ViT\nbackbones on the ImageNet dataset, and our approach consistently outperforms\nrecent competitors. For example, DeiT-Tiny can be trimmed down to 50\\% of the\noriginal FLOPs almost without losing accuracy. Codes are available\nonline:~\\url{https://github.com/VITA-Group/UVC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shixing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiayi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Huan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jianchao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2-speed network ensemble for efficient classification of incremental land-use/land-cover satellite image chips. (arXiv:2203.08267v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08267","description":"<p>The ever-growing volume of satellite imagery data presents a challenge for\nindustry and governments making data-driven decisions based on the timely\nanalysis of very large data sets. Commonly used deep learning algorithms for\nautomatic classification of satellite images are time and resource-intensive to\ntrain. The cost of retraining in the context of Big Data presents a practical\nchallenge when new image data and/or classes are added to a training corpus.\nRecognizing the need for an adaptable, accurate, and scalable satellite image\nchip classification scheme, in this research we present an ensemble of: i) a\nslow to train but high accuracy vision transformer; and ii) a fast to train,\nlow-parameter convolutional neural network. The vision transformer model\nprovides a scalable and accurate foundation model. The high-speed CNN provides\nan efficient means of incorporating newly labelled data into analysis, at the\nexpense of lower accuracy. To simulate incremental data, the very large\n(~400,000 images) So2Sat LCZ42 satellite image chip dataset is divided into\nfour intervals, with the high-speed CNN retrained every interval and the vision\ntransformer trained every half interval. This experimental setup mimics an\nincrease in data volume and diversity over time. For the task of automated\nland-cover/land-use classification, the ensemble models for each data increment\noutperform each of the component models, with best accuracy of 65% against a\nholdout test partition of the So2Sat dataset. The proposed ensemble and\nstaggered training schedule provide a scalable and cost-effective satellite\nimage classification scheme that is optimized to process very large volumes of\nsatellite data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horry_M/0/1/0/all/0/1\">Michael James Horry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Subrata Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_B/0/1/0/all/0/1\">Biswajeet Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_N/0/1/0/all/0/1\">Nagesh Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sanjoy Paul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Driving Anomaly Detection Using Conditional Generative Adversarial Network. (arXiv:2203.08289v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08289","description":"<p>Anomaly driving detection is an important problem in advanced driver\nassistance systems (ADAS). It is important to identify potential hazard\nscenarios as early as possible to avoid potential accidents. This study\nproposes an unsupervised method to quantify driving anomalies using a\nconditional generative adversarial network (GAN). The approach predicts\nupcoming driving scenarios by conditioning the models on the previously\nobserved signals. The system uses the difference of the output from the\ndiscriminator between the predicted and actual signals as a metric to quantify\nthe anomaly degree of a driving segment. We take a driver-centric approach,\nconsidering physiological signals from the driver and controller area\nnetwork-Bus (CAN-Bus) signals from the vehicle. The approach is implemented\nwith convolutional neural networks (CNNs) to extract discriminative feature\nrepresentations, and with long short-term memory (LSTM) cells to capture\ntemporal information. The study is implemented and evaluated with the driving\nanomaly dataset (DAD), which includes 250 hours of naturalistic recordings\nmanually annotated with driving events. The experimental results reveal that\nrecordings annotated with events that are likely to be anomalous, such as\navoiding on-road pedestrians and traffic rule violations, have higher anomaly\nscores than recordings without any event annotation. The results are validated\nwith perceptual evaluations, where annotators are asked to assess the risk and\nfamiliarity of the videos detected with high anomaly scores. The results\nindicate that the driving segments with higher anomaly scores are more risky\nand less regularly seen on the road than other driving segments, validating the\nproposed unsupervised approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yuning Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misu_T/0/1/0/all/0/1\">Teruhisa Misu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busso_C/0/1/0/all/0/1\">Carlos Busso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An explainability framework for cortical surface-based deep learning. (arXiv:2203.08312v1 [q-bio.NC])","link":"http://arxiv.org/abs/2203.08312","description":"<p>The emergence of explainability methods has enabled a better comprehension of\nhow deep neural networks operate through concepts that are easily understood\nand implemented by the end user. While most explainability methods have been\ndesigned for traditional deep learning, some have been further developed for\ngeometric deep learning, in which data are predominantly represented as graphs.\nThese representations are regularly derived from medical imaging data,\nparticularly in the field of neuroimaging, in which graphs are used to\nrepresent brain structural and functional wiring patterns (brain connectomes)\nand cortical surface models are used to represent the anatomical structure of\nthe brain. Although explainability techniques have been developed for\nidentifying important vertices (brain areas) and features for graph\nclassification, these methods are still lacking for more complex tasks, such as\nsurface-based modality transfer (or vertex-wise regression). Here, we address\nthe need for surface-based explainability approaches by developing a framework\nfor cortical surface-based deep learning, providing a transparent system for\nmodality transfer tasks. First, we adapted a perturbation-based approach for\nuse with surface data. Then, we applied our perturbation-based method to\ninvestigate the key features and vertices used by a geometric deep learning\nmodel developed to predict brain function from anatomy directly on a cortical\nsurface model. We show that our explainability framework is not only able to\nidentify important features and their spatial location but that it is also\nreliable and valid.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Ribeiro_F/0/1/0/all/0/1\">Fernanda L. Ribeiro</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bollmann_S/0/1/0/all/0/1\">Steffen Bollmann</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cunnington_R/0/1/0/all/0/1\">Ross Cunnington</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Puckett_A/0/1/0/all/0/1\">Alexander M. Puckett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motif Mining: Finding and Summarizing Remixed Image Content. (arXiv:2203.08327v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08327","description":"<p>On the internet, images are no longer static; they have become dynamic\ncontent. Thanks to the availability of smartphones with cameras and easy-to-use\nediting software, images can be remixed (i.e., redacted, edited, and recombined\nwith other content) on-the-fly and with a world-wide audience that can repeat\nthe process. From digital art to memes, the evolution of images through time is\nnow an important topic of study for digital humanists, social scientists, and\nmedia forensics specialists. However, because typical data sets in computer\nvision are composed of static content, the development of automated algorithms\nto analyze remixed content has been limited. In this paper, we introduce the\nidea of Motif Mining - the process of finding and summarizing remixed image\ncontent in large collections of unlabeled and unsorted data. In this paper,\nthis idea is formalized and a reference implementation is introduced.\nExperiments are conducted on three meme-style data sets, including a newly\ncollected set associated with the information war in the Russo-Ukrainian\nconflict. The proposed motif mining approach is able to identify related\nremixed content that, when compared to similar approaches, more closely aligns\nwith the preferences and expectations of human observers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theisen_W/0/1/0/all/0/1\">William Theisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_D/0/1/0/all/0/1\">Daniel Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmichael_Z/0/1/0/all/0/1\">Zachariah Carmichael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_D/0/1/0/all/0/1\">Daniel Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weninger_T/0/1/0/all/0/1\">Tim Weninger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1\">Walter Scheirer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection. (arXiv:2203.08332v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08332","description":"<p>Monocular 3D object detection is one of the most challenging tasks in 3D\nscene understanding. Due to the ill-posed nature of monocular imagery, existing\nmonocular 3D detection methods highly rely on training with the manually\nannotated 3D box labels on the LiDAR point clouds. This annotation process is\nvery laborious and expensive. To dispense with the reliance on 3D box labels,\nin this paper we explore the weakly supervised monocular 3D detection.\nSpecifically, we first detect 2D boxes on the image. Then, we adopt the\ngenerated 2D boxes to select corresponding RoI LiDAR points as the weak\nsupervision. Eventually, we adopt a network to predict 3D boxes which can\ntightly align with associated RoI LiDAR points. This network is learned by\nminimizing our newly-proposed 3D alignment loss between the 3D box estimates\nand the corresponding RoI LiDAR points. We will illustrate the potential\nchallenges of the above learning problem and resolve these challenges by\nintroducing several effective designs into our method. Codes will be available\nat https://github.com/SPengLiang/WeakM3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Senbo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Boxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. (arXiv:2203.08344v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08344","description":"<p>We aim to improve the performance of regressing hand keypoints and segmenting\npixel-level hand masks under new imaging conditions (e.g., outdoors) when we\nonly have labeled images taken under very different conditions (e.g., indoors).\nIn the real world, it is important that the model trained for both tasks works\nunder various imaging conditions. However, their variation covered by existing\nlabeled hand datasets is limited. Thus, it is necessary to adapt the model\ntrained on the labeled images (source) to unlabeled images (target) with unseen\nimaging conditions. While self-training domain adaptation methods (i.e.,\nlearning from the unlabeled target images in a self-supervised manner) have\nbeen developed for both tasks, their training may degrade performance when the\npredictions on the target images are noisy. To avoid this, it is crucial to\nassign a low importance (confidence) weight to the noisy predictions during\nself-training. In this paper, we propose to utilize the divergence of two\npredictions to estimate the confidence of the target image for both tasks.\nThese predictions are given from two separate networks, and their divergence\nhelps identify the noisy predictions. To integrate our proposed confidence\nestimation into self-training, we propose a teacher-student framework where the\ntwo networks (teachers) provide supervision to a network (student) for\nself-training, and the teachers are learned from the student by knowledge\ndistillation. Our experiments show its superiority over state-of-the-art\nmethods in adaptation settings with different lighting, grasping objects,\nbackgrounds, and camera viewpoints. Our method improves by 4% the multi-task\nscore on HO3D compared to the latest adversarial adaptation method. We also\nvalidate our method on Ego4D, egocentric videos with rapid changes in imaging\nconditions outdoors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1\">Takehiko Ohkawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu-Jhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qichen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1\">Rosuke Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient Correction beyond Gradient Descent. (arXiv:2203.08345v1 [cs.LG])","link":"http://arxiv.org/abs/2203.08345","description":"<p>The great success neural networks have achieved is inseparable from the\napplication of gradient-descent (GD) algorithms. Based on GD, many variant\nalgorithms have emerged to improve the GD optimization process. The gradient\nfor back-propagation is apparently the most crucial aspect for the training of\na neural network. The quality of the calculated gradient can be affected by\nmultiple aspects, e.g., noisy data, calculation error, algorithm limitation,\nand so on. To reveal gradient information beyond gradient descent, we introduce\na framework (\\textbf{GCGD}) to perform gradient correction. GCGD consists of\ntwo plug-in modules: 1) inspired by the idea of gradient prediction, we propose\na \\textbf{GC-W} module for weight gradient correction; 2) based on Neural ODE,\nwe propose a \\textbf{GC-ODE} module for hidden states gradient correction.\nExperiment results show that our gradient correction framework can effectively\nimprove the gradient quality to reduce training epochs by $\\sim$ 20\\% and also\nimprove the network performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zefan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Teng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">WenJun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Represent, Compare, and Learn: A Similarity-Aware Framework for Class-Agnostic Counting. (arXiv:2203.08354v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08354","description":"<p>Class-agnostic counting (CAC) aims to count all instances in a query image\ngiven few exemplars. A standard pipeline is to extract visual features from\nexemplars and match them with query images to infer object counts. Two\nessential components in this pipeline are feature representation and similarity\nmetric. Existing methods either adopt a pretrained network to represent\nfeatures or learn a new one, while applying a naive similarity metric with\nfixed inner product. We find this paradigm leads to noisy similarity matching\nand hence harms counting performance. In this work, we propose a\nsimilarity-aware CAC framework that jointly learns representation and\nsimilarity metric. We first instantiate our framework with a naive baseline\ncalled Bilinear Matching Network (BMNet), whose key component is a learnable\nbilinear similarity metric. To further embody the core of our framework, we\nextend BMNet to BMNet+ that models similarity from three aspects: 1)\nrepresenting the instances via their self-similarity to enhance feature\nrobustness against intra-class variations; 2) comparing the similarity\ndynamically to focus on the key patterns of each exemplar; 3) learning from a\nsupervision signal to impose explicit constraints on matching results.\nExtensive experiments on a recent CAC dataset FSC147 show that our models\nsignificantly outperform state-of-the-art CAC approaches. In addition, we also\nvalidate the cross-dataset generality of BMNet and BMNet+ on a car counting\ndataset CARPK. Code is at tiny.one/BMNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Min Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiguo Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spot the Difference: A Cooperative Object-Referring Game in Non-Perfectly Co-Observable Scene. (arXiv:2203.08362v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08362","description":"<p>Visual dialog has witnessed great progress after introducing various\nvision-oriented goals into the conversation, especially such as GuessWhich and\nGuessWhat, where the only image is visible by either and both of the questioner\nand the answerer, respectively. Researchers explore more on visual dialog tasks\nin such kind of single- or perfectly co-observable visual scene, while somewhat\nneglect the exploration on tasks of non perfectly co-observable visual scene,\nwhere the images accessed by two agents may not be exactly the same, often\noccurred in practice. Although building common ground in non-perfectly\nco-observable visual scene through conversation is significant for advanced\ndialog agents, the lack of such dialog task and corresponding large-scale\ndataset makes it impossible to carry out in-depth research. To break this\nlimitation, we propose an object-referring game in non-perfectly co-observable\nvisual scene, where the goal is to spot the difference between the similar\nvisual scenes through conversing in natural language. The task addresses\nchallenges of the dialog strategy in non-perfectly co-observable visual scene\nand the ability of categorizing objects. Correspondingly, we construct a\nlarge-scale multimodal dataset, named SpotDiff, which contains 87k Virtual\nReality images and 97k dialogs generated by self-play. Finally, we give\nbenchmark models for this task, and conduct extensive experiments to evaluate\nits performance as well as analyze its main challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Duo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_Q/0/1/0/all/0/1\">Qingyi Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hairun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fangxiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-Precision Neural Network Quantization via Learned Layer-wise Importance. (arXiv:2203.08368v1 [cs.LG])","link":"http://arxiv.org/abs/2203.08368","description":"<p>The exponentially large discrete search space in mixed-precision quantization\n(MPQ) makes it hard to determine the optimal bit-width for each layer. Previous\nworks usually resort to iterative search methods on the training set, which\nconsume hundreds or even thousands of GPU-hours. In this study, we reveal that\nsome unique learnable parameters in quantization, namely the scale factors in\nthe quantizer, can serve as importance indicators of a layer, reflecting the\ncontribution of that layer to the final accuracy at certain bit-widths. These\nimportance indicators naturally perceive the numerical transformation during\nquantization-aware training, which can precisely and correctly provide\nquantization sensitivity metrics of layers. However, a deep network always\ncontains hundreds of such indicators, and training them one by one would lead\nto an excessive time cost. To overcome this issue, we propose a joint training\nscheme that can obtain all indicators at once. It considerably speeds up the\nindicators training process by parallelizing the original sequential training\nprocesses. With these learned importance indicators, we formulate the MPQ\nsearch problem as a one-time integer linear programming (ILP) problem. That\navoids the iterative search and significantly reduces search time without\nlimiting the bit-width search space. For example, MPQ search on ResNet18 with\nour indicators takes only 0.06 seconds. Also, extensive experiments show our\napproach can achieve SOTA accuracy on ImageNet for far-ranging models with\nvarious constraints (e.g., BitOps, compress rate).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_K/0/1/0/all/0/1\">Kai Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yifei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wen Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Diffusion Implicit Bridges for Image-to-Image Translation. (arXiv:2203.08382v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08382","description":"<p>Common image-to-image translation methods rely on joint training over data\nfrom both source and target domains. This excludes cases where domain data is\nprivate (e.g., in a federated setting), and often means that a new model has to\nbe trained for a new pair of domains. We present Dual Diffusion Implicit\nBridges (DDIBs), an image translation method based on diffusion models, that\ncircumvents training on domain pairs. DDIBs allow translations between\narbitrary pairs of source-target domains, given independently trained diffusion\nmodels on the respective domains. Image translation with DDIBs is a two-step\nprocess: DDIBs first obtain latent encodings for source images with the source\ndiffusion model, and next decode such encodings using the target model to\nconstruct target images. Moreover, DDIBs enable cycle-consistency by default\nand is theoretically connected to optimal transport. Experimentally, we apply\nDDIBs on a variety of synthetic and high-resolution image datasets,\ndemonstrating their utility in example-guided color transfer, image-to-image\ntranslation as well as their connections to optimal transport methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chenlin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?. (arXiv:2203.08392v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08392","description":"<p>Vision transformers (ViTs) have recently set off a new wave in neural\narchitecture design thanks to their record-breaking performance in various\nvision tasks. In parallel, to fulfill the goal of deploying ViTs into\nreal-world vision applications, their robustness against potential malicious\nattacks has gained increasing attention. In particular, recent works show that\nViTs are more robust against adversarial attacks as compared with convolutional\nneural networks (CNNs), and conjecture that this is because ViTs focus more on\ncapturing global interactions among different input/feature patches, leading to\ntheir improved robustness to local perturbations imposed by adversarial\nattacks. In this work, we ask an intriguing question: \"Under what kinds of\nperturbations do ViTs become more vulnerable learners compared to CNNs?\" Driven\nby this question, we first conduct a comprehensive experiment regarding the\nrobustness of both ViTs and CNNs under various existing adversarial attacks to\nunderstand the underlying reason favoring their robustness. Based on the drawn\ninsights, we then propose a dedicated attack framework, dubbed Patch-Fool, that\nfools the self-attention mechanism by attacking its basic component (i.e., a\nsingle patch) with a series of attention-aware optimization techniques.\nInterestingly, our Patch-Fool framework shows for the first time that ViTs are\nnot necessarily more robust than CNNs against adversarial perturbations. In\nparticular, we find that ViTs are more vulnerable learners compared with CNNs\nagainst our Patch-Fool attack which is consistent across extensive experiments,\nand the observations from Sparse/Mild Patch-Fool, two variants of Patch-Fool,\nindicate an intriguing insight that the perturbation density and strength on\neach patch seem to be the key factors that influence the robustness ranking\nbetween ViTs and CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shunyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1\">Cheng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-preserving Online AutoML for Domain-Specific Face Detection. (arXiv:2203.08399v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08399","description":"<p>Despite the impressive progress of general face detection, the tuning of\nhyper-parameters and architectures is still critical for the performance of a\ndomain-specific face detector. Though existing AutoML works can speedup such\nprocess, they either require tuning from scratch for a new scenario or do not\nconsider data privacy. To scale up, we derive a new AutoML setting from a\nplatform perspective. In such setting, new datasets sequentially arrive at the\nplatform, where an architecture and hyper-parameter configuration is\nrecommended to train the optimal face detector for each dataset. This, however,\nbrings two major challenges: (1) how to predict the best configuration for any\ngiven dataset without touching their raw images due to the privacy concern? and\n(2) how to continuously improve the AutoML algorithm from previous tasks and\noffer a better warm-up for future ones? We introduce \"HyperFD\", a new\nprivacy-preserving online AutoML framework for face detection. At its core\npart, a novel meta-feature representation of a dataset as well as its learning\nparadigm is proposed. Thanks to HyperFD, each local task (client) is able to\neffectively leverage the learning \"experience\" of previous tasks without\nuploading raw images to the platform; meanwhile, the meta-feature extractor is\ncontinuously learned to better trade off the bias and variance. Extensive\nexperiments demonstrate the effectiveness and efficiency of our design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chenqian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoyuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RBC: Rectifying the Biased Context in Continual Semantic Segmentation. (arXiv:2203.08404v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08404","description":"<p>Recent years have witnessed a great development of Convolutional Neural\nNetworks in semantic segmentation, where all classes of training images are\nsimultaneously available. In practice, new images are usually made available in\na consecutive manner, leading to a problem called Continual Semantic\nSegmentation (CSS). Typically, CSS faces the forgetting problem since previous\ntraining images are unavailable, and the semantic shift problem of the\nbackground class. Considering the semantic segmentation as a context-dependent\npixel-level classification task, we explore CSS from a new perspective of\ncontext analysis in this paper. We observe that the context of old-class pixels\nin the new images is much more biased on new classes than that in the old\nimages, which can sharply aggravate the old-class forgetting and new-class\noverfitting. To tackle the obstacle, we propose a biased-context-rectified CSS\nframework with a context-rectified image-duplet learning scheme and a\nbiased-context-insensitive consistency loss. Furthermore, we propose an\nadaptive re-weighting class-balanced learning strategy for the biased class\ndistribution. Our approach outperforms state-of-the-art methods by a large\nmargin in existing CSS scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanbin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fengyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xinghe Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scale Context-Guided Lumbar Spine Disease Identification with Coarse-to-fine Localization and Classification. (arXiv:2203.08408v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08408","description":"<p>Accurate and efficient lumbar spine disease identification is crucial for\nclinical diagnosis. However, existing deep learning models with millions of\nparameters often fail to learn with only hundreds or dozens of medical images.\nThese models also ignore the contextual relationship between adjacent objects,\nsuch as between vertebras and intervertebral discs. This work introduces a\nmulti-scale context-guided network with coarse-to-fine localization and\nclassification, named CCF-Net, for lumbar spine disease identification.\nSpecifically, in learning, we divide the localization objective into two\nparallel tasks, coarse and fine, which are more straightforward and effectively\nreduce the number of parameters and computational cost. The experimental\nresults show that the coarse-to-fine design presents the potential to achieve\nhigh performance with fewer parameters and data requirements. Moreover, the\nmulti-scale context-guided module can significantly improve the performance by\n6.45% and 5.51% with ResNet18 and ResNet50, respectively. Our code is available\nat https://github.com/czifan/CCFNet.pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction. (arXiv:2203.08411v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08411","description":"<p>Sequence modeling has demonstrated state-of-the-art performance on natural\nlanguage and document understanding tasks. However, it is challenging to\ncorrectly serialize tokens in form-like documents in practice due to their\nvariety of layout patterns. We propose FormNet, a structure-aware sequence\nmodel to mitigate the suboptimal serialization of forms. First, we design Rich\nAttention that leverages the spatial relationship between tokens in a form for\nmore precise attention score calculation. Second, we construct Super-Tokens for\neach word by embedding representations from their neighboring tokens through\ngraph convolutions. FormNet therefore explicitly recovers local syntactic\ninformation that may have been lost during serialization. In experiments,\nFormNet outperforms existing methods with a more compact model size and less\npre-training data, establishing new state-of-the-art performance on CORD, FUNSD\nand Payment benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozat_T/0/1/0/all/0/1\">Timothy Dozat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perot_V/0/1/0/all/0/1\">Vincent Perot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guolong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_N/0/1/0/all/0/1\">Nan Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Semantic Segmentation by Distilling Feature Correspondences. (arXiv:2203.08414v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08414","description":"<p>Unsupervised semantic segmentation aims to discover and localize semantically\nmeaningful categories within image corpora without any form of annotation. To\nsolve this task, algorithms must produce features for every pixel that are both\nsemantically meaningful and compact enough to form distinct clusters. Unlike\nprevious works which achieve this with a single end-to-end framework, we\npropose to separate feature learning from cluster compactification.\nEmpirically, we show that current unsupervised feature learning frameworks\nalready generate dense features whose correlations are semantically consistent.\nThis observation motivates us to design STEGO ($\\textbf{S}$elf-supervised\n$\\textbf{T}$ransformer with $\\textbf{E}$nergy-based $\\textbf{G}$raph\n$\\textbf{O}$ptimization), a novel framework that distills unsupervised features\ninto high-quality discrete semantic labels. At the core of STEGO is a novel\ncontrastive loss function that encourages features to form compact clusters\nwhile preserving their relationships across the corpora. STEGO yields a\nsignificant improvement over the prior state of the art, on both the CocoStuff\n($\\textbf{+14 mIoU}$) and Cityscapes ($\\textbf{+9 mIoU}$) semantic segmentation\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_M/0/1/0/all/0/1\">Mark Hamilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhoutong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WegFormer: Transformers for Weakly Supervised Semantic Segmentation. (arXiv:2203.08421v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08421","description":"<p>Although convolutional neural networks (CNNs) have achieved remarkable\nprogress in weakly supervised semantic segmentation (WSSS), the effective\nreceptive field of CNN is insufficient to capture global context information,\nleading to sub-optimal results. Inspired by the great success of Transformers\nin fundamental vision areas, this work for the first time introduces\nTransformer to build a simple and effective WSSS framework, termed WegFormer.\nUnlike existing CNN-based methods, WegFormer uses Vision Transformer (ViT) as a\nclassifier to produce high-quality pseudo segmentation masks. To this end, we\nintroduce three tailored components in our Transformer-based framework, which\nare (1) a Deep Taylor Decomposition (DTD) to generate attention maps, (2) a\nsoft erasing module to smooth the attention maps, and (3) an efficient\npotential object mining (EPOM) to filter noisy activation in the background.\nWithout any bells and whistles, WegFormer achieves state-of-the-art 70.5% mIoU\non the PASCAL VOC dataset, significantly outperforming the previous best\nmethod. We hope WegFormer provides a new perspective to tap the potential of\nTransformer in weakly supervised semantic segmentation. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunmeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribute Group Editing for Reliable Few-shot Image Generation. (arXiv:2203.08422v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08422","description":"<p>Few-shot image generation is a challenging task even using the\nstate-of-the-art Generative Adversarial Networks (GANs). Due to the unstable\nGAN training process and the limited training data, the generated images are\noften of low quality and low diversity. In this work, we propose a new\nediting-based method, i.e., Attribute Group Editing (AGE), for few-shot image\ngeneration. The basic assumption is that any image is a collection of\nattributes and the editing direction for a specific attribute is shared across\nall categories. AGE examines the internal representation learned in GANs and\nidentifies semantically meaningful directions. Specifically, the class\nembedding, i.e., the mean vector of the latent codes from a specific category,\nis used to represent the category-relevant attributes, and the\ncategory-irrelevant attributes are learned globally by Sparse Dictionary\nLearning on the difference between the sample embedding and the class\nembedding. Given a GAN well trained on seen categories, diverse images of\nunseen categories can be synthesized through editing category-irrelevant\nattributes while keeping category-relevant attributes unchanged. Without\nre-training the GAN, AGE is capable of not only producing more realistic and\ndiverse images for downstream visual applications with limited data but\nachieving controllable image editing with interpretable category-irrelevant\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guanqi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuzhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_D/0/1/0/all/0/1\">Dandan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiFT: Differentiable Differential Feature Transform for Multi-View Stereo. (arXiv:2203.08435v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08435","description":"<p>We present a novel framework to automatically learn to transform the\ndifferential cues from a stack of images densely captured with a rotational\nmotion into spatially discriminative and view-invariant per-pixel features at\neach view. These low-level features can be directly fed to any existing\nmulti-view stereo technique for enhanced 3D reconstruction. The lighting\ncondition during acquisition can also be jointly optimized in a differentiable\nfashion. We sample from a dozen of pre-scanned objects with a wide variety of\ngeometry and reflectance to synthesize a large amount of high-quality training\ndata. The effectiveness of our features is demonstrated on a number of\nchallenging objects acquired with a lightstage, comparing favorably with\nstate-of-the-art techniques. Finally, we explore additional applications of\ngeometric detail visualization and computational stylization of complex\nappearance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_K/0/1/0/all/0/1\">Kaizhang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Set Recognition using Vision Transformer with an Additional Detection Head. (arXiv:2203.08441v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08441","description":"<p>Deep neural networks have demonstrated prominent capacities for image\nclassification tasks in a closed set setting, where the test data come from the\nsame distribution as the training data. However, in a more realistic open set\nscenario, traditional classifiers with incomplete knowledge cannot tackle test\ndata that are not from the training classes. Open set recognition (OSR) aims to\naddress this problem by both identifying unknown classes and distinguishing\nknown classes simultaneously. In this paper, we propose a novel approach to OSR\nthat is based on the vision transformer (ViT) technique. Specifically, our\napproach employs two separate training stages. First, a ViT model is trained to\nperform closed set classification. Then, an additional detection head is\nattached to the embedded features extracted by the ViT, trained to force the\nrepresentations of known data to class-specific clusters compactly. Test\nexamples are identified as known or unknown based on their distance to the\ncluster centers. To the best of our knowledge, this is the first time to\nleverage ViT for the purpose of OSR, and our extensive evaluation against\nseveral OSR benchmark datasets reveals that our approach significantly\noutperforms other baseline methods and obtains new state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_F/0/1/0/all/0/1\">Feiyang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutsoukos_X/0/1/0/all/0/1\">Xenofon Koutsoukos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panini-Net: GAN Prior Based Degradation-Aware Feature Interpolation for Face Restoration. (arXiv:2203.08444v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08444","description":"<p>Emerging high-quality face restoration (FR) methods often utilize pre-trained\nGAN models (\\textit{i.e.}, StyleGAN2) as GAN Prior. However, these methods\nusually struggle to balance realness and fidelity when facing various\ndegradation levels. Besides, there is still a noticeable visual quality gap\ncompared with pre-trained GAN models. In this paper, we propose a novel GAN\nPrior based degradation-aware feature interpolation network, dubbed Panini-Net,\nfor FR tasks by explicitly learning the abstract representations to distinguish\nvarious degradations. Specifically, an unsupervised degradation representation\nlearning (UDRL) strategy is first developed to extract degradation\nrepresentations (DR) of the input degraded images. Then, a degradation-aware\nfeature interpolation (DAFI) module is proposed to dynamically fuse the two\ntypes of informative features (\\textit{i.e.}, features from input images and\nfeatures from GAN Prior) with flexible adaption to various degradations based\non DR. Ablation studies reveal the working mechanism of DAFI and its potential\nfor editable FR. Extensive experiments demonstrate that our Panini-Net achieves\nstate-of-the-art performance for multi-degradation face restoration and face\nsuper-resolution. The source code is available at\nhttps://github.com/jianzhangcs/panini.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yinhuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yujie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Devil Is in the Details: Window-based Attention for Image Compression. (arXiv:2203.08450v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08450","description":"<p>Learned image compression methods have exhibited superior rate-distortion\nperformance than classical image compression standards. Most existing learned\nimage compression models are based on Convolutional Neural Networks (CNNs).\nDespite great contributions, a main drawback of CNN based model is that its\nstructure is not designed for capturing local redundancy, especially the\nnon-repetitive textures, which severely affects the reconstruction quality.\nTherefore, how to make full use of both global structure and local texture\nbecomes the core problem for learning-based image compression. Inspired by\nrecent progresses of Vision Transformer (ViT) and Swin Transformer, we found\nthat combining the local-aware attention mechanism with the global-related\nfeature learning could meet the expectation in image compression. In this\npaper, we first extensively study the effects of multiple kinds of attention\nmechanisms for local features learning, then introduce a more straightforward\nyet effective window-based local attention block. The proposed window-based\nattention is very flexible which could work as a plug-and-play component to\nenhance CNN and Transformer models. Moreover, we propose a novel Symmetrical\nTransFormer (STF) framework with absolute transformer blocks in the\ndown-sampling encoder and up-sampling decoder. Extensive experimental\nevaluations have shown that the proposed method is effective and outperforms\nthe state-of-the-art methods. The code is publicly available at\nhttps://github.com/Googolxx/STF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_R/0/1/0/all/0/1\">Renjie Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chunfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PPCD-GAN: Progressive Pruning and Class-Aware Distillation for Large-Scale Conditional GANs Compression. (arXiv:2203.08456v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08456","description":"<p>We push forward neural network compression research by exploiting a novel\nchallenging task of large-scale conditional generative adversarial networks\n(GANs) compression. To this end, we propose a gradually shrinking GAN\n(PPCD-GAN) by introducing progressive pruning residual block (PP-Res) and\nclass-aware distillation. The PP-Res is an extension of the conventional\nresidual block where each convolutional layer is followed by a learnable mask\nlayer to progressively prune network parameters as training proceeds. The\nclass-aware distillation, on the other hand, enhances the stability of training\nby transferring immense knowledge from a well-trained teacher model through\ninstructive attention maps. We train the pruning and distillation processes\nsimultaneously on a well-known GAN architecture in an end-to-end manner. After\ntraining, all redundant parameters as well as the mask layers are discarded,\nyielding a lighter network while retaining the performance. We comprehensively\nillustrate, on ImageNet 128x128 dataset, PPCD-GAN reduces up to 5.2x (81%)\nparameters against state-of-the-arts while keeping better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Duc Minh Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1\">Akihiro Sugimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakayama_H/0/1/0/all/0/1\">Hideki Nakayama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing Local Similarities for Retrieval-based 3D Orientation Estimation of Unseen Objects. (arXiv:2203.08472v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08472","description":"<p>In this paper, we tackle the task of estimating the 3D orientation of\npreviously-unseen objects from monocular images. This task contrasts with the\none considered by most existing deep learning methods which typically assume\nthat the testing objects have been observed during training. To handle the\nunseen objects, we follow a retrieval-based strategy and prevent the network\nfrom learning object-specific features by computing multi-scale local\nsimilarities between the query image and synthetically-generated reference\nimages. We then introduce an adaptive fusion module that robustly aggregates\nthe local similarities into a global similarity score of pairwise images.\nFurthermore, we speed up the retrieval process by developing a fast\nclustering-based retrieval strategy. Our experiments on the LineMOD,\nLineMOD-Occluded, and T-LESS datasets show that our method yields a\nsignificantly better generalization to unseen objects than previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yinlin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Efficient 3D Learner via Knowledge Transferred from 2D Model. (arXiv:2203.08479v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08479","description":"<p>Collecting and labeling the registered 3D point cloud is costly. As a result,\n3D resources for training are typically limited in quantity compared to the 2D\nimages counterpart. In this work, we deal with the data scarcity challenge of\n3D tasks by transferring knowledge from strong 2D models via RGB-D images.\nSpecifically, we utilize a strong and well-trained semantic segmentation model\nfor 2D images to augment RGB-D images with pseudo-label. The augmented dataset\ncan then be used to pre-train 3D models. Finally, by simply fine-tuning on a\nfew labeled 3D instances, our method already outperforms existing\nstate-of-the-art that is tailored for 3D label efficiency. We also show that\nthe results of mean-teacher and entropy minimization can be improved by our\npre-training, suggesting that the transferred knowledge is helpful in\nsemi-supervised setting. We verify the effectiveness of our approach on two\npopular 3D models and three different tasks. On ScanNet official evaluation, we\nestablish new state-of-the-art semantic segmentation results on the\ndata-efficient track.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Ping-Chung Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding. (arXiv:2203.08481v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08481","description":"<p>Visual grounding, i.e., localizing objects in images according to natural\nlanguage queries, is an important topic in visual language understanding. The\nmost effective approaches for this task are based on deep learning, which\ngenerally require expensive manually labeled image-query or patch-query pairs.\nTo eliminate the heavy dependence on human annotations, we present a novel\nmethod, named Pseudo-Q, to automatically generate pseudo language queries for\nsupervised training. Our method leverages an off-the-shelf object detector to\nidentify visual objects from unlabeled images, and then language queries for\nthese objects are obtained in an unsupervised fashion with a pseudo-query\ngeneration module. Then, we design a task-related query prompt module to\nspecifically tailor generated pseudo language queries for visual grounding\ntasks. Further, in order to fully capture the contextual relationships between\nimages and language queries, we develop a visual-language model equipped with\nmulti-level cross-modality attention mechanism. Extensive experimental results\ndemonstrate that our method has two notable benefits: (1) it can reduce human\nannotation costs significantly, e.g., 31% on RefCOCO without degrading original\nmodel's performance under the fully supervised setting, and (2) without bells\nand whistles, it achieves superior or comparable performance compared to\nstate-of-the-art weakly-supervised visual grounding methods on all the five\ndatasets we have experimented. Code is available at\nhttps://github.com/LeapLabTHU/Pseudo-Q.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haojun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongchen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QS-Attn: Query-Selected Attention for Contrastive Learning in I2I Translation. (arXiv:2203.08483v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08483","description":"<p>Unpaired image-to-image (I2I) translation often requires to maximize the\nmutual information between the source and the translated images across\ndifferent domains, which is critical for the generator to keep the source\ncontent and prevent it from unnecessary modifications. The self-supervised\ncontrastive learning has already been successfully applied in the I2I. By\nconstraining features from the same location to be closer than those from\ndifferent ones, it implicitly ensures the result to take content from the\nsource. However, previous work uses the features from random locations to\nimpose the constraint, which may not be appropriate since some locations\ncontain less information of source domain. Moreover, the feature itself does\nnot reflect the relation with others. This paper deals with these problems by\nintentionally selecting significant anchor points for contrastive learning. We\ndesign a query-selected attention (QS-Attn) module, which compares feature\ndistances in the source domain, giving an attention matrix with a probability\ndistribution in each row. Then we select queries according to their measurement\nof significance, computed from the distribution. The selected ones are regarded\nas anchors for contrastive loss. At the same time, the reduced attention matrix\nis employed to route features in both domains, so that source relations\nmaintain in the synthesis. We validate our proposed method in three different\nI2I datasets, showing that it increases the image quality without adding\nlearnable parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xueqi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinyue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiusheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhengyi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Li Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingli Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointAttN: You Only Need Attention for Point Cloud Completion. (arXiv:2203.08485v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08485","description":"<p>Point cloud completion referring to completing 3D shapes from partial 3D\npoint clouds is a fundamental problem for 3D point cloud analysis tasks.\nBenefiting from the development of deep neural networks, researches on point\ncloud completion have made great progress in recent years. However, the\nexplicit local region partition like kNNs involved in existing methods makes\nthem sensitive to the density distribution of point clouds. Moreover, it serves\nlimited receptive fields that prevent capturing features from long-range\ncontext information. To solve the problems, we leverage the cross-attention and\nself-attention mechanisms to design novel neural network for processing point\ncloud in a per-point manner to eliminate kNNs. Two essential blocks Geometric\nDetails Perception (GDP) and Self-Feature Augment (SFA) are proposed to\nestablish the short-range and long-range structural relationships directly\namong points in a simple yet effective way via attention mechanism. Then based\non GDP and SFA, we construct a new framework with popular encoder-decoder\narchitecture for point cloud completion. The proposed framework, namely\nPointAttN, is simple, neat and effective, which can precisely capture the\nstructural information of 3D shapes and predict complete point clouds with\nhighly detailed geometries. Experimental results demonstrate that our PointAttN\noutperforms state-of-the-art methods by a large margin on popular benchmarks\nlike Completion3D and PCN. Code is available at:\nhttps://github.com/ohhhyeahhh/PointAttN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Ying Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dongyan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingshan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Historical Document Image Datasets. (arXiv:2203.08504v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08504","description":"<p>This paper presents a systematic literature review of image datasets for\ndocument image analysis, focusing on historical documents, such as handwritten\nmanuscripts and early prints. Finding appropriate datasets for historical\ndocument analysis is a crucial prerequisite to facilitate research using\ndifferent machine learning algorithms. However, because of the very large\nvariety of the actual data (e.g., scripts, tasks, dates, support systems, and\namount of deterioration), the different formats for data and label\nrepresentation, and the different evaluation processes and benchmarks, finding\nappropriate datasets is a difficult task. This work fills this gap, presenting\na meta-study on existing datasets. After a systematic selection process\n(according to PRISMA guidelines), we select 56 studies that are chosen based on\ndifferent factors, such as the year of publication, number of methods\nimplemented in the article, reliability of the chosen algorithms, dataset size,\nand journal outlet. We summarize each study by assigning it to one of three\npre-defined tasks: document classification, layout structure, or semantic\nanalysis. We present the statistics, document type, language, tasks, input\nvisual aspects, and ground truth information for every dataset. In addition, we\nprovide the benchmark tasks and results from these papers or recent\ncompetitions. We further discuss gaps and challenges in this domain. We\nadvocate for providing conversion tools to common formats (e.g., COCO format\nfor computer vision tasks) and always providing a set of evaluation metrics,\ninstead of just one, to make results comparable across studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikolaidou_K/0/1/0/all/0/1\">Konstantina Nikolaidou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seuret_M/0/1/0/all/0/1\">Mathias Seuret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokayed_H/0/1/0/all/0/1\">Hamam Mokayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-focus thermal image fusion. (arXiv:2203.08513v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08513","description":"<p>This paper proposes a novel algorithm for multi-focus thermal image fusion.\nThe algorithm is based on local activity analysis and advanced pre-selection of\nimages into fusion process. The algorithm improves the object temperature\nmeasurement error up to 5 Celsius degrees. The proposed algorithm is evaluated\nby half total error rate, root mean squared error, cross correlation and visual\ninspection. To the best of our knowledge, this is the first work devoted to\nmulti-focus thermal image fusion. For testing of proposed algorithm we acquire\nsix thermal image set with objects at different focal depth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benes_R/0/1/0/all/0/1\">Radek Benes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dvorak_P/0/1/0/all/0/1\">Pavel Dvorak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Duro_V/0/1/0/all/0/1\">Virginia Espinosa-Duro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekyska_J/0/1/0/all/0/1\">Jiri Mekyska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fantastic Style Channels and Where to Find Them: A Submodular Framework for Discovering Diverse Directions in GANs. (arXiv:2203.08516v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08516","description":"<p>The discovery of interpretable directions in the latent spaces of pre-trained\nGAN models has recently become a popular topic. In particular, StyleGAN2 has\nenabled various image generation and manipulation tasks due to its rich and\ndisentangled latent spaces. The discovery of such directions is typically done\neither in a supervised manner, which requires annotated data for each desired\nmanipulation or in an unsupervised manner, which requires a manual effort to\nidentify the directions. As a result, existing work typically finds only a\nhandful of directions in which controllable edits can be made. In this study,\nwe design a novel submodular framework that finds the most representative and\ndiverse subset of directions in the latent space of StyleGAN2. Our approach\ntakes advantage of the latent space of channel-wise style parameters, so-called\nstylespace, in which we cluster channels that perform similar manipulations\ninto groups. Our framework promotes diversity by using the notion of clusters\nand can be efficiently solved with a greedy optimization scheme. We evaluate\nour framework with qualitative and quantitative experiments and show that our\nmethod finds more diverse and disentangled directions. Our project page can be\nfound at <a href=\"http://catlab-team.github.io/fantasticstyles.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simsar_E/0/1/0/all/0/1\">Enis Simsar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocasari_U/0/1/0/all/0/1\">Umut Kocasari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Er_E/0/1/0/all/0/1\">Ezgi G&#xfc;lperi Er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanardag_P/0/1/0/all/0/1\">Pinar Yanardag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Practical Certifiable Patch Defense with Vision Transformer. (arXiv:2203.08519v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08519","description":"<p>Patch attacks, one of the most threatening forms of physical attack in\nadversarial examples, can lead networks to induce misclassification by\nmodifying pixels arbitrarily in a continuous region. Certifiable patch defense\ncan guarantee robustness that the classifier is not affected by patch attacks.\nExisting certifiable patch defenses sacrifice the clean accuracy of classifiers\nand only obtain a low certified accuracy on toy datasets. Furthermore, the\nclean and certified accuracy of these methods is still significantly lower than\nthe accuracy of normal classification networks, which limits their application\nin practice. To move towards a practical certifiable patch defense, we\nintroduce Vision Transformer (ViT) into the framework of Derandomized Smoothing\n(DS). Specifically, we propose a progressive smoothed image modeling task to\ntrain Vision Transformer, which can capture the more discriminable local\ncontext of an image while preserving the global semantic information. For\nefficient inference and deployment in the real world, we innovatively\nreconstruct the global self-attention structure of the original ViT into\nisolated band unit self-attention. On ImageNet, under 2% area patch attacks our\nmethod achieves 41.70% certified accuracy, a nearly 1-fold increase over the\nprevious best method (26.00%). Simultaneously, our method achieves 78.58% clean\naccuracy, which is quite close to the normal ResNet-101 accuracy. Extensive\nexperiments show that our method obtains state-of-the-art clean and certified\naccuracy with inferring efficiently on CIFAR-10 and ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianghe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video. (arXiv:2203.08534v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08534","description":"<p>Learning to capture human motion is essential to 3D human pose and shape\nestimation from monocular video. However, the existing methods mainly rely on\nrecurrent or convolutional operation to model such temporal information, which\nlimits the ability to capture non-local context relations of human motion. To\naddress this problem, we propose a motion pose and shape network (MPS-Net) to\neffectively capture humans in motion to estimate accurate and temporally\ncoherent 3D human pose and shape from a video. Specifically, we first propose a\nmotion continuity attention (MoCA) module that leverages visual cues observed\nfrom human motion to adaptively recalibrate the range that needs attention in\nthe sequence to better capture the motion continuity dependencies. Then, we\ndevelop a hierarchical attentive feature integration (HAFI) module to\neffectively combine adjacent past and future feature representations to\nstrengthen temporal correlation and refine the feature representation of the\ncurrent frame. By coupling the MoCA and HAFI modules, the proposed MPS-Net\nexcels in estimating 3D human pose and shape in the video. Though conceptually\nsimple, our MPS-Net not only outperforms the state-of-the-art methods on the\n3DPW, MPI-INF-3DHP, and Human3.6M benchmark datasets, but also uses fewer\nnetwork parameters. The video demos can be found at\nhttps://mps-net.github.io/MPS-Net/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wen-Li Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jen-Chun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tyng-Luh Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hong-Yuan Mark Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scribble-Supervised LiDAR Semantic Segmentation. (arXiv:2203.08537v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08537","description":"<p>Densely annotating LiDAR point clouds remains too expensive and\ntime-consuming to keep up with the ever growing volume of data. While current\nliterature focuses on fully-supervised performance, developing efficient\nmethods that take advantage of realistic weak supervision have yet to be\nexplored. In this paper, we propose using scribbles to annotate LiDAR point\nclouds and release ScribbleKITTI, the first scribble-annotated dataset for\nLiDAR semantic segmentation. Furthermore, we present a pipeline to reduce the\nperformance gap that arises when using such weak annotations. Our pipeline\ncomprises of three stand-alone contributions that can be combined with any\nLiDAR semantic segmentation model to achieve up to 95.7% of the\nfully-supervised performance while using only 8% labeled points. Our scribble\nannotations and code are available at github.com/ouenal/scribblekitti.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unal_O/0/1/0/all/0/1\">Ozan Unal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Language Guidance into Vision-based Deep Metric Learning. (arXiv:2203.08543v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08543","description":"<p>Deep Metric Learning (DML) proposes to learn metric spaces which encode\nsemantic similarities as embedding space distances. These spaces should be\ntransferable to classes beyond those seen during training. Commonly, DML\nmethods task networks to solve contrastive ranking tasks defined over binary\nclass assignments. However, such approaches ignore higher-level semantic\nrelations between the actual classes. This causes learned embedding spaces to\nencode incomplete semantic context and misrepresent the semantic relation\nbetween classes, impacting the generalizability of the learned metric space. To\ntackle this issue, we propose a language guidance objective for visual\nsimilarity learning. Leveraging language embeddings of expert- and\npseudo-classnames, we contextualize and realign visual representation spaces\ncorresponding to meaningful language semantics for better semantic consistency.\nExtensive experiments and ablations provide a strong motivation for our\nproposed approach and show language guidance offering significant,\nmodel-agnostic improvements for DML, achieving competitive and state-of-the-art\nresults on all benchmarks. Code available at\nhttps://github.com/ExplainableML/LanguageGuidance_for_DML.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1\">Karsten Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-isotropy Regularization for Proxy-based Deep Metric Learning. (arXiv:2203.08547v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08547","description":"<p>Deep Metric Learning (DML) aims to learn representation spaces on which\nsemantic relations can simply be expressed through predefined distance metrics.\nBest performing approaches commonly leverage class proxies as sample stand-ins\nfor better convergence and generalization. However, these proxy-methods solely\noptimize for sample-proxy distances. Given the inherent non-bijectiveness of\nused distance functions, this can induce locally isotropic sample\ndistributions, leading to crucial semantic context being missed due to\ndifficulties resolving local structures and intraclass relations between\nsamples. To alleviate this problem, we propose non-isotropy regularization\n($\\mathbb{NIR}$) for proxy-based Deep Metric Learning. By leveraging\nNormalizing Flows, we enforce unique translatability of samples from their\nrespective class proxies. This allows us to explicitly induce a non-isotropic\ndistribution of samples around a proxy to optimize for. In doing so, we equip\nproxy-based objectives to better learn local structures. Extensive experiments\nhighlight consistent generalization benefits of $\\mathbb{NIR}$ while achieving\ncompetitive and state-of-the-art performance on the standard benchmarks\nCUB200-2011, Cars196 and Stanford Online Products. In addition, we find the\nsuperior convergence properties of proxy-based methods to still be retained or\neven improved, making $\\mathbb{NIR}$ very attractive for practical usage. Code\navailable at https://github.com/ExplainableML/NonIsotropicProxyDML.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1\">Karsten Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is it all a cluster game? -- Exploring Out-of-Distribution Detection based on Clustering in the Embedding Space. (arXiv:2203.08549v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08549","description":"<p>It is essential for safety-critical applications of deep neural networks to\ndetermine when new inputs are significantly different from the training\ndistribution. In this paper, we explore this out-of-distribution (OOD)\ndetection problem for image classification using clusters of semantically\nsimilar embeddings of the training data and exploit the differences in distance\nrelationships to these clusters between in- and out-of-distribution data. We\nstudy the structure and separation of clusters in the embedding space and find\nthat supervised contrastive learning leads to well-separated clusters while its\nself-supervised counterpart fails to do so. In our extensive analysis of\ndifferent training methods, clustering strategies, distance metrics, and\nthresholding approaches, we observe that there is no clear winner. The optimal\napproach depends on the model architecture and selected datasets for in- and\nout-of-distribution. While we could reproduce the outstanding results for\ncontrastive training on CIFAR-10 as in-distribution data, we find standard\ncross-entropy paired with cosine similarity outperforms all contrastive\ntraining methods when training on CIFAR-100 instead. Cross-entropy provides\ncompetitive results as compared to expensive contrastive training methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinhamahapatra_P/0/1/0/all/0/1\">Poulami Sinhamahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koner_R/0/1/0/all/0/1\">Rajat Koner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_K/0/1/0/all/0/1\">Karsten Roscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonoJSG: Joint Semantic and Geometric Cost Volume for Monocular 3D Object Detection. (arXiv:2203.08563v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08563","description":"<p>Due to the inherent ill-posed nature of 2D-3D projection, monocular 3D object\ndetection lacks accurate depth recovery ability. Although the deep neural\nnetwork (DNN) enables monocular depth-sensing from high-level learned features,\nthe pixel-level cues are usually omitted due to the deep convolution mechanism.\nTo benefit from both the powerful feature representation in DNN and pixel-level\ngeometric constraints, we reformulate the monocular object depth estimation as\na progressive refinement problem and propose a joint semantic and geometric\ncost volume to model the depth error. Specifically, we first leverage neural\nnetworks to learn the object position, dimension, and dense normalized 3D\nobject coordinates. Based on the object depth, the dense coordinates patch\ntogether with the corresponding object features is reprojected to the image\nspace to build a cost volume in a joint semantic and geometric error manner.\nThe final depth is obtained by feeding the cost volume to a refinement network,\nwhere the distribution of semantic and geometric error is regularized by direct\ndepth supervision. Through effectively mitigating depth error by the refinement\nframework, we achieve state-of-the-art results on both the KITTI and Waymo\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1\">Qing Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peiliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaozhi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDTER: Edge Detection with Transformer. (arXiv:2203.08566v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08566","description":"<p>Convolutional neural networks have made significant progresses in edge\ndetection by progressively exploring the context and semantic features.\nHowever, local details are gradually suppressed with the enlarging of receptive\nfields. Recently, vision transformer has shown excellent capability in\ncapturing long-range dependencies. Inspired by this, we propose a novel\ntransformer-based edge detector, \\emph{Edge Detection TransformER (EDTER)}, to\nextract clear and crisp object boundaries and meaningful edges by exploiting\nthe full image context information and detailed local cues simultaneously.\nEDTER works in two stages. In Stage I, a global transformer encoder is used to\ncapture long-range global context on coarse-grained image patches. Then in\nStage II, a local transformer encoder works on fine-grained patches to excavate\nthe short-range local cues. Each transformer encoder is followed by an\nelaborately designed Bi-directional Multi-Level Aggregation decoder to achieve\nhigh-resolution features. Finally, the global context and local cues are\ncombined by a Feature Fusion Module and fed into a decision head for edge\nprediction. Extensive experiments on BSDS500, NYUDv2, and Multicue demonstrate\nthe superiority of EDTER in comparison with state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_M/0/1/0/all/0/1\">Mengyang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yaping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1\">Qingji Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PMAL: Open Set Recognition via Robust Prototype Mining. (arXiv:2203.08569v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08569","description":"<p>Open Set Recognition (OSR) has been an emerging topic. Besides recognizing\npredefined classes, the system needs to reject the unknowns. Prototype learning\nis a potential manner to handle the problem, as its ability to improve\nintra-class compactness of representations is much needed in discrimination\nbetween the known and the unknowns. In this work, we propose a novel Prototype\nMining And Learning (PMAL) framework. It has a prototype mining mechanism\nbefore the phase of optimizing embedding space, explicitly considering two\ncrucial properties, namely high-quality and diversity of the prototype set.\nConcretely, a set of high-quality candidates are firstly extracted from\ntraining samples based on data uncertainty learning, avoiding the interference\nfrom unexpected noise. Considering the multifarious appearance of objects even\nin a single category, a diversity-based strategy for prototype set filtering is\nproposed. Accordingly, the embedding space can be better optimized to\ndiscriminate therein the predefined classes and between known and unknowns.\nExtensive experiments verify the two good characteristics (i.e., high-quality\nand diversity) embraced in prototype mining, and show the remarkable\nperformance of the proposed framework compared to state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunxu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Infrared Image and Video Sets. (arXiv:2203.08581v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08581","description":"<p>In this survey, we compile a list of publicly available infrared image and\nvideo sets for artificial intelligence and computer vision researchers. We\nmainly focus on IR image and video sets which are collected and labelled for\ncomputer vision applications such as object detection, object segmentation,\nclassification, and motion detection. We categorize 92 different publicly\navailable or private sets according to their sensor types, image resolution,\nand scale. We describe each and every set in detail regarding their collection\npurpose, operation environment, optical system properties, and area of\napplication. We also cover a general overview of fundamental concepts that\nrelate to IR imagery, such as IR radiation, IR detectors, IR optics and\napplication fields. We analyse the statistical significance of the entire\ncorpus from different perspectives. We believe that this survey will be a\nguideline for computer vision and artificial intelligence researchers that are\ninterested in working with the spectra beyond the visible domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Danaci_K/0/1/0/all/0/1\">Kevser Irem Danaci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akagunduz_E/0/1/0/all/0/1\">Erdem Akagunduz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep vanishing point detection: Geometric priors make dataset variations vanish. (arXiv:2203.08586v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08586","description":"<p>Deep learning has improved vanishing point detection in images. Yet, deep\nnetworks require expensive annotated datasets trained on costly hardware and do\nnot generalize to even slightly different domains, and minor problem variants.\nHere, we address these issues by injecting deep vanishing point detection\nnetworks with prior knowledge. This prior knowledge no longer needs to be\nlearned from data, saving valuable annotation efforts and compute, unlocking\nrealistic few-sample scenarios, and reducing the impact of domain changes.\nMoreover, the interpretability of the priors allows to adapt deep networks to\nminor problem variations such as switching between Manhattan and non-Manhattan\nworlds. We seamlessly incorporate two geometric priors: (i) Hough Transform --\nmapping image pixels to straight lines, and (ii) Gaussian sphere -- mapping\nlines to great circles whose intersections denote vanishing points.\nExperimentally, we ablate our choices and show comparable accuracy to existing\nmodels in the large-data setting. We validate our model's improved data\nefficiency, robustness to domain changes, adaptability to non-Manhattan\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yancong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiersma_R/0/1/0/all/0/1\">Ruben Wiersma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pintea_S/0/1/0/all/0/1\">Silvia L. Pintea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hildebrandt_K/0/1/0/all/0/1\">Klaus Hildebrandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisemann_E/0/1/0/all/0/1\">Elmar Eisemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan C. van Gemert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CtlGAN: Few-shot Artistic Portraits Generation with Contrastive Transfer Learning. (arXiv:2203.08612v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08612","description":"<p>Generating artistic portraits is a challenging problem in computer vision.\nExisting portrait stylization models that generate good quality results are\nbased on Image-to-Image Translation and require abundant data from both source\nand target domains. However, without enough data, these methods would result in\noverfitting. In this work, we propose CtlGAN, a new few-shot artistic portraits\ngeneration model with a novel contrastive transfer learning strategy. We adapt\na pretrained StyleGAN in the source domain to a target artistic domain with no\nmore than 10 artistic faces. To reduce overfitting to the few training\nexamples, we introduce a novel Cross-Domain Triplet loss which explicitly\nencourages the target instances generated from different latent codes to be\ndistinguishable. We propose a new encoder which embeds real faces into Z+ space\nand proposes a dual-path training strategy to better cope with the adapted\ndecoder and eliminate the artifacts. Extensive qualitative, quantitative\ncomparisons and a user study show our method significantly outperforms\nstate-of-the-arts under 10-shot and 1-shot settings and generates high quality\nartistic portraits. The code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1\">Ran Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Measurement Density Estimation in Sequential Monte Carlo via Normalizing Flow. (arXiv:2203.08617v1 [cs.AI])","link":"http://arxiv.org/abs/2203.08617","description":"<p>Tuning of measurement models is challenging in real-world applications of\nsequential Monte Carlo methods. Recent advances in differentiable particle\nfilters have led to various efforts to learn measurement models through neural\nnetworks. But existing approaches in the differentiable particle filter\nframework do not admit valid probability densities in constructing measurement\nmodels, leading to incorrect quantification of the measurement uncertainty\ngiven state information. We propose to learn expressive and valid probability\ndensities in measurement models through conditional normalizing flows, to\ncapture the complex likelihood of measurements given states. We show that the\nproposed approach leads to improved estimation performance and faster training\nconvergence in a visual tracking experiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiongjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coverage Optimization of Camera Network for Continuous Deformable Object. (arXiv:2203.08632v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08632","description":"<p>In this paper, a deformable object is considered for cameras deployment with\nthe aim of visual coverage. The object contour is discretized into sampled\npoints as meshes, and the deformation is represented as continuous trajectories\nfor the sampled points. To reduce the computational complexity, some feature\npoints are carefully selected representing the continuous deformation process,\nand the visual coverage for the deformable object is transferred to cover the\nspecific feature points. In particular, the vertexes of a rectangle that can\ncontain the entire deformation trajectory of every sampled point on the object\ncontour are chosen as the feature points. An improved wolf pack algorithm is\nthen proposed to solve the optimization problem. Finally, simulation results\nare given to demonstrate the effectiveness of the proposed deployment method of\ncamera network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1\">Li Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complexity Reduction of Learned In-Loop Filtering in Video Coding. (arXiv:2203.08650v1 [eess.IV])","link":"http://arxiv.org/abs/2203.08650","description":"<p>In video coding, in-loop filters are applied on reconstructed video frames to\nenhance their perceptual quality, before storing the frames for output.\nConventional in? loop filters are obtained by hand-crafted methods. Recently,\nlearned filters based on convolutional neural networks that utilize attention\nmechanisms have been shown to improve upon traditional techniques. However,\nthese solutions are typically significantly more computationally expensive,\nlimiting their potential for practical applications. The proposed method uses a\nnovel combination of sparsity and structured pruning for complexity reduction\nof learned in-loop filters. This is done through a three-step training process\nof magnitude-guidedweight pruning, insignificant neuron identification and\nremoval, and fine-tuning. Through initial tests we find that network parameters\ncan be significantly reduced with a minimal impact on network performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bayliss_W/0/1/0/all/0/1\">Woody Bayliss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1\">Luka Murn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Izquierdo_E/0/1/0/all/0/1\">Ebroul Izquierdo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topology-Preserving Shape Reconstruction and Registration via Neural Diffeomorphic Flow. (arXiv:2203.08652v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08652","description":"<p>Deep Implicit Functions (DIFs) represent 3D geometry with continuous signed\ndistance functions learned through deep neural nets. Recently DIFs-based\nmethods have been proposed to handle shape reconstruction and dense point\ncorrespondences simultaneously, capturing semantic relationships across shapes\nof the same class by learning a DIFs-modeled shape template. These methods\nprovide great flexibility and accuracy in reconstructing 3D shapes and\ninferring correspondences. However, the point correspondences built from these\nmethods do not intrinsically preserve the topology of the shapes, unlike\nmesh-based template matching methods. This limits their applications on 3D\ngeometries where underlying topological structures exist and matter, such as\nanatomical structures in medical images. In this paper, we propose a new model\ncalled Neural Diffeomorphic Flow (NDF) to learn deep implicit shape templates,\nrepresenting shapes as conditional diffeomorphic deformations of templates,\nintrinsically preserving shape topologies. The diffeomorphic deformation is\nrealized by an auto-decoder consisting of Neural Ordinary Differential Equation\n(NODE) blocks that progressively map shapes to implicit templates. We conduct\nextensive experiments on several medical image organ segmentation datasets to\nevaluate the effectiveness of NDF on reconstructing and aligning shapes. NDF\nachieves consistently state-of-the-art organ shape reconstruction and\nregistration results in both accuracy and quality. The source code is publicly\navailable at https://github.com/Siwensun/Neural_Diffeomorphic_Flow--NDF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shanlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deying Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiangyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohui Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Occlusion Fields: An Implicit Representation for Non-Line-of-Sight Surface Reconstruction. (arXiv:2203.08657v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08657","description":"<p>Non-line-of-sight reconstruction (NLoS) is a novel indirect imaging modality\nthat aims to recover objects or scene parts outside the field of view from\nmeasurements of light that is indirectly scattered off a directly visible,\ndiffuse wall. Despite recent advances in acquisition and reconstruction\ntechniques, the well-posedness of the problem at large, and the recoverability\nof objects and their shapes in particular, remains an open question. The\ncommonly employed Fermat path criterion is rather conservative with this\nregard, as it classifies some surfaces as unrecoverable, although they\ncontribute to the signal.\n</p>\n<p>In this paper, we use a simpler necessary criterion for an opaque surface\npatch to be recoverable. Such piece of surface must be directly visible from\nsome point on the wall, and it must occlude the space behind itself. Inspired\nby recent advances in neural implicit representations, we devise a new\nrepresentation and reconstruction technique for NLoS scenes that unifies the\ntreatment of recoverability with the reconstruction itself. Our approach, which\nwe validate on various synthetic and experimental datasets, exhibits\ninteresting properties. Unlike memory-inefficient volumetric representations,\nours allows to infer adaptively tessellated surfaces from time-of-flight\nmeasurements of moderate resolution. It can further recover features beyond the\nFermat path criterion, and it is robust to significant amounts of\nself-occlusion. We believe that this is the first time that these properties\nhave been achieved in one system that, as an additional benefit, is trainable\nand hence suited for data-driven approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grau_J/0/1/0/all/0/1\">Javier Grau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plack_M/0/1/0/all/0/1\">Markus Plack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haehn_P/0/1/0/all/0/1\">Patrick Haehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1\">Michael Weinmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullin_M/0/1/0/all/0/1\">Matthias Hullin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Flow: Cross-layer Graph Flow Distillation for Dual-Efficient Medical Image Segmentation. (arXiv:2203.08667v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08667","description":"<p>With the development of deep convolutional neural networks, medical image\nsegmentation has achieved a series of breakthroughs in recent years. However,\nthe higher-performance convolutional neural networks always mean numerous\nparameters and expensive computation costs, which will hinder the applications\nin clinical scenario. Meanwhile, the scarceness of large-scale annotated\nmedical image datasets further impedes the application of high-performance\nnetworks. To tackle these problems, we propose Graph Flow, a novel\ncomprehensive knowledge distillation method, to exploit the cross-layer graph\nflow knowledge for both network-efficient and annotation-efficient medical\nimage segmentation.Specifically, our Graph Flow Distillation constructs a\nvariation graph which is employed to measure the flow of channel-wise salience\nfeatures between different layers. Next, the knowledge included in the\nvariation graph is transferred from a well-trained cumbersome teacher network\nto a non-trained compact student network.In addition, an unsupervised\nParaphraser Module is designed to refine the knowledge of the teacher network,\nwhich is also beneficial for the stabilization of training\nprocedure.Furthermore, we build a unified distillation framework by integrating\nthe adversarial distillation and the vanilla logits distillation, which can\nfurther promote the final performance respectively. As a result, extensive\nexperiments conducted on Gastric Cancer Segmentation Dataset and Synapse\nMulti-organ Segmentation Dataset demonstrate the prominent ability of our\nmethod which achieves state-of-the-art performance on these different-modality\nand multi-category medical image data. Moreover, we demonstrates the\neffectiveness of our Graph Flow through a new semi-supervised paradigm for\ndual-efficient medical image segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wenxuan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Know your sensORs $\\unicode{x2013}$ A Modality Study For Surgical Action Classification. (arXiv:2203.08674v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08674","description":"<p>The surgical operating room (OR) presents many opportunities for automation\nand optimization. Videos from various sources in the OR are becoming\nincreasingly available. The medical community seeks to leverage this wealth of\ndata to develop automated methods to advance interventional care, lower costs,\nand improve overall patient outcomes. Existing datasets from OR room cameras\nare thus far limited in size or modalities acquired, leaving it unclear which\nsensor modalities are best suited for tasks such as recognizing surgical action\nfrom videos. This study demonstrates that surgical action recognition\nperformance can vary depending on the image modalities used. We perform a\nmethodical analysis on several commonly available sensor modalities, presenting\ntwo fusion approaches that improve classification performance. The analyses are\ncarried out on a set of multi-view RGB-D video recordings of 18 laparoscopic\nprocedures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastian_L/0/1/0/all/0/1\">Lennart Bastian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czempiel_T/0/1/0/all/0/1\">Tobias Czempiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heiliger_C/0/1/0/all/0/1\">Christian Heiliger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karcz_K/0/1/0/all/0/1\">Konrad Karcz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eck_U/0/1/0/all/0/1\">Ulrich Eck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupled Knowledge Distillation. (arXiv:2203.08679v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08679","description":"<p>State-of-the-art distillation methods are mainly based on distilling deep\nfeatures from intermediate layers, while the significance of logit distillation\nis greatly overlooked. To provide a novel viewpoint to study logit\ndistillation, we reformulate the classical KD loss into two parts, i.e., target\nclass knowledge distillation (TCKD) and non-target class knowledge distillation\n(NCKD). We empirically investigate and prove the effects of the two parts: TCKD\ntransfers knowledge concerning the \"difficulty\" of training samples, while NCKD\nis the prominent reason why logit distillation works. More importantly, we\nreveal that the classical KD loss is a coupled formulation, which (1)\nsuppresses the effectiveness of NCKD and (2) limits the flexibility to balance\nthese two parts. To address these issues, we present Decoupled Knowledge\nDistillation (DKD), enabling TCKD and NCKD to play their roles more efficiently\nand flexibly. Compared with complex feature-based methods, our DKD achieves\ncomparable or even better results and has better training efficiency on\nCIFAR-100, ImageNet, and MS-COCO datasets for image classification and object\ndetection tasks. This paper proves the great potential of logit distillation,\nand we hope it will be helpful for future research. The code is available at\nhttps://github.com/megvii-research/mdistiller.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Borui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Quan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Renjie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yiyu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiajun Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning video retrieval models with relevance-aware online mining. (arXiv:2203.08688v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08688","description":"<p>Due to the amount of videos and related captions uploaded every hour, deep\nlearning-based solutions for cross-modal video retrieval are attracting more\nand more attention. A typical approach consists in learning a joint text-video\nembedding space, where the similarity of a video and its associated caption is\nmaximized, whereas a lower similarity is enforced with all the other captions,\ncalled negatives. This approach assumes that only the video and caption pairs\nin the dataset are valid, but different captions - positives - may also\ndescribe its visual contents, hence some of them may be wrongly penalized. To\naddress this shortcoming, we propose the Relevance-Aware Negatives and\nPositives mining (RANP) which, based on the semantics of the negatives,\nimproves their selection while also increasing the similarity of other valid\npositives. We explore the influence of these techniques on two video-text\ndatasets: EPIC-Kitchens-100 and MSR-VTT. By using the proposed techniques, we\nachieve considerable improvements in terms of nDCG and mAP, leading to\nstate-of-the-art results, e.g. +5.3% nDCG and +3.0% mAP on EPIC-Kitchens-100.\nWe share code and pretrained models at\n\\url{https://github.com/aranciokov/ranp}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Falcon_A/0/1/0/all/0/1\">Alex Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1\">Giuseppe Serra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanz_O/0/1/0/all/0/1\">Oswald Lanz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation. (arXiv:2203.08713v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08713","description":"<p>This paper proposes a simple baseline framework for video-based 2D/3D human\npose estimation that can achieve 10 times efficiency improvement over existing\nworks without any performance degradation, named DeciWatch. Unlike current\nsolutions that estimate each frame in a video, DeciWatch introduces a simple\nyet effective sample-denoise-recover framework that only watches sparsely\nsampled frames, taking advantage of the continuity of human motions and the\nlightweight pose representation. Specifically, DeciWatch uniformly samples less\nthan 10% video frames for detailed estimation, denoises the estimated 2D/3D\nposes with an efficient Transformer architecture, and then accurately recovers\nthe rest of the frames using another Transformer-based network. Comprehensive\nexperimental results on three video-based human pose estimation and body mesh\nrecovery tasks with four datasets validate the efficiency and effectiveness of\nDeciWatch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_X/0/1/0/all/0/1\">Xuan Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruiyuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational Self-Supervised Learning. (arXiv:2203.08717v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08717","description":"<p>Self-supervised Learning (SSL) including the mainstream contrastive learning\nhas achieved great success in learning visual representations without data\nannotations. However, most methods mainly focus on the instance level\ninformation (\\ie, the different augmented images of the same instance should\nhave the same feature or cluster into the same class), but there is a lack of\nattention on the relationships between different instances. In this paper, we\nintroduce a novel SSL paradigm, which we term as relational self-supervised\nlearning (ReSSL) framework that learns representations by modeling the\nrelationship between different instances. Specifically, our proposed method\nemploys sharpened distribution of pairwise similarities among different\ninstances as \\textit{relation} metric, which is thus utilized to match the\nfeature embeddings of different augmentations. To boost the performance, we\nargue that weak augmentations matter to represent a more reliable relation, and\nleverage momentum strategy for practical efficiency. The designed asymmetric\npredictor head and an InfoNCE warm-up strategy enhance the robustness to\nhyper-parameters and benefit the resulting performance. Experimental results\nshow that our proposed ReSSL substantially outperforms the state-of-the-art\nmethods across different network architectures, including various lightweight\nnetworks (\\eg, EfficientNet and MobileNet).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mingkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attacking deep networks with surrogate-based adversarial black-box methods is easy. (arXiv:2203.08725v1 [cs.LG])","link":"http://arxiv.org/abs/2203.08725","description":"<p>A recent line of work on black-box adversarial attacks has revived the use of\ntransfer from surrogate models by integrating it into query-based search.\nHowever, we find that existing approaches of this type underperform their\npotential, and can be overly complicated besides. Here, we provide a short and\nsimple algorithm which achieves state-of-the-art results through a search which\nuses the surrogate network's class-score gradients, with no need for other\npriors or heuristics. The guiding assumption of the algorithm is that the\nstudied networks are in a fundamental sense learning similar functions, and\nthat a transfer attack from one to the other should thus be fairly \"easy\". This\nassumption is validated by the extremely low query counts and failure rates\nachieved: e.g. an untargeted attack on a VGG-16 ImageNet network using a\nResNet-152 as the surrogate yields a median query count of 6 at a success rate\nof 99.9%. Code is available at https://github.com/fiveai/GFCS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lord_N/0/1/0/all/0/1\">Nicholas A. Lord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_R/0/1/0/all/0/1\">Romain Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertinetto_L/0/1/0/all/0/1\">Luca Bertinetto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tangles and Hierarchical Clustering. (arXiv:2203.08731v1 [cs.DM])","link":"http://arxiv.org/abs/2203.08731","description":"<p>We establish a connection between tangles, a concept from structural graph\ntheory that plays a central role in Robertson and Seymour's graph minor\nproject, and hierarchical clustering. Tangles cannot only be defined for\ngraphs, but in fact for arbitrary connectivity functions, which are functions\ndefined on the subsets of some finite universe. In typical clustering\napplications these universes consist of points in some metric space.\nConnectivity functions are usually required to be submodular. It is our first\ncontribution to show that the central duality theorem connecting tangles with\nhierarchical decompositions (so-called branch decompositions) also holds if\nsubmodularity is replaced by a different property that we call\nmaximum-submodular. We then define a connectivity function on finite data sets\nin an arbitrary metric space and prove that its tangles are in one-to-one\ncorrespondence with the clusters obtained by applying the well-known single\nlinkage clustering algorithms to the same data set. Lastly we generalize this\ncorrespondence for any hierarchical clustering. We show that the data structure\nthat represents hierarchical clustering results, called dendograms, are\nequivalent to maximum-submodular connectivity functions and their tangles. The\nidea of viewing tangles as clusters has first been proposed by Diestel and\nWhittle in 2016 as an approach to image segmentation. To the best of our\nknowledge, our result is the first that establishes a precise technical\nconnection between tangles and clusters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fluck_E/0/1/0/all/0/1\">Eva Fluck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Where To Look -- Generative NAS is Surprisingly Efficient. (arXiv:2203.08734v1 [cs.LG])","link":"http://arxiv.org/abs/2203.08734","description":"<p>The efficient, automated search for well-performing neural architectures\n(NAS) has drawn increasing attention in the recent past. Thereby, the\npredominant research objective is to reduce the necessity of costly evaluations\nof neural architectures while efficiently exploring large search spaces. To\nthis aim, surrogate models embed architectures in a latent space and predict\ntheir performance, while generative models for neural architectures enable\noptimization-based search within the latent space the generator draws from.\nBoth, surrogate and generative models, have the aim of facilitating\nquery-efficient search in a well-structured latent space. In this paper, we\nfurther improve the trade-off between query-efficiency and promising\narchitecture generation by leveraging advantages from both, efficient surrogate\nmodels and generative design. To this end, we propose a generative model,\npaired with a surrogate predictor, that iteratively learns to generate samples\nfrom increasingly promising latent subspaces. This approach leads to very\neffective and efficient architecture search, while keeping the query amount\nlow. In addition, our approach allows in a straightforward manner to jointly\noptimize for multiple objectives such as accuracy and hardware latency. We show\nthe benefit of this approach not only w.r.t. the optimization of architectures\nfor highest classification accuracy but also in the context of hardware\nconstraints and outperform state-of-the art methods on several NAS benchmarks\nfor single and multiple objectives. We also achieve state-of-the-art\nperformance on ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_J/0/1/0/all/0/1\">Jovita Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Steffen Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1\">Margret Keuper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Do Adversarially trained Neural Networks Focus: A Fourier Domain-based Study. (arXiv:2203.08739v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08739","description":"<p>Although many fields have witnessed the superior performance brought about by\ndeep learning, the robustness of neural networks remains an open issue.\nSpecifically, a small adversarial perturbation on the input may cause the model\nto produce a completely different output. Such poor robustness implies many\npotential hazards, especially in security-critical applications, e.g.,\nautonomous driving and mobile robotics. This work studies what information the\nadversarially trained model focuses on. Empirically, we notice that the\ndifferences between the clean and adversarial data are mainly distributed in\nthe low-frequency region. We then find that an adversarially-trained model is\nmore robust than its naturally-trained counterpart due to the reason that the\nformer pays more attention to learning the dominant information in\nlow-frequency components. In addition, we consider two common ways to improve\nmodel robustness, namely, by data augmentation and by using stronger network\narchitectures, and understand these techniques from a frequency-domain\nperspective. We are hopeful this work can shed light on the design of more\nrobust neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Binxiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chaofan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Rui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Ngai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnseenNet: Fast Training Detector for Any Unseen Concept. (arXiv:2203.08759v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08759","description":"<p>Training of object detection models using less data is currently the focus of\nexisting N-shot learning models in computer vision. Such methods use\nobject-level labels and takes hours to train on unseen classes. There are many\ncases where we have large amount of image-level labels available for training\nbut cannot be utilized by few shot object detection models for training. There\nis a need for a machine learning framework that can be used for training any\nunseen class and can become useful in real-time situations. In this paper, we\nproposed an \"Unseen Class Detector\" that can be trained within a very short\ntime for any possible unseen class without bounding boxes with competitive\naccuracy. We build our approach on \"Strong\" and \"Weak\" baseline detectors,\nwhich we trained on existing object detection and image classification\ndatasets, respectively. Unseen concepts are fine-tuned on the strong baseline\ndetector using only image-level labels and further adapted by transferring the\nclassifier-detector knowledge between baselines. We use semantic as well as\nvisual similarities to identify the source class (i.e. Sheep) for the\nfine-tuning and adaptation of unseen class (i.e. Goat). Our model (UnseenNet)\nis trained on the ImageNet classification dataset for unseen classes and tested\non an object detection dataset (OpenImages). UnseenNet improves the mean\naverage precision (mAP) by 10% to 30% over existing baselines (semi-supervised\nand few-shot) of object detection on different unseen class splits. Moreover,\ntraining time of our model is &lt;10 min for each unseen class. Qualitative\nresults demonstrate that UnseenNet is suitable not only for few classes of\nPascal VOC but for unseen classes of any dataset or web. Code is available at\nhttps://github.com/Asra-Aslam/UnseenNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aslam_A/0/1/0/all/0/1\">Asra Aslam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curry_E/0/1/0/all/0/1\">Edward Curry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation. (arXiv:2203.08764v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08764","description":"<p>In computer vision, pre-training models based on largescale supervised\nlearning have been proven effective over the past few years. However, existing\nworks mostly focus on learning from individual task with single data source\n(e.g., ImageNet for classification or COCO for detection). This restricted form\nlimits their generalizability and usability due to the lack of vast semantic\ninformation from various tasks and data sources. Here, we demonstrate that\njointly learning from heterogeneous tasks and multiple data sources contributes\nto universal visual representation, leading to better transferring results of\nvarious downstream tasks. Thus, learning how to bridge the gaps among different\ntasks and data sources is the key, but it still remains an open question. In\nthis work, we propose a representation learning framework called X-Learner,\nwhich learns the universal feature of multiple vision tasks supervised by\nvarious sources, with expansion and squeeze stage: 1) Expansion Stage:\nX-Learner learns the task-specific feature to alleviate task interference and\nenrich the representation by reconciliation layer. 2) Squeeze Stage: X-Learner\ncondenses the model to a reasonable size and learns the universal and\ngeneralizable representation for various tasks transferring. Extensive\nexperiments demonstrate that X-Learner achieves strong performance on different\ntasks without extra annotations, modalities and computational costs compared to\nexisting representation learning methods. Notably, a single X-Learner model\nshows remarkable gains of 3.0%, 3.3% and 1.8% over current pretrained models on\n12 downstream datasets for classification, object detection and semantic\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yinan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gengshi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_J/0/1/0/all/0/1\">Jianing Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kun_W/0/1/0/all/0/1\">Wang Kun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhenfei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1\">Lu Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient conditioned face animation using frontally-viewed embedding. (arXiv:2203.08765v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08765","description":"<p>As the quality of few shot facial animation from landmarks increases, new\napplications become possible, such as ultra low bandwidth video chat\ncompression with a high degree of realism. However, there are some important\nchallenges to tackle in order to improve the experience in real world\nconditions. In particular, the current approaches fail to represent profile\nviews without distortions, while running in a low compute regime. We focus on\nthis key problem by introducing a multi-frames embedding dubbed Frontalizer to\nimprove profile views rendering. In addition to this core improvement, we\nexplore the learning of a latent code conditioning generations along with\nlandmarks to better convey facial expressions. Our dense models achieves 22% of\nimprovement in perceptual quality and 73% reduction of landmark error over the\nfirst order model baseline on a subset of DFDC videos containing head\nmovements. Declined with mobile architectures, our models outperform the\nprevious state-of-the-art (improving perceptual quality by more than 16% and\nreducing landmark error by more than 47% on two datasets) while running on real\ntime on iPhone 8 with very low bandwidth requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oquab_M/0/1/0/all/0/1\">Maxime Oquab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haziza_D/0/1/0/all/0/1\">Daniel Haziza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_L/0/1/0/all/0/1\">Ludovic Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zand_K/0/1/0/all/0/1\">Katayoun Zand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peirong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couprie_C/0/1/0/all/0/1\">Camille Couprie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object discovery and representation networks. (arXiv:2203.08777v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08777","description":"<p>The promise of self-supervised learning (SSL) is to leverage large amounts of\nunlabeled data to solve complex tasks. While there has been excellent progress\nwith simple, image-level learning, recent methods have shown the advantage of\nincluding knowledge of image structure. However, by introducing hand-crafted\nimage segmentations to define regions of interest, or specialized augmentation\nstrategies, these methods sacrifice the simplicity and generality that makes\nSSL so powerful. Instead, we propose a self-supervised learning paradigm that\ndiscovers the structure encoded in these priors by itself. Our method, Odin,\ncouples object discovery and representation networks to discover meaningful\nimage segmentations without any supervision. The resulting learning paradigm is\nsimpler, less brittle, and more general, and achieves state-of-the-art transfer\nlearning results for object detection and instance segmentation on COCO, and\nsemantic segmentation on PASCAL and Cityscapes, while strongly surpassing\nsupervised pre-training for video segmentation on DAVIS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier J. H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoran_D/0/1/0/all/0/1\">Daniel Zoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_R/0/1/0/all/0/1\">Relja Arandjelovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research. (arXiv:2203.08792v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08792","description":"<p>There has been significant progress in machine learning algorithms for human\npose estimation that may provide immense value in rehabilitation and movement\nsciences. However, there remain several challenges to routine use of these\ntools for clinical practice and translational research, including: 1) high\ntechnical barrier to entry, 2) rapidly evolving space of algorithms, 3)\nchallenging algorithmic interdependencies, and 4) complex data management\nrequirements between these components. To mitigate these barriers, we developed\na human pose estimation pipeline that facilitates running state-of-the-art\nalgorithms on data acquired in clinical context. Our system allows for running\ndifferent implementations of several classes of algorithms and handles their\ninterdependencies easily. These algorithm classes include subject\nidentification and tracking, 2D keypoint detection, 3D joint location\nestimation, and estimating the pose of body models. The system uses a database\nto manage videos, intermediate analyses, and data for computations at each\nstage. It also provides tools for data visualization, including generating\nvideo overlays that also obscure faces to enhance privacy. Our goal in this\nwork is not to train new algorithms, but to advance the use of cutting-edge\nhuman pose estimation algorithms for clinical and translation research. We show\nthat this tool facilitates analyzing large numbers of videos of human movement\nranging from gait laboratories analyses, to clinic and therapy visits, to\npeople in the community. We also highlight limitations of these algorithms when\napplied to clinical populations in a rehabilitation setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cotton_R/0/1/0/all/0/1\">R. James Cotton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero Pixel Directional Boundary by Vector Transform. (arXiv:2203.08795v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08795","description":"<p>Boundaries are among the primary visual cues used by human and computer\nvision systems. One of the key problems in boundary detection is the label\nrepresentation, which typically leads to class imbalance and, as a consequence,\nto thick boundaries that require non-differential post-processing steps to be\nthinned. In this paper, we re-interpret boundaries as 1-D surfaces and\nformulate a one-to-one vector transform function that allows for training of\nboundary prediction completely avoiding the class imbalance issue.\nSpecifically, we define the boundary representation at any point as the unit\nvector pointing to the closest boundary surface. Our problem formulation leads\nto the estimation of direction as well as richer contextual information of the\nboundary, and, if desired, the availability of zero-pixel thin boundaries also\nat training time. Our method uses no hyper-parameter in the training loss and a\nfixed stable hyper-parameter at inference. We provide theoretical\njustification/discussions of the vector transform representation. We evaluate\nthe proposed loss method using a standard architecture and show the excellent\nperformance over other losses and representations on several datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rella_E/0/1/0/all/0/1\">Edoardo Mello Rella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1\">Ajad Chhatkuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Continual Learning Framework for Adaptive Defect Classification and Inspection. (arXiv:2203.08796v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08796","description":"<p>Machine-vision-based defect classification techniques have been widely\nadopted for automatic quality inspection in manufacturing processes. This\narticle describes a general framework for classifying defects from high volume\ndata batches with efficient inspection of unlabelled samples. The concept is to\nconstruct a detector to identify new defect types, send them to the inspection\nstation for labelling, and dynamically update the classifier in an efficient\nmanner that reduces both storage and computational needs imposed by data\nsamples of previously observed batches. Both a simulation study on image\nclassification and a case study on surface defect detection via 3D point clouds\nare performed to demonstrate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenbo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontar_R/0/1/0/all/0/1\">Raed Al Kontar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Judy Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tzyy-Shuh Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Theme-Aware Aesthetic Distribution Prediction With Full-Resolution Photographs. (arXiv:1908.01308v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1908.01308","description":"<p>Aesthetic quality assessment (AQA) is a challenging task due to complex\naesthetic factors. Currently, it is common to conduct AQA using deep neural\nnetworks that require fixed-size inputs. Existing methods mainly transform\nimages by resizing, cropping, and padding or employ adaptive pooling to\nalternately capture the aesthetic features from fixed-size inputs. However,\nthese transformations potentially damage aesthetic features. To address this\nissue, we propose a simple but effective method to accomplish full-resolution\nimage AQA by combining image padding with region of image (RoM) pooling.\nPadding turns inputs into the same size. RoM pooling pools image features and\ndiscards extra padded features to eliminate the side effects of padding. In\naddition, the image aspect ratios are encoded and fused with visual features to\nremedy the shape information loss of RoM pooling. Furthermore, we observe that\nthe same image may receive different aesthetic evaluations under different\nthemes, which we call theme criterion bias. Hence, a theme-aware model that\nuses theme information to guide model predictions is proposed. Finally, we\ndesign an attention-based feature fusion module to effectively utilize both the\nshape and theme information. Extensive experiments prove the effectiveness of\nthe proposed method over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1\">Gengyun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peipei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep Learning-based Architectures for Semantic Segmentation on 2D images. (arXiv:1912.10230v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.10230","description":"<p>Semantic segmentation is the pixel-wise labelling of an image. Since the\nproblem is defined at the pixel level, determining image class labels only is\nnot acceptable, but localising them at the original image pixel resolution is\nnecessary. Boosted by the extraordinary ability of convolutional neural\nnetworks (CNN) in creating semantic, high level and hierarchical image\nfeatures; several deep learning-based 2D semantic segmentation approaches have\nbeen proposed within the last decade. In this survey, we mainly focus on the\nrecent scientific developments in semantic segmentation, specifically on deep\nlearning-based methods using 2D images. We started with an analysis of the\npublic image sets and leaderboards for 2D semantic segmentation, with an\noverview of the techniques employed in performance evaluation. In examining the\nevolution of the field, we chronologically categorised the approaches into\nthree main periods, namely pre-and early deep learning era, the fully\nconvolutional era, and the post-FCN era. We technically analysed the solutions\nput forward in terms of solving the fundamental problems of the field, such as\nfine-grained localisation and scale invariance. Before drawing our conclusions,\nwe present a table of methods from all mentioned eras, with a summary of each\napproach that explains their contribution to the field. We conclude the survey\nby discussing the current challenges of the field and to what extent they have\nbeen solved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ulku_I/0/1/0/all/0/1\">Irem Ulku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akagunduz_E/0/1/0/all/0/1\">Erdem Akagunduz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Parameter Allocation Search. (arXiv:2006.10598v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.10598","description":"<p>Training neural networks requires increasing amounts of memory. Parameter\nsharing can reduce memory and communication costs, but existing methods assume\nnetworks have many identical layers and utilize hand-crafted sharing strategies\nthat fail to generalize. We introduce Neural Parameter Allocation Search\n(NPAS), a novel task where the goal is to train a neural network given an\narbitrary, fixed parameter budget. NPAS covers both low-budget regimes, which\nproduce compact networks, as well as a novel high-budget regime, where\nadditional capacity can be added to boost performance without increasing\ninference FLOPs. To address NPAS, we introduce Shapeshifter Networks (SSNs),\nwhich automatically learn where and how to share parameters in a network to\nsupport any parameter budget without requiring any changes to the architecture\nor loss function. NPAS and SSNs provide a complete framework for addressing\ngeneralized parameter sharing, and can also be combined with prior work for\nadditional performance gains. We demonstrate the effectiveness of our approach\nusing nine network architectures across four diverse tasks, including ImageNet\nclassification and transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dryden_N/0/1/0/all/0/1\">Nikoli Dryden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frost_J/0/1/0/all/0/1\">Julius Frost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1\">Torsten Hoefler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Super Resolution Based on Deep Learning: A Comprehensive Survey. (arXiv:2007.12928v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.12928","description":"<p>In recent years, deep learning has made great progress in many fields such as\nimage recognition, natural language processing, speech recognition and video\nsuper-resolution. In this survey, we comprehensively investigate 33\nstate-of-the-art video super-resolution (VSR) methods based on deep learning.\nIt is well known that the leverage of information within video frames is\nimportant for video super-resolution. Thus we propose a taxonomy and classify\nthe methods into six sub-categories according to the ways of utilizing\ninter-frame information. Moreover, the architectures and implementation details\nof all the methods are depicted in detail. Finally, we summarize and compare\nthe performance of the representative VSR method on some benchmark datasets. We\nalso discuss some challenges, which need to be further addressed by researchers\nin the community of VSR. To the best of our knowledge, this work is the first\nsystematic review on VSR tasks, and it is expected to make a contribution to\nthe development of recent studies in this area and potentially deepen our\nunderstanding to the VSR techniques based on deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Z/0/1/0/all/0/1\">Zhubo Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fanhua Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win. (arXiv:2010.03533v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.03533","description":"<p>Sparse Neural Networks (NNs) can match the generalization of dense NNs using\na fraction of the compute/storage for inference, and also have the potential to\nenable efficient training. However, naively training unstructured sparse NNs\nfrom random initialization results in significantly worse generalization, with\nthe notable exceptions of Lottery Tickets (LTs) and Dynamic Sparse Training\n(DST). Through our analysis of gradient flow during training we attempt to\nanswer: (1) why training unstructured sparse networks from random\ninitialization performs poorly and; (2) what makes LTs and DST the exceptions?\nWe show that sparse NNs have poor gradient flow at initialization and\ndemonstrate the importance of using sparsity-aware initialization. Furthermore,\nwe find that DST methods significantly improve gradient flow during training\nover traditional sparse training methods. Finally, we show that LTs do not\nimprove gradient flow, rather their success lies in re-learning the pruning\nsolution they are derived from - however, this comes at the cost of learning\nnovel solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Evci_U/0/1/0/all/0/1\">Utku Evci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ioannou_Y/0/1/0/all/0/1\">Yani A. Ioannou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keskin_C/0/1/0/all/0/1\">Cem Keskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dauphin_Y/0/1/0/all/0/1\">Yann Dauphin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Surprising Positive Knowledge Transfer in Continual 3D Object Shape Reconstruction. (arXiv:2101.07295v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.07295","description":"<p>Continual learning has been extensively studied for classification tasks with\nmethods developed to primarily avoid catastrophic forgetting, a phenomenon\nwhere earlier learned concepts are forgotten at the expense of more recent\nsamples. In this work, we present a set of continual 3D object shape\nreconstruction tasks including complete 3D shape reconstruction from different\ninput modalities and visible surface (2.5D) reconstruction which surprisingly\ndemonstrates positive knowledge (backward and forward) transfer when training\nwith solely vanilla SGD and without additional heuristics. We provide evidence\nthat continuously updated representation learning of single-view 3D shape\nreconstruction improves the performance on learned and novel categories over\ntime. We provide a novel analysis of knowledge transfer ability by looking at\nthe output distribution shift across sequential learning tasks. Finally, we\nshow that the robustness of these tasks leads to the potential of having a\nproxy representation learning task for continual classification. The codebase,\ndataset, and pre-trained models released with this article can be found at\nhttps://github.com/rehg-lab/CLRec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thai_A/0/1/0/all/0/1\">Anh Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stojanov_S/0/1/0/all/0/1\">Stefan Stojanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_I/0/1/0/all/0/1\">Isaac Rehg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning. (arXiv:2103.00370v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.00370","description":"<p>Visual search, recommendation, and contrastive similarity learning power\ntechnologies that impact billions of users worldwide. Modern model\narchitectures can be complex and difficult to interpret, and there are several\ncompeting techniques one can use to explain a search engine's behavior. We show\nthat the theory of fair credit assignment provides a $\\textit{unique}$\naxiomatic solution that generalizes several existing recommendation- and\nmetric-explainability techniques in the literature. Using this formalism, we\nshow when existing approaches violate \"fairness\" and derive methods that\nsidestep these shortcomings and naturally handle counterfactual information.\nMore specifically, we show existing approaches implicitly approximate\nsecond-order Shapley-Taylor indices and extend CAM, GradCAM, LIME, SHAP, SBSM,\nand other methods to search engines. These extensions can extract pairwise\ncorrespondences between images from trained $\\textit{opaque-box}$ models. We\nalso introduce a fast kernel-based method for estimating Shapley-Taylor indices\nthat require orders of magnitude fewer function evaluations to converge.\nFinally, we show that these game-theoretic measures yield more consistent\nexplanations for image similarity architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_M/0/1/0/all/0/1\">Mark Hamilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundberg_S/0/1/0/all/0/1\">Scott Lundberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Stephanie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Ultra-Resolution Neural Style Transfer via Thumbnail Instance Normalization. (arXiv:2103.11784v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11784","description":"<p>We present an extremely simple Ultra-Resolution Style Transfer framework,\ntermed URST, to flexibly process arbitrary high-resolution images (e.g.,\n10000x10000 pixels) style transfer for the first time. Most of the existing\nstate-of-the-art methods would fall short due to massive memory cost and small\nstroke size when processing ultra-high resolution images. URST completely\navoids the memory problem caused by ultra-high resolution images by (1)\ndividing the image into small patches and (2) performing patch-wise style\ntransfer with a novel Thumbnail Instance Normalization (TIN). Specifically, TIN\ncan extract thumbnail features' normalization statistics and apply them to\nsmall patches, ensuring the style consistency among different patches.\n</p>\n<p>Overall, the URST framework has three merits compared to prior arts. (1) We\ndivide input image into small patches and adopt TIN, successfully transferring\nimage style with arbitrary high-resolution. (2) Experiments show that our URST\nsurpasses existing SOTA methods on ultra-high resolution images benefiting from\nthe effectiveness of the proposed stroke perceptual loss in enlarging the\nstroke size. (3) Our URST can be easily plugged into most existing style\ntransfer methods and directly improve their performance even without training.\nCode is available at https://git.io/URST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text to Image Generation with Semantic-Spatial Aware GAN. (arXiv:2104.00567v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00567","description":"<p>Text-to-image synthesis (T2I) aims to generate photo-realistic images which\nare semantically consistent with the text descriptions. Existing methods are\nusually built upon conditional generative adversarial networks (GANs) and\ninitialize an image from noise with sentence embedding, and then refine the\nfeatures with fine-grained word embedding iteratively. A close inspection of\ntheir generated images reveals a major limitation: even though the generated\nimage holistically matches the description, individual image regions or parts\nof somethings are often not recognizable or consistent with words in the\nsentence, e.g. \"a white crown\". To address this problem, we propose a novel\nframework Semantic-Spatial Aware GAN for synthesizing images from input text.\nConcretely, we introduce a simple and effective Semantic-Spatial Aware block,\nwhich (1) learns semantic-adaptive transformation conditioned on text to\neffectively fuse text features and image features, and (2) learns a semantic\nmask in a weakly-supervised way that depends on the current text-image fusion\nprocess in order to guide the transformation spatially. Experiments on the\nchallenging COCO and CUB bird datasets demonstrate the advantage of our method\nover the recent state-of-the-art approaches, regarding both visual fidelity and\nalignment with input text description. Code available at\nhttps://github.com/wtliao/text2image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wentong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Michael Ying Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Deep Neural Networks Forget Facial Action Units? -- Exploring the Effects of Transfer Learning in Health Related Facial Expression Recognition. (arXiv:2104.07389v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07389","description":"<p>In this paper, we present a process to investigate the effects of transfer\nlearning for automatic facial expression recognition from emotions to pain. To\nthis end, we first train a VGG16 convolutional neural network to automatically\ndiscern between eight categorical emotions. We then fine-tune successively\nlarger parts of this network to learn suitable representations for the task of\nautomatic pain recognition. Subsequently, we apply those fine-tuned\nrepresentations again to the original task of emotion recognition to further\ninvestigate the differences in performance between the models. In the second\nstep, we use Layer-wise Relevance Propagation to analyze predictions of the\nmodel that have been predicted correctly previously but are now wrongly\nclassified. Based on this analysis, we rely on the visual inspection of a human\nobserver to generate hypotheses about what has been forgotten by the model.\nFinally, we test those hypotheses quantitatively utilizing concept embedding\nanalysis methods. Our results show that the network, which was fully fine-tuned\nfor pain recognition, indeed payed less attention to two action units that are\nrelevant for expression recognition but not for pain recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prajod_P/0/1/0/all/0/1\">Pooja Prajod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiller_D/0/1/0/all/0/1\">Dominik Schiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1\">Tobias Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1\">Elisabeth Andr&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13450","description":"<p>Digital watermarking is widely used for copyright protection. Traditional 3D\nwatermarking approaches or commercial software are typically designed to embed\nmessages into 3D meshes, and later retrieve the messages directly from\ndistorted/undistorted watermarked 3D meshes. However, in many cases, users only\nhave access to rendered 2D images instead of 3D meshes. Unfortunately,\nretrieving messages from 2D renderings of 3D meshes is still challenging and\nunderexplored. We introduce a novel end-to-end learning framework to solve this\nproblem through: 1) an encoder to covertly embed messages in both mesh geometry\nand textures; 2) a differentiable renderer to render watermarked 3D objects\nfrom different camera angles and under varied lighting conditions; 3) a decoder\nto recover the messages from 2D rendered images. From our experiments, we show\nthat our model can learn to embed information visually imperceptible to humans,\nand to retrieve the embedded information from 2D renderings that undergo 3D\ndistortions. In addition, we demonstrate that our method can also work with\nother renderers, such as ray tracers and real-time renderers with and without\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_I/0/1/0/all/0/1\">Innfarn Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huiwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stava_O/0/1/0/all/0/1\">Ondrej Stava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator. (arXiv:2105.11589v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11589","description":"<p>Interactive robots navigating photo-realistic environments need to be trained\nto effectively leverage and handle the dynamic nature of dialogue in addition\nto the challenges underlying vision-and-language navigation (VLN). In this\npaper, we present VISITRON, a multi-modal Transformer-based navigator better\nsuited to the interactive regime inherent to Cooperative Vision-and-Dialog\nNavigation (CVDN). VISITRON is trained to: i) identify and associate\nobject-level concepts and semantics between the environment and dialogue\nhistory, ii) identify when to interact vs. navigate via imitation learning of a\nbinary classification head. We perform extensive pre-training and fine-tuning\nablations with VISITRON to gain empirical insights and improve performance on\nCVDN. VISITRON's ability to identify when to interact leads to a natural\ngeneralization of the game-play mode introduced by Roman et al.\n(<a href=\"/abs/2005.00728\">arXiv:2005.00728</a>) for enabling the use of such models in different\nenvironments. VISITRON is competitive with models on the static CVDN\nleaderboard and attains state-of-the-art performance on the Success weighted by\nPath Length (SPL) metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1\">Robinson Piramuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan T&#xfc;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-T&#xfc;r</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental False Negative Detection for Contrastive Learning. (arXiv:2106.03719v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03719","description":"<p>Self-supervised learning has recently shown great potential in vision tasks\nthrough contrastive learning, which aims to discriminate each image, or\ninstance, in the dataset. However, such instance-level learning ignores the\nsemantic relationship among instances and sometimes undesirably repels the\nanchor from the semantically similar samples, termed as \"false negatives\". In\nthis work, we show that the unfavorable effect from false negatives is more\nsignificant for the large-scale datasets with more semantic concepts. To\naddress the issue, we propose a novel self-supervised contrastive learning\nframework that incrementally detects and explicitly removes the false negative\nsamples. Specifically, following the training process, our method dynamically\ndetects increasing high-quality false negatives considering that the encoder\ngradually improves and the embedding space becomes more semantically\nstructural. Next, we discuss two strategies to explicitly remove the detected\nfalse negatives during contrastive learning. Extensive experiments show that\nour framework outperforms other self-supervised contrastive learning methods on\nmultiple benchmarks in a limited resource setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tsai-Shien Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei-Chih Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1\">Hung-Yu Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Shao-Yi Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the relation between statistical learning and perceptual distances. (arXiv:2106.04427v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04427","description":"<p>It has been demonstrated many times that the behavior of the human visual\nsystem is connected to the statistics of natural images. Since machine learning\nrelies on the statistics of training data as well, the above connection has\ninteresting implications when using perceptual distances (which mimic the\nbehavior of the human visual system) as a loss function. In this paper, we aim\nto unravel the non-trivial relationships between the probability distribution\nof the data, perceptual distances, and unsupervised machine learning. To this\nend, we show that perceptual sensitivity is correlated with the probability of\nan image in its close neighborhood. We also explore the relation between\ndistances induced by autoencoders and the probability distribution of the\ntraining data, as well as how these induced distances are correlated with human\nperception. Finally, we find perceptual distances do not always lead to\nnoticeable gains in performance over Euclidean distance in common image\nprocessing tasks, except when data is scarce and the perceptual distance\nprovides regularization. We propose this may be due to a \\emph{double-counting}\neffect of the image statistics, once in the perceptual distance and once in the\ntraining procedure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1\">Alexander Hepburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1\">Valero Laparra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1\">Raul Santos-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balle_J/0/1/0/all/0/1\">Johannes Ball&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1\">Jes&#xfa;s Malo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Models as a Data Source for Multiview Representation Learning. (arXiv:2106.05258v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05258","description":"<p>Generative models are now capable of producing highly realistic images that\nlook nearly indistinguishable from the data on which they are trained. This\nraises the question: if we have good enough generative models, do we still need\ndatasets? We investigate this question in the setting of learning\ngeneral-purpose visual representations from a black-box generative model rather\nthan directly from data. Given an off-the-shelf image generator without any\naccess to its training data, we train representations from the samples output\nby this generator. We compare several representation learning methods that can\nbe applied to this setting, using the latent space of the generator to generate\nmultiple \"views\" of the same semantic content. We show that for contrastive\nmethods, this multiview data can naturally be used to identify positive pairs\n(nearby in latent space) and negative pairs (far apart in latent space). We\nfind that the resulting representations rival or even outperform those learned\ndirectly from real data, but that good performance requires care in the\nsampling strategy applied and the training method. Generative models can be\nviewed as a compressed and organized copy of a dataset, and we envision a\nfuture where more and more \"model zoos\" proliferate while datasets become\nincreasingly unwieldy, missing, or private. This paper suggests several\ntechniques for dealing with visual representation learning in such a future.\nCode is available on our project page https://ali-design.github.io/GenRep/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahanian_A/0/1/0/all/0/1\">Ali Jahanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puig_X/0/1/0/all/0/1\">Xavier Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking Outside the Window: Wide-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2106.15754v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15754","description":"<p>Long-range contextual information is crucial for the semantic segmentation of\nHigh-Resolution (HR) Remote Sensing Images (RSIs). However, image cropping\noperations, commonly used for training neural networks, limit the perception of\nlong-range contexts in large RSIs. To overcome this limitation, we propose a\nWide-Context Network (WiCoNet) for the semantic segmentation of HR RSIs. Apart\nfrom extracting local features with a conventional CNN, the WiCoNet has an\nextra context branch to aggregate information from a larger image area.\nMoreover, we introduce a Context Transformer to embed contextual information\nfrom the context branch and selectively project it onto the local features. The\nContext Transformer extends the Vision Transformer, an emerging kind of neural\nnetwork, to model the dual-branch semantic correlations. It overcomes the\nlocality limitation of CNNs and enables the WiCoNet to see the bigger picture\nbefore segmenting the land-cover/land-use (LCLU) classes. Ablation studies and\ncomparative experiments conducted on several benchmark datasets demonstrate the\neffectiveness of the proposed method. In addition, we present a new Beijing\nLand-Use (BLU) dataset. This is a large-scale HR satellite dataset with\nhigh-quality and fine-grained reference labels, which can facilitate future\nstudies in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaofu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaojie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuebin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast whole-slide cartography in colon cancer histology using superpixels and CNN classification. (arXiv:2106.15893v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.15893","description":"<p>Automatic outlining of different tissue types in digitized histological\nspecimen provides a basis for follow-up analyses and can potentially guide\nsubsequent medical decisions. The immense size of whole-slide-images (WSI),\nhowever, poses a challenge in terms of computation time. In this regard, the\nanalysis of non-overlapping patches outperforms pixelwise segmentation\napproaches, but still leaves room for optimization. Furthermore, the division\ninto patches, regardless of the biological structures they contain, is a\ndrawback due to the loss of local dependencies. We propose to subdivide the WSI\ninto coherent regions prior to classification by grouping visually similar\nadjacent pixels into superpixels. Afterwards, only a random subset of patches\nper superpixel is classified and patch labels are combined into a superpixel\nlabel. We propose a metric for identifying superpixels with an uncertain\nclassification and evaluate two medical applications, namely tumor area and\ninvasive margin estimation and tumor composition analysis. The algorithm has\nbeen developed on 159 hand-annotated WSIs of colon resections and its\nperformance is compared to an analysis without prior segmentation. The\nalgorithm shows an average speed-up of 41% and an increase in accuracy from\n93.8% to 95.7%. By assigning a rejection label to uncertain superpixels, we\nfurther increase the accuracy by 0.4%. Whilst tumor area estimation shows high\nconcordance to the annotated area, the analysis of tumor composition highlights\nlimitations of our approach. By combining superpixel segmentation and patch\nclassification, we designed a fast and accurate framework for whole-slide\ncartography that is AI-model agnostic and provides the basis for various\nmedical endpoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wilm_F/0/1/0/all/0/1\">Frauke Wilm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benz_M/0/1/0/all/0/1\">Michaela Benz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bruns_V/0/1/0/all/0/1\">Volker Bruns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baghdadlian_S/0/1/0/all/0/1\">Serop Baghdadlian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dexl_J/0/1/0/all/0/1\">Jakob Dexl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hartmann_D/0/1/0/all/0/1\">David Hartmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuritcyn_P/0/1/0/all/0/1\">Petr Kuritcyn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weidenfeller_M/0/1/0/all/0/1\">Martin Weidenfeller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wittenberg_T/0/1/0/all/0/1\">Thomas Wittenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Merkel_S/0/1/0/all/0/1\">Susanne Merkel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hartmann_A/0/1/0/all/0/1\">Arndt Hartmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eckstein_M/0/1/0/all/0/1\">Markus Eckstein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geppert_C/0/1/0/all/0/1\">Carol I. Geppert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind Image Super-resolution with Elaborate Degradation Modeling on Noise and Kernel. (arXiv:2107.00986v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00986","description":"<p>While researches on model-based blind single image super-resolution (SISR)\nhave achieved tremendous successes recently, most of them do not consider the\nimage degradation sufficiently. Firstly, they always assume image noise obeys\nan independent and identically distributed (i.i.d.) Gaussian or Laplacian\ndistribution, which largely underestimates the complexity of real noise.\nSecondly, previous commonly-used kernel priors (e.g., normalization, sparsity)\nare not effective enough to guarantee a rational kernel solution, and thus\ndegenerates the performance of subsequent SISR task. To address the above\nissues, this paper proposes a model-based blind SISR method under the\nprobabilistic framework, which elaborately models image degradation from the\nperspectives of noise and blur kernel. Specifically, instead of the traditional\ni.i.d. noise assumption, a patch-based non-i.i.d. noise model is proposed to\ntackle the complicated real noise, expecting to increase the degrees of freedom\nof the model for noise representation. As for the blur kernel, we novelly\nconstruct a concise yet effective kernel generator, and plug it into the\nproposed blind SISR method as an explicit kernel prior (EKP). To solve the\nproposed model, a theoretically grounded Monte Carlo EM algorithm is\nspecifically designed. Comprehensive experiments demonstrate the superiority of\nour method over current state-of-the-arts on synthetic and real datasets. The\nsource code is available at https://github.com/zsyOAOA/BSRDM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zongsheng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianwen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kwan-Yee K. Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Discovery of Object Radiance Fields. (arXiv:2107.07905v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.07905","description":"<p>We study the problem of inferring an object-centric scene representation from\na single image, aiming to derive a representation that explains the image\nformation process, captures the scene's 3D nature, and is learned without\nsupervision. Most existing methods on scene decomposition lack one or more of\nthese characteristics, due to the fundamental challenge in integrating the\ncomplex 3D-to-2D image formation process into powerful inference schemes like\ndeep networks. In this paper, we propose unsupervised discovery of Object\nRadiance Fields (uORF), integrating recent progresses in neural 3D scene\nrepresentations and rendering with deep inference networks for unsupervised 3D\nscene decomposition. Trained on multi-view RGB images without annotations, uORF\nlearns to decompose complex scenes with diverse, textured background from a\nsingle image. We show that uORF enables novel tasks, such as scene segmentation\nand editing in 3D, and it performs well on these tasks and on novel view\nsynthesis on three datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong-Xing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs. (arXiv:2107.14795v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.14795","description":"<p>A central goal of machine learning is the development of systems that can\nsolve many problems in as many data domains as possible. Current architectures,\nhowever, cannot be applied beyond a small set of stereotyped settings, as they\nbake in domain &amp; task assumptions or scale poorly to large inputs or outputs.\nIn this work, we propose Perceiver IO, a general-purpose architecture that\nhandles data from arbitrary settings while scaling linearly with the size of\ninputs and outputs. Our model augments the Perceiver with a flexible querying\nmechanism that enables outputs of various sizes and semantics, doing away with\nthe need for task-specific architecture engineering. The same architecture\nachieves strong results on tasks spanning natural language and visual\nunderstanding, multi-task and multi-modal reasoning, and StarCraft II. As\nhighlights, Perceiver IO outperforms a Transformer-based BERT baseline on the\nGLUE language benchmark despite removing input tokenization and achieves\nstate-of-the-art performance on Sintel optical flow estimation with no explicit\nmechanisms for multiscale correspondence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1\">Carl Doersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1\">Catalin Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoran_D/0/1/0/all/0/1\">Daniel Zoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matthew M. Botvinick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#x101;o Carreira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rapid Elastic Architecture Search under Specialized Classes and Resource Constraints. (arXiv:2108.01224v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01224","description":"<p>In many real-world applications, we often need to handle various deployment\nscenarios, where the resource constraint and the superclass of interest\ncorresponding to a group of classes are dynamically specified. How to\nefficiently deploy deep models for diverse deployment scenarios is a new\nchallenge. Previous NAS approaches seek to design architectures for all classes\nsimultaneously, which may not be optimal for some individual superclasses. A\nstraightforward solution is to search an architecture from scratch for each\ndeployment scenario, which however is computation-intensive and impractical. To\naddress this, we present a novel and general framework, called Elastic\nArchitecture Search (EAS), permitting instant specializations at runtime for\ndiverse superclasses with various resource constraints. To this end, we first\npropose to effectively train an over-parameterized network via a superclass\ndropout strategy during training. In this way, the resulting model is robust to\nthe subsequent superclasses dropping at inference time. Based on the\nwell-trained over-parameterized network, we then propose an efficient\narchitecture generator to obtain promising architectures within a single\nforward pass. Experiments on three image classification datasets show that EAS\nis able to find more compact networks with better performance while remarkably\nbeing orders of magnitude faster than state-of-the-art NAS methods, e.g.,\noutperforming OFA (once-for-all) by 1.3% on Top-1 accuracy at a budget around\n361M #MAdds on ImageNet-10. More critically, EAS is able to find compact\narchitectures within 0.1 second for 50 deployment scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SphereFace2: Binary Classification is All You Need for Deep Face Recognition. (arXiv:2108.01513v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01513","description":"<p>State-of-the-art deep face recognition methods are mostly trained with a\nsoftmax-based multi-class classification framework. Despite being popular and\neffective, these methods still have a few shortcomings that limit empirical\nperformance. In this paper, we start by identifying the discrepancy between\ntraining and evaluation in the existing multi-class classification framework\nand then discuss the potential limitations caused by the \"competitive\" nature\nof softmax normalization. Motivated by these limitations, we propose a novel\nbinary classification training framework, termed SphereFace2. In contrast to\nexisting methods, SphereFace2 circumvents the softmax normalization, as well as\nthe corresponding closed-set assumption. This effectively bridges the gap\nbetween training and evaluation, enabling the representations to be improved\nindividually by each binary classification task. Besides designing a specific\nwell-performing loss function, we summarize a few general principles for this\n\"one-vs-all\" binary classification framework so that it can outperform current\ncompetitive methods. Our experiments on popular benchmarks demonstrate that\nSphereFace2 can consistently outperform state-of-the-art deep face recognition\nmethods. The code has been made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yandong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The University of California San Francisco Preoperative Diffuse Glioma MRI (UCSF-PDGM) Dataset. (arXiv:2109.00356v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00356","description":"<p>Here we present the University of California San Francisco Preoperative\nDiffuse Glioma MRI (UCSF-PDGM) dataset. The UCSF-PDGM dataset includes 500\nsubjects with histopathologically-proven diffuse gliomas who were imaged with a\nstandardized 3 Tesla preoperative brain tumor MRI protocol featuring\npredominantly 3D imaging, as well as advanced diffusion and perfusion imaging\ntechniques. The dataset also includes isocitrate dehydrogenase (IDH) mutation\nstatus for all cases and O6-methylguanine-DNA methyltransferase (MGMT) promotor\nmethylation status for World Health Organization (WHO) grade III and IV\ngliomas. The UCSF-PDGM has been made publicly available in the hopes that\nresearchers around the world will use these data to continue to push the\nboundaries of AI applications for diffuse gliomas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calabrese_E/0/1/0/all/0/1\">Evan Calabrese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villanueva_Meyer_J/0/1/0/all/0/1\">Javier E. Villanueva-Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudie_J/0/1/0/all/0/1\">Jeffrey D. Rudie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauschecker_A/0/1/0/all/0/1\">Andreas M. Rauschecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baid_U/0/1/0/all/0/1\">Ujjwal Baid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Soonmee Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mongan_J/0/1/0/all/0/1\">John T. Mongan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hess_C/0/1/0/all/0/1\">Christopher P. Hess</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SphereFace Revived: Unifying Hyperspherical Face Recognition. (arXiv:2109.05565v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05565","description":"<p>This paper addresses the deep face recognition problem under an open-set\nprotocol, where ideal face features are expected to have smaller maximal\nintra-class distance than minimal inter-class distance under a suitably chosen\nmetric space. To this end, hyperspherical face recognition, as a promising line\nof research, has attracted increasing attention and gradually become a major\nfocus in face recognition research. As one of the earliest works in\nhyperspherical face recognition, SphereFace explicitly proposed to learn face\nembeddings with large inter-class angular margin. However, SphereFace still\nsuffers from severe training instability which limits its application in\npractice. In order to address this problem, we introduce a unified framework to\nunderstand large angular margin in hyperspherical face recognition. Under this\nframework, we extend the study of SphereFace and propose an improved variant\nwith substantially better training stability -- SphereFace-R. Specifically, we\npropose two novel ways to implement the multiplicative margin, and study\nSphereFace-R under three different feature normalization schemes (no feature\nnormalization, hard feature normalization and soft feature normalization). We\nalso propose an implementation strategy -- \"characteristic gradient detachment\"\n-- to stabilize training. Extensive experiments on SphereFace-R show that it is\nconsistently better than or competitive with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yandong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Downsample for Segmentation of Ultra-High Resolution Images. (arXiv:2109.11071v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11071","description":"<p>Many computer vision systems require low-cost segmentation algorithms based\non deep learning, either because of the enormous size of input images or\nlimited computational budget. Common solutions uniformly downsample the input\nimages to meet memory constraints, assuming all pixels are equally informative.\nIn this work, we demonstrate that this assumption can harm the segmentation\nperformance because the segmentation difficulty varies spatially. We combat\nthis problem by introducing a learnable downsampling module, which can be\noptimised together with the given segmentation model in an end-to-end fashion.\nWe formulate the problem of training such downsampling module as optimisation\nof sampling density distributions over the input images given their\nlow-resolution views. To defend against degenerate solutions (e.g.\nover-sampling trivial regions like the backgrounds), we propose a\nregularisation term that encourages the sampling locations to concentrate\naround the object boundaries. We find the downsampling module learns to sample\nmore densely at difficult locations, thereby improving the segmentation\nperformance. Our experiments on benchmarks of high-resolution street view,\naerial and medical images demonstrate substantial improvements in terms of\nefficiency-and-accuracy trade-off compared to both uniform downsampling and two\nrecent advanced downsampling techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1\">Ryutaro Tanno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertzanidou_T/0/1/0/all/0/1\">Thomy Mertzanidou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagiotaki_E/0/1/0/all/0/1\">Eleftheria Panagiotaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C. Alexander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spike-inspired Rank Coding for Fast and Accurate Recurrent Neural Networks. (arXiv:2110.02865v3 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2110.02865","description":"<p>Biological spiking neural networks (SNNs) can temporally encode information\nin their outputs, e.g. in the rank order in which neurons fire, whereas\nartificial neural networks (ANNs) conventionally do not. As a result, models of\nSNNs for neuromorphic computing are regarded as potentially more rapid and\nefficient than ANNs when dealing with temporal input. On the other hand, ANNs\nare simpler to train, and usually achieve superior performance. Here we show\nthat temporal coding such as rank coding (RC) inspired by SNNs can also be\napplied to conventional ANNs such as LSTMs, and leads to computational savings\nand speedups. In our RC for ANNs, we apply backpropagation through time using\nthe standard real-valued activations, but only from a strategically early time\nstep of each sequential input example, decided by a threshold-crossing event.\nLearning then incorporates naturally also _when_ to produce an output, without\nother changes to the model or the algorithm. Both the forward and the backward\ntraining pass can be significantly shortened by skipping the remaining input\nsequence after that first event. RC-training also significantly reduces\ntime-to-insight during inference, with a minimal decrease in accuracy. The\ndesired speed-accuracy trade-off is tunable by varying the threshold or a\nregularization parameter that rewards output entropy. We demonstrate these in\ntwo toy problems of sequence classification, and in a temporally-encoded MNIST\ndataset where our RC model achieves 99.19% accuracy after the first input\ntime-step, outperforming the state of the art in temporal coding with SNNs, as\nwell as in spoken-word classification of Google Speech Commands, outperforming\nnon-RC-trained early inference with LSTMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeffares_A/0/1/0/all/0/1\">Alan Jeffares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qinghai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1\">Timoleon Moraitis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSSM: A Blueprint for Image-to-Shape Deep Learning Models. (arXiv:2110.07152v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07152","description":"<p>Statistical shape modeling (SSM) characterizes anatomical variations in a\npopulation of shapes generated from medical images. SSM requires consistent\nshape representation across samples in shape cohort. Establishing this\nrepresentation entails a processing pipeline that includes anatomy\nsegmentation, re-sampling, registration, and non-linear optimization. These\nshape representations are then used to extract low-dimensional shape\ndescriptors that facilitate subsequent analyses in different applications.\nHowever, the current process of obtaining these shape descriptors from imaging\ndata relies on human and computational resources, requiring domain expertise\nfor segmenting anatomies of interest. Moreover, this same taxing pipeline needs\nto be repeated to infer shape descriptors for new image data using a\npre-trained/existing shape model. Here, we propose DeepSSM, a deep\nlearning-based framework for learning the functional mapping from images to\nlow-dimensional shape descriptors and their associated shape representations,\nthereby inferring statistical representation of anatomy directly from 3D\nimages. Once trained using an existing shape model, DeepSSM circumvents the\nheavy and manual pre-processing and segmentation and significantly improves the\ncomputational time, making it a viable solution for fully end-to-end SSM\napplications. In addition, we introduce a model-based data-augmentation\nstrategy to address data scarcity. Finally, this paper presents and analyzes\ntwo different architectural variants of DeepSSM with different loss functions\nusing three medical datasets and their downstream clinical application.\nExperiments showcase that DeepSSM performs comparably or better to the\nstate-of-the-art SSM both quantitatively and on application-driven downstream\ntasks. Therefore, DeepSSM aims to provide a comprehensive blueprint for deep\nlearning-based image-to-shape models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhalodia_R/0/1/0/all/0/1\">Riddhish Bhalodia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1\">Shireen Elhabian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1\">Jadie Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1\">Wenzheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavan_L/0/1/0/all/0/1\">Ladislav Kavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitaker_R/0/1/0/all/0/1\">Ross Whitaker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Procedural Knowledge by Sequencing Multimodal Instructional Manuals. (arXiv:2110.08486v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08486","description":"<p>The ability to sequence unordered events is an essential skill to comprehend\nand reason about real world task procedures, which often requires thorough\nunderstanding of temporal common sense and multimodal information, as these\nprocedures are often communicated through a combination of texts and images.\nSuch capability is essential for applications such as sequential task planning\nand multi-source instruction summarization. While humans are capable of\nreasoning about and sequencing unordered multimodal procedural instructions,\nwhether current machine learning models have such essential capability is still\nan open question. In this work, we benchmark models' capability of reasoning\nover and sequencing unordered multimodal instructions by curating datasets from\npopular online instructional manuals and collecting comprehensive human\nannotations. We find models not only perform significantly worse than humans\nbut also seem incapable of efficiently utilizing the multimodal information. To\nimprove machines' performance on multimodal event sequencing, we propose\nsequentiality-aware pretraining techniques that exploit the sequential\nalignment properties of both texts and images, resulting in &gt; 5% significant\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1\">Alex Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alipoormolabashi_P/0/1/0/all/0/1\">Pegah Alipoormolabashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1\">Marjorie Freedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1\">Ralph Weischedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Efficiency Misnomer. (arXiv:2110.12894v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.12894","description":"<p>Model efficiency is a critical aspect of developing and deploying machine\nlearning models. Inference time and latency directly affect the user\nexperience, and some applications have hard requirements. In addition to\ninference costs, model training also have direct financial and environmental\nimpacts. Although there are numerous well-established metrics (cost indicators)\nfor measuring model efficiency, researchers and practitioners often assume that\nthese metrics are correlated with each other and report only few of them. In\nthis paper, we thoroughly discuss common cost indicators, their advantages and\ndisadvantages, and how they can contradict each other. We demonstrate how\nincomplete reporting of cost indicators can lead to partial conclusions and a\nblurred or incomplete picture of the practical considerations of different\nmodels. We further present suggestions to improve reporting of efficiency\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1\">Ashish Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporally Consistent Online Depth Estimation in Dynamic Scenes. (arXiv:2111.09337v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09337","description":"<p>Temporally consistent depth estimation is crucial for real-time applications.\nWhile stereo depth estimation has received substantial attention, there is\nrelatively little work focused on maintaining temporal stability. Indeed, based\non our analysis, current techniques still suffer from poor temporal\nconsistency. Stabilizing depth temporally in dynamic scenes is challenging due\nto concurrent object and camera motion. In an online setting, this process is\nfurther aggravated because only past frames are available. We present an\napproach to produce temporally consistent depth estimates in dynamic scenes in\nan online setting. Our network augments per-frame stereo networks with novel\nmotion and fusion networks. The motion network accounts for object and camera\nmotion by predicting a per-pixel SE3 transformation. The fusion network\nimproves temporal consistency in predictions by aggregating the current and\nprevious estimates. We conduct extensive experiments across varied datasets. We\ndemonstrate that our proposed approach outperforms competing methods in terms\nof temporal consistency and per-frame accuracy, both quantitatively and\nqualitatively. Our code will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoshuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creighton_F/0/1/0/all/0/1\">Francis X. Creighton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Russell H. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1\">Ganesh Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mesa: A Memory-saving Training Framework for Transformers. (arXiv:2111.11124v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11124","description":"<p>There has been an explosion of interest in designing high-performance\nTransformers. While Transformers have delivered significant performance\nimprovements, training such networks is extremely memory intensive owing to\nstoring all intermediate activations that are needed for gradient computation\nduring backpropagation, especially for long sequences. To this end, we present\nMesa, a memory-saving training framework for Transformers. Specifically, Mesa\nuses exact activations during forward pass while storing a low-precision\nversion of activations to reduce memory consumption during training. The\nlow-precision activations are then dequantized during back-propagation to\ncompute gradients. Besides, to address the heterogeneous activation\ndistributions in the multi-head self-attention layers, we propose a head-wise\nactivation quantization strategy, which quantizes activations based on the\nstatistics of each head to minimize the approximation error. To further boost\ntraining efficiency, we learn quantization parameters by running estimates.\nMore importantly, by re-investing the saved memory in employing a larger batch\nsize or scaling up model size, we may further improve the performance under\nconstrained computational resources. Extensive experiments on ImageNet,\nCIFAR-100 and ADE20K demonstrate that Mesa can achieve flexible memory-savings\n(up to 50%) during training while achieving comparable or even better\nperformance. Code is available at https://github.com/zip-group/Mesa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointMixer: MLP-Mixer for Point Cloud Understanding. (arXiv:2111.11187v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11187","description":"<p>MLP-Mixer has newly appeared as a new challenger against the realm of CNNs\nand transformer. Despite its simplicity compared to transformer, the concept of\nchannel-mixing MLPs and token-mixing MLPs achieves noticeable performance in\nvisual recognition tasks. Unlike images, point clouds are inherently sparse,\nunordered and irregular, which limits the direct use of MLP-Mixer for point\ncloud understanding. In this paper, we propose PointMixer, a universal point\nset operator that facilitates information sharing among unstructured 3D points.\nBy simply replacing token-mixing MLPs with a softmax function, PointMixer can\n\"mix\" features within/between point sets. By doing so, PointMixer can be\nbroadly used in the network as inter-set mixing, intra-set mixing, and pyramid\nmixing. Extensive experiments show the competitive or superior performance of\nPointMixer in semantic segmentation, classification, and point reconstruction\nagainst transformer-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Jaesung Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rameau_F/0/1/0/all/0/1\">Francois Rameau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning Self-attentions into Convolutional Layers in Single Path. (arXiv:2111.11802v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11802","description":"<p>Vision Transformers (ViTs) have achieved impressive performance over various\ncomputer vision tasks. However, modeling global correlations with multi-head\nself-attention (MSA) layers leads to two widely recognized issues: the massive\ncomputational resource consumption and the lack of intrinsic inductive bias for\nmodeling local visual patterns like convolution. To solve both issues\nseamlessly, we devise a simple yet effective method named Single-Path Vision\nTransformer pruning (SPViT), to efficiently and automatically compress the\npre-trained ViTs into compact models with proper locality added. Specifically,\nwe first propose a novel weight-sharing scheme between MSA and convolutional\noperations, delivering a single-path space to encode all candidate operations.\nIn this way, we cast the operation search problem as finding which subset of\nparameters to use in each MSA layer, which significantly reduces the\ncomputational cost and optimization difficulty, and the convolution kernels can\nbe well initialized using pre-trained MSA parameters. Relying on the\nsingle-path space, we further introduce learnable binary gates to encode the\noperation choices, which are jointly optimized with network parameters to\nautomatically determine the configuration of each layer. We conduct extensive\nexperiments on two representative ViT models showing our method achieves a\nfavorable accuracy-efficiency trade-off. For example, our SPViT achieves SOTA\npruning performance by trimming 52.6% FLOPs for DeiT-B with only 0.3% top-1\naccuracy loss. Code is available at https://github.com/zip-group/SPViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-free unsupervised domain adaptation for cross-modality abdominal multi-organ segmentation. (arXiv:2111.12221v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12221","description":"<p>It is valuable to achieve domain adaptation to transfer the learned knowledge\nfrom the source labeled CT dataset to the target unlabeled MR dataset for\nabdominal multi-organ segmentation. Meanwhile, it is highly desirable to avoid\nhigh annotation cost of target dataset and protect privacy of source dataset.\nTherefore, we propose an effective source-free unsupervised domain adaptation\nmethod for cross-modality abdominal multi-organ segmentation without accessing\nthe source dataset. The process of the proposed framework includes two stages.\nAt the first stage, the feature map statistics-guided model adaptation\ncombining with entropy minimization is developed to help the top segmentation\nnetwork to achieve reliable segmentations on the target images. The\npseudo-labels outputted from the top segmentation network is used to guide the\nstyle compensation network to generate source-like images. The pseudo-labels\noutputted from the middle segmentation network is used to supervise the\nlearning of the desired model (the bottom segmentation network). At the second\nstage, the circular learning and the pixel-adaptive mask refinement are used to\nfurther improve the performance of the desired model. With this approach, we\nachieve satisfactory performance on the abdominal multi-organ segmentation,\noutperforming recent state-of-the-art domain adaptation methods. The proposed\napproach can also be easily extended to the situation when there exists target\nannotation data. With only one labeled MR volume, the performance is improved\nto close to supervised learning. Furthermore, the proposed approach is proved\nto be effective in source-free unsupervised domain adaptation in reverse\ndirection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu-Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weitian Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sharpness-aware Quantization for Deep Neural Networks. (arXiv:2111.12273v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12273","description":"<p>Network quantization is an effective compression method to reduce the model\nsize and computational cost. Despite the high compression ratio, training a\nlow-precision model is difficult due to the discrete and non-differentiable\nnature of quantization, resulting in considerable performance degradation.\nRecently, Sharpness-Aware Minimization (SAM) has been proposed to improve the\ngeneralization performance of the models by simultaneously minimizing the loss\nvalue and the loss curvature. However, SAM can not be directly applied to\nquantized models due to the discretization process in network quantization. In\nthis paper, we devise a Sharpness-Aware Quantization (SAQ) method to train\nquantized models, leading to better generalization performance. Moreover, since\neach layer contributes differently to the loss value and the loss sharpness of\na network, we further devise an effective method that learns a configuration\ngenerator to automatically determine the bitwidth configurations of each layer,\nencouraging lower bits for flat regions and vice versa for sharp landscapes,\nwhile simultaneously promoting the flatness of minima to enable more aggressive\nquantization. Extensive experiments on CIFAR-100 and ImageNet show the superior\nperformance of the proposed methods. For example, our quantized ResNet-18 with\n53.7x Bit-Operation (BOP) reduction even outperforms the full-precision one by\n0.7% in terms of the Top-1 accuracy. Code is available at\nhttps://github.com/zip-group/SAQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Equivariant Imaging: a fully unsupervised framework for learning to image from noisy and partial measurements. (arXiv:2111.12855v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12855","description":"<p>Deep networks provide state-of-the-art performance in multiple imaging\ninverse problems ranging from medical imaging to computational photography.\nHowever, most existing networks are trained with clean signals which are often\nhard or impossible to obtain. Equivariant imaging (EI) is a recent\nself-supervised learning framework that exploits the group invariance present\nin signal distributions to learn a reconstruction function from partial\nmeasurement data alone. While EI results are impressive, its performance\ndegrades with increasing noise. In this paper, we propose a Robust Equivariant\nImaging (REI) framework which can learn to image from noisy partial\nmeasurements alone. The proposed method uses Stein's Unbiased Risk Estimator\n(SURE) to obtain a fully unsupervised training loss that is robust to noise. We\nshow that REI leads to considerable performance gains on linear and nonlinear\ninverse problems, thereby paving the way for robust unsupervised imaging with\ndeep networks. Code is available at: https://github.com/edongdongchen/REI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tachella_J/0/1/0/all/0/1\">Juli&#xe1;n Tachella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davies_M/0/1/0/all/0/1\">Mike E. Davies</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-Efficient Semantic Segmentation with Diffusion Models. (arXiv:2112.03126v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03126","description":"<p>Denoising diffusion probabilistic models have recently received much research\nattention since they outperform alternative approaches, such as GANs, and\ncurrently provide state-of-the-art generative performance. The superior\nperformance of diffusion models has made them an appealing tool in several\napplications, including inpainting, super-resolution, and semantic editing. In\nthis paper, we demonstrate that diffusion models can also serve as an\ninstrument for semantic segmentation, especially in the setup when labeled data\nis scarce. In particular, for several pretrained diffusion models, we\ninvestigate the intermediate activations from the networks that perform the\nMarkov step of the reverse diffusion process. We show that these activations\neffectively capture the semantic information from an input image and appear to\nbe excellent pixel-level representations for the segmentation problem. Based on\nthese observations, we describe a simple segmentation method, which can work\neven if only a few training images are provided. Our approach significantly\noutperforms the existing alternatives on several datasets for the same amount\nof human supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baranchuk_D/0/1/0/all/0/1\">Dmitry Baranchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubachev_I/0/1/0/all/0/1\">Ivan Rubachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1\">Andrey Voynov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khrulkov_V/0/1/0/all/0/1\">Valentin Khrulkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1\">Artem Babenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaptPose: Cross-Dataset Adaptation for 3D Human Pose Estimation by Learnable Motion Generation. (arXiv:2112.11593v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11593","description":"<p>This paper addresses the problem of cross-dataset generalization of 3D human\npose estimation models. Testing a pre-trained 3D pose estimator on a new\ndataset results in a major performance drop. Previous methods have mainly\naddressed this problem by improving the diversity of the training data. We\nargue that diversity alone is not sufficient and that the characteristics of\nthe training data need to be adapted to those of the new dataset such as camera\nviewpoint, position, human actions, and body size. To this end, we propose\nAdaptPose, an end-to-end framework that generates synthetic 3D human motions\nfrom a source dataset and uses them to fine-tune a 3D pose estimator. AdaptPose\nfollows an adversarial training scheme. From a source 3D pose the generator\ngenerates a sequence of 3D poses and a camera orientation that is used to\nproject the generated poses to a novel view. Without any 3D labels or camera\ninformation AdaptPose successfully learns to create synthetic 3D poses from the\ntarget dataset while only being trained on 2D poses. In experiments on the\nHuman3.6M, MPI-INF-3DHP, 3DPW, and Ski-Pose datasets our method outperforms\nprevious work in cross-dataset evaluations by 14% and previous semi-supervised\nlearning methods that use partial 3D annotations by 16%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gholami_M/0/1/0/all/0/1\">Mohsen Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1\">Bastian Wandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_R/0/1/0/all/0/1\">Rabab Ward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound. (arXiv:2201.02639v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02639","description":"<p>As humans, we navigate a multimodal world, building a holistic understanding\nfrom all our senses. We introduce MERLOT Reserve, a model that represents\nvideos jointly over time -- through a new training objective that learns from\naudio, subtitles, and video frames. Given a video, we replace snippets of text\nand audio with a MASK token; the model learns by choosing the correct\nmasked-out snippet. Our objective learns faster than alternatives, and performs\nwell at scale: we pretrain on 20 million YouTube videos.\n</p>\n<p>Empirical results show that MERLOT Reserve learns strong multimodal\nrepresentations. When finetuned, it sets state-of-the-art on Visual Commonsense\nReasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%,\nand 1.5% respectively. Ablations show that these tasks benefit from audio\npretraining -- even VCR, a QA task centered around images (without sound).\nMoreover, our objective enables out-of-the-box prediction, revealing strong\nmultimodal commonsense understanding. In a fully zero-shot setting, our model\nobtains competitive results on four video tasks, even outperforming supervised\napproaches on the recently proposed Situated Reasoning (STAR) benchmark.\n</p>\n<p>We analyze why audio enables better vision-language representations,\nsuggesting significant opportunities for future research. We conclude by\ndiscussing ethical and societal implications of multimodal pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collapse by Conditioning: Training Class-conditional GANs with Limited Data. (arXiv:2201.06578v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06578","description":"<p>Class-conditioning offers a direct means to control a Generative Adversarial\nNetwork (GAN) based on a discrete input variable. While necessary in many\napplications, the additional information provided by the class labels could\neven be expected to benefit the training of the GAN itself. On the contrary, we\nobserve that class-conditioning causes mode collapse in limited data settings,\nwhere unconditional learning leads to satisfactory generative ability.\nMotivated by this observation, we propose a training strategy for\nclass-conditional GANs (cGANs) that effectively prevents the observed\nmode-collapse by leveraging unconditional learning. Our training strategy\nstarts with an unconditional GAN and gradually injects the class conditioning\ninto the generator and the objective function. The proposed method for training\ncGANs with limited data results not only in stable training but also in\ngenerating high-quality images, thanks to the early-stage exploitation of the\nshared information across classes. We analyze the observed mode collapse\nproblem in comprehensive experiments on four datasets. Our approach\ndemonstrates outstanding results compared with state-of-the-art methods and\nestablished baselines. The code is available at\nhttps://github.com/mshahbazi72/transitional-cGAN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahbazi_M/0/1/0/all/0/1\">Mohamad Shahbazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepMix: Mobility-aware, Lightweight, and Hybrid 3D Object Detection for Headsets. (arXiv:2201.08812v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08812","description":"<p>Mobile headsets should be capable of understanding 3D physical environments\nto offer a truly immersive experience for augmented/mixed reality (AR/MR).\nHowever, their small form-factor and limited computation resources make it\nextremely challenging to execute in real-time 3D vision algorithms, which are\nknown to be more compute-intensive than their 2D counterparts. In this paper,\nwe propose DeepMix, a mobility-aware, lightweight, and hybrid 3D object\ndetection framework for improving the user experience of AR/MR on mobile\nheadsets. Motivated by our analysis and evaluation of state-of-the-art 3D\nobject detection models, DeepMix intelligently combines edge-assisted 2D object\ndetection and novel, on-device 3D bounding box estimations that leverage depth\ndata captured by headsets. This leads to low end-to-end latency and\nsignificantly boosts detection accuracy in mobile scenarios. A unique feature\nof DeepMix is that it fully exploits the mobility of headsets to fine-tune\ndetection results and boost detection accuracy. To the best of our knowledge,\nDeepMix is the first 3D object detection that achieves 30 FPS (an end-to-end\nlatency much lower than the 100 ms stringent requirement of interactive AR/MR).\nWe implement a prototype of DeepMix on Microsoft HoloLens and evaluate its\nperformance via both extensive controlled experiments and a user study with 30+\nparticipants. DeepMix not only improves detection accuracy by 9.1--37.3% but\nalso reduces end-to-end latency by 2.68--9.15x, compared to the baseline that\nuses existing 3D object detection models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yongjie Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xueyu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Nan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tao Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-NeRF: Point-based Neural Radiance Fields. (arXiv:2201.08845v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08845","description":"<p>Volumetric neural rendering methods like NeRF generate high-quality view\nsynthesis results but are optimized per-scene leading to prohibitive\nreconstruction time. On the other hand, deep multi-view stereo methods can\nquickly reconstruct scene geometry via direct network inference. Point-NeRF\ncombines the advantages of these two approaches by using neural 3D point\nclouds, with associated neural features, to model a radiance field. Point-NeRF\ncan be rendered efficiently by aggregating neural point features near scene\nsurfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can\nbe initialized via direct inference of a pre-trained deep network to produce a\nneural point cloud; this point cloud can be finetuned to surpass the visual\nquality of NeRF with 30X faster training time. Point-NeRF can be combined with\nother 3D reconstruction methods and handles the errors and outliers in such\nmethods via a novel pruning and growing mechanism. The experiments on the DTU,\nthe NeRF Synthetics , the ScanNet and the Tanks and Temples datasets\ndemonstrate Point-NeRF can surpass the existing methods and achieve the\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiangeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zexiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1\">Julien Philip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sai Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhixin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1\">Ulrich Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indicative Image Retrieval: Turning Blackbox Learning into Grey. (arXiv:2201.11898v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11898","description":"<p>Deep learning became the game changer for image retrieval soon after it was\nintroduced. It promotes the feature extraction (by representation learning) as\nthe core of image retrieval, with the relevance/matching evaluation being\ndegenerated into simple similarity metrics. In many applications, we need the\nmatching evidence to be indicated rather than just have the ranked list (e.g.,\nthe locations of the target proteins/cells/lesions in medical images). It is\nlike the matched words need to be highlighted in search engines. However, this\nis not easy to implement without explicit relevance/matching modeling. The deep\nrepresentation learning models are not feasible because of their blackbox\nnature. In this paper, we revisit the importance of relevance/matching modeling\nin deep learning era with an indicative retrieval setting. The study shows that\nit is possible to skip the representation learning and model the matching\nevidence directly. By removing the dependency on the pre-trained models, it has\navoided a lot of related issues (e.g., the domain gap between classification\nand retrieval, the detail-diffusion caused by convolution, and so on). More\nimportantly, the study demonstrates that the matching can be explicitly modeled\nand backtracked later for generating the matching evidence indications. It can\nimprove the explainability of deep inference. Our method obtains a best\nperformance in literature on both Oxford-5k and Paris-6k, and sets a new record\nof 97.77% on Oxford-5k (97.81% on Paris-6k) without extracting any deep\nfeatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xulu Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenqun Yang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoyong Wei</a> (1 and 3) ((1) Sichuan University, (2) Chinese University of Hong Kong, (3) Hong Kong Polytechnic Univeristy)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-Transfer: Learning to Route Transferrable Representations. (arXiv:2202.01011v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01011","description":"<p>Knowledge transfer between heterogeneous source and target networks and tasks\nhas received a lot of attention in recent times as large amounts of quality\nlabeled data can be difficult to obtain in many applications. Existing\napproaches typically constrain the target deep neural network (DNN) feature\nrepresentations to be close to the source DNNs feature representations, which\ncan be limiting. We, in this paper, propose a novel adversarial multi-armed\nbandit approach that automatically learns to route source representations to\nappropriate target representations following which they are combined in\nmeaningful ways to produce accurate target models. We see upwards of 5\\%\naccuracy improvements compared with the state-of-the-art knowledge transfer\nmethods on four benchmark (target) image datasets CUB200, Stanford Dogs, MIT67,\nand Stanford40 where the source dataset is ImageNet. We qualitatively analyze\nthe goodness of our transfer scheme by showing individual examples of the\nimportant features focused on by our target network at different layers\ncompared with the (closest) competitors. We also observe that our improvement\nover other methods is higher for smaller target datasets making it an effective\ntool for small data applications that may benefit from transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murugesan_K/0/1/0/all/0/1\">Keerthiram Murugesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadashivaiah_V/0/1/0/all/0/1\">Vijay Sadashivaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luss_R/0/1/0/all/0/1\">Ronny Luss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhurandhar_A/0/1/0/all/0/1\">Amit Dhurandhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN-generated Faces Detection: A Survey and New Perspectives (2022). (arXiv:2202.07145v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07145","description":"<p>Generative Adversarial Networks (GAN) have led to the generation of very\nrealistic face images, which have been used in fake social media accounts and\nother disinformation matters that can generate profound impacts. Therefore, the\ncorresponding GAN-face detection techniques are under active development that\ncan examine and expose such fake faces. In this work, we aim to provide a\ncomprehensive review of recent progress in GAN-face detection. We focus on\nmethods that can detect face images that are generated or synthesized from GAN\nmodels. We classify the existing detection works into four categories: (1) deep\nlearning-based, (2) physical-based, (3) physiological-based methods, and (4)\nevaluation and comparison against human visual performance. For each category,\nwe summarize the key ideas and connect them with method implementations. We\nalso discuss open problems and suggest future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding. (arXiv:2203.00867v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00867","description":"<p>Image inpainting has made significant advances in recent years. However, it\nis still challenging to recover corrupted images with both vivid textures and\nreasonable structures. Some specific methods only tackle regular textures while\nlosing holistic structures due to the limited receptive fields of convolutional\nneural networks (CNNs). On the other hand, attention-based models can learn\nbetter long-range dependency for the structure recovery, but they are limited\nby the heavy computation for inference with large image sizes. To address these\nissues, we propose to leverage an additional structure restorer to facilitate\nthe image inpainting incrementally. The proposed model restores holistic image\nstructures with a powerful attention-based transformer model in a fixed\nlow-resolution sketch space. Such a grayscale space is easy to be upsampled to\nlarger scales to convey correct structural information. Our structure restorer\ncan be integrated with other pretrained inpainting models efficiently with the\nzero-initialized residual addition. Furthermore, a masking positional encoding\nstrategy is utilized to improve the performance with large irregular masks.\nExtensive experiments on various datasets validate the efficacy of our model\ncompared with other competitors. Our codes are released in\nhttps://github.com/DQiaole/ZITS_inpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiaole Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chenjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased Batch Normalization via Gaussian Process for Generalizable Person Re-Identification. (arXiv:2203.01723v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01723","description":"<p>Generalizable person re-identification aims to learn a model with only\nseveral labeled source domains that can perform well on unseen domains. Without\naccess to the unseen domain, the feature statistics of the batch normalization\n(BN) layer learned from a limited number of source domains is doubtlessly\nbiased for unseen domain. This would mislead the feature representation\nlearning for unseen domain and deteriorate the generalizaiton ability of the\nmodel. In this paper, we propose a novel Debiased Batch Normalization via\nGaussian Process approach (GDNorm) for generalizable person re-identification,\nwhich models the feature statistic estimation from BN layers as a dynamically\nself-refining Gaussian process to alleviate the bias to unseen domain for\nimproving the generalization. Specifically, we establish a lightweight model\nwith multiple set of domain-specific BN layers to capture the discriminability\nof individual source domain, and learn the corresponding parameters of the\ndomain-specific BN layers. These parameters of different source domains are\nemployed to deduce a Gaussian process. We randomly sample several paths from\nthis Gaussian process served as the BN estimations of potential new domains\noutside of existing source domains, which can further optimize these learned\nparameters from source domains, and estimate more accurate Gaussian process by\nthem in return, tending to real data distribution. Even without a large number\nof source domains, GDNorm can still provide debiased BN estimation by using the\nmean path of the Gaussian process, while maintaining low computational cost\nduring testing. Extensive experiments demonstrate that our GDNorm effectively\nimproves the generalization ability of the model on unseen domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhipeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modality-Adaptive Mixup and Invariant Decomposition for RGB-Infrared Person Re-Identification. (arXiv:2203.01735v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01735","description":"<p>RGB-infrared person re-identification is an emerging cross-modality\nre-identification task, which is very challenging due to significant modality\ndiscrepancy between RGB and infrared images. In this work, we propose a novel\nmodality-adaptive mixup and invariant decomposition (MID) approach for\nRGB-infrared person re-identification towards learning modality-invariant and\ndiscriminative representations. MID designs a modality-adaptive mixup scheme to\ngenerate suitable mixed modality images between RGB and infrared images for\nmitigating the inherent modality discrepancy at the pixel-level. It formulates\nmodality mixup procedure as Markov decision process, where an actor-critic\nagent learns dynamical and local linear interpolation policy between different\nregions of cross-modality images under a deep reinforcement learning framework.\nSuch policy guarantees modality-invariance in a more continuous latent space\nand avoids manifold intrusion by the corrupted mixed modality samples.\nMoreover, to further counter modality discrepancy and enforce invariant visual\nsemantics at the feature-level, MID employs modality-adaptive convolution\ndecomposition to disassemble a regular convolution layer into modality-specific\nbasis layers and a modality-shared coefficient layer. Extensive experimental\nresults on two challenging benchmarks demonstrate superior performance of MID\nover state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhipeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Playable Environments: Video Manipulation in Space and Time. (arXiv:2203.01914v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01914","description":"<p>We present Playable Environments - a new representation for interactive video\ngeneration and manipulation in space and time. With a single image at inference\ntime, our novel framework allows the user to move objects in 3D while\ngenerating a video by providing a sequence of desired actions. The actions are\nlearnt in an unsupervised manner. The camera can be controlled to get the\ndesired viewpoint. Our method builds an environment state for each frame, which\ncan be manipulated by our proposed action module and decoded back to the image\nspace with volumetric rendering. To support diverse appearances of objects, we\nextend neural radiance fields with style-based modulation. Our method trains on\na collection of various monocular videos requiring only the estimated camera\nparameters and 2D object locations. To set a challenging benchmark, we\nintroduce two large scale video datasets with significant camera movements. As\nevidenced by our experiments, playable environments enable several creative\napplications not attainable by prior video synthesis works, including playable\n3D video generation, stylization and manipulation. Further details, code and\nexamples are available at\nhttps://willi-menapace.github.io/playable-environments-website\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Menapace_W/0/1/0/all/0/1\">Willi Menapace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siarohin_A/0/1/0/all/0/1\">Aliaksandr Siarohin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FS-COCO: Towards Understanding of Freehand Sketches of Common Objects in Context. (arXiv:2203.02113v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02113","description":"<p>We advance sketch research to scenes with the first dataset of freehand scene\nsketches, FS-COCO. With practical applications in mind, we collect sketches\nthat convey well scene content but can be sketched within a few minutes by a\nperson with any sketching skills. Our dataset comprises 10,000 freehand scene\nvector sketches with per point space-time information by 100 non-expert\nindividuals, offering both object- and scene-level abstraction. Each sketch is\naugmented with its text description. Using our dataset, we study for the first\ntime the problem of the fine-grained image retrieval from freehand scene\nsketches and sketch captions. We draw insights on (i) Scene salience encoded in\nsketches with strokes temporal order; (ii) The retrieval performance accuracy\nfrom scene sketches against image captions; (iii) Complementarity of\ninformation in sketches and image captions, as well as the potential benefit of\ncombining the two modalities. In addition, we propose new solutions enabled by\nour dataset (i) We adopt meta-learning to show how the retrieval model can be\nfine-tuned to a new user style given just a small set of sketches, (ii) We\nextend a popular vector sketch LSTM-based encoder to handle sketches with\nlarger complexity than was supported by previous work. Namely, we propose a\nhierarchical sketch decoder, which we leverage at a sketch-specific \"pretext\"\ntask. Our dataset enables for the first time research on freehand scene sketch\nunderstanding and its practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Nath Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1\">Aneeshan Sain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gryaditskaya_Y/0/1/0/all/0/1\">Yulia Gryaditskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-temporal Gait Feature with Adaptive Distance Alignment. (arXiv:2203.03376v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03376","description":"<p>Gait recognition is an important recognition technology, because it is not\neasy to camouflage and does not need cooperation to recognize subjects.\nHowever, there are still serious challenges in gait recognition, that is,\npeople with similar walking posture are often recognized incorrectly. In this\npaper, We try to increase the discrimination of extraced gait features of\ndifferent subjects to increase the recognition efficiency of subjects with\nsimilar walking posture. It includes the optimization of network structure and\nthe refinement of extracted gait features. So our method is proposed, it\nconsists of Spatio-temporal Feature Extraction (SFE) and Adaptive Distance\nAlignment (ADA), which SFE uses Temporal Feature Fusion (TFF) and Fine-grained\nFeature Extraction (FFE) to effectively extract the spatio-temporal features\nfrom raw silhouettes, ADA uses a large number of unlabeled gait data in real\nlife as a benchmark to refine the extracted spatio-temporal features to make\nthem have low inter-class similarity and high intra-class similarity. Extensive\nexperiments on mini-OUMVLP and CASIA-B have proved that we have a good result\nthan some state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Rectangling for Image Stitching: A Learning Baseline. (arXiv:2203.03831v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03831","description":"<p>Stitched images provide a wide field-of-view (FoV) but suffer from unpleasant\nirregular boundaries. To deal with this problem, existing image rectangling\nmethods devote to searching an initial mesh and optimizing a target mesh to\nform the mesh deformation in two stages. Then rectangular images can be\ngenerated by warping stitched images. However, these solutions only work for\nimages with rich linear structures, leading to noticeable distortions for\nportraits and landscapes with non-linear objects. In this paper, we address\nthese issues by proposing the first deep learning solution to image\nrectangling. Concretely, we predefine a rigid target mesh and only estimate an\ninitial mesh to form the mesh deformation, contributing to a compact one-stage\nsolution. The initial mesh is predicted using a fully convolutional network\nwith a residual progressive regression strategy. To obtain results with high\ncontent fidelity, a comprehensive objective function is proposed to\nsimultaneously encourage the boundary rectangular, mesh shape-preserving, and\ncontent perceptually natural. Besides, we build the first image stitching\nrectangling dataset with a large diversity in irregular boundaries and scenes.\nExperiments demonstrate our superiority over traditional methods both\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-DIAE: Degradation Invariant Autoencoders for Text Recognition and Document Enhancement. (arXiv:2203.04814v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04814","description":"<p>In this work, we propose Text-Degradation Invariant Auto Encoder (Text-DIAE)\naimed to solve two tasks, text recognition (handwritten or scene-text) and\ndocument image enhancement. We define three pretext tasks as learning\nobjectives to be optimized during pre-training without the usage of labelled\ndata. Each of the pre-text objectives is specifically tailored for the final\ndownstream tasks. We conduct several ablation experiments that show the\nimportance of each degradation for a specific domain. Exhaustive\nexperimentation shows that our method does not have limitations of previous\nstate-of-the-art based on contrastive losses while at the same time requiring\nessentially fewer data samples to converge. Finally, we demonstrate that our\nmethod surpasses the state-of-the-art significantly in existing supervised and\nself-supervised settings in handwritten and scene text recognition and document\nimage enhancement. Our code and trained models will be made publicly available\nat~\\url{ <a href=\"http://Upon_Acceptance\">this http URL</a>}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1\">Mohamed Ali Souibgui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sanket Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mafla_A/0/1/0/all/0/1\">Andres Mafla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biten_A/0/1/0/all/0/1\">Ali Furkan Biten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1\">Alicia Forn&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kessentini_Y/0/1/0/all/0/1\">Yousri Kessentini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1\">Josep Llad&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Lluis Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning. (arXiv:2203.06359v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06359","description":"<p>Non-exemplar class-incremental learning is to recognize both the old and new\nclasses when old class samples cannot be saved. It is a challenging task since\nrepresentation optimization and feature retention can only be achieved under\nsupervision from new classes. To address this problem, we propose a novel\nself-sustaining representation expansion scheme. Our scheme consists of a\nstructure reorganization strategy that fuses main-branch expansion and\nside-branch updating to maintain the old features, and a main-branch\ndistillation scheme to transfer the invariant knowledge. Furthermore, a\nprototype selection mechanism is proposed to enhance the discrimination between\nthe old and new classes by selectively incorporating new samples into the\ndistillation process. Extensive experiments on three benchmarks demonstrate\nsignificant incremental performance, outperforming the state-of-the-art methods\nby a margin of 3%, 3% and 6%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors. (arXiv:2203.06691v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06691","description":"<p>The main question this work aims at answering is: can morphing attack\ndetection (MAD) solutions be successfully developed based on synthetic data?.\nTowards that, this work introduces the first synthetic-based MAD development\ndataset, namely the Synthetic Morphing Attack Detection Development dataset\n(SMDD). This dataset is utilized successfully to train three MAD backbones\nwhere it proved to lead to high MAD performance, even on completely unknown\nattack types. Additionally, an essential aspect of this work is the detailed\nlegal analyses of the challenges of using and sharing real biometric data,\nrendering our proposed SMDD dataset extremely essential. The SMDD dataset,\nconsisting of 30,000 attack and 50,000 bona fide samples, is made publicly\navailable for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1\">C&#xe9;sar Augusto Fontanillo L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_N/0/1/0/all/0/1\">No&#xe9;mie Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh Vu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Two-Block RNN-based Trajectory Prediction from Incomplete Trajectory. (arXiv:2203.07098v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07098","description":"<p>Trajectory prediction has gained great attention and significant progress has\nbeen made in recent years. However, most works rely on a key assumption that\neach video is successfully preprocessed by detection and tracking algorithms\nand the complete observed trajectory is always available. However, in complex\nreal-world environments, we often encounter miss-detection of target agents\n(e.g., pedestrian, vehicles) caused by the bad image conditions, such as the\nocclusion by other agents. In this paper, we address the problem of trajectory\nprediction from incomplete observed trajectory due to miss-detection, where the\nobserved trajectory includes several missing data points. We introduce a\ntwo-block RNN model that approximates the inference steps of the Bayesian\nfiltering framework and seeks the optimal estimation of the hidden state when\nmiss-detection occurs. The model uses two RNNs depending on the detection\nresult. One RNN approximates the inference step of the Bayesian filter with the\nnew measurement when the detection succeeds, while the other does the\napproximation when the detection fails. Our experiments show that the proposed\nmodel improves the prediction accuracy compared to the three baseline\nimputation methods on publicly available datasets: ETH and UCY ($9\\%$ and $7\\%$\nimprovement on the ADE and FDE metrics). We also show that our proposed method\ncan achieve better prediction compared to the baselines when there is no\nmiss-detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fujii_R/0/1/0/all/0/1\">Ryo Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vongkulbhisal_J/0/1/0/all/0/1\">Jayakorn Vongkulbhisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1\">Ryo Hachiuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1\">Hideo Saito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rich CNN-Transformer Feature Aggregation Networks for Super-Resolution. (arXiv:2203.07682v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07682","description":"<p>Recent vision transformers along with self-attention have achieved promising\nresults on various computer vision tasks. In particular, a pure\ntransformer-based image restoration architecture surpasses the existing\nCNN-based methods using multi-task pre-training with a large number of\ntrainable parameters. In this paper, we introduce an effective hybrid\narchitecture for super-resolution (SR) tasks, which leverages local features\nfrom CNNs and long-range dependencies captured by transformers to further\nimprove the SR results. Specifically, our architecture comprises of transformer\nand convolution branches, and we substantially elevate the performance by\nmutually fusing two branches to complement each representation. Furthermore, we\npropose a cross-scale token attention module, which allows the transformer to\nefficiently exploit the informative relationships among tokens across different\nscales. Our proposed method achieves state-of-the-art SR results on numerous\nbenchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jinsu Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae Hyun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit field supervision for robust non-rigid shape matching. (arXiv:2203.07694v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07694","description":"<p>Establishing a correspondence between two non-rigidly deforming shapes is one\nof the most fundamental problems in visual computing. Existing methods often\nshow weak resilience when presented with challenges innate to real-world data\nsuch as noise, outliers, self-occlusion etc. On the other hand, auto-decoders\nhave demonstrated strong expressive power in learning geometrically meaningful\nlatent embeddings. However, their use in shape analysis and especially in\nnon-rigid shape correspondence has been limited. In this paper, we introduce an\napproach based on auto-decoder framework, that learns a continuous shape-wise\ndeformation field over a fixed template. By supervising the deformation field\nfor points on-surface and regularising for points off-surface through a novel\nSigned Distance Regularisation (SDR), we learn an alignment between the\ntemplate and shape volumes. Unlike classical correspondence techniques, our\nmethod is remarkably robust in the presence of strong artefacts and can be\ngeneralised to arbitrary shape categories. Trained on clean water-tight meshes,\nwithout any data-augmentation, we demonstrate compelling performance on\ncompromised data and real-world scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundararaman_R/0/1/0/all/0/1\">Ramana Sundararaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_G/0/1/0/all/0/1\">Gautam Pai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distribution-Aware Single-Stage Models for Multi-Person 3D Pose Estimation. (arXiv:2203.07697v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07697","description":"<p>In this paper, we present a novel Distribution-Aware Single-stage (DAS) model\nfor tackling the challenging multi-person 3D pose estimation problem. Different\nfrom existing top-down and bottom-up methods, the proposed DAS model\nsimultaneously localizes person positions and their corresponding body joints\nin the 3D camera space in a one-pass manner. This leads to a simplified\npipeline with enhanced efficiency. In addition, DAS learns the true\ndistribution of body joints for the regression of their positions, rather than\nmaking a simple Laplacian or Gaussian assumption as previous works. This\nprovides valuable priors for model prediction and thus boosts the\nregression-based scheme to achieve competitive performance with volumetric-base\nones. Moreover, DAS exploits a recursive update strategy for progressively\napproaching to regression target, alleviating the optimization difficulty and\nfurther lifting the regression performance. DAS is implemented with a fully\nConvolutional Neural Network and end-to-end learnable. Comprehensive\nexperiments on benchmarks CMU Panoptic and MuPoTS-3D demonstrate the superior\nefficiency of the proposed DAS model, specifically 1.5x speedup over previous\nbest model, and its stat-of-the-art accuracy for multi-person 3D pose\nestimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zitian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1\">Xuecheng Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaochao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Detection as Probabilistic Set Prediction. (arXiv:2203.07980v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07980","description":"<p>Accurate uncertainty estimates are essential for deploying deep object\ndetectors in safety-critical systems. The development and evaluation of\nprobabilistic object detectors have been hindered by shortcomings in existing\nperformance measures, which tend to involve arbitrary thresholds or limit the\ndetector's choice of distributions. In this work, we propose to view object\ndetection as a set prediction task where detectors predict the distribution\nover the set of objects. Using the negative log-likelihood for random finite\nsets, we present a proper scoring rule for evaluating and training\nprobabilistic object detectors. The proposed method can be applied to existing\nprobabilistic detectors, is free from thresholds, and enables fair comparison\nbetween architectures. Three different types of detectors are evaluated on the\nCOCO dataset. Our results indicate that the training of existing detectors is\noptimized toward non-probabilistic metrics. We hope to encourage the\ndevelopment of new object detectors that can accurately estimate their own\nuncertainty. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hess_G/0/1/0/all/0/1\">Georg Hess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_C/0/1/0/all/0/1\">Christoffer Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1\">Lennart Svensson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Hyperbolic Embeddings in 2D Object Detection. (arXiv:2203.08049v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08049","description":"<p>Object detection, for the most part, has been formulated in the euclidean\nspace, where euclidean or spherical geodesic distances measure the similarity\nof an image region to an object class prototype. In this work, we study whether\na hyperbolic geometry better matches the underlying structure of the object\nclassification space. We incorporate a hyperbolic classifier in two-stage,\nkeypoint-based, and transformer-based object detection architectures and\nevaluate them on large-scale, long-tailed, and zero-shot object detection\nbenchmarks. In our extensive experimental evaluations, we observe categorical\nclass hierarchies emerging in the structure of the classification space,\nresulting in lower classification errors and boosting the overall object\ndetection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Christopher Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1\">Alexander Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CryoAI: Amortized Inference of Poses for Ab Initio Reconstruction of 3D Molecular Volumes from Real Cryo-EM Images. (arXiv:2203.08138v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08138","description":"<p>Cryo-electron microscopy (cryo-EM) has become a tool of fundamental\nimportance in structural biology, helping us understand the basic building\nblocks of life. The algorithmic challenge of cryo-EM is to jointly estimate the\nunknown 3D poses and the 3D electron scattering potential of a biomolecule from\nmillions of extremely noisy 2D images. Existing reconstruction algorithms,\nhowever, cannot easily keep pace with the rapidly growing size of cryo-EM\ndatasets due to their high computational and memory cost. We introduce cryoAI,\nan ab initio reconstruction algorithm for homogeneous conformations that uses\ndirect gradient-based optimization of particle poses and the electron\nscattering potential from single-particle cryo-EM data. CryoAI combines a\nlearned encoder that predicts the poses of each particle image with a\nphysics-based decoder to aggregate each particle image into an implicit\nrepresentation of the scattering potential volume. This volume is stored in the\nFourier domain for computational efficiency and leverages a modern coordinate\nnetwork architecture for memory efficiency. Combined with a symmetrized loss\nfunction, this framework achieves results of a quality on par with\nstate-of-the-art cryo-EM solvers for both simulated and experimental data, one\norder of magnitude faster for large datasets and with significantly lower\nmemory requirements than existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_A/0/1/0/all/0/1\">Axel Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poitevin_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Poitevin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_J/0/1/0/all/0/1\">Julien Martel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nashed_Y/0/1/0/all/0/1\">Youssef Nashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peck_A/0/1/0/all/0/1\">Ariana Peck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miolane_N/0/1/0/all/0/1\">Nina Miolane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_D/0/1/0/all/0/1\">Daniel Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunne_M/0/1/0/all/0/1\">Mike Dunne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}