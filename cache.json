{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ALLWAS: Active Learning on Language models in WASserstein space. (arXiv:2109.01691v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01691","description":"<p>Active learning has emerged as a standard paradigm in areas with scarcity of\nlabeled training data, such as in the medical domain. Language models have\nemerged as the prevalent choice of several natural language tasks due to the\nperformance boost offered by these models. However, in several domains, such as\nmedicine, the scarcity of labeled training data is a common issue. Also, these\nmodels may not work well in cases where class imbalance is prevalent. Active\nlearning may prove helpful in these cases to boost the performance with a\nlimited label budget. To this end, we propose a novel method using sampling\ntechniques based on submodular optimization and optimal transport for active\nlearning in language models, dubbed ALLWAS. We construct a sampling strategy\nbased on submodular optimization of the designed objective in the gradient\ndomain. Furthermore, to enable learning from few samples, we propose a novel\nstrategy for sampling from the Wasserstein barycenters. Our empirical\nevaluations on standard benchmark datasets for text classification show that\nour methods perform significantly better (&gt;20% relative increase in some cases)\nthan existing approaches for active learning on language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastos_A/0/1/0/all/0/1\">Anson Bastos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_M/0/1/0/all/0/1\">Manohar Kaul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models. (arXiv:2109.01754v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01754","description":"<p>Large-scale conversational assistants like Alexa, Siri, Cortana and Google\nAssistant process every utterance using multiple models for domain, intent and\nnamed entity recognition. Given the decoupled nature of model development and\nlarge traffic volumes, it is extremely difficult to identify utterances\nprocessed erroneously by such systems. We address this challenge to detect\ndomain classification errors using offline Transformer models. We combine\nutterance encodings from a RoBERTa model with the Nbest hypothesis produced by\nthe production system. We then fine-tune end-to-end in a multitask setting\nusing a small dataset of humanannotated utterances with domain classification\nerrors. We tested our approach for detecting misclassifications from one domain\nthat accounts for &lt;0.5% of the traffic in a large-scale conversational AI\nsystem. Our approach achieves an F1 score of 30% outperforming a bi- LSTM\nbaseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this\nfurther by 2.2% to 32.2% by ensembling multiple models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fofadiya_D/0/1/0/all/0/1\">Darshan Fofadiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_P/0/1/0/all/0/1\">Prathap Ramachandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Cross-Domain Named Entity Recognition. (arXiv:2109.01758v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01758","description":"<p>Current work in named entity recognition (NER) shows that data augmentation\ntechniques can produce more robust models. However, most existing techniques\nfocus on augmenting in-domain data in low-resource scenarios where annotated\ndata is quite limited. In contrast, we study cross-domain data augmentation for\nthe NER task. We investigate the possibility of leveraging data from\nhigh-resource domains by projecting it into the low-resource domains.\nSpecifically, we propose a novel neural architecture to transform the data\nrepresentation from a high-resource to a low-resource domain by learning the\npatterns (e.g. style, noise, abbreviations, etc.) in the text that\ndifferentiate them and a shared feature space where both domains are aligned.\nWe experiment with diverse datasets and show that transforming the data to the\nlow-resource domain representation achieves significant improvements over only\nusing data from high-resource domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_G/0/1/0/all/0/1\">Gustavo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_L/0/1/0/all/0/1\">Leonardo Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Efficient and Effective Similarity Search and Recommendation. (arXiv:2109.01815v1 [cs.IR])","link":"http://arxiv.org/abs/2109.01815","description":"<p>How data is represented and operationalized is critical for building\ncomputational solutions that are both effective and efficient. A common\napproach is to represent data objects as binary vectors, denoted \\textit{hash\ncodes}, which require little storage and enable efficient similarity search\nthrough direct indexing into a hash table or through similarity computations in\nan appropriate space. Due to the limited expressibility of hash codes, compared\nto real-valued representations, a core open challenge is how to generate hash\ncodes that well capture semantic content or latent properties using a small\nnumber of bits, while ensuring that the hash codes are distributed in a way\nthat does not reduce their search efficiency. State of the art methods use\nrepresentation learning for generating such hash codes, focusing on neural\nautoencoder architectures where semantics are encoded into the hash codes by\nlearning to reconstruct the original inputs of the hash codes. This thesis\naddresses the above challenge and makes a number of contributions to\nrepresentation learning that (i) improve effectiveness of hash codes through\nmore expressive representations and a more effective similarity measure than\nthe current state of the art, namely the Hamming distance, and (ii) improve\nefficiency of hash codes by learning representations that are especially suited\nto the choice of search method. The contributions are empirically validated on\nseveral tasks related to similarity search and recommendation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_C/0/1/0/all/0/1\">Casper Hansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frustratingly Simple Pretraining Alternatives to Masked Language Modeling. (arXiv:2109.01819v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01819","description":"<p>Masked language modeling (MLM), a self-supervised pretraining objective, is\nwidely used in natural language processing for learning text representations.\nMLM trains a model to predict a random sample of input tokens that have been\nreplaced by a [MASK] placeholder in a multi-class setting over the entire\nvocabulary. When pretraining, it is common to use alongside MLM other auxiliary\nobjectives on the token or sequence level to improve downstream performance\n(e.g. next sentence prediction). However, no previous work so far has attempted\nin examining whether other simpler linguistically intuitive or not objectives\ncan be used standalone as main pretraining objectives. In this paper, we\nexplore five simple pretraining objectives based on token-level classification\ntasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our\nproposed methods achieve comparable or better performance to MLM using a\nBERT-BASE architecture. We further validate our methods using smaller models,\nshowing that pretraining a model with 41% of the BERT-BASE's parameters,\nBERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1\">Atsuki Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysostomou_G/0/1/0/all/0/1\">George Chrysostomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1\">Katerina Margatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Expressive Communication with Internet Memes: A New Multimodal Conversation Dataset and Benchmark. (arXiv:2109.01839v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01839","description":"<p>As a kind of new expression elements, Internet memes are popular and\nextensively used in online chatting scenarios since they manage to make\ndialogues vivid, moving, and interesting. However, most current dialogue\nresearches focus on text-only dialogue tasks. In this paper, we propose a new\ntask named as \\textbf{M}eme incorporated \\textbf{O}pen-domain \\textbf{D}ialogue\n(MOD). Compared to previous dialogue tasks, MOD is much more challenging since\nit requires the model to understand the multimodal elements as well as the\nemotions behind them. To facilitate the MOD research, we construct a\nlarge-scale open-domain multimodal dialogue dataset incorporating abundant\nInternet memes into utterances. The dataset consists of $\\sim$45K Chinese\nconversations with $\\sim$606K utterances. Each conversation contains about $13$\nutterances with about $4$ Internet memes on average and each utterance equipped\nwith an Internet meme is annotated with the corresponding emotion. In addition,\nwe present a simple and effective method, which utilizes a unified generation\nnetwork to solve the MOD task. Experimental results demonstrate that our method\ntrained on the proposed corpus is able to achieve expressive communication\nincluding texts and memes. The corpus and models have been publicly available\nat https://github.com/lizekang/DSTC10-MOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1\">Zhengcong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Contrastive Learning for Multimodal Unreliable News Detection in COVID-19 Pandemic. (arXiv:2109.01850v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01850","description":"<p>As the digital news industry becomes the main channel of information\ndissemination, the adverse impact of fake news is explosively magnified. The\ncredibility of a news report should not be considered in isolation. Rather,\npreviously published news articles on the similar event could be used to assess\nthe credibility of a news report. Inspired by this, we propose a BERT-based\nmultimodal unreliable news detection framework, which captures both textual and\nvisual information from unreliable articles utilising the contrastive learning\nstrategy. The contrastive learner interacts with the unreliable news classifier\nto push similar credible news (or similar unreliable news) closer while moving\nnews articles with similar content but opposite credibility labels away from\neach other in the multimodal embedding space. Experimental results on a\nCOVID-19 related dataset, ReCOVery, show that our model outperforms a number of\ncompetitive baseline in unreliable news detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing Paraphrase Away from Original Sentence: A Multi-Round Paraphrase Generation Approach. (arXiv:2109.01862v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01862","description":"<p>In recent years, neural paraphrase generation based on Seq2Seq has achieved\nsuperior performance, however, the generated paraphrase still has the problem\nof lack of diversity. In this paper, we focus on improving the diversity\nbetween the generated paraphrase and the original sentence, i.e., making\ngenerated paraphrase different from the original sentence as much as possible.\nWe propose BTmPG (Back-Translation guided multi-round Paraphrase Generation),\nwhich leverages multi-round paraphrase generation to improve diversity and\nemploys back-translation to preserve semantic information. We evaluate BTmPG on\ntwo benchmark datasets. Both automatic and human evaluation show BTmPG can\nimprove the diversity of paraphrase while preserving the semantics of the\noriginal sentence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncovering the Limits of Text-based Emotion Detection. (arXiv:2109.01900v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01900","description":"<p>Identifying emotions from text is crucial for a variety of real world tasks.\nWe consider the two largest now-available corpora for emotion classification:\nGoEmotions, with 58k messages labelled by readers, and Vent, with 33M\nwriter-labelled messages. We design a benchmark and evaluate several feature\nspaces and learning algorithms, including two simple yet novel models on top of\nBERT that outperform previous strong baselines on GoEmotions. Through an\nexperiment with human participants, we also analyze the differences between how\nwriters express emotions and how readers perceive them. Our results suggest\nthat emotions expressed by writers are harder to identify than emotions that\nreaders perceive. We share a public web interface for researchers to explore\nour models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Gonzalez_N/0/1/0/all/0/1\">Nurudin Alvarez-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaltenbrunner_A/0/1/0/all/0/1\">Andreas Kaltenbrunner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1\">Vicen&#xe7; G&#xf3;mez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural Network-Based Linguistic Similarity Measure for Entrainment in Conversations. (arXiv:2109.01924v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01924","description":"<p>Linguistic entrainment is a phenomenon where people tend to mimic each other\nin conversation. The core instrument to quantify entrainment is a linguistic\nsimilarity measure between conversational partners. Most of the current\nsimilarity measures are based on bag-of-words approaches that rely on\nlinguistic markers, ignoring the overall language structure and dialogue\ncontext. To address this issue, we propose to use a neural network model to\nperform the similarity measure for entrainment. Our model is context-aware, and\nit further leverages a novel component to learn the shared high-level\nlinguistic features across dialogues. We first investigate the effectiveness of\nour novel component. Then we use the model to perform similarity measure in a\ncorpus-based entrainment analysis. We observe promising results for both\nevaluation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingzhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Relative Spatial Reasoning for Visual Question Answering. (arXiv:2109.01934v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01934","description":"<p>Vision-and-language (V\\&amp;L) reasoning necessitates perception of visual\nconcepts such as objects and actions, understanding semantics and language\ngrounding, and reasoning about the interplay between the two modalities. One\ncrucial aspect of visual reasoning is spatial understanding, which involves\nunderstanding relative locations of objects, i.e.\\ implicitly learning the\ngeometry of the scene. In this work, we evaluate the faithfulness of V\\&amp;L\nmodels to such geometric understanding, by formulating the prediction of\npair-wise relative locations of objects as a classification as well as a\nregression task. Our findings suggest that state-of-the-art transformer-based\nV\\&amp;L models lack sufficient abilities to excel at this task. Motivated by this,\nwe design two objectives as proxies for 3D spatial reasoning (SR) -- object\ncentroid estimation, and relative position estimation, and train V\\&amp;L with weak\nsupervision from off-the-shelf depth estimators. This leads to considerable\nimprovements in accuracy for the \"GQA\" visual question answering challenge (in\nfully supervised, few-shot, and O.O.D settings) as well as improvements in\nrelative spatial reasoning. Code and data will be released\n\\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting: Phenotype Annotation Use Case. (arXiv:2109.01935v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01935","description":"<p>Contextualised word embeddings is a powerful tool to detect contextual\nsynonyms. However, most of the current state-of-the-art (SOTA) deep learning\nconcept extraction methods remain supervised and underexploit the potential of\nthe context. In this paper, we propose a self-supervised pre-training approach\nwhich is able to detect contextual synonyms of concepts being training on the\ndata created by shallow matching. We apply our methodology in the sparse\nmulti-class setting (over 15,000 concepts) to extract phenotype information\nfrom electronic health records. We further investigate data augmentation\ntechniques to address the problem of the class sparsity. Our approach achieves\na new SOTA for the unsupervised phenotype concept annotation on clinical text\non F1 and Recall outperforming the previous SOTA with a gain of up to 4.5 and\n4.0 absolute points, respectively. After fine-tuning with as little as 20\\% of\nthe labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic\nevaluation on three ICU benchmarks also shows the benefit of using the\nphenotypes annotated by our model as features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolanos_L/0/1/0/all/0/1\">Luis Bolanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanwar_A/0/1/0/all/0/1\">Ashwani Tanwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freire_G/0/1/0/all/0/1\">Guilherme Freire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1\">Julia Ive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vibhor Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the ability of monolingual models to learn language-agnostic representations. (arXiv:2109.01942v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01942","description":"<p>Pretrained multilingual models have become a de facto default approach for\nzero-shot cross-lingual transfer. Previous work has shown that these models are\nable to achieve cross-lingual representations when pretrained on two or more\nlanguages with shared parameters. In this work, we provide evidence that a\nmodel can achieve language-agnostic representations even when pretrained on a\nsingle language. That is, we find that monolingual models pretrained and\nfinetuned on different languages achieve competitive performance compared to\nthe ones that use the same target language. Surprisingly, the models show a\nsimilar performance on a same task regardless of the pretraining language. For\nexample, models pretrained on distant languages such as German and Portuguese\nperform similarly on English tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souza_L/0/1/0/all/0/1\">Leandro Rodrigues de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment. (arXiv:2109.01949v1 [cs.LG])","link":"http://arxiv.org/abs/2109.01949","description":"<p>Self-supervised learning provides an opportunity to explore unlabeled chest\nX-rays and their associated free-text reports accumulated in clinical routine\nwithout manual supervision. This paper proposes a Joint Image Text\nRepresentation Learning Network (JoImTeRNet) for pre-training on chest X-ray\nimages and their radiology reports. The model was pre-trained on both the\nglobal image-sentence level and the local image region-word level for\nvisual-textual matching. Both are bidirectionally constrained on Cross-Entropy\nbased and ranking-based Triplet Matching Losses. The region-word matching is\ncalculated using the attention mechanism without direct supervision about their\nmapping. The pre-trained multi-modal representation learning paves the way for\ndownstream tasks concerning image and/or text encoding. We demonstrate the\nrepresentation learning quality by cross-modality retrievals and multi-label\nclassifications on two datasets: OpenI-IU and MIMIC-CXR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1\">Mohammad Abuzar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1\">Dana Moukheiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur Srihari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01951","description":"<p>The task of learning from only a few examples (called a few-shot setting) is\nof key importance and relevance to a real-world setting. For question answering\n(QA), the current state-of-the-art pre-trained models typically need\nfine-tuning on tens of thousands of examples to obtain good results. Their\nperformance degrades significantly in a few-shot setting (&lt; 100 examples). To\naddress this, we propose a simple fine-tuning framework that leverages\npre-trained text-to-text models and is directly aligned with their pre-training\nframework. Specifically, we construct the input as a concatenation of the\nquestion, a mask token representing the answer span and a context. Given this\ninput, the model is fine-tuned using the same objective as that of its\npre-training objective. Through experimental studies on various few-shot\nconfigurations, we show that this formulation leads to significant gains on\nmultiple QA benchmarks (an absolute gain of 34.2 F1 points on average when\nthere are only 16 training examples). The gains extend further when used with\nlarger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)\nand translate well to a multilingual setting . On the multilingual TydiQA\nbenchmark, our model outperforms the XLM-Roberta-large by an absolute margin of\nupto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;= 64\ntraining examples). We conduct detailed ablation studies to analyze factors\ncontributing to these gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SideControl: Controlled Open-domain Dialogue Generation via Additive Side Networks. (arXiv:2109.01958v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01958","description":"<p>Transformer-based pre-trained language models boost the performance of\nopen-domain dialogue systems. Prior works leverage Transformer-based\npre-trained language models to generate texts with desired attributes in two\ngeneral approaches: (1) gradient-based methods: updating all latent\nrepresentations of pre-trained models with gradients from attribute models; (2)\nweighted-decoding methods: re-ranking beam candidates from pre-trained models\nwith attribute functions. However, gradient-based methods lead to high\ncomputation cost and can easily get overfitted on small training sets, while\nweighted-decoding methods are inherently constrained by the low-variance\nhigh-bias pre-trained model. In this work, we propose a novel approach to\ncontrol the generation of Transformer-based pre-trained language models: the\nSideControl framework, which leverages a novel control attributes loss to\nincorporate useful control signals, and is shown to perform well with very\nlimited training samples. We evaluate our proposed method on two benchmark\nopen-domain dialogue datasets, and results show that the SideControl framework\nhas better controllability, higher generation quality and better\nsample-efficiency than existing gradient-based and weighted-decoding baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Evaluation for Explainable AI. (arXiv:2109.01962v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01962","description":"<p>While recent years have witnessed the emergence of various explainable\nmethods in machine learning, to what degree the explanations really represent\nthe reasoning process behind the model prediction -- namely, the faithfulness\nof explanation -- is still an open problem. One commonly used way to measure\nfaithfulness is \\textit{erasure-based} criteria. Though conceptually simple,\nerasure-based criterion could inevitably introduce biases and artifacts. We\npropose a new methodology to evaluate the faithfulness of explanations from the\n\\textit{counterfactual reasoning} perspective: the model should produce\nsubstantially different outputs for the original input and its corresponding\ncounterfactual edited on a faithful feature. Specially, we introduce two\nalgorithms to find the proper counterfactuals in both discrete and continuous\nscenarios and then use the acquired counterfactuals to measure faithfulness.\nEmpirical results on several datasets show that compared with existing metrics,\nour proposed counterfactual evaluation method can achieve top correlation with\nthe ground truth under diffe\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Juntao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Hierarchical Structures with Differentiable Nondeterministic Stacks. (arXiv:2109.01982v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01982","description":"<p>Learning hierarchical structures in sequential data -- from simple\nalgorithmic patterns to natural language -- in a reliable, generalizable way\nremains a challenging problem for neural language models. Past work has shown\nthat recurrent neural networks (RNNs) struggle to generalize on held-out\nalgorithmic or syntactic patterns without supervision or some inductive bias.\nTo remedy this, many papers have explored augmenting RNNs with various\ndifferentiable stacks, by analogy with finite automata and pushdown automata.\nIn this paper, we present a stack RNN model based on the recently proposed\nNondeterministic Stack RNN (NS-RNN) that achieves lower cross-entropy than all\nprevious stack RNNs on five context-free language modeling tasks (within 0.05\nnats of the information-theoretic lower bound), including a task in which the\nNS-RNN previously failed to outperform a deterministic stack RNN baseline. Our\nmodel assigns arbitrary positive weights instead of probabilities to stack\nactions, and we provide an analysis of why this improves training. We also\npropose a restricted version of the NS-RNN that makes it practical to use for\nlanguage modeling on natural language and present results on the Penn Treebank\ncorpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1\">Brian DuSell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-entry Prediction for Online Conversations via Self-Supervised Learning. (arXiv:2109.02020v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02020","description":"<p>In recent years, world business in online discussions and opinion sharing on\nsocial media is booming. Re-entry prediction task is thus proposed to help\npeople keep track of the discussions which they wish to continue. Nevertheless,\nexisting works only focus on exploiting chatting history and context\ninformation, and ignore the potential useful learning signals underlying\nconversation data, such as conversation thread patterns and repeated engagement\nof target users, which help better understand the behavior of target users in\nconversations. In this paper, we propose three interesting and well-founded\nauxiliary tasks, namely, Spread Pattern, Repeated Target user, and Turn\nAuthorship, as the self-supervised signals for re-entry prediction. These\nauxiliary tasks are trained together with the main task in a multi-task manner.\nExperimental results on two datasets newly collected from Twitter and Reddit\nshow that our method outperforms the previous state-of-the-arts with fewer\nparameters and faster convergence. Extensive experiments and analysis show the\neffectiveness of our proposed models and also point out some key ideas in\ndesigning self-supervised tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Efficient Masked Language Modeling for Vision and Language. (arXiv:2109.02040v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02040","description":"<p>Masked language modeling (MLM) is one of the key sub-tasks in vision-language\npretraining. In the cross-modal setting, tokens in the sentence are masked at\nrandom, and the model predicts the masked tokens given the image and the text.\nIn this paper, we observe several key disadvantages of MLM in this setting.\nFirst, as captions tend to be short, in a third of the sentences no token is\nsampled. Second, the majority of masked tokens are stop-words and punctuation,\nleading to under-utilization of the image. We investigate a range of\nalternative masking strategies specific to the cross-modal setting that address\nthese shortcomings, aiming for better fusion of text and image in the learned\nrepresentation. When pre-training the LXMERT model, our alternative masking\nstrategies consistently improve over the original masking strategy on three\ndownstream tasks, especially in low resource settings. Further, our\npre-training approach substantially outperforms the baseline model on a\nprompt-based probing task designed to elicit image objects. These results and\nour analysis indicate that our method allows for better utilization of the\ntraining data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhadad_M/0/1/0/all/0/1\">Michael Elhadad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Attention Branch Network with Combined Loss Function for Automatic Speaker Verification Spoof Detection. (arXiv:2109.02051v1 [cs.SD])","link":"http://arxiv.org/abs/2109.02051","description":"<p>Many endeavors have sought to develop countermeasure techniques as\nenhancements on Automatic Speaker Verification (ASV) systems, in order to make\nthem more robust against spoof attacks. As evidenced by the latest ASVspoof\n2019 countermeasure challenge, models currently deployed for the task of ASV\nare, at their best, devoid of suitable degrees of generalization to unseen\nattacks. Upon further investigation of the proposed methods, it appears that a\nbroader three-tiered view of the proposed systems. comprised of the classifier,\nfeature extraction phase, and model loss function, may to some extent lessen\nthe problem. Accordingly, the present study proposes the Efficient Attention\nBranch Network (EABN) modular architecture with a combined loss function to\naddress the generalization problem...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rostami_A/0/1/0/all/0/1\">Amir Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homayounpour_M/0/1/0/all/0/1\">Mohammad Mehdi Homayounpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickabadi_A/0/1/0/all/0/1\">Ahmad Nickabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Self-Debiasing Framework for Robust NLU Training. (arXiv:2109.02071v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02071","description":"<p>Existing Natural Language Understanding (NLU) models have been shown to\nincorporate dataset biases leading to strong performance on in-distribution\n(ID) test sets but poor performance on out-of-distribution (OOD) ones. We\nintroduce a simple yet effective debiasing framework whereby the shallow\nrepresentations of the main model are used to derive a bias model and both\nmodels are trained simultaneously. We demonstrate on three well studied NLU\ntasks that despite its simplicity, our method leads to competitive OOD results.\nIt significantly outperforms other debiasing approaches on two tasks, while\nstill delivering high in-distribution performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction. (arXiv:2109.02099v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02099","description":"<p>Distantly supervised relation extraction (RE) automatically aligns\nunstructured text with relation instances in a knowledge base (KB). Due to the\nincompleteness of current KBs, sentences implying certain relations may be\nannotated as N/A instances, which causes the so-called false negative (FN)\nproblem. Current RE methods usually overlook this problem, inducing improper\nbiases in both training and testing procedures. To address this issue, we\npropose a two-stage approach. First, it finds out possible FN samples by\nheuristically leveraging the memory mechanism of deep neural networks. Then, it\naligns those unlabeled data with the training data into a unified feature space\nby adversarial training to assign pseudo labels and further utilize the\ninformation contained in them. Experiments on two wildly-used benchmark\ndatasets demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_K/0/1/0/all/0/1\">Kailong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Botao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration. (arXiv:2109.02102v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02102","description":"<p>This paper demonstrates that by fine-tuning an autoregressive language model\n(GPT-Neo) on appropriately structured step-by-step demonstrations, it is\npossible to teach it to execute a mathematical task that has previously proved\ndifficult for Transformers - longhand modulo operations - with a relatively\nsmall number of examples. Specifically, we fine-tune GPT-Neo to solve the\nnumbers__div_remainder task from the DeepMind Mathematics Dataset; Saxton et\nal. (<a href=\"/abs/1904.01557\">arXiv:1904.01557</a>) reported below 40% accuracy on this task with 2 million\ntraining examples. We show that after fine-tuning on 200 appropriately\nstructured demonstrations of solving long division problems and reporting the\nremainders, the smallest available GPT-Neo model achieves over 80% accuracy.\nThis is achieved by constructing an appropriate dataset for fine-tuning, with\nno changes to the learning algorithm. These results suggest that fine-tuning\nautoregressive language models on small sets of well-crafted demonstrations may\nbe a useful paradigm for enabling individuals without training in machine\nlearning to coax such models to perform some kinds of complex multi-step tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Recchia_G/0/1/0/all/0/1\">Gabriel Recchia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Models for Text Coherence Assessment. (arXiv:2109.02176v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02176","description":"<p>Coherence is an important aspect of text quality and is crucial for ensuring\nits readability. It is essential desirable for outputs from text generation\nsystems like summarization, question answering, machine translation, question\ngeneration, table-to-text, etc. An automated coherence scoring model is also\nhelpful in essay scoring or providing writing feedback. A large body of\nprevious work has leveraged entity-based methods, syntactic patterns, discourse\nrelations, and more recently traditional deep learning architectures for text\ncoherence assessment. Previous work suffers from drawbacks like the inability\nto handle long-range dependencies, out-of-vocabulary words, or model sequence\ninformation. We hypothesize that coherence assessment is a cognitively complex\ntask that requires deeper models and can benefit from other related tasks.\nAccordingly, in this paper, we propose four different Transformer-based\narchitectures for the task: vanilla Transformer, hierarchical Transformer,\nmulti-task learning-based model, and a model with fact-based input\nrepresentation. Our experiments with popular benchmark datasets across multiple\ndomains on four different coherence assessment tasks demonstrate that our\nmodels achieve state-of-the-art results outperforming existing models by a good\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abhishek_T/0/1/0/all/0/1\">Tushar Abhishek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_D/0/1/0/all/0/1\">Daksh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nearest Neighbour Few-Shot Learning for Cross-lingual Classification. (arXiv:2109.02221v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02221","description":"<p>Even though large pre-trained multilingual models (e.g. mBERT, XLM-R) have\nled to significant performance gains on a wide range of cross-lingual NLP\ntasks, success on many downstream tasks still relies on the availability of\nsufficient annotated data. Traditional fine-tuning of pre-trained models using\nonly a few target samples can cause over-fitting. This can be quite limiting as\nmost languages in the world are under-resourced. In this work, we investigate\ncross-lingual adaptation using a simple nearest neighbor few-shot (&lt;15 samples)\ninference technique for classification tasks. We experiment using a total of 16\ndistinct languages across two NLP tasks- XNLI and PAWS-X. Our approach\nconsistently improves traditional fine-tuning using only a handful of labeled\nsamples in target locales. We also demonstrate its generalization capability\nacross tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haider_B/0/1/0/all/0/1\">Batool Haider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack. (arXiv:2109.02229v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02229","description":"<p>Over the past few years, various word-level textual attack approaches have\nbeen proposed to reveal the vulnerability of deep neural networks used in\nnatural language processing. Typically, these approaches involve an important\noptimization step to determine which substitute to be used for each word in the\noriginal input. However, current research on this step is still rather limited,\nfrom the perspectives of both problem-understanding and problem-solving. In\nthis paper, we address these issues by uncovering the theoretical properties of\nthe problem and proposing an efficient local search algorithm (LS) to solve it.\nWe establish the first provable approximation guarantee on solving the problem\nin general cases. Notably, for adversarial textual attack, it is even better\nthan the previous bound which only holds in special case. Extensive experiments\ninvolving five NLP tasks, six datasets and eleven NLP models show that LS can\nlargely reduce the number of queries usually by an order of magnitude to\nachieve high attack success rates. Further experiments show that the\nadversarial examples crafted by LS usually have higher quality, exhibit better\ntransferability, and can bring more robustness improvement to victim models by\nadversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Ning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT might be Overkill: A Tiny but Effective Biomedical Entity Linker based on Residual Convolutional Neural Networks. (arXiv:2109.02237v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02237","description":"<p>Biomedical entity linking is the task of linking entity mentions in a\nbiomedical document to referent entities in a knowledge base. Recently, many\nBERT-based models have been introduced for the task. While these models have\nachieved competitive results on many datasets, they are computationally\nexpensive and contain about 110M parameters. Little is known about the factors\ncontributing to their impressive performance and whether the\nover-parameterization is needed. In this work, we shed some light on the inner\nworking mechanisms of these large BERT-based models. Through a set of probing\nexperiments, we have found that the entity linking performance only changes\nslightly when the input word order is shuffled or when the attention scope is\nlimited to a fixed window size. From these observations, we propose an\nefficient convolutional neural network with residual connections for biomedical\nentity linking. Because of the sparse connectivity and weight sharing\nproperties, our model has a small number of parameters and is highly efficient.\nOn five public datasets, our model achieves comparable or even better linking\naccuracy than the state-of-the-art BERT-based models while having about 60\ntimes fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STaCK: Sentence Ordering with Temporal Commonsense Knowledge. (arXiv:2109.02247v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02247","description":"<p>Sentence order prediction is the task of finding the correct order of\nsentences in a randomly ordered document. Correctly ordering the sentences\nrequires an understanding of coherence with respect to the chronological\nsequence of events described in the text. Document-level contextual\nunderstanding and commonsense knowledge centered around these events are often\nessential in uncovering this coherence and predicting the exact chronological\norder. In this paper, we introduce STaCK -- a framework based on graph neural\nnetworks and temporal commonsense knowledge to model global information and\npredict the relative order of sentences. Our graph network accumulates temporal\nevidence using knowledge of `past' and `future' and formulates sentence\nordering as a constrained edge classification problem. We report results on\nfive different datasets, and empirically show that the proposed method is\nnaturally suitable for order prediction. The implementation of this work is\npublicly available at: https://github.com/declare-lab/sentence-ordering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sent2Span: Span Detection for PICO Extraction in the Biomedical Text without Span Annotations. (arXiv:2109.02254v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02254","description":"<p>The rapid growth in published clinical trials makes it difficult to maintain\nup-to-date systematic reviews, which requires finding all relevant trials. This\nleads to policy and practice decisions based on out-of-date, incomplete, and\nbiased subsets of available clinical evidence. Extracting and then normalising\nPopulation, Intervention, Comparator, and Outcome (PICO) information from\nclinical trial articles may be an effective way to automatically assign trials\nto systematic reviews and avoid searching and screening - the two most\ntime-consuming systematic review processes. We propose and test a novel\napproach to PICO span detection. The major difference between our proposed\nmethod and previous approaches comes from detecting spans without needing\nannotated span data and using only crowdsourced sentence-level annotations.\nExperiments on two datasets show that PICO span detection results achieve much\nhigher results for recall when compared to fully supervised methods with PICO\nsentence detection at least as good as human annotations. By removing the\nreliance on expert annotations for span detection, this work could be used in\nhuman-machine pipeline for turning low-quality crowdsourced, and sentence-level\nPICO annotations into structured information that can be used to quickly assign\ntrials to relevant systematic reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourgeois_F/0/1/0/all/0/1\">Florence T. Bourgeois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_A/0/1/0/all/0/1\">Adam G. Dunn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training. (arXiv:2109.02284v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02284","description":"<p>Learning multilingual and multi-domain translation model is challenging as\nthe heterogeneous and imbalanced data make the model converge inconsistently\nover different corpora in real world. One common practice is to adjust the\nshare of each corpus in the training, so that the learning process is balanced\nand low-resource cases can benefit from the high resource ones. However,\nautomatic balancing methods usually depend on the intra- and inter-dataset\ncharacteristics, which is usually agnostic or requires human priors. In this\nwork, we propose an approach, MultiUAT, that dynamically adjusts the training\ndata usage based on the model's uncertainty on a small set of trusted clean\ndata for multi-corpus machine translation. We experiments with two classes of\nuncertainty measures on multilingual (16 languages with 4 settings) and\nmulti-domain settings (4 for in-domain and 2 for out-of-domain on\nEnglish-German translation) and demonstrate our approach MultiUAT substantially\noutperforms its baselines, including both static and dynamic strategies. We\nanalyze the cross-domain transfer and show the deficiency of static and\nsimilarity based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Numerical Reasoning Skills in the Modular Approach for Complex Question Answering on Text. (arXiv:2109.02289v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02289","description":"<p>Numerical reasoning skills are essential for complex question answering (CQA)\nover text. It requires opertaions including counting, comparison, addition and\nsubtraction. A successful approach to CQA on text, Neural Module Networks\n(NMNs), follows the programmer-interpreter paradigm and leverages specialised\nmodules to perform compositional reasoning. However, the NMNs framework does\nnot consider the relationship between numbers and entities in both questions\nand paragraphs. We propose effective techniques to improve NMNs' numerical\nreasoning capabilities by making the interpreter question-aware and capturing\nthe relationship between entities and numbers. On the same subset of the DROP\ndataset for CQA on text, experimental results show that our additions\noutperform the original NMNs by 3.0 points for the overall F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiao-Yu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Visual Dialog Questioner with Entity-based Strategy Learning and Augmented Guesser. (arXiv:2109.02297v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02297","description":"<p>Considering the importance of building a good Visual Dialog (VD) Questioner,\nmany researchers study the topic under a Q-Bot-A-Bot image-guessing game\nsetting, where the Questioner needs to raise a series of questions to collect\ninformation of an undisclosed image. Despite progress has been made in\nSupervised Learning (SL) and Reinforcement Learning (RL), issues still exist.\nFirstly, previous methods do not provide explicit and effective guidance for\nQuestioner to generate visually related and informative questions. Secondly,\nthe effect of RL is hampered by an incompetent component, i.e., the Guesser,\nwho makes image predictions based on the generated dialogs and assigns rewards\naccordingly. To enhance VD Questioner: 1) we propose a Related entity enhanced\nQuestioner (ReeQ) that generates questions under the guidance of related\nentities and learns entity-based questioning strategy from human dialogs; 2) we\npropose an Augmented Guesser (AugG) that is strong and is optimized for the VD\nsetting especially. Experimental results on the VisDial v1.0 dataset show that\nour approach achieves state-of-theart performance on both image-guessing task\nand question diversity. Human study further proves that our model generates\nmore visually related, informative and coherent questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Duo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightTag: Text Annotation Platform. (arXiv:2109.02320v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02320","description":"<p>Text annotation tools assume that their user's goal is to create a labeled\ncorpus. However, users view annotation as a necessary evil on the way to\ndeliver business value through NLP. Thus an annotation tool should optimize for\nthe throughput of the global NLP process, not only the productivity of\nindividual annotators. LightTag is a text annotation tool designed and built on\nthat principle. This paper shares our design rationale, data modeling choices,\nand user interface decisions then illustrates how those choices serve the full\nNLP lifecycle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perry_T/0/1/0/all/0/1\">Tal Perry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hocalarim: Mining Turkish Student Reviews. (arXiv:2109.02325v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02325","description":"<p>We introduce Hocalarim (MyProfessors), the largest student review dataset\navailable for the Turkish language. It consists of over 5000 professor reviews\nleft online by students, with different aspects of education rated on a scale\nof 1 to 5 stars. We investigate the properties of the dataset and present its\nstatistics. We examine the impact of students' institution type on their\nratings and the correlation of students' bias to give positive or negative\nfeedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_I/0/1/0/all/0/1\">Ibrahim Faruk Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calik_N/0/1/0/all/0/1\">Necmettin Bera Calik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yapucuoglu_M/0/1/0/all/0/1\">Mert Yapucuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1\">Ahmet Yavuz Uluslu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Putting a Spin on Language: A Quantum Interpretation of Unary Connectives for Linguistic Applications. (arXiv:2004.04128v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.04128","description":"<p>Extended versions of the Lambek Calculus currently used in computational\nlinguistics rely on unary modalities to allow for the controlled application of\nstructural rules affecting word order and phrase structure. These controlled\nstructural operations give rise to derivational ambiguities that are missed by\nthe original Lambek Calculus or its pregroup simplification. Proposals for\ncompositional interpretation of extended Lambek Calculus in the compact closed\ncategory of FVect and linear maps have been made, but in these proposals the\nsyntax-semantics mapping ignores the control modalities, effectively\nrestricting their role to the syntax. Our aim is to turn the modalities into\nfirst-class citizens of the vectorial interpretation. Building on the\ndirectional density matrix semantics, we extend the interpretation of the type\nsystem with an extra spin density matrix space. The interpretation of proofs\nthen results in ambiguous derivations being tensored with orthogonal spin\nstates. Our method introduces a way of simultaneously representing co-existing\ninterpretations of ambiguous utterances, and provides a uniform framework for\nthe integration of lexical and derivational ambiguity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Correia_A/0/1/0/all/0/1\">Adriana D. Correia</a> (Utrecht University), <a href=\"http://arxiv.org/find/cs/1/au:+Stoof_H/0/1/0/all/0/1\">Henk T. C. Stoof</a> (Utrecht University), <a href=\"http://arxiv.org/find/cs/1/au:+Moortgat_M/0/1/0/all/0/1\">Michael Moortgat</a> (Utrecht University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Natural Language Processing on Near-Term Quantum Computers. (arXiv:2005.04147v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.04147","description":"<p>In this work, we describe a full-stack pipeline for natural language\nprocessing on near-term quantum computers, aka QNLP. The language-modelling\nframework we employ is that of compositional distributional semantics\n(DisCoCat), which extends and complements the compositional structure of\npregroup grammars. Within this model, the grammatical reduction of a sentence\nis interpreted as a diagram, encoding a specific interaction of words according\nto the grammar. It is this interaction which, together with a specific choice\nof word embedding, realises the meaning (or \"semantics\") of a sentence.\nBuilding on the formal quantum-like nature of such interactions, we present a\nmethod for mapping DisCoCat diagrams to quantum circuits. Our methodology is\ncompatible both with NISQ devices and with established Quantum Machine Learning\ntechniques, paving the way to near-term applications of quantum technology to\nnatural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meichanetzidis_K/0/1/0/all/0/1\">Konstantinos Meichanetzidis</a> (University of Oxford and Cambridge Quantum Computing Ltd.), <a href=\"http://arxiv.org/find/cs/1/au:+Gogioso_S/0/1/0/all/0/1\">Stefano Gogioso</a> (Hashberg), <a href=\"http://arxiv.org/find/cs/1/au:+Felice_G/0/1/0/all/0/1\">Giovanni de Felice</a> (University of Oxford and Cambridge Quantum Computing Ltd.), <a href=\"http://arxiv.org/find/cs/1/au:+Chiappori_N/0/1/0/all/0/1\">Nicol&#xf2; Chiappori</a> (Hashberg), <a href=\"http://arxiv.org/find/cs/1/au:+Toumi_A/0/1/0/all/0/1\">Alexis Toumi</a> (University of Oxford and Cambridge Quantum Computing Ltd.), <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a> (University of Oxford and Cambridge Quantum Computing Ltd.)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact and dynamics of hate and counter speech online. (arXiv:2009.08392v3 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2009.08392","description":"<p>Citizen-generated counter speech is a promising way to fight hate speech and\npromote peaceful, non-polarized discourse. However, there is a lack of\nlarge-scale longitudinal studies of its effectiveness for reducing hate speech.\nTo this end, we perform an exploratory analysis of the effectiveness of counter\nspeech using several different macro- and micro-level measures to analyze\n180,000 political conversations that took place on German Twitter over four\nyears. We report on the dynamic interactions of hate and counter speech over\ntime and provide insights into whether, as in `classic' bullying situations,\norganized efforts are more effective than independent individuals in steering\nonline discourse. Taken together, our results build a multifaceted picture of\nthe dynamics of hate and counter speech online. While we make no causal claims\ndue to the complexity of discourse dynamics, our findings suggest that\norganized hate speech is associated with changes in public discourse and that\ncounter speech -- especially when organized -- may help curb hateful rhetoric\nin online discourse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garland_J/0/1/0/all/0/1\">Joshua Garland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazi_Zahedi_K/0/1/0/all/0/1\">Keyan Ghazi-Zahedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_J/0/1/0/all/0/1\">Jean-Gabriel Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebert_Dufresne_L/0/1/0/all/0/1\">Laurent H&#xe9;bert-Dufresne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galesic_M/0/1/0/all/0/1\">Mirta Galesic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP). (arXiv:2010.16413v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.16413","description":"<p>The COVID-19 pandemic has had a significant impact on society, both because\nof the serious health effects of COVID-19 and because of public health measures\nimplemented to slow its spread. Many of these difficulties are fundamentally\ninformation needs; attempts to address these needs have caused an information\noverload for both researchers and the public. Natural language processing\n(NLP), the branch of artificial intelligence that interprets human language,\ncan be applied to address many of the information needs made urgent by the\nCOVID-19 pandemic. This review surveys approximately 150 NLP studies and more\nthan 50 systems and datasets addressing the COVID-19 pandemic. We detail work\non four core NLP tasks: information retrieval, named entity recognition,\nliterature-based discovery, and question answering. We also describe work that\ndirectly addresses aspects of the pandemic through four additional tasks: topic\nmodeling, sentiment and emotion analysis, caseload forecasting, and\nmisinformation detection. We conclude by discussing observable trends and\nremaining challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1\">Robert Leaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allot_A/0/1/0/all/0/1\">Alexis Allot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Ling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chih-Hsuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shankai Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference. (arXiv:2011.14203v5 [cs.AR] UPDATED)","link":"http://arxiv.org/abs/2011.14203","description":"<p>Transformer-based language models such as BERT provide significant accuracy\nimprovement for a multitude of natural language processing (NLP) tasks.\nHowever, their hefty computational and memory demands make them challenging to\ndeploy to resource-constrained edge platforms with strict latency requirements.\nWe present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware\nenergy optimization for multi-task NLP. EdgeBERT employs entropy-based early\nexit predication in order to perform dynamic voltage-frequency scaling (DVFS),\nat a sentence granularity, for minimal energy consumption while adhering to a\nprescribed target latency. Computation and memory footprint overheads are\nfurther alleviated by employing a calibrated combination of adaptive attention\nspan, selective network pruning, and floating-point quantization. Furthermore,\nin order to maximize the synergistic benefits of these algorithms in always-on\nand intermediate edge computing settings, we specialize a 12nm scalable\nhardware accelerator system, integrating a fast-switching low-dropout voltage\nregulator (LDO), an all-digital phase-locked loop (ADPLL), as well as,\nhigh-density embedded non-volatile memories (eNVMs) wherein the sparse\nfloating-point bit encodings of the shared multi-task parameters are carefully\nstored. Altogether, latency-aware multi-task NLP inference acceleration on the\nEdgeBERT hardware system generates up to 7x, 2.5x, and 53x lower energy\ncompared to the conventional inference without early stopping, the\nlatency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson\nTegra X2 mobile GPU, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tambe_T/0/1/0/all/0/1\">Thierry Tambe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooper_C/0/1/0/all/0/1\">Coleman Hooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pentecost_L/0/1/0/all/0/1\">Lillian Pentecost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_T/0/1/0/all/0/1\">Tianyu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">En-Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donato_M/0/1/0/all/0/1\">Marco Donato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1\">Paul N. Whatmough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brooks_D/0/1/0/all/0/1\">David Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Gu-Yeon Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Feed-Forward Layers Are Key-Value Memories. (arXiv:2012.14913v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14913","description":"<p>Feed-forward layers constitute two-thirds of a transformer model's\nparameters, yet their role in the network remains under-explored. We show that\nfeed-forward layers in transformer-based language models operate as key-value\nmemories, where each key correlates with textual patterns in the training\nexamples, and each value induces a distribution over the output vocabulary. Our\nexperiments show that the learned patterns are human-interpretable, and that\nlower layers tend to capture shallow patterns, while upper layers learn more\nsemantic ones. The values complement the keys' input patterns by inducing\noutput distributions that concentrate probability mass on tokens likely to\nappear immediately after each pattern, particularly in the upper layers.\nFinally, we demonstrate that the output of a feed-forward layer is a\ncomposition of its memories, which is subsequently refined throughout the\nmodel's layers via residual connections to produce the final output\ndistribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_R/0/1/0/all/0/1\">Roei Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Dynamics in Movie Dialogues. (arXiv:2103.01345v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01345","description":"<p>Emotion dynamics is a framework for measuring how an individual's emotions\nchange over time. It is a powerful tool for understanding how we behave and\ninteract with the world. In this paper, we introduce a framework to track\nemotion dynamics through one's utterances. Specifically we introduce a number\nof utterance emotion dynamics (UED) metrics inspired by work in Psychology. We\nuse this approach to trace emotional arcs of movie characters. We analyze\nthousands of such character arcs to test hypotheses that inform our broader\nunderstanding of stories. Notably, we show that there is a tendency for\ncharacters to use increasingly more negative words and become increasingly\nemotionally discordant with each other until about 90 percent of the narrative\nlength. UED also has applications in behavior studies, social sciences, and\npublic health.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hipson_W/0/1/0/all/0/1\">Will E. Hipson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models. (arXiv:2103.06678v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.06678","description":"<p>In this paper, we explore the effects of language variants, data sizes, and\nfine-tuning task types in Arabic pre-trained language models. To do so, we\nbuild three pre-trained language models across three variants of Arabic: Modern\nStandard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a\nfourth language model which is pre-trained on a mix of the three. We also\nexamine the importance of pre-training data size by building additional models\nthat are pre-trained on a scaled-down set of the MSA variant. We compare our\ndifferent models to each other, as well as to eight publicly available models\nby fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest\nthat the variant proximity of pre-training data to fine-tuning data is more\nimportant than the pre-training data size. We exploit this insight in defining\nan optimized system selection model for the studied tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_G/0/1/0/all/0/1\">Go Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhafni_B/0/1/0/all/0/1\">Bashar Alhafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baimukan_N/0/1/0/all/0/1\">Nurpeiis Baimukan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouamor_H/0/1/0/all/0/1\">Houda Bouamor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructive and Toxic Speech Detection for Open-domain Social Media Comments in Vietnamese. (arXiv:2103.10069v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.10069","description":"<p>The rise of social media has led to the increasing of comments on online\nforums. However, there still exists invalid comments which are not informative\nfor users. Moreover, those comments are also quite toxic and harmful to people.\nIn this paper, we create a dataset for constructive and toxic speech detection,\nnamed UIT-ViCTSD (Vietnamese Constructive and Toxic Speech Detection dataset)\nwith 10,000 human-annotated comments. For these tasks, we propose a system for\nconstructive and toxic speech detection with the state-of-the-art transfer\nlearning model in Vietnamese NLP as PhoBERT. With this system, we obtain\nF1-scores of 78.59% and 59.40% for classifying constructive and toxic comments,\nrespectively. Besides, we implement various baseline models as traditional\nMachine Learning and Deep Neural Network-Based models to evaluate the dataset.\nWith the results, we can solve several tasks on the online discussions and\ndevelop the framework for identifying constructiveness and toxicity of\nVietnamese social media comments automatically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Luan Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's in your Head? Emergent Behaviour in Multi-Task Transformer Models. (arXiv:2104.06129v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06129","description":"<p>The primary paradigm for multi-task training in natural language processing\nis to represent the input with a shared pre-trained language model, and add a\nsmall, thin network (head) per task. Given an input, a target head is the head\nthat is selected for outputting the final prediction. In this work, we examine\nthe behaviour of non-target heads, that is, the output of heads when given\ninput that belongs to a different task than the one they were trained for. We\nfind that non-target heads exhibit emergent behaviour, which may either explain\nthe target task, or generalize beyond their original task. For example, in a\nnumerical reasoning task, a span extraction head extracts from the input the\narguments to a computation that results in a number generated by a target\ngenerative head. In addition, a summarization head that is trained with a\ntarget question answering head, outputs query-based summaries when given a\nquestion and a context from which the answer is to be extracted. This emergent\nbehaviour suggests that multi-task training leads to non-trivial extrapolation\nof skills, which can be harnessed for interpretability and generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_U/0/1/0/all/0/1\">Uri Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Arie_A/0/1/0/all/0/1\">Aviv Ben-Arie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Planning with Learned Entity Prompts for Abstractive Summarization. (arXiv:2104.07606v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07606","description":"<p>We introduce a simple but flexible mechanism to learn an intermediate plan to\nground the generation of abstractive summaries. Specifically, we prepend (or\nprompt) target summaries with entity chains -- ordered sequences of entities\nmentioned in the summary. Transformer-based sequence-to-sequence models are\nthen trained to generate the entity chain and then continue generating the\nsummary conditioned on the entity chain and the input. We experimented with\nboth pretraining and finetuning with this content planning objective. When\nevaluated on CNN/DailyMail, XSum, SAMSum and BillSum, we demonstrate\nempirically that the grounded generation with the planning objective improves\nentity specificity and planning in summaries for all datasets, and achieves\nstate-of-the-art performance on XSum and SAMSum in terms of Rouge. Moreover, we\ndemonstrate empirically that planning with entity chains provides a mechanism\nto control hallucinations in abstractive summaries. By prompting the decoder\nwith a modified content plan that drops hallucinated entities, we outperform\nstate-of-the-art approaches for faithfulness when evaluated automatically and\nby humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Shashi Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoes_G/0/1/0/all/0/1\">Gon&#xe7;alo Simoes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_V/0/1/0/all/0/1\">Vitaly Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1\">Ryan McDonald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Masked Segmental Language Model for Unsupervised Natural Language Segmentation. (arXiv:2104.07829v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07829","description":"<p>Segmentation remains an important preprocessing step both in languages where\n\"words\" or other important syntactic/semantic units (like morphemes) are not\nclearly delineated by white space, as well as when dealing with continuous\nspeech data, where there is often no meaningful pause between words.\nNear-perfect supervised methods have been developed for use in resource-rich\nlanguages such as Chinese, but many of the world's languages are both\nmorphologically complex, and have no large dataset of \"gold\" segmentations into\nmeaningful units. To solve this problem, we propose a new type of Segmental\nLanguage Model (Sun and Deng, 2018; Kawakami et al., 2019; Wang et al., 2021)\nfor use in both unsupervised and lightly supervised segmentation tasks. We\nintroduce a Masked Segmental Language Model (MSLM) built on a span-masking\ntransformer architecture, harnessing the power of a bi-directional masked\nmodeling context and attention. In a series of experiments, our model\nconsistently outperforms Recurrent SLMs on Chinese (PKU Corpus) in segmentation\nquality, and performs similarly to the Recurrent model on English (PTB). We\nconclude by discussing the different challenges posed in segmenting\nphonemic-type writing systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Downey_C/0/1/0/all/0/1\">C.M. Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levow_G/0/1/0/all/0/1\">Gina-Anne Levow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinert_Threlkeld_S/0/1/0/all/0/1\">Shane Steinert-Threlkeld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching-oriented Product Quantization For Ad-hoc Retrieval. (arXiv:2104.07858v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07858","description":"<p>Product quantization (PQ) is a widely used technique for ad-hoc retrieval.\nRecent studies propose supervised PQ, where the embedding and quantization\nmodels can be jointly trained with supervised learning. However, there is a\nlack of appropriate formulation of the joint training objective; thus, the\nimprovements over previous non-supervised baselines are limited in reality. In\nthis work, we propose the Matching-oriented Product Quantization (MoPQ), where\na novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the\nminimization of MCL, we are able to maximize the matching probability of query\nand ground-truth key, which contributes to the optimal retrieval accuracy.\nGiven that the exact computation of MCL is intractable due to the demand of\nvast contrastive samples, we further propose the Differentiable Cross-device\nSampling (DCS), which significantly augments the contrastive samples for\nprecise approximation of MCL. We conduct extensive experimental studies on four\nreal-world datasets, whose results verify the effectiveness of MoPQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08773","description":"<p>Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. NLP models built with the conventional paradigm, however, often\nstruggle with generalization across tasks (e.g., a question-answering system\ncannot solve classification tasks). A long-standing challenge in AI is to build\na model that is equipped with the understanding of human-readable instructions\nthat define the tasks, and can generalize to new tasks. To study this, we\nintroduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their\nhuman-authored instructions and 193k task instances. The instructions are\nobtained from crowdsourcing instructions used to collect existing NLP datasets\nand mapped to a unified schema. We adopt generative pre-trained language models\nto encode task-specific instructions along with input and generate task output.\nOur results indicate that models can benefit from instructions to generalize\nacross tasks. These models, however, are far behind supervised task-specific\nmodels, indicating significant room for more progress in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An automated domain-independent text reading, interpreting and extracting approach for reviewing the scientific literature. (arXiv:2107.14638v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.14638","description":"<p>It is presented here a machine learning-based (ML) natural language\nprocessing (NLP) approach capable to automatically recognize and extract\ncategorical and numerical parameters from a corpus of articles. The approach\n(named a.RIX) operates with a concomitant/interchangeable use of ML models such\nas neuron networks (NNs), latent semantic analysis (LSA), naive-Bayes\nclassifiers (NBC), and a pattern recognition model using regular expression\n(REGEX). A corpus of 7,873 scientific articles dealing with natural products\n(NPs) was used to demonstrate the efficiency of the a.RIX engine. The engine\nautomatically extracts categorical and numerical parameters such as (i) the\nplant species from which active molecules are extracted, (ii) the\nmicroorganisms species for which active molecules can act against, and (iii)\nthe values of minimum inhibitory concentration (MIC) against these\nmicroorganisms. The parameters are extracted without part-of-speech tagging\n(POS) and named entity recognition (NER) approaches (i.e. without the need of\ntext annotation), and the models training is performed with unsupervised\napproaches. In this way, a.RIX can be essentially used on articles from any\nscientific field. Finally, it can potentially make obsolete the current article\nreviewing process in some areas, especially those in which machine learning\nmodels capture texts structure, text semantics, and latent knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Amauri J Paula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opinion Prediction with User Fingerprinting. (arXiv:2108.00270v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00270","description":"<p>Opinion prediction is an emerging research area with diverse real-world\napplications, such as market research and situational awareness. We identify\ntwo lines of approaches to the problem of opinion prediction. One uses\ntopic-based sentiment analysis with time-series modeling, while the other uses\nstatic embedding of text. The latter approaches seek user-specific solutions by\ngenerating user fingerprints. Such approaches are useful in predicting user's\nreactions to unseen content. In this work, we propose a novel dynamic\nfingerprinting method that leverages contextual embedding of user's comments\nconditioned on relevant user's reading history. We integrate BERT variants with\na recurrent neural network to generate predictions. The results show up to 13\\%\nimprovement in micro F1-score compared to previous approaches. Experimental\nresults show novel insights that were previously unknown such as better\npredictions for an increase in dynamic history length, the impact of the nature\nof the article on performance, thereby laying the foundation for further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tumarada_K/0/1/0/all/0/1\">Kishore Tumarada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dr. Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dr. Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragut_D/0/1/0/all/0/1\">Dr. Eduard Dragut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gnawali_D/0/1/0/all/0/1\">Dr. Omprakash Gnawali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_D/0/1/0/all/0/1\">Dr. Arjun Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07790","description":"<p>Language models trained on large-scale unfiltered datasets curated from the\nopen web acquire systemic biases, prejudices, and harmful views from their\ntraining data. We present a methodology for programmatically identifying and\nremoving harmful text from web-scale datasets. A pretrained language model is\nused to calculate the log-likelihood of researcher-written trigger phrases\nconditioned on a specific document, which is used to identify and filter\ndocuments from the dataset. We demonstrate that models trained on this filtered\ndataset exhibit lower propensity to generate harmful text, with a marginal\ndecrease in performance on standard language modeling benchmarks compared to\nunfiltered baselines. We provide a partial explanation for this performance gap\nby surfacing examples of hate speech and other undesirable content from\nstandard language modeling benchmarks. Finally, we discuss the generalization\nof this method and how trigger phrases which reflect specific values can be\nused by researchers to build language models which are more closely aligned\nwith their values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1\">Helen Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raterink_C/0/1/0/all/0/1\">Cooper Raterink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_J/0/1/0/all/0/1\">Jo&#xe3;o G.M. Ara&#xfa;jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_I/0/1/0/all/0/1\">Ivan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Carol Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1\">Adrien Morisot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frosst_N/0/1/0/all/0/1\">Nicholas Frosst</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08468","description":"<p>We study the problem of query attribute value extraction, which aims to\nidentify named entities from user queries as diverse surface form attribute\nvalues and afterward transform them into formally canonical forms. Such a\nproblem consists of two phases: {named entity recognition (NER)} and {attribute\nvalue normalization (AVN)}. However, existing works only focus on the NER phase\nbut neglect equally important AVN. To bridge this gap, this paper proposes a\nunified query attribute value extraction system in e-commerce search named\nQUEACO, which involves both two phases. Moreover, by leveraging large-scale\nweakly-labeled behavior data, we further improve the extraction performance\nwith less supervision cost. Specifically, for the NER phase, QUEACO adopts a\nnovel teacher-student network, where a teacher network that is trained on the\nstrongly-labeled data generates pseudo-labels to refine the weakly-labeled data\nfor training a student network. Meanwhile, the teacher network can be\ndynamically adapted by the feedback of the student's performance on\nstrongly-labeled data to maximally denoise the noisy supervisions from the weak\nlabels. For the AVN phase, we also leverage the weakly-labeled\nquery-to-attribute behavior data to normalize surface form attribute values\nfrom queries into canonical forms from products. Extensive experiments on a\nreal-world large-scale E-commerce dataset demonstrate the effectiveness of\nQUEACO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tony Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fastformer: Additive Attention Can Be All You Need. (arXiv:2108.09084v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09084","description":"<p>Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10750","description":"<p>Relation Extraction (RE) from tables is the task of identifying relations\nbetween pairs of columns of a table. Generally, RE models for this task require\nlabelled tables for training. These labelled tables can also be generated\nartificially from a Knowledge Graph (KG), which makes the cost to acquire them\nmuch lower in comparison to manual annotations. However, unlike real tables,\nthese synthetic tables lack associated metadata, such as, column-headers,\ncaptions, etc; this is because synthetic tables are created out of KGs that do\nnot store such metadata. Meanwhile, previous works have shown that metadata is\nimportant for accurate RE from tables. To address this issue, we propose\nmethods to artificially create some of this metadata for synthetic tables.\nAfterward, we experiment with a BERT-based model, in line with recently\npublished works, that takes as input a combination of proposed artificial\nmetadata and table content. Our empirical results show that this leads to an\nimprovement of 9\\%-45\\% in F1 score, in absolute terms, over 2 tabular\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gaurav Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siffi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Joshua Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_A/0/1/0/all/0/1\">Amir Saffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features is dependent\nupon each other. Experiment results on six public datasets show that our model\nperforms significantly better than previous approaches. In addition, contrary\nto what previous work claims, our auxiliary experiments suggest that relation\nprediction is contributory to named entity prediction in a non-negligible way.\nThe source code can be found at https://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSDS: A Fine-Grained Chinese Dataset for Customer Service Dialogue Summarization. (arXiv:2108.13139v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13139","description":"<p>Dialogue summarization has drawn much attention recently. Especially in the\ncustomer service domain, agents could use dialogue summaries to help boost\ntheir works by quickly knowing customer's issues and service progress. These\napplications require summaries to contain the perspective of a single speaker\nand have a clear topic flow structure, while neither are available in existing\ndatasets. Therefore, in this paper, we introduce a novel Chinese dataset for\nCustomer Service Dialogue Summarization (CSDS). CSDS improves the abstractive\nsummaries in two aspects: (1) In addition to the overall summary for the whole\ndialogue, role-oriented summaries are also provided to acquire different\nspeakers' viewpoints. (2) All the summaries sum up each topic separately, thus\ncontaining the topic-level structure of the dialogue. We define tasks in CSDS\nas generating the overall summary and different role-oriented summaries for a\ngiven dialogue. Next, we compare various summarization methods on CSDS, and\nexperiment results show that existing methods are prone to generate redundant\nand incoherent summaries. Besides, the performance becomes much worse when\nanalyzing the performance on role-oriented summaries and topic structures. We\nhope that this study could benchmark Chinese dialogue summarization and benefit\nfurther studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liqun Ma</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Junnan Zhu</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Lu Xiang</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a> (1 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a> (1 and 2) ((1) National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China, (2) School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China, (3) Fanyu AI Laboratory, Zhongke Fanyu Technology Co., Ltd, Beijing, China)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions. (arXiv:2108.13875v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13875","description":"<p>Scenario-based question answering (SQA) requires retrieving and reading\nparagraphs from a large corpus to answer a question which is contextualized by\na long scenario description. Since a scenario contains both keyphrases for\nretrieval and much noise, retrieval for SQA is extremely difficult. Moreover,\nit can hardly be supervised due to the lack of relevance labels of paragraphs\nfor SQA. To meet the challenge, in this paper we propose a joint\nretriever-reader model called JEEVES where the retriever is implicitly\nsupervised only using QA labels via a novel word weighting mechanism. JEEVES\nsignificantly outperforms a variety of strong baselines on multiple-choice\nquestions in three SQA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Ao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yulin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00430","description":"<p>Medical dialogue systems (MDSs) aim to assist doctors and patients with a\nrange of professional medical services, i.e., diagnosis, consultation, and\ntreatment. However, one-stop MDS is still unexplored because: (1) no dataset\nhas so large-scale dialogues contains both multiple medical services and\nfine-grained medical labels (i.e., intents, slots, values); (2) no model has\naddressed a MDS based on multiple-service conversations in a unified framework.\nIn this work, we first build a Multiple-domain Multiple-service medical\ndialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between\ndoctors and patients, covering 276 types of diseases, 2,468 medical entities,\nand 3 specialties of medical services. To the best of our knowledge, it is the\nonly medical dialogue dataset that includes both multiple medical services and\nfine-grained medical labels. Then, we formulate a one-stop MDS as a\nsequence-to-sequence generation problem. We unify a MDS with causal language\nmodeling and conditional causal language modeling, respectively. Specifically,\nwe employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)\nand their variants to get benchmarks on M^2-MedDialog dataset. We also propose\npseudo labeling and natural perturbation methods to expand M2-MedDialog dataset\nand enhance the state-of-the-art pretrained models. We demonstrate the results\nachieved by the benchmarks so far through extensive experiments on\nM2-MedDialog. We release the dataset, the code, as well as the evaluation\nscripts to facilitate future research in this important research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guojun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiahuan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huasheng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning. (arXiv:2109.00840v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00840","description":"<p>Though language model text embeddings have revolutionized NLP research, their\nability to capture high-level semantic information, such as relations between\nentities in text, is limited. In this paper, we propose a novel contrastive\nlearning framework that trains sentence embeddings to encode the relations in a\ngraph structure. Given a sentence (unstructured text) and its graph, we use\ncontrastive learning to impose relation-related structure on the token-level\nrepresentations of the sentence obtained with a CharacterBERT (El Boukkouri et\nal.,2020) model. The resulting relation-aware sentence embeddings achieve\nstate-of-the-art results on the relation extraction task using only a simple\nKNN classifier, thereby demonstrating the success of the proposed method.\nAdditional visualization by a tSNE analysis shows the effectiveness of the\nlearned representation space compared to baselines. Furthermore, we show that\nwe can learn a different space for named entity recognition, again using a\ncontrastive learning objective, and demonstrate how to successfully combine\nboth representation spaces in an entity-relation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theodoropoulos_C/0/1/0/all/0/1\">Christos Theodoropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coman_A/0/1/0/all/0/1\">Andrei C. Coman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TravelBERT: Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation. (arXiv:2109.01048v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01048","description":"<p>Existing technologies expand BERT from different perspectives, e.g. designing\ndifferent pre-training tasks, different semantic granularities and different\nmodel architectures. Few models consider expanding BERT from different text\nformats. In this paper, we propose a heterogeneous knowledge language model\n(HKLM), a unified pre-trained language model (PLM) for all forms of text,\nincluding unstructured text, semi-structured text and well-structured text. To\ncapture the corresponding relations among these multi-format knowledge, our\napproach uses masked language model objective to learn word knowledge, uses\ntriple classification objective and title matching objective to learn entity\nknowledge and topic knowledge respectively. To obtain the aforementioned\nmulti-format text, we construct a corpus in the tourism domain and conduct\nexperiments on 5 tourism NLP datasets. The results show that our approach\noutperforms the pre-training of plain text using only 1/4 of the data. The\ncode, datasets, corpus and knowledge graph will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhiheng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jinghui Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized Embeddings based Convolutional Neural Networks for Duplicate Question Identification. (arXiv:2109.01560v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01560","description":"<p>Question Paraphrase Identification (QPI) is a critical task for large-scale\nQuestion-Answering forums. The purpose of QPI is to determine whether a given\npair of questions are semantically identical or not. Previous approaches for\nthis task have yielded promising results, but have often relied on complex\nrecurrence mechanisms that are expensive and time-consuming in nature. In this\npaper, we propose a novel architecture combining a Bidirectional Transformer\nEncoder with Convolutional Neural Networks for the QPI task. We produce the\npredictions from the proposed architecture using two different inference\nsetups: Siamese and Matched Aggregation. Experimental results demonstrate that\nour model achieves state-of-the-art performance on the Quora Question Pairs\ndataset. We empirically prove that the addition of convolution layers to the\nmodel architecture improves the results in both inference setups. We also\ninvestigate the impact of partial and complete fine-tuning and analyze the\ntrade-off between computational power and accuracy in the process. Based on the\nobtained results, we conclude that the Matched-Aggregation setup consistently\noutperforms the Siamese setup. Our work provides insights into what\narchitecture combinations and setups are likely to produce better results for\nthe QPI task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakhrani_H/0/1/0/all/0/1\">Harsh Sakhrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_S/0/1/0/all/0/1\">Saloni Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratadiya_P/0/1/0/all/0/1\">Pratik Ratadiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handwritten Character Recognition of South Indian Scripts: A Review. (arXiv:1106.0107v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/1106.0107","description":"<p>Handwritten character recognition is always a frontier area of research in\nthe field of pattern recognition and image processing and there is a large\ndemand for OCR on hand written documents. Even though, sufficient studies have\nperformed in foreign scripts like Chinese, Japanese and Arabic characters, only\na very few work can be traced for handwritten character recognition of Indian\nscripts especially for the South Indian scripts. This paper provides an\noverview of offline handwritten character recognition in South Indian Scripts,\nnamely Malayalam, Tamil, Kannada and Telungu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jomy_J/0/1/0/all/0/1\">John Jomy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramod_K/0/1/0/all/0/1\">K. V. Pramod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_B/0/1/0/all/0/1\">Balakrishnan Kannan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Exploring Separable Attention for Multi-Contrast MR Image Super-Resolution. (arXiv:2109.01664v1 [eess.IV])","link":"http://arxiv.org/abs/2109.01664","description":"<p>Super-resolving the Magnetic Resonance (MR) image of a target contrast under\nthe guidance of the corresponding auxiliary contrast, which provides additional\nanatomical information, is a new and effective solution for fast MR imaging.\nHowever, current multi-contrast super-resolution (SR) methods tend to\nconcatenate different contrasts directly, ignoring their relationships in\ndifferent clues, \\eg, in the foreground and background. In this paper, we\npropose a separable attention network (comprising a foreground priority\nattention and background separation attention), named SANet. Our method can\nexplore the foreground and background areas in the forward and reverse\ndirections with the help of the auxiliary contrast, enabling it to learn\nclearer anatomical structures and edge information for the SR of a\ntarget-contrast MR image. SANet provides three appealing benefits: (1) It is\nthe first model to explore a separable attention mechanism that uses the\nauxiliary contrast to predict the foreground and background regions, diverting\nmore attention to refining any uncertain details between these regions and\ncorrecting the fine areas in the reconstructed results. (2) A multi-stage\nintegration module is proposed to learn the response of multi-contrast fusion\nat different stages, obtain the dependency between the fused features, and\nimprove their representation ability. (3) Extensive experiments with various\nstate-of-the-art multi-contrast SR methods on fastMRI and clinical \\textit{in\nvivo} datasets demonstrate the superiority of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1\">Chun-Mei Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yunlu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chengliang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Reliable Are Out-of-Distribution Generalization Methods for Medical Image Segmentation?. (arXiv:2109.01668v1 [eess.IV])","link":"http://arxiv.org/abs/2109.01668","description":"<p>The recent achievements of Deep Learning rely on the test data being similar\nin distribution to the training data. In an ideal case, Deep Learning models\nwould achieve Out-of-Distribution (OoD) Generalization, i.e. reliably make\npredictions on out-of-distribution data. Yet in practice, models usually fail\nto generalize well when facing a shift in distribution. Several methods were\nthereby designed to improve the robustness of the features learned by a model\nthrough Regularization- or Domain-Prediction-based schemes. Segmenting medical\nimages such as MRIs of the hippocampus is essential for the diagnosis and\ntreatment of neuropsychiatric disorders. But these brain images often suffer\nfrom distribution shift due to the patient's age and various pathologies\naffecting the shape of the organ. In this work, we evaluate OoD Generalization\nsolutions for the problem of hippocampus segmentation in MR data using both\nfully- and semi-supervised training. We find that no method performs reliably\nin all experiments. Only the V-REx loss stands out as it remains easy to tune,\nwhile it outperforms a standard U-Net in most cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sanner_A/0/1/0/all/0/1\">Antoine Sanner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Few-Shot Segmentation Via Meta-Learning. (arXiv:2109.01693v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01693","description":"<p>Semantic segmentation is a classic computer vision task with multiple\napplications, which includes medical and remote sensing image analysis. Despite\nrecent advances with deep-based approaches, labeling samples (pixels) for\ntraining models is laborious and, in some cases, unfeasible. In this paper, we\npresent two novel meta learning methods, named WeaSeL and ProtoSeg, for the\nfew-shot semantic segmentation task with sparse annotations. We conducted\nextensive evaluation of the proposed methods in different applications (12\ndatasets) in medical imaging and agricultural remote sensing, which are very\ndistinct fields of knowledge and usually subject to data scarcity. The results\ndemonstrated the potential of our method, achieving suitable results for\nsegmenting both coffee/orange crops and anatomical parts of the human body in\ncomparison with full dense annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gama_P/0/1/0/all/0/1\">Pedro H. T. Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_J/0/1/0/all/0/1\">Jos&#xe9; Marcato Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting 3D ResNets for Video Recognition. (arXiv:2109.01696v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01696","description":"<p>A recent work from Bello shows that training and scaling strategies may be\nmore significant than model architectures for visual recognition. This short\nnote studies effective training and scaling strategies for video recognition\nmodels. We propose a simple scaling strategy for 3D ResNets, in combination\nwith improved training strategies and minor architectural changes. The\nresulting models, termed 3D ResNet-RS, attain competitive performance of 81.0\non Kinetics-400 and 83.8 on Kinetics-600 without pre-training. When pre-trained\non a large Web Video Text dataset, our best model achieves 83.5 and 84.3 on\nKinetics-400 and Kinetics-600. The proposed scaling rule is further evaluated\nin a self-supervised setup using contrastive learning, demonstrating improved\nperformance. Code is available at:\nhttps://github.com/tensorflow/models/tree/master/official.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xianzhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yeqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_I/0/1/0/all/0/1\">Irwan Bello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Accurate Alignment in Real-time 3D Hand-Mesh Reconstruction. (arXiv:2109.01723v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01723","description":"<p>3D hand-mesh reconstruction from RGB images facilitates many applications,\nincluding augmented reality (AR). However, this requires not only real-time\nspeed and accurate hand pose and shape but also plausible mesh-image alignment.\nWhile existing works already achieve promising results, meeting all three\nrequirements is very challenging. This paper presents a novel pipeline by\ndecoupling the hand-mesh reconstruction task into three stages: a joint stage\nto predict hand joints and segmentation; a mesh stage to predict a rough hand\nmesh; and a refine stage to fine-tune it with an offset mesh for mesh-image\nalignment. With careful design in the network structure and in the loss\nfunctions, we can promote high-quality finger-level mesh-image alignment and\ndrive the models together to deliver real-time predictions. Extensive\nquantitative and qualitative results on benchmark datasets demonstrate that the\nquality of our results outperforms the state-of-the-art methods on\nhand-mesh/pose precision and hand-image alignment. In the end, we also showcase\nseveral real-time AR scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigating the Mise-en-Page: Interpretive Machine Learning Approaches to the Visual Layouts of Multi-Ethnic Periodicals. (arXiv:2109.01732v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01732","description":"<p>This paper presents a computational method of analysis that draws from\nmachine learning, library science, and literary studies to map the visual\nlayouts of multi-ethnic newspapers from the late 19th and early 20th century\nUnited States. This work departs from prior approaches to newspapers that focus\non individual pieces of textual and visual content. Our method combines\nChronicling America's MARC data and the Newspaper Navigator machine learning\ndataset to identify the visual patterns of newspaper page layouts. By analyzing\nhigh-dimensional visual similarity, we aim to better understand how editors\nspoke and protested through the layout of their papers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Benjamin Charles Germain Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baco_J/0/1/0/all/0/1\">Joshua Ortiz Baco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salter_S/0/1/0/all/0/1\">Sarah H. Salter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casey_J/0/1/0/all/0/1\">Jim Casey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"F3S: Free Flow Fever Screening. (arXiv:2109.01733v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01733","description":"<p>Identification of people with elevated body temperature can reduce or\ndramatically slow down the spread of infectious diseases like COVID-19. We\npresent a novel fever-screening system, F3S, that uses edge machine learning\ntechniques to accurately measure core body temperatures of multiple individuals\nin a free-flow setting. F3S performs real-time sensor fusion of visual camera\nwith thermal camera data streams to detect elevated body temperature, and it\nhas several unique features: (a) visual and thermal streams represent very\ndifferent modalities, and we dynamically associate semantically-equivalent\nregions across visual and thermal frames by using a new, dynamic alignment\ntechnique that analyzes content and context in real-time, (b) we track people\nthrough occlusions, identify the eye (inner canthus), forehead, face and head\nregions where possible, and provide an accurate temperature reading by using a\nprioritized refinement algorithm, and (c) we robustly detect elevated body\ntemperature even in the presence of personal protective equipment like masks,\nor sunglasses or hats, all of which can be affected by hot weather and lead to\nspurious temperature readings. F3S has been deployed at over a dozen large\ncommercial establishments, providing contact-less, free-flow, real-time fever\nscreening for thousands of employees and customers in indoors and outdoor\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Kunal Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coviello_G/0/1/0/all/0/1\">Giuseppe Coviello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Min Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debnath_B/0/1/0/all/0/1\">Biplob Debnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiung_W/0/1/0/all/0/1\">Wang-Pin Hsiung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaradas_M/0/1/0/all/0/1\">Murugan Sankaradas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_O/0/1/0/all/0/1\">Oliver Po</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drolia_U/0/1/0/all/0/1\">Utsav Drolia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakradhar_S/0/1/0/all/0/1\">Srimat Chakradhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01745","description":"<p>The COVID-19 pandemic raises the problem of adapting face recognition systems\nto the new reality, where people may wear surgical masks to cover their noses\nand mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for\ntraining these systems were released before the pandemic, so they now seem\nunsuited due to the lack of examples of people wearing masks. We propose a\nmethod for enhancing data sets containing faces without masks by creating\nsynthetic masks and overlaying them on faces in the original images. Our method\nrelies on Spark AR Studio, a developer program made by Facebook that is used to\ncreate Instagram face filters. In our approach, we use 9 masks of different\ncolors, shapes and fabrics. We employ our method to generate a number of\n445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254\n(96.8%) masks for the CelebA data set, releasing the mask images at\nhttps://github.com/securifai/masked_faces. We show that our method produces\nsignificantly more realistic training examples of masks overlaid on faces by\nasking volunteers to qualitatively compare it to other methods or data sets\ndesigned for the same task. We also demonstrate the usefulness of our method by\nevaluating state-of-the-art face recognition systems (FaceNet, VGG-face,\nArcFace) trained on the enhanced data sets and showing that they outperform\nequivalent systems trained on the original data sets (containing faces without\nmasks), when the test benchmark contains masked faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mare_T/0/1/0/all/0/1\">Tudor Mare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duta_G/0/1/0/all/0/1\">Georgian Duta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandru_A/0/1/0/all/0/1\">Adrian Sandru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexe_B/0/1/0/all/0/1\">Bogdan Alexe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeNeRF: Disentangled Neural Radiance Fields for Object Categories. (arXiv:2109.01750v1 [cs.GR])","link":"http://arxiv.org/abs/2109.01750","description":"<p>CodeNeRF is an implicit 3D neural representation that learns the variation of\nobject shapes and textures across a category and can be trained, from a set of\nposed images, to synthesize novel views of unseen objects. Unlike the original\nNeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture\nby learning separate embeddings. At test time, given a single unposed image of\nan unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and\nappearance codes via optimization. Unseen objects can be reconstructed from a\nsingle image, and then rendered from new viewpoints or their shape and texture\nedited by varying the latent codes. We conduct experiments on the SRN\nbenchmark, which show that CodeNeRF generalises well to unseen objects and\nachieves on-par performance with methods that require known camera pose at test\ntime. Our results on real-world images demonstrate that CodeNeRF can bridge the\nsim-to-real gap. Project page: \\url{https://github.com/wayne1123/code-nerf}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1\">Wonbong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agapito_L/0/1/0/all/0/1\">Lourdes Agapito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seam Carving Detection and Localization using Two-Stage Deep Neural Networks. (arXiv:2109.01764v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01764","description":"<p>Seam carving is a method to resize an image in a content aware fashion.\nHowever, this method can also be used to carve out objects from images. In this\npaper, we propose a two-step method to detect and localize seam carved images.\nFirst, we build a detector to detect small patches in an image that has been\nseam carved. Next, we compute a heatmap on an image based on the patch\ndetector's output. Using these heatmaps, we build another detector to detect if\na whole image is seam carved or not. Our experimental results show that our\napproach is effective in detecting and localizing seam carved images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nataraj_L/0/1/0/all/0/1\">Lakshmanan Nataraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gudavalli_C/0/1/0/all/0/1\">Chandrakanth Gudavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_T/0/1/0/all/0/1\">Tajuddin Manhar Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1\">Shivkumar Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1\">B.S. Manjunath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To be Critical: Self-Calibrated Weakly Supervised Learning for Salient Object Detection. (arXiv:2109.01770v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01770","description":"<p>Weakly-supervised salient object detection (WSOD) aims to develop saliency\nmodels using image-level annotations. Despite of the success of previous works,\nexplorations on an effective training strategy for the saliency network and\naccurate matches between image-level annotations and salient objects are still\ninadequate. In this work, 1) we propose a self-calibrated training strategy by\nexplicitly establishing a mutual calibration loop between pseudo labels and\nnetwork predictions, liberating the saliency network from error-prone\npropagation caused by pseudo labels. 2) we prove that even a much smaller\ndataset (merely 1.8% of ImageNet) with well-matched annotations can facilitate\nmodels to achieve better performance as well as generalizability. This sheds\nnew light on the development of WSOD and encourages more contributions to the\ncommunity. Comprehensive experiments demonstrate that our method outperforms\nall the existing WSOD methods by adopting the self-calibrated strategy only.\nSteady improvements are further achieved by training on the proposed dataset.\nAdditionally, our method achieves 94.7% of the performance of fully-supervised\nmethods on average. And what is more, the fully supervised models adopting our\npredicted results as \"ground truths\" achieve successful results (95.6% for\nBASNet and 97.3% for ITSD on F-measure), while costing only 0.32% of labeling\ntime for pixel-level annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piao_Y/0/1/0/all/0/1\">Yongri Piao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengxuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PR-Net: Preference Reasoning for Personalized Video Highlight Detection. (arXiv:2109.01799v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01799","description":"<p>Personalized video highlight detection aims to shorten a long video to\ninteresting moments according to a user's preference, which has recently raised\nthe community's attention. Current methods regard the user's history as\nholistic information to predict the user's preference but negating the inherent\ndiversity of the user's interests, resulting in vague preference\nrepresentation. In this paper, we propose a simple yet efficient preference\nreasoning framework (PR-Net) to explicitly take the diverse interests into\naccount for frame-level highlight prediction. Specifically, distinct\nuser-specific preferences for each input query frame are produced, presented as\nthe similarity weighted sum of history highlights to the corresponding query\nframe. Next, distinct comprehensive preferences are formed by the user-specific\npreferences and a learnable generic preference for more overall highlight\nmeasurement. Lastly, the degree of highlight and non-highlight for each query\nframe is calculated as semantic similarity to its comprehensive and\nnon-highlight preferences, respectively. Besides, to alleviate the ambiguity\ndue to the incomplete annotation, a new bi-directional contrastive loss is\nproposed to ensure a compact and differentiable metric space. In this way, our\nmethod significantly outperforms state-of-the-art methods with a relative\nimprovement of 12% in mean accuracy precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runnan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Penghao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenzhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nenglun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Pai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Approach for UAV Small Object Detection with Simulation-based Transfer Learning and Adaptive Fusion. (arXiv:2109.01800v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01800","description":"<p>Precisely detection of Unmanned Aerial Vehicles(UAVs) plays a critical role\nin UAV defense systems. Deep learning is widely adopted for UAV object\ndetection whereas researches on this topic are limited by the amount of dataset\nand small scale of UAV. To tackle these problems, a novel comprehensive\napproach that combines transfer learning based on simulation data and adaptive\nfusion is proposed. Firstly, the open-source plugin AirSim proposed by\nMicrosoft is used to generate mass realistic simulation data. Secondly,\ntransfer learning is applied to obtain a pre-trained YOLOv5 model on the\nsimulated dataset and fine-tuned model on the real-world dataset. Finally, an\nadaptive fusion mechanism is proposed to further improve small object detection\nperformance. Experiment results demonstrate the effectiveness of\nsimulation-based transfer learning which leads to a 2.7% performance increase\non UAV object detection. Furthermore, with transfer learning and adaptive\nfusion mechanism, 7.1% improvement is achieved compared to the original YOLO v5\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rui_C/0/1/0/all/0/1\">Chen Rui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youwei_G/0/1/0/all/0/1\">Guo Youwei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huafei_Z/0/1/0/all/0/1\">Zheng Huafei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongyu_J/0/1/0/all/0/1\">Jiang Hongyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Transfer Learning for Event-based End-task Prediction via Pluggable Event to Image Translation. (arXiv:2109.01801v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01801","description":"<p>Event cameras are novel sensors that perceive the per-pixel intensity changes\nand output asynchronous event streams with high dynamic range and less motion\nblur. It has been shown that events alone can be used for end-task learning,\n\\eg, semantic segmentation, based on encoder-decoder-like networks. However, as\nevents are sparse and mostly reflect edge information, it is difficult to\nrecover original details merely relying on the decoder. Moreover, most methods\nresort to pixel-wise loss alone for supervision, which might be insufficient to\nfully exploit the visual details from sparse events, thus leading to less\noptimal performance. In this paper, we propose a simple yet flexible two-stream\nframework named Dual Transfer Learning (DTL) to effectively enhance the\nperformance on the end-tasks without adding extra inference cost. The proposed\napproach consists of three parts: event to end-task learning (EEL) branch,\nevent to image translation (EIT) branch, and transfer learning (TL) module that\nsimultaneously explores the feature-level affinity information and pixel-level\nknowledge from the EIT branch to improve the EEL branch. This simple yet novel\nmethod leads to strong representation learning from events and is evidenced by\nthe significant performance boost on the end-tasks such as semantic\nsegmentation and depth estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chae_Y/0/1/0/all/0/1\">Yujeong Chae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stimuli-Aware Visual Emotion Analysis. (arXiv:2109.01812v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01812","description":"<p>Visual emotion analysis (VEA) has attracted great attention recently, due to\nthe increasing tendency of expressing and understanding emotions through images\non social networks. Different from traditional vision tasks, VEA is inherently\nmore challenging since it involves a much higher level of complexity and\nambiguity in human cognitive process. Most of the existing methods adopt deep\nlearning techniques to extract general features from the whole image,\ndisregarding the specific features evoked by various emotional stimuli.\nInspired by the \\textit{Stimuli-Organism-Response (S-O-R)} emotion model in\npsychological theory, we proposed a stimuli-aware VEA method consisting of\nthree stages, namely stimuli selection (S), feature extraction (O) and emotion\nprediction (R). First, specific emotional stimuli (i.e., color, object, face)\nare selected from images by employing the off-the-shelf tools. To the best of\nour knowledge, it is the first time to introduce stimuli selection process into\nVEA in an end-to-end network. Then, we design three specific networks, i.e.,\nGlobal-Net, Semantic-Net and Expression-Net, to extract distinct emotional\nfeatures from different stimuli simultaneously. Finally, benefiting from the\ninherent structure of Mikel's wheel, we design a novel hierarchical\ncross-entropy loss to distinguish hard false examples from easy ones in an\nemotion-specific manner. Experiments demonstrate that the proposed method\nconsistently outperforms the state-of-the-art approaches on four public visual\nemotion datasets. Ablation study and visualizations further prove the validity\nand interpretability of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiumei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RiWNet: A moving object instance segmentation Network being Robust in adverse Weather conditions. (arXiv:2109.01820v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01820","description":"<p>Segmenting each moving object instance in a scene is essential for many\napplications. But like many other computer vision tasks, this task performs\nwell in optimal weather, but then adverse weather tends to fail. To be robust\nin weather conditions, the usual way is to train network in data of given\nweather pattern or to fuse multiple sensors. We focus on a new possibility,\nthat is, to improve its resilience to weather interference through the\nnetwork's structural design. First, we propose a novel FPN structure called\nRiWFPN with a progressive top-down interaction and attention refinement module.\nRiWFPN can directly replace other FPN structures to improve the robustness of\nthe network in non-optimal weather conditions. Then we extend SOLOV2 to capture\ntemporal information in video to learn motion information, and propose a moving\nobject instance segmentation network with RiWFPN called RiWNet. Finally, in\norder to verify the effect of moving instance segmentation in different weather\ndisturbances, we propose a VKTTI-moving dataset which is a moving instance\nsegmentation dataset based on the VKTTI dataset, taking into account different\nweather scenes such as rain, fog, sunset, morning as well as overcast. The\nexperiment proves how RiWFPN improves the network's resilience to adverse\nweather effects compared to other FPN structures. We compare RiWNet to several\nother state-of-the-art methods in some challenging datasets, and RiWNet shows\nbetter performance especially under adverse weather conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Spatial-Temporal Graph Convolutional Networks with Domain Generalization for Sleep Stage Classification. (arXiv:2109.01824v1 [eess.SP])","link":"http://arxiv.org/abs/2109.01824","description":"<p>Sleep stage classification is essential for sleep assessment and disease\ndiagnosis. Although previous attempts to classify sleep stages have achieved\nhigh classification performance, several challenges remain open: 1) How to\neffectively utilize time-varying spatial and temporal features from\nmulti-channel brain signals remains challenging. Prior works have not been able\nto fully utilize the spatial topological information among brain regions. 2)\nDue to the many differences found in individual biological signals, how to\novercome the differences of subjects and improve the generalization of deep\nneural networks is important. 3) Most deep learning methods ignore the\ninterpretability of the model to the brain. To address the above challenges, we\npropose a multi-view spatial-temporal graph convolutional networks (MSTGCN)\nwith domain generalization for sleep stage classification. Specifically, we\nconstruct two brain view graphs for MSTGCN based on the functional connectivity\nand physical distance proximity of the brain regions. The MSTGCN consists of\ngraph convolutions for extracting spatial features and temporal convolutions\nfor capturing the transition rules among sleep stages. In addition, attention\nmechanism is employed for capturing the most relevant spatial-temporal\ninformation for sleep stage classification. Finally, domain generalization and\nMSTGCN are integrated into a unified framework to extract subject-invariant\nsleep features. Experiments on two public datasets demonstrate that the\nproposed model outperforms the state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jia_Z/0/1/0/all/0/1\">Ziyu Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Youfang Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ning_X/0/1/0/all/0/1\">Xiaojun Ning</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yuanlai He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_R/0/1/0/all/0/1\">Ronghao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lehman_L/0/1/0/all/0/1\">Li-wei H. Lehman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GOHOME: Graph-Oriented Heatmap Output forfuture Motion Estimation. (arXiv:2109.01827v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01827","description":"<p>In this paper, we propose GOHOME, a method leveraging graph representations\nof the High Definition Map and sparse projections to generate a heatmap output\nrepresenting the future position probability distribution for a given agent in\na traffic scene. This heatmap output yields an unconstrained 2D grid\nrepresentation of agent future possible locations, allowing inherent\nmultimodality and a measure of the uncertainty of the prediction. Our\ngraph-oriented model avoids the high computation burden of representing the\nsurrounding context as squared images and processing it with classical CNNs,\nbut focuses instead only on the most probable lanes where the agent could end\nup in the immediate future. GOHOME reaches 3$rd$ on Argoverse Motion\nForecasting Benchmark on the MissRate$_6$ metric while achieving significant\nspeed-up and memory burden diminution compared to 1$^{st}$ place method HOME.\nWe also highlight that heatmap output enables multimodal ensembling and improve\n1$^{st}$ place MissRate$_6$ by more than 15$\\%$ with our best ensemble.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilles_T/0/1/0/all/0/1\">Thomas Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabatini_S/0/1/0/all/0/1\">Stefano Sabatini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OCTAVA: an open-source toolbox for quantitative analysis of optical coherence tomography angiography images. (arXiv:2109.01835v1 [eess.IV])","link":"http://arxiv.org/abs/2109.01835","description":"<p>Optical coherence tomography angiography (OCTA) performs non-invasive\nvisualization and characterization of microvasculature in research and clinical\napplications mainly in ophthalmology and dermatology. A wide variety of\ninstruments, imaging protocols, processing methods and metrics have been used\nto describe the microvasculature, such that comparing different study outcomes\nis currently not feasible. With the goal of contributing to standardization of\nOCTA data analysis, we report a user-friendly, open-source toolbox, OCTAVA\n(OCTA Vascular Analyzer), to automate the pre-processing, segmentation, and\nquantitative analysis of en face OCTA maximum intensity projection images in a\nstandardized workflow. We present each analysis step, including optimization of\nfiltering and choice of segmentation algorithm, and definition of metrics. We\nperform quantitative analysis of OCTA images from different commercial and\nnon-commercial instruments and samples and show OCTAVA can accurately and\nreproducibly determine metrics for characterization of microvasculature. Wide\nadoption could enable studies and aggregation of data on a scale sufficient to\ndevelop reliable microvascular biomarkers for early detection, and to guide\ntreatment, of microvascular disease.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Untracht_G/0/1/0/all/0/1\">Gavrielle R. Untracht</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matos_R/0/1/0/all/0/1\">Rolando Matos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dikaios_N/0/1/0/all/0/1\">Nikolaos Dikaios</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bapir_M/0/1/0/all/0/1\">Mariam Bapir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Durrani_A/0/1/0/all/0/1\">Abdullah K. Durrani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Butsabong_T/0/1/0/all/0/1\">Teemapron Butsabong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campagnolo_P/0/1/0/all/0/1\">Paola Campagnolo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sampson_D/0/1/0/all/0/1\">David D. Sampson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heiss_C/0/1/0/all/0/1\">Christian Heiss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sampson_D/0/1/0/all/0/1\">Danuta M. Sampson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAMA: A Rapid Multicut Algorithm on GPU. (arXiv:2109.01838v1 [cs.DC])","link":"http://arxiv.org/abs/2109.01838","description":"<p>We propose a highly parallel primal-dual algorithm for the multicut (a.k.a.\ncorrelation clustering) problem, a classical graph clustering problem widely\nused in machine learning and computer vision. Our algorithm consists of three\nsteps executed recursively: (1) Finding conflicted cycles that correspond to\nviolated inequalities of the underlying multicut relaxation, (2) Performing\nmessage passing between the edges and cycles to optimize the Lagrange\nrelaxation coming from the found violated cycles producing reduced costs and\n(3) Contracting edges with high reduced costs through matrix-matrix\nmultiplications.\n</p>\n<p>Our algorithm produces primal solutions and dual lower bounds that estimate\nthe distance to optimum. We implement our algorithm on GPUs and show resulting\none to two order-of-magnitudes improvements in execution speed without\nsacrificing solution quality compared to traditional serial algorithms that run\non CPUs. We can solve very large scale benchmark problems with up to\n$\\mathcal{O}(10^8)$ variables in a few seconds with small primal-dual gaps. We\nmake our code available at https://github.com/pawelswoboda/RAMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1\">Ahmed Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Expressive Communication with Internet Memes: A New Multimodal Conversation Dataset and Benchmark. (arXiv:2109.01839v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01839","description":"<p>As a kind of new expression elements, Internet memes are popular and\nextensively used in online chatting scenarios since they manage to make\ndialogues vivid, moving, and interesting. However, most current dialogue\nresearches focus on text-only dialogue tasks. In this paper, we propose a new\ntask named as \\textbf{M}eme incorporated \\textbf{O}pen-domain \\textbf{D}ialogue\n(MOD). Compared to previous dialogue tasks, MOD is much more challenging since\nit requires the model to understand the multimodal elements as well as the\nemotions behind them. To facilitate the MOD research, we construct a\nlarge-scale open-domain multimodal dialogue dataset incorporating abundant\nInternet memes into utterances. The dataset consists of $\\sim$45K Chinese\nconversations with $\\sim$606K utterances. Each conversation contains about $13$\nutterances with about $4$ Internet memes on average and each utterance equipped\nwith an Internet meme is annotated with the corresponding emotion. In addition,\nwe present a simple and effective method, which utilizes a unified generation\nnetwork to solve the MOD task. Experimental results demonstrate that our method\ntrained on the proposed corpus is able to achieve expressive communication\nincluding texts and memes. The corpus and models have been publicly available\nat https://github.com/lizekang/DSTC10-MOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1\">Zhengcong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Privacy-Preserving Image Retrieval Scheme Using A Codebook Generated From Independent Plain-Image Dataset. (arXiv:2109.01841v1 [eess.IV])","link":"http://arxiv.org/abs/2109.01841","description":"<p>In this paper, we propose a privacy-preserving image-retrieval scheme using a\ncodebook generated by using a plain-image dataset. Encryption-then-compression\n(EtC) images, which were proposed for EtC systems, have been used in\nconventional privacy-preserving image-retrieval schemes, in which a codebook is\ngenerated from EtC images uploaded by image owners, and extended SIMPLE\ndescriptors are then calculated as image descriptors by using the codebook. In\ncontrast, in the proposed scheme, a codebook is generated from a dataset\nindependent of uploaded images. The use of an independent dataset enables us\nnot only to use a codebook that does not require recalculation but also to\nconstantly provide a high retrieval accuracy. In an experiment, the proposed\nscheme is demonstrated to maintain a high retrieval performance, even if\ncodebooks are generated from a plain image dataset independent of image owners'\nencrypted images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Iida_K/0/1/0/all/0/1\">Kenta Iida</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On robustness of generative representations against catastrophic forgetting. (arXiv:2109.01844v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01844","description":"<p>Catastrophic forgetting of previously learned knowledge while learning new\ntasks is a widely observed limitation of contemporary neural networks. Although\nmany continual learning methods are proposed to mitigate this drawback, the\nmain question remains unanswered: what is the root cause of catastrophic\nforgetting? In this work, we aim at answering this question by posing and\nvalidating a set of research hypotheses related to the specificity of\nrepresentations built internally by neural models. More specifically, we design\na set of empirical evaluations that compare the robustness of representations\nin discriminative and generative models against catastrophic forgetting. We\nobserve that representations learned by discriminative models are more prone to\ncatastrophic forgetting than their generative counterparts, which sheds new\nlight on the advantages of developing generative models for continual learning.\nFinally, our work opens new research pathways and possibilities to adopt\ngenerative models in continual learning beyond mere replay mechanisms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Masarczyk_W/0/1/0/all/0/1\">Wojciech Masarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deja_K/0/1/0/all/0/1\">Kamil Deja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering. (arXiv:2109.01847v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01847","description":"<p>Implicit neural rendering techniques have shown promising results for novel\nview synthesis. However, existing methods usually encode the entire scene as a\nwhole, which is generally not aware of the object identity and limits the\nability to the high-level editing tasks such as moving or adding furniture. In\nthis paper, we present a novel neural scene rendering system, which learns an\nobject-compositional neural radiance field and produces realistic rendering\nwith editing capability for a clustered and real-world scene. Specifically, we\ndesign a novel two-pathway architecture, in which the scene branch encodes the\nscene geometry and appearance, and the object branch encodes each standalone\nobject conditioned on learnable object activation codes. To survive the\ntraining in heavily cluttered scenes, we propose a scene-guided training\nstrategy to solve the 3D space ambiguity in the occluded regions and learn\nsharp boundaries for each object. Extensive experiments demonstrate that our\nsystem not only achieves competitive performance for static scene novel-view\nsynthesis, but also produces realistic rendering for object-level editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bangbang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yijin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting isocitrate dehydrogenase mutationstatus in glioma using structural brain networksand graph neural networks. (arXiv:2109.01854v1 [eess.IV])","link":"http://arxiv.org/abs/2109.01854","description":"<p>Glioma is a common malignant brain tumor that shows distinct survival among\npatients. The isocitrate dehydrogenase (IDH) gene mutation status provides\ncritical diagnostic and prognostic value for glioma and is now accepted as the\nstandard of care. A non-invasive prediction of IDH mutation based on the\npre-treatment MRI has crucial clinical significance. Machine learning and deep\nlearning models show reasonable performance in predicting IDH mutation status.\nHowever, most models neglect the systematic brain alterations caused by tumor\ninvasion, where the infiltration along white matter tracts throughout the brain\nis identified as a hallmark of glioma. Structural brain network provides an\neffective tool to characterise brain organisation, which could be captured by\nthe graph neural networks (GNN) for a more accurate prediction of IDH mutation\nstatus.\n</p>\n<p>Here we propose a method to predict the IDH mutation using GNN, based on the\nstructural brain network of patients. Specifically, we firstly construct a\nnetwork template of healthy subjects, which consists of atlases of edges (white\nmatter tracts) and nodes (cortical and subcortical brain regions) to provide\nregions of interest (ROI). Next, we employ autoencoders to extract the latent\nmulti-modal MRI features from the ROIs of the edge and node in patients. These\nfeatures of edge and node of brain networks are used to train a GNN\narchitecture in predicting IDH mutation status. The results show that the\nproposed method outperforms the baseline models using 3D-CNN and 3D-DenseNet.\nIn addition, the model interpretation suggests its ability to identify the\ntracts infiltrated by tumor and corresponds to clinical prior knowledge. In\nconclusion, integrating brain networks with GNN offers a new avenue to study\nbrain lesions using computational neuroscience and computer vision approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1\">Yiran Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yonghao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Price_S/0/1/0/all/0/1\">Stephen J. Price</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatiotemporal Inconsistency Learning for DeepFake Video Detection. (arXiv:2109.01860v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01860","description":"<p>The rapid development of facial manipulation techniques has aroused public\nconcerns in recent years. Following the success of deep learning, existing\nmethods always formulate DeepFake video detection as a binary classification\nproblem and develop frame-based and video-based solutions. However, little\nattention has been paid to capturing the spatial-temporal inconsistency in\nforged videos. To address this issue, we term this task as a Spatial-Temporal\nInconsistency Learning (STIL) process and instantiate it into a novel STIL\nblock, which consists of a Spatial Inconsistency Module (SIM), a Temporal\nInconsistency Module (TIM), and an Information Supplement Module (ISM).\nSpecifically, we present a novel temporal modeling paradigm in TIM by\nexploiting the temporal difference over adjacent frames along with both\nhorizontal and vertical directions. And the ISM simultaneously utilizes the\nspatial information from SIM and temporal information from TIM to establish a\nmore comprehensive spatial-temporal representation. Moreover, our STIL block is\nflexible and could be plugged into existing 2D CNNs. Extensive experiments and\nvisualizations are presented to demonstrate the effectiveness of our method\nagainst the state-of-the-art competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhihao Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Mitosis Detection Using a Cascade Mask-RCNN Approach With Domain-Specific Residual Cycle-GAN Data Augmentation. (arXiv:2109.01878v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01878","description":"<p>For the MIDOG mitosis detection challenge, we created a cascade algorithm\nconsisting of a Mask-RCNN detector, followed by a classification ensemble\nconsisting of ResNet50 and DenseNet201 to refine detected mitotic candidates.\nThe MIDOG training data consists of 200 frames originating from four scanners,\nthree of which are annotated for mitotic instances with centroid annotations.\nOur main algorithmic choices are as follows: first, to enhance the\ngeneralizability of our detector and classification networks, we use a\nstate-of-the-art residual Cycle-GAN to transform each scanner domain to every\nother scanner domain. During training, we then randomly load, for each image,\none of the four domains. In this way, our networks can learn from the fourth\nnon-annotated scanner domain even if we don't have annotations for it. Second,\nfor training the detector network, rather than using centroid-based fixed-size\nbounding boxes, we create mitosis-specific bounding boxes. We do this by\nmanually annotating a small selection of mitoses, training a Mask-RCNN on this\nsmall dataset, and applying it to the rest of the data to obtain full\nannotations. We trained the follow-up classification ensemble using only the\nchallenge-provided positive and hard-negative examples. On the preliminary test\nset, the algorithm scores an F1 score of 0.7578, putting us as the second-place\nteam on the leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_G/0/1/0/all/0/1\">Gauthier Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dedieu_J/0/1/0/all/0/1\">Jules Dedieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertrand_C/0/1/0/all/0/1\">Capucine Bertrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshayedi_A/0/1/0/all/0/1\">Alireza Moshayedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mammadov_A/0/1/0/all/0/1\">Ali Mammadov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petit_S/0/1/0/all/0/1\">St&#xe9;phanie Petit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadj_S/0/1/0/all/0/1\">Saima Ben Hadj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fick_R/0/1/0/all/0/1\">Rutger H.J. Fick</a> (Tribvn Healthcare)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moving Object Detection for Event-based Vision using k-means Clustering. (arXiv:2109.01879v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01879","description":"<p>Moving object detection is a crucial task in computer vision. Event-based\ncameras are bio-inspired cameras that work by mimicking the working of the\nhuman eye. These cameras have multiple advantages over conventional frame-based\ncameras, like reduced latency, HDR, reduced motion blur during high motion, low\npower consumption, etc. However, these advantages come at a high cost, as\nevent-based cameras are noise sensitive and have low resolution. Moreover, the\ntask of moving object detection in these cameras is difficult, as event-based\nsensors capture only the binary changes in brightness of a scene, lacking\nuseful visual features like texture and color. In this paper, we investigate\nthe application of the k-means clustering technique in detecting moving objects\nin event-based data. Experimental results in publicly available datasets using\nk-means show significant improvement in performance over the state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1\">Anindya Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mayukhmali Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning facilitates fully automated brain image registration of optoacoustic tomography and magnetic resonance imaging. (arXiv:2109.01880v1 [eess.IV])","link":"http://arxiv.org/abs/2109.01880","description":"<p>Multi-spectral optoacoustic tomography (MSOT) is an emerging optical imaging\nmethod providing multiplex molecular and functional information from the rodent\nbrain. It can be greatly augmented by magnetic resonance imaging (MRI) that\noffers excellent soft-tissue contrast and high-resolution brain anatomy.\nNevertheless, registration of multi-modal images remains challenging, chiefly\ndue to the entirely different image contrast rendered by these modalities.\nPreviously reported registration algorithms mostly relied on manual\nuser-dependent brain segmentation, which compromised data interpretation and\naccurate quantification. Here we propose a fully automated registration method\nfor MSOT-MRI multimodal imaging empowered by deep learning. The automated\nworkflow includes neural network-based image segmentation to generate suitable\nmasks, which are subsequently registered using an additional neural network.\nPerformance of the algorithm is showcased with datasets acquired by\ncross-sectional MSOT and high-field MRI preclinical scanners. The automated\nregistration method is further validated with manual and half-automated\nregistration, demonstrating its robustness and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yexing Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lafci_B/0/1/0/all/0/1\">Berkan Lafci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luzgin_A/0/1/0/all/0/1\">Artur Luzgin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klohs_J/0/1/0/all/0/1\">Jan Klohs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dean_Ben_X/0/1/0/all/0/1\">Xose Luis Dean-Ben</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_R/0/1/0/all/0/1\">Ruiqing Ni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razansky_D/0/1/0/all/0/1\">Daniel Razansky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_W/0/1/0/all/0/1\">Wuwei Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Image-Anomaly Mitigation for Autonomous Mobile Robots. (arXiv:2109.01889v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01889","description":"<p>Camera anomalies like rain or dust can severelydegrade image quality and its\nrelated tasks, such as localizationand segmentation. In this work we address\nthis importantissue by implementing a pre-processing step that can\neffectivelymitigate such artifacts in a real-time fashion, thus supportingthe\ndeployment of autonomous systems with limited computecapabilities. We propose a\nshallow generator with aggregation,trained in an adversarial setting to solve\nthe ill-posed problemof reconstructing the occluded regions. We add an enhancer\ntofurther preserve high-frequency details and image colorization.We also\nproduce one of the largest publicly available datasets1to train our\narchitecture and use realistic synthetic raindrops toobtain an improved\ninitialization of the model. We benchmarkour framework on existing datasets and\non our own imagesobtaining state-of-the-art results while enabling real-time\nper-formance, with up to 40x faster inference time than existingapproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fumagalli_G/0/1/0/all/0/1\">Gianmario Fumagalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_Y/0/1/0/all/0/1\">Yannick Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dymczyk_M/0/1/0/all/0/1\">Marcin Dymczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1\">Roland Siegwart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dube_R/0/1/0/all/0/1\">Renaud Dub&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust fine-tuning of zero-shot models. (arXiv:2109.01903v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01903","description":"<p>Large pre-trained models such as CLIP offer consistent accuracy across a\nrange of data distributions when performing zero-shot inference (i.e., without\nfine-tuning on a specific dataset). Although existing fine-tuning approaches\nsubstantially improve accuracy in-distribution, they also reduce\nout-of-distribution robustness. We address this tension by introducing a simple\nand effective method for improving robustness: ensembling the weights of the\nzero-shot and fine-tuned models. Compared to standard fine-tuning, the\nresulting weight-space ensembles provide large accuracy improvements\nout-of-distribution, while matching or improving in-distribution accuracy. On\nImageNet and five derived distribution shifts, weight-space ensembles improve\nout-of-distribution accuracy by 2 to 10 percentage points while increasing\nin-distribution accuracy by nearly 1 percentage point relative to standard\nfine-tuning. These improvements come at no additional computational cost during\nfine-tuning or inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mike Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong Wook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1\">Hongseok Namkoong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Spatial Attention Network for Semantic Segmentation. (arXiv:2109.01915v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01915","description":"<p>The spatial attention mechanism captures long-range dependencies by\naggregating global contextual information to each query location, which is\nbeneficial for semantic segmentation. In this paper, we present a sparse\nspatial attention network (SSANet) to improve the efficiency of the spatial\nattention mechanism without sacrificing the performance. Specifically, a sparse\nnon-local (SNL) block is proposed to sample a subset of key and value elements\nfor each query element to capture long-range relations adaptively and generate\na sparse affinity matrix to aggregate contextual information efficiently.\nExperimental results show that the proposed approach outperforms other context\naggregation methods and achieves state-of-the-art performance on the\nCityscapes, PASCAL Context and ADE20K datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hujun Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Visual Transformer Based Crowd Counting. (arXiv:2109.01926v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01926","description":"<p>Crowd estimation is a very challenging problem. The most recent study tries\nto exploit auditory information to aid the visual models, however, the\nperformance is limited due to the lack of an effective approach for feature\nextraction and integration. The paper proposes a new audiovisual multi-task\nnetwork to address the critical challenges in crowd counting by effectively\nutilizing both visual and audio inputs for better modalities association and\nproductive feature extraction. The proposed network introduces the notion of\nauxiliary and explicit image patch-importance ranking (PIR) and patch-wise\ncrowd estimate (PCE) information to produce a third (run-time) modality. These\nmodalities (audio, visual, run-time) undergo a transformer-inspired\ncross-modality co-attention mechanism to finally output the crowd estimate. To\nacquire rich visual features, we propose a multi-branch structure with\ntransformer-style fusion in-between. Extensive experimental evaluations show\nthat the proposed scheme outperforms the state-of-the-art networks under all\nevaluation settings with up to 33.8% improvement. We also analyze and compare\nthe vision-only variant of our network and empirically demonstrate its\nsuperiority over previous approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sajid_U/0/1/0/all/0/1\">Usman Sajid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajid_H/0/1/0/all/0/1\">Hasan Sajid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taejoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ISyNet: Convolutional Neural Networks design for AI accelerator. (arXiv:2109.01932v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01932","description":"<p>In recent years Deep Learning reached significant results in many practical\nproblems, such as computer vision, natural language processing, speech\nrecognition and many others. For many years the main goal of the research was\nto improve the quality of models, even if the complexity was impractically\nhigh. However, for the production solutions, which often require real-time\nwork, the latency of the model plays a very important role. Current\nstate-of-the-art architectures are found with neural architecture search (NAS)\ntaking model complexity into account. However, designing of the search space\nsuitable for specific hardware is still a challenging task. To address this\nproblem we propose a measure of hardware efficiency of neural architecture\nsearch space - matrix efficiency measure (MEM); a search space comprising of\nhardware-efficient operations; a latency-aware scaling method; and ISyNet - a\nset of architectures designed to be fast on the specialized neural processing\nunit (NPU) hardware and accurate at the same time. We show the advantage of the\ndesigned architectures for the NPU devices on ImageNet and the generalization\nability for the downstream classification and detection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Letunovskiy_A/0/1/0/all/0/1\">Alexey Letunovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korviakov_V/0/1/0/all/0/1\">Vladimir Korviakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polovnikov_V/0/1/0/all/0/1\">Vladimir Polovnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kargapoltseva_A/0/1/0/all/0/1\">Anastasiia Kargapoltseva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazurenko_I/0/1/0/all/0/1\">Ivan Mazurenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yepan Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Relative Spatial Reasoning for Visual Question Answering. (arXiv:2109.01934v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01934","description":"<p>Vision-and-language (V\\&amp;L) reasoning necessitates perception of visual\nconcepts such as objects and actions, understanding semantics and language\ngrounding, and reasoning about the interplay between the two modalities. One\ncrucial aspect of visual reasoning is spatial understanding, which involves\nunderstanding relative locations of objects, i.e.\\ implicitly learning the\ngeometry of the scene. In this work, we evaluate the faithfulness of V\\&amp;L\nmodels to such geometric understanding, by formulating the prediction of\npair-wise relative locations of objects as a classification as well as a\nregression task. Our findings suggest that state-of-the-art transformer-based\nV\\&amp;L models lack sufficient abilities to excel at this task. Motivated by this,\nwe design two objectives as proxies for 3D spatial reasoning (SR) -- object\ncentroid estimation, and relative position estimation, and train V\\&amp;L with weak\nsupervision from off-the-shelf depth estimators. This leads to considerable\nimprovements in accuracy for the \"GQA\" visual question answering challenge (in\nfully supervised, few-shot, and O.O.D settings) as well as improvements in\nrelative spatial reasoning. Code and data will be released\n\\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilizing Adversarial Targeted Attacks to Boost Adversarial Robustness. (arXiv:2109.01945v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01945","description":"<p>Adversarial attacks have been shown to be highly effective at degrading the\nperformance of deep neural networks (DNNs). The most prominent defense is\nadversarial training, a method for learning a robust model. Nevertheless,\nadversarial training does not make DNNs immune to adversarial perturbations. We\npropose a novel solution by adopting the recently suggested Predictive\nNormalized Maximum Likelihood. Specifically, our defense performs adversarial\ntargeted attacks according to different hypotheses, where each hypothesis\nassumes a specific label for the test sample. Then, by comparing the hypothesis\nprobabilities, we predict the label. Our refinement process corresponds to\nrecent findings of the adversarial subspace properties. We extensively evaluate\nour approach on 16 adversarial attack benchmarks using ResNet-50,\nWideResNet-28, and a2-layer ConvNet trained with ImageNet, CIFAR10, and MNIST,\nshowing a significant improvement of up to 5.7%, 3.7%, and 0.6% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pesso_U/0/1/0/all/0/1\">Uriya Pesso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibas_K/0/1/0/all/0/1\">Koby Bibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_M/0/1/0/all/0/1\">Meir Feder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment. (arXiv:2109.01949v1 [cs.LG])","link":"http://arxiv.org/abs/2109.01949","description":"<p>Self-supervised learning provides an opportunity to explore unlabeled chest\nX-rays and their associated free-text reports accumulated in clinical routine\nwithout manual supervision. This paper proposes a Joint Image Text\nRepresentation Learning Network (JoImTeRNet) for pre-training on chest X-ray\nimages and their radiology reports. The model was pre-trained on both the\nglobal image-sentence level and the local image region-word level for\nvisual-textual matching. Both are bidirectionally constrained on Cross-Entropy\nbased and ranking-based Triplet Matching Losses. The region-word matching is\ncalculated using the attention mechanism without direct supervision about their\nmapping. The pre-trained multi-modal representation learning paves the way for\ndownstream tasks concerning image and/or text encoding. We demonstrate the\nrepresentation learning quality by cross-modality retrievals and multi-label\nclassifications on two datasets: OpenI-IU and MIMIC-CXR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1\">Mohammad Abuzar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1\">Dana Moukheiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur Srihari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Saliency Prior for Reducing Visual Distraction. (arXiv:2109.01980v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01980","description":"<p>Using only a model that was trained to predict where people look at images,\nand no additional training data, we can produce a range of powerful editing\neffects for reducing distraction in images. Given an image and a mask\nspecifying the region to edit, we backpropagate through a state-of-the-art\nsaliency model to parameterize a differentiable editing operator, such that the\nsaliency within the masked region is reduced. We demonstrate several operators,\nincluding: a recoloring operator, which learns to apply a color transform that\ncamouflages and blends distractors into their surroundings; a warping operator,\nwhich warps less salient image regions to cover distractors, gradually\ncollapsing objects into themselves and effectively removing them (an effect\nakin to inpainting); a GAN operator, which uses a semantic prior to fully\nreplace image regions with plausible, less salient alternatives. The resulting\neffects are consistent with cognitive research on the human visual system\n(e.g., since color mismatch is salient, the recoloring operator learns to\nharmonize objects' colors with their surrounding to reduce their saliency),\nand, importantly, are all achieved solely through the guidance of the\npretrained saliency model, with no additional supervision. We present results\non a variety of natural images and conduct a perceptual study to evaluate and\nvalidate the changes in viewers' eye-gaze between the original images and our\nedited results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1\">Kfir Aberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandelsman_Y/0/1/0/all/0/1\">Yossi Gandelsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosseri_I/0/1/0/all/0/1\">Inbar Mosseri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">David E. Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohlhoff_K/0/1/0/all/0/1\">Kai Kohlhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1\">Yael Pritch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_M/0/1/0/all/0/1\">Michael Rubinstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Compression with Recurrent Neural Network and Generalized Divisive Normalization. (arXiv:2109.01999v1 [eess.IV])","link":"http://arxiv.org/abs/2109.01999","description":"<p>Image compression is a method to remove spatial redundancy between adjacent\npixels and reconstruct a high-quality image. In the past few years, deep\nlearning has gained huge attention from the research community and produced\npromising image reconstruction results. Therefore, recent methods focused on\ndeveloping deeper and more complex networks, which significantly increased\nnetwork complexity. In this paper, two effective novel blocks are developed:\nanalysis and synthesis block that employs the convolution layer and Generalized\nDivisive Normalization (GDN) in the variable-rate encoder and decoder side. Our\nnetwork utilizes a pixel RNN approach for quantization. Furthermore, to improve\nthe whole network, we encode a residual image using LSTM cells to reduce\nunnecessary information. Experimental results demonstrated that the proposed\nvariable-rate framework with novel blocks outperforms existing methods and\nstandard image codecs, such as George's ~\\cite{002} and JPEG in terms of image\nsimilarity. The project page along with code and models are available at\nhttps://khawar512.github.io/cvpr/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Islam_K/0/1/0/all/0/1\">Khawar Islam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dang_L/0/1/0/all/0/1\">L. Minh Dang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Sujin Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moon_H/0/1/0/all/0/1\">Hyeonjoon Moon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse-MLP: A Fully-MLP Architecture with Conditional Computation. (arXiv:2109.02008v1 [cs.LG])","link":"http://arxiv.org/abs/2109.02008","description":"<p>Mixture of Experts (MoE) with sparse conditional computation has been proved\nan effective architecture for scaling attention-based models to more parameters\nwith comparable computation cost. In this paper, we propose Sparse-MLP, scaling\nthe recent MLP-Mixer model with sparse MoE layers, to achieve a more\ncomputation-efficient architecture. We replace a subset of dense MLP blocks in\nthe MLP-Mixer model with Sparse blocks. In each Sparse block, we apply two\nstages of MoE layers: one with MLP experts mixing information within channels\nalong image patch dimension, one with MLP experts mixing information within\npatches along the channel dimension. Besides, to reduce computational cost in\nrouting and improve experts capacity, we design Re-represent layers in each\nSparse block. These layers are to re-scale image representations by two simple\nbut effective linear transformations. By pre-training on ImageNet-1k with MoCo\nv3 algorithm, our models can outperform dense MLP models with comparable\nparameters and less computational cost on several downstream image\nclassification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigational Path-Planning For All-Terrain Autonomous Agricultural Robot. (arXiv:2109.02015v1 [cs.RO])","link":"http://arxiv.org/abs/2109.02015","description":"<p>The shortage of workforce and increasing cost of maintenance has forced many\nfarm industrialists to shift towards automated and mechanized approaches. The\nkey component for autonomous systems is the path planning techniques used.\nCoverage path planning (CPP) algorithm is used for navigating over farmlands to\nperform various agricultural operations such as seeding, ploughing, or spraying\npesticides and fertilizers. This report paper compares novel algorithms for\nautonomous navigation of farmlands. For reduction of navigational constraints,\na high-resolution grid map representation is taken into consideration specific\nto Indian environments. The free space is covered by distinguishing the grid\ncells as covered, unexplored, partially explored and presence of an obstacle.\nThe performance of the compared algorithms is evaluated with metrics such as\ntime efficiency, space efficiency, accuracy, and robustness to changes in the\nenvironment. Robotic Operating System (ROS), Dassault Systemes Experience\nPlatform (3DS Experience), MATLAB along Python were used for the simulation of\nthe compared algorithms. The results proved the applicability of the algorithms\nfor autonomous field navigation and feasibility with robotic path planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghodke_V/0/1/0/all/0/1\">Vedant Ghodke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Efficient Masked Language Modeling for Vision and Language. (arXiv:2109.02040v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02040","description":"<p>Masked language modeling (MLM) is one of the key sub-tasks in vision-language\npretraining. In the cross-modal setting, tokens in the sentence are masked at\nrandom, and the model predicts the masked tokens given the image and the text.\nIn this paper, we observe several key disadvantages of MLM in this setting.\nFirst, as captions tend to be short, in a third of the sentences no token is\nsampled. Second, the majority of masked tokens are stop-words and punctuation,\nleading to under-utilization of the image. We investigate a range of\nalternative masking strategies specific to the cross-modal setting that address\nthese shortcomings, aiming for better fusion of text and image in the learned\nrepresentation. When pre-training the LXMERT model, our alternative masking\nstrategies consistently improve over the original masking strategy on three\ndownstream tasks, especially in low resource settings. Further, our\npre-training approach substantially outperforms the baseline model on a\nprompt-based probing task designed to elicit image objects. These results and\nour analysis indicate that our method allows for better utilization of the\ntraining data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhadad_M/0/1/0/all/0/1\">Michael Elhadad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensor Data Augmentation with Resampling for Contrastive Learning in Human Activity Recognition. (arXiv:2109.02054v1 [cs.HC])","link":"http://arxiv.org/abs/2109.02054","description":"<p>Human activity recognition plays an increasingly important role not only in\nour daily lives, but also in the medical and rehabilitation fields. The\ndevelopment of deep learning has also contributed to the advancement of human\nactivity recognition, but the large amount of data annotation work required to\ntrain deep learning models is a major obstacle to the development of human\nactivity recognition. Contrastive learning has started to be used in the field\nof sensor-based human activity recognition due to its ability to avoid the cost\nof labeling large datasets and its ability to better distinguish between sample\nrepresentations of different instances. Among them, data augmentation, an\nimportant part of contrast learning, has a significant impact on model\neffectiveness, but current data augmentation methods do not perform too\nsuccessfully in contrast learning frameworks for wearable sensor-based activity\nrecognition. To optimize the effect of contrast learning models, in this paper,\nwe investigate the sampling frequency of sensors and propose a resampling data\naugmentation method. In addition, we also propose a contrast learning framework\nbased on human activity recognition and apply the resampling augmentation\nmethod to the data augmentation phase of contrast learning. The experimental\nresults show that the resampling augmentation method outperforms supervised\nlearning by 9.88% on UCI HAR and 7.69% on Motion Sensor in the fine-tuning\nevaluation of contrast learning with a small amount of labeled data, and also\nreveal that not all data augmentation methods will have positive effects in the\ncontrast learning framework. Finally, we explored the influence of the\ncombination of different augmentation methods on contrastive learning, and the\nexperimental results showed that the effect of most combination augmentation\nmethods was better than that of single augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_J/0/1/0/all/0/1\">Jingyuan Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1\">Huansheng Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yaping Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Object-to-Zone Graph for Object Navigation. (arXiv:2109.02066v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02066","description":"<p>The goal of object navigation is to reach the expected objects according to\nvisual information in the unseen environments. Previous works usually implement\ndeep models to train an agent to predict actions in real-time. However, in the\nunseen environment, when the target object is not in egocentric view, the agent\nmay not be able to make wise decisions due to the lack of guidance. In this\npaper, we propose a hierarchical object-to-zone (HOZ) graph to guide the agent\nin a coarse-to-fine manner, and an online-learning mechanism is also proposed\nto update HOZ according to the real-time observation in new environments. In\nparticular, the HOZ graph is composed of scene nodes, zone nodes and object\nnodes. With the pre-learned HOZ graph, the real-time observation and the target\ngoal, the agent can constantly plan an optimal path from zone to zone. In the\nestimated path, the next potential zone is regarded as sub-goal, which is also\nfed into the deep reinforcement learning model for action prediction. Our\nmethods are evaluated on the AI2-Thor simulator. In addition to widely used\nevaluation metrics SR and SPL, we also propose a new evaluation metric of SAE\nthat focuses on the effective action rate. Experimental results demonstrate the\neffectiveness and efficiency of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sixian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xinhang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yubing Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yakui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusformer: A Transformer-based Fusion Approach for Hyperspectral Image Super-resolution. (arXiv:2109.02079v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02079","description":"<p>Hyperspectral image has become increasingly crucial due to its abundant\nspectral information. However, It has poor spatial resolution with the\nlimitation of the current imaging mechanism. Nowadays, many convolutional\nneural networks have been proposed for the hyperspectral image super-resolution\nproblem. However, convolutional neural network (CNN) based methods only\nconsider the local information instead of the global one with the limited\nkernel size of receptive field in the convolution operation. In this paper, we\ndesign a network based on the transformer for fusing the low-resolution\nhyperspectral images and high-resolution multispectral images to obtain the\nhigh-resolution hyperspectral images. Thanks to the representing ability of the\ntransformer, our approach is able to explore the intrinsic relationships of\nfeatures globally. Furthermore, considering the LR-HSIs hold the main spectral\nstructure, the network focuses on the spatial detail estimation releasing from\nthe burden of reconstructing the whole data. It reduces the mapping space of\nthe proposed network, which enhances the final performance. Various experiments\nand quality indexes show our approach's superiority compared with other\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jin-Fan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Zhu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liang-Jian Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis. (arXiv:2109.02081v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02081","description":"<p>Deep person generation has attracted extensive research attention due to its\nwide applications in virtual agents, video conferencing, online shopping and\nart/movie production. With the advancement of deep learning, visual appearances\n(face, pose, cloth) of a person image can be easily generated or manipulated on\ndemand. In this survey, we first summarize the scope of person generation, and\nthen systematically review recent progress and technical trends in deep person\ngeneration, covering three major tasks: talking-head generation (face),\npose-guided person generation (pose) and garment-oriented person generation\n(cloth). More than two hundred papers are covered for a thorough overview, and\nthe milestone works are highlighted to witness the major technical\nbreakthrough. Based on these fundamental tasks, a number of applications are\ninvestigated, e.g., virtual fitting, digital human, generative data\naugmentation. We hope this survey could shed some light on the future prospects\nof deep person generation, and provide a helpful foundation for full\napplications towards digital human.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sha_T/0/1/0/all/0/1\">Tong Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"(M)SLAe-Net: Multi-Scale Multi-Level Attention embedded Network for Retinal Vessel Segmentation. (arXiv:2109.02084v1 [eess.IV])","link":"http://arxiv.org/abs/2109.02084","description":"<p>Segmentation plays a crucial role in diagnosis. Studying the retinal\nvasculatures from fundus images help identify early signs of many crucial\nillnesses such as diabetic retinopathy. Due to the varying shape, size, and\npatterns of retinal vessels, along with artefacts and noises in fundus images,\nno one-stage method can accurately segment retinal vessels. In this work, we\npropose a multi-scale, multi-level attention embedded CNN architecture\n((M)SLAe-Net) to address the issue of multi-stage processing for robust and\nprecise segmentation of retinal vessels. We do this by extracting features at\nmultiple scales and multiple levels of the network, enabling our model to\nholistically extracts the local and global features. Multi-scale features are\nextracted using our novel dynamic dilated pyramid pooling (D-DPP) module. We\nalso aggregate the features from all the network levels. These effectively\nresolved the issues of varying shapes and artefacts and hence the need for\nmultiple stages. To assist in better pixel-level classification, we use the\nSqueeze and Attention(SA) module, a smartly adapted version of the Squeeze and\nExcitation(SE) module for segmentation tasks in our network to facilitate\npixel-group attention. Our unique network design and novel D-DPP module with\nefficient task-specific loss function for thin vessels enabled our model for\nbetter cross data performance. Exhaustive experimental results on DRIVE, STARE,\nHRF, and CHASE-DB1 show the superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saini_S/0/1/0/all/0/1\">Shreshth Saini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agrawal_G/0/1/0/all/0/1\">Geetika Agrawal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Timbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks. (arXiv:2109.02096v1 [cs.SD])","link":"http://arxiv.org/abs/2109.02096","description":"<p>This research project investigates the application of deep learning to timbre\ntransfer, where the timbre of a source audio can be converted to the timbre of\na target audio with minimal loss in quality. The adopted approach combines\nVariational Autoencoders with Generative Adversarial Networks to construct\nmeaningful representations of the source audio and produce realistic\ngenerations of the target audio and is applied to the Flickr 8k Audio dataset\nfor transferring the vocal timbre between speakers and the URMP dataset for\ntransferring the musical timbre between instruments. Furthermore, variations of\nthe adopted approach are trained, and generalised performance is compared using\nthe metrics SSIM (Structural Similarity Index) and FAD (Frech\\'et Audio\nDistance). It was found that a many-to-many approach supersedes a one-to-one\napproach in terms of reconstructive capabilities, and that the adoption of a\nbasic over a bottleneck residual block design is more suitable for enriching\ncontent information about a latent space. It was also found that the decision\non whether cyclic loss takes on a variational autoencoder or vanilla\nautoencoder approach does not have a significant impact on reconstructive and\nadversarial translation aspects of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonnici_R/0/1/0/all/0/1\">Russell Sammut Bonnici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1\">Charalampos Saitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benning_M/0/1/0/all/0/1\">Martin Benning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss. (arXiv:2109.02100v1 [cs.LG])","link":"http://arxiv.org/abs/2109.02100","description":"<p>Network quantization, which aims to reduce the bit-lengths of the network\nweights and activations, has emerged for their deployments to resource-limited\ndevices. Although recent studies have successfully discretized a full-precision\nnetwork, they still incur large quantization errors after training, thus giving\nrise to a significant performance gap between a full-precision network and its\nquantized counterpart. In this work, we propose a novel quantization method for\nneural networks, Cluster-Promoting Quantization (CPQ) that finds the optimal\nquantization grids while naturally encouraging the underlying full-precision\nweights to gather around those quantization grids cohesively during training.\nThis property of CPQ is thanks to our two main ingredients that enable\ndifferentiable quantization: i) the use of the categorical distribution\ndesigned by a specific probabilistic parametrization in the forward pass and\nii) our proposed multi-class straight-through estimator (STE) in the backward\npass. Since our second component, multi-class STE, is intrinsically biased, we\nadditionally propose a new bit-drop technique, DropBits, that revises the\nstandard dropout regularization to randomly drop bits instead of neurons. As a\nnatural extension of DropBits, we further introduce the way of learning\nheterogeneous quantization levels to find proper bit-length for each layer by\nimposing an additional regularization on DropBits. We experimentally validate\nour method on various benchmark datasets and network architectures, and also\nsupport a new hypothesis for quantization: learning heterogeneous quantization\nlevels outperforms the case using the same but fixed quantization levels from\nscratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jung Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1\">Jihun Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognition of COVID-19 Disease Utilizing X-Ray Imaging of the Chest Using CNN. (arXiv:2109.02103v1 [eess.IV])","link":"http://arxiv.org/abs/2109.02103","description":"<p>Since this COVID-19 pandemic thrives, the utilization of X-Ray images of the\nChest (CXR) as a complementary screening technique to RT-PCR testing grows to\nits clinical use for respiratory complaints. Many new deep learning approaches\nhave developed as a consequence. The goal of this research is to assess the\nconvolutional neural networks (CNNs) to diagnosis COVID-19 utisizing X-ray\nimages of chest. The performance of CNN with one, three, and four convolution\nlayers has been evaluated in this research. A dataset of 13,808 CXR photographs\nare used in this research. When evaluated on X-ray images with three splits of\nthe dataset, our preliminary experimental results show that the CNN model with\nthree convolution layers can reliably detect with 96 percent accuracy\n(precision being 96 percent). This fact indicates the commitment of our\nsuggested model for reliable screening of COVID-19.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hussain_M/0/1/0/all/0/1\">Md Gulzar Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shiren_Y/0/1/0/all/0/1\">Ye Shiren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Underwater 3D Reconstruction Using Light Fields. (arXiv:2109.02116v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02116","description":"<p>Underwater 3D reconstruction is challenging due to the refraction of light at\nthe water-air interface (most electronic devices cannot be directly submerged\nin water). In this paper, we present an underwater 3D reconstruction solution\nusing light field cameras. We first develop a light field camera calibration\nalgorithm that simultaneously estimates the camera parameters and the geometry\nof the water-air interface. We then design a novel depth estimation algorithm\nfor 3D reconstruction. Specifically, we match correspondences on curved\nepipolar lines caused by water refraction. We also observe that the\nview-dependent specular reflection is very weak in the underwater environment,\nresulting the angularly sampled rays in light field has uniform intensity. We\ntherefore propose an angular uniformity constraint for depth optimization. We\nalso develop a fast algorithm for locating the angular patches in presence of\nnon-linear light paths. Extensive synthetic and real experiments demonstrate\nthat our method can perform underwater 3D reconstruction with high accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuqi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jinwei Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification of Driver Phone Usage Violations via State-of-the-Art Object Detection with Tracking. (arXiv:2109.02119v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02119","description":"<p>The use of mobiles phones when driving have been a major factor when it comes\nto road traffic incidents and the process of capturing such violations can be a\nlaborious task. Advancements in both modern object detection frameworks and\nhigh-performance hardware has paved the way for a more automated approach when\nit comes to video surveillance. In this work, we propose a custom-trained\nstate-of-the-art object detector to work with roadside cameras to capture\ndriver phone usage without the need for human intervention. The proposed\napproach also addresses the issues caused by windscreen glare and introduces\nthe steps required to remedy this. Twelve pre-trained models are fine-tuned\nwith our custom dataset using four popular object detection methods: YOLO, SSD,\nFaster R-CNN, and CenterNet. Out of all the object detectors tested, the YOLO\nyields the highest accuracy levels of up to 96% (AP10) and frame rates of up to\n~30 FPS. DeepSort object tracking algorithm is also integrated into the\nbest-performing model to collect records of only the unique violations, and\nenable the proposed approach to count the number of vehicles. The proposed\nautomated system will collect the output images of the identified violations,\ntimestamps of each violation, and total vehicle count. Data can be accessed via\na purpose-built user interface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carrell_S/0/1/0/all/0/1\">Steven Carrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Neural Radiance Fields:Quantifying Uncertainty in Implicit 3D Representations. (arXiv:2109.02123v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02123","description":"<p>Neural Radiance Fields (NeRF) has become a popular framework for learning\nimplicit 3D representations and addressing different tasks such as novel-view\nsynthesis or depth-map estimation. However, in downstream applications where\ndecisions need to be made based on automatic predictions, it is critical to\nleverage the confidence associated with the model estimations. Whereas\nuncertainty quantification is a long-standing problem in Machine Learning, it\nhas been largely overlooked in the recent NeRF literature. In this context, we\npropose Stochastic Neural Radiance Fields (S-NeRF), a generalization of\nstandard NeRF that learns a probability distribution over all the possible\nradiance fields modeling the scene. This distribution allows to quantify the\nuncertainty associated with the scene information provided by the model. S-NeRF\noptimization is posed as a Bayesian learning problem which is efficiently\naddressed using the Variational Inference framework. Exhaustive experiments\nover benchmark datasets demonstrate that S-NeRF is able to provide more\nreliable predictions and confidence values than generic approaches previously\nproposed for uncertainty estimation in other domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianxiong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_A/0/1/0/all/0/1\">Adria Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1\">Antonio Agudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_F/0/1/0/all/0/1\">Francesc Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Action Recognition Using Confidence Distillation. (arXiv:2109.02137v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02137","description":"<p>Modern neural networks are powerful predictive models. However, when it comes\nto recognizing that they may be wrong about their predictions, they perform\npoorly. For example, for one of the most common activation functions, the ReLU\nand its variants, even a well-calibrated model can produce incorrect but high\nconfidence predictions. In the related task of action recognition, most current\nclassification methods are based on clip-level classifiers that densely sample\na given video for non-overlapping, same-sized clips and aggregate the results\nusing an aggregation function - typically averaging - to achieve video level\npredictions. While this approach has shown to be effective, it is sub-optimal\nin recognition accuracy and has a high computational overhead. To mitigate both\nthese issues, we propose the confidence distillation framework to teach a\nrepresentation of uncertainty of the teacher to the student sampler and divide\nthe task of full video prediction between the student and the teacher models.\nWe conduct extensive experiments on three action recognition datasets and\ndemonstrate that our framework achieves significant improvements in action\nrecognition accuracy (up to 20%) and computational efficiency (more than 40%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shalmani_S/0/1/0/all/0/1\">Shervin Manzuri Shalmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_F/0/1/0/all/0/1\">Fei Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rong Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Domain Feature Extraction Methods for Unconstrained Handwritten Malayalam Character Recognition. (arXiv:2109.02153v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02153","description":"<p>Handwritten character recognition is an active research challenge,especially\nfor Indian scripts. This paper deals with handwritten Malayalam, with a\ncomplete set of basic characters, vowel and consonant signs and compound\ncharacters that may be present in the script. Spatial domain features suitable\nfor recognition are chosen in this work. For classification, k-NN, SVM and ELM\nare employed\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+John_J/0/1/0/all/0/1\">Jomy John</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Attentive Deep Neural Network for Exposing GAN-generated Faces. (arXiv:2109.02167v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02167","description":"<p>GAN-based techniques that generate and synthesize realistic faces have caused\nsevere social concerns and security problems. Existing methods for detecting\nGAN-generated faces can perform well on limited public datasets. However,\nimages from existing public datasets do not represent real-world scenarios well\nenough in terms of view variations and data distributions (where real faces\nlargely outnumber synthetic faces). The state-of-the-art methods do not\ngeneralize well in real-world problems and lack the interpretability of\ndetection results. Performance of existing GAN-face detection models degrades\nsignificantly when facing imbalanced data distributions. To address these\nshortcomings, we propose a robust, attentive, end-to-end network that can spot\nGAN-generated faces by analyzing their eye inconsistencies. Specifically, our\nmodel learns to identify inconsistent eye components by localizing and\ncomparing the iris artifacts between the two eyes automatically. Our deep\nnetwork addresses the imbalance learning issues by considering the AUC loss and\nthe traditional cross-entropy loss jointly. Comprehensive evaluations of the\nFFHQ dataset in terms of both balanced and imbalanced scenarios demonstrate the\nsuperiority of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Right Ventricular Segmentation from Short- and Long-Axis MRIs via Information Transition. (arXiv:2109.02171v1 [eess.IV])","link":"http://arxiv.org/abs/2109.02171","description":"<p>Right ventricular (RV) segmentation from magnetic resonance imaging (MRI) is\na crucial step for cardiac morphology and function analysis. However, automatic\nRV segmentation from MRI is still challenging, mainly due to the heterogeneous\nintensity, the complex variable shapes, and the unclear RV boundary. Moreover,\ncurrent methods for the RV segmentation tend to suffer from performance\ndegradation at the basal and apical slices of MRI. In this work, we propose an\nautomatic RV segmentation framework, where the information from long-axis (LA)\nviews is utilized to assist the segmentation of short-axis (SA) views via\ninformation transition. Specifically, we employed the transformed segmentation\nfrom LA views as a prior information, to extract the ROI from SA views for\nbetter segmentation. The information transition aims to remove the surrounding\nambiguous regions in the SA views. %, such as the tricuspid valve regions. We\ntested our model on a public dataset with 360 multi-center, multi-vendor and\nmulti-disease subjects that consist of both LA and SA MRIs. Our experimental\nresults show that including LA views can be effective to improve the accuracy\nof the SA segmentation. Our model is publicly available at\nhttps://github.com/NanYoMy/MMs-2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_W/0/1/0/all/0/1\">Wangbin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Liqun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Agent Variational Occlusion Inference Using People as Sensors. (arXiv:2109.02173v1 [cs.RO])","link":"http://arxiv.org/abs/2109.02173","description":"<p>Autonomous vehicles must reason about spatial occlusions in urban\nenvironments to ensure safety without being overly cautious. Prior work\nexplored occlusion inference from observed social behaviors of road agents.\nInferring occupancy from agent behaviors is an inherently multimodal problem; a\ndriver may behave in the same manner for different occupancy patterns ahead of\nthem (e.g., a driver may move at constant speed in traffic or on an open road).\nPast work, however, does not account for this multimodality, thus neglecting to\nmodel this source of aleatoric uncertainty in the relationship between driver\nbehaviors and their environment. We propose an occlusion inference method that\ncharacterizes observed behaviors of human agents as sensor measurements, and\nfuses them with those from a standard sensor suite. To capture the aleatoric\nuncertainty, we train a conditional variational autoencoder with a discrete\nlatent space to learn a multimodal mapping from observed driver trajectories to\nan occupancy grid representation of the view ahead of the driver. Our method\nhandles multi-agent scenarios, combining measurements from multiple observed\ndrivers using evidential theory to solve the sensor fusion problem. Our\napproach is validated on a real-world dataset, outperforming baselines and\ndemonstrating real-time capable performance. Our code is available at\nhttps://github.com/sisl/MultiAgentVariationalOcclusionInference .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Itkina_M/0/1/0/all/0/1\">Masha Itkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mun_Y/0/1/0/all/0/1\">Ye-Ji Mun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1\">Katherine Driggs-Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Square Root Marginalization for Sliding-Window Bundle Adjustment. (arXiv:2109.02182v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02182","description":"<p>In this paper we propose a novel square root sliding-window bundle adjustment\nsuitable for real-time odometry applications. The square root formulation\npervades three major aspects of our optimization-based sliding-window\nestimator: for bundle adjustment we eliminate landmark variables with nullspace\nprojection; to store the marginalization prior we employ a matrix square root\nof the Hessian; and when marginalizing old poses we avoid forming normal\nequations and update the square root prior directly with a specialized QR\ndecomposition. We show that the proposed square root marginalization is\nalgebraically equivalent to the conventional use of Schur complement (SC) on\nthe Hessian. Moreover, it elegantly deals with rank-deficient Jacobians\nproducing a prior equivalent to SC with Moore-Penrose inverse. Our evaluation\nof visual and visual-inertial odometry on real-world datasets demonstrates that\nthe proposed estimator is 36% faster than the baseline. It furthermore shows\nthat in single precision, conventional Hessian-based marginalization leads to\nnumeric failures and reduced accuracy. We analyse numeric properties of the\nmarginalization prior to explain why our square root form does not suffer from\nthe same effect and therefore entails superior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Demmel_N/0/1/0/all/0/1\">Nikolaus Demmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_D/0/1/0/all/0/1\">David Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommer_C/0/1/0/all/0/1\">Christiane Sommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usenko_V/0/1/0/all/0/1\">Vladyslav Usenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parsing Table Structures in the Wild. (arXiv:2109.02199v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02199","description":"<p>This paper tackles the problem of table structure parsing (TSP) from images\nin the wild. In contrast to existing studies that mainly focus on parsing\nwell-aligned tabular images with simple layouts from scanned PDF documents, we\naim to establish a practical table structure parsing system for real-world\nscenarios where tabular input images are taken or scanned with severe\ndeformation, bending or occlusions. For designing such a system, we propose an\napproach named Cycle-CenterNet on the top of CenterNet with a novel\ncycle-pairing module to simultaneously detect and group tabular cells into\nstructured tables. In the cycle-pairing module, a new pairing loss function is\nproposed for the network training. Alongside with our Cycle-CenterNet, we also\npresent a large-scale dataset, named Wired Table in the Wild (WTW), which\nincludes well-annotated structure parsing of multiple style tables in several\nscenes like the photo, scanning files, web pages, \\emph{etc.}. In experiments,\nwe demonstrate that our Cycle-CenterNet consistently achieves the best accuracy\nof table structure parsing on the new WTW dataset by 24.6\\% absolute\nimprovement evaluated by the TEDS metric. A more comprehensive experimental\nanalysis also validates the advantages of our proposed methods for the TSP\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_R/0/1/0/all/0/1\">Rujiao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feiyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongpan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Fine-Grained Motion Embedding for Landscape Animation. (arXiv:2109.02216v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02216","description":"<p>In this paper we focus on landscape animation, which aims to generate\ntime-lapse videos from a single landscape image. Motion is crucial for\nlandscape animation as it determines how objects move in videos. Existing\nmethods are able to generate appealing videos by learning motion from real\ntime-lapse videos. However, current methods suffer from inaccurate motion\ngeneration, which leads to unrealistic video results. To tackle this problem,\nwe propose a model named FGLA to generate high-quality and realistic videos by\nlearning Fine-Grained motion embedding for Landscape Animation. Our model\nconsists of two parts: (1) a motion encoder which embeds time-lapse motion in a\nfine-grained way. (2) a motion generator which generates realistic motion to\nanimate input images. To train and evaluate on diverse time-lapse videos, we\nbuild the largest high-resolution Time-lapse video dataset with Diverse scenes,\nnamely Time-lapse-D, which includes 16,874 video clips with over 10 million\nframes. Quantitative and qualitative experimental results demonstrate the\nsuperiority of our method. In particular, our method achieves relative\nimprovements by 19% on LIPIS and 5.6% on FVD compared with state-of-the-art\nmethods on our dataset. A user study carried out with 700 human subjects shows\nthat our approach visually outperforms existing methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hongwei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Graph Networks for Kinship Verification: from Star-shaped to Hierarchical. (arXiv:2109.02219v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02219","description":"<p>In this paper, we investigate the problem of facial kinship verification by\nlearning hierarchical reasoning graph networks. Conventional methods usually\nfocus on learning discriminative features for each facial image of a paired\nsample and neglect how to fuse the obtained two facial image features and\nreason about the relations between them. To address this, we propose a\nStar-shaped Reasoning Graph Network (S-RGN). Our S-RGN first constructs a\nstar-shaped graph where each surrounding node encodes the information of\ncomparisons in a feature dimension and the central node is employed as the\nbridge for the interaction of surrounding nodes. Then we perform relational\nreasoning on this star graph with iterative message passing. The proposed S-RGN\nuses only one central node to analyze and process information from all\nsurrounding nodes, which limits its reasoning capacity. We further develop a\nHierarchical Reasoning Graph Network (H-RGN) to exploit more powerful and\nflexible capacity. More specifically, our H-RGN introduces a set of latent\nreasoning nodes and constructs a hierarchical graph with them. Then bottom-up\ncomparative information abstraction and top-down comprehensive signal\npropagation are iteratively performed on the hierarchical graph to update the\nnode features. Extensive experimental results on four widely used kinship\ndatabases show that the proposed methods achieve very competitive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuerkaixi_A/0/1/0/all/0/1\">Abudukelimu Wuerkaixi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianjiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization. (arXiv:2109.02220v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02220","description":"<p>Model compression techniques are recently gaining explosive attention for\nobtaining efficient AI models for various real-time applications. Channel\npruning is one important compression strategy and is widely used in slimming\nvarious DNNs. Previous gate-based or importance-based pruning methods aim to\nremove channels whose importance is smallest. However, it remains unclear what\ncriteria the channel importance should be measured on, leading to various\nchannel selection heuristics. Some other sampling-based pruning methods deploy\nsampling strategies to train sub-nets, which often causes the training\ninstability and the compressed model's degraded performance. In view of the\nresearch gaps, we present a new module named Gates with Differentiable\nPolarization (GDP), inspired by principled optimization ideas. GDP can be\nplugged before convolutional layers without bells and whistles, to control the\non-and-off of each channel or whole layer block. During the training process,\nthe polarization effect will drive a subset of gates to smoothly decrease to\nexact zero, while other gates gradually stay away from zero by a large margin.\nWhen training terminates, those zero-gated channels can be painlessly removed,\nwhile other non-zero gates can be absorbed into the succeeding convolution\nkernel, causing completely no interruption to training nor damage to the\ntrained model. Experiments conducted over CIFAR-10 and ImageNet datasets show\nthat the proposed GDP algorithm achieves the state-of-the-art performance on\nvarious benchmark DNNs at a broad range of pruning ratios. We also apply GDP to\nDeepLabV3Plus-ResNet50 on the challenging Pascal VOC segmentation task, whose\ntest performance sees no drop (even slightly improved) with over 60% FLOPs\nsaving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Huan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jianchao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeneAnnotator: A Semi-automatic Annotation Tool for Visual Scene Graph. (arXiv:2109.02226v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02226","description":"<p>In this manuscript, we introduce a semi-automatic scene graph annotation tool\nfor images, the GeneAnnotator. This software allows human annotators to\ndescribe the existing relationships between participators in the visual scene\nin the form of directed graphs, hence enabling the learning and reasoning on\nvisual relationships, e.g., image captioning, VQA and scene graph generation,\netc. The annotations for certain image datasets could either be merged in a\nsingle VG150 data-format file to support most existing models for scene graph\nlearning or transformed into a separated annotation file for each single image\nto build customized datasets. Moreover, GeneAnnotator provides a rule-based\nrelationship recommending algorithm to reduce the heavy annotation workload.\nWith GeneAnnotator, we propose Traffic Genome, a comprehensive scene graph\ndataset with 1000 diverse traffic images, which in return validates the\neffectiveness of the proposed software for scene graph annotation. The project\nsource code, with usage examples and sample data is available at\nhttps://github.com/Milomilo0320/A-Semi-automatic-Annotation-Software-for-Scene-Graph,\nunder the Apache open-source license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenning Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuehu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Generate Scene Graph from Natural Language Supervision. (arXiv:2109.02227v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02227","description":"<p>Learning from image-text data has demonstrated recent success for many\nrecognition tasks, yet is currently limited to visual features or individual\nvisual concepts such as objects. In this paper, we propose one of the first\nmethods that learn from image-sentence pairs to extract a graphical\nrepresentation of localized objects and their relationships within an image,\nknown as scene graph. To bridge the gap between images and texts, we leverage\nan off-the-shelf object detector to identify and localize object instances,\nmatch labels of detected regions to concepts parsed from captions, and thus\ncreate \"pseudo\" labels for learning scene graph. Further, we design a\nTransformer-based model to predict these \"pseudo\" labels via a masked token\nprediction task. Learning from only image-sentence pairs, our model achieves\n30% relative gain over a latest method trained with human-annotated unlocalized\nscene graphs. Our model also shows strong results for weakly and fully\nsupervised scene graph generation. In addition, we explore an open-vocabulary\nsetting for detecting scene graphs, and present the first result for open-set\nscene graph generation. Our code is available at\nhttps://github.com/YiwuZhong/SGG_from_NLS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiwu Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image recognition via Vietoris-Rips complex. (arXiv:2109.02231v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02231","description":"<p>Extracting informative features from images has been of capital importance in\ncomputer vision. In this paper, we propose a way to extract such features from\nimages by a method based on algebraic topology. To that end, we construct a\nweighted graph from an image, which extracts local information of an image. By\nconsidering this weighted graph as a pseudo-metric space, we construct a\nVietoris-Rips complex with a parameter $\\varepsilon$ by a well-known process of\nalgebraic topology. We can extract information of complexity of the image and\ncan detect a sub-image with a relatively high concentration of information from\nthis Vietoris-Rips complex. The parameter $\\varepsilon$ of the Vietoris-Rips\ncomplex produces robustness to noise. We empirically show that the extracted\nfeature captures well images' characteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asao_Y/0/1/0/all/0/1\">Yasuhiko Asao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagase_J/0/1/0/all/0/1\">Jumpei Nagase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakamoto_R/0/1/0/all/0/1\">Ryotaro Sakamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takagi_S/0/1/0/all/0/1\">Shiro Takagi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Product Quantization for Deep Unsupervised Image Retrieval. (arXiv:2109.02244v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02244","description":"<p>Supervised deep learning-based hash and vector quantization are enabling fast\nand large-scale image retrieval systems. By fully exploiting label annotations,\nthey are achieving outstanding retrieval performances compared to the\nconventional methods. However, it is painstaking to assign labels precisely for\na vast amount of training data, and also, the annotation process is\nerror-prone. To tackle these issues, we propose the first deep unsupervised\nimage retrieval method dubbed Self-supervised Product Quantization (SPQ)\nnetwork, which is label-free and trained in a self-supervised manner. We design\na Cross Quantized Contrastive learning strategy that jointly learns codewords\nand deep visual descriptors by comparing individually transformed images\n(views). Our method analyzes the image contents to extract descriptive\nfeatures, allowing us to understand image representations for accurate\nretrieval. By conducting extensive experiments on benchmarks, we demonstrate\nthat the proposed method yields state-of-the-art results even without\nsupervised pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Young Kyun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Models Improve Radiomics Performance in Different Tasks and Different Datasets: An Experimental Study. (arXiv:2109.02252v1 [q-bio.QM])","link":"http://arxiv.org/abs/2109.02252","description":"<p>Radiomics is an active area of research focusing on high throughput feature\nextraction from medical images with a wide array of applications in clinical\npractice, such as clinical decision support in oncology. However, noise in low\ndose computed tomography (CT) scans can impair the accurate extraction of\nradiomic features. In this article, we investigate the possibility of using\ndeep learning generative models to improve the performance of radiomics from\nlow dose CTs. We used two datasets of low dose CT scans -NSCLC Radiogenomics\nand LIDC-IDRI - as test datasets for two tasks - pre-treatment survival\nprediction and lung cancer diagnosis. We used encoder-decoder networks and\nconditional generative adversarial networks (CGANs) trained in a previous study\nas generative models to transform low dose CT images into full dose CT images.\nRadiomic features extracted from the original and improved CT scans were used\nto build two classifiers - a support vector machine (SVM) and a deep attention\nbased multiple instance learning model - for survival prediction and lung\ncancer diagnosis respectively. Finally, we compared the performance of the\nmodels derived from the original and improved CT scans. Encoder-decoder\nnetworks and CGANs improved the area under the curve (AUC) of survival\nprediction from 0.52 to 0.57 (p-value&lt;0.01). On the other hand, Encoder-decoder\nnetwork and CGAN can improve the AUC of lung cancer diagnosis from 0.84 to 0.88\nand 0.89 respectively (p-value&lt;0.01). Moreover, there are no statistically\nsignificant differences in improving AUC by using encoder-decoder network and\nCGAN (p-value=0.34) when networks trained at 75 and 100 epochs. Generative\nmodels can improve the performance of low dose CT-based radiomics in different\ntasks. Hence, denoising using generative models seems to be a necessary\npre-processing step for calculating radiomic features from low dose CTs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_J/0/1/0/all/0/1\">Junhua Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bermejo_I/0/1/0/all/0/1\">Inigo Bermejo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dekker_A/0/1/0/all/0/1\">Andre Dekker</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wee_L/0/1/0/all/0/1\">Leonard Wee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTRL-C: Camera calibration TRansformer with Line-Classification. (arXiv:2109.02259v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02259","description":"<p>Single image camera calibration is the task of estimating the camera\nparameters from a single input image, such as the vanishing points, focal\nlength, and horizon line. In this work, we propose Camera calibration\nTRansformer with Line-Classification (CTRL-C), an end-to-end neural\nnetwork-based approach to single image camera calibration, which directly\nestimates the camera parameters from an image and a set of line segments. Our\nnetwork adopts the transformer architecture to capture the global structure of\nan image with multi-modal inputs in an end-to-end manner. We also propose an\nauxiliary task of line classification to train the network to extract the\nglobal geometric information from lines effectively. Our experiments\ndemonstrate that CTRL-C outperforms the previous state-of-the-art methods on\nthe Google Street View and SUN360 benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Go_H/0/1/0/all/0/1\">Hyunsung Go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunjoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhyuk Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Spatial-Temporal Semantic Consistency for Video Scene Parsing. (arXiv:2109.02281v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02281","description":"<p>Compared with image scene parsing, video scene parsing introduces temporal\ninformation, which can effectively improve the consistency and accuracy of\nprediction. In this paper, we propose a Spatial-Temporal Semantic Consistency\nmethod to capture class-exclusive context information. Specifically, we design\na spatial-temporal consistency loss to constrain the semantic consistency in\nspatial and temporal dimensions. In addition, we adopt an pseudo-labeling\nstrategy to enrich the training dataset. We obtain the scores of 59.84% and\n58.85% mIoU on development (test part 1) and testing set of VSPW, respectively.\nAnd our method wins the 1st place on VSPW challenge at ICCV2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingjian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Melania Trump have a body double from the perspective of automatic face recognition?. (arXiv:2109.02283v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02283","description":"<p>In this paper, we explore whether automatic face recognition can help in\nverifying widespread misinformation on social media, particularly conspiracy\ntheories that are based on the existence of body doubles. The conspiracy theory\naddressed in this paper is the case of the Melania Trump body double. We\nemployed four different state-of-the-art descriptors for face recognition to\nverify the integrity of the claim of the studied conspiracy theory. In\naddition, we assessed the impact of different image quality metrics on the\nvariation of face recognition results. Two sets of image quality metrics were\nconsidered: acquisition-related metrics and subject-related metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mallat_K/0/1/0/all/0/1\">Khawla Mallat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becerra_Riera_F/0/1/0/all/0/1\">Fabiola Becerra-Riera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_Gonzalez_A/0/1/0/all/0/1\">Annette Morales-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_H/0/1/0/all/0/1\">Heydi M&#xe9;ndez-V&#xe1;zquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dugelay_J/0/1/0/all/0/1\">Jean-Luc Dugelay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Realistic Single-View 3D Object Reconstructionwith Unsupervised Learning from Multiple Images. (arXiv:2109.02288v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02288","description":"<p>Recovering the 3D structure of an object from a single image is a challenging\ntask due to its ill-posed nature. One approach is to utilize the plentiful\nphotos of the same object category to learn a strong 3D shape prior for the\nobject. This approach has successfully been demonstrated by a recent work of Wu\net al. (2020), which obtained impressive 3D reconstruction networks with\nunsupervised learning. However, their algorithm is only applicable to symmetric\nobjects. In this paper, we eliminate the symmetry requirement with a novel\nunsupervised algorithm that can learn a 3D reconstruction network from a\nmulti-image dataset. Our algorithm is more general and covers the\nsymmetry-required scenario as a special case. Besides, we employ a novel albedo\nloss that improves the reconstructed details and realisticity. Our method\nsurpasses the previous work in both quality and robustness, as shown in\nexperiments on datasets of various structures, including single-view,\nmulti-view, image-collection, and video sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_L/0/1/0/all/0/1\">Long-Nhat Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1\">Anh Tuan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_Q/0/1/0/all/0/1\">Quynh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoai_M/0/1/0/all/0/1\">Minh Hoai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Encoder-decoder with Multi-level Attention for 3D Human Shape and Pose Estimation. (arXiv:2109.02303v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02303","description":"<p>3D human shape and pose estimation is the essential task for human motion\nanalysis, which is widely used in many 3D applications. However, existing\nmethods cannot simultaneously capture the relations at multiple levels,\nincluding spatial-temporal level and human joint level. Therefore they fail to\nmake accurate predictions in some hard scenarios when there is cluttered\nbackground, occlusion, or extreme pose. To this end, we propose Multi-level\nAttention Encoder-Decoder Network (MAED), including a Spatial-Temporal Encoder\n(STE) and a Kinematic Topology Decoder (KTD) to model multi-level attentions in\na unified framework. STE consists of a series of cascaded blocks based on\nMulti-Head Self-Attention, and each block uses two parallel branches to learn\nspatial and temporal attention respectively. Meanwhile, KTD aims at modeling\nthe joint level attention. It regards pose estimation as a top-down\nhierarchical process similar to SMPL kinematic tree. With the training set of\n3DPW, MAED outperforms previous state-of-the-art methods by 6.2, 7.2, and 2.4\nmm of PA-MPJPE on the three widely used benchmarks 3DPW, MPI-INF-3DHP, and\nHuman3.6M respectively. Our code is available at\nhttps://github.com/ziniuwan/maed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Ziniu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengjia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1\">Maoqing Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Segmentation of the Optic Nerve Head Region in Optical Coherence Tomography: A Methodological Review. (arXiv:2109.02322v1 [eess.IV])","link":"http://arxiv.org/abs/2109.02322","description":"<p>The optic nerve head represents the intraocular section of the optic nerve\n(ONH), which is prone to damage by intraocular pressure. The advent of optical\ncoherence tomography (OCT) has enabled the evaluation of novel optic nerve head\nparameters, namely the depth and curvature of the lamina cribrosa (LC).\nTogether with the Bruch's membrane opening minimum-rim-width, these seem to be\npromising optic nerve head parameters for diagnosis and monitoring of retinal\ndiseases such as glaucoma. Nonetheless, these optical coherence tomography\nderived biomarkers are mostly extracted through manual segmentation, which is\ntime-consuming and prone to bias, thus limiting their usability in clinical\npractice. The automatic segmentation of optic nerve head in OCT scans could\nfurther improve the current clinical management of glaucoma and other diseases.\n</p>\n<p>This review summarizes the current state-of-the-art in automatic segmentation\nof the ONH in OCT. PubMed and Scopus were used to perform a systematic review.\nAdditional works from other databases (IEEE, Google Scholar and ARVO IOVS) were\nalso included, resulting in a total of 27 reviewed studies.\n</p>\n<p>For each algorithm, the methods, the size and type of dataset used for\nvalidation, and the respective results were carefully analyzed. The results\nshow that deep learning-based algorithms provide the highest accuracy,\nsensitivity and specificity for segmenting the different structures of the ONH\nincluding the LC. However, a lack of consensus regarding the definition of\nsegmented regions, extracted parameters and validation approaches has been\nobserved, highlighting the importance and need of standardized methodologies\nfor ONH segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Marques_R/0/1/0/all/0/1\">Rita Marques</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jesus_D/0/1/0/all/0/1\">Danilo Andrade De Jesus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breda_J/0/1/0/all/0/1\">Jo&#xe3;o Barbosa Breda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eijgen_J/0/1/0/all/0/1\">Jan Van Eijgen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stalmans_I/0/1/0/all/0/1\">Ingeborg Stalmans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walsum_T/0/1/0/all/0/1\">Theo van Walsum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1\">Stefan Klein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vaz_P/0/1/0/all/0/1\">Pedro G. Vaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brea_L/0/1/0/all/0/1\">Luisa S&#xe1;nchez Brea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Cardiac Resting Phase Detection Targeted on the Right Coronary Artery. (arXiv:2109.02342v1 [eess.IV])","link":"http://arxiv.org/abs/2109.02342","description":"<p>Purpose: Static cardiac imaging such as late gadolinium enhancement, mapping,\nor 3-D coronary angiography require prior information, e.g., the phase during a\ncardiac cycle with least motion, called resting phase (RP). The purpose of this\nwork is to propose a fully automated framework that allows the detection of the\nright coronary artery (RCA) RP within CINE series. Methods: The proposed\nprototype system consists of three main steps. First, the localization of the\nregions of interest (ROI) is performed. Second, as CINE series are\ntime-resolved, the cropped ROI series over all time points are taken for\ntracking motions quantitatively. Third, the output motion values are used to\nclassify RPs. In this work, we focused on the detection of the area with the\nouter edge of the cross-section of the RCA as our target. The proposed\nframework was evaluated on 102 clinically acquired dataset at 1.5T and 3T. The\nautomatically classified RPs were compared with the ground truth RPs annotated\nmanually by a medical expert for testing the robustness and feasibility of the\nframework. Results: The predicted RCA RPs showed high agreement with the\nexperts annotated RPs with 92.7% accuracy, 90.5% sensitivity and 95.0%\nspecificity for the unseen study dataset. The mean absolute difference of the\nstart and end RP was 13.6 ${\\pm}$ 18.6 ms for the validation study dataset\n(n=102). Conclusion: In this work, automated RP detection has been introduced\nby the proposed framework and demonstrated feasibility, robustness, and\napplicability for diverse static imaging acquisitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yoon_S/0/1/0/all/0/1\">Seung Su Yoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Preuhs_E/0/1/0/all/0/1\">Elisabeth Preuhs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_M/0/1/0/all/0/1\">Michaela Schmidt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Forman_C/0/1/0/all/0/1\">Christoph Forman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chitiboi_T/0/1/0/all/0/1\">Teodora Chitiboi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1\">Puneet Sharma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fernandes_J/0/1/0/all/0/1\">Juliano Lara Fernandes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tillmanns_C/0/1/0/all/0/1\">Christoph Tillmanns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wetzl_J/0/1/0/all/0/1\">Jens Wetzl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2109.02344v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02344","description":"<p>Multi-view representation learning captures comprehensive information from\nmultiple views of a shared context. Recent works intuitively apply contrastive\nlearning (CL) to learn representations, regarded as a pairwise manner, which is\nstill scalable: view-specific noise is not filtered in learning view-shared\nrepresentations; the fake negative pairs, where the negative terms are actually\nwithin the same class as the positive, and the real negative pairs are\ncoequally treated; and evenly measuring the similarities between terms might\ninterfere with optimization. Importantly, few works research the theoretical\nframework of generalized self-supervised multi-view learning, especially for\nmore than two views. To this end, we rethink the existing multi-view learning\nparadigm from the information theoretical perspective and then propose a novel\ninformation theoretical framework for generalized multi-view learning. Guided\nby it, we build a multi-view coding method with a three-tier progressive\narchitecture, namely Information theory-guided heuristic Progressive Multi-view\nCoding (IPMC). In the distribution-tier, IPMC aligns the distribution between\nviews to reduce view-specific noise. In the set-tier, IPMC builds self-adjusted\npools for contrasting, which utilizes a view filter to adaptively modify the\npools. Lastly, in the instance-tier, we adopt a designed unified loss to learn\ndiscriminative representations and reduce the gradient interference.\nTheoretically and empirically, we demonstrate the superiority of IPMC over\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangmeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1\">Wenwen Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzak_F/0/1/0/all/0/1\">Farid Razzak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changwen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Normalization and Full Distribution Training. (arXiv:2109.02345v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02345","description":"<p>In this work, we introduce pixel wise tensor normalization, which is inserted\nafter rectifier linear units and, together with batch normalization, provides a\nsignificant improvement in the accuracy of modern deep neural networks. In\naddition, this work deals with the robustness of networks. We show that the\nfactorized superposition of images from the training set and the reformulation\nof the multi class problem into a multi-label problem yields significantly more\nrobust networks. The reformulation and the adjustment of the multi class log\nloss also improves the results compared to the overlay with only one class as\nlabel.\nhttps://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2FTNandFDT&amp;mode=list\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fuhl_W/0/1/0/all/0/1\">Wolfgang Fuhl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair Federated Learning for Heterogeneous Face Data. (arXiv:2109.02351v1 [cs.LG])","link":"http://arxiv.org/abs/2109.02351","description":"<p>We consider the problem of achieving fair classification in Federated\nLearning (FL) under data heterogeneity. Most of the approaches proposed for\nfair classification require diverse data that represent the different\ndemographic groups involved. In contrast, it is common for each client to own\ndata that represents only a single demographic group. Hence the existing\napproaches cannot be adopted for fair classification models at the client\nlevel. To resolve this challenge, we propose several aggregation techniques. We\nempirically validate these techniques by comparing the resulting fairness\nmetrics and accuracy on CelebA, UTK, and FairFace datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanaparthy_S/0/1/0/all/0/1\">Samhita Kanaparthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padala_M/0/1/0/all/0/1\">Manisha Padala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damle_S/0/1/0/all/0/1\">Sankarshan Damle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gujar_S/0/1/0/all/0/1\">Sujit Gujar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Recognition with Deep Learning from Biased Image Datasets. (arXiv:2109.02357v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02357","description":"<p>In practice, and more especially when training deep neural networks, visual\nrecognition rules are often learned based on various sources of information. On\nthe other hand, the recent deployment of facial recognition systems with uneven\npredictive performances on different population segments highlights the\nrepresentativeness issues possibly induced by a naive aggregation of image\ndatasets. Indeed, sampling bias does not vanish simply by considering larger\ndatasets, and ignoring its impact may completely jeopardize the generalization\ncapacity of the learned prediction rules. In this paper, we show how biasing\nmodels, originally introduced for nonparametric estimation in (Gill et al.,\n1988), and recently revisited from the perspective of statistical learning\ntheory in (Laforgue and Cl\\'emen\\c{c}on, 2019), can be applied to remedy these\nproblems in the context of visual recognition. Based on the (approximate)\nknowledge of the biasing mechanisms at work, our approach consists in\nreweighting the observations, so as to form a nearly debiased estimator of the\ntarget distribution. One key condition for our method to be theoretically valid\nis that the supports of the distributions generating the biased datasets at\ndisposal must overlap, and cover the support of the target distribution. In\norder to meet this requirement in practice, we propose to use a low dimensional\nimage representation, shared across the image databases. Finally, we provide\nnumerical experiments highlighting the relevance of our approach whenever the\nbiasing functions are appropriately chosen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vogel_R/0/1/0/all/0/1\">Robin Vogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clemencon_S/0/1/0/all/0/1\">Stephan Cl&#xe9;men&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laforgue_P/0/1/0/all/0/1\">Pierre Laforgue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing the Machine Readability of Traffic Sign Pictograms in Austria and Germany. (arXiv:2109.02362v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02362","description":"<p>We compare the machine readability of pictograms found on Austrian and German\ntraffic signs. To that end, we train classification models on synthetic data\nsets and evaluate their classification accuracy in a controlled setting. In\nparticular, we focus on differences between currently deployed pictograms in\nthe two countries, and a set of new pictograms designed to increase human\nreadability. Besides other results, we find that machine-learning models\ngeneralize poorly to data sets with pictogram designs they have not been\ntrained on. We conclude that manufacturers of advanced driver-assistance\nsystems (ADAS) must take special care to properly address small visual\ndifferences between current and newly designed traffic sign pictograms, as well\nas between pictograms from different countries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maletzky_A/0/1/0/all/0/1\">Alexander Maletzky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thumfart_S/0/1/0/all/0/1\">Stefan Thumfart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wruss_C/0/1/0/all/0/1\">Christoph Wru&#xdf;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Multidiversified Ensemble Clustering of High-Dimensional Data: From Subspaces to Metrics and Beyond. (arXiv:1710.03113v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1710.03113","description":"<p>The rapid emergence of high-dimensional data in various areas has brought new\nchallenges to current ensemble clustering research. To deal with the curse of\ndimensionality, recently considerable efforts in ensemble clustering have been\nmade by means of different subspace-based techniques. However, besides the\nemphasis on subspaces, rather limited attention has been paid to the potential\ndiversity in similarity/dissimilarity metrics. It remains a surprisingly open\nproblem in ensemble clustering how to create and aggregate a large population\nof diversified metrics, and furthermore, how to jointly investigate the\nmulti-level diversity in the large populations of metrics, subspaces, and\nclusters in a unified framework. To tackle this problem, this paper proposes a\nnovel multidiversified ensemble clustering approach. In particular, we create a\nlarge number of diversified metrics by randomizing a scaled exponential\nsimilarity kernel, which are then coupled with random subspaces to form a large\nset of metric-subspace pairs. Based on the similarity matrices derived from\nthese metric-subspace pairs, an ensemble of diversified base clusterings can\nthereby be constructed. Further, an entropy-based criterion is utilized to\nexplore the cluster-wise diversity in ensembles, based on which three specific\nensemble clustering algorithms are presented by incorporating three types of\nconsensus functions. Extensive experiments are conducted on 30 high-dimensional\ndatasets, including 18 cancer gene expression datasets and 12 image/speech\ndatasets, which demonstrate the superiority of our algorithms over the\nstate-of-the-art. The source code is available at\nhttps://github.com/huangdonghere/MDEC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jian-Huang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1\">Chee-Keong Kwoh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Customization Strategies and Convergence Behaviors of Task-specific ADMM. (arXiv:1909.10819v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1909.10819","description":"<p>Alternating Direction Method of Multiplier (ADMM) has been a popular\nalgorithmic framework for separable optimization problems with linear\nconstraints. For numerical ADMM fail to exploit the particular structure of the\nproblem at hand nor the input data information, leveraging task-specific\nmodules (e.g., neural networks and other data-driven architectures) to extend\nADMM is a significant but challenging task. This work focuses on designing a\nflexible algorithmic framework to incorporate various task-specific modules\n(with no additional constraints) to improve the performance of ADMM in\nreal-world applications. Specifically, we propose Guidance from Optimality\n(GO), a new customization strategy, to embed task-specific modules into ADMM\n(GO-ADMM). By introducing an optimality-based criterion to guide the\npropagation, GO-ADMM establishes an updating scheme agnostic to the choice of\nadditional modules. The existing task-specific methods just plug their\ntask-specific modules into the numerical iterations in a straightforward\nmanner. Even with some restrictive constraints on the plug-in modules, they can\nonly obtain some relatively weaker convergence properties for the resulted ADMM\niterations. Fortunately, without any restrictions on the embedded modules, we\nprove the convergence of GO-ADMM regarding objective values and constraint\nviolations, and derive the worst-case convergence rate measured by iteration\ncomplexity. Extensive experiments are conducted to verify the theoretical\nresults and demonstrate the efficiency of GO-ADMM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_P/0/1/0/all/0/1\">Pan Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance. (arXiv:1909.10837v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1909.10837","description":"<p>Spiking neural network (SNN) is interesting both theoretically and\npractically because of its strong bio-inspiration nature and potentially\noutstanding energy efficiency. Unfortunately, its development has fallen far\nbehind the conventional deep neural network (DNN), mainly because of difficult\ntraining and lack of widely accepted hardware experiment platforms. In this\npaper, we show that a deep temporal-coded SNN can be trained easily and\ndirectly over the benchmark datasets CIFAR10 and ImageNet, with testing\naccuracy within 1% of the DNN of equivalent size and architecture. Training\nbecomes similar to DNN thanks to the closed-form solution to the spiking\nwaveform dynamics. Considering that SNNs should be implemented in practical\nneuromorphic hardwares, we train the deep SNN with weights quantized to 8, 4, 2\nbits and with weights perturbed by random noise to demonstrate its robustness\nin practical applications. In addition, we develop a phase-domain signal\nprocessing circuit schematic to implement our spiking neuron with 90% gain of\nenergy efficiency over existing work. This paper demonstrates that the\ntemporal-coded deep SNN is feasible for applications with high performance and\nhigh energy efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shibo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LI_X/0/1/0/all/0/1\">Xiaohua LI</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1\">Sanjeev T. Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1\">Arindam Sanyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Do Compressed Deep Neural Networks Forget?. (arXiv:1911.05248v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1911.05248","description":"<p>Deep neural network pruning and quantization techniques have demonstrated it\nis possible to achieve high levels of compression with surprisingly little\ndegradation to test set accuracy. However, this measure of performance conceals\nsignificant differences in how different classes and images are impacted by\nmodel compression techniques. We find that models with radically different\nnumbers of weights have comparable top-line performance metrics but diverge\nconsiderably in behavior on a narrow subset of the dataset. This small subset\nof data points, which we term Pruning Identified Exemplars (PIEs) are\nsystematically more impacted by the introduction of sparsity. Compression\ndisproportionately impacts model performance on the underrepresented long-tail\nof the data distribution. PIEs over-index on atypical or noisy images that are\nfar more challenging for both humans and algorithms to classify. Our work\nprovides intuition into the role of capacity in deep neural networks and the\ntrade-offs incurred by compression. An understanding of this disparate impact\nis critical given the widespread deployment of compressed models in the wild.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_G/0/1/0/all/0/1\">Gregory Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dauphin_Y/0/1/0/all/0/1\">Yann Dauphin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frome_A/0/1/0/all/0/1\">Andrea Frome</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Multi-Object Detector by Estimating Bounding Box Distribution for Input Image. (arXiv:1911.12721v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.12721","description":"<p>In multi-object detection using neural networks, the fundamental problem is,\n\"How should the network learn a variable number of bounding boxes in different\ninput images?\". Previous methods train a multi-object detection network through\na procedure that directly assigns the ground truth bounding boxes to the\nspecific locations of the network's output. However, this procedure makes the\ntraining of a multi-object detection network too heuristic and complicated. In\nthis paper, we reformulate the multi-object detection task as a problem of\ndensity estimation of bounding boxes. Instead of assigning each ground truth to\nspecific locations of network's output, we train a network by estimating the\nprobability density of bounding boxes in an input image using a mixture model.\nFor this purpose, we propose a novel network for object detection called\nMixture Density Object Detector (MDOD), and the corresponding objective\nfunction for the density-estimation-based training. We applied MDOD to MS COCO\ndataset. Our proposed method not only deals with multi-object detection\nproblems in a new approach, but also improves detection performances through\nMDOD. The code is available: https://github.com/yoojy31/MDOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaeyoung Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hojun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_I/0/1/0/all/0/1\">Inseop Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_G/0/1/0/all/0/1\">Geonseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Long Tail Visual Relationship Recognition with Large Vocabulary. (arXiv:2004.00436v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.00436","description":"<p>Several approaches have been proposed in recent literature to alleviate the\nlong-tail problem, mainly in object classification tasks. In this paper, we\nmake the first large-scale study concerning the task of Long-Tail Visual\nRelationship Recognition (LTVRR). LTVRR aims at improving the learning of\nstructured visual relationships that come from the long-tail (e.g., \"rabbit\ngrazing on grass\"). In this setup, the subject, relation, and object classes\neach follow a long-tail distribution. To begin our study and make a future\nbenchmark for the community, we introduce two LTVRR-related benchmarks, dubbed\nVG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets.\nWe use these benchmarks to study the performance of several state-of-the-art\nlong-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic\nhubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR\nsetup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top\nof existing models and despite being simple, our results show that they can\nremarkably improve the performance, especially on tail classes. Benchmarks,\ncode, and models have been made available at:\nhttps://github.com/Vision-CAIR/LTVRR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelkarim_S/0/1/0/all/0/1\">Sherif Abdelkarim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aniket Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achlioptas_P/0/1/0/all/0/1\">Panos Achlioptas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaji Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1\">Kenneth Church</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AIBench Scenario: Scenario-distilling AI Benchmarking. (arXiv:2005.03459v4 [cs.PF] UPDATED)","link":"http://arxiv.org/abs/2005.03459","description":"<p>Modern real-world application scenarios like Internet services consist of a\ndiversity of AI and non-AI modules with huge code sizes and long and\ncomplicated execution paths, which raises serious benchmarking or evaluating\nchallenges. Using AI components or micro benchmarks alone can lead to\nerror-prone conclusions. This paper presents a methodology to attack the above\nchallenge. We formalize a real-world application scenario as a Directed Acyclic\nGraph-based model and propose the rules to distill it into a permutation of\nessential AI and non-AI tasks, which we call a scenario benchmark. Together\nwith seventeen industry partners, we extract nine typical scenario benchmarks.\nWe design and implement an extensible, configurable, and flexible benchmark\nframework. We implement two Internet service AI scenario benchmarks based on\nthe framework as proxies to two real-world application scenarios. We consider\nscenario, component, and micro benchmarks as three indispensable parts for\nevaluating. Our evaluation shows the advantage of our methodology against using\ncomponent or micro AI benchmarks alone. The specifications, source code,\ntestbed, and results are publicly available from\n\\url{https://www.benchcouncil.org/aibench/scenario/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wanling Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Jianfeng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Chuanxin Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chunjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihan Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Privacy Preserving Edge Computing Framework for Image Classification. (arXiv:2005.04563v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2005.04563","description":"<p>In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fagbohungbe_O/0/1/0/all/0/1\">Omobayode Fagbohungbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reza_S/0/1/0/all/0/1\">Sheikh Rufsan Reza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xishuang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Lijun Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TearingNet: Point Cloud Autoencoder to Learn Topology-Friendly Representations. (arXiv:2006.10187v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.10187","description":"<p>Topology matters. Despite the recent success of point cloud processing with\ngeometric deep learning, it remains arduous to capture the complex topologies\nof point cloud data with a learning model. Given a point cloud dataset\ncontaining objects with various genera, or scenes with multiple objects, we\npropose an autoencoder, TearingNet, which tackles the challenging task of\nrepresenting the point clouds using a fixed-length descriptor. Unlike existing\nworks directly deforming predefined primitives of genus zero (e.g., a 2D square\npatch) to an object-level point cloud, our TearingNet is characterized by a\nproposed Tearing network module and a Folding network module interacting with\neach other iteratively. Particularly, the Tearing network module learns the\npoint cloud topology explicitly. By breaking the edges of a primitive graph, it\ntears the graph into patches or with holes to emulate the topology of a target\npoint cloud, leading to faithful reconstructions. Experimentation shows the\nsuperiority of our proposal in terms of reconstructing point clouds as well as\ngenerating more topology-friendly representations than benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiahao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Duanshun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_D/0/1/0/all/0/1\">Dong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Explanation Based on Gradual Construction for Deep Networks. (arXiv:2008.01897v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2008.01897","description":"<p>To understand the black-box characteristics of deep networks, counterfactual\nexplanation that deduces not only the important features of an input space but\nalso how those features should be modified to classify input as a target class\nhas gained an increasing interest. The patterns that deep networks have learned\nfrom a training dataset can be grasped by observing the feature variation among\nvarious classes. However, current approaches perform the feature modification\nto increase the classification probability for the target class irrespective of\nthe internal characteristics of deep networks. This often leads to unclear\nexplanations that deviate from real-world data distributions. To address this\nproblem, we propose a counterfactual explanation method that exploits the\nstatistics learned from a training dataset. Especially, we gradually construct\nan explanation by iterating over masking and composition steps. The masking\nstep aims to select an important feature from the input data to be classified\nas a target class. Meanwhile, the composition step aims to optimize the\npreviously selected feature by ensuring that its output score is close to the\nlogit space of the training data that are classified as the target class.\nExperimental results show that our method produces human-friendly\ninterpretations on various classification datasets and verify that such\ninterpretations can be achieved with fewer feature modification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hong-Gyu Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Sin-Han Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hee-Dong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Won_D/0/1/0/all/0/1\">Dong-Ok Won</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Signal Processing for Geometric Data and Beyond: Theory and Applications. (arXiv:2008.01918v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.01918","description":"<p>Geometric data acquired from real-world scenes, e.g., 2D depth images, 3D\npoint clouds, and 4D dynamic point clouds, have found a wide range of\napplications including immersive telepresence, autonomous driving,\nsurveillance, etc. Due to irregular sampling patterns of most geometric data,\ntraditional image/video processing methodologies are limited, while Graph\nSignal Processing (GSP) -- a fast-developing field in the signal processing\ncommunity -- enables processing signals that reside on irregular domains and\nplays a critical role in numerous applications of geometric data from low-level\nprocessing to high-level analysis. To further advance the research in this\nfield, we provide the first timely and comprehensive overview of GSP\nmethodologies for geometric data in a unified manner by bridging the\nconnections between geometric data and graphs, among the various geometric data\nmodalities, and with spectral/nodal graph filtering techniques. We also discuss\nthe recently developed Graph Neural Networks (GNNs) and interpret the operation\nof these networks from the perspective of GSP. We conclude with a brief\ndiscussion of open problems and challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiahao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_D/0/1/0/all/0/1\">Dong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetro_A/0/1/0/all/0/1\">Anthony Vetro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Assessing the Generalization Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and Adversarial Samples. (arXiv:2008.09381v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2008.09381","description":"<p>Deep Neural Networks (DNNs) achieve state-of-the-art performance on numerous\napplications. However, it is difficult to tell beforehand if a DNN receiving an\ninput will deliver the correct output since their decision criteria are usually\nnontransparent. A DNN delivers the correct output if the input is within the\narea enclosed by its generalization envelope. In this case, the information\ncontained in the input sample is processed reasonably by the network. It is of\nlarge practical importance to assess at inference time if a DNN generalizes\ncorrectly. Currently, the approaches to achieve this goal are investigated in\ndifferent problem set-ups rather independently from one another, leading to\nthree main research and literature fields: predictive uncertainty,\nout-of-distribution detection and adversarial example detection. This survey\nconnects the three fields within the larger framework of investigating the\ngeneralization performance of machine learning methods and in particular DNNs.\nWe underline the common ground, point at the most promising approaches and give\na structured overview of the methods that provide at inference time means to\nestablish if the current input is within the generalization envelope of a DNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lust_J/0/1/0/all/0/1\">Julia Lust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1\">Alexandru Paul Condurache</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Spectral Image Synthesis for Crop/Weed Segmentation in Precision Farming. (arXiv:2009.05750v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.05750","description":"<p>An effective perception system is a fundamental component for farming robots,\nas it enables them to properly perceive the surrounding environment and to\ncarry out targeted operations. The most recent methods make use of\nstate-of-the-art machine learning techniques to learn a valid model for the\ntarget task. However, those techniques need a large amount of labeled data for\ntraining. A recent approach to deal with this issue is data augmentation\nthrough Generative Adversarial Networks (GANs), where entire synthetic scenes\nare added to the training data, thus enlarging and diversifying their\ninformative content. In this work, we propose an alternative solution with\nrespect to the common data augmentation methods, applying it to the fundamental\nproblem of crop/weed segmentation in precision farming. Starting from real\nimages, we create semi-artificial samples by replacing the most relevant object\nclasses (i.e., crop and weeds) with their synthesized counterparts. To do that,\nwe employ a conditional GAN (cGAN), where the generative model is trained by\nconditioning the shape of the generated object. Moreover, in addition to RGB\ndata, we take into account also near-infrared (NIR) information, generating\nfour channel multi-spectral synthetic images. Quantitative experiments, carried\nout on three publicly available datasets, show that (i) our model is capable of\ngenerating realistic multi-spectral images of plants and (ii) the usage of such\nsynthetic images in the training process improves the segmentation performance\nof state-of-the-art semantic segmentation convolutional networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fawakherji_M/0/1/0/all/0/1\">Mulham Fawakherji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potena_C/0/1/0/all/0/1\">Ciro Potena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pretto_A/0/1/0/all/0/1\">Alberto Pretto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloisi_D/0/1/0/all/0/1\">Domenico D. Bloisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nardi_D/0/1/0/all/0/1\">Daniele Nardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. (arXiv:2009.10623v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.10623","description":"<p>From CNNs to attention mechanisms, encoding inductive biases into neural\nnetworks has been a fruitful source of improvement in machine learning. Adding\nauxiliary losses to the main objective function is a general way of encoding\nbiases that can help networks learn better representations. However, since\nauxiliary losses are minimized only on training data, they suffer from the same\ngeneralization gap as regular task losses. Moreover, by adding a term to the\nloss function, the model optimizes a different objective than the one we care\nabout. In this work we address both problems: first, we take inspiration from\n\\textit{transductive learning} and note that after receiving an input but\nbefore making a prediction, we can fine-tune our networks on any unsupervised\nloss. We call this process {\\em tailoring}, because we customize the model to\neach input to ensure our prediction satisfies the inductive bias. Second, we\nformulate {\\em meta-tailoring}, a nested optimization similar to that in\nmeta-learning, and train our models to perform well on the task objective after\nadapting them using an unsupervised loss. The advantages of tailoring and\nmeta-tailoring are discussed theoretically and demonstrated empirically on a\ndiverse set of examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1\">Ferran Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1\">Maria Bauza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuru_N/0/1/0/all/0/1\">Nurullah Giray Kuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie Pack Kaelbling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out-of-Distribution Detection for Automotive Perception. (arXiv:2011.01413v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.01413","description":"<p>Neural networks (NNs) are widely used for object classification in autonomous\ndriving. However, NNs can fail on input data not well represented by the\ntraining dataset, known as out-of-distribution (OOD) data. A mechanism to\ndetect OOD samples is important for safety-critical applications, such as\nautomotive perception, to trigger a safe fallback mode. NNs often rely on\nsoftmax normalization for confidence estimation, which can lead to high\nconfidences being assigned to OOD samples, thus hindering the detection of\nfailures. This paper presents a method for determining whether inputs are OOD,\nwhich does not require OOD data during training and does not increase the\ncomputational cost of inference. The latter property is especially important in\nautomotive applications with limited computational resources and real-time\nconstraints. Our proposed approach outperforms state-of-the-art methods on\nreal-world automotive datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nitsch_J/0/1/0/all/0/1\">Julia Nitsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itkina_M/0/1/0/all/0/1\">Masha Itkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1\">Ransalu Senanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_J/0/1/0/all/0/1\">Juan Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1\">Max Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1\">Roland Siegwart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_C/0/1/0/all/0/1\">Cesar Cadena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHAD3S: A model to Sketch, Shade and Shadow. (arXiv:2011.06822v3 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2011.06822","description":"<p>Hatching is a common method used by artists to accentuate the third dimension\nof a sketch, and to illuminate the scene. Our system SHAD3S attempts to compete\nwith a human at hatching generic three-dimensional (3D) shapes, and also tries\nto assist her in a form exploration exercise. The novelty of our approach lies\nin the fact that we make no assumptions about the input other than that it\nrepresents a 3D shape, and yet, given a contextual information of illumination\nand texture, we synthesise an accurate hatch pattern over the sketch, without\naccess to 3D or pseudo 3D. In the process, we contribute towards a) a cheap yet\neffective method to synthesise a sufficiently large high fidelity dataset,\npertinent to task; b) creating a pipeline with conditional generative\nadversarial network (CGAN); and c) creating an interactive utility with GIMP,\nthat is a tool for artists to engage with automated hatching or a\nform-exploration exercise. User evaluation of the tool suggests that the model\nperformance does generalise satisfactorily over diverse input, both in terms of\nstyle as well as shape. A simple comparison of inception scores suggest that\nthe generated distribution is as diverse as the ground truth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkataramaiyer_R/0/1/0/all/0/1\">Raghav B. Venkataramaiyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Abhishek Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Saisha Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Data Hiding Using Inverse Gradient Attention. (arXiv:2011.10850v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.10850","description":"<p>Data hiding is the procedure of encoding desired information into the cover\nimage to resist potential noises while ensuring the embedded image has few\nperceptual perturbations from the original one. Recently, with the tremendous\nsuccesses gained by deep neural networks in various fields, the researches of\ndata hiding with deep learning models have attracted an increasing number of\nattentions. In the data hiding task, each pixel of cover images should be\ntreated differently since they have divergent tolerabilities. The neglect of\nconsidering the sensitivity of each pixel will inevitably affect the model\nrobustness for information hiding. Targeting this problem, we propose a novel\ndeep data hiding scheme with Inverse Gradient Attention (IGA), combing the\nideas of adversarial learning and attention mechanism to endow different\nsensitivities for different pixels. With the proposed component, the model can\nspotlight pixels with more robustness for data hiding. Empirically, extensive\nexperiments show that the proposed model outperforms the state-of-the-art\nmethods on two prevalent datasets under multiple evaluations. Besides, we\nfurther identify and discuss the connections between the proposed inverse\ngradient attention and high-frequency regions within images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honglei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuanzhouhan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yidong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dissecting Image Crops. (arXiv:2011.11831v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11831","description":"<p>The elementary operation of cropping underpins nearly every computer vision\nsystem, ranging from data augmentation and translation invariance to\ncomputational photography and representation learning. This paper investigates\nthe subtle traces introduced by this operation. For example, despite\nrefinements to camera optics, lenses will leave behind certain clues, notably\nchromatic aberration and vignetting. Photographers also leave behind other\nclues relating to image aesthetics and scene composition. We study how to\ndetect these traces, and investigate the impact that cropping has on the image\ndistribution. While our aim is to dissect the fundamental impact of spatial\ncrops, there are also a number of practical implications to our work, such as\nrevealing faulty photojournalism and equipping neural network researchers with\na better understanding of shortcut learning. Code is available at\nhttps://github.com/basilevh/dissecting-image-crops.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoorick_B/0/1/0/all/0/1\">Basile Van Hoorick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01338","description":"<p>Training deep learning models in technical domains is often accompanied by\nthe challenge that although the task is clear, insufficient data for training\nis available. In this work, we propose a novel approach based on the\ncombination of Siamese networks and radial basis function networks to perform\ndata-efficient classification without pretraining by measuring the distance\nbetween images in semantic space in a data-efficient manner. We develop the\nmodels using three technical datasets, the NEU dataset, the BSD dataset, and\nthe TEX dataset. In addition to the technical domain, we show the general\napplicability to classical datasets (cifar10 and MNIST) as well. The approach\nis tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise\nreduction of the number of samples available for training. The authors show\nthat the proposed approach outperforms the state-of-the-art models in the low\ndata regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlagenhauf_T/0/1/0/all/0/1\">Tobias Schlagenhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildirim_F/0/1/0/all/0/1\">Faruk Yildirim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruckner_B/0/1/0/all/0/1\">Benedikt Br&#xfc;ckner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleischer_J/0/1/0/all/0/1\">J&#xfc;rgen Fleischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Surprising Efficiency of Committee-based Models. (arXiv:2012.01988v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01988","description":"<p>Committee-based models, i.e., model ensembles or cascades, are underexplored\nin recent work on developing efficient models. While committee-based models\nthemselves are not new, there lacks a systematic understanding of their\nefficiency in comparison with single models. To fill this gap, we conduct a\ncomprehensive analysis of the efficiency of committee-based models. We find\nthat committee-based models provide a complementary paradigm to achieve\nsuperior efficiency without tuning the architecture: even the most simplistic\nmethod for building ensembles or cascades from existing pre-trained networks\ncan attain a significant speedup and higher accuracy over state-of-the-art\nsingle models, and also outperforms sophisticated neural architecture search\nmethods (e.g., BigNAS). The superior efficiency of committee-based models holds\ntrue for several tasks, including image classification, video classification,\nand semantic segmentation, and various architecture families, such as\nEfficientNet, ResNet, MobileNetV2, and X3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondratyuk_D/0/1/0/all/0/1\">Dan Kondratyuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christiansen_E/0/1/0/all/0/1\">Eric Christiansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Movshovitz_Attias_Y/0/1/0/all/0/1\">Yair Movshovitz-Attias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eban_E/0/1/0/all/0/1\">Elad Eban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Radiance Flow for 4D View Synthesis and Video Processing. (arXiv:2012.09790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.09790","description":"<p>We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D\nspatial-temporal representation of a dynamic scene from a set of RGB images.\nKey to our approach is the use of a neural implicit representation that learns\nto capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing\nconsistency across different modalities, our representation enables multi-view\nrendering in diverse dynamic scenes, including water pouring, robotic\ninteraction, and real images, outperforming state-of-the-art methods for\nspatial-temporal view synthesis. Our approach works even when inputs images are\ncaptured with only one camera. We further demonstrate that the learned\nrepresentation can serve as an implicit scene prior, enabling video processing\ntasks such as image super-resolution and de-noising without any additional\nsupervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong-Xing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Tightness of Semidefinite Relaxations for Rotation Estimation. (arXiv:2101.02099v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.02099","description":"<p>Why is it that semidefinite relaxations have been so successful in numerous\napplications in computer vision and robotics for solving non-convex\noptimization problems involving rotations? In studying the empirical\nperformance we note that there are few failure cases reported in the\nliterature, in particular for estimation problems with a single rotation,\nmotivating us to gain further theoretical understanding.\n</p>\n<p>A general framework based on tools from algebraic geometry is introduced for\nanalyzing the power of semidefinite relaxations of problems with quadratic\nobjective functions and rotational constraints. Applications include\nregistration, hand-eye calibration and rotation averaging. We characterize the\nextreme points, and show that there exist failure cases for which the\nrelaxation is not tight, even in the case of a single rotation. We also show\nthat some problem classes are always tight given an appropriate\nparametrization. Our theoretical findings are accompanied with numerical\nsimulations, providing further evidence and understanding of the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brynte_L/0/1/0/all/0/1\">Lucas Brynte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_V/0/1/0/all/0/1\">Viktor Larsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1\">Jos&#xe9; Pedro Iglesias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsson_C/0/1/0/all/0/1\">Carl Olsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahl_F/0/1/0/all/0/1\">Fredrik Kahl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Continual Learning in Image Classification: An Empirical Survey. (arXiv:2101.10423v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.10423","description":"<p>Online continual learning for image classification studies the problem of\nlearning to classify images from an online stream of data and tasks, where\ntasks may include new classes (class incremental) or data nonstationarity\n(domain incremental). One of the key challenges of continual learning is to\navoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence\nof more recent tasks. Over the past few years, many methods and tricks have\nbeen introduced to address this problem, but many have not been fairly and\nsystematically compared under a variety of realistic and practical settings. To\nbetter understand the relative advantages of various approaches and the\nsettings where they work best, this survey aims to (1) compare state-of-the-art\nmethods such as MIR, iCARL, and GDumb and determine which works best at\ndifferent experimental settings; (2) determine if the best class incremental\nmethods are also competitive in domain incremental setting; (3) evaluate the\nperformance of 7 simple but effective trick such as \"review\" trick and nearest\nclass mean (NCM) classifier to assess their relative impact. Regarding (1), we\nobserve iCaRL remains competitive when the memory buffer is small; GDumb\noutperforms many recently proposed methods in medium-size datasets and MIR\nperforms the best in larger-scale datasets. For (2), we note that GDumb\nperforms quite poorly while MIR -- already competitive for (1) -- is also\nstrongly competitive in this very different but important setting. Overall,\nthis allows us to conclude that MIR is overall a strong and versatile method\nacross a wide variety of settings. For (3), we find that all 7 tricks are\nbeneficial, and when augmented with the \"review\" trick and NCM classifier, MIR\nproduces performance levels that bring online continual learning much closer to\nits ultimate goal of matching offline training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_Z/0/1/0/all/0/1\">Zheda Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruiwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jihwan Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quispe_D/0/1/0/all/0/1\">David Quispe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning deep learning model parameters for improved super-resolution of dynamic MRI with prior-knowledge. (arXiv:2102.02711v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2102.02711","description":"<p>Dynamic imaging is a beneficial tool for interventions to assess\nphysiological changes. Nonetheless during dynamic MRI, while achieving a high\ntemporal resolution, the spatial resolution is compromised. To overcome this\nspatio-temporal trade-off, this research presents a super-resolution (SR) MRI\nreconstruction with prior knowledge based fine-tuning to maximise spatial\ninformation while reducing the required scan-time for dynamic MRIs. An U-Net\nbased network with perceptual loss is trained on a benchmark dataset and\nfine-tuned using one subject-specific static high resolution MRI as prior\nknowledge to obtain high resolution dynamic images during the inference stage.\n3D dynamic data for three subjects were acquired with different parameters to\ntest the generalisation capabilities of the network. The method was tested for\ndifferent levels of in-plane undersampling for dynamic MRI. The reconstructed\ndynamic SR results after fine-tuning showed higher similarity with the high\nresolution ground-truth, while quantitatively achieving statistically\nsignificant improvement. The average SSIM of the lowest resolution experimented\nduring this research (6.25~\\% of the k-space) before and after fine-tuning were\n0.939 $\\pm$ 0.008 and 0.957 $\\pm$ 0.006 respectively. This could theoretically\nresult in an acceleration factor of 16, which can potentially be acquired in\nless than half a second. The proposed approach shows that the super-resolution\nMRI reconstruction with prior-information can alleviate the spatio-temporal\ntrade-off in dynamic MRI, even for high acceleration factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sarasaen_C/0/1/0/all/0/1\">Chompunuch Sarasaen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumick Chatterjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breitkopf_M/0/1/0/all/0/1\">Mario Breitkopf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rose_G/0/1/0/all/0/1\">Georg Rose</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Speck_O/0/1/0/all/0/1\">Oliver Speck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CORSAIR: Convolutional Object Retrieval and Symmetry-AIded Registration. (arXiv:2103.06911v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.06911","description":"<p>This paper considers online object-level mapping using partial point-cloud\nobservations obtained online in an unknown environment. We develop and approach\nfor fully Convolutional Object Retrieval and Symmetry-AIded Registration\n(CORSAIR). Our model extends the Fully Convolutional Geometric Features model\nto learn a global object-shape embedding in addition to local point-wise\nfeatures from the point-cloud observations. The global feature is used to\nretrieve a similar object from a category database, and the local features are\nused for robust pose registration between the observed and the retrieved\nobject. Our formulation also leverages symmetries, present in the object\nshapes, to obtain promising local-feature pairs from different symmetry classes\nfor matching. We present results from synthetic and real-world datasets with\ndifferent object categories to verify the robustness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qiaojun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jadhav_S/0/1/0/all/0/1\">Sai Jadhav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1\">Nikolay Atanasov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VMAF And Variants: Towards A Unified VQA. (arXiv:2103.07770v6 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.07770","description":"<p>Video quality assessment (VQA) is now a fast-growing subject, maturing in the\nfull reference (FR) case, yet challenging in the exploding no reference (NR)\ncase. We investigate variants of the popular VMAF video quality assessment\nalgorithm for the FR case, using both support vector regression and feedforward\nneural networks. We extend it to the NR case, using some different features but\nsimilar learning, to develop a partially unified framework for VQA. When fully\ntrained, FR algorithms such as VMAF perform well on test datasets, with 90%+\nmatch in PCC and SRCC; but for predicting performance in the wild, we\ntrain/test from scratch for each database. With an 80/20 train/test split, we\nstill achieve 90%+ performance on average in both PCC and SRCC, with 8-9% gains\nover VMAF. Moreover, we even get decent performance (~75%) if we ignore the\nreference, treating FR as NR, partly justifying our attempts at unification. In\nthe true NR case, we reduce complexity vs. leading recent algorithms VIDEVAL,\nRAPIQUE, yet achieve a stunning 90% in SRCC (~12% gain), while roughly matching\nin PCC (78% vs. 79.6%). At lower complexities, we can still achieve 87% in\nSRCC, 70% in PCC. In short, we find encouraging improvements in trainability in\nboth FR and NR, while also constraining computational complexity against\nleading methods\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Topiwala_P/0/1/0/all/0/1\">Pankaj Topiwala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pian_J/0/1/0/all/0/1\">Jiangfeng Pian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biondi_K/0/1/0/all/0/1\">Katalina Biondi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krovvidi_A/0/1/0/all/0/1\">Arvind Krovvidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Triplet-Watershed for Hyperspectral Image Classification. (arXiv:2103.09384v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09384","description":"<p>Hyperspectral images (HSI) consist of rich spatial and spectral information,\nwhich can potentially be used for several applications. However, noise, band\ncorrelations and high dimensionality restrict the applicability of such data.\nThis is recently addressed using creative deep learning network architectures\nsuch as ResNet, SSRN, and A2S2K. However, the last layer, i.e the\nclassification layer, remains unchanged and is taken to be the softmax\nclassifier. In this article, we propose to use a watershed classifier.\nWatershed classifier extends the watershed operator from Mathematical\nMorphology for classification. In its vanilla form, the watershed classifier\ndoes not have any trainable parameters. In this article, we propose a novel\napproach to train deep learning networks to obtain representations suitable for\nthe watershed classifier. The watershed classifier exploits the connectivity\npatterns, a characteristic of HSI datasets, for better inference. We show that\nexploiting such characteristics allows the Triplet-Watershed to achieve\nstate-of-art results in supervised and semi-supervised contexts. These results\nare validated on Indianpines (IP), University of Pavia (UP), Kennedy Space\nCenter (KSC) and University of Houston (UH) datasets, relying on simple convnet\narchitecture using a quarter of parameters compared to previous\nstate-of-the-art networks. The source code for reproducing the experiments and\nsupplementary material (high resolution images) is available at\nhttps://github.com/ac20/TripletWatershed Code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Challa_A/0/1/0/all/0/1\">Aditya Challa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danda_S/0/1/0/all/0/1\">Sravan Danda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagar_B/0/1/0/all/0/1\">B.S.Daya Sagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1\">Laurent Najman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-level Image Retrieval using Reranking Transformers. (arXiv:2103.12236v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12236","description":"<p>Instance-level image retrieval is the task of searching in a large database\nfor images that match an object in a query image. To address this task, systems\nusually rely on a retrieval step that uses global image descriptors, and a\nsubsequent step that performs domain-specific refinements or reranking by\nleveraging operations such as geometric verification based on local features.\nIn this work, we propose Reranking Transformers (RRTs) as a general model to\nincorporate both local and global features to rerank the matching images in a\nsupervised fashion and thus replace the relatively expensive process of\ngeometric verification. RRTs are lightweight and can be easily parallelized so\nthat reranking a set of top matching results can be performed in a single\nforward-pass. We perform extensive experiments on the Revisited Oxford and\nParis datasets, and the Google Landmarks v2 dataset, showing that RRTs\noutperform previous reranking approaches while using much fewer local\ndescriptors. Moreover, we demonstrate that, unlike existing approaches, RRTs\ncan be optimized jointly with the feature extractor, which can lead to feature\nrepresentations tailored to downstream tasks and further accuracy improvements.\nThe code and trained models are publicly available at\nhttps://github.com/uvavision/RerankingTransformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fuwen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiangbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Graphs: A Survey of Generations and Applications. (arXiv:2104.01111v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01111","description":"<p>Scene graph is a structured representation of a scene that can clearly\nexpress the objects, attributes, and relationships between objects in the\nscene. As computer vision technology continues to develop, people are no longer\nsatisfied with simply detecting and recognizing objects in images; instead,\npeople look forward to a higher level of understanding and reasoning about\nvisual scenes. For example, given an image, we want to not only detect and\nrecognize objects in the image, but also know the relationship between objects\n(visual relationship detection), and generate a text description (image\ncaptioning) based on the image content. Alternatively, we might want the\nmachine to tell us what the little girl in the image is doing (Visual Question\nAnswering (VQA)), or even remove the dog from the image and find similar images\n(image editing and retrieval), etc. These tasks require a higher level of\nunderstanding and reasoning for image vision tasks. The scene graph is just\nsuch a powerful tool for scene understanding. Therefore, scene graphs have\nattracted the attention of a large number of researchers, and related research\nis often cross-modal, complex, and rapidly developing. However, no relatively\nsystematic survey of scene graphs exists at present. To this end, this survey\nconducts a comprehensive investigation of the current scene graph research.\nMore specifically, we first summarized the general definition of the scene\ngraph, then conducted a comprehensive and systematic discussion on the\ngeneration method of the scene graph (SGG) and the SGG with the aid of prior\nknowledge. We then investigated the main applications of scene graphs and\nsummarized the most commonly used datasets. Finally, we provide some insights\ninto the future development of scene graphs. We believe this will be a very\nhelpful foundation for future research on scene graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengzhen Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alex Hauptmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robotic Waste Sorter with Agile Manipulation and Quickly Trainable Detector. (arXiv:2104.01260v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2104.01260","description":"<p>Owing to human labor shortages, the automation of labor-intensive manual\nwaste-sorting is needed. The goal of automating waste-sorting is to replace the\nhuman role of robust detection and agile manipulation of waste items with\nrobots. To achieve this, we propose three methods. First, we provide a combined\nmanipulation method using graspless push-and-drop and pick-and-release\nmanipulation. Second, we provide a robotic system that can automatically\ncollect object images to quickly train a deep neural-network model. Third, we\nprovide a method to mitigate the differences in the appearance of target\nobjects from two scenes: one for dataset collection and the other for waste\nsorting in a recycling factory. If differences exist, the performance of a\ntrained waste detector may decrease. We address differences in illumination and\nbackground by applying object scaling, histogram matching with histogram\nequalization, and background synthesis to the source target-object images. Via\nexperiments in an indoor experimental workplace for waste-sorting, we confirm\nthat the proposed methods enable quick collection of the training image sets\nfor three classes of waste items (i.e., aluminum can, glass bottle, and plastic\nbottle) and detection with higher performance than the methods that do not\nconsider the differences. We also confirm that the proposed method enables the\nrobot quickly manipulate the objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiyokawa_T/0/1/0/all/0/1\">Takuya Kiyokawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katayama_H/0/1/0/all/0/1\">Hiroki Katayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tatsuta_Y/0/1/0/all/0/1\">Yuya Tatsuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamatsu_J/0/1/0/all/0/1\">Jun Takamatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogasawara_T/0/1/0/all/0/1\">Tsukasa Ogasawara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intelligent Monitoring of Stress Induced by Water Deficiency in Plants using Deep Learning. (arXiv:2104.07911v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07911","description":"<p>In the recent decade, high-throughput plant phenotyping techniques, which\ncombine non-invasive image analysis and machine learning, have been\nsuccessfully applied to identify and quantify plant health and diseases.\nHowever, these techniques usually do not consider the progressive nature of\nplant stress and often require images showing severe signs of stress to ensure\nhigh confidence detection, thereby reducing the feasibility for early detection\nand recovery of plants under stress. To overcome the problem mentioned above,\nwe propose a deep learning pipeline for the temporal analysis of the visual\nchanges induced in the plant due to stress and apply it to the specific water\nstress identification case in Chickpea plant shoot images. For this, we have\nconsidered an image dataset of two chickpea varieties JG-62 and Pusa-372, under\nthree water stress conditions; control, young seedling, and before flowering,\ncaptured over five months. We have employed a variant of Convolutional Neural\nNetwork - Long Short Term Memory (CNN-LSTM) network to learn spatio-temporal\npatterns from the chickpea plant dataset and use them for water stress\nclassification. Our model has achieved ceiling level classification performance\nof 98.52% on JG-62 and 97.78% on Pusa-372 chickpea plant data and has\noutperformed the best reported time-invariant technique by at least 14% for\nboth JG-62 and Pusa-372 species, to the best of our knowledge. Furthermore, our\nCNN-LSTM model has demonstrated robustness to noisy input, with a less than\n2.5% dip in average model accuracy and a small standard deviation about the\nmean for both species. Lastly, we have performed an ablation study to analyze\nthe performance of the CNN-LSTM model by decreasing the number of temporal\nsession data used for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azimi_S/0/1/0/all/0/1\">Shiva Azimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadhawan_R/0/1/0/all/0/1\">Rohan Wadhawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_T/0/1/0/all/0/1\">Tapan K. Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08773","description":"<p>Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. NLP models built with the conventional paradigm, however, often\nstruggle with generalization across tasks (e.g., a question-answering system\ncannot solve classification tasks). A long-standing challenge in AI is to build\na model that is equipped with the understanding of human-readable instructions\nthat define the tasks, and can generalize to new tasks. To study this, we\nintroduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their\nhuman-authored instructions and 193k task instances. The instructions are\nobtained from crowdsourcing instructions used to collect existing NLP datasets\nand mapped to a unified schema. We adopt generative pre-trained language models\nto encode task-specific instructions along with input and generate task output.\nOur results indicate that models can benefit from instructions to generalize\nacross tasks. These models, however, are far behind supervised task-specific\nmodels, indicating significant room for more progress in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v12 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.10762","description":"<p>Random field and random cluster theory are used to describe certain\nmathematical results concerning the probability distribution of image pixel\nintensities characterized as generic $2D$ integer arrays. The size of the\nsmallest bounded region within an image is estimated for segmenting an image,\nfrom which, the equilibrium distribution of intensities can be recovered. From\nthe estimated bounded regions, properties of the sub-optimal and equilibrium\ndistributions of intensities are derived, which leads to an image compression\nmethodology whereby only slightly more than half of all pixels are required for\na worst-case reconstruction of the original image. A custom deep belief network\nand heuristic allows for the unsupervised segmentation, detection and\nlocalization of objects in an image. An example illustrates the mathematical\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v4 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2104.10785","description":"<p>In this paper, we tackle two important problems in low-rank learning, which\nare partial singular value decomposition and numerical rank estimation of huge\nmatrices. By using the concepts of Krylov subspaces such as Golub-Kahan\nbidiagonalization (GK-bidiagonalization) as well as Ritz vectors, we propose\ntwo methods for solving these problems in a fast and accurate way. Our\nexperiments show the advantages of the proposed methods compared to the\ntraditional and randomized singular value decomposition methods. The proposed\nmethods are appropriate for applications involving huge matrices where the\naccuracy of the desired singular values and also all of their corresponding\nsingular vectors are essential. As a real application, we evaluate the\nperformance of our methods on the problem of Riemannian similarity learning\nbetween two various image datasets of MNIST and USPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Godaz_R/0/1/0/all/0/1\">Reza Godaz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Monsefi_R/0/1/0/all/0/1\">Reza Monsefi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Toutounian_F/0/1/0/all/0/1\">Faezeh Toutounian</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hosseini_R/0/1/0/all/0/1\">Reshad Hosseini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos. (arXiv:2104.12671v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12671","description":"<p>Multimodal self-supervised learning is getting more and more attention as it\nallows not only to train large networks without human supervision but also to\nsearch and retrieve data across various modalities. In this context, this paper\nproposes a self-supervised training framework that learns a common multimodal\nembedding space that, in addition to sharing representations across different\nmodalities, enforces a grouping of semantically similar instances. To this end,\nwe extend the concept of instance-level contrastive learning with a multimodal\nclustering step in the training pipeline to capture semantic similarities\nacross modalities. The resulting embedding space enables retrieval of samples\nacross all modalities, even from unseen datasets and different domains. To\nevaluate our approach, we train our model on the HowTo100M dataset and evaluate\nits zero-shot retrieval capabilities in two challenging domains, namely\ntext-to-video retrieval, and temporal action localization, showing\nstate-of-the-art results on four different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_K/0/1/0/all/0/1\">Kevin Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inpainting Transformer for Anomaly Detection. (arXiv:2104.13897v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13897","description":"<p>Anomaly detection in computer vision is the task of identifying images which\ndeviate from a set of normal images. A common approach is to train deep\nconvolutional autoencoders to inpaint covered parts of an image and compare the\noutput with the original image. By training on anomaly-free samples only, the\nmodel is assumed to not being able to reconstruct anomalous regions properly.\nFor anomaly detection by inpainting we suggest it to be beneficial to\nincorporate information from potentially distant regions. In particular we pose\nanomaly detection as a patch-inpainting problem and propose to solve it with a\npurely self-attention based approach discarding convolutions. The proposed\nInpainting Transformer (InTra) is trained to inpaint covered patches in a large\nsequence of image patches, thereby integrating information across large regions\nof the input image. When training from scratch, in comparison to other methods\nnot using extra training data, InTra achieves results on par with the current\nstate-of-the-art on the MVTec AD dataset for detection and surpassing them on\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pirnay_J/0/1/0/all/0/1\">Jonathan Pirnay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_K/0/1/0/all/0/1\">Keng Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to drive from a world on rails. (arXiv:2105.00636v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2105.00636","description":"<p>We learn an interactive vision-based driving policy from pre-recorded driving\nlogs via a model-based approach. A forward model of the world supervises a\ndriving policy that predicts the outcome of any potential driving trajectory.\nTo support learning from pre-recorded logs, we assume that the world is on\nrails, meaning neither the agent nor its actions influence the environment.\nThis assumption greatly simplifies the learning problem, factorizing the\ndynamics into a nonreactive world model and a low-dimensional and compact\nforward model of the ego-vehicle. Our approach computes action-values for each\ntraining trajectory using a tabular dynamic-programming evaluation of the\nBellman equations; these action-values in turn supervise the final vision-based\ndriving policy. Despite the world-on-rails assumption, the final driving policy\nacts well in a dynamic and reactive world. At the time of writing, our method\nranks first on the CARLA leaderboard, attaining a 25% higher driving score\nwhile using 40 times less data. Our method is also an order of magnitude more\nsample-efficient than state-of-the-art model-free reinforcement learning\ntechniques on navigational tasks in the ProcGen benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1\">Philipp Kr&#xe4;henb&#xfc;hl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task fully convolutional network for tree species mapping in dense forests using small training hyperspectral data. (arXiv:2106.00799v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00799","description":"<p>This work proposes a multi-task fully convolutional architecture for tree\nspecies mapping in dense forests from sparse and scarce polygon-level\nannotations using hyperspectral UAV-borne data. Our model implements a partial\nloss function that enables dense tree semantic labeling outcomes from non-dense\ntraining samples, and a distance regression complementary task that enforces\ntree crown boundary constraints and substantially improves the model\nperformance. Our multi-task architecture uses a shared backbone network that\nlearns common representations for both tasks and two task-specific decoders,\none for the semantic segmentation output and one for the distance map\nregression. We report that introducing the complementary task boosts the\nsemantic segmentation performance compared to the single-task counterpart in up\nto 11% reaching an average user's accuracy of 88.63% and an average producer's\naccuracy of 88.59%, achieving state-of-art performance for tree species\nclassification in tropical forests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_L/0/1/0/all/0/1\">Laura Elena Cu&#xe9; La Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sothe_C/0/1/0/all/0/1\">Camile Sothe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feitosa_R/0/1/0/all/0/1\">Raul Queiroz Feitosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_C/0/1/0/all/0/1\">Cl&#xe1;udia Maria de Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schimalski_M/0/1/0/all/0/1\">Marcos Benedito Schimalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Dario Augusto Borges Oliveira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rotation Equivariant Feature Image Pyramid Network for Object Detection in Optical Remote Sensing Imagery. (arXiv:2106.00880v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00880","description":"<p>Detection of objects is extremely important in various aerial vision-based\napplications. Over the last few years, the methods based on convolution neural\nnetworks have made substantial progress. However, because of the large variety\nof object scales, densities, and arbitrary orientations, the current detectors\nstruggle with the extraction of semantically strong features for small-scale\nobjects by a predefined convolution kernel. To address this problem, we propose\nthe rotation equivariant feature image pyramid network (REFIPN), an image\npyramid network based on rotation equivariance convolution. The proposed model\nadopts single-shot detector in parallel with a lightweight image pyramid module\nto extract representative features and generate regions of interest in an\noptimization approach. The proposed network extracts feature in a wide range of\nscales and orientations by using novel convolution filters. These features are\nused to generate vector fields and determine the weight and angle of the\nhighest-scoring orientation for all spatial locations on an image. By this\napproach, the performance for small-sized object detection is enhanced without\nsacrificing the performance for large-sized object detection. The performance\nof the proposed model is validated on two commonly used aerial benchmarks and\nthe results show our proposed model can achieve state-of-the-art performance\nwith satisfactory efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shamsolmoali_P/0/1/0/all/0/1\">Pourya Shamsolmoali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareapoor_M/0/1/0/all/0/1\">Masoumeh Zareapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling the Background Bias in Sparse Object Detection via Cropped Windows. (arXiv:2106.02288v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02288","description":"<p>Object detection on Unmanned Aerial Vehicles (UAVs) is still a challenging\ntask. The recordings are mostly sparse and contain only small objects. In this\nwork, we propose a simple tiling method that improves the detection capability\nin the remote sensing case without modifying the model itself. By reducing the\nbackground bias and enabling the usage of higher image resolutions during\ntraining, our method can improve the performance of models substantially. The\nprocedure was validated on three different data sets and outperformed similar\napproaches in performance and speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varga_L/0/1/0/all/0/1\">Leon Amadeus Varga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual Temporal Samples for Recurrent Neural Networks: applied to semantic segmentation in agriculture. (arXiv:2106.10118v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10118","description":"<p>This paper explores the potential for performing temporal semantic\nsegmentation in the context of agricultural robotics without temporally\nlabelled data. We achieve this by proposing to generate virtual temporal\nsamples from labelled still images. By exploiting the relatively static scene\nand assuming that the robot (camera) moves we are able to generate virtually\nlabelled temporal sequences with no extra annotation effort. Normally, to train\na recurrent neural network (RNN), labelled samples from a video (temporal)\nsequence are required which is laborious and has stymied work in this\ndirection. By generating virtual temporal samples, we demonstrate that it is\npossible to train a lightweight RNN to perform semantic segmentation on two\nchallenging agricultural datasets. Our results show that by training a temporal\nsemantic segmenter using virtual samples we can increase the performance by an\nabsolute amount of $4.6$ and $4.9$ on sweet pepper and sugar beet datasets,\nrespectively. This indicates that our virtual data augmentation technique is\nable to accurately classify agricultural images temporally without the use of\ncomplicated synthetic data generation techniques nor with the overhead of\nlabelling large amounts of temporal sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_A/0/1/0/all/0/1\">Alireza Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halstead_M/0/1/0/all/0/1\">Michael Halstead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCool_C/0/1/0/all/0/1\">Chris McCool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Marching Cubes. (arXiv:2106.11272v3 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2106.11272","description":"<p>We introduce Neural Marching Cubes (NMC), a data-driven approach for\nextracting a triangle mesh from a discretized implicit field. Classical MC is\ndefined by coarse tessellation templates isolated to individual cubes. While\nmore refined tessellations have been proposed, they all make heuristic\nassumptions, such as trilinearity, when determining the vertex positions and\nlocal mesh topologies in each cube. In principle, none of these approaches can\nreconstruct geometric features that reveal coherence or dependencies between\nnearby cubes (e.g., a sharp edge), as such information is unaccounted for,\nresulting in poor estimates of the true underlying implicit field. To tackle\nthese challenges, we re-cast MC from a deep learning perspective, by designing\ntessellation templates more apt at preserving geometric features, and learning\nthe vertex positions and mesh topologies from training meshes, to account for\ncontextual information from nearby cubes. We develop a compact per-cube\nparameterization to represent the output triangle mesh, while being compatible\nwith neural processing, so that a simple 3D convolutional network can be\nemployed for the training. We show that all topological cases in each cube that\nare applicable to our design can be easily derived using our representation,\nand the resulting tessellations can also be obtained naturally and efficiently\nby following a few design guidelines. In addition, our network learns local\nfeatures with limited receptive fields, hence it generalizes well to new shapes\nand new datasets. We evaluate our neural MC approach by quantitative and\nqualitative comparisons to all well-known MC variants. In particular, we\ndemonstrate the ability of our network to recover sharp features such as edges\nand corners, a long-standing issue of MC and its variants. Our network also\nreconstructs local mesh topologies more accurately than previous approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey of Image-Based Food Recognition and Volume Estimation Methods for Dietary Assessment. (arXiv:2106.11776v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11776","description":"<p>Dietary studies showed that dietary-related problem such as obesity is\nassociated with other chronic diseases like hypertension, irregular blood sugar\nlevels, and increased risk of heart attacks. The primary cause of these\nproblems is poor lifestyle choices and unhealthy dietary habits, which are\nmanageable using interactive mHealth apps. However, traditional dietary\nmonitoring systems using manual food logging suffer from imprecision,\nunderreporting, time consumption, and low adherence. Recent dietary monitoring\nsystems tackle these challenges by automatic assessment of dietary intake\nthrough machine learning methods. This survey discusses the most performing\nmethodologies that have been developed so far for automatic food recognition\nand volume estimation. First, we will present the rationale of visual-based\nmethods for food recognition. The core of the paper is the presentation,\ndiscussion and evaluation of these methods on popular food image databases.\nFollowing that, we discussed the mobile applications that are implementing\nthese methods. The survey ends with a discussion of research gaps and open\nissues in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1\">Ghalib Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topological Semantic Mapping by Consolidation of Deep Visual Features. (arXiv:2106.12709v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12709","description":"<p>Many works in the recent literature introduce semantic mapping methods that\nuse CNNs (Convolutional Neural Networks) to recognize semantic properties in\nimages. The types of properties (eg.: room size, place category, and objects)\nand their classes (eg.: kitchen and bathroom, for place category) are usually\npredefined and restricted to a specific task. Thus, all the visual data\nacquired and processed during the construction of the maps are lost and only\nthe recognized semantic properties remain on the maps. In contrast, this work\nintroduces a topological semantic mapping method that uses deep visual features\nextracted by a CNN (GoogLeNet), from 2D images captured in multiple views of\nthe environment as the robot operates, to create, through averages,\nconsolidated representations of the visual features acquired in the regions\ncovered by each topological node. These representations allow flexible\nrecognition of semantic properties of the regions and use in other visual\ntasks. Experiments with a real-world indoor dataset showed that the method is\nable to consolidate the visual features of regions and use them to recognize\nobjects and place categories as semantic properties, and to indicate the\ntopological location of images, with very promising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sousa_Y/0/1/0/all/0/1\">Ygor C. N. Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassani_H/0/1/0/all/0/1\">Hansenclever F. Bassani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Raw-to-Raw Mapping. (arXiv:2106.13883v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13883","description":"<p>The raw-RGB colors of a camera sensor vary due to the spectral sensitivity\ndifferences across different sensor makes and models. This paper focuses on the\ntask of mapping between different sensor raw-RGB color spaces. Prior work\naddressed this problem using a pairwise calibration to achieve accurate color\nmapping. Although being accurate, this approach is less practical as it\nrequires: (1) capturing pair of images by both camera devices with a color\ncalibration object placed in each new scene; (2) accurate image alignment or\nmanual annotation of the color calibration object. This paper aims to tackle\ncolor mapping in the raw space through a more practical setup. Specifically, we\npresent a semi-supervised raw-to-raw mapping method trained on a small set of\npaired images alongside an unpaired set of images captured by each camera\ndevice. Through extensive experiments, we show that our method achieves better\nresults compared to other domain adaptation alternatives in addition to the\nsingle-calibration solution. We have generated a new dataset of raw images from\ntwo different smartphone cameras as part of this effort. Our dataset includes\nunpaired and paired sets for our semi-supervised training and evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAMS: Color-Aware Multi-Style Transfer. (arXiv:2106.13920v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13920","description":"<p>Image style transfer aims to manipulate the appearance of a source image, or\n\"content\" image, to share similar texture and colors of a target \"style\" image.\nIdeally, the style transfer manipulation should also preserve the semantic\ncontent of the source image. A commonly used approach to assist in transferring\nstyles is based on Gram matrix optimization. One problem of Gram matrix-based\noptimization is that it does not consider the correlation between colors and\ntheir styles. Specifically, certain textures or structures should be associated\nwith specific colors. This is particularly challenging when the target style\nimage exhibits multiple style types. In this work, we propose a color-aware\nmulti-style transfer method that generates aesthetically pleasing results while\npreserving the style-color correlation between style and generated images. We\nachieve this desired outcome by introducing a simple but efficient modification\nto classic Gram matrix-based style transfer optimization. A nice feature of our\nmethod is that it enables the users to manually select the color associations\nbetween the target style and content image for more transfer flexibility. We\nvalidated our method with several qualitative comparisons, including a user\nstudy conducted with 30 participants. In comparison with prior work, our method\nis simple, easy to implement, and achieves visually appealing results when\ntargeting images that have multiple styles. Source code is available at\nhttps://github.com/mahmoudnafifi/color-aware-style-transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussien_M/0/1/0/all/0/1\">Mostafa Hussien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Heterogeneous Label Noise Impact Generalization in Neural Nets?. (arXiv:2106.15475v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15475","description":"<p>Incorrectly labeled examples, or label noise, is common in real-world\ncomputer vision datasets. While the impact of label noise on learning in deep\nneural networks has been studied in prior work, these studies have exclusively\nfocused on homogeneous label noise, i.e., the degree of label noise is the same\nacross all categories. However, in the real-world, label noise is often\nheterogeneous, with some categories being affected to a greater extent than\nothers. Here, we address this gap in the literature. We hypothesized that\nheterogeneous label noise would only affect the classes that had label noise\nunless there was transfer from those classes to the classes without label\nnoise. To test this hypothesis, we designed a series of computer vision studies\nusing MNIST, CIFAR-10, CIFAR-100, and MS-COCO where we imposed heterogeneous\nlabel noise during the training of multi-class, multi-task, and multi-label\nsystems. Our results provide evidence in support of our hypothesis: label noise\nonly affects the class affected by it unless there is transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanal_B/0/1/0/all/0/1\">Bidur Khanal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Greedy Offset-Guided Keypoint Grouping for Human Pose Estimation. (arXiv:2107.03098v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03098","description":"<p>We propose a simple yet reliable bottom-up approach with a good trade-off\nbetween accuracy and efficiency for the problem of multi-person pose\nestimation. Given an image, we employ an Hourglass Network to infer all the\nkeypoints from different persons indiscriminately as well as the guiding\noffsets connecting the adjacent keypoints belonging to the same persons. Then,\nwe greedily group the candidate keypoints into multiple human poses (if any),\nutilizing the predicted guiding offsets. And we refer to this process as greedy\noffset-guided keypoint grouping (GOG). Moreover, we revisit the\nencoding-decoding method for the multi-person keypoint coordinates and reveal\nsome important facts affecting accuracy. Experiments have demonstrated the\nobvious performance improvements brought by the introduced components. Our\napproach is comparable to the state of the art on the challenging COCO dataset\nunder fair conditions. The source code and our pre-trained model are publicly\navailable online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Linhua Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zengfu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view Image-based Hand Geometry Refinement using Differentiable Monte Carlo Ray Tracing. (arXiv:2107.05509v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05509","description":"<p>The amount and quality of datasets and tools available in the research field\nof hand pose and shape estimation act as evidence to the significant progress\nthat has been made.However, even the datasets of the highest quality, reported\nto date, have shortcomings in annotation. We propose a refinement approach,\nbased on differentiable ray tracing,and demonstrate how a high-quality publicly\navailable, multi-camera dataset of hands(InterHand2.6M) can become an even\nbetter dataset, with respect to annotation quality. Differentiable ray tracing\nhas not been employed so far to relevant problems and is hereby shown to be\nsuperior to the approximative alternatives that have been employed in the past.\nTo tackle the lack of reliable ground truth, as far as quantitative evaluation\nis concerned, we resort to realistic synthetic data, to show that the\nimprovement we induce is indeed significant. The same becomes evident in real\ndata through visual evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karvounas_G/0/1/0/all/0/1\">Giorgos Karvounas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyriazis_N/0/1/0/all/0/1\">Nikolaos Kyriazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomidis_I/0/1/0/all/0/1\">Iason Oikonomidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoli_A/0/1/0/all/0/1\">Aggeliki Tsoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argyros_A/0/1/0/all/0/1\">Antonis A. Argyros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRIVE: Deep Reinforced Accident Anticipation with Visual Explanation. (arXiv:2107.10189v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10189","description":"<p>Traffic accident anticipation aims to accurately and promptly predict the\noccurrence of a future accident from dashcam videos, which is vital for a\nsafety-guaranteed self-driving system. To encourage an early and accurate\ndecision, existing approaches typically focus on capturing the cues of spatial\nand temporal context before a future accident occurs. However, their\ndecision-making lacks visual explanation and ignores the dynamic interaction\nwith the environment. In this paper, we propose Deep ReInforced accident\nanticipation with Visual Explanation, named DRIVE. The method simulates both\nthe bottom-up and top-down visual attention mechanism in a dashcam observation\nenvironment so that the decision from the proposed stochastic multi-task agent\ncan be visually explained by attentive regions. Moreover, the proposed dense\nanticipation reward and sparse fixation reward are effective in training the\nDRIVE model with our improved reinforcement learning algorithm. Experimental\nresults show that the DRIVE model achieves state-of-the-art performance on\nmultiple real-world traffic accident datasets. Code and pre-trained model are\navailable at \\url{https://www.rit.edu/actionlab/drive}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1\">Wentao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yu Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Image Semantic Segmentation. (arXiv:2107.13978v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13978","description":"<p>Semantic segmentation models trained on public datasets have achieved great\nsuccess in recent years. However, these models didn't consider the\npersonalization issue of segmentation though it is important in practice. In\nthis paper, we address the problem of personalized image segmentation. The\nobjective is to generate more accurate segmentation results on unlabeled\npersonalized images by investigating the data's personalized traits. To open up\nfuture research in this area, we collect a large dataset containing various\nusers' personalized images called PIS (Personalized Image Semantic\nSegmentation). We also survey some recent researches related to this problem\nand report their performance on our dataset. Furthermore, by observing the\ncorrelation among a user's personalized images, we propose a baseline method\nthat incorporates the inter-image context when segmenting certain images.\nExtensive experiments show that our method outperforms the existing methods on\nthe proposed dataset. The code and the PIS dataset will be made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chang-Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng-Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_F/0/1/0/all/0/1\">Feng Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Free Lunch for Co-Saliency Detection: Context Adjustment. (arXiv:2108.02093v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02093","description":"<p>We unveil a long-standing problem in the prevailing co-saliency detection\nsystems: there is indeed inconsistency between training and testing.\nConstructing a high-quality co-saliency detection dataset involves\ntime-consuming and labor-intensive pixel-level labeling, which has forced most\nrecent works to rely instead on semantic segmentation or saliency detection\ndatasets for training. However, the lack of proper co-saliency and the absence\nof multiple foreground objects in these datasets can lead to spurious\nvariations and inherent biases learned by models. To tackle this, we introduce\nthe idea of counterfactual training through context adjustment, and propose a\n\"cost-free\" group-cut-paste (GCP) procedure to leverage images from\noff-the-shelf saliency detection datasets and synthesize new samples. Following\nGCP, we collect a novel dataset called Context Adjustment Training (CAT). CAT\nconsists of 33,500 images, making it four times larger than the current\nco-saliency detection datasets. All images are automatically annotated with\nhigh-quality mask annotations, object categories, and edge maps. Extensive\nexperiments with state-of-the-art models are conducted to demonstrate the\nsuperiority of our dataset. We hope that the scale, diversity, and quality of\nour dataset can benefit researchers in this area and beyond. The dataset and\nbenchmark toolkit will be publicly accessible through our project page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingdong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_P/0/1/0/all/0/1\">Prakhar Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02644","description":"<p>Capsule Networks (CapsNets) is a machine learning architecture proposed to\novercome some of the shortcomings of convolutional neural networks (CNNs).\nHowever, CapsNets have mainly outperformed CNNs in datasets where images are\nsmall and/or the objects to identify have minimal background noise. In this\nwork, we present a new architecture, parallel CapsNets, which exploits the\nconcept of branching the network to isolate certain capsules, allowing each\nbranch to identify different entities. We applied our concept to the two\ncurrent types of CapsNet architectures, studying the performance for networks\nwith different layers of capsules. We tested our design in a public, highly\nunbalanced dataset of acute myeloid leukaemia images (15 classes). Our\nexperiments showed that conventional CapsNets show similar performance than our\nbaseline CNN (ResNeXt-50) but depict instability problems. In contrast,\nparallel CapsNets can outperform ResNeXt-50, is more stable, and shows better\nrotational invariance than both, conventional CapsNets and ResNeXt-50.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1\">Arijit Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engkvist_O/0/1/0/all/0/1\">Ola Engkvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Embodied Vision Navigation: A Survey. (arXiv:2108.04097v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.04097","description":"<p>Navigation is one of the fundamental features of a autonomous robot. And the\nability of long-term navigation with semantic instruction is a `holy grail`\ngoals of intelligent robots. The development of 3D simulation technology\nprovide a large scale of data to simulate the real-world environment. The deep\nlearning proves its ability to robustly learn various embodied navigation\ntasks. However, deep learning on embodied navigation is still in its infancy\ndue to the unique challenges faced by the navigation exploration and learning\nfrom partial observed visual input. Recently, deep learning in embodied\nnavigation has become even thriving, with numerous methods have been proposed\nto tackle different challenges in this area. To give a promising direction for\nfuture research, in this paper, we present a comprehensive review of embodied\nnavigation tasks and the recent progress in deep learning based methods. It\nincludes two major tasks: target-oriented navigation and the\ninstruction-oriented navigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengda Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset. (arXiv:2108.05080v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05080","description":"<p>While significant advancements have been made in the generation of deepfakes\nusing deep learning technologies, its misuse is a well-known issue now.\nDeepfakes can cause severe security and privacy issues as they can be used to\nimpersonate a person's identity in a video by replacing his/her face with\nanother person's face. Recently, a new problem of generating synthesized human\nvoice of a person is emerging, where AI-based deep learning models can\nsynthesize any person's voice requiring just a few seconds of audio. With the\nemerging threat of impersonation attacks using deepfake audios and videos, a\nnew generation of deepfake detectors is needed to focus on both video and audio\ncollectively. A large amount of good quality datasets is typically required to\ncapture the real-world scenarios to develop a competent deepfake detector.\nExisting deepfake datasets either contain deepfake videos or audios, which are\nracially biased as well. Hence, there is a crucial need for creating a good\nvideo as well as an audio deepfake dataset, which can be used to detect audio\nand video deepfake simultaneously. To fill this gap, we propose a novel\nAudio-Video Deepfake dataset (FakeAVCeleb) that contains not only deepfake\nvideos but also respective synthesized lip-synced fake audios. We generate this\ndataset using the current most popular deepfake generation methods. We selected\nreal YouTube videos of celebrities with four racial backgrounds (Caucasian,\nBlack, East Asian, and South Asian) to develop a more realistic multimodal\ndataset that addresses racial bias and further help develop multimodal deepfake\ndetectors. We performed several experiments using state-of-the-art detection\nmethods to evaluate our deepfake dataset and demonstrate the challenges and\nusefulness of our multimodal Audio-Video deepfake dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalid_H/0/1/0/all/0/1\">Hasam Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence Adaptive Regularization for Deep Learning with Noisy Labels. (arXiv:2108.08212v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.08212","description":"<p>Recent studies on the memorization effects of deep neural networks on noisy\nlabels show that the networks first fit the correctly-labeled training samples\nbefore memorizing the mislabeled samples. Motivated by this early-learning\nphenomenon, we propose a novel method to prevent memorization of the mislabeled\nsamples. Unlike the existing approaches which use the model output to identify\nor ignore the mislabeled samples, we introduce an indicator branch to the\noriginal model and enable the model to produce a confidence value for each\nsample. The confidence values are incorporated in our loss function which is\nlearned to assign large confidence values to correctly-labeled samples and\nsmall confidence values to mislabeled samples. We also propose an auxiliary\nregularization term to further improve the robustness of the model. To improve\nthe performance, we gradually correct the noisy labels with a well-designed\ntarget estimation strategy. We provide the theoretical analysis and conduct the\nexperiments on synthetic and real-world datasets, demonstrating that our\napproach achieves comparable results to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yangdi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_Y/0/1/0/all/0/1\">Yang Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wenbo He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ranking Models in Unlabeled New Environments. (arXiv:2108.10310v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10310","description":"<p>Consider a scenario where we are supplied with a number of ready-to-use\nmodels trained on a certain source domain and hope to directly apply the most\nappropriate ones to different target domains based on the models' relative\nperformance. Ideally we should annotate a validation set for model performance\nassessment on each new target environment, but such annotations are often very\nexpensive. Under this circumstance, we introduce the problem of ranking models\nin unlabeled new environments. For this problem, we propose to adopt a proxy\ndataset that 1) is fully labeled and 2) well reflects the true model rankings\nin a given target environment, and use the performance rankings on the proxy\nsets as surrogates. We first select labeled datasets as the proxy.\nSpecifically, datasets that are more similar to the unlabeled target domain are\nfound to better preserve the relative performance rankings. Motivated by this,\nwe further propose to search the proxy set by sampling images from various\ndatasets that have similar distributions as the target. We analyze the problem\nand its solutions on the person re-identification (re-ID) task, for which\nsufficient datasets are publicly available, and show that a carefully\nconstructed proxy set effectively captures relative performance ranking in new\nenvironments. Code is available at \\url{https://github.com/sxzrt/Proxy-Set}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoxiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yunzhong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weijian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPFN: Cascaded Primitive Fitting Networks for High-Resolution Point Clouds. (arXiv:2109.00113v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00113","description":"<p>Representing human-made objects as a collection of base primitives has a long\nhistory in computer vision and reverse engineering. In the case of\nhigh-resolution point cloud scans, the challenge is to be able to detect both\nlarge primitives as well as those explaining the detailed parts. While the\nclassical RANSAC approach requires case-specific parameter tuning,\nstate-of-the-art networks are limited by memory consumption of their backbone\nmodules such as PointNet++, and hence fail to detect the fine-scale primitives.\nWe present Cascaded Primitive Fitting Networks (CPFN) that relies on an\nadaptive patch sampling network to assemble detection results of global and\nlocal primitive detection networks. As a key enabler, we present a merging\nformulation that dynamically aggregates the primitives across global and local\nscales. Our evaluation demonstrates that CPFN improves the state-of-the-art\nSPFN performance by 13-14% on high-resolution point cloud datasets and\nspecifically improves the detection of fine-scale primitives by 20-22%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_E/0/1/0/all/0/1\">Eric-Tuan L&#xea;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhyuk Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1\">Duygu Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mech_R/0/1/0/all/0/1\">Radomir Mech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boubekeur_T/0/1/0/all/0/1\">Tamy Boubekeur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regional Adversarial Training for Better Robust Generalization. (arXiv:2109.00678v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00678","description":"<p>Adversarial training (AT) has been demonstrated as one of the most promising\ndefense methods against various adversarial attacks. To our knowledge, existing\nAT-based methods usually train with the locally most adversarial perturbed\npoints and treat all the perturbed points equally, which may lead to\nconsiderably weaker adversarial robust generalization on test data. In this\nwork, we introduce a new adversarial training framework that considers the\ndiversity as well as characteristics of the perturbed points in the vicinity of\nbenign samples. To realize the framework, we propose a Regional Adversarial\nTraining (RAT) defense method that first utilizes the attack path generated by\nthe typical iterative attack method of projected gradient descent (PGD), and\nconstructs an adversarial region based on the attack path. Then, RAT samples\ndiverse perturbed training points efficiently inside this region, and utilizes\na distance-aware label smoothing mechanism to capture our intuition that\nperturbed points at different locations should have different impact on the\nmodel performance. Extensive experiments on several benchmark datasets show\nthat RAT consistently makes significant improvement on standard adversarial\ntraining (SAT), and exhibits better robust generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chuanbiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yichen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Baoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-temporal-spectral-angular observation model that integrates observations from UAV and mobile mapping vehicle for better urban mapping. (arXiv:2109.00900v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00900","description":"<p>In a complex urban scene, observation from a single sensor unavoidably leads\nto voids in observations, failing to describe urban objects in a comprehensive\nmanner. In this paper, we propose a spatio-temporal-spectral-angular\nobservation model to integrate observations from UAV and mobile mapping vehicle\nplatform, realizing a joint, coordinated observation operation from both air\nand ground. We develop a multi-source remote sensing data acquisition system to\neffectively acquire multi-angle data of complex urban scenes. Multi-source data\nfusion solves the missing data problem caused by occlusion and achieves\naccurate, rapid, and complete collection of holographic spatial and temporal\ninformation in complex urban scenes. We carried out an experiment on Baisha\nTown, Chongqing, China and obtained multi-sensor, multi-angle data from UAV and\nmobile mapping vehicle. We first extracted the point cloud from UAV and then\nintegrated the UAV and mobile mapping vehicle point cloud. The integrated\nresults combined both the characteristic of UAV and mobile mapping vehicle\npoint cloud, confirming the practicability of the proposed joint data\nacquisition platform and the effectiveness of spatio-temporal-spectral-angular\nobservation model. Compared with the observation from UAV or mobile mapping\nvehicle alone, the integrated system provides an effective data acquisition\nsolution towards comprehensive urban monitoring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhenfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhipeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards disease-aware image editing of chest X-rays. (arXiv:2109.01071v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.01071","description":"<p>Disease-aware image editing by means of generative adversarial networks\n(GANs) constitutes a promising avenue for advancing the use of AI in the\nhealthcare sector. Here, we present a proof of concept of this idea. While\nGAN-based techniques have been successful in generating and manipulating\nnatural images, their application to the medical domain, however, is still in\nits infancy. Working with the CheXpert data set, we show that StyleGAN can be\ntrained to generate realistic chest X-rays. Inspired by the Cyclic Reverse\nGenerator (CRG) framework, we train an encoder that allows for faithfully\ninverting the generator on synthetic X-rays and provides organ-level\nreconstructions of real ones. Employing a guided manipulation of latent codes,\nwe confer the medical condition of cardiomegaly (increased heart size) onto\nreal X-rays from healthy patients. This work was presented in the Medical\nImaging meets Neurips Workshop 2020, which was held as part of the 34th\nConference on Neural Information Processing Systems (NeurIPS 2020) in\nVancouver, Canada\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saboo_A/0/1/0/all/0/1\">Aakash Saboo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramachandran_S/0/1/0/all/0/1\">Sai Niranjan Ramachandran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dierkes_K/0/1/0/all/0/1\">Kai Dierkes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keles_H/0/1/0/all/0/1\">Hacer Yalim Keles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Networks for Data Augmentation of Human Physical Activity Recognition. (arXiv:2109.01081v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.01081","description":"<p>Data augmentation is a widely used technique in classification to increase\ndata used in training. It improves generalization and reduces amount of\nannotated human activity data needed for training which reduces labour and time\nneeded with the dataset. Sensor time-series data, unlike images, cannot be\naugmented by computationally simple transformation algorithms. State of the art\nmodels like Recurrent Generative Adversarial Networks (RGAN) are used to\ngenerate realistic synthetic data. In this paper, transformer based generative\nadversarial networks which have global attention on data, are compared on\nPAMAP2 and Real World Human Activity Recognition data sets with RGAN. The newer\napproach provides improvements in time and savings in computational resources\nneeded for data augmentation than previous approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_S/0/1/0/all/0/1\">Sandeep Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoelzemann_A/0/1/0/all/0/1\">Alexander Hoelzemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laerhoven_K/0/1/0/all/0/1\">Kristof Van Laerhoven</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Semi-Automated Algorithm for Volumetric Segmentation of the Left Ventricle in Temporal 3D Echocardiography Sequences. (arXiv:2109.01132v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.01132","description":"<p>Purpose: Echocardiography is commonly used as a non-invasive imaging tool in\nclinical practice for the assessment of cardiac function. However, delineation\nof the left ventricle is challenging due to the inherent properties of\nultrasound imaging, such as the presence of speckle noise and the low\nsignal-to-noise ratio. Methods: We propose a semi-automated segmentation\nalgorithm for the delineation of the left ventricle in temporal 3D\nechocardiography sequences. The method requires minimal user interaction and\nrelies on a diffeomorphic registration approach. Advantages of the method\ninclude no dependence on prior geometrical information, training data, or\nregistration from an atlas. Results: The method was evaluated using\nthree-dimensional ultrasound scan sequences from 18 patients from the\nMazankowski Alberta Heart Institute, Edmonton, Canada, and compared to manual\ndelineations provided by an expert cardiologist and four other registration\nalgorithms. The segmentation approach yielded the following results over the\ncardiac cycle: a mean absolute difference of 1.01 (0.21) mm, a Hausdorff\ndistance of 4.41 (1.43) mm, and a Dice overlap score of 0.93 (0.02).\nConclusions: The method performed well compared to the four other registration\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Krishnaswamy_D/0/1/0/all/0/1\">Deepa Krishnaswamy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1\">Abhilash R. Hareendranathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suwatanaviroj_T/0/1/0/all/0/1\">Tan Suwatanaviroj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boulanger_P/0/1/0/all/0/1\">Pierre Boulanger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Becher_H/0/1/0/all/0/1\">Harald Becher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noga_M/0/1/0/all/0/1\">Michelle Noga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Punithakumar_K/0/1/0/all/0/1\">Kumaradevan Punithakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Target Shape for LiDAR Pose Estimation. (arXiv:2109.01181v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01181","description":"<p>Targets are essential in problems such as object tracking in cluttered or\ntextureless environments, camera (and multi-sensor) calibration tasks, and\nsimultaneous localization and mapping (SLAM). Target shapes for these tasks\ntypically are symmetric (square, rectangular, or circular) and work well for\nstructured, dense sensor data such as pixel arrays (i.e., image). However,\nsymmetric shapes lead to pose ambiguity when using sparse sensor data such as\nLiDAR point clouds and suffer from the quantization uncertainty of the LiDAR.\nThis paper introduces the concept of optimizing target shape to remove pose\nambiguity for LiDAR point clouds. A target is designed to induce large\ngradients at edge points under rotation and translation relative to the LiDAR\nto ameliorate the quantization uncertainty associated with point cloud\nsparseness. Moreover, given a target shape, we present a means that leverages\nthe target's geometry to estimate the target's vertices while globally\nestimating the pose. Both the simulation and the experimental results (verified\nby a motion capture system) confirm that by using the optimal shape and the\nglobal solver, we achieve centimeter error in translation and a few degrees in\nrotation even when a partially illuminated target is placed 30 meters away. All\nthe implementations and datasets are available at\nhttps://github.com/UMich-BipedLab/optimal_shape_global_pose_estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiunn-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_W/0/1/0/all/0/1\">William Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grizzle_J/0/1/0/all/0/1\">Jessy W. Grizzle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Camera Super-Resolution with Aligned Attention Modules. (arXiv:2109.01349v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01349","description":"<p>We present a novel approach to reference-based super-resolution (RefSR) with\nthe focus on dual-camera super-resolution (DCSR), which utilizes reference\nimages for high-quality and high-fidelity results. Our proposed method\ngeneralizes the standard patch-based feature matching with spatial alignment\noperations. We further explore the dual-camera super-resolution that is one\npromising application of RefSR, and build a dataset that consists of 146 image\npairs from the main and telephoto cameras in a smartphone. To bridge the domain\ngaps between real-world images and the training images, we propose a\nself-supervised domain adaptation strategy for real-world images. Extensive\nexperiments on our dataset and a public benchmark demonstrate clear improvement\nachieved by our method over state of the art in both quantitative evaluation\nand visual comparisons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaxin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenxiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models. (arXiv:2109.01401v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2109.01401","description":"<p>We propose CX-ToM, short for counterfactual explanations with theory-of mind,\na new explainable AI (XAI) framework for explaining decisions made by a deep\nconvolutional neural network (CNN). In contrast to the current methods in XAI\nthat generate explanations as a single shot response, we pose explanation as an\niterative communication process, i.e. dialog, between the machine and human\nuser. More concretely, our CX-ToM framework generates sequence of explanations\nin a dialog by mediating the differences between the minds of machine and human\nuser. To do this, we use Theory of Mind (ToM) which helps us in explicitly\nmodeling human's intention, machine's mind as inferred by the human as well as\nhuman's mind as inferred by the machine. Moreover, most state-of-the-art XAI\nframeworks provide attention (or heat map) based explanations. In our work, we\nshow that these attention based explanations are not sufficient for increasing\nhuman trust in the underlying CNN model. In CX-ToM, we instead use\ncounterfactual explanations called fault-lines which we define as follows:\ngiven an input image I for which a CNN classification model M predicts class\nc_pred, a fault-line identifies the minimal semantic-level features (e.g.,\nstripes on zebra, pointed ears of dog), referred to as explainable concepts,\nthat need to be added to or deleted from I in order to alter the classification\ncategory of I by M to another specified class c_alt. We argue that, due to the\niterative, conceptual and counterfactual nature of CX-ToM explanations, our\nframework is practical and more natural for both expert and non-expert users to\nunderstand the internal workings of complex deep learning models. Extensive\nquantitative and qualitative experiments verify our hypotheses, demonstrating\nthat our CX-ToM significantly outperforms the state-of-the-art explainable AI\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akula_A/0/1/0/all/0/1\">Arjun R. Akula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changsong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saba_Sadiya_S/0/1/0/all/0/1\">Sari Saba-Sadiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongjing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todorovic_S/0/1/0/all/0/1\">Sinisa Todorovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handwritten Character Recognition of South Indian Scripts: A Review. (arXiv:1106.0107v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/1106.0107","description":"<p>Handwritten character recognition is always a frontier area of research in\nthe field of pattern recognition and image processing and there is a large\ndemand for OCR on hand written documents. Even though, sufficient studies have\nperformed in foreign scripts like Chinese, Japanese and Arabic characters, only\na very few work can be traced for handwritten character recognition of Indian\nscripts especially for the South Indian scripts. This paper provides an\noverview of offline handwritten character recognition in South Indian Scripts,\nnamely Malayalam, Tamil, Kannada and Telungu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jomy_J/0/1/0/all/0/1\">John Jomy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramod_K/0/1/0/all/0/1\">K. V. Pramod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_B/0/1/0/all/0/1\">Balakrishnan Kannan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}