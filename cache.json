{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-05T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"AI & Racial Equity: Understanding Sentiment Analysis Artificial Intelligence, Data Security, and Systemic Theory in Criminal Justice Systems. (arXiv:2201.00855v1 [cs.CY])","link":"http://arxiv.org/abs/2201.00855","description":"<p>Various forms of implications of artificial intelligence that either\nexacerbate or decrease racial systemic injustice have been explored in this\napplied research endeavor. Taking each thematic area of identifying, analyzing,\nand debating an systemic issue have been leveraged in investigating merits and\ndrawbacks of using algorithms to automate human decision making in racially\nsensitive environments. It has been asserted through the analysis of historical\nsystemic patterns, implicit biases, existing algorithmic risks, and legal\nimplications that natural language processing based AI, such as risk assessment\ntools, have racially disparate outcomes. It is concluded that more litigative\npolicies are needed to regulate and restrict how internal government\ninstitutions and corporations utilize algorithms, privacy and security risks,\nand auditing requirements in order to diverge from racially injustice outcomes\nand practices of the past.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1\">Alia Abbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Adversarial Benchmark for Fake News Detection Models. (arXiv:2201.00912v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00912","description":"<p>With the proliferation of online misinformation, fake news detection has\ngained importance in the artificial intelligence community. In this paper, we\npropose an adversarial benchmark that tests the ability of fake news detectors\nto reason about real-world facts. We formulate adversarial attacks that target\nthree aspects of \"understanding\": compositional semantics, lexical relations,\nand sensitivity to modifiers. We test our benchmark using BERT classifiers\nfine-tuned on the LIAR <a href=\"/abs/arch-ive/1705648\">arXiv:arch-ive/1705648</a> and Kaggle Fake-News datasets,\nand show that both models fail to respond to changes in compositional and\nlexical meaning. Our results strengthen the need for such models to be used in\nconjunction with other fact checking methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Flores_L/0/1/0/all/0/1\">Lorenzo Jaime Yu Flores</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yiding Hao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantics-Preserved Distortion for Personal Privacy Protection. (arXiv:2201.00965v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00965","description":"<p>Privacy protection is an important and concerning topic in Federated\nLearning, especially for Natural Language Processing. In client devices, a\nlarge number of texts containing personal information are produced by users\nevery day. As the direct application of information from users is likely to\ninvade personal privacy, many methods have been proposed in Federated Learning\nto block the center model from the raw information in client devices. In this\npaper, we try to do this more linguistically via distorting the text while\npreserving the semantics. In practice, we leverage a recently proposed metric,\nNeighboring Distribution Divergence, to evaluate the semantic preservation\nduring the distortion. Based on the metric, we propose two frameworks for\nsemantics-preserved distortion, a generative one and a substitutive one. Due to\nthe lack of privacy-related tasks in the current Natural Language Processing\nfield, we conduct experiments on named entity recognition and constituency\nparsing. Results from our experiments show the plausibility and efficiency of\nour distortion as a method for personal privacy protection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety. (arXiv:2201.00969v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00969","description":"<p>There is amazing progress in Deep Learning based models for Image captioning\nand Low Light image enhancement. For the first time in literature, this paper\ndevelops a Deep Learning model that translates night scenes to sentences,\nopening new possibilities for AI applications in the safety of visually\nimpaired women. Inspired by Image Captioning and Visual Question Answering, a\nnovel Interactive Image Captioning is developed. A user can make the AI focus\non any chosen person of interest by influencing the attention scoring.\nAttention context vectors are computed from CNN feature vectors and\nuser-provided start word. The Encoder-Attention-Decoder neural network learns\nto produce captions from low brightness images. This paper demonstrates how\nwomen safety can be enabled by researching a novel AI capability in the\nInteractive Vision-Language model for perception of the environment in the\nnight.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+A_R/0/1/0/all/0/1\">Rajagopal A</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_N/0/1/0/all/0/1\">Nirmala V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedamanickam_A/0/1/0/all/0/1\">Arun Muthuraj Vedamanickam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Submix: Practical Private Prediction for Large-Scale Language Models. (arXiv:2201.00971v1 [cs.LG])","link":"http://arxiv.org/abs/2201.00971","description":"<p>Recent data-extraction attacks have exposed that language models can memorize\nsome training samples verbatim. This is a vulnerability that can compromise the\nprivacy of the model's training data. In this work, we introduce SubMix: a\npractical protocol for private next-token prediction designed to prevent\nprivacy violations by language models that were fine-tuned on a private corpus\nafter pre-training on a public corpus. We show that SubMix limits the leakage\nof information that is unique to any individual user in the private corpus via\na relaxation of group differentially private prediction. Importantly, SubMix\nadmits a tight, data-dependent privacy accounting mechanism, which allows it to\nthwart existing data-extraction attacks while maintaining the utility of the\nlanguage model. SubMix is the first protocol that maintains privacy even when\npublicly releasing tens of thousands of next-token predictions made by large\ntransformer-based models such as GPT-2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ginart_A/0/1/0/all/0/1\">Antonio Ginart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaten_L/0/1/0/all/0/1\">Laurens van der Maaten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleM: Stylized Metrics for Image Captioning Built with Contrastive N-grams. (arXiv:2201.00975v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00975","description":"<p>In this paper, we build two automatic evaluation metrics for evaluating the\nassociation between a machine-generated caption and a ground truth stylized\ncaption: OnlyStyle and StyleCIDEr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_B/0/1/0/all/0/1\">Brent Harrison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Stacked Local Attention Networks for Diverse Video Captioning. (arXiv:2201.00985v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00985","description":"<p>While describing Spatio-temporal events in natural language, video captioning\nmodels mostly rely on the encoder's latent visual representation. Recent\nprogress on the encoder-decoder model attends encoder features mainly in linear\ninteraction with the decoder. However, growing model complexity for visual data\nencourages more explicit feature interaction for fine-grained information,\nwhich is currently absent in the video captioning domain. Moreover, feature\naggregations methods have been used to unveil richer visual representation,\neither by the concatenation or using a linear layer. Though feature sets for a\nvideo semantically overlap to some extent, these approaches result in objective\nmismatch and feature redundancy. In addition, diversity in captions is a\nfundamental component of expressing one event from several meaningful\nperspectives, currently missing in the temporal, i.e., video captioning domain.\nTo this end, we propose Variational Stacked Local Attention Network (VSLAN),\nwhich exploits low-rank bilinear pooling for self-attentive feature interaction\nand stacking multiple video feature streams in a discount fashion. Each feature\nstack's learned attributes contribute to our proposed diversity encoding\nmodule, followed by the decoding query stage to facilitate end-to-end diverse\nand natural captions without any explicit supervision on attributes. We\nevaluate VSLAN on MSVD and MSR-VTT datasets in terms of syntax and diversity.\nThe CIDEr score of VSLAN outperforms current off-the-shelf methods by $7.8\\%$\non MSVD and $4.5\\%$ on MSR-VTT, respectively. On the same datasets, VSLAN\nachieves competitive results in caption diversity metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deb_T/0/1/0/all/0/1\">Tonmoay Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadmanee_A/0/1/0/all/0/1\">Akib Sadmanee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaumik_K/0/1/0/all/0/1\">Kishor Kumar Bhaumik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Amin Ahsan Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">M Ashraful Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1\">A K M Mahbubur Rahman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDFEND: Multi-domain Fake News Detection. (arXiv:2201.00987v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00987","description":"<p>Fake news spread widely on social media in various domains, which lead to\nreal-world threats in many aspects like politics, disasters, and finance. Most\nexisting approaches focus on single-domain fake news detection (SFND), which\nleads to unsatisfying performance when these methods are applied to\nmulti-domain fake news detection. As an emerging field, multi-domain fake news\ndetection (MFND) is increasingly attracting attention. However, data\ndistributions, such as word frequency and propagation patterns, vary from\ndomain to domain, namely domain shift. Facing the challenge of serious domain\nshift, existing fake news detection techniques perform poorly for multi-domain\nscenarios. Therefore, it is demanding to design a specialized model for MFND.\nIn this paper, we first design a benchmark of fake news dataset for MFND with\ndomain label annotated, namely Weibo21, which consists of 4,488 fake news and\n4,640 real news from 9 different domains. We further propose an effective\nMulti-domain Fake News Detection Model (MDFEND) by utilizing a domain gate to\naggregate multiple representations extracted by a mixture of experts. The\nexperiments show that MDFEND can significantly improve the performance of\nmulti-domain fake news detection. Our dataset and code are available at\nhttps://github.com/kennqiang/MDFEND-Weibo21.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nan_Q/0/1/0/all/0/1\">Qiong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DigNet: Digging Clues from Local-Global Interactive Graph for Aspect-level Sentiment Classification. (arXiv:2201.00989v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00989","description":"<p>In aspect-level sentiment classification (ASC), state-of-the-art models\nencode either syntax graph or relation graph to capture the local syntactic\ninformation or global relational information. Despite the advantages of syntax\nand relation graphs, they have respective shortages which are neglected,\nlimiting the representation power in the graph modeling process. To resolve\ntheir limitations, we design a novel local-global interactive graph, which\nmarries their advantages by stitching the two graphs via interactive edges. To\nmodel this local-global interactive graph, we propose a novel neural network\ntermed DigNet, whose core module is the stacked local-global interactive (LGI)\nlayers performing two processes: intra-graph message passing and cross-graph\nmessage passing. In this way, the local syntactic and global relational\ninformation can be reconciled as a whole in understanding the aspect-level\nsentiment. Concretely, we design two variants of local-global interactive\ngraphs with different kinds of interactive edges and three variants of LGI\nlayers. We conduct experiments on several public benchmark datasets and the\nresults show that we outperform previous best scores by 3\\%, 2.32\\%, and 6.33\\%\nin terms of Macro-F1 on Lap14, Res14, and Res15 datasets, respectively,\nconfirming the effectiveness and superiority of the proposed local-global\ninteractive graph and DigNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1\">Bowen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor Tsang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01140","description":"<p>The rapid mutation of the influenza virus threatens public health.\nReassortment among viruses with different hosts can lead to a fatal pandemic.\nHowever, it is difficult to detect the original host of the virus during or\nafter an outbreak as influenza viruses can circulate between different species.\nTherefore, early and rapid detection of the viral host would help reduce the\nfurther spread of the virus. We use various machine learning models with\nfeatures derived from the position-specific scoring matrix (PSSM) and features\nlearned from word embedding and word encoding to infer the origin host of\nviruses. The results show that the performance of the PSSM-based model reaches\nthe MCC around 95%, and the F1 around 96%. The MCC obtained using the model\nwith word embedding is around 96%, and the F1 is around 97%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wojtczak_D/0/1/0/all/0/1\">Dominik Wojtczak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech-to-SQL: Towards Speech-driven SQL Query Generation From Natural Language Question. (arXiv:2201.01209v1 [cs.DB])","link":"http://arxiv.org/abs/2201.01209","description":"<p>Speech-based inputs have been gaining significant momentum with the\npopularity of smartphones and tablets in our daily lives, since voice is the\nmost easiest and efficient way for human-computer interaction. This paper works\ntowards designing more effective speech-based interfaces to query the\nstructured data in relational databases. We first identify a new task named\nSpeech-to-SQL, which aims to understand the information conveyed by human\nspeech and directly translate it into structured query language (SQL)\nstatements. A naive solution to this problem can work in a cascaded manner,\nthat is, an automatic speech recognition (ASR) component followed by a\ntext-to-SQL component. However, it requires a high-quality ASR system and also\nsuffers from the error compounding problem between the two components,\nresulting in limited performance. To handle these challenges, we further\npropose a novel end-to-end neural architecture named SpeechSQLNet to directly\ntranslate human speech into SQL queries without an external ASR step.\nSpeechSQLNet has the advantage of making full use of the rich linguistic\ninformation presented in speech. To the best of our knowledge, this is the\nfirst attempt to directly synthesize SQL based on arbitrary natural language\nquestions, rather than a natural language-based version of SQL or its variants\nwith a limited SQL grammar. To validate the effectiveness of the proposed\nproblem and model, we further construct a dataset named SpeechQL, by\npiggybacking the widely-used text-to-SQL datasets. Extensive experimental\nevaluations on this dataset show that SpeechSQLNet can directly synthesize\nhigh-quality SQL queries from human speech, outperforming various competitive\ncounterparts as well as the cascaded methods in terms of exact match\naccuracies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuanfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_R/0/1/0/all/0/1\">Raymond Chi-Wing Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuefang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Di Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Stage Episodic Control for Strategic Exploration in Text Games. (arXiv:2201.01251v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01251","description":"<p>Text adventure games present unique challenges to reinforcement learning\nmethods due to their combinatorially large action spaces and sparse rewards.\nThe interplay of these two factors is particularly demanding because large\naction spaces require extensive exploration, while sparse rewards provide\nlimited feedback. This work proposes to tackle the explore-vs-exploit dilemma\nusing a multi-stage approach that explicitly disentangles these two strategies\nwithin each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins\neach episode using an exploitation policy that imitates a set of promising\ntrajectories from the past, and then switches over to an exploration policy\naimed at discovering novel actions that lead to unseen state spaces. This\npolicy decomposition allows us to combine global decisions about which parts of\nthe game space to return to with curiosity-based local exploration in that\nspace, motivated by how a human may approach these games. Our method\nsignificantly outperforms prior approaches by 27% and 11% average normalized\nscore over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in\nboth deterministic and stochastic settings, respectively. On the game of Zork1,\nin particular, XTX obtains a score of 103, more than a 2x improvement over\nprior methods, and pushes past several known bottlenecks in the game that have\nplagued previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tuyls_J/0/1/0/all/0/1\">Jens Tuyls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Model Supervised by Understanding Map. (arXiv:2110.06043v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06043","description":"<p>Inspired by the notion of Center of Mass in physics, an extension called\nSemantic Center of Mass (SCOM) is proposed, and used to discover the abstract\n\"topic\" of a document. The notion is under a framework model called\nUnderstanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM\nis to let both the document content and a semantic network -- specifically,\nUnderstanding Map -- play a role, in interpreting the meaning of a document.\nBased on different justifications, three possible methods are devised to\ndiscover the SCOM of a document. Some experiments on artificial documents and\nUnderstanding Maps are conducted to test their outcomes. In addition, its\nability of vectorization of documents and capturing sequential information are\ntested. We also compared UM-S-TM with probabilistic topic models like Latent\nDirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gangli Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Enactivist account of \"Mind Reading\" in Natural Language Understanding. (arXiv:2111.06179v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.06179","description":"<p>In this paper we apply our understanding of the radical enactivist agenda to\nthe classic AI-hard problem of Natural Language Understanding. When Turing\ndevised his famous test the assumption was that a computer could use language\nand the challenge would be to mimic human intelligence. It turned out playing\nchess and formal logic were easy compared to understanding what people say. The\ntechniques of good old-fashioned AI (GOFAI) assume symbolic representation is\nthe core of reasoning and by that paradigm human communication consists of\ntransferring representations from one mind to another. However, one finds that\nrepresentations appear in another's mind, without appearing in the intermediary\nlanguage. People communicate by mind reading it seems. Systems with speech\ninterfaces such as Alexa and Siri are of course common, but they are limited.\nRather than adding mind reading skills, we introduced a \"cheat\" that enabled\nour systems to fake it. The cheat is simple and only slightly interesting to\ncomputer scientists and not at all interesting to philosophers. However,\nreading about the enactivist idea that we \"directly perceive\" the intentions of\nothers, our cheat took on a new light and in this paper look again at how\nnatural language understanding might actually work between humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1\">Peter Wallis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building a great multi-lingual teacher with sparsely-gated mixture of experts for speech recognition. (arXiv:2112.05820v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.05820","description":"<p>The sparsely-gated Mixture of Experts (MoE) can magnify a network capacity\nwith a little computational complexity. In this work, we investigate how\nmulti-lingual Automatic Speech Recognition (ASR) networks can be scaled up with\na simple routing algorithm in order to achieve better accuracy. More\nspecifically, we apply the sparsely-gated MoE technique to two types of\nnetworks: Sequence-to-Sequence Transformer (S2S-T) and Transformer Transducer\n(T-T). We demonstrate through a set of ASR experiments on multiple language\ndata that the MoE networks can reduce the relative word error rates by 16.3%\nand 4.6% with the S2S-T and T-T, respectively. Moreover, we thoroughly\ninvestigate the effect of the MoE on the T-T architecture in various\nconditions: streaming mode, non-streaming mode, the use of language ID and the\nlabel decoder with the MoE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumatani_K/0/1/0/all/0/1\">Kenichi Kumatani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gmyr_R/0/1/0/all/0/1\">Robert Gmyr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salinas_F/0/1/0/all/0/1\">Felipe Cruz Salinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wei Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Devang Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_E/0/1/0/all/0/1\">Eric Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Graph-Aware Reinforcement Learning to Identify Winning Strategies in Diplomacy Games (Student Abstract). (arXiv:2112.15331v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.15331","description":"<p>This abstract proposes an approach towards goal-oriented modeling of the\ndetection and modeling complex social phenomena in multiparty discourse in an\nonline political strategy game. We developed a two-tier approach that first\nencodes sociolinguistic behavior as linguistic features then use reinforcement\nlearning to estimate the advantage afforded to any player. In the first tier,\nsociolinguistic behavior, such as Friendship and Reasoning, that speakers use\nto influence others are encoded as linguistic features to identify the\npersuasive strategies applied by each player in simultaneous two-party\ndialogues. In the second tier, a reinforcement learning approach is used to\nestimate a graph-aware reward function to quantify the advantage afforded to\neach player based on their standing in this multiparty setup. We apply this\ntechnique to the game Diplomacy, using a dataset comprising of over 15,000\nmessages exchanged between 78 users. Our graph-aware approach shows robust\nperformance compared to a context-agnostic setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_H/0/1/0/all/0/1\">Hansin Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1\">Lynnette Hui Xian Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaidka_K/0/1/0/all/0/1\">Kokil Jaidka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topical Classification of Food Safety Publications with a Knowledge Base. (arXiv:2201.00374v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.00374","description":"<p>The vast body of scientific publications presents an increasing challenge of\nfinding those that are relevant to a given research question, and making\ninformed decisions on their basis. This becomes extremely difficult without the\nuse of automated tools. Here, one possible area for improvement is automatic\nclassification of publication abstracts according to their topic. This work\nintroduces a novel, knowledge base-oriented publication classifier. The\nproposed method focuses on achieving scalability and easy adaptability to other\ndomains. Classification speed and accuracy are shown to be satisfactory, in the\nvery demanding field of food safety. Further development and evaluation of the\nmethod is needed, as the proposed approach shows much potential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sowinski_P/0/1/0/all/0/1\">Piotr Sowinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasielewska_Michniewska_K/0/1/0/all/0/1\">Katarzyna Wasielewska-Michniewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganzha_M/0/1/0/all/0/1\">Maria Ganzha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paprzycki_M/0/1/0/all/0/1\">Marcin Paprzycki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Low dosage 3D volume fluorescence microscopy imaging using compressive sensing. (arXiv:2201.00820v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00820","description":"<p>Fluorescence microscopy has been a significant tool to observe long-term\nimaging of embryos (in vivo) growth over time. However, cumulative exposure is\nphototoxic to such sensitive live samples. While techniques like light-sheet\nfluorescence microscopy (LSFM) allow for reduced exposure, it is not well\nsuited for deep imaging models. Other computational techniques are\ncomputationally expensive and often lack restoration quality. To address this\nchallenge, one can use various low-dosage imaging techniques that are developed\nto achieve the 3D volume reconstruction using a few slices in the axial\ndirection (z-axis); however, they often lack restoration quality. Also,\nacquiring dense images (with small steps) in the axial direction is\ncomputationally expensive. To address this challenge, we present a compressive\nsensing (CS) based approach to fully reconstruct 3D volumes with the same\nsignal-to-noise ratio (SNR) with less than half of the excitation dosage. We\npresent the theory and experimentally validate the approach. To demonstrate our\ntechnique, we capture a 3D volume of the RFP labeled neurons in the zebrafish\nembryo spinal cord (30um thickness) with the axial sampling of 0.1um using a\nconfocal microscope. From the results, we observe the CS-based approach\nachieves accurate 3D volume reconstruction from less than 20% of the entire\nstack optical sections. The developed CS-based methodology in this work can be\neasily applied to other deep imaging modalities such as two-photon and\nlight-sheet microscopy, where reducing sample photo-toxicity is a critical\nchallenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mannam_V/0/1/0/all/0/1\">Varun Mannam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brandt_J/0/1/0/all/0/1\">Jacob Brandt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smith_C/0/1/0/all/0/1\">Cody J. Smith</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Howard_S/0/1/0/all/0/1\">Scott Howard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Runway Extraction and Improved Mapping from Space Imagery. (arXiv:2201.00848v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00848","description":"<p>Change detection methods applied to monitoring key infrastructure like\nairport runways represent an important capability for disaster relief and urban\nplanning. The present work identifies two generative adversarial networks (GAN)\narchitectures that translate reversibly between plausible runway maps and\nsatellite imagery. We illustrate the training capability using paired images\n(satellite-map) from the same point of view and using the Pix2Pix architecture\nor conditional GANs. In the absence of available pairs, we likewise show that\nCycleGAN architectures with four network heads (discriminator-generator pairs)\ncan also provide effective style transfer from raw image pixels to outline or\nfeature maps. To emphasize the runway and tarmac boundaries, we experimentally\nshow that the traditional grey-tan map palette is not a required training input\nbut can be augmented by higher contrast mapping palettes (red-black) for\nsharper runway boundaries. We preview a potentially novel use case (called\n\"sketch2satellite\") where a human roughly draws the current runway boundaries\nand automates the machine output of plausible satellite images. Finally, we\nidentify examples of faulty runway maps where the published satellite and\nmapped runways disagree but an automated update renders the correct map using\nGANs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David A. Noever</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data. (arXiv:2201.00849v1 [cs.LG])","link":"http://arxiv.org/abs/2201.00849","description":"<p>Corrupted labels and class imbalance are commonly encountered in practically\ncollected training data, which easily leads to over-fitting of deep neural\nnetworks (DNNs). Existing approaches alleviate these issues by adopting a\nsample re-weighting strategy, which is to re-weight sample by designing\nweighting function. However, it is only applicable for training data containing\nonly either one type of data biases. In practice, however, biased samples with\ncorrupted labels and of tailed classes commonly co-exist in training data. How\nto handle them simultaneously is a key but under-explored problem. In this\npaper, we find that these two types of biased samples, though have similar\ntransient loss, have distinguishable trend and characteristics in loss curves,\nwhich could provide valuable priors for sample weight assignment. Motivated by\nthis, we delve into the loss curves and propose a novel probe-and-allocate\ntraining strategy: In the probing stage, we train the network on the whole\nbiased training data without intervention, and record the loss curve of each\nsample as an additional attribute; In the allocating stage, we feed the\nresulting attribute to a newly designed curve-perception network, named\nCurveNet, to learn to identify the bias type of each sample and assign proper\nweights through meta-learning adaptively. The training speed of meta learning\nalso blocks its application. To solve it, we propose a method named skip layer\nmeta optimization (SLMO) to accelerate training speed by skipping the bottom\nlayers. Extensive synthetic and real experiments well validate the proposed\nmethod, which achieves state-of-the-art performance on multiple challenging\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shenwang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Bo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tingfa Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gaussian-Hermite Moment Invariants of General Vector Functions to Rotation-Affine Transform. (arXiv:2201.00877v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00877","description":"<p>With the development of data acquisition technology, multi-channel data is\ncollected and widely used in many fields. Most of them can be expressed as\nvarious types of vector functions. Feature extraction of vector functions for\nidentifying certain patterns of interest is a critical but challenging task. In\nthis paper, we focus on constructing moment invariants of general vector\nfunctions. Specifically, we define rotation-affine transform to describe real\ndeformations of general vector functions, and then design a structural frame to\nsystematically generate Gaussian-Hermite moment invariants to this transform\nmodel. This is the first time that a uniform frame has been proposed in the\nliterature to construct orthogonal moment invariants of general vector\nfunctions. Given a certain type of multi-channel data, we demonstrate how to\nutilize the new method to derive all possible invariants and to eliminate\nvarious dependences among them. For RGB images, 2D and 3D flow fields, we\nobtain the complete and independent sets of the invariants with low orders and\nlow degrees. Based on synthetic and popular datasets of vector-valued data, the\nexperiments are carried out to evaluate the stability and discriminability of\nthese invariants, and also their robustness to noise. The results clearly show\nthat the moment invariants proposed in our paper have better performance than\nother previously used moment invariants of vector functions in RGB image\nclassification, vortex detection in 2D vector fields and template matching for\n3D flow fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_H/0/1/0/all/0/1\">Hanlin Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rice Diseases Detection and Classification Using Attention Based Neural Network and Bayesian Optimization. (arXiv:2201.00893v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00893","description":"<p>In this research, an attention-based depthwise separable neural network with\nBayesian optimization (ADSNN-BO) is proposed to detect and classify rice\ndisease from rice leaf images. Rice diseases frequently result in 20 to 40 \\%\ncorp production loss in yield and is highly related to the global economy.\nRapid disease identification is critical to plan treatment promptly and reduce\nthe corp losses. Rice disease diagnosis is still mainly performed manually. To\nachieve AI assisted rapid and accurate disease detection, we proposed the\nADSNN-BO model based on MobileNet structure and augmented attention mechanism.\nMoreover, Bayesian optimization method is applied to tune hyper-parameters of\nthe model. Cross-validated classification experiments are conducted based on a\npublic rice disease dataset with four categories in total. The experimental\nresults demonstrate that our mobile compatible ADSNN-BO model achieves a test\naccuracy of 94.65\\%, which outperforms all of the state-of-the-art models\ntested. To check the interpretability of our proposed model, feature analysis\nincluding activation map and filters visualization approach are also conducted.\nResults show that our proposed attention-based mechanism can more effectively\nguide the ADSNN-BO model to learn informative features. The outcome of this\nresearch will promote the implementation of artificial intelligence for fast\nplant disease diagnosis and control in the agricultural field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhaohua Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Gradient Mapping Guided Explainable Deep Neural Network for Extracapsular Extension Identification in 3D Head and Neck Cancer Computed Tomography Images. (arXiv:2201.00895v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00895","description":"<p>Diagnosis and treatment management for head and neck squamous cell carcinoma\n(HNSCC) is guided by routine diagnostic head and neck computed tomography (CT)\nscans to identify tumor and lymph node features. Extracapsular extension (ECE)\nis a strong predictor of patients' survival outcomes with HNSCC. It is\nessential to detect the occurrence of ECE as it changes staging and management\nfor the patients. Current clinical ECE detection relies on visual\nidentification and pathologic confirmation conducted by radiologists. Machine\nlearning (ML)-based ECE diagnosis has shown high potential in the recent years.\nHowever, manual annotation of lymph node region is a required data\npreprocessing step in most of the current ML-based ECE diagnosis studies. In\naddition, this manual annotation process is time-consuming, labor-intensive,\nand error-prone. Therefore, in this paper, we propose a Gradient Mapping Guided\nExplainable Network (GMGENet) framework to perform ECE identification\nautomatically without requiring annotated lymph node region information. The\ngradient-weighted class activation mapping (Grad-CAM) technique is proposed to\nguide the deep learning algorithm to focus on the regions that are highly\nrelated to ECE. Informative volumes of interest (VOIs) are extracted without\nlabeled lymph node region information. In evaluation, the proposed method is\nwell-trained and tested using cross validation, achieving test accuracy and AUC\nof 90.2% and 91.1%, respectively. The presence or absence of ECE has been\nanalyzed and correlated with gold standard histopathological findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yibin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_A/0/1/0/all/0/1\">Abdur Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duggar_W/0/1/0/all/0/1\">W. Neil. Duggar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roberts_P/0/1/0/all/0/1\">P. Russell Roberts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_T/0/1/0/all/0/1\">Toms V. Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bian_L/0/1/0/all/0/1\">Linkan Bian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"External Attention Assisted Multi-Phase Splenic Vascular Injury Segmentation with Limited Data. (arXiv:2201.00942v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00942","description":"<p>The spleen is one of the most commonly injured solid organs in blunt\nabdominal trauma. The development of automatic segmentation systems from\nmulti-phase CT for splenic vascular injury can augment severity grading for\nimproving clinical decision support and outcome prediction. However, accurate\nsegmentation of splenic vascular injury is challenging for the following\nreasons: 1) Splenic vascular injury can be highly variant in shape, texture,\nsize, and overall appearance; and 2) Data acquisition is a complex and\nexpensive procedure that requires intensive efforts from both data scientists\nand radiologists, which makes large-scale well-annotated datasets hard to\nacquire in general.\n</p>\n<p>In light of these challenges, we hereby design a novel framework for\nmulti-phase splenic vascular injury segmentation, especially with limited data.\nOn the one hand, we propose to leverage external data to mine pseudo splenic\nmasks as the spatial attention, dubbed external attention, for guiding the\nsegmentation of splenic vascular injury. On the other hand, we develop a\nsynthetic phase augmentation module, which builds upon generative adversarial\nnetworks, for populating the internal data by fully leveraging the relation\nbetween different phases. By jointly enforcing external attention and\npopulating internal data representation during training, our proposed method\noutperforms other competing methods and substantially improves the popular\nDeepLab-v3+ baseline by more than 7% in terms of average DSC, which confirms\nits effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuyin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dreizin_D/0/1/0/all/0/1\">David Dreizin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fengze Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuille_A/0/1/0/all/0/1\">Alan L. Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HWRCNet: Handwritten Word Recognition in JPEG Compressed Domain using CNN-BiLSTM Network. (arXiv:2201.00947v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00947","description":"<p>The handwritten word recognition from images using deep learning is an active\nresearch area with promising performance. It practical scenario, it might be\nrequired to process the handwritten images in the compressed domain due to due\nto security reasons. However, the utilization of deep learning is still very\nlimited for the processing of compressed images. Motivated by the need of\nprocessing document images in the compressed domain using recent developments\nin deep learning, we propose a HWRCNet model for handwritten word recognition\nin JPEG compressed domain. The proposed model combines the Convolutional Neural\nNetwork (CNN) and Bi-Directional Long Short Term Memory (BiLSTM) based\nRecurrent Neural Network (RNN). Basically, we train the model using compressed\ndomain images and observe a very appealing performance with 89.05% word\nrecognition accuracy and 13.37% character error rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_M/0/1/0/all/0/1\">Mudit Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Kumar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shiv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatwani_K/0/1/0/all/0/1\">Karan Chatwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stain Normalized Breast Histopathology Image Recognition using Convolutional Neural Networks for Cancer Detection. (arXiv:2201.00957v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00957","description":"<p>Computer assisted diagnosis in digital pathology is becoming ubiquitous as it\ncan provide more efficient and objective healthcare diagnostics. Recent\nadvances have shown that the convolutional Neural Network (CNN) architectures,\na well-established deep learning paradigm, can be used to design a Computer\nAided Diagnostic (CAD) System for breast cancer detection. However, the\nchallenges due to stain variability and the effect of stain normalization with\nsuch deep learning frameworks are yet to be well explored. Moreover,\nperformance analysis with arguably more efficient network models, which may be\nimportant for high throughput screening, is also not well explored.To address\nthis challenge, we consider some contemporary CNN models for binary\nclassification of breast histopathology images that involves (1) the data\npreprocessing with stain normalized images using an adaptive colour\ndeconvolution (ACD) based color normalization algorithm to handle the stain\nvariabilities; and (2) applying transfer learning based training of some\narguably more efficient CNN models, namely Visual Geometry Group Network\n(VGG16), MobileNet and EfficientNet. We have validated the trained CNN networks\non a publicly available BreaKHis dataset, for 200x and 400x magnified\nhistopathology images. The experimental analysis shows that pretrained networks\nin most cases yield better quality results on data augmented breast\nhistopathology images with stain normalization, than the case without stain\nnormalization. Further, we evaluated the performance and efficiency of popular\nlightweight networks using stain normalized images and found that EfficientNet\noutperforms VGG16 and MobileNet in terms of test accuracy and F1 Score. We\nobserved that efficiency in terms of test time is better in EfficientNet than\nother networks; VGG Net, MobileNet, without much drop in the classification\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Krishna_S/0/1/0/all/0/1\">Sruthi Krishna</a>, <a href=\"http://arxiv.org/find/eess/1/au:+S_S/0/1/0/all/0/1\">Suganthi S.S</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krishnamoorthy_S/0/1/0/all/0/1\">Shivsubramani Krishnamoorthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhavsar_A/0/1/0/all/0/1\">Arnav Bhavsar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI visualization in Nanoscale Microscopy. (arXiv:2201.00966v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00966","description":"<p>Artificial Intelligence &amp; Nanotechnology are promising areas for the future\nof humanity. While Deep Learning based Computer Vision has found applications\nin many fields from medicine to automotive, its application in nanotechnology\ncan open doors for new scientific discoveries. Can we apply AI to explore\nobjects that our eyes can't see such as nano scale sized objects? An AI\nplatform to visualize nanoscale patterns learnt by a Deep Learning neural\nnetwork can open new frontiers for nanotechnology. The objective of this paper\nis to develop a Deep Learning based visualization system on images of\nnanomaterials obtained by scanning electron microscope. This paper contributes\nan AI platform to enable any nanoscience researcher to use AI in visual\nexploration of nanoscale morphologies of nanomaterials. This AI is developed by\na technique of visualizing intermediate activations of a Convolutional\nAutoEncoder. In this method, a nano scale specimen image is transformed into\nits feature representations by a Convolution Neural Network. The Convolutional\nAutoEncoder is trained on 100% SEM dataset, and then CNN visualization is\napplied. This AI generates various conceptual feature representations of the\nnanomaterial.\n</p>\n<p>While Deep Learning based image classification of SEM images are widely\npublished in literature, there are not much publications that have visualized\nDeep neural networks of nanomaterials. There is a significant opportunity to\ngain insights from the learnings extracted by machine learning. This paper\nunlocks the potential of applying Deep Learning based Visualization on electron\nmicroscopy to offer AI extracted features and architectural patterns of various\nnanomaterials. This is a contribution in Explainable AI in nano scale objects.\nThis paper contributes an open source AI with reproducible results at URL\n(https://sites.google.com/view/aifornanotechnology)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+A_R/0/1/0/all/0/1\">Rajagopal A</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+V_N/0/1/0/all/0/1\">Nirmala V</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+J_A/0/1/0/all/0/1\">Andrew J</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Vedamanickam%2E_A/0/1/0/all/0/1\">Arun Muthuraj Vedamanickam.</a> ((1) Indian Institute of Technology Madras, (2) Queen Marys College, (3) Karunya Institute of Technology and Sciences. India)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety. (arXiv:2201.00969v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00969","description":"<p>There is amazing progress in Deep Learning based models for Image captioning\nand Low Light image enhancement. For the first time in literature, this paper\ndevelops a Deep Learning model that translates night scenes to sentences,\nopening new possibilities for AI applications in the safety of visually\nimpaired women. Inspired by Image Captioning and Visual Question Answering, a\nnovel Interactive Image Captioning is developed. A user can make the AI focus\non any chosen person of interest by influencing the attention scoring.\nAttention context vectors are computed from CNN feature vectors and\nuser-provided start word. The Encoder-Attention-Decoder neural network learns\nto produce captions from low brightness images. This paper demonstrates how\nwomen safety can be enabled by researching a novel AI capability in the\nInteractive Vision-Language model for perception of the environment in the\nnight.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+A_R/0/1/0/all/0/1\">Rajagopal A</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_N/0/1/0/all/0/1\">Nirmala V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedamanickam_A/0/1/0/all/0/1\">Arun Muthuraj Vedamanickam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleM: Stylized Metrics for Image Captioning Built with Contrastive N-grams. (arXiv:2201.00975v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00975","description":"<p>In this paper, we build two automatic evaluation metrics for evaluating the\nassociation between a machine-generated caption and a ground truth stylized\ncaption: OnlyStyle and StyleCIDEr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_B/0/1/0/all/0/1\">Brent Harrison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Underwater Object Classification and Detection: first results and open challenges. (arXiv:2201.00977v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00977","description":"<p>This work reviews the problem of object detection in underwater environments.\nWe analyse and quantify the shortcomings of conventional state-of-the-art\n(SOTA) algorithms in the computer vision community when applied to this\nchallenging environment, as well as providing insights and general guidelines\nfor future research efforts. First, we assessed if pretraining with the\nconventional ImageNet is beneficial when the object detector needs to be\napplied to environments that may be characterised by a different feature\ndistribution. We then investigate whether two-stage detectors yields to better\nperformance with respect to single-stage detectors, in terms of accuracy,\nintersection of union (IoU), floating operation per second (FLOPS), and\ninference time. Finally, we assessed the generalisation capability of each\nmodel to a lower quality dataset to simulate performance on a real scenario, in\nwhich harsher conditions ought to be expected. Our experimental results provide\nevidence that underwater object detection requires searching for \"ad-hoc\"\narchitectures than merely training SOTA architectures on new data, and that\npretraining is not beneficial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jesus_A/0/1/0/all/0/1\">Andre Jesus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zito_C/0/1/0/all/0/1\">Claudio Zito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tortorici_C/0/1/0/all/0/1\">Claudio Tortorici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roura_E/0/1/0/all/0/1\">Eloy Roura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masi_G/0/1/0/all/0/1\">Giulia De Masi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture. (arXiv:2201.00978v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00978","description":"<p>Transformer networks have achieved great progress for computer vision tasks.\nTransformer-in-Transformer (TNT) architecture utilizes inner transformer and\nouter transformer to extract both local and global representations. In this\nwork, we present new TNT baselines by introducing two advanced designs: 1)\npyramid architecture, and 2) convolutional stem. The new \"PyramidTNT\"\nsignificantly improves the original TNT by establishing hierarchical\nrepresentations. PyramidTNT achieves better performances than the previous\nstate-of-the-art vision transformers such as Swin Transformer. We hope this new\nbaseline will be helpful to the further research and application of vision\ntransformer. Code will be available at\nhttps://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Stacked Local Attention Networks for Diverse Video Captioning. (arXiv:2201.00985v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00985","description":"<p>While describing Spatio-temporal events in natural language, video captioning\nmodels mostly rely on the encoder's latent visual representation. Recent\nprogress on the encoder-decoder model attends encoder features mainly in linear\ninteraction with the decoder. However, growing model complexity for visual data\nencourages more explicit feature interaction for fine-grained information,\nwhich is currently absent in the video captioning domain. Moreover, feature\naggregations methods have been used to unveil richer visual representation,\neither by the concatenation or using a linear layer. Though feature sets for a\nvideo semantically overlap to some extent, these approaches result in objective\nmismatch and feature redundancy. In addition, diversity in captions is a\nfundamental component of expressing one event from several meaningful\nperspectives, currently missing in the temporal, i.e., video captioning domain.\nTo this end, we propose Variational Stacked Local Attention Network (VSLAN),\nwhich exploits low-rank bilinear pooling for self-attentive feature interaction\nand stacking multiple video feature streams in a discount fashion. Each feature\nstack's learned attributes contribute to our proposed diversity encoding\nmodule, followed by the decoding query stage to facilitate end-to-end diverse\nand natural captions without any explicit supervision on attributes. We\nevaluate VSLAN on MSVD and MSR-VTT datasets in terms of syntax and diversity.\nThe CIDEr score of VSLAN outperforms current off-the-shelf methods by $7.8\\%$\non MSVD and $4.5\\%$ on MSR-VTT, respectively. On the same datasets, VSLAN\nachieves competitive results in caption diversity metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deb_T/0/1/0/all/0/1\">Tonmoay Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadmanee_A/0/1/0/all/0/1\">Akib Sadmanee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaumik_K/0/1/0/all/0/1\">Kishor Kumar Bhaumik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Amin Ahsan Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">M Ashraful Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1\">A K M Mahbubur Rahman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Mechanism Meets with Hybrid Dense Network for Hyperspectral Image Classification. (arXiv:2201.01001v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01001","description":"<p>Convolutional Neural Networks (CNN) are more suitable, indeed. However, fixed\nkernel sizes make traditional CNN too specific, neither flexible nor conducive\nto feature learning, thus impacting on the classification accuracy. The\nconvolution of different kernel size networks may overcome this problem by\ncapturing more discriminating and relevant information. In light of this, the\nproposed solution aims at combining the core idea of 3D and 2D Inception net\nwith the Attention mechanism to boost the HSIC CNN performance in a hybrid\nscenario. The resulting \\textit{attention-fused hybrid network} (AfNet) is\nbased on three attention-fused parallel hybrid sub-nets with different kernels\nin each block repeatedly using high-level features to enhance the final\nground-truth maps. In short, AfNet is able to selectively filter out the\ndiscriminative features critical for classification. Several tests on HSI\ndatasets provided competitive results for AfNet compared to state-of-the-art\nmodels. The proposed pipeline achieved, indeed, an overall accuracy of 97\\% for\nthe Indian Pines, 100\\% for Botswana, 99\\% for Pavia University, Pavia Center,\nand Salinas datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_M/0/1/0/all/0/1\">Muhammad Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Adil Mehmood Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzara_M/0/1/0/all/0/1\">Manuel Mazzara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Distefano_S/0/1/0/all/0/1\">Salvatore Distefano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Swalpa Kumar Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Representation Adaptation Network for Cross-domain Image Classification. (arXiv:2201.01002v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01002","description":"<p>In image classification, it is often expensive and time-consuming to acquire\nsufficient labels. To solve this problem, domain adaptation often provides an\nattractive option given a large amount of labeled data from a similar nature\nbut different domain. Existing approaches mainly align the distributions of\nrepresentations extracted by a single structure and the representations may\nonly contain partial information, e.g., only contain part of the saturation,\nbrightness, and hue information. Along this line, we propose\nMulti-Representation Adaptation which can dramatically improve the\nclassification accuracy for cross-domain image classification and specially\naims to align the distributions of multiple representations extracted by a\nhybrid structure named Inception Adaptation Module (IAM). Based on this, we\npresent Multi-Representation Adaptation Network (MRAN) to accomplish the\ncross-domain image classification task via multi-representation alignment which\ncan capture the information from different aspects. In addition, we extend\nMaximum Mean Discrepancy (MMD) to compute the adaptation loss. Our approach can\nbe easily implemented by extending most feed-forward models with IAM, and the\nnetwork can be trained efficiently via back-propagation. Experiments conducted\non three benchmark image datasets demonstrate the effectiveness of MRAN. The\ncode has been available at https://github.com/easezyc/deep-transfer-learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingwu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhiping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenjuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligning Domain-specific Distribution and Classifier for Cross-domain Classification from Multiple Sources. (arXiv:2201.01003v1 [cs.LG])","link":"http://arxiv.org/abs/2201.01003","description":"<p>While Unsupervised Domain Adaptation (UDA) algorithms, i.e., there are only\nlabeled data from source domains, have been actively studied in recent years,\nmost algorithms and theoretical results focus on Single-source Unsupervised\nDomain Adaptation (SUDA). However, in the practical scenario, labeled data can\nbe typically collected from multiple diverse sources, and they might be\ndifferent not only from the target domain but also from each other. Thus,\ndomain adapters from multiple sources should not be modeled in the same way.\nRecent deep learning based Multi-source Unsupervised Domain Adaptation (MUDA)\nalgorithms focus on extracting common domain-invariant representations for all\ndomains by aligning distribution of all pairs of source and target domains in a\ncommon feature space. However, it is often very hard to extract the same\ndomain-invariant representations for all domains in MUDA. In addition, these\nmethods match distributions without considering domain-specific decision\nboundaries between classes. To solve these problems, we propose a new framework\nwith two alignment stages for MUDA which not only respectively aligns the\ndistributions of each pair of source and target domains in multiple specific\nfeature spaces, but also aligns the outputs of classifiers by utilizing the\ndomain-specific decision boundaries. Extensive experiments demonstrate that our\nmethod can achieve remarkable results on popular benchmark datasets for image\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Deqing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Generate Novel Classes for Deep Metric Learning. (arXiv:2201.01008v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01008","description":"<p>Deep metric learning aims to learn an embedding space where the distance\nbetween data reflects their class equivalence, even when their classes are\nunseen during training. However, the limited number of classes available in\ntraining precludes generalization of the learned embedding space. Motivated by\nthis, we introduce a new data augmentation approach that synthesizes novel\nclasses and their embedding vectors. Our approach can provide rich semantic\ninformation to an embedding model and improve its generalization by augmenting\ntraining data with novel classes unavailable in the original data. We implement\nthis idea by learning and exploiting a conditional generative model, which,\ngiven a class label and a noise, produces a random embedding vector of the\nclass. Our proposed generator allows the loss to use richer class relations by\naugmenting realistic and diverse classes, resulting in better generalization to\nunseen samples. Experimental results on public benchmark datasets demonstrate\nthat our method clearly enhances the performance of proxy-based losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyungmoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Seunghoon Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoCoPnet: Exploring Local Motion and Contrast Priors for Infrared Small Target Super-Resolution. (arXiv:2201.01014v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01014","description":"<p>Infrared small target super-resolution (SR) aims to recover reliable and\ndetailed high-resolution image with highcontrast targets from its\nlow-resolution counterparts. Since the infrared small target lacks color and\nfine structure information, it is significant to exploit the supplementary\ninformation among sequence images to enhance the target. In this paper, we\npropose the first infrared small target SR method named local motion and\ncontrast prior driven deep network (MoCoPnet) to integrate the domain knowledge\nof infrared small target into deep network, which can mitigate the intrinsic\nfeature scarcity of infrared small targets. Specifically, motivated by the\nlocal motion prior in the spatio-temporal dimension, we propose a local\nspatiotemporal attention module to perform implicit frame alignment and\nincorporate the local spatio-temporal information to enhance the local features\n(especially for small targets). Motivated by the local contrast prior in the\nspatial dimension, we propose a central difference residual group to\nincorporate the central difference convolution into the feature extraction\nbackbone, which can achieve center-oriented gradient-aware feature extraction\nto further improve the target contrast. Extensive experiments have demonstrated\nthat our method can recover accurate spatial dependency and improve the target\ncontrast. Comparative results show that MoCoPnet can outperform the\nstate-of-the-art video SR and single image SR methods in terms of both SR\nperformance and target enhancement. Based on the SR results, we further\ninvestigate the influence of SR on infrared small target detection and the\nexperimental results demonstrate that MoCoPnet promotes the detection\nperformance. The code is available at https://github.com/XinyiYing/MoCoPnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ying_X/0/1/0/all/0/1\">Xinyi Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheng_W/0/1/0/all/0/1\">Weidong Sheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Z/0/1/0/all/0/1\">Zaipin Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zho_S/0/1/0/all/0/1\">Shilin Zho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detailed Facial Geometry Recovery from Multi-view Images by Learning an Implicit Function. (arXiv:2201.01016v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01016","description":"<p>Recovering detailed facial geometry from a set of calibrated multi-view\nimages is valuable for its wide range of applications. Traditional multi-view\nstereo (MVS) methods adopt optimization methods to regularize the matching\ncost. Recently, learning-based methods integrate all these into an end-to-end\nneural network and show superiority of efficiency. In this paper, we propose a\nnovel architecture to recover extremely detailed 3D faces in roughly 10\nseconds. Unlike previous learning-based methods that regularize the cost volume\nvia 3D CNN, we propose to learn an implicit function for regressing the\nmatching cost. By fitting a 3D morphable model from multi-view images, the\nfeatures of multiple images are extracted and aggregated in the mesh-attached\nUV space, which makes the implicit function more effective in recovering\ndetailed facial shape. Our method outperforms SOTA learning-based MVS in\naccuracy by a large margin on the FaceScape dataset. The code and data will be\nreleased soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yunze Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haotian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_Z/0/1/0/all/0/1\">Zhengyu Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiangju Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised continual learning for class-incremental segmentation. (arXiv:2201.01029v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01029","description":"<p>Transfer learning is a powerful way to adapt existing deep learning models to\nnew emerging use-cases in remote sensing. Starting from a neural network\nalready trained for semantic segmentation, we propose to modify its label space\nto swiftly adapt it to new classes under weak supervision. To alleviate the\nbackground shift and the catastrophic forgetting problems inherent to this form\nof continual learning, we compare different regularization terms and leverage a\npseudo-label strategy. We experimentally show the relevance of our approach on\nthree public remote sensing datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lenczner_G/0/1/0/all/0/1\">Gaston Lenczner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_Hon_Tong_A/0/1/0/all/0/1\">Adrien Chan-Hon-Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luminari_N/0/1/0/all/0/1\">Nicola Luminari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saux_B/0/1/0/all/0/1\">Bertrand Le Saux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Visual Sampling Model Inspired by Receptive Field. (arXiv:2201.01030v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01030","description":"<p>Spike camera mimicking the retina fovea can report per-pixel luminance\nintensity accumulation by firing spikes. As a bio-inspired vision sensor with\nhigh temporal resolution, it has a huge potential for computer vision. However,\nthe sampling model in current Spike camera is so susceptible to quantization\nand noise that it cannot capture the texture details of objects effectively. In\nthis work, a robust visual sampling model inspired by receptive field (RVSM) is\nproposed where wavelet filter generated by difference of Gaussian (DoG) and\nGaussian filter are used to simulate receptive field. Using corresponding\nmethod similar to inverse wavelet transform, spike data from RVSM can be\nconverted into images. To test the performance, we also propose a high-speed\nmotion spike dataset (HMD) including a variety of motion scenes. By comparing\nreconstructed images in HMD, we find RVSM can improve the ability of capturing\ninformation of Spike camera greatly. More importantly, due to mimicking\nreceptive field mechanism to collect regional information, RVSM can filter high\nintensity noise effectively and improves the problem that Spike camera is\nsensitive to noise largely. Besides, due to the strong generalization of\nsampling structure, RVSM is also suitable for other neuromorphic vision sensor.\nAbove experiments are finished in a Spike camera simulator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Liwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_D/0/1/0/all/0/1\">Dawei Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Hinders Perceptual Quality of PSNR-oriented Methods?. (arXiv:2201.01034v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01034","description":"<p>In this paper, we discover two factors that inhibit POMs from achieving high\nperceptual quality: 1) center-oriented optimization (COO) problem and 2)\nmodel's low-frequency tendency. First, POMs tend to generate an SR image whose\nposition in the feature space is closest to the distribution center of all\npotential high-resolution (HR) images, resulting in such POMs losing\nhigh-frequency details. Second, $90\\%$ area of an image consists of\nlow-frequency signals; in contrast, human perception relies on an image's\nhigh-frequency details. However, POMs apply the same calculation to process\ndifferent-frequency areas, so that POMs tend to restore the low-frequency\nregions. Based on these two factors, we propose a Detail Enhanced Contrastive\nLoss (DECLoss), by combining a high-frequency enhancement module and spatial\ncontrastive learning module, to reduce the influence of the COO problem and\nlow-Frequency tendency. Experimental results show the efficiency and\neffectiveness when applying DECLoss on several regular SR models. E.g, in EDSR,\nour proposed method achieves 3.60$\\times$ faster learning speed compared to a\nGAN-based method with a subtle degradation in visual quality. In addition, our\nfinal results show that an SR network equipped with our DECLoss generates more\nrealistic and visually pleasing textures compared to state-of-the-art methods.\n%The source code of the proposed method is included in the supplementary\nmaterial and will be made publicly available in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_T/0/1/0/all/0/1\">Tianshuo Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mi_P/0/1/0/all/0/1\">Peng Mi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_X/0/1/0/all/0/1\">Xiawu Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lijiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_G/0/1/0/all/0/1\">Guannan Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyi Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sound and Visual Representation Learning with Multiple Pretraining Tasks. (arXiv:2201.01046v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01046","description":"<p>Different self-supervised tasks (SSL) reveal different features from the\ndata. The learned feature representations can exhibit different performance for\neach downstream task. In this light, this work aims to combine Multiple SSL\ntasks (Multi-SSL) that generalizes well for all downstream tasks. Specifically,\nfor this study, we investigate binaural sounds and image data in isolation. For\nbinaural sounds, we propose three SSL tasks namely, spatial alignment, temporal\nsynchronization of foreground objects and binaural audio and temporal gap\nprediction. We investigate several approaches of Multi-SSL and give insights\ninto the downstream task performance on video retrieval, spatial sound super\nresolution, and semantic prediction on the OmniAudio dataset. Our experiments\non binaural sound representations demonstrate that Multi-SSL via incremental\nlearning (IL) of SSL tasks outperforms single SSL task models and fully\nsupervised models in the downstream task performance. As a check of\napplicability on other modality, we also formulate our Multi-SSL models for\nimage representation learning and we use the recently proposed SSL tasks,\nMoCov2 and DenseCL. Here, Multi-SSL surpasses recent methods such as MoCov2,\nDenseCL and DetCo by 2.06%, 3.27% and 1.19% on VOC07 classification and +2.83,\n+1.56 and +1.61 AP on COCO detection. Code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_A/0/1/0/all/0/1\">Arun Balajee Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIAL: Deep Interactive and Active Learning for Semantic Segmentation in Remote Sensing. (arXiv:2201.01047v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01047","description":"<p>We propose in this article to build up a collaboration between a deep neural\nnetwork and a human in the loop to swiftly obtain accurate segmentation maps of\nremote sensing images. In a nutshell, the agent iteratively interacts with the\nnetwork to correct its initially flawed predictions. Concretely, these\ninteractions are annotations representing the semantic labels. Our\nmethodological contribution is twofold. First, we propose two interactive\nlearning schemes to integrate user inputs into deep neural networks. The first\none concatenates the annotations with the other network's inputs. The second\none uses the annotations as a sparse ground-truth to retrain the network.\nSecond, we propose an active learning strategy to guide the user towards the\nmost relevant areas to annotate. To this purpose, we compare different\nstate-of-the-art acquisition functions to evaluate the neural network\nuncertainty such as ConfidNet, entropy or ODIN. Through experiments on three\nremote sensing datasets, we show the effectiveness of the proposed methods.\nNotably, we show that active learning based on uncertainty estimation enables\nto quickly lead the user towards mistakes and that it is thus relevant to guide\nthe user interventions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lenczner_G/0/1/0/all/0/1\">Gaston Lenczner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_Hon_Tong_A/0/1/0/all/0/1\">Adrien Chan-Hon-Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saux_B/0/1/0/all/0/1\">Bertrand Le Saux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luminari_N/0/1/0/all/0/1\">Nicola Luminari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besnerais_G/0/1/0/all/0/1\">Guy Le Besnerais</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Unsupervised Open World Semantic Segmentation. (arXiv:2201.01073v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01073","description":"<p>For the semantic segmentation of images, state-of-the-art deep neural\nnetworks (DNNs) achieve high segmentation accuracy if that task is restricted\nto a closed set of classes. However, as of now DNNs have limited ability to\noperate in an open world, where they are tasked to identify pixels belonging to\nunknown objects and eventually to learn novel classes, incrementally. Humans\nhave the capability to say: I don't know what that is, but I've already seen\nsomething like that. Therefore, it is desirable to perform such an incremental\nlearning task in an unsupervised fashion. We introduce a method where unknown\nobjects are clustered based on visual similarity. Those clusters are utilized\nto define new classes and serve as training data for unsupervised incremental\nlearning. More precisely, the connected components of a predicted semantic\nsegmentation are assessed by a segmentation quality estimate. connected\ncomponents with a low estimated prediction quality are candidates for a\nsubsequent clustering. Additionally, the component-wise quality assessment\nallows for obtaining predicted segmentation masks for the image regions\npotentially containing unknown objects. The respective pixels of such masks are\npseudo-labeled and afterwards used for re-training the DNN, i.e., without the\nuse of ground truth generated by humans. In our experiments we demonstrate\nthat, without access to ground truth and even with few data, a DNN's class\nspace can be extended by a novel class, achieving considerable segmentation\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uhlemeyer_S/0/1/0/all/0/1\">Svenja Uhlemeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding and Harnessing the Effect of Image Transformation in Adversarial Detection. (arXiv:2201.01080v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01080","description":"<p>Deep neural networks (DNNs) are under threat from adversarial examples.\nAdversarial detection is a fundamental work for robust DNNs-based service,\nwhich distinguishes adversarial images from benign images. Image transformation\nis one of the most effective approaches to detect adversarial examples. During\nthe last few years, a variety of image transformations have been studied and\ndiscussed to design reliable adversarial detectors. In this paper, we\nsystematically review the recent progress on adversarial detection via image\ntransformations with a novel taxonomy. Then we conduct an extensive set of\nexperiments to test the detection performance of image transformations towards\nthe state-of-the-art adversarial attacks. Furthermore, we reveal that the\nsingle transformation is not capable of detecting robust adversarial examples,\nand propose an improved approach by combining multiple image transformations.\nThe results show that the joint approach achieves significant improvement in\ndetection accuracy and recall. We suggest that the joint detector is a more\neffective tool to detect adversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuefeng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weidong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying the exterior image of buildings on a 3D map and extracting elevation information using deep learning and digital image processing. (arXiv:2201.01081v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01081","description":"<p>Despite the fact that architectural administration information in Korea has\nbeen providing high-quality information for a long period of time, the level of\nutility of the information is not high because it focuses on administrative\ninformation. While this is the case, a three-dimensional (3D) map with higher\nresolution has emerged along with the technological development. However, it\ncannot function better than visual transmission, as it includes only image\ninformation focusing on the exterior of the building. If information related to\nthe exterior of the building can be extracted or identified from a 3D map, it\nis expected that the utility of the information will be more valuable as the\nnational architectural administration information can then potentially be\nextended to include such information regarding the building exteriors to the\nlevel of BIM(Building Information Modeling). This study aims to present and\nassess a basic method of extracting information related to the appearance of\nthe exterior of a building for the purpose of 3D mapping using deep learning\nand digital image processing. After extracting and preprocessing images from\nthe map, information was identified using the Fast R-CNN(Regions with\nConvolutional Neuron Networks) model. The information was identified using the\nFaster R-CNN model after extracting and preprocessing images from the map. As a\nresult, it showed approximately 93% and 91% accuracy in terms of detecting the\nelevation and window parts of the building, respectively, as well as excellent\nperformance in an experiment aimed at extracting the elevation information of\nthe building. Nonetheless, it is expected that improved results will be\nobtained by supplementing the probability of mixing the false detection rate or\nnoise data caused by the misunderstanding of experimenters in relation to the\nunclear boundaries of windows.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shon_D/0/1/0/all/0/1\">Donghwa Shon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_B/0/1/0/all/0/1\">Byeongjoon Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_N/0/1/0/all/0/1\">Nahyang Byun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Quality-aware Representation for Multi-person Pose Regression. (arXiv:2201.01087v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01087","description":"<p>Off-the-shelf single-stage multi-person pose regression methods generally\nleverage the instance score (i.e., confidence of the instance localization) to\nindicate the pose quality for selecting the pose candidates. We consider that\nthere are two gaps involved in existing paradigm:~1) The instance score is not\nwell interrelated with the pose regression quality.~2) The instance feature\nrepresentation, which is used for predicting the instance score, does not\nexplicitly encode the structural pose information to predict the reasonable\nscore that represents pose regression quality. To address the aforementioned\nissues, we propose to learn the pose regression quality-aware representation.\nConcretely, for the first gap, instead of using the previous instance\nconfidence label (e.g., discrete {1,0} or Gaussian representation) to denote\nthe position and confidence for person instance, we firstly introduce the\nConsistent Instance Representation (CIR) that unifies the pose regression\nquality score of instance and the confidence of background into a pixel-wise\nscore map to calibrates the inconsistency between instance score and pose\nregression quality. To fill the second gap, we further present the Query\nEncoding Module (QEM) including the Keypoint Query Encoding (KQE) to encode the\npositional and semantic information for each keypoint and the Pose Query\nEncoding (PQE) which explicitly encodes the predicted structural pose\ninformation to better fit the Consistent Instance Representation (CIR). By\nusing the proposed components, we significantly alleviate the above gaps. Our\nmethod outperforms previous single-stage regression-based even bottom-up\nmethods and achieves the state-of-the-art result of 71.7 AP on MS COCO test-dev\nset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yabo Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dongdong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Short Range Correlation Transformer for Occluded Person Re-Identification. (arXiv:2201.01090v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01090","description":"<p>Occluded person re-identification is one of the challenging areas of computer\nvision, which faces problems such as inefficient feature representation and low\nrecognition accuracy. Convolutional neural network pays more attention to the\nextraction of local features, therefore it is difficult to extract features of\noccluded pedestrians and the effect is not so satisfied. Recently, vision\ntransformer is introduced into the field of re-identification and achieves the\nmost advanced results by constructing the relationship of global features\nbetween patch sequences. However, the performance of vision transformer in\nextracting local features is inferior to that of convolutional neural network.\nTherefore, we design a partial feature transformer-based person\nre-identification framework named PFT. The proposed PFT utilizes three modules\nto enhance the efficiency of vision transformer. (1) Patch full dimension\nenhancement module. We design a learnable tensor with the same size as patch\nsequences, which is full-dimensional and deeply embedded in patch sequences to\nenrich the diversity of training samples. (2) Fusion and reconstruction module.\nWe extract the less important part of obtained patch sequences, and fuse them\nwith original patch sequence to reconstruct the original patch sequences. (3)\nSpatial Slicing Module. We slice and group patch sequences from spatial\ndirection, which can effectively improve the short-range correlation of patch\nsequences. Experimental results over occluded and holistic re-identification\ndatasets demonstrate that the proposed PFT network achieves superior\nperformance consistently and outperforms the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunbin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Songhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongsheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhiwei Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Transferable Unrestricted Adversarial Examples with Minimum Changes. (arXiv:2201.01102v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01102","description":"<p>Transfer-based adversarial example is one of the most important classes of\nblack-box attacks. However, there is a trade-off between transferability and\nimperceptibility of the adversarial perturbation. Prior work in this direction\noften requires a fixed but large $\\ell_p$-norm perturbation budget to reach a\ngood transfer success rate, leading to perceptible adversarial perturbations.\nOn the other hand, most of the current unrestricted adversarial attacks that\naim to generate semantic-preserving perturbations suffer from weaker\ntransferability to the target model. In this work, we propose a geometry-aware\nframework to generate transferable adversarial examples with minimum changes.\nAnalogous to model selection in statistical machine learning, we leverage a\nvalidation model to select the optimal perturbation budget for each image under\nboth the $\\ell_{\\infty}$-norm and unrestricted threat models. Extensive\nexperiments verify the effectiveness of our framework on balancing\nimperceptibility and transferability of the crafted adversarial examples. The\nmethodology is the foundation of our entry to the CVPR'21 Security AI\nChallenger: Unrestricted Adversarial Attacks on ImageNet, in which we ranked\n1st place out of 1,559 teams and surpassed the runner-up submissions by 4.59%\nand 23.91% in terms of final score and average image quality level,\nrespectively. Code is available at https://github.com/Equationliu/GA-Attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangcheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Depression Detection Using Skeleton-Based Gait Information. (arXiv:2201.01115v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01115","description":"<p>In recent years, the incidence of depression is rising rapidly worldwide, but\nlarge-scale depression screening is still challenging. Gait analysis provides a\nnon-contact, low-cost, and efficient early screening method for depression.\nHowever, the early screening of depression based on gait analysis lacks\nsufficient effective sample data. In this paper, we propose a skeleton data\naugmentation method for assessing the risk of depression. First, we propose\nfive techniques to augment skeleton data and apply them to depression and\nemotion datasets. Then, we divide augmentation methods into two types\n(non-noise augmentation and noise augmentation) based on the mutual information\nand the classification accuracy. Finally, we explore which augmentation\nstrategies can capture the characteristics of human skeleton data more\neffectively. Experimental results show that the augmented training data set\nthat retains more of the raw skeleton data properties determines the\nperformance of the detection model. Specifically, rotation augmentation and\nchannel mask augmentation make the depression detection accuracy reach 92.15%\nand 91.34%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingjing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haifeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepVisualInsight: Time-Travelling Visualization for Spatio-Temporal Causality of Deep Classification Training. (arXiv:2201.01155v1 [cs.LG])","link":"http://arxiv.org/abs/2201.01155","description":"<p>Understanding how the predictions of deep learning models are formed during\nthe training process is crucial to improve model performance and fix model\ndefects, especially when we need to investigate nontrivial training strategies\nsuch as active learning, and track the root cause of unexpected training\nresults such as performance degeneration.\n</p>\n<p>In this work, we propose a time-travelling visual solution DeepVisualInsight\n(DVI), aiming to manifest the spatio-temporal causality while training a deep\nlearning image classifier. The spatio-temporal causality demonstrates how the\ngradient-descent algorithm and various training data sampling techniques can\ninfluence and reshape the layout of learnt input representation and the\nclassification boundaries in consecutive epochs. Such causality allows us to\nobserve and analyze the whole learning process in the visible low dimensional\nspace. Technically, we propose four spatial and temporal properties and design\nour visualization solution to satisfy them. These properties preserve the most\nimportant information when inverse-)projecting input samples between the\nvisible low-dimensional and the invisible high-dimensional space, for causal\nanalyses. Our extensive experiments show that, comparing to baseline\napproaches, we achieve the best visualization performance regarding the\nspatial/temporal properties and visualization efficiency. Moreover, our case\nstudy shows that our visual solution can well reflect the characteristics of\nvarious training scenarios, showing good potential of DVI as a debugging tool\nfor analyzing deep learning training processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xianglin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruofan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jin Song Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hong Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepFGS: Fine-Grained Scalable Coding for Learned Image Compression. (arXiv:2201.01173v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01173","description":"<p>Scalable coding, which can adapt to channel bandwidth variation, performs\nwell in today's complex network environment. However, the existing scalable\ncompression methods face two challenges: reduced compression performance and\ninsufficient scalability. In this paper, we propose the first learned\nfine-grained scalable image compression model (DeepFGS) to overcome the above\ntwo shortcomings. Specifically, we introduce a feature separation backbone to\ndivide the image information into basic and scalable features, then\nredistribute the features channel by channel through an information\nrearrangement strategy. In this way, we can generate a continuously scalable\nbitstream via one-pass encoding. In addition, we reuse the decoder to reduce\nthe parameters and computational complexity of DeepFGS. Experiments demonstrate\nthat our DeepFGS outperforms all learning-based scalable image compression\nmodels and conventional scalable image codecs in PSNR and MS-SSIM metrics. To\nthe best of our knowledge, our DeepFGS is the first exploration of learned\nfine-grained scalable coding, which achieves the finest scalability compared\nwith learning-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_Y/0/1/0/all/0/1\">Yongqi Zhai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Ronggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated 3D reconstruction of LoD2 and LoD1 models for all 10 million buildings of the Netherlands. (arXiv:2201.01191v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01191","description":"<p>In this paper we present our workflow to automatically reconstruct 3D\nbuilding models based on 2D building polygons and a LiDAR point cloud. The\nworkflow generates models at different levels of detail (LoDs) to support data\nrequirements of different applications from one consistent source. Specific\nattention has been paid to make the workflow robust to quickly run a new\niteration in case of improvements in an algorithm or in case new input data\nbecome available. The quality of the reconstructed data highly depends on the\nquality of the input data and is monitored in several steps of the process. A\n3D viewer has been developed to view and download the openly available 3D data\nat different LoDs in different formats. The workflow has been applied to all 10\nmillion buildings of The Netherlands. The 3D service will be updated after new\ninput data becomes available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peters_R/0/1/0/all/0/1\">Ravi Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dukai_B/0/1/0/all/0/1\">Bal&#xe1;zs Dukai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitalis_S/0/1/0/all/0/1\">Stelios Vitalis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liempt_J/0/1/0/all/0/1\">Jordi van Liempt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoter_J/0/1/0/all/0/1\">Jantien Stoter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The cluster structure function. (arXiv:2201.01222v1 [cs.LG])","link":"http://arxiv.org/abs/2201.01222","description":"<p>For each partition of a data set into a given number of parts there is a\npartition such that every part is as much as possible a good model (an\n\"algorithmic sufficient statistic\") for the data in that part. Since this can\nbe done for every number between one and the number of data, the result is a\nfunction, the cluster structure function. It maps the number of parts of a\npartition to values related to the deficiencies of being good models by the\nparts. Such a function starts with a value at least zero for no partition of\nthe data set and descents to zero for the partition of the data set into\nsingleton parts. The optimal clustering is the one chosen to minimize the\ncluster structure function. The theory behind the method is expressed in\nalgorithmic information theory (Kolmogorov complexity). In practice the\nKolmogorov complexities involved are approximated by a concrete compressor. We\ngive examples using real data sets: the MNIST handwritten digits and the\nsegmentation of real cells as used in stem cell research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Andrew R. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitanyi_P/0/1/0/all/0/1\">Paul M.B. Vit&#xe1;nyi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Semi-supervised Federated Learning for Images Automatic Recognition in Internet of Drones. (arXiv:2201.01230v1 [cs.LG])","link":"http://arxiv.org/abs/2201.01230","description":"<p>Air access networks have been recognized as a significant driver of various\nInternet of Things (IoT) services and applications. In particular, the aerial\ncomputing network infrastructure centered on the Internet of Drones has set off\na new revolution in automatic image recognition. This emerging technology\nrelies on sharing ground truth labeled data between Unmanned Aerial Vehicle\n(UAV) swarms to train a high-quality automatic image recognition model.\nHowever, such an approach will bring data privacy and data availability\nchallenges. To address these issues, we first present a Semi-supervised\nFederated Learning (SSFL) framework for privacy-preserving UAV image\nrecognition. Specifically, we propose model parameters mixing strategy to\nimprove the naive combination of FL and semi-supervised learning methods under\ntwo realistic scenarios (labels-at-client and labels-at-server), which is\nreferred to as Federated Mixing (FedMix). Furthermore, there are significant\ndifferences in the number, features, and distribution of local data collected\nby UAVs using different camera modules in different environments, i.e.,\nstatistical heterogeneity. To alleviate the statistical heterogeneity problem,\nwe propose an aggregation rule based on the frequency of the client's\nparticipation in training, namely the FedFreq aggregation rule, which can\nadjust the weight of the corresponding local model according to its frequency.\nNumerical results demonstrate that the performance of our proposed method is\nsignificantly better than those of the current baseline and is robust to\ndifferent non-IID levels of client data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiyao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaohui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zehui Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jiawen Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kejia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1\">Dusit Niyato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning for Retinal Vascular Disease Detection: A Pilot Study with Diabetic Retinopathy and Retinopathy of Prematurity. (arXiv:2201.01250v1 [cs.LG])","link":"http://arxiv.org/abs/2201.01250","description":"<p>Retinal vascular diseases affect the well-being of human body and sometimes\nprovide vital signs of otherwise undetected bodily damage. Recently, deep\nlearning techniques have been successfully applied for detection of diabetic\nretinopathy (DR). The main obstacle of applying deep learning techniques to\ndetect most other retinal vascular diseases is the limited amount of data\navailable. In this paper, we propose a transfer learning technique that aims to\nutilize the feature similarities for detecting retinal vascular diseases. We\nchoose the well-studied DR detection as a source task and identify the early\ndetection of retinopathy of prematurity (ROP) as the target task. Our\nexperimental results demonstrate that our DR-pretrained approach dominates in\nall metrics the conventional ImageNet-pretrained transfer learning approach,\ncurrently adopted in medical image analysis. Moreover, our approach is more\nrobust with respect to the stochasticity in the training process and with\nrespect to reduced training samples. This study suggests the potential of our\nproposed transfer learning approach for a broad range of retinal vascular\ndiseases or pathologies, where data is limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikuchi_Y/0/1/0/all/0/1\">Yusuke Kikuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jinglin Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qiong Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Rui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images. (arXiv:2201.01266v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01266","description":"<p>Semantic segmentation of brain tumors is a fundamental medical image analysis\ntask involving multiple MRI imaging modalities that can assist clinicians in\ndiagnosing the patient and successively studying the progression of the\nmalignant entity. In recent years, Fully Convolutional Neural Networks (FCNNs)\napproaches have become the de facto standard for 3D medical image segmentation.\nThe popular \"U-shaped\" network architecture has achieved state-of-the-art\nperformance benchmarks on different 2D and 3D semantic segmentation tasks and\nacross various imaging modalities. However, due to the limited kernel size of\nconvolution layers in FCNNs, their performance of modeling long-range\ninformation is sub-optimal, and this can lead to deficiencies in the\nsegmentation of tumors with variable sizes. On the other hand, transformer\nmodels have demonstrated excellent capabilities in capturing such long-range\ninformation in multiple domains, including natural language processing and\ncomputer vision. Inspired by the success of vision transformers and their\nvariants, we propose a novel segmentation model termed Swin UNEt TRansformers\n(Swin UNETR). Specifically, the task of 3D brain tumor semantic segmentation is\nreformulated as a sequence to sequence prediction problem wherein multi-modal\ninput data is projected into a 1D sequence of embedding and used as an input to\na hierarchical Swin transformer as the encoder. The swin transformer encoder\nextracts features at five different resolutions by utilizing shifted windows\nfor computing self-attention and is connected to an FCNN-based decoder at each\nresolution via skip connections. We have participated in BraTS 2021\nsegmentation challenge, and our proposed model ranks among the top-performing\napproaches in the validation phase. Code: https://monai.io/research/swin-unetr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nath_V/0/1/0/all/0/1\">Vishwesh Nath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Quadruple Pattern: A Novel Descriptor for Facial Image Recognition and Retrieval. (arXiv:2201.01275v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01275","description":"<p>In this paper a novel hand crafted local quadruple pattern (LQPAT) is\nproposed for facial image recognition and retrieval. Most of the existing\nhand-crafted descriptors encodes only a limited number of pixels in the local\nneighbourhood. Under unconstrained environment the performance of these\ndescriptors tends to degrade drastically. The major problem in increasing the\nlocal neighbourhood is that, it also increases the feature length of the\ndescriptor. The proposed descriptor try to overcome these problems by defining\nan efficient encoding structure with optimal feature length. The proposed\ndescriptor encodes relations amongst the neighbours in quadruple space. Two\nmicro patterns are computed from the local relationships to form the\ndescriptor. The retrieval and recognition accuracies of the proposed descriptor\nhas been compared with state of the art hand crafted descriptors on bench mark\ndatabases namely; Caltech-face, LFW, Colour-FERET, and CASIA-face-v5. Result\nanalysis shows that the proposed descriptor performs well under uncontrolled\nvariations in pose, illumination, background and expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Soumendu Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pavan Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Directional Gradient Pattern: A Local Descriptor for Face Recognition. (arXiv:2201.01276v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01276","description":"<p>In this paper a local pattern descriptor in high order derivative space is\nproposed for face recognition. The proposed local directional gradient pattern\n(LDGP) is a 1D local micropattern computed by encoding the relationships\nbetween the higher order derivatives of the reference pixel in four distinct\ndirections. The proposed descriptor identifies the relationship between the\nhigh order derivatives of the referenced pixel in four different directions to\ncompute the micropattern which corresponds to the local feature. Proposed\ndescriptor considerably reduces the length of the micropattern which\nconsequently reduces the extraction time and matching time while maintaining\nthe recognition rate. Results of the extensive experiments conducted on\nbenchmark databases AT&amp;T, Extended Yale B and CMU-PIE show that the proposed\ndescriptor significantly reduces the extraction as well as matching time while\nthe recognition rate is almost similar to the existing state of the art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Soumendu Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pavan Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Learning from 100 Million Medical Images. (arXiv:2201.01283v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01283","description":"<p>Building accurate and robust artificial intelligence systems for medical\nimage assessment requires not only the research and design of advanced deep\nlearning models but also the creation of large and curated sets of annotated\ntraining examples. Constructing such datasets, however, is often very costly --\ndue to the complex nature of annotation tasks and the high level of expertise\nrequired for the interpretation of medical images (e.g., expert radiologists).\nTo counter this limitation, we propose a method for self-supervised learning of\nrich image features based on contrastive learning and online feature\nclustering. For this purpose we leverage large training datasets of over\n100,000,000 medical images of various modalities, including radiography,\ncomputed tomography (CT), magnetic resonance (MR) imaging and ultrasonography.\nWe propose to use these features to guide model training in supervised and\nhybrid self-supervised/supervised regime on various downstream tasks. We\nhighlight a number of advantages of this strategy on challenging image\nassessment problems in radiography, CT and MR: 1) Significant increase in\naccuracy compared to the state-of-the-art (e.g., AUC boost of 3-7% for\ndetection of abnormalities from chest radiography scans and hemorrhage\ndetection on brain CT); 2) Acceleration of model convergence during training by\nup to 85% compared to using no pretraining (e.g., 83% when training a model for\ndetection of brain metastases in MR scans); 3) Increase in robustness to\nvarious image augmentations, such as intensity variations, rotations or scaling\nreflective of data variation seen in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghesu_F/0/1/0/all/0/1\">Florin C. Ghesu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_B/0/1/0/all/0/1\">Bogdan Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_A/0/1/0/all/0/1\">Awais Mansoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjin Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_D/0/1/0/all/0/1\">Dominik Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_P/0/1/0/all/0/1\">Pragneshkumar Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwanath_R/0/1/0/all/0/1\">R.S. Vishwanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balter_J/0/1/0/all/0/1\">James M. Balter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grbic_S/0/1/0/all/0/1\">Sasa Grbic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comaniciu_D/0/1/0/all/0/1\">Dorin Comaniciu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-Based Siamese Network for Change Detection. (arXiv:2201.01293v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01293","description":"<p>This paper presents a transformer-based Siamese network architecture\n(abbreviated by ChangeFormer) for Change Detection (CD) from a pair of\nco-registered remote sensing images. Different from recent CD frameworks, which\nare based on fully convolutional networks (ConvNets), the proposed method\nunifies hierarchically structured transformer encoder with Multi-Layer\nPerception (MLP) decoder in a Siamese network architecture to efficiently\nrender multi-scale long-range details required for accurate CD. Experiments on\ntwo CD datasets show that the proposed end-to-end trainable ChangeFormer\narchitecture achieves better CD performance than previous counterparts. Our\ncode is available at https://github.com/wgcban/ChangeFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DVSR: 3D EPI Volume-based Approach for Angular and Spatial Light field Image Super-resolution. (arXiv:2201.01294v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01294","description":"<p>Light field (LF) imaging, which captures both spatial and angular information\nof a scene, is undoubtedly beneficial to numerous applications. Although\nvarious techniques have been proposed for LF acquisition, achieving both\nangularly and spatially high-resolution LF remains a technology challenge. In\nthis paper, a learning-based approach applied to 3D epipolar image (EPI) is\nproposed to reconstruct high-resolution LF. Through a 2-stage super-resolution\nframework, the proposed approach effectively addresses various LF\nsuper-resolution (SR) problems, i.e., spatial SR, angular SR, and\nangular-spatial SR. While the first stage provides flexible options to\nup-sample EPI volume to the desired resolution, the second stage, which\nconsists of a novel EPI volume-based refinement network (EVRN), substantially\nenhances the quality of the high-resolution EPI volume. An extensive evaluation\non 90 challenging synthetic and real-world light field scenes from 7 published\ndatasets shows that the proposed approach outperforms state-of-the-art methods\nto a large extend for both spatial and angular super-resolution problem, i.e.,\nan average peak signal to noise ratio improvement of more than 2.0 dB, 1.4 dB,\nand 3.14 dB in spatial SR $\\times 2$, spatial SR $\\times 4$, and angular SR\nrespectively. The reconstructed 4D light field demonstrates a balanced\nperformance distribution across all perspective images and presents superior\nvisual quality compared to the previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trung-Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berberich_J/0/1/0/all/0/1\">Jan Berberich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_S/0/1/0/all/0/1\">Sven Simon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Multi-Object Tracking with Unsupervised Re-Identification Learning and Occlusion Estimation. (arXiv:2201.01297v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01297","description":"<p>Occlusion between different objects is a typical challenge in Multi-Object\nTracking (MOT), which often leads to inferior tracking results due to the\nmissing detected objects. The common practice in multi-object tracking is\nre-identifying the missed objects after their reappearance. Though tracking\nperformance can be boosted by the re-identification, the annotation of identity\nis required to train the model. In addition, such practice of re-identification\nstill can not track those highly occluded objects when they are missed by the\ndetector. In this paper, we focus on online multi-object tracking and design\ntwo novel modules, the unsupervised re-identification learning module and the\nocclusion estimation module, to handle these problems. Specifically, the\nproposed unsupervised re-identification learning module does not require any\n(pseudo) identity information nor suffer from the scalability issue. The\nproposed occlusion estimation module tries to predict the locations where\nocclusions happen, which are used to estimate the positions of missed objects\nby the detector. Our study shows that, when applied to state-of-the-art MOT\nmethods, the proposed unsupervised re-identification learning is comparable to\nsupervised re-identification learning, and the tracking performance is further\nimproved by the proposed occlusion estimation module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiankun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Object Removal and Spatio-Temporal RGB-D Inpainting via Geometry-Aware Adversarial Learning. (arXiv:2008.05058v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.05058","description":"<p>Dynamic objects have a significant impact on the robot's perception of the\nenvironment which degrades the performance of essential tasks such as\nlocalization and mapping. In this work, we address this problem by synthesizing\nplausible color, texture and geometry in regions occluded by dynamic objects.\nWe propose the novel geometry-aware DynaFill architecture that follows a\ncoarse-to-fine topology and incorporates our gated recurrent feedback mechanism\nto adaptively fuse information from previous timesteps. We optimize our\narchitecture using adversarial training to synthesize fine realistic textures\nwhich enables it to hallucinate color and depth structure in occluded regions\nonline in a spatially and temporally coherent manner, without relying on future\nframe information. Casting our inpainting problem as an image-to-image\ntranslation task, our model also corrects regions correlated with the presence\nof dynamic objects in the scene, such as shadows or reflections. We introduce a\nlarge-scale hyperrealistic dataset with RGB-D images, semantic segmentation\nlabels, camera poses as well as groundtruth RGB-D information of occluded\nregions. Extensive quantitative and qualitative evaluations show that our\napproach achieves state-of-the-art performance, even in challenging weather\nconditions. Furthermore, we present results for retrieval-based visual\nlocalization with the synthesized images that demonstrate the utility of our\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Besic_B/0/1/0/all/0/1\">Borna Be&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to Minimax Optimization. (arXiv:2008.08170v5 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2008.08170","description":"<p>In the paper, we propose a class of accelerated zeroth-order and first-order\nmomentum methods for both nonconvex mini-optimization and minimax-optimization.\nSpecifically, we propose a new accelerated zeroth-order momentum (Acc-ZOM)\nmethod for black-box mini-optimization. Moreover, we prove that our Acc-ZOM\nmethod achieves a lower query complexity of $\\tilde{O}(d^{3/4}\\epsilon^{-3})$\nfor finding an $\\epsilon$-stationary point, which improves the best known\nresult by a factor of $O(d^{1/4})$ where $d$ denotes the variable dimension. In\nparticular, the Acc-ZOM does not require large batches required in the existing\nzeroth-order stochastic algorithms. Meanwhile, we propose an accelerated\n\\textbf{zeroth-order} momentum descent ascent (Acc-ZOMDA) method for\n\\textbf{black-box} minimax-optimization, which obtains a query complexity of\n$\\tilde{O}((d_1+d_2)^{3/4}\\kappa_y^{4.5}\\epsilon^{-3})$ without large batches\nfor finding an $\\epsilon$-stationary point, where $d_1$ and $d_2$ denote\nvariable dimensions and $\\kappa_y$ is condition number. Moreover, we propose an\naccelerated \\textbf{first-order} momentum descent ascent (Acc-MDA) method for\n\\textbf{white-box} minimax optimization, which has a gradient complexity of\n$\\tilde{O}(\\kappa_y^{4.5}\\epsilon^{-3})$ without large batches for finding an\n$\\epsilon$-stationary point. In particular, our Acc-MDA can obtain a lower\ngradient complexity of $\\tilde{O}(\\kappa_y^{2.5}\\epsilon^{-3})$ with a batch\nsize $O(\\kappa_y^4)$. Extensive experimental results on the black-box\nadversarial attack to deep neural networks (DNNs) and poisoning attack\ndemonstrate efficiency of our algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gao_S/0/1/0/all/0/1\">Shangqian Gao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Normalization: Improving Deep Convolutional Network Robustness and Training. (arXiv:2103.00673v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.00673","description":"<p>Normalization techniques have become a basic component in modern\nconvolutional neural networks (ConvNets). In particular, many recent works\ndemonstrate that promoting the orthogonality of the weights helps train deep\nmodels and improve robustness. For ConvNets, most existing methods are based on\npenalizing or normalizing weight matrices derived from concatenating or\nflattening the convolutional kernels. These methods often destroy or ignore the\nbenign convolutional structure of the kernels; therefore, they are often\nexpensive or impractical for deep ConvNets. In contrast, we introduce a simple\nand efficient \"Convolutional Normalization\" (ConvNorm) method that can fully\nexploit the convolutional structure in the Fourier domain and serve as a simple\nplug-and-play module to be conveniently incorporated into any ConvNets. Our\nmethod is inspired by recent work on preconditioning methods for convolutional\nsparse coding and can effectively promote each layer's channel-wise isometry.\nFurthermore, we show that our ConvNorm can reduce the layerwise spectral norm\nof the weight matrices and hence improve the Lipschitzness of the network,\nleading to easier training and improved robustness for deep ConvNets. Applied\nto classification under noise corruptions and generative adversarial network\n(GAN), we show that the ConvNorm improves the robustness of common ConvNets\nsuch as ResNet and the performance of GAN. We verify our findings via numerical\nexperiments on CIFAR and ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yuexiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chong You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhihui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Q/0/1/0/all/0/1\">Qing Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey On Universal Adversarial Attack. (arXiv:2103.01498v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.01498","description":"<p>The intriguing phenomenon of adversarial examples has attracted significant\nattention in machine learning and what might be more surprising to the\ncommunity is the existence of universal adversarial perturbations (UAPs), i.e.\na single perturbation to fool the target DNN for most images. With the focus on\nUAP against deep classifiers, this survey summarizes the recent progress on\nuniversal adversarial attacks, discussing the challenges from both the attack\nand defense sides, as well as the reason for the existence of UAP. We aim to\nextend this work as a dynamic survey that will regularly update its content to\nfollow new works regarding UAP or universal attack in a wide range of domains,\nsuch as image, audio, video, text, etc. Relevant updates will be discussed at:\nhttps://bit.ly/2SbQlLG. We welcome authors of future works in this field to\ncontact us for including your new finding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benz_P/0/1/0/all/0/1\">Philipp Benz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenguo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karjauv_A/0/1/0/all/0/1\">Adil Karjauv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the effectiveness of adversarial training against common corruptions. (arXiv:2103.02325v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.02325","description":"<p>The literature on robustness towards common corruptions shows no consensus on\nwhether adversarial training can improve the performance in this setting.\nFirst, we show that, when used with an appropriately selected perturbation\nradius, $\\ell_p$ adversarial training can serve as a strong baseline against\ncommon corruptions improving both accuracy and calibration. Then we explain why\nadversarial training performs better than data augmentation with simple\nGaussian noise which has been observed to be a meaningful baseline on common\ncorruptions. Related to this, we identify the $\\sigma$-overfitting phenomenon\nwhen Gaussian augmentation overfits to a particular standard deviation used for\ntraining which has a significant detrimental effect on common corruption\naccuracy. We discuss how to alleviate this problem and then how to further\nenhance $\\ell_p$ adversarial training by introducing an efficient relaxation of\nadversarial training with learned perceptual image patch similarity as the\ndistance metric. Through experiments on CIFAR-10 and ImageNet-100, we show that\nour approach does not only improve the $\\ell_p$ adversarial training baseline\nbut also has cumulative gains with data augmentation methods such as AugMix,\nDeepAugment, ANT, and SIN, leading to state-of-the-art performance on common\ncorruptions.\n</p>\n<p>The code of our experiments is publicly available at\nhttps://github.com/tml-epfl/adv-training-corruptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kireev_K/0/1/0/all/0/1\">Klim Kireev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andriushchenko_M/0/1/0/all/0/1\">Maksym Andriushchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1\">Nicolas Flammarion</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Networks Learn Meta-Structures from Noisy Labels in Semantic Segmentation. (arXiv:2103.11594v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11594","description":"<p>How deep neural networks (DNNs) learn from noisy labels has been studied\nextensively in image classification but much less in image segmentation. So\nfar, our understanding of the learning behavior of DNNs trained by noisy\nsegmentation labels remains limited. In this study, we address this deficiency\nin both binary segmentation of biological microscopy images and multi-class\nsegmentation of natural images. We generate extremely noisy labels by randomly\nsampling a small fraction (e.g., 10%) or flipping a large fraction (e.g., 90%)\nof the ground truth labels. When trained with these noisy labels, DNNs provide\nlargely the same segmentation performance as trained by the original ground\ntruth. This indicates that DNNs learn structures hidden in labels rather than\npixel-level labels per se in their supervised training for semantic\nsegmentation. We refer to these hidden structures in labels as meta-structures.\nWhen DNNs are trained by labels with different perturbations to the\nmeta-structure, we find consistent degradation in their segmentation\nperformance. In contrast, incorporation of meta-structure information\nsubstantially improves performance of an unsupervised segmentation model\ndeveloped for binary semantic segmentation. We define meta-structures\nmathematically as spatial density distributions and show both theoretically and\nexperimentally how this formulation explains key observed learning behavior of\nDNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yaoru Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guole Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanhao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Ge Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Implicit Statistical Shape Models for 3D Medical Image Delineation. (arXiv:2104.02847v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02847","description":"<p>3D delineation of anatomical structures is a cardinal goal in medical imaging\nanalysis. Prior to deep learning, statistical shape models that imposed\nanatomical constraints and produced high quality surfaces were a core\ntechnology. Prior to deep learning, statistical shape models that imposed\nanatomical constraints and produced high quality surfaces were a core\ntechnology. Today fully-convolutional networks (FCNs), while dominant, do not\noffer these capabilities. We present deep implicit statistical shape models\n(DISSMs), a new approach to delineation that marries the representation power\nof convolutional neural networks (CNNs) with the robustness of SSMs. DISSMs use\na deep implicit surface representation to produce a compact and descriptive\nshape latent space that permits statistical models of anatomical variance. To\nreliably fit anatomically plausible shapes to an image, we introduce a novel\nrigid and non-rigid pose estimation pipeline that is modelled as a Markov\ndecision process(MDP). We outline a training regime that includes inverted\nepisodic training and a deep realization of marginal space learning (MSL).\nIntra-dataset experiments on the task of pathological liver segmentation\ndemonstrate that DISSMs can perform more robustly than three leading FCN\nmodels, including nnU-Net: reducing the mean Hausdorff distance (HD) by\n7.7-14.3mm and improving the worst case Dice-Sorensen coefficient (DSC) by\n1.2-2.3%. More critically, cross-dataset experiments on a dataset directly\nreflecting clinical deployment scenarios demonstrate that DISSMs improve the\nmean DSC and HD by 3.5-5.9% and 12.3-24.5mm, respectively, and the worst-case\nDSC by 5.4-7.3%. These improvements are over and above any benefits from\nrepresenting delineations with high-quality surface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Ashwin Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_S/0/1/0/all/0/1\">Shun Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Dakai Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_A/0/1/0/all/0/1\">Adam P. Harrison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shadow Generation for Composite Image in Real-world Scenes. (arXiv:2104.10338v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10338","description":"<p>Image composition targets at inserting a foreground object into a background\nimage. Most previous image composition methods focus on adjusting the\nforeground to make it compatible with background while ignoring the shadow\neffect of foreground on the background. In this work, we focus on generating\nplausible shadow for the foreground object in the composite image. First, we\ncontribute a real-world shadow generation dataset DESOBA by generating\nsynthetic composite images based on paired real images and deshadowed images.\nThen, we propose a novel shadow generation network SGRNet, which consists of a\nshadow mask prediction stage and a shadow filling stage. In the shadow mask\nprediction stage, foreground and background information are thoroughly\ninteracted to generate foreground shadow mask. In the shadow filling stage,\nshadow parameters are predicted to fill the shadow area. Extensive experiments\non our DESOBA dataset and real composite images demonstrate the effectiveness\nof our proposed method. Our dataset and code are available at\nhttps://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianfu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain and Disentangled Face Manipulation with 3D Guidance. (arXiv:2104.11228v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11228","description":"<p>Face image manipulation via three-dimensional guidance has been widely\napplied in various interactive scenarios due to its semantically-meaningful\nunderstanding and user-friendly controllability. However, existing\n3D-morphable-model-based manipulation methods are not directly applicable to\nout-of-domain faces, such as non-photorealistic paintings, cartoon portraits,\nor even animals, mainly due to the formidable difficulties in building the\nmodel for each specific face domain. To overcome this challenge, we propose, as\nfar as we know, the first method to manipulate faces in arbitrary domains using\nhuman 3DMM. This is achieved through two major steps: 1) disentangled mapping\nfrom 3DMM parameters to the latent space embedding of a pre-trained StyleGAN2\nthat guarantees disentangled and precise controls for each semantic attribute;\nand 2) cross-domain adaptation that bridges domain discrepancies and makes\nhuman 3DMM applicable to out-of-domain faces by enforcing a consistent latent\nspace embedding. Experiments and comparisons demonstrate the superiority of our\nhigh-quality semantic manipulation method on a variety of face domains with all\nmajor 3D facial attributes controllable-pose, expression, shape, albedo, and\nillumination. Moreover, we develop an intuitive editing interface to support\nuser-friendly control and instant feedback. Our project page is\nhttps://cassiepython.github.io/cddfm3d/index.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1\">Menglei Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control. (arXiv:2106.02019v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02019","description":"<p>We propose Neural Actor (NA), a new method for high-quality synthesis of\nhumans from arbitrary viewpoints and under arbitrary controllable poses. Our\nmethod is built upon recent neural scene representation and rendering works\nwhich learn representations of geometry and appearance from only 2D images.\nWhile existing works demonstrated compelling rendering of static scenes and\nplayback of dynamic scenes, photo-realistic reconstruction and rendering of\nhumans with neural implicit methods, in particular under user-controlled novel\nposes, is still difficult. To address this problem, we utilize a coarse body\nmodel as the proxy to unwarp the surrounding 3D space into a canonical pose. A\nneural radiance field learns pose-dependent geometric deformations and pose-\nand view-dependent appearance effects in the canonical space from multi-view\nvideo input. To synthesize novel views of high fidelity dynamic geometry and\nappearance, we leverage 2D texture maps defined on the body model as latent\nvariables for predicting residual deformations and the dynamic appearance.\nExperiments demonstrate that our method achieves better quality than the\nstate-of-the-arts on playback as well as novel pose synthesis, and can even\ngeneralize well to new poses that starkly differ from the training poses.\nFurthermore, our method also supports body shape control of the synthesized\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1\">Marc Habermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnev_V/0/1/0/all/0/1\">Viktor Rudnev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_K/0/1/0/all/0/1\">Kripasindhu Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor feature hallucination for few-shot learning. (arXiv:2106.05321v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05321","description":"<p>Few-shot learning addresses the challenge of learning how to address novel\ntasks given not just limited supervision but limited data as well. An\nattractive solution is synthetic data generation. However, most such methods\nare overly sophisticated, focusing on high-quality, realistic data in the input\nspace. It is unclear whether adapting them to the few-shot regime and using\nthem for the downstream task of classification is the right approach. Previous\nworks on synthetic data generation for few-shot classification focus on\nexploiting complex models, e.g. a Wasserstein GAN with multiple regularizers or\na network that transfers latent diversities from known to novel classes.\n</p>\n<p>We follow a different approach and investigate how a simple and\nstraightforward synthetic data generation method can be used effectively. We\nmake two contributions, namely we show that: (1) using a simple loss function\nis more than enough for training a feature generator in the few-shot setting;\nand (2) learning to generate tensor features instead of vector features is\nsuperior. Extensive experiments on miniImagenet, CUB and CIFAR-FS datasets show\nthat our method sets a new state of the art, outperforming more sophisticated\nfew-shot data augmentation methods. The source code can be found at\nhttps://github.com/MichalisLazarou/TFH_fewshot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lazarou_M/0/1/0/all/0/1\">Michalis Lazarou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stathaki_T/0/1/0/all/0/1\">Tania Stathaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisBuddy -- A Smart Wearable Assistant for the Visually Challenged. (arXiv:2108.07761v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07761","description":"<p>Vision plays a crucial role in comprehending the world around us. More than\n85% of the external information is obtained through the vision system. It\ninfluences our mobility, cognition, information access, and interaction with\nthe environment and other people. Blindness prevents a person from gaining\nknowledge of the surrounding environment and makes unassisted navigation,\nobject recognition, obstacle avoidance, and reading tasks significant\nchallenges. Many existing systems are often limited by cost and complexity. To\nhelp the visually challenged overcome these difficulties faced in everyday\nlife, we propose VisBuddy, a smart assistant to help the visually challenged\nwith their day-to-day activities. VisBuddy is a voice-based assistant where the\nuser can give voice commands to perform specific tasks. It uses the techniques\nof image captioning for describing the user's surroundings, optical character\nrecognition (OCR) for reading the text in the user's view, object detection to\nsearch and find the objects in a room and web scraping to give the user the\nlatest news. VisBuddy has been built by combining the concepts from Deep\nLearning and the Internet of Things. Thus, VisBuddy serves as a cost-efficient,\npowerful, all-in-one assistant for the visually challenged by helping them with\ntheir day-to-day activities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_I/0/1/0/all/0/1\">Ishwarya Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meenakshisundaram_N/0/1/0/all/0/1\">Nishaali Meenakshisundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_I/0/1/0/all/0/1\">Ishwarya Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+D_S/0/1/0/all/0/1\">Shiloah Elizabeth D</a>, <a href=\"http://arxiv.org/find/cs/1/au:+C_S/0/1/0/all/0/1\">Sunil Retmin Raj C</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anchor DETR: Query Design for Transformer-Based Object Detection. (arXiv:2109.07107v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07107","description":"<p>In this paper, we propose a novel query design for the transformer-based\nobject detection. In previous transformer-based detectors, the object queries\nare a set of learned embeddings. However, each learned embedding does not have\nan explicit physical meaning and we cannot explain where it will focus on. It\nis difficult to optimize as the prediction slot of each object query does not\nhave a specific mode. In other words, each object query will not focus on a\nspecific region. To solved these problems, in our query design, object queries\nare based on anchor points, which are widely used in CNN-based detectors. So\neach object query focuses on the objects near the anchor point. Moreover, our\nquery design can predict multiple objects at one position to solve the\ndifficulty: \"one region, multiple objects\". In addition, we design an attention\nvariant, which can reduce the memory cost while achieving similar or better\nperformance than the standard attention in DETR. Thanks to the query design and\nthe attention variant, the proposed detector that we called Anchor DETR, can\nachieve better performance and run faster than the DETR with 10$\\times$ fewer\ntraining epochs. For example, it achieves 44.2 AP with 19 FPS on the MSCOCO\ndataset when using the ResNet50-DC5 feature for training 50 epochs. Extensive\nexperiments on the MSCOCO benchmark prove the effectiveness of the proposed\nmethods. Code is available at\n\\url{https://github.com/megvii-research/AnchorDETR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks. (arXiv:2110.03825v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03825","description":"<p>Deep neural networks (DNNs) are known to be vulnerable to adversarial\nattacks. A range of defense methods have been proposed to train adversarially\nrobust DNNs, among which adversarial training has demonstrated promising\nresults. However, despite preliminary understandings developed for adversarial\ntraining, it is still not clear, from the architectural perspective, what\nconfigurations can lead to more robust DNNs. In this paper, we address this gap\nvia a comprehensive investigation on the impact of network width and depth on\nthe robustness of adversarially trained DNNs. Specifically, we make the\nfollowing key observations: 1) more parameters (higher model capacity) does not\nnecessarily help adversarial robustness; 2) reducing capacity at the last stage\n(the last group of blocks) of the network can actually improve adversarial\nrobustness; and 3) under the same parameter budget, there exists an optimal\narchitectural configuration for adversarial robustness. We also provide a\ntheoretical analysis explaning why such network configuration can help\nrobustness. These architectural insights can help design adversarially robust\nDNNs. Code is available at \\url{https://github.com/HanxunH/RobustWRN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hanxun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Monazam Erfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1\">James Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salt and pepper noise removal method based on stationary Framelet transform with non-convex sparsity regularization. (arXiv:2110.09113v6 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.09113","description":"<p>Salt and pepper noise removal is a common inverse problem in image\nprocessing. Traditional denoising methods have two limitations. First, noise\ncharacteristics are often not described accurately. For example, the noise\nlocation information is often ignored and the sparsity of the salt and pepper\nnoise is often described by L1 norm, which cannot illustrate the sparse\nvariables clearly. Second, conventional methods separate the contaminated image\ninto a recovered image and a noise part, thus resulting in recovering an image\nwith unsatisfied smooth parts and detail parts. In this study, we introduce a\nnoise detection strategy to determine the position of the noise, and a\nnon-convex sparsity regularization depicted by Lp quasi-norm is employed to\ndescribe the sparsity of the noise, thereby addressing the first limitation.\nThe morphological component analysis framework with stationary Framelet\ntransform is adopted to decompose the processed image into cartoon, texture,\nand noise parts to resolve the second limitation. Then, the alternating\ndirection method of multipliers (ADMM) is employed to solve the proposed model.\nFinally, experiments are conducted to verify the proposed method and compare it\nwith some current state-of-the-art denoising methods. The experimental results\nshow that the proposed method can remove salt and pepper noise while preserving\nthe details of the processed image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yingpin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yuming Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Huiying Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Jianhua Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1\">Chaoqun Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanping Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Multi-Organ Segmentation Using SpatialConfiguration-Net with Low GPU Memory Requirements. (arXiv:2111.13630v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.13630","description":"<p>Even though many semantic segmentation methods exist that are able to perform\nwell on many medical datasets, often, they are not designed for direct use in\nclinical practice. The two main concerns are generalization to unseen data with\na different visual appearance, e.g., images acquired using a different scanner,\nand efficiency in terms of computation time and required Graphics Processing\nUnit (GPU) memory. In this work, we employ a multi-organ segmentation model\nbased on the SpatialConfiguration-Net (SCN), which integrates prior knowledge\nof the spatial configuration among the labelled organs to resolve spurious\nresponses in the network outputs. Furthermore, we modified the architecture of\nthe segmentation model to reduce its memory footprint as much as possible\nwithout drastically impacting the quality of the predictions. Lastly, we\nimplemented a minimal inference script for which we optimized both, execution\ntime and required GPU memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Thaler_F/0/1/0/all/0/1\">Franz Thaler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Payer_C/0/1/0/all/0/1\">Christian Payer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bischof_H/0/1/0/all/0/1\">Horst Bischof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stern_D/0/1/0/all/0/1\">Darko Stern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing the Deep: Finding Class Specific Filters in Deep CNNs. (arXiv:2112.07719v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07719","description":"<p>Interpretability of Deep Neural Networks has become a major area of\nexploration. Although these networks have achieved state of the art accuracy in\nmany tasks, it is extremely difficult to interpret and explain their decisions.\nIn this work we analyze the final and penultimate layers of Deep Convolutional\nNetworks and provide an efficient method for identifying subsets of features\nthat contribute most towards the network's decision for a class. We demonstrate\nthat the number of such features per class is much lower in comparison to the\ndimension of the final layer and therefore the decision surface of Deep CNNs\nlies on a low dimensional manifold and is proportional to the network depth.\nOur methods allow to decompose the final layer into separate subspaces which is\nfar more interpretable and has a lower computational cost as compared to the\nfinal layer of the full network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Badola_A/0/1/0/all/0/1\">Akshay Badola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_C/0/1/0/all/0/1\">Cherian Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanabhan_V/0/1/0/all/0/1\">Vineet Padmanabhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_R/0/1/0/all/0/1\">Rajendra Lal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Positional Encoding Augmented GAN for the Assessment of Wind Flow for Pedestrian Comfort in Urban Areas. (arXiv:2112.08447v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08447","description":"<p>Approximating wind flows using computational fluid dynamics (CFD) methods can\nbe time-consuming. Creating a tool for interactively designing prototypes while\nobserving the wind flow change requires simpler models to simulate faster.\nInstead of running numerical approximations resulting in detailed calculations,\ndata-driven methods and deep learning might be able to give similar results in\na fraction of the time. This work rephrases the problem from computing 3D flow\nfields using CFD to a 2D image-to-image translation-based problem on the\nbuilding footprints to predict the flow field at pedestrian height level. We\ninvestigate the use of generative adversarial networks (GAN), such as Pix2Pix\n[1] and CycleGAN [2] representing state-of-the-art for image-to-image\ntranslation task in various domains as well as U-Net autoencoder [3]. The\nmodels can learn the underlying distribution of a dataset in a data-driven\nmanner, which we argue can help the model learn the underlying\nReynolds-averaged Navier-Stokes (RANS) equations from CFD. We experiment on\nnovel simulated datasets on various three-dimensional bluff-shaped buildings\nwith and without height information. Moreover, we present an extensive\nqualitative and quantitative evaluation of the generated images for a selection\nof models and compare their performance with the simulations delivered by CFD.\nWe then show that adding positional data to the input can produce more accurate\nresults by proposing a general framework for injecting such information on the\ndifferent architectures. Furthermore, we show that the models performances\nimprove by applying attention mechanisms and spectral normalization to\nfacilitate stable training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoeiness_H/0/1/0/all/0/1\">Henrik Hoeiness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gjerde_K/0/1/0/all/0/1\">Kristoffer Gjerde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oggiano_L/0/1/0/all/0/1\">Luca Oggiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giljarhus_K/0/1/0/all/0/1\">Knut Erik Teigen Giljarhus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruocco_M/0/1/0/all/0/1\">Massimiliano Ruocco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Microfossil Identification via Deep Metric Learning. (arXiv:2112.09490v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09490","description":"<p>We apply deep metric learning for the first time to the prob-lem of\nclassifying planktic foraminifer shells on microscopic images. This species\nrecognition task is an important information source and scientific pillar for\nreconstructing past climates. All foraminifer CNN recognition pipelines in the\nliterature produce black-box classifiers that lack visualisation options for\nhuman experts and cannot be applied to open set problems. Here, we benchmark\nmetric learning against these pipelines, produce the first scientific\nvisualisation of the phenotypic planktic foraminifer morphology space, and\ndemonstrate that metric learning can be used to cluster species unseen during\ntraining. We show that metric learning out-performs all published CNN-based\nstate-of-the-art benchmarks in this domain. We evaluate our approach on the\n34,640 expert-annotated images of the Endless Forams public library of 35\nmodern planktic foraminifera species. Our results on this data show leading 92%\naccuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,\nand 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered\nin training. We conclude that metric learning is highly effective for this\ndomain and serves as an important tool towards expert-in-the-loop automation of\nmicrofossil identification. Key code, network weights, and data splits are\npublished with this paper for full reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karaderi_T/0/1/0/all/0/1\">Tayfun Karaderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1\">Tilo Burghardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiang_A/0/1/0/all/0/1\">Allison Y. Hsiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaer_J/0/1/0/all/0/1\">Jacob Ramaer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1\">Daniela N. Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Path Structural Contrastive Embeddings for Learning Novel Objects. (arXiv:2112.12359v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12359","description":"<p>Learning novel classes from a very few labeled samples has attracted\nincreasing attention in machine learning areas. Recent research on either\nmeta-learning based or transfer-learning based paradigm demonstrates that\ngaining information on a good feature space can be an effective solution to\nachieve favorable performance on few-shot tasks. In this paper, we propose a\nsimple but effective paradigm that decouples the tasks of learning feature\nrepresentations and classifiers and only learns the feature embedding\narchitecture from base classes via the typical transfer-learning training\nstrategy. To maintain both the generalization ability across base and novel\nclasses and discrimination ability within each class, we propose a dual path\nfeature learning scheme that effectively combines structural similarity with\ncontrastive feature construction. In this way, both inner-class alignment and\ninter-class uniformity can be well balanced, and result in improved\nperformance. Experiments on three popular benchmarks show that when\nincorporated with a simple prototype based classifier, our method can still\nachieve promising results for both standard and generalized few-shot problems\nin either an inductive or transductive inference setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1\">Elvis Han Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Donghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1\">Weng Kee Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generation of Synthetic Rat Brain MRI scans with a 3D Enhanced Alpha-GAN. (arXiv:2112.13626v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.13626","description":"<p>Translational brain research using Magnetic Resonance Imaging (MRI) is\nbecoming increasingly popular as animal models are an essential part of\nscientific studies and more ultra-high-field scanners are becoming available.\nSome disadvantages of MRI are the availability of MRI scanners and the time\nrequired for a full scanning session (it usually takes over 30 minutes).\nPrivacy laws and the 3Rs ethics rule also make it difficult to create large\ndatasets for training deep learning models. Generative Adversarial Networks\n(GANs) can perform data augmentation with higher quality than other techniques.\nIn this work, the alpha-GAN architecture is used to test its ability to produce\nrealistic 3D MRI scans of the rat brain. As far as the authors are aware, this\nis the first time that a GAN-based approach has been used for data augmentation\nin preclinical data. The generated scans are evaluated using various\nqualitative and quantitative metrics. A Turing test conducted by 4 experts has\nshown that the generated scans can trick almost any expert. The generated scans\nwere also used to evaluate their impact on the performance of an existing deep\nlearning model developed for segmenting the rat brain into white matter, grey\nmatter and cerebrospinal fluid. The models were compared using the Dice score.\nThe best results for whole brain and white matter segmentation were obtained\nwhen 174 real scans and 348 synthetic scans were used, with improvements of\n0.0172 and 0.0129, respectively. Using 174 real scans and 87 synthetic scans\nresulted in improvements of 0.0038 and 0.0764 for grey matter and CSF\nsegmentation, respectively. Thus, by using the proposed new normalisation layer\nand loss functions, it was possible to improve the realism of the generated rat\nMRI scans and it was shown that using the generated data improved the\nsegmentation model more than using the conventional data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ferreira_A/0/1/0/all/0/1\">Andr&#xe9; Ferreira</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Magalhaes_R/0/1/0/all/0/1\">Ricardo Magalh&#xe3;es</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Meriaux_S/0/1/0/all/0/1\">S&#xe9;bastien M&#xe9;riaux</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Alves_V/0/1/0/all/0/1\">Victor Alves</a> (1) ((1) Centro Algoritmi, University of Minho, Braga, Portugal, (2) Universit&#xe9; Paris-Saclay, CEA, CNRS, BAOBAB, NeuroSpin, Gif-sur-Yvette, France)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Associative Adversarial Learning Based on Selective Attack. (arXiv:2112.13989v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13989","description":"<p>A human's attention can intuitively adapt to corrupted areas of an image by\nrecalling a similar uncorrupted image they have previously seen. This\nobservation motivates us to improve the attention of adversarial images by\nconsidering their clean counterparts. To accomplish this, we introduce\nAssociative Adversarial Learning (AAL) into adversarial learning to guide a\nselective attack. We formulate the intrinsic relationship between attention and\nattack (perturbation) as a coupling optimization problem to improve their\ninteraction. This leads to an attention backtracking algorithm that can\neffectively enhance the attention's adversarial robustness. Our method is\ngeneric and can be used to address a variety of tasks by simply choosing\ndifferent kernels for the associative attention that select other regions for a\nspecific attack. Experimental results show that the selective attack improves\nthe model's performance. We show that our method improves the recognition\naccuracy of adversarial training on ImageNet by 8.32% compared with the\nbaseline. It also increases object detection mAP on PascalVOC by 2.02% and\nrecognition accuracy of few-shot learning on miniImageNet by 1.63%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1\">Xiaoyue Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1\">Song Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1\">David Doermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrated Hyperspectral Image Reconstruction via Graph-based Self-Tuning Network. (arXiv:2112.15362v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.15362","description":"<p>Recently, hyperspectral imaging (HSI) has attracted increasing research\nattention, especially for the ones based on a coded aperture snapshot spectral\nimaging (CASSI) system. Existing deep HSI reconstruction models are generally\ntrained on paired data to retrieve original signals upon 2D compressed\nmeasurements given by a particular optical hardware mask in CASSI, during which\nthe mask largely impacts the reconstruction performance and could work as a\n\"model hyperparameter\" governing on data augmentations. This mask-specific\ntraining style will lead to a hardware miscalibration issue, which sets up\nbarriers to deploying deep HSI models among different hardware and noisy\nenvironments. To address this challenge, we introduce mask uncertainty for HSI\nwith a complete variational Bayesian learning treatment and explicitly model it\nthrough a mask decomposition inspired by real hardware. Specifically, we\npropose a novel Graph-based Self-Tuning (GST) network to reason uncertainties\nadapting to varying spatial structures of masks among different hardware.\nMoreover, we develop a bilevel optimization framework to balance HSI\nreconstruction and uncertainty estimation, accounting for the hyperparameter\nproperty of masks. Extensive experimental results and model discussions\nvalidate the effectiveness (over 33/30 dB) of the proposed GST method under two\nmiscalibration scenarios and demonstrate a highly competitive performance\ncompared with the state-of-the-art well-calibrated methods. Our code and\npre-trained model are available at\nhttps://github.com/Jiamian-Wang/mask_uncertainty_spectral_SCI\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiamian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Ziyi Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Cross-dataset Generalization in License Plate Recognition. (arXiv:2201.00267v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00267","description":"<p>Automatic License Plate Recognition (ALPR) systems have shown remarkable\nperformance on license plates (LPs) from multiple regions due to advances in\ndeep learning and the increasing availability of datasets. The evaluation of\ndeep ALPR systems is usually done within each dataset; therefore, it is\nquestionable if such results are a reliable indicator of generalization\nability. In this paper, we propose a traditional-split versus\nleave-one-dataset-out experimental setup to empirically assess the\ncross-dataset generalization of 12 Optical Character Recognition (OCR) models\napplied to LP recognition on nine publicly available datasets with a great\nvariety in several aspects (e.g., acquisition settings, image resolution, and\nLP layouts). We also introduce a public dataset for end-to-end ALPR that is the\nfirst to contain images of vehicles with Mercosur LPs and the one with the\nhighest number of motorcycle images. The experimental results shed light on the\nlimitations of the traditional-split protocol for evaluating approaches in the\nALPR context, as there are significant drops in performance for most datasets\nwhen training and testing the models in a leave-one-dataset-out fashion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1\">Rayson Laroca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_E/0/1/0/all/0/1\">Everton V. Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucio_D/0/1/0/all/0/1\">Diego R. Lucio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estevam_V/0/1/0/all/0/1\">Valter Estevam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and High-Quality Image Denoising via Malleable Convolutions. (arXiv:2201.00392v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00392","description":"<p>Many image processing networks apply a single set of static convolutional\nkernels across the entire input image, which is sub-optimal for natural images,\nas they often consist of heterogeneous visual patterns. Recent works in\nclassification, segmentation, and image restoration have demonstrated that\ndynamic kernels outperform static kernels at modeling local image statistics.\nHowever, these works often adopt per-pixel convolution kernels, which introduce\nhigh memory and computation costs. To achieve spatial-varying processing\nwithout significant overhead, we present Malleable Convolution (MalleConv), as\nan efficient variant of dynamic convolution. The weights of MalleConv are\ndynamically produced by an efficient predictor network capable of generating\ncontent-dependent outputs at specific spatial locations. Unlike previous works,\nMalleConv generates a much smaller set of spatially-varying kernels from input,\nwhich enlarges the network's receptive field and significantly reduces\ncomputational and memory costs. These kernels are then applied to a\nfull-resolution feature map through an efficient slice-and-conv operator with\nminimum memory overhead. We further build an efficient denoising network using\nMalleConv, coined as MalleNet. It achieves high quality results without very\ndeep architecture, e.g., reaching 8.91x faster speed compared to the best\nperformed denoising algorithms (SwinIR), while maintaining similar performance.\nWe also show that a single MalleConv added to a standard convolution-based\nbackbone can contribute significantly to reducing the computational cost or\nboosting image quality at a similar cost. Project page:\nhttps://yifanjiang.net/MalleConv.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wronski_B/0/1/0/all/0/1\">Bart Wronski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mildenhall_B/0/1/0/all/0/1\">Ben Mildenhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jon Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Tianfan Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Open World Object Detection. (arXiv:2201.00471v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00471","description":"<p>Open World Object Detection (OWOD), simulating the real dynamic world where\nknowledge grows continuously, attempts to detect both known and unknown classes\nand incrementally learn the identified unknown ones. We find that although the\nonly previous OWOD work constructively puts forward to the OWOD definition, the\nexperimental settings are unreasonable with the illogical benchmark, confusing\nmetric calculation, and inappropriate method. In this paper, we rethink the\nOWOD experimental setting and propose five fundamental benchmark principles to\nguide the OWOD benchmark construction. Moreover, we design two fair evaluation\nprotocols specific to the OWOD problem, filling the void of evaluating from the\nperspective of unknown classes. Furthermore, we introduce a novel and effective\nOWOD framework containing an auxiliary Proposal ADvisor (PAD) and a\nClass-specific Expelling Classifier (CEC). The non-parametric PAD could assist\nthe RPN in identifying accurate unknown proposals without supervision, while\nCEC calibrates the over-confident activation boundary and filters out confusing\npredictions through a class-specific expelling function. Comprehensive\nexperiments conducted on our fair benchmark demonstrate that our method\noutperforms other state-of-the-art object detection approaches in terms of both\nexisting and our new metrics. Our benchmark and code are available at\nhttps://github.com/RE-OWOD/RE-OWOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaowei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yifan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yixuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuqing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Duorui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}