{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Unsupervised Personalization of an Emotion Recognition System: The Unique Properties of the Externalization of Valence in Speech. (arXiv:2201.07876v1 [cs.SD])","link":"http://arxiv.org/abs/2201.07876","description":"<p>The prediction of valence from speech is an important, but challenging\nproblem. The externalization of valence in speech has speaker-dependent cues,\nwhich contribute to performances that are often significantly lower than the\nprediction of other emotional attributes such as arousal and dominance. A\npractical approach to improve valence prediction from speech is to adapt the\nmodels to the target speakers in the test set. Adapting a speech emotion\nrecognition (SER) system to a particular speaker is a hard problem, especially\nwith deep neural networks (DNNs), since it requires optimizing millions of\nparameters. This study proposes an unsupervised approach to address this\nproblem by searching for speakers in the train set with similar acoustic\npatterns as the speaker in the test set. Speech samples from the selected\nspeakers are used to create the adaptation set. This approach leverages\ntransfer learning using pre-trained models, which are adapted with these speech\nsamples. We propose three alternative adaptation strategies: unique speaker,\noversampling and weighting approaches. These methods differ on the use of the\nadaptation set in the personalization of the valence models. The results\ndemonstrate that a valence prediction model can be efficiently personalized\nwith these unsupervised approaches, leading to relative improvements as high as\n13.52%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_K/0/1/0/all/0/1\">Kusha Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busso_C/0/1/0/all/0/1\">Carlos Busso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASL Video Corpora & Sign Bank: Resources Available through the American Sign Language Linguistic Research Project (ASLLRP). (arXiv:2201.07899v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07899","description":"<p>The American Sign Language Linguistic Research Project (ASLLRP) provides\nInternet access to high-quality ASL video data, generally including front and\nside views and a close-up of the face. The manual and non-manual components of\nthe signing have been linguistically annotated using SignStream(R). The\nrecently expanded video corpora can be browsed and searched through the Data\nAccess Interface (DAI 2) we have designed; it is possible to carry out complex\nsearches. The data from our corpora can also be downloaded; annotations are\navailable in an XML export format. We have also developed the ASLLRP Sign Bank,\nwhich contains almost 6,000 sign entries for lexical signs, with distinct\nEnglish-based glosses, with a total of 41,830 examples of lexical signs (in\naddition to about 300 gestures, over 1,000 fingerspelled signs, and 475\nclassifier examples). The Sign Bank is likewise accessible and searchable on\nthe Internet; it can also be accessed from within SignStream(R) (software to\nfacilitate linguistic annotation and analysis of visual language data) to make\nannotations more accurate and efficient. Here we describe the available\nresources. These data have been used for many types of research in linguistics\nand in computer-based sign language recognition from video; examples of such\nresearch are provided in the latter part of this article.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neidle_C/0/1/0/all/0/1\">Carol Neidle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opoku_A/0/1/0/all/0/1\">Augustine Opoku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Machine Common Sense via Cloze Testing. (arXiv:2201.07902v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07902","description":"<p>Language models (LMs) show state of the art performance for common sense (CS)\nquestion answering, but whether this ability implies a human-level mastery of\nCS remains an open question. Understanding the limitations and strengths of LMs\ncan help researchers improve these models, potentially by developing novel ways\nof integrating external CS knowledge. We devise a series of tests and\nmeasurements to systematically quantify their performance on different aspects\nof CS. We propose the use of cloze testing combined with word embeddings to\nmeasure the LM's robustness and confidence. Our results show than although\nlanguage models tend to achieve human-like accuracy, their confidence is\nsubpar. Future work can leverage this information to build more complex\nsystems, such as an ensemble of symbolic and distributed knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qasemi_E/0/1/0/all/0/1\">Ehsan Qasemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kezar_L/0/1/0/all/0/1\">Lee Kezar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szekely_P/0/1/0/all/0/1\">Pedro Szekely</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPTAM: Constituency Parse Tree Aggregation Method. (arXiv:2201.07905v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07905","description":"<p>Diverse Natural Language Processing tasks employ constituency parsing to\nunderstand the syntactic structure of a sentence according to a phrase\nstructure grammar. Many state-of-the-art constituency parsers are proposed, but\nthey may provide different results for the same sentences, especially for\ncorpora outside their training domains. This paper adopts the truth discovery\nidea to aggregate constituency parse trees from different parsers by estimating\ntheir reliability in the absence of ground truth. Our goal is to consistently\nobtain high-quality aggregated constituency parse trees. We formulate the\nconstituency parse tree aggregation problem in two steps, structure aggregation\nand constituent label aggregation. Specifically, we propose the first truth\ndiscovery solution for tree structures by minimizing the weighted sum of\nRobinson-Foulds (RF) distances, a classic symmetric distance metric between two\ntrees. Extensive experiments are conducted on benchmark datasets in different\nlanguages and domains. The experimental results show that our method, CPTAM,\noutperforms the state-of-the-art aggregation baselines. We also demonstrate\nthat the weights estimated by CPTAM can adequately evaluate constituency\nparsers in the absence of ground truth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Adithya Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabetpour_N/0/1/0/all/0/1\">Nasim Sabetpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markin_A/0/1/0/all/0/1\">Alexey Markin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eulenstein_O/0/1/0/all/0/1\">Oliver Eulenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis: Predicting Yelp Scores. (arXiv:2201.07999v1 [cs.LG])","link":"http://arxiv.org/abs/2201.07999","description":"<p>In this work, we predict the sentiment of restaurant reviews based on a\nsubset of the Yelp Open Dataset. We utilize the meta features and text\navailable in the dataset and evaluate several machine learning and\nstate-of-the-art deep learning approaches for the prediction task. Through\nseveral qualitative experiments, we show the success of the deep models with\nattention mechanism in learning a balanced model for reviews across different\nrestaurants. Finally, we propose a novel Multi-tasked joint BERT model that\nimproves the overall classification performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guda_B/0/1/0/all/0/1\">Bhanu Prakash Reddy Guda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1\">Mashrin Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karkhanis_D/0/1/0/all/0/1\">Deep Karkhanis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction of a Quality Estimation Dataset for Automatic Evaluation of Japanese Grammatical Error Correction. (arXiv:2201.08038v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08038","description":"<p>In grammatical error correction (GEC), automatic evaluation is an important\nfactor for research and development of GEC systems. Previous studies on\nautomatic evaluation have demonstrated that quality estimation models built\nfrom datasets with manual evaluation can achieve high performance in automatic\nevaluation of English GEC without using reference sentences.. However, quality\nestimation models have not yet been studied in Japanese, because there are no\ndatasets for constructing quality estimation models. Therefore, in this study,\nwe created a quality estimation dataset with manual evaluation to build an\nautomatic evaluation model for Japanese GEC. Moreover, we conducted a\nmeta-evaluation to verify the dataset's usefulness in building the Japanese\nquality estimation model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_D/0/1/0/all/0/1\">Daisuke Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_Y/0/1/0/all/0/1\">Yujin Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_I/0/1/0/all/0/1\">Ikumi Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aida_T/0/1/0/all/0/1\">Taichi Aida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirasawa_T/0/1/0/all/0/1\">Tosho Hirasawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakatsuji_M/0/1/0/all/0/1\">Michitaka Nakatsuji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mita_M/0/1/0/all/0/1\">Masato Mita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komachi_M/0/1/0/all/0/1\">Mamoru Komachi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VISA: An Ambiguous Subtitles Dataset for Visual Scene-Aware Machine Translation. (arXiv:2201.08054v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08054","description":"<p>Existing multimodal machine translation (MMT) datasets consist of images and\nvideo captions or general subtitles, which rarely contain linguistic ambiguity,\nmaking visual information not so effective to generate appropriate\ntranslations. We introduce VISA, a new dataset that consists of 40k\nJapanese-English parallel sentence pairs and corresponding video clips with the\nfollowing key features: (1) the parallel sentences are subtitles from movies\nand TV episodes; (2) the source subtitles are ambiguous, which means they have\nmultiple possible translations with different meanings; (3) we divide the\ndataset into Polysemy and Omission according to the cause of ambiguity. We show\nthat VISA is challenging for the latest MMT system, and we hope that the\ndataset can facilitate MMT research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yihang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimizu_S/0/1/0/all/0/1\">Shuichiro Shimizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Weiqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistically-driven Multi-task Pre-training for Low-resource Neural Machine Translation. (arXiv:2201.08070v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08070","description":"<p>In the present study, we propose novel sequence-to-sequence pre-training\nobjectives for low-resource machine translation (NMT): Japanese-specific\nsequence to sequence (JASS) for language pairs involving Japanese as the source\nor target language, and English-specific sequence to sequence (ENSS) for\nlanguage pairs involving English. JASS focuses on masking and reordering\nJapanese linguistic units known as bunsetsu, whereas ENSS is proposed based on\nphrase structure masking and reordering tasks. Experiments on ASPEC\nJapanese--English &amp; Japanese--Chinese, Wikipedia Japanese--Chinese, News\nEnglish--Korean corpora demonstrate that JASS and ENSS outperform MASS and\nother existing language-agnostic pre-training methods by up to +2.9 BLEU points\nfor the Japanese--English tasks, up to +7.0 BLEU points for the\nJapanese--Chinese tasks and up to +1.3 BLEU points for English--Korean tasks.\nEmpirical analysis, which focuses on the relationship between individual parts\nin JASS and ENSS, reveals the complementary nature of the subtasks of JASS and\nENSS. Adequacy evaluation using LASER, human evaluation, and case studies\nreveals that our proposed methods significantly outperform pre-training methods\nwithout injected linguistic knowledge and they have a larger positive impact on\nthe adequacy as compared to the fluency. We release codes here:\nhttps://github.com/Mao-KU/JASS/tree/master/linguistically-driven-pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Elements of Temporal Sentence Grounding in Videos: A Survey and Future Directions. (arXiv:2201.08071v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08071","description":"<p>Temporal sentence grounding in videos (TSGV), a.k.a., natural language video\nlocalization (NLVL) or video moment retrieval (VMR), aims to retrieve a\ntemporal moment that semantically corresponds to a language query from an\nuntrimmed video. Connecting computer vision and natural language, TSGV has\ndrawn significant attention from researchers in both communities. This survey\nattempts to provide a summary of fundamental concepts in TSGV and current\nresearch status, as well as future research directions. As the background, we\npresent a common structure of functional components in TSGV, in a tutorial\nstyle: from feature extraction from raw video and language query, to answer\nprediction of the target moment. Then we review the techniques for multimodal\nunderstanding and interaction, which is the key focus of TSGV for effective\nalignment between the two modalities. We construct a taxonomy of TSGV\ntechniques and elaborate methods in different categories with their strengths\nand weaknesses. Lastly, we discuss issues with the current TSGV research and\nshare our insights about promising research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_W/0/1/0/all/0/1\">Wei Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training. (arXiv:2201.08081v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08081","description":"<p>Language-based environment manipulation requires agents to manipulate the\nenvironment following natural language instructions, which is challenging due\nto the huge space of the environments. To address this challenge, various\napproaches have been proposed in recent work. Although these approaches work\nwell for their intended environments, they are difficult to generalize across\nenvironments. In this work, we propose LEMON, a general framework for\nlanguage-based environment manipulation tasks. Specifically, we first propose a\nunified approach to deal with various environments using the same generative\nlanguage model. Then we propose an execution-guided pre-training strategy to\ninject prior knowledge of environments to the language model with a pure\nsynthetic pre-training corpus. Experimental results on tasks including Alchemy,\nScene, Tangrams and ProPara demonstrate the effectiveness of LEMON: it achieves\nnew state-of-the-art results on Alchemy, Scene and ProPara, and the\nexecution-guided pre-training strategy brings remarkable improvements on all\nexperimental tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Did You Not Compare With That? Identifying Papers for Use as Baselines. (arXiv:2201.08089v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08089","description":"<p>We propose the task of automatically identifying papers used as baselines in\na scientific article. We frame the problem as a binary classification task\nwhere all the references in a paper are to be classified as either baselines or\nnon-baselines. This is a challenging problem due to the numerous ways in which\na baseline reference can appear in a paper. We develop a dataset of $2,075$\npapers from ACL anthology corpus with all their references manually annotated\nas one of the two classes. We develop a multi-module attention-based neural\nclassifier for the baseline classification task that outperforms four\nstate-of-the-art citation role classification methods when applied to the\nbaseline classification task. We also present an analysis of the errors made by\nthe proposed classifier, eliciting the challenges that make baseline\nidentification a challenging problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bedi_M/0/1/0/all/0/1\">Manjot Bedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_T/0/1/0/all/0/1\">Tanisha Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Sumit Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Question Answering Leaderboard: A Community Resource to Prevent a Replication Crisis. (arXiv:2201.08174v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08174","description":"<p>Data-driven systems need to be evaluated to establish trust in the scientific\napproach and its applicability. In particular, this is true for Knowledge Graph\n(KG) Question Answering (QA), where complex data structures are made accessible\nvia natural-language interfaces. Evaluating the capabilities of these systems\nhas been a driver for the community for more than ten years while establishing\ndifferent KGQA benchmark datasets. However, comparing different approaches is\ncumbersome. The lack of existing and curated leaderboards leads to a missing\nglobal view over the research field and could inject mistrust into the results.\nIn particular, the latest and most-used datasets in the KGQA community, LC-QuAD\nand QALD, miss providing central and up-to-date points of trust. In this paper,\nwe survey and analyze a wide range of evaluation results with significant\ncoverage of 100 publications and 98 systems from the last decade. We provide a\nnew central and open leaderboard for any KGQA benchmark dataset as a focal\npoint for the community - https://kgqa.github.io/leaderboard. Our analysis\nhighlights existing problems during the evaluation of KGQA systems. Thus, we\nwill point to possible improvements for future evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perevalov_A/0/1/0/all/0/1\">Aleksandr Perevalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovriguina_L/0/1/0/all/0/1\">Liubov Kovriguina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Longquan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Both_A/0/1/0/all/0/1\">Andreas Both</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-based Hybrid Local Search for the Hard-label Textual Attack. (arXiv:2201.08193v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08193","description":"<p>Deep neural networks are vulnerable to adversarial examples in Natural\nLanguage Processing. However, existing textual adversarial attacks usually\nutilize the gradient or prediction confidence to generate adversarial examples,\nmaking it hard to be deployed in real-world applications. To this end, we\nconsider a rarely investigated but more rigorous setting, namely hard-label\nattack, in which the attacker could only access the prediction label. In\nparticular, we find that the changes on prediction label caused by word\nsubstitutions on the adversarial example could precisely reflect the importance\nof different words. Based on this observation, we propose a novel hard-label\nattack, called Learning-based Hybrid Local Search (LHLS) algorithm, which\neffectively estimates word importance with the prediction label from the attack\nhistory and integrate such information into hybrid local search algorithm to\noptimize the adversarial perturbation. Extensive evaluations for text\nclassification and textual entailment using various datasets and models show\nthat our LHLS significantly outperforms existing hard-label attacks regarding\nthe attack performance as well as adversary quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Latent-Variable Model for Intrinsic Probing. (arXiv:2201.08214v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08214","description":"<p>The success of pre-trained contextualized representations has prompted\nresearchers to analyze them for the presence of linguistic information. Indeed,\nit is natural to assume that these pre-trained representations do encode some\nlevel of linguistic knowledge as they have brought about large empirical\nimprovements on a wide variety of NLP tasks, which suggests they are learning\ntrue linguistic generalization. In this work, we focus on intrinsic probing, an\nanalysis technique where the goal is not only to identify whether a\nrepresentation encodes a linguistic attribute, but also to pinpoint where this\nattribute is encoded. We propose a novel latent-variable formulation for\nconstructing intrinsic probes and derive a tractable variational approximation\nto the log-likelihood. Our results show that our model is versatile and yields\ntighter mutual information estimates than two intrinsic probes previously\nproposed in the literature. Finally, we find empirical evidence that\npre-trained representations develop a cross-lingually entangled notion of\nmorphosyntax.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanczak_K/0/1/0/all/0/1\">Karolina Sta&#x144;czak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1\">Lucas Torroba Hennigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaMDA: Language Models for Dialog Applications. (arXiv:2201.08239v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08239","description":"<p>We present LaMDA: Language Models for Dialog Applications. LaMDA is a family\nof Transformer-based neural language models specialized for dialog, which have\nup to 137B parameters and are pre-trained on 1.56T words of public dialog data\nand web text. While model scaling alone can improve quality, it shows less\nimprovements on safety and factual grounding. We demonstrate that fine-tuning\nwith annotated data and enabling the model to consult external knowledge\nsources can lead to significant improvements towards the two key challenges of\nsafety and factual grounding. The first challenge, safety, involves ensuring\nthat the model's responses are consistent with a set of human values, such as\npreventing harmful suggestions and unfair bias. We quantify safety using a\nmetric based on an illustrative set of human values, and we find that filtering\ncandidate responses using a LaMDA classifier fine-tuned with a small amount of\ncrowdworker-annotated data offers a promising approach to improving model\nsafety. The second challenge, factual grounding, involves enabling the model to\nconsult external knowledge sources, such as an information retrieval system, a\nlanguage translator, and a calculator. We quantify factuality using a\ngroundedness metric, and we find that our approach enables the model to\ngenerate responses grounded in known sources, rather than responses that merely\nsound plausible. Finally, we explore the use of LaMDA in the domains of\neducation and content recommendations, and analyze their helpfulness and role\nconsistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thoppilan_R/0/1/0/all/0/1\">Romal Thoppilan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_D/0/1/0/all/0/1\">Daniel De Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1\">Jamie Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulshreshtha_A/0/1/0/all/0/1\">Apoorv Kulshreshtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Tze Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_A/0/1/0/all/0/1\">Alicia Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bos_T/0/1/0/all/0/1\">Taylor Bos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_L/0/1/0/all/0/1\">Leslie Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">YaGuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hongrae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huaixiu Steven Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghafouri_A/0/1/0/all/0/1\">Amin Ghafouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menegali_M/0/1/0/all/0/1\">Marcelo Menegali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1\">Maxim Krikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepikhin_D/0/1/0/all/0/1\">Dmitry Lepikhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dehao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chung-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krivokon_I/0/1/0/all/0/1\">Igor Krivokon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusch_W/0/1/0/all/0/1\">Will Rusch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pickett_M/0/1/0/all/0/1\">Marc Pickett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_Hellstern_K/0/1/0/all/0/1\">Kathleen Meier-Hellstern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1\">Meredith Ringel Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_T/0/1/0/all/0/1\">Tulsee Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_R/0/1/0/all/0/1\">Renelito Delos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duke_T/0/1/0/all/0/1\">Toju Duke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soraker_J/0/1/0/all/0/1\">Johnny Soraker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zevenbergen_B/0/1/0/all/0/1\">Ben Zevenbergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Mark Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_B/0/1/0/all/0/1\">Ben Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olson_K/0/1/0/all/0/1\">Kristen Olson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molina_A/0/1/0/all/0/1\">Alejandra Molina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_John_E/0/1/0/all/0/1\">Erin Hoffman-John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Josh Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aroyo_L/0/1/0/all/0/1\">Lora Aroyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajakumar_R/0/1/0/all/0/1\">Ravi Rajakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butryna_A/0/1/0/all/0/1\">Alena Butryna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamm_M/0/1/0/all/0/1\">Matthew Lamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzmina_V/0/1/0/all/0/1\">Viktoriya Kuzmina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fenton_J/0/1/0/all/0/1\">Joe Fenton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Aaron Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_R/0/1/0/all/0/1\">Rachel Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurzweil_R/0/1/0/all/0/1\">Ray Kurzweil</a>, et al. (5 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Generative Pretraining for Multimodal Video Captioning. (arXiv:2201.08264v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08264","description":"<p>Recent video and language pretraining frameworks lack the ability to generate\nsentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new\npretraining framework for learning from unlabelled videos which can be\neffectively used for generative tasks such as multimodal video captioning.\nUnlike recent video-language pretraining frameworks, our framework trains both\na multimodal video encoder and a sentence decoder jointly. To overcome the lack\nof captions in unlabelled videos, we leverage the future utterance as an\nadditional text source and propose a bidirectional generation objective -- we\ngenerate future utterances given the present mulitmodal context, and also the\npresent utterance given future observations. With this objective, we train an\nencoder-decoder model end-to-end to generate a caption from raw pixels and\ntranscribed speech directly. Our model achieves state-of-the-art performance\nfor multimodal video captioning on four standard benchmarks, as well as for\nother video understanding tasks such as VideoQA, video retrieval and action\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_P/0/1/0/all/0/1\">Paul Hongsuck Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis. (arXiv:2201.08277v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08277","description":"<p>Sentiment analysis is one of the most widely studied applications in NLP, but\nmost work focuses on languages with large amounts of data. We introduce the\nfirst large-scale human-annotated Twitter sentiment dataset for the four most\nwidely spoken languages in Nigeria (Hausa, Igbo, Nigerian-Pidgin, and Yoruba)\nconsisting of around 30,000 annotated tweets per language (except for\nNigerian-Pidgin), including a significant fraction of code-mixed tweets. We\npropose text collection, filtering, processing, and labelling methods that\nenable us to create datasets for these low-resource languages. We evaluate a\nrange of pre-trained models and transfer strategies on the dataset. We find\nthat language-specific models and language-adaptive fine-tuning generally\nperform best. We release the datasets, trained models, sentiment lexicons, and\ncode to incentivize research on sentiment analysis in under-represented\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Ibrahim Said Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_B/0/1/0/all/0/1\">Bello Shehu Bello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aremu_A/0/1/0/all/0/1\">Anuoluwapo Aremu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_S/0/1/0/all/0/1\">Saheed Abdul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brazdil_P/0/1/0/all/0/1\">Pavel Brazdil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs. (arXiv:2201.08318v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08318","description":"<p>Automatic grading models are valued for the time and effort saved during the\ninstruction of large student bodies. Especially with the increasing\ndigitization of education and interest in large-scale standardized testing, the\npopularity of automatic grading has risen to the point where commercial\nsolutions are widely available and used. However, for short answer formats,\nautomatic grading is challenging due to natural language ambiguity and\nversatility. While automatic short answer grading models are beginning to\ncompare to human performance on some datasets, their robustness, especially to\nadversarially manipulated data, is questionable. Exploitable vulnerabilities in\ngrading models can have far-reaching consequences ranging from cheating\nstudents receiving undeserved credit to undermining automatic grading\naltogether - even when most predictions are valid. In this paper, we devise a\nblack-box adversarial attack tailored to the educational short answer grading\nscenario to investigate the grading models' robustness. In our attack, we\ninsert adjectives and adverbs into natural places of incorrect student answers,\nfooling the model into predicting them as correct. We observed a loss of\nprediction accuracy between 10 and 22 percentage points using the\nstate-of-the-art models BERT and T5. While our attack made answers appear less\nnatural to humans in our experiments, it did not significantly increase the\ngraders' suspicions of cheating. Based on our experiments, we provide\nrecommendations for utilizing automatic grading systems more safely in\npractice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Filighera_A/0/1/0/all/0/1\">Anna Filighera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochs_S/0/1/0/all/0/1\">Sebastian Ochs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steuer_T/0/1/0/all/0/1\">Tim Steuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tregel_T/0/1/0/all/0/1\">Thomas Tregel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signature Entrenchment and Conceptual Changes in Automated Theory Repair. (arXiv:2201.08340v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08340","description":"<p>Human beliefs change, but so do the concepts that underpin them. The recent\nAbduction, Belief Revision and Conceptual Change (ABC) repair system combines\nseveral methods from automated theory repair to expand, contract, or reform\nlogical structures representing conceptual knowledge in artificial agents. In\nthis paper we focus on conceptual change: repair not only of the membership of\nlogical concepts, such as what animals can fly, but also concepts themselves,\nsuch that birds may be divided into flightless and flying birds, by changing\nthe signature of the logical theory used to represent them. We offer a method\nfor automatically evaluating entrenchment in the signature of a Datalog theory,\nin order to constrain automated theory repair to succinct and intuitive\noutcomes. Formally, signature entrenchment measures the inferential\ncontributions of every logical language element used to express conceptual\nknowledge, i.e., predicates and the arguments, ranking possible repairs to\nretain valuable logical concepts and reject redundant or implausible\nalternatives. This quantitative measurement of signature entrenchment offers a\nguide to the plausibility of conceptual changes, which we aim to contrast with\nhuman judgements of concept entrenchment in future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bundy_A/0/1/0/all/0/1\">Alan Bundy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philalithis_E/0/1/0/all/0/1\">Eugene Philalithis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Model to Measure the Spread Power of Rumors. (arXiv:2002.07563v4 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2002.07563","description":"<p>With technologies that have democratized the production and reproduction of\ninformation, a significant portion of daily interacted posts in social media\nhas been infected by rumors. Despite the extensive research on rumor detection\nand verification, so far, the problem of calculating the spread power of rumors\nhas not been considered. To address this research gap, the present study seeks\na model to calculate the Spread Power of Rumor (SPR) as the function of\ncontent-based features in two categories: False Rumor (FR) and True Rumor (TR).\nFor this purpose, the theory of Allport and Postman will be adopted, which it\nclaims that importance and ambiguity are the key variables in rumor-mongering\nand the power of rumor. Totally 42 content features in two categories\n\"importance\" (28 features) and \"ambiguity\" (14 features) are introduced to\ncompute SPR. The proposed model is evaluated on two datasets, Twitter and\nTelegram. The results showed that (i) the spread power of False Rumor documents\nis rarely more than True Rumors. (ii) there is a significant difference between\nthe SPR means of two groups False Rumor and True Rumor. (iii) SPR as a\ncriterion can have a positive impact on distinguishing False Rumors and True\nRumors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahanbakhsh_Nagadeh_Z/0/1/0/all/0/1\">Zoleikha Jahanbakhsh-Nagadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Majid Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahkar_Farshi_T/0/1/0/all/0/1\">Taymaz Rahkar-Farshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgari_Chenaghlu_M/0/1/0/all/0/1\">Meysam Asgari-Chenaghlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikzad_Khasmakhi_N/0/1/0/all/0/1\">Narjes Nikzad-Khasmakhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_A/0/1/0/all/0/1\">Ali-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjbar_Khadivi_M/0/1/0/all/0/1\">Mehrdad Ranjbar-Khadivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafarani_Moattar_E/0/1/0/all/0/1\">Elnaz Zafarani-Moattar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1\">Mohammad-Ali Balafar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Hyperbolic Geometry for FG-NET over Distantly Supervised data. (arXiv:2101.11212v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.11212","description":"<p>Fine-Grained Named Entity Typing (\\FGNET{}) classifies an entity mention into\na fine range of entity types. A large number of entity types make it difficult\nto manually label the training data, thus distant supervision is used to\nautomatically acquire the training data. Distant supervision incurs a lot of\ntraining noise which hinders the performance improvement of the FG-NET systems.\nIn this paper, we propose to use hyperbolic geometry for FG-NET with the hope\nthat it can help overcoming the noise incurred by distant supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Muhammad Asif Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Goldilocks: Just-Right Tuning of BERT for Technology-Assisted Review. (arXiv:2105.01044v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2105.01044","description":"<p>Technology-assisted review (TAR) refers to iterative active learning\nworkflows for document review in high recall retrieval (HRR) tasks. TAR\nresearch and most commercial TAR software have applied linear models such as\nlogistic regression to lexical features. Transformer-based models with\nsupervised tuning are known to improve effectiveness on many text\nclassification tasks, suggesting their use in TAR. We indeed find that the\npre-trained BERT model reduces review cost by 10% to 15% in TAR workflows\nsimulated on the RCV1-v2 newswire collection. In contrast, we likewise\ndetermined that linear models outperform BERT for simulated legal discovery\ntopics on the Jeb Bush e-mail collection. This suggests the match between\ntransformer pre-training corpora and the task domain is of greater significance\nthan generally appreciated. Additionally, we show that just-right language\nmodel fine-tuning on the task collection before starting active learning is\ncritical. Too little or too much fine-tuning hinders performance, worse than\nthat of linear models, even for a favorable corpus such as RCV1-v2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eugene Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacAvaney_S/0/1/0/all/0/1\">Sean MacAvaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_D/0/1/0/all/0/1\">David D. Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieder_O/0/1/0/all/0/1\">Ophir Frieder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Task-Oriented Dialog Modeling with Semi-Structured Knowledge Management. (arXiv:2106.11796v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.11796","description":"<p>Current task-oriented dialog (TOD) systems mostly manage structured knowledge\n(e.g. databases and tables) to guide the goal-oriented conversations. However,\nthey fall short of handling dialogs which also involve unstructured knowledge\n(e.g. reviews and documents). In this paper, we formulate a task of modeling\nTOD grounded on a fusion of structured and unstructured knowledge. To address\nthis task, we propose a TOD system with semi-structured knowledge management,\nSeKnow, which extends the belief state to manage knowledge with both structured\nand unstructured contents. Furthermore, we introduce two implementations of\nSeKnow based on a non-pretrained sequence-to-sequence model and a pretrained\nlanguage model, respectively. Both implementations use the end-to-end manner to\njointly optimize dialog modeling grounded on structured and unstructured\nknowledge. We conduct experiments on a modified version of MultiWOZ 2.1\ndataset, Mod-MultiWOZ 2.1, where dialogs are processed to involve\nsemi-structured knowledge. Experimental results show that SeKnow has strong\nperformances in both end-to-end dialog and intermediate knowledge management,\ncompared to existing TOD systems and their extensions with pipeline knowledge\nmanagement schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Silin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1\">Ryuichi Takanobu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing task-oriented and open-domain dialogues in conversational agents. (arXiv:2109.04137v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04137","description":"<p>The goal of building intelligent dialogue systems has largely been\n\\textit{separately} pursued under two paradigms: task-oriented dialogue (TOD)\nsystems, which perform goal-oriented functions, and open-domain dialogue (ODD)\nsystems, which focus on non-goal-oriented chitchat. The two dialogue modes can\npotentially be intertwined together seamlessly in the same conversation, as\neasily done by a friendly human assistant. Such ability is desirable in\nconversational agents, as the integration makes them more accessible and\nuseful. Our paper addresses this problem of fusing TODs and ODDs in multi-turn\ndialogues. Based on the popular TOD dataset MultiWOZ, we build a new dataset\nFusedChat, by rewriting the existing TOD turns and adding new ODD turns. This\nprocedure constructs conversation sessions containing exchanges from both\ndialogue modes. It features inter-mode contextual dependency, i.e., the\ndialogue turns from the two modes depend on each other. Rich dependency\npatterns including co-reference and ellipsis are features. The new dataset,\nwith 60k new human-written ODD turns and 5k re-written TOD turns, offers a\nbenchmark to test a dialogue model's ability to perform inter-mode\nconversations. This is a more challenging task since the model has to determine\nthe appropriate dialogue mode and generate the response based on the inter-mode\ncontext. But such models would better mimic human-level conversation\ncapabilities. We evaluate baseline models on this task, including\n\\textit{classification-based} two-stage models and \\textit{two-in-one} fused\nmodels. We publicly release FusedChat and the baselines to propel future work\non inter-mode dialogue systems https://github.com/tomyoung903/FusedChat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Young_T/0/1/0/all/0/1\">Tom Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Frank Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandelea_V/0/1/0/all/0/1\">Vlad Pandelea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jinjie Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can depth-adaptive BERT perform better on binary classification tasks. (arXiv:2111.10951v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10951","description":"<p>In light of the success of transferring language models into NLP tasks, we\nask whether the full BERT model is always the best and does it exist a simple\nbut effective method to find the winning ticket in state-of-the-art deep neural\nnetworks without complex calculations. We construct a series of BERT-based\nmodels with different size and compare their predictions on 8 binary\nclassification tasks. The results show there truly exist smaller sub-networks\nperforming better than the full model. Then we present a further study and\npropose a simple method to shrink BERT appropriately before fine-tuning. Some\nextended experiments indicate that our method could save time and storage\noverhead extraordinarily with little even no accuracy loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lixiang Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models. (arXiv:2201.05966v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05966","description":"<p>Structured knowledge grounding (SKG) leverages structured knowledge to\ncomplete user requests, such as semantic parsing over databases and question\nanswering over knowledge bases. Since the inputs and outputs of SKG tasks are\nheterogeneous, they have been studied separately by different communities,\nwhich limits systematic and compatible research on SKG. In this paper, we\novercome this limitation by proposing the SKG framework, which unifies 21 SKG\ntasks into a text-to-text format, aiming to promote systematic SKG research,\ninstead of being exclusive to a single task, domain, or dataset. We use\nUnifiedSKG to benchmark T5 with different sizes and show that T5, with simple\nmodifications when necessary, achieves state-of-the-art performance on almost\nall of the 21 tasks. We further demonstrate that multi-task prefix-tuning\nimproves the performance on most tasks, largely improving the overall\nperformance. UnifiedSKG also facilitates the investigation of zero-shot and\nfew-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot\nand few-shot learning for SKG. We also use UnifiedSKG to conduct a series of\ncontrolled experiments on structured knowledge encoding variants across SKG\ntasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at\nhttps://github.com/hkunlp/unifiedskg Latest collections at\nhttps://unifiedskg.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianbao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Henry Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholak_T/0/1/0/all/0/1\">Torsten Scholak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida I. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1\">Victor Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengzu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyle_C/0/1/0/all/0/1\">Connor Boyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Ziyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Neural Machine Translation by Denoising Training. (arXiv:2201.07365v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.07365","description":"<p>We present a simple and effective pretraining strategy {D}en{o}ising\n{T}raining DoT for neural machine translation. Specifically, we update the\nmodel parameters with source- and target-side denoising tasks at the early\nstage and then tune the model normally. Notably, our approach does not increase\nany parameters or training steps, requiring the parallel data merely.\nExperiments show that DoT consistently improves the neural machine translation\nperformance across 12 bilingual and 16 multilingual directions (data size\nranges from 80K to 20M). In addition, we show that DoT can complement existing\ndata manipulation strategies, i.e. curriculum learning, knowledge distillation,\ndata diversification, bidirectional training, and back-translation.\nEncouragingly, we found that DoT outperforms costly pretrained model mBART in\nhigh-resource settings. Analyses show DoT is a novel in-domain cross-lingual\npretraining strategy and could offer further improvements with task-relevant\nself-supervisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Keqin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Many Ways to be Lonely: Fine-grained Characterization of Loneliness and its Potential Changes in COVID-19. (arXiv:2201.07423v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.07423","description":"<p>Loneliness has been associated with negative outcomes for physical and mental\nhealth. Understanding how people express and cope with various forms of\nloneliness is critical for early screening and targeted interventions to reduce\nloneliness, particularly among vulnerable groups such as young adults. To\nexamine how different forms of loneliness and coping strategies manifest in\nloneliness self-disclosure, we built a dataset, FIG-Loneliness (FIne-Grained\nLoneliness) by using Reddit posts in two young adult-focused forums and two\nloneliness related forums consisting of a diverse age group. We provide\nannotations by trained human annotators for binary and fine-grained loneliness\nclassifications of the posts. Trained on FIG-Loneliness, two BERT-based models\nwere used to understand loneliness forms and authors' coping strategies in\nthese forums. Our binary loneliness classification archived an accuracy above\n97%, and fine-grained loneliness category classification reached an average\naccuracy of 77% across all labeled categories. With FIG-Loneliness and model\npredictions, we found that loneliness expressions in the young adult related\nforums are distinct from other forums. Those in young adult-focused forums are\nmore likely to express concerns pertaining to peer relationship, and are\npotentially more sensitive to geographical isolation impacted by the COVID-19\npandemic lockdown. Also, we show that different forms of loneliness have\ndifferential use in coping strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yueyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yunfan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leqi_L/0/1/0/all/0/1\">Liu Leqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkielman_P/0/1/0/all/0/1\">Piotr Winkielman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"BLINC: Lightweight Bimodal Learning for Low-Complexity VVC Intra Coding. (arXiv:2201.07823v1 [cs.MM])","link":"http://arxiv.org/abs/2201.07823","description":"<p>The latest video coding standard, Versatile Video Coding (VVC), achieves\nalmost twice coding efficiency compared to its predecessor, the High Efficiency\nVideo Coding (HEVC). However, achieving this efficiency (for intra coding)\nrequires 31x computational complexity compared to HEVC, making it challenging\nfor low power and real-time applications. This paper, proposes a novel machine\nlearning approach that jointly and separately employs two modalities of\nfeatures, to simplify the intra coding decision. First a set of features are\nextracted that use the existing DCT core of VVC, to assess the texture\ncharacteristics, and forms the first modality of data. This produces high\nquality features with almost no overhead. The distribution of intra modes at\nthe neighboring blocks is also used to form the second modality of data, which\nprovides statistical information about the frame. Second, a two-step feature\nreduction method is designed that reduces the size of feature set, such that a\nlightweight model with a limited number of parameters can be used to learn the\nintra mode decision task. Third, three separate training strategies are\nproposed (1) an offline training strategy using the first (single) modality of\ndata, (2) an online training strategy that uses the second (single) modality,\nand (3) a mixed online-offline strategy that uses bimodal learning. Finally, a\nlow-complexity encoding algorithms is proposed based on the proposed learning\nstrategies. Extensive experimental results show that the proposed methods can\nreduce up to 24% of encoding time, with a negligible loss of coding efficiency.\nMoreover, it is demonstrated how a bimodal learning strategy can boost the\nperformance of learning. Lastly, the proposed method has a very low\ncomputational overhead (0.2%), and uses existing components of a VVC encoder,\nwhich makes it much more practical compared to competing solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pakdaman_F/0/1/0/all/0/1\">Farhad Pakdaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelimanesh_M/0/1/0/all/0/1\">Mohammad Ali Adelimanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_M/0/1/0/all/0/1\">Mahmoud Reza Hashemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROS georegistration: Aerial Multi-spectral Image Simulator for the Robot Operating System. (arXiv:2201.07863v1 [cs.RO])","link":"http://arxiv.org/abs/2201.07863","description":"<p>This article describes a software package called ROS georegistration intended\nfor use with the Robot Operating System (ROS) and the Gazebo 3D simulation\nenvironment. ROSgeoregistration provides tools for the simulation, test and\ndeployment of aerial georegistration algorithms and is made available with a\nlink provided in the paper. A model creation package is provided which\ndownloads multi-spectral images from the Google Earth Engine database and, if\nnecessary, incorporates these images into a single, possibly very large,\nreference image. Additionally a Gazebo plugin which uses the real-time sensor\npose and image formation model to generate simulated imagery using the\nspecified reference image is provided along with related plugins for UAV\nrelevant data. The novelty of this work is threefold: (1) this is the first\nsystem to link the massive multi-spectral imaging database of Google's Earth\nEngine to the Gazebo simulator, (2) this is the first example of a system that\ncan simulate geospatially and radiometrically accurate imagery from multiple\nsensor views of the same terrain region, and (3) integration with other UAS\ntools creates a new holistic UAS simulation environment to support UAS system\nand subsystem development where real-world testing would generally be\nprohibitive. Sensed imagery and ground truth registration information is\npublished to client applications which can receive imagery synchronously with\ntelemetry from other payload sensors, e.g., IMU, GPS/GNSS, barometer, and\nwindspeed sensor data. To highlight functionality, we demonstrate\nROSgeoregistration for simulating Electro-Optical (EO) and Synthetic Aperture\nRadar (SAR) image sensors and an example use case for developing and evaluating\nimage-based UAS position feedback, i.e., pose for image-based Guidance\nNavigation and Control (GNC) applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Willis_A/0/1/0/all/0/1\">Andrew R. Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brink_K/0/1/0/all/0/1\">Kevin Brink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dipple_K/0/1/0/all/0/1\">Kathleen Dipple</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Automated Robotic Arm: A Machine Learning Approach. (arXiv:2201.07882v1 [cs.RO])","link":"http://arxiv.org/abs/2201.07882","description":"<p>The term robot generally refers to a machine that looks and works in a way\nsimilar to a human. The modern industry is rapidly shifting from manual control\nof systems to automation, in order to increase productivity and to deliver\nquality products. Computer-based systems, though feasible for improving quality\nand productivity, are inflexible to work with, and the cost of such systems is\nsignificantly high. This led to the swift adoption of automated systems to\nperform industrial tasks. One such task of industrial significance is of\npicking and placing objects from one place to another. The implementation of\nautomation in pick and place tasks helps to improve efficiency of system and\nalso the performance. In this paper, we propose to demonstrate the designing\nand working of an automated robotic arm with the Machine Learning approach. The\nwork uses Machine Learning approach for object identification detection and\ntraversal, which is adopted with Tensor flow package for better and accurate\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_K/0/1/0/all/0/1\">Krishnaraj Rao N S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+J_A/0/1/0/all/0/1\">Avinash N J</a>, <a href=\"http://arxiv.org/find/cs/1/au:+H_R/0/1/0/all/0/1\">Rama Moorthy H</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Karthik K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Sudesh Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1\">Santosh S</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Networks for Spherical Signal Processing via Spherical Haar Tight Framelets. (arXiv:2201.07890v1 [eess.SP])","link":"http://arxiv.org/abs/2201.07890","description":"<p>In this paper, we develop a general theoretical framework for constructing\nHaar-type tight framelets on any compact set with a hierarchical partition. In\nparticular, we construct a novel area-regular hierarchical partition on the\n2-sphere and establish its corresponding spherical Haar tight framelets with\ndirectionality. We conclude by evaluating and illustrating the effectiveness of\nour area-regular spherical Haar tight framelets in several denoising\nexperiments. Furthermore, we propose a convolutional neural network (CNN) model\nfor spherical signal denoising which employs the fast framelet decomposition\nand reconstruction algorithms. Experiment results show that our proposed CNN\nmodel outperforms threshold methods, and processes strong generalization and\nrobustness properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jianfei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_H/0/1/0/all/0/1\">Han Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiaosheng Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Homogenization of Existing Inertial-Based Datasets to Support Human Activity Recognition. (arXiv:2201.07891v1 [eess.SP])","link":"http://arxiv.org/abs/2201.07891","description":"<p>Several techniques have been proposed to address the problem of recognizing\nactivities of daily living from signals. Deep learning techniques applied to\ninertial signals have proven to be effective, achieving significant\nclassification accuracy. Recently, research in human activity recognition (HAR)\nmodels has been almost totally model-centric. It has been proven that the\nnumber of training samples and their quality are critical for obtaining deep\nlearning models that both perform well independently of their architecture, and\nthat are more robust to intraclass variability and interclass similarity.\nUnfortunately, publicly available datasets do not always contain hight quality\ndata and a sufficiently large and diverse number of samples (e.g., number of\nsubjects, type of activity performed, and duration of trials). Furthermore,\ndatasets are heterogeneous among them and therefore cannot be trivially\ncombined to obtain a larger set. The final aim of our work is the definition\nand implementation of a platform that integrates datasets of inertial signals\nin order to make available to the scientific community large datasets of\nhomogeneous signals, enriched, when possible, with context information (e.g.,\ncharacteristics of the subjects and device position). The main focus of our\nplatform is to emphasise data quality, which is essential for training\nefficient models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Amrani_H/0/1/0/all/0/1\">Hamza Amrani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Micucci_D/0/1/0/all/0/1\">Daniela Micucci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mobilio_M/0/1/0/all/0/1\">Marco Mobilio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Napoletano_P/0/1/0/all/0/1\">Paolo Napoletano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Performance of Pre-Trained Networks by Matched Augmentation Distributions. (arXiv:2201.07894v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07894","description":"<p>There exists a distribution discrepancy between training and testing, in the\nway images are fed to modern CNNs. Recent work tried to bridge this gap either\nby fine-tuning or re-training the network at different resolutions. However\nre-training a network is rarely cheap and not always viable. To this end, we\npropose a simple solution to address the train-test distributional shift and\nenhance the performance of pre-trained models -- which commonly ship as a\npackage with deep learning platforms \\eg, PyTorch. Specifically, we demonstrate\nthat running inference on the center crop of an image is not always the best as\nimportant discriminatory information may be cropped-off. Instead we propose to\ncombine results for multiple random crops for a test image. This not only\nmatches the train time augmentation but also provides the full coverage of the\ninput image. We explore combining representation of random crops through\naveraging at different levels \\ie, deep feature level, logit level, and softmax\nlevel. We demonstrate that, for various families of modern deep networks, such\naveraging results in better validation accuracy compared to using a single\ncentral crop per image. The softmax averaging results in the best performance\nfor various pre-trained networks without requiring any re-training or\nfine-tuning whatsoever. On modern GPUs with batch processing, the paper's\napproach to inference of pre-trained networks, is essentially free as all\nimages in a batch can all be processed at once.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Touqeer Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafarzadeh_M/0/1/0/all/0/1\">Mohsen Jafarzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamija_A/0/1/0/all/0/1\">Akshay Raj Dhamija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabinowitz_R/0/1/0/all/0/1\">Ryan Rabinowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_S/0/1/0/all/0/1\">Steve Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunchun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boult_T/0/1/0/all/0/1\">Terrance E. Boult</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASL Video Corpora & Sign Bank: Resources Available through the American Sign Language Linguistic Research Project (ASLLRP). (arXiv:2201.07899v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07899","description":"<p>The American Sign Language Linguistic Research Project (ASLLRP) provides\nInternet access to high-quality ASL video data, generally including front and\nside views and a close-up of the face. The manual and non-manual components of\nthe signing have been linguistically annotated using SignStream(R). The\nrecently expanded video corpora can be browsed and searched through the Data\nAccess Interface (DAI 2) we have designed; it is possible to carry out complex\nsearches. The data from our corpora can also be downloaded; annotations are\navailable in an XML export format. We have also developed the ASLLRP Sign Bank,\nwhich contains almost 6,000 sign entries for lexical signs, with distinct\nEnglish-based glosses, with a total of 41,830 examples of lexical signs (in\naddition to about 300 gestures, over 1,000 fingerspelled signs, and 475\nclassifier examples). The Sign Bank is likewise accessible and searchable on\nthe Internet; it can also be accessed from within SignStream(R) (software to\nfacilitate linguistic annotation and analysis of visual language data) to make\nannotations more accurate and efficient. Here we describe the available\nresources. These data have been used for many types of research in linguistics\nand in computer-based sign language recognition from video; examples of such\nresearch are provided in the latter part of this article.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neidle_C/0/1/0/all/0/1\">Carol Neidle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opoku_A/0/1/0/all/0/1\">Augustine Opoku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of Facial Expressions and Emotion in ASL. (arXiv:2201.07906v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07906","description":"<p>There is little prior work on quantifying the relationships between facial\nexpressions and emotionality in American Sign Language. In this final report,\nwe provide two methods for studying these relationships through probability and\nprediction. Using a large corpus of natural signing manually annotated with\nfacial features paired with lexical emotion datasets, we find that there exist\nmany relationships between emotionality and the face, and that a simple\nclassifier can predict what someone is saying in terms of broad emotional\ncategories only by looking at the face.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kezar_L/0/1/0/all/0/1\">Lee Kezar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-by-Novel-View-Synthesis for Full-Face Appearance-based 3D Gaze Estimation. (arXiv:2201.07927v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07927","description":"<p>Despite recent advances in appearance-based gaze estimation techniques, the\nneed for training data that covers the target head pose and gaze distribution\nremains a crucial challenge for practical deployment. This work examines a\nnovel approach for synthesizing gaze estimation training data based on\nmonocular 3D face reconstruction. Unlike prior works using multi-view\nreconstruction, photo-realistic CG models, or generative neural networks, our\napproach can manipulate and extend the head pose range of existing training\ndata without any additional requirements. We introduce a projective matching\nprocedure to align the reconstructed 3D facial mesh to the camera coordinate\nsystem and synthesize face images with accurate gaze labels. We also propose a\nmask-guided gaze estimation model and data augmentation strategies to further\nimprove the estimation accuracy by taking advantage of the synthetic training\ndata. Experiments using multiple public datasets show that our approach can\nsignificantly improve the estimation performance on challenging cross-dataset\nsettings with non-overlapping gaze distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jiawei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimoyama_T/0/1/0/all/0/1\">Takuru Shimoyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugano_Y/0/1/0/all/0/1\">Yusuke Sugano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Egocentric 3D Human Pose in the Wild with External Weak Supervision. (arXiv:2201.07929v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07929","description":"<p>Egocentric 3D human pose estimation with a single fisheye camera has drawn a\nsignificant amount of attention recently. However, existing methods struggle\nwith pose estimation from in-the-wild images, because they can only be trained\non synthetic data due to the unavailability of large-scale in-the-wild\negocentric datasets. Furthermore, these methods easily fail when the body parts\nare occluded by or interacting with the surrounding scene. To address the\nshortage of in-the-wild data, we collect a large-scale in-the-wild egocentric\ndataset called Egocentric Poses in the Wild (EgoPW). This dataset is captured\nby a head-mounted fisheye camera and an auxiliary external camera, which\nprovides an additional observation of the human body from a third-person\nperspective during training. We present a new egocentric pose estimation\nmethod, which can be trained on the new dataset with weak external supervision.\nSpecifically, we first generate pseudo labels for the EgoPW dataset with a\nspatio-temporal optimization method by incorporating the external-view\nsupervision. The pseudo labels are then used to train an egocentric pose\nestimation network. To facilitate the network training, we propose a novel\nlearning strategy to supervise the egocentric features with the high-quality\nfeatures extracted by a pretrained external-view pose estimation model. The\nexperiments show that our method predicts accurate 3D poses from a single\nin-the-wild egocentric image and outperforms the state-of-the-art methods both\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_K/0/1/0/all/0/1\">Kripasindhu Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luvizon_D/0/1/0/all/0/1\">Diogo Luvizon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Experimental Large-Scale Jet Flames' Geometrical Features Extraction for Risk Management Using Infrared Images and Deep Learning Segmentation Methods. (arXiv:2201.07931v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07931","description":"<p>Jet fires are relatively small and have the least severe effects among the\ndiverse fire accidents that can occur in industrial plants; however, they are\nusually involved in a process known as the domino effect, that leads to more\nsevere events, such as explosions or the initiation of another fire, making the\nanalysis of such fires an important part of risk analysis. This research work\nexplores the application of deep learning models in an alternative approach\nthat uses the semantic segmentation of jet fires flames to extract main\ngeometrical attributes, relevant for fire risk assessments. A comparison is\nmade between traditional image processing methods and some state-of-the-art\ndeep learning models. It is found that the best approach is a deep learning\narchitecture known as UNet, along with its two improvements, Attention UNet and\nUNet++. The models are then used to segment a group of vertical jet flames of\nvarying pipe outlet diameters to extract their main geometrical\ncharacteristics. Attention UNet obtained the best general performance in the\napproximation of both height and area of the flames, while also showing a\nstatistically significant difference between it and UNet++. UNet obtained the\nbest overall performance for the approximation of the lift-off distances;\nhowever, there is not enough data to prove a statistically significant\ndifference between Attention UNet and UNet++. The only instance where UNet++\noutperformed the other models, was while obtaining the lift-off distances of\nthe jet flames with 0.01275 m pipe outlet diameter. In general, the explored\nmodels show good agreement between the experimental and predicted values for\nrelatively large turbulent propane jet flames, released in sonic and subsonic\nregimes; thus, making these radiation zones segmentation models, a suitable\napproach for different jet flame risk management scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Guerrero_C/0/1/0/all/0/1\">Carmina P&#xe9;rez-Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palacios_A/0/1/0/all/0/1\">Adriana Palacios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mata_C/0/1/0/all/0/1\">Christian Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casal_J/0/1/0/all/0/1\">Joaquim Casal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Mendoza_M/0/1/0/all/0/1\">Miguel Gonzalez-Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcon_Morales_L/0/1/0/all/0/1\">Luis Eduardo Falc&#xf3;n-Morales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards deep observation: A systematic survey on artificial intelligence techniques to monitor fetus via Ultrasound Images. (arXiv:2201.07935v1 [cs.LG])","link":"http://arxiv.org/abs/2201.07935","description":"<p>Developing innovative informatics approaches aimed to enhance fetal\nmonitoring is a burgeoning field of study in reproductive medicine. Several\nreviews have been conducted regarding Artificial intelligence (AI) techniques\nto improve pregnancy outcomes. They are limited by focusing on specific data\nsuch as mother's care during pregnancy. This systematic survey aims to explore\nhow artificial intelligence (AI) can assist with fetal growth monitoring via\nUltrasound (US) image. We used eight medical and computer science bibliographic\ndatabases, including PubMed, Embase, PsycINFO, ScienceDirect, IEEE explore, ACM\nLibrary, Google Scholar, and the Web of Science. We retrieved studies published\nbetween 2010 to 2021. Data extracted from studies were synthesized using a\nnarrative approach. Out of 1269 retrieved studies, we included 107 distinct\nstudies from queries that were relevant to the topic in the survey. We found\nthat 2D ultrasound images were more popular (n=88) than 3D and 4D ultrasound\nimages (n=19). Classification is the most used method (n=42), followed by\nsegmentation (n=31), classification integrated with segmentation (n=16) and\nother miscellaneous such as object-detection, regression and reinforcement\nlearning (n=18). The most common areas within the pregnancy domain were the\nfetus head (n=43), then fetus body (n=31), fetus heart (n=13), fetus abdomen\n(n=10), and lastly the fetus face (n=10). In the most recent studies, deep\nlearning techniques were primarily used (n=81), followed by machine learning\n(n=16), artificial neural network (n=7), and reinforcement learning (n=2). AI\ntechniques played a crucial role in predicting fetal diseases and identifying\nfetus anatomy structures during pregnancy. More research is required to\nvalidate this technology from a physician's perspective, such as pilot studies\nand randomized controlled trials on AI and its applications in a hospital\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alzubaidi_M/0/1/0/all/0/1\">Mahmood Alzubaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agus_M/0/1/0/all/0/1\">Marco Agus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alyafei_K/0/1/0/all/0/1\">Khalid Alyafei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althelaya_K/0/1/0/all/0/1\">Khaled A Althelaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_U/0/1/0/all/0/1\">Uzair Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdalrazaq_A/0/1/0/all/0/1\">Alaa A. Abdalrazaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anbar_M/0/1/0/all/0/1\">Mohammed Anbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_Z/0/1/0/all/0/1\">Zafar Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Househ_M/0/1/0/all/0/1\">Mowafa Househ</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GASCN: Graph Attention Shape Completion Network. (arXiv:2201.07937v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07937","description":"<p>Shape completion, the problem of inferring the complete geometry of an object\ngiven a partial point cloud, is an important problem in robotics and computer\nvision. This paper proposes the Graph Attention Shape Completion Network\n(GASCN), a novel neural network model that solves this problem. This model\ncombines a graph-based model for encoding local point cloud information with an\nMLP-based architecture for encoding global information. For each completed\npoint, our model infers the normal and extent of the local surface patch which\nis used to produce dense yet precise shape completions. We report experiments\nthat demonstrate that GASCN outperforms standard shape completion methods on a\nstandard benchmark drawn from the Shapenet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haojie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platt_R/0/1/0/all/0/1\">Robert Platt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Video Representation Learning with Cascade Positive Retrieval. (arXiv:2201.07989v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07989","description":"<p>Self-supervised video representation learning has been shown to effectively\nimprove downstream tasks such as video retrieval and action recognition.In this\npaper, we present the Cascade Positive Retrieval (CPR) that successively mines\npositive examples w.r.t. the query for contrastive learning in a cascade of\nstages. Specifically, CPR exploits multiple views of a query example in\ndifferent modalities, where an alternative view may help find another positive\nexample dissimilar in the query view. We explore the effects of possible CPR\nconfigurations in ablations including the number of mining stages, the top\nsimilar example selection ratio in each stage, and progressive training with an\nincremental number of the final Top-k selection. The overall mining quality is\nmeasured to reflect the recall across training set classes. CPR reaches a\nmedian class mining recall of 83.3%, outperforming previous work by 5.5%.\nImplementation-wise, CPR is complementary to pretext tasks and can be easily\napplied to previous work. In the evaluation of pretraining on UCF101, CPR\nconsistently improves existing work and even achieves state-of-the-art R@1 of\n56.7% and 24.4% in video retrieval as well as 83.8% and 54.8% in action\nrecognition on UCF101 and HMDB51. For transfer from large video dataset\nKinetics400 to UCF101 and HDMB, CPR benefits existing work, showing competitive\nTop-1 accuracies of 85.1% and 57.4% despite pretraining at a lower resolution\nand frame sampling rate. The code will be released soon for reproducing the\nresults. The code is available at https://github.com/necla-ml/CPR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cheng-En Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Farley Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yu Hen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadav_A/0/1/0/all/0/1\">Asim Kadav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CELESTIAL: Classification Enabled via Labelless Embeddings with Self-supervised Telescope Image Analysis Learning. (arXiv:2201.08001v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08001","description":"<p>A common class of problems in remote sensing is scene classification, a\nfundamentally important task for natural hazards identification, geographic\nimage retrieval, and environment monitoring. Recent developments in this field\nrely label-dependent supervised learning techniques which is antithetical to\nthe 35 petabytes of unlabelled satellite imagery in NASA GIBS. To solve this\nproblem, we establish CELESTIAL-a self-supervised learning pipeline for\neffectively leveraging sparsely-labeled satellite imagery. This pipeline\nsuccessfully adapts SimCLR, an algorithm that first learns image\nrepresentations on unlabelled data and then fine-tunes this knowledge on the\nprovided labels. Our results show CELESTIAL requires only a third of the labels\nthat the supervised method needs to attain the same accuracy on an experimental\ndataset. The first unsupervised tier can enable applications such as reverse\nimage search for NASA Worldview (i.e. searching similar atmospheric phenomenon\nover years of unlabelled data with minimal samples) and the second supervised\ntier can lower the necessity of expensive data annotation significantly. In the\nfuture, we hope we can generalize the CELESTIAL pipeline to other data types,\nalgorithms, and applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotha_S/0/1/0/all/0/1\">Suhas Kotha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Kasam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRMI: A Dataset of Minirhizotron Images for Diverse Plant Root Study. (arXiv:2201.08002v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08002","description":"<p>Understanding a plant's root system architecture (RSA) is crucial for a\nvariety of plant science problem domains including sustainability and climate\nadaptation. Minirhizotron (MR) technology is a widely-used approach for\nphenotyping RSA non-destructively by capturing root imagery over time.\nPrecisely segmenting roots from the soil in MR imagery is a critical step in\nstudying RSA features. In this paper, we introduce a large-scale dataset of\nplant root images captured by MR technology. In total, there are over 72K RGB\nroot images across six different species including cotton, papaya, peanut,\nsesame, sunflower, and switchgrass in the dataset. The images span a variety of\nconditions including varied root age, root structures, soil types, and depths\nunder the soil surface. All of the images have been annotated with weak\nimage-level labels indicating whether each image contains roots or not. The\nimage-level labels can be used to support weakly supervised learning in plant\nroot segmentation tasks. In addition, 63K images have been manually annotated\nto generate pixel-level binary masks indicating whether each pixel corresponds\nto root or not. These pixel-level binary masks can be used as ground truth for\nsupervised learning in semantic segmentation tasks. By introducing this\ndataset, we aim to facilitate the automatic segmentation of roots and the\nresearch of RSA with deep learning and other image analysis algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weihuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guohao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gloaguen_R/0/1/0/all/0/1\">Romain Gloaguen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zare_A/0/1/0/all/0/1\">Alina Zare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonnette_J/0/1/0/all/0/1\">Jason Bonnette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_Cabrera_J/0/1/0/all/0/1\">Joel Reyes-Cabrera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajurkar_A/0/1/0/all/0/1\">Ashish Rajurkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowland_D/0/1/0/all/0/1\">Diane Rowland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matamala_R/0/1/0/all/0/1\">Roser Matamala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jastrow_J/0/1/0/all/0/1\">Julie D. Jastrow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juenger_T/0/1/0/all/0/1\">Thomas E. Juenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritschi_F/0/1/0/all/0/1\">Felix B. Fritschi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Joint Morphological Profiles and Patch Tensor Change Detection for Hyperspectral Imagery. (arXiv:2201.08027v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08027","description":"<p>Multi-temporal hyperspectral images can be used to detect changed\ninformation, which has gradually attracted researchers' attention. However,\ntraditional change detection algorithms have not deeply explored the relevance\nof spatial and spectral changed features, which leads to low detection\naccuracy. To better excavate both spectral and spatial information of changed\nfeatures, a joint morphology and patch-tensor change detection (JMPT) method is\nproposed. Initially, a patch-based tensor strategy is adopted to exploit\nsimilar property of spatial structure, where the non-overlapping local patch\nimage is reshaped into a new tensor cube, and then three-order Tucker\ndecompositon and image reconstruction strategies are adopted to obtain more\nrobust multi-temporal hyperspectral datasets. Meanwhile, multiple morphological\nprofiles including max-tree and min-tree are applied to extract different\nattributes of multi-temporal images. Finally, these results are fused to\ngeneral a final change detection map. Experiments conducted on two real\nhyperspectral datasets demonstrate that the proposed detector achieves better\ndetection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zengfu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization via Frequency-based Feature Disentanglement and Interaction. (arXiv:2201.08029v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08029","description":"<p>Data out-of-distribution is a meta-challenge for all statistical learning\nalgorithms that strongly rely on the i.i.d. assumption. It leads to unavoidable\nlabor costs and confidence crises in realistic applications. For that, domain\ngeneralization aims at mining domain-irrelevant knowledge from multiple source\ndomains that can generalize to unseen target domains with unknown\ndistributions. In this paper, leveraging the image frequency domain, we\nuniquely work with two key observations: (i) the high-frequency information of\nimages depict object edge structure, which is naturally consistent across\ndifferent domains, and (ii) the low-frequency component retains object smooth\nstructure but are much more domain-specific. Motivated by these insights, we\nintroduce (i) an encoder-decoder structure for high-frequency and low-frequency\nfeature disentangling, (ii) an information interaction mechanism that ensures\nhelpful knowledge from both two parts can cooperate effectively, and (iii) a\nnovel data augmentation technique that works on the frequency domain for\nencouraging robustness of the network. The proposed method obtains\nstate-of-the-art results on three widely used domain generalization benchmarks\n(Digit-DG, Office-Home, and PACS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Ruoyi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dongliang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Salient Object Detection in Optical Remote Sensing Images via Feature Correlation. (arXiv:2201.08049v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08049","description":"<p>Salient object detection in optical remote sensing images (ORSI-SOD) has been\nwidely explored for understanding ORSIs. However, previous methods focus mainly\non improving the detection accuracy while neglecting the cost in memory and\ncomputation, which may hinder their real-world applications. In this paper, we\npropose a novel lightweight ORSI-SOD solution, named CorrNet, to address these\nissues. In CorrNet, we first lighten the backbone (VGG-16) and build a\nlightweight subnet for feature extraction. Then, following the coarse-to-fine\nstrategy, we generate an initial coarse saliency map from high-level semantic\nfeatures in a Correlation Module (CorrM). The coarse saliency map serves as the\nlocation guidance for low-level features. In CorrM, we mine the object location\ninformation between high-level semantic features through the cross-layer\ncorrelation operation. Finally, based on low-level detailed features, we refine\nthe coarse saliency map in the refinement subnet equipped with Dense\nLightweight Refinement Blocks, and produce the final fine saliency map. By\nreducing the parameters and computations of each component, CorrNet ends up\nhaving only 4.09M parameters and running with 21.09G FLOPs. Experimental\nresults on two public datasets demonstrate that our lightweight CorrNet\nachieves competitive or even better performance compared with 26\nstate-of-the-art methods (including 16 large CNN-based methods and 2\nlightweight methods), and meanwhile enjoys the clear memory and run time\nefficiency. The code and results of our method are available at\nhttps://github.com/MathLee/CorrNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zhen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_a/0/1/0/all/0/1\">and Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TerViT: An Efficient Ternary Vision Transformer. (arXiv:2201.08050v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08050","description":"<p>Vision transformers (ViTs) have demonstrated great potential in various\nvisual tasks, but suffer from expensive computational and memory cost problems\nwhen deployed on resource-constrained devices. In this paper, we introduce a\nternary vision transformer (TerViT) to ternarize the weights in ViTs, which are\nchallenged by the large loss surface gap between real-valued and ternary\nparameters. To address the issue, we introduce a progressive training scheme by\nfirst training 8-bit transformers and then TerViT, and achieve a better\noptimization than conventional methods. Furthermore, we introduce channel-wise\nternarization, by partitioning each matrix to different channels, each of which\nis with an unique distribution and ternarization interval. We apply our methods\nto popular DeiT and Swin backbones, and extensive results show that we can\nachieve competitive performance. For example, TerViT can quantize Swin-S to\n13.1MB model size while achieving above 79% Top-1 accuracy on ImageNet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Sheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Bohan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jinhu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Vegetation Stratum Occupancy from Airborne LiDAR Data with Deep Learning. (arXiv:2201.08051v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08051","description":"<p>We propose a new deep learning-based method for estimating the occupancy of\nvegetation strata from airborne 3D LiDAR point clouds. Our model predicts\nrasterized occupancy maps for three vegetation strata corresponding to lower,\nmedium, and higher cover. Our weakly-supervised training scheme allows our\nnetwork to only be supervised with vegetation occupancy values aggregated over\ncylindrical plots containing thousands of points. Such ground truth is easier\nto produce than pixel-wise or point-wise annotations. Our method outperforms\nhandcrafted and deep learning baselines in terms of precision by up to 30%,\nwhile simultaneously providing visual and interpretable predictions. We provide\nan open-source implementation along with a dataset of 199 agricultural plots to\ntrain and evaluate weakly supervised occupancy regression algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalinicheva_E/0/1/0/all/0/1\">Ekaterina Kalinicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallet_C/0/1/0/all/0/1\">Cl&#xe9;ment Mallet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chehata_N/0/1/0/all/0/1\">Nesrine Chehata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Elements of Temporal Sentence Grounding in Videos: A Survey and Future Directions. (arXiv:2201.08071v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08071","description":"<p>Temporal sentence grounding in videos (TSGV), a.k.a., natural language video\nlocalization (NLVL) or video moment retrieval (VMR), aims to retrieve a\ntemporal moment that semantically corresponds to a language query from an\nuntrimmed video. Connecting computer vision and natural language, TSGV has\ndrawn significant attention from researchers in both communities. This survey\nattempts to provide a summary of fundamental concepts in TSGV and current\nresearch status, as well as future research directions. As the background, we\npresent a common structure of functional components in TSGV, in a tutorial\nstyle: from feature extraction from raw video and language query, to answer\nprediction of the target moment. Then we review the techniques for multimodal\nunderstanding and interaction, which is the key focus of TSGV for effective\nalignment between the two modalities. We construct a taxonomy of TSGV\ntechniques and elaborate methods in different categories with their strengths\nand weaknesses. Lastly, we discuss issues with the current TSGV research and\nshare our insights about promising research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_W/0/1/0/all/0/1\">Wei Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AirPose: Multi-View Fusion Network for Aerial 3D Human Pose and Shape Estimation. (arXiv:2201.08093v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08093","description":"<p>In this letter, we present a novel markerless 3D human motion capture (MoCap)\nsystem for unstructured, outdoor environments that uses a team of autonomous\nunmanned aerial vehicles (UAVs) with on-board RGB cameras and computation.\nExisting methods are limited by calibrated cameras and off-line processing.\nThus, we present the first method (AirPose) to estimate human pose and shape\nusing images captured by multiple extrinsically uncalibrated flying cameras.\nAirPose itself calibrates the cameras relative to the person instead of relying\non any pre-calibration. It uses distributed neural networks running on each UAV\nthat communicate viewpoint-independent information with each other about the\nperson (i.e., their 3D shape and articulated pose). The person's shape and pose\nare parameterized using the SMPL-X body model, resulting in a compact\nrepresentation, that minimizes communication between the UAVs. The network is\ntrained using synthetic images of realistic virtual environments, and\nfine-tuned on a small set of real images. We also introduce an\noptimization-based post-processing method (AirPose$^{+}$) for offline\napplications that require higher MoCap quality. We make our method's code and\ndata available for research at\nhttps://github.com/robot-perception-group/AirPose. A video describing the\napproach and results is available at https://youtu.be/xLYe1TNHsfs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saini_N/0/1/0/all/0/1\">Nitin Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonetto_E/0/1/0/all/0/1\">Elia Bonetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1\">Aamir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What can we learn from misclassified ImageNet images?. (arXiv:2201.08098v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08098","description":"<p>Understanding the patterns of misclassified ImageNet images is particularly\nimportant, as it could guide us to design deep neural networks (DNN) that\ngeneralize better. However, the richness of ImageNet imposes difficulties for\nresearchers to visually find any useful patterns of misclassification. Here, to\nhelp find these patterns, we propose \"Superclassing ImageNet dataset\". It is a\nsubset of ImageNet which consists of 10 superclasses, each containing 7-116\nrelated subclasses (e.g., 52 bird types, 116 dog types). By training neural\nnetworks on this dataset, we found that: (i) Misclassifications are rarely\nacross superclasses, but mainly among subclasses within a superclass. (ii)\nEnsemble networks trained each only on subclasses of a given superclass perform\nbetter than the same network trained on all subclasses of all superclasses.\nHence, we propose a two-stage Super-Sub framework, and demonstrate that: (i)\nThe framework improves overall classification performance by 3.3%, by first\ninferring a superclass using a generalist superclass-level network, and then\nusing a specialized network for final subclass-level classification. (ii)\nAlthough the total parameter storage cost increases to a factor N+1 for N\nsuperclasses compared to using a single network, with finetuning, delta and\nquantization aware training techniques this can be reduced to 0.2N+1. Another\nadvantage of this efficient implementation is that the memory cost on the GPU\nduring inference is equivalent to using only one network. The reason is we\ninitiate each subclass-level network through addition of small parameter\nvariations (deltas) to the superclass-level network. (iii) Finally, our\nframework promises to be more scalable and generalizable than the common\nalternative of simply scaling up a vanilla network in size, since very large\nnetworks often suffer from overfitting and gradient vanishing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Shixian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rios_A/0/1/0/all/0/1\">Amanda Sofie Rios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lekkala_K/0/1/0/all/0/1\">Kiran Lekkala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1\">Laurent Itti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Computational Model for Machine Thinking. (arXiv:2201.08122v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08122","description":"<p>A machine thinking model is proposed in this report based on recent advances\nof computer vision and the recent results of neuroscience devoted to brain\nunderstanding. We deliver the result of machine thinking in the form of\nsentences of natural-language or drawn sketches either informative or\ndecisional. This result is obtained from a reasoning performed on new acquired\ndata and memorized data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Larabi_S/0/1/0/all/0/1\">Slimane Larabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Unsupervised Contrastive Hashing for Large-Scale Cross-Modal Text-Image Retrieval in Remote Sensing. (arXiv:2201.08125v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08125","description":"<p>Due to the availability of large-scale multi-modal data (e.g., satellite\nimages acquired by different sensors, text sentences, etc) archives, the\ndevelopment of cross-modal retrieval systems that can search and retrieve\nsemantically relevant data across different modalities based on a query in any\nmodality has attracted great attention in RS. In this paper, we focus our\nattention on cross-modal text-image retrieval, where queries from one modality\n(e.g., text) can be matched to archive entries from another (e.g., image). Most\nof the existing cross-modal text-image retrieval systems require a high number\nof labeled training samples and also do not allow fast and memory-efficient\nretrieval due to their intrinsic characteristics. These issues limit the\napplicability of the existing cross-modal retrieval systems for large-scale\napplications in RS. To address this problem, in this paper we introduce a novel\ndeep unsupervised cross-modal contrastive hashing (DUCH) method for RS\ntext-image retrieval. The proposed DUCH is made up of two main modules: 1)\nfeature extraction module (which extracts deep representations of the\ntext-image modalities); and 2) hashing module (which learns to generate\ncross-modal binary hash codes from the extracted representations). Within the\nhashing module, we introduce a novel multi-objective loss function including:\ni) contrastive objectives that enable similarity preservation in both intra-\nand inter-modal similarities; ii) an adversarial objective that is enforced\nacross two modalities for cross-modal representation consistency; iii)\nbinarization objectives for generating representative hash codes. Experimental\nresults show that the proposed DUCH outperforms state-of-the-art unsupervised\ncross-modal hashing methods on two multi-modal (image and text) benchmark\narchives in RS. Our code is publicly available at\nhttps://git.tu-berlin.de/rsim/duch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mikriukov_G/0/1/0/all/0/1\">Georgii Mikriukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravanbakhsh_M/0/1/0/all/0/1\">Mahdyar Ravanbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeoFill: Reference-Based Image Inpainting of Scenes with Complex Geometry. (arXiv:2201.08131v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08131","description":"<p>Reference-guided image inpainting restores image pixels by leveraging the\ncontent from another reference image. The previous state-of-the-art, TransFill,\nwarps the source image with multiple homographies, and fuses them together for\nhole filling. Inspired by structure from motion pipelines and recent progress\nin monocular depth estimation, we propose a more principled approach that does\nnot require heuristic planar assumptions. We leverage a monocular depth\nestimate and predict relative pose between cameras, then align the reference\nimage to the target by a differentiable 3D reprojection and a joint\noptimization of relative pose and depth map scale and offset. Our approach\nachieves state-of-the-art performance on both RealEstate10K and\nMannequinChallenge dataset with large baselines, complex geometry and extreme\ncamera motions. We experimentally verify our approach is also better at\nhandling large holes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunhan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_C/0/1/0/all/0/1\">Connelly Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuqian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirghodsi_S/0/1/0/all/0/1\">Sohrab Amirghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPAMs: Structured Implicit Parametric Models. (arXiv:2201.08141v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08141","description":"<p>Parametric 3D models have formed a fundamental role in modeling deformable\nobjects, such as human bodies, faces, and hands; however, the construction of\nsuch parametric models requires significant manual intervention and domain\nexpertise. Recently, neural implicit 3D representations have shown great\nexpressibility in capturing 3D shape geometry. We observe that deformable\nobject motion is often semantically structured, and thus propose to learn\nStructured-implicit PArametric Models (SPAMs) as a deformable object\nrepresentation that structurally decomposes non-rigid object motion into\npart-based disentangled representations of shape and pose, with each being\nrepresented by deep implicit functions. This enables a structured\ncharacterization of object movement, with part decomposition characterizing a\nlower-dimensional space in which we can establish coarse motion correspondence.\nIn particular, we can leverage the part decompositions at test time to fit to\nnew depth sequences of unobserved shapes, by establishing part correspondences\nbetween the input observation and our learned part spaces; this guides a robust\njoint optimization between the shape and pose of all parts, even under dramatic\nmotion sequences. Experiments demonstrate that our part-aware shape and pose\nunderstanding lead to state-of-the-art performance in reconstruction and\ntracking of depth sequences of complex deforming object motion. We plan to\nrelease models to the public at https://pablopalafox.github.io/spams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palafox_P/0/1/0/all/0/1\">Pablo Palafox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarafianos_N/0/1/0/all/0/1\">Nikolaos Sarafianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physically Embodied Deep Image Optimisation. (arXiv:2201.08142v1 [cs.RO])","link":"http://arxiv.org/abs/2201.08142","description":"<p>Physical sketches are created by learning programs to control a drawing\nrobot. A differentiable rasteriser is used to optimise sets of drawing strokes\nto match an input image, using deep networks to provide an encoding for which\nwe can compute a loss. The optimised drawing primitives can then be translated\ninto G-code commands which command a robot to draw the image using drawing\ninstruments such as pens and pencils on a physical support medium.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihai_D/0/1/0/all/0/1\">Daniela Mihai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WPPNets: Unsupervised CNN Training with Wasserstein Patch Priors for Image Superresolution. (arXiv:2201.08157v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08157","description":"<p>We introduce WPPNets, which are CNNs trained by a new unsupervised loss\nfunction for image superresolution of materials microstructures. Instead of\nrequiring access to a large database of registered high- and low-resolution\nimages, we only assume to know a large database of low resolution images, the\nforward operator and one high-resolution reference image. Then, we propose a\nloss function based on the Wasserstein patch prior which measures the\nWasserstein-2 distance between the patch distributions of the predictions and\nthe reference image. We demonstrate by numerical examples that WPPNets\noutperform other methods with similar assumptions. In particular, we show that\nWPPNets are much more stable under inaccurate knowledge or perturbations of the\nforward operator. This enables us to use them in real-world applications, where\nneither a large database of registered data nor the exact forward operator are\ngiven.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Altekruger_F/0/1/0/all/0/1\">Fabian Altekr&#xfc;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertrich_J/0/1/0/all/0/1\">Johannes Hertrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HumanIBR: High Quality Image-based Rendering of Challenging Human Performers using Sparse Views. (arXiv:2201.08158v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08158","description":"<p>In this paper, we introduce HumanIBR, a method that addresses the challenge\nof novel view rendering of human performers that wear clothes with complex\npatterns using a sparse set of camera views. Some recent works have achieved\nremarkable rendering quality on humans that wear pure clothes using sparse\nviews, but if the clothes have complex color patterns, the rendering quality is\nstill very low. To this end, the proposed HumanIBR uses a human reconstruction\nnet with pixel-aligned spatial transformer and a render net that uses\ngeometry-guided pixel-wise feature integration to achieve to goal of high\nquality human reconstruction and rendering. The designed pixel-aligned spatial\ntransformer calculates the correlations between the input views, producing\nhuman reconstruction results with high-frequency details presented in the input\nviews. Based on the reconstruction, the geometry-guided pixel-wise visibility\nreasoning provides a guidance for multi-view feature integration, enabling the\nrender net to render high quality images on novel views. Unlike previous neural\nrendering works that always need to train or fine-tune a separate network for\neach scene or human, our method is a general framework that is able to\ngeneralize to novel humans. Experiments show that our approach outperforms all\nthe prior general or human-specific works on both synthetic data and real-world\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tiansong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Ruizhi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CP-Net: Contour-Perturbed Reconstruction Network for Self-Supervised Point Cloud Learning. (arXiv:2201.08215v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08215","description":"<p>Self-supervised learning has not been fully explored for point cloud\nanalysis. Current frameworks are mainly based on point cloud reconstruction.\nGiven only 3D coordinates, such approaches tend to learn local geometric\nstructures and contours, while failing in understanding high level semantic\ncontent. Consequently, they achieve unsatisfactory performance in downstream\ntasks such as classification, segmentation, etc. To fill this gap, we propose a\ngeneric Contour-Perturbed Reconstruction Network (CP-Net), which can\neffectively guide self-supervised reconstruction to learn semantic content in\nthe point cloud, and thus promote discriminative power of point cloud\nrepresentation. First, we introduce a concise contour-perturbed augmentation\nmodule for point cloud reconstruction. With guidance of geometry disentangling,\nwe divide point cloud into contour and content components. Subsequently, we\nperturb the contour components and preserve the content components on the point\ncloud. As a result, self supervisor can effectively focus on semantic content,\nby reconstructing the original point cloud from such perturbed one. Second, we\nuse this perturbed reconstruction as an assistant branch, to guide the learning\nof basic reconstruction branch via a distinct dual-branch consistency loss. In\nthis case, our CP-Net not only captures structural contour but also learn\nsemantic content for discriminative downstream tasks. Finally, we perform\nextensive experiments on a number of point cloud benchmarks. Part segmentation\nresults demonstrate that our CP-Net (81.5% of mIoU) outperforms the previous\nself-supervised models, and narrows the gap with the fully-supervised methods.\nFor classification, we get a competitive result with the fully-supervised\nmethods on ModelNet40 (92.5% accuracy) and ScanObjectNN (87.9% accuracy). The\ncodes and models will be released afterwards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingye Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watermarking Pre-trained Encoders in Contrastive Learning. (arXiv:2201.08217v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08217","description":"<p>Contrastive learning has become a popular technique to pre-train image\nencoders, which could be used to build various downstream classification models\nin an efficient way. This process requires a large amount of data and\ncomputation resources. Hence, the pre-trained encoders are an important\nintellectual property that needs to be carefully protected. It is challenging\nto migrate existing watermarking techniques from the classification tasks to\nthe contrastive learning scenario, as the owner of the encoder lacks the\nknowledge of the downstream tasks which will be developed from the encoder in\nthe future. We propose the \\textit{first} watermarking methodology for the\npre-trained encoders. We introduce a task-agnostic loss function to effectively\nembed into the encoder a backdoor as the watermark. This backdoor can still\nexist in any downstream models transferred from the encoder. Extensive\nevaluations over different contrastive learning algorithms, datasets, and\ndownstream tasks indicate our watermarks exhibit high effectiveness and\nrobustness against different adversarial operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yutong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Han Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+L_J/0/1/0/all/0/1\">Jiwei L</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Meikang Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Generative Pretraining for Multimodal Video Captioning. (arXiv:2201.08264v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08264","description":"<p>Recent video and language pretraining frameworks lack the ability to generate\nsentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new\npretraining framework for learning from unlabelled videos which can be\neffectively used for generative tasks such as multimodal video captioning.\nUnlike recent video-language pretraining frameworks, our framework trains both\na multimodal video encoder and a sentence decoder jointly. To overcome the lack\nof captions in unlabelled videos, we leverage the future utterance as an\nadditional text source and propose a bidirectional generation objective -- we\ngenerate future utterances given the present mulitmodal context, and also the\npresent utterance given future observations. With this objective, we train an\nencoder-decoder model end-to-end to generate a caption from raw pixels and\ntranscribed speech directly. Our model achieves state-of-the-art performance\nfor multimodal video captioning on four standard benchmarks, as well as for\nother video understanding tasks such as VideoQA, video retrieval and action\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_P/0/1/0/all/0/1\">Paul Hongsuck Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Rendering for Integral Imaging Light Field Displays Based on a Voxel-Pixel Lookup Table. (arXiv:2201.08266v1 [cs.GR])","link":"http://arxiv.org/abs/2201.08266","description":"<p>A real-time elemental image array (EIA) generation method which does not\nsacrifice accuracy nor rely on high-performance hardware is developed, through\nraytracing and pre-stored voxel-pixel lookup table (LUT). Benefiting from both\noffline and online working flow, experiments verified the effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Q/0/1/0/all/0/1\">Quanzhen Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling and hexahedral meshing of arterial networks from centerlines. (arXiv:2201.08279v1 [cs.CG])","link":"http://arxiv.org/abs/2201.08279","description":"<p>Computational fluid dynamics (CFD) simulation provides valuable information\non blood flow from the vascular geometry. However, it requires to extract\naccurate models of arteries from low resolution medical images, which remains\nchallenging. Centerline-based representation is widely used to model large\nvascular networks with small vessels, as it enables manual editing and encodes\nthe topological information. In this work, we propose an automatic method to\ngenerate an hexahedral mesh suitable for CFD directly from centerlines. The\nproposed method is an improvement of the state-of-the-art in terms of\nrobustness, mesh quality and reproductibility.\n</p>\n<p>Both the modeling and meshing tasks are addressed. A new vessel model based\non penalized splines is proposed to overcome the limitations inherent to the\ncenterline representation, such as noise and sparsity. Bifurcations are\nreconstructed using a physiologically accurate parametric model that we\nextended to planar n-furcations. Finally, a volume mesh with structured,\nhexahedral and flow oriented cells is produced from the proposed vascular\nnetwork model.\n</p>\n<p>The proposed method offers a better robustness and mesh quality than the\nstate-of-the-art methods. As it combines both modeling and meshing techniques,\nit can be applied to edit the geometry and topology of vascular models\neffortlessly to study the impact on hemodynamics. We demonstrate the efficiency\nof our method by entirely meshing a dataset of 60 cerebral vascular networks.\n92\\% of the vessels and 83\\% of the bifurcations where mesh without defects\nneeding manual intervention, despite the challenging aspect of the input data.\nThe source code will be released publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Decroocq_M/0/1/0/all/0/1\">M&#xe9;ghane Decroocq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frindel_C/0/1/0/all/0/1\">Carole Frindel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohta_M/0/1/0/all/0/1\">Makoto Ohta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavoue_G/0/1/0/all/0/1\">Guillaume Lavou&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIVA-DAF: A Deep Learning Framework for Historical Document Image Analysis. (arXiv:2201.08295v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08295","description":"<p>In this paper, we introduce a new deep learning framework called DIVA-DAF. We\nhave developed this framework to support our research on historical document\nimage analysis tasks and to develop techniques to reduce the need for\nmanually-labeled ground truth. We want to apply self-supervised learning\ntechniques and use different kinds of training data. Our new framework aids us\nin performing rapid prototyping and reproducible experiments. We present a\nfirst semantic segmentation experiment on DIVA-HisDB using our framework,\nachieving state-of-the-art results. The DIVA-DAF framework is open-source, and\nwe encourage other research groups to use it for their experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vogtlin_L/0/1/0/all/0/1\">Lars V&#xf6;gtlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maergner_P/0/1/0/all/0/1\">Paul Maergner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingold_R/0/1/0/all/0/1\">Rolf Ingold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stitch it in Time: GAN-Based Facial Editing of Real Videos. (arXiv:2201.08361v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08361","description":"<p>The ability of Generative Adversarial Networks to encode rich semantics\nwithin their latent space has been widely adopted for facial image editing.\nHowever, replicating their success with videos has proven challenging. Sets of\nhigh-quality facial videos are lacking, and working with videos introduces a\nfundamental barrier to overcome - temporal coherency. We propose that this\nbarrier is largely artificial. The source video is already temporally coherent,\nand deviations from this state arise in part due to careless treatment of\nindividual components in the editing pipeline. We leverage the natural\nalignment of StyleGAN and the tendency of neural networks to learn low\nfrequency functions, and demonstrate that they provide a strongly consistent\nprior. We draw on these insights and propose a framework for semantic editing\nof faces in videos, demonstrating significant improvements over the current\nstate-of-the-art. Our method produces meaningful face manipulations, maintains\na higher degree of temporal consistency, and can be applied to challenging,\nhigh quality, talking head videos which current methods struggle with.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzaban_R/0/1/0/all/0/1\">Rotem Tzaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokady_R/0/1/0/all/0/1\">Ron Mokady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1\">Rinon Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit H. Bermano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Weakly Supervised Pre-Training of Visual Perception Models. (arXiv:2201.08371v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08371","description":"<p>Model pre-training is a cornerstone of modern visual recognition systems.\nAlthough fully supervised pre-training on datasets like ImageNet is still the\nde-facto standard, recent studies suggest that large-scale weakly supervised\npre-training can outperform fully supervised approaches. This paper revisits\nweakly-supervised pre-training of models using hashtag supervision with modern\nversions of residual networks and the largest-ever dataset of images and\ncorresponding hashtags. We study the performance of the resulting models in\nvarious transfer-learning settings including zero-shot transfer. We also\ncompare our models with those obtained via large-scale self-supervised\nlearning. We find our weakly-supervised models to be very competitive across\nall settings, and find they substantially outperform their self-supervised\ncounterparts. We also include an investigation into whether our models learned\npotentially troubling associations or stereotypes. Overall, our results provide\na compelling argument for the use of weakly supervised learning in the\ndevelopment of visual recognition systems. Our models, Supervised Weakly\nthrough hashtAGs (SWAG), are available publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mannat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gustafson_L/0/1/0/all/0/1\">Laura Gustafson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adcock_A/0/1/0/all/0/1\">Aaron Adcock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reis_V/0/1/0/all/0/1\">Vinicius de Freitas Reis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedik_B/0/1/0/all/0/1\">Bugra Gedik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosaraju_R/0/1/0/all/0/1\">Raj Prateek Kosaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Dhruv Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1\">Ross Girshick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollar_P/0/1/0/all/0/1\">Piotr Doll&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaten_L/0/1/0/all/0/1\">Laurens van der Maaten</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omnivore: A Single Model for Many Visual Modalities. (arXiv:2201.08377v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08377","description":"<p>Prior work has studied different visual modalities in isolation and developed\nseparate architectures for recognition of images, videos, and 3D data. Instead,\nin this paper, we propose a single model which excels at classifying images,\nvideos, and single-view 3D data using exactly the same model parameters. Our\n'Omnivore' model leverages the flexibility of transformer-based architectures\nand is trained jointly on classification tasks from different modalities.\nOmnivore is simple to train, uses off-the-shelf standard datasets, and performs\nat-par or better than modality-specific models of the same size. A single\nOmnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN\nRGB-D. After finetuning, our models outperform prior work on a variety of\nvision tasks and generalize across modalities. Omnivore's shared visual\nrepresentation naturally enables cross-modal recognition without access to\ncorrespondences between modalities. We hope our results motivate researchers to\nmodel visual modalities together.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1\">Rohit Girdhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mannat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_N/0/1/0/all/0/1\">Nikhila Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaten_L/0/1/0/all/0/1\">Laurens van der Maaten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Pixel Trajectories with Multiscale Contrastive Random Walks. (arXiv:2201.08379v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08379","description":"<p>A range of video modeling tasks, from optical flow to multiple object\ntracking, share the same fundamental challenge: establishing space-time\ncorrespondence. Yet, approaches that dominate each space differ. We take a step\ntowards bridging this gap by extending the recent contrastive random walk\nformulation to much denser, pixel-level space-time graphs. The main\ncontribution is introducing hierarchy into the search problem by computing the\ntransition matrix between two frames in a coarse-to-fine manner, forming a\nmultiscale contrastive random walk when extended in time. This establishes a\nunified technique for self-supervised learning of optical flow, keypoint\ntracking, and video object segmentation. Experiments demonstrate that, for each\nof these tasks, the unified model achieves performance competitive with strong\nself-supervised approaches specific to that task. Project site:\nhttps://jasonbian97.github.io/flowwalk\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1\">Zhangxing Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabri_A/0/1/0/all/0/1\">Allan Jabri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owens_A/0/1/0/all/0/1\">Andrew Owens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition. (arXiv:2201.08383v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08383","description":"<p>While today's video recognition systems parse snapshots or short clips\naccurately, they cannot connect the dots and reason across a longer range of\ntime yet. Most existing video architectures can only process &lt;5 seconds of a\nvideo without hitting the computation or memory bottlenecks.\n</p>\n<p>In this paper, we propose a new strategy to overcome this challenge. Instead\nof trying to process more frames at once like most existing methods, we propose\nto process videos in an online fashion and cache \"memory\" at each iteration.\nThrough the memory, the model can reference prior context for long-term\nmodeling, with only a marginal cost. Based on this idea, we build MeMViT, a\nMemory-augmented Multiscale Vision Transformer, that has a temporal support 30x\nlonger than existing models with only 4.5% more compute; traditional methods\nneed &gt;3,000% more compute to do the same. On a wide range of settings, the\nincreased temporal support enabled by MeMViT brings large gains in recognition\naccuracy consistently. MeMViT obtains state-of-the-art results on the AVA,\nEPIC-Kitchens-100 action classification, and action anticipation datasets. Code\nand models will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chao-Yuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1\">Bo Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fixing the train-test resolution discrepancy. (arXiv:1906.06423v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1906.06423","description":"<p>Data-augmentation is key to the training of neural networks for image\nclassification. This paper first shows that existing augmentations induce a\nsignificant discrepancy between the typical size of the objects seen by the\nclassifier at train and test time. We experimentally validate that, for a\ntarget test resolution, using a lower train resolution offers better\nclassification at test time.\n</p>\n<p>We then propose a simple yet effective and efficient strategy to optimize the\nclassifier performance when the train and test resolutions differ. It involves\nonly a computationally cheap fine-tuning of the network at the test resolution.\nThis enables training strong classifiers using small training images. For\ninstance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained\non 128x128 images, and 79.8% with one trained on 224x224 image. In addition, if\nwe use extra training data we get 82.5% with the ResNet-50 train with 224x224\nimages.\n</p>\n<p>Conversely, when training a ResNeXt-101 32x48d pre-trained in\nweakly-supervised fashion on 940 million public images at resolution 224x224\nand further optimizing for test resolution 320x320, we obtain a test top-1\naccuracy of 86.4% (top-5: 98.0%) (single-crop). To the best of our knowledge\nthis is the highest ImageNet single-crop, top-1 and top-5 accuracy to date.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1\">Hugo Touvron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1\">Herv&#xe9; J&#xe9;gou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dimensionality reduction to maximize prediction generalization capability. (arXiv:2003.00470v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2003.00470","description":"<p>Generalization of time series prediction remains an important open issue in\nmachine learning, wherein earlier methods have either large generalization\nerror or local minima. We develop an analytically solvable, unsupervised\nlearning scheme that extracts the most informative components for predicting\nfuture inputs, termed predictive principal component analysis (PredPCA). Our\nscheme can effectively remove unpredictable noise and minimize test prediction\nerror through convex optimization. Mathematical analyses demonstrate that,\nprovided with sufficient training samples and sufficiently high-dimensional\nobservations, PredPCA can asymptotically identify hidden states, system\nparameters, and dimensionalities of canonical nonlinear generative processes,\nwith a global convergence guarantee. We demonstrate the performance of PredPCA\nusing sequential visual inputs comprising hand-digits, rotating 3D objects, and\nnatural scenes. It reliably estimates distinct hidden states and predicts\nfuture outcomes of previously unseen test input data, based exclusively on\nnoisy observations. The simple architecture and low computational cost of\nPredPCA are highly desirable for neuromorphic hardware.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Isomura_T/0/1/0/all/0/1\">Takuya Isomura</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Toyoizumi_T/0/1/0/all/0/1\">Taro Toyoizumi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Bounding-box Regression with Distance-IoU Loss for Visual Tracking. (arXiv:2007.01864v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.01864","description":"<p>Most existing trackers are based on using a classifier and multi-scale\nestimation to estimate the target state. Consequently, and as expected,\ntrackers have become more stable while tracking accuracy has stagnated. While\ntrackers adopt a maximum overlap method based on an intersection-over-union\n(IoU) loss to mitigate this problem, there are defects in the IoU loss itself,\nthat make it impossible to continue to optimize the objective function when a\ngiven bounding box is completely contained within/without another bounding box;\nthis makes it very challenging to accurately estimate the target state.\nAccordingly, in this paper, we address the above-mentioned problem by proposing\na novel tracking method based on a distance-IoU (DIoU) loss, such that the\nproposed tracker consists of target estimation and target classification. The\ntarget estimation part is trained to predict the DIoU score between the target\nground-truth bounding-box and the estimated bounding-box. The DIoU loss can\nmaintain the advantage provided by the IoU loss while minimizing the distance\nbetween the center points of two bounding boxes, thereby making the target\nestimation more accurate. Moreover, we introduce a classification part that is\ntrained online and optimized with a Conjugate-Gradient-based strategy to\nguarantee real-time tracking speed. Comprehensive experimental results\ndemonstrate that the proposed method achieves competitive tracking accuracy\nwhen compared to state-of-the-art trackers while with a real-time tracking\nspeed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_D/0/1/0/all/0/1\">Di Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiu Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Nana Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyu He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer Vision and Normalizing Flow-Based Defect Detection. (arXiv:2012.06737v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.06737","description":"<p>Visual defect detection is critical to ensure the quality of most products.\nHowever, majority of small medium manufactures still rely on tedious and\nerror-prune human manual inspection. The main reasons include: 1) the existing\nautomated visual defect detection systems require altering production assembly\nlines, which is time consuming and expensive 2) the existing systems require\nmanually collecting defective samples and labeling them for a comparison-based\nalgorithm or training a machine learning model. This introduces heavy burden\nfor Small and Medium-sized Enterprise (SME) manufactures as defects do not\nhappen often and are difficult and time-consuming to collect. Furthermore, we\ncannot exhaustively collect or define all defect types as any new deviation\nfrom acceptable products are defects. In this paper, we overcome these\nchallenges and design a three-stage plug-and-play fully automated unsupervised\n360-degree defect detection system. In our system, products are freely placed\non an unaltered assembly line and receive 360 degree visual inspection with\nmultiple cameras from different angles. As such, the images collected from\nreal-world product assembly lines contain lots of background noise. The\nproducts face different angles. The product sizes vary due to the distance to\ncameras. All these make defect detection much more difficult. Our system use\nobject detection, background subtraction and unsupervised normalizing\nflow-based defect detection techniques to tackle these difficulty. Experiments\nshow our system can achieve 0.90 AUROC in a real-world non-altered drink ware\nproduction assembly line.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zijian Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tie_X/0/1/0/all/0/1\">Xinran Tie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1\">Lihang Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shi Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance and Panoptic Segmentation Using Conditional Convolutions. (arXiv:2102.03026v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03026","description":"<p>We propose a simple yet effective framework for instance and panoptic\nsegmentation, termed CondInst (conditional convolutions for instance and\npanoptic segmentation). In the literature, top-performing instance segmentation\nmethods typically follow the paradigm of Mask R-CNN and rely on ROI operations\n(typically ROIAlign) to attend to each instance. In contrast, we propose to\nattend to the instances with dynamic conditional convolutions. Instead of using\ninstance-wise ROIs as inputs to the instance mask head of fixed weights, we\ndesign dynamic instance-aware mask heads, conditioned on the instances to be\npredicted. CondInst enjoys three advantages: 1.) Instance and panoptic\nsegmentation are unified into a fully convolutional network, eliminating the\nneed for ROI cropping and feature alignment. 2.) The elimination of the ROI\ncropping also significantly improves the output instance mask resolution. 3.)\nDue to the much improved capacity of dynamically-generated conditional\nconvolutions, the mask head can be very compact (e.g., 3 conv. layers, each\nhaving only 8 channels), leading to significantly faster inference time per\ninstance and making the overall inference time almost constant, irrelevant to\nthe number of instances. We demonstrate a simpler method that can achieve\nimproved accuracy and inference speed on both instance and panoptic\nsegmentation tasks. On the COCO dataset, we outperform a few state-of-the-art\nmethods. We hope that CondInst can be a strong baseline for instance and\npanoptic segmentation. Code is available at: https://git.io/AdelaiDet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Single Image Generation with Controllable Global Structure. (arXiv:2102.04780v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.04780","description":"<p>Image generation from a single image using generative adversarial networks is\nquite interesting due to the realism of generated images. However, recent\napproaches need improvement for such realistic and diverse image generation,\nwhen the global context of the image is important such as in face, animal, and\narchitectural image generation. This is mainly due to the use of fewer\nconvolutional layers for mainly capturing the patch statistics and, thereby,\nnot being able to capture global statistics very well. We solve this problem by\nusing attention blocks at selected scales and feeding a random Gaussian blurred\nimage to the discriminator for training. Our results are visually better than\nthe state-of-the-art particularly in generating images that require global\ncontext. The diversity of our image generation, measured using the average\nstandard deviation of pixels, is also better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahendren_S/0/1/0/all/0/1\">Sutharsan Mahendren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edussooriya_C/0/1/0/all/0/1\">Chamira Edussooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Head-Position Prediction in First-Person View by Considering Head Pose for Human-Robot Eye Contact. (arXiv:2103.06417v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2103.06417","description":"<p>For a humanoid robot to make eye contact and initiate communication with a\nperson, it is necessary to estimate the person's head position. However, eye\ncontact becomes difficult due to the mechanical delay of the robot when the\nperson is moving. Owing to these issues, it is important to conduct a\nhead-position prediction to mitigate the effect of the delay in the robot\nmotion. Based on the fact that humans turn their heads before changing\ndirection while walking, we hypothesized that the accuracy of three-dimensional\n(3D) head-position prediction from a first-person view can be improved by\nconsidering the head pose. We compared our method with a conventional Kalman\nfilter-based approach, and found our method to be more accurate. The experiment\nresults show that considering the head pose helps improve the accuracy of 3D\nhead-position prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tamaru_Y/0/1/0/all/0/1\">Yuki Tamaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozaki_Y/0/1/0/all/0/1\">Yasunori Ozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okafuji_Y/0/1/0/all/0/1\">Yuki Okafuji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakanishi_J/0/1/0/all/0/1\">Junya Nakanishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_Y/0/1/0/all/0/1\">Yuichiro Yoshikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baba_J/0/1/0/all/0/1\">Jun Baba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Duplex Contextual Relation Network for Polyp Segmentation. (arXiv:2103.06725v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.06725","description":"<p>Polyp segmentation is of great importance in the early diagnosis and\ntreatment of colorectal cancer. Since polyps vary in their shape, size, color,\nand texture, accurate polyp segmentation is very challenging. One promising way\nto mitigate the diversity of polyps is to model the contextual relation for\neach pixel such as using attention mechanism. However, previous methods only\nfocus on learning the dependencies between the position within an individual\nimage and ignore the contextual relation across different images. In this\npaper, we propose Duplex Contextual Relation Network (DCRNet) to capture both\nwithin-image and cross-image contextual relations. Specifically, we first\ndesign Interior Contextual-Relation Module to estimate the similarity between\neach position and all the positions within the same image. Then Exterior\nContextual-Relation Module is incorporated to estimate the similarity between\neach position and the positions across different images. Based on the above two\ntypes of similarity, the feature at one position can be further enhanced by the\ncontextual region embedding within and across images. To store the\ncharacteristic region embedding from all the images, a memory bank is designed\nand operates as a queue. Therefore, the proposed method can relate similar\nfeatures even though they come from different images. We evaluate the proposed\nmethod on the EndoScene, Kvasir-SEG and the recently released large-scale\nPICCOLO dataset. Experimental results show that the proposed DCRNet outperforms\nthe state-of-the-art methods in terms of the widely-used evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yin_Z/0/1/0/all/0/1\">Zijin Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_K/0/1/0/all/0/1\">Kongming Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MBAPose: Mask and Bounding-Box Aware Pose Estimation of Surgical Instruments with Photorealistic Domain Randomization. (arXiv:2103.08105v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.08105","description":"<p>Surgical robots are usually controlled using a priori models based on the\nrobots' geometric parameters, which are calibrated before the surgical\nprocedure. One of the challenges in using robots in real surgical settings is\nthat those parameters can change over time, consequently deteriorating control\naccuracy. In this context, our group has been investigating online calibration\nstrategies without added sensors. In one step toward that goal, we have\ndeveloped an algorithm to estimate the pose of the instruments' shafts in\nendoscopic images. In this study, we build upon that earlier work and propose a\nnew framework to more precisely estimate the pose of a rigid surgical\ninstrument. Our strategy is based on a novel pose estimation model called\nMBAPose and the use of synthetic training data. Our experiments demonstrated an\nimprovement of 21 % for translation error and 26 % for orientation error on\nsynthetic test data with respect to our previous work. Results with real test\ndata provide a baseline for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoshimura_M/0/1/0/all/0/1\">Masakazu Yoshimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinho_M/0/1/0/all/0/1\">Murilo Marques Marinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_K/0/1/0/all/0/1\">Kanako Harada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitsuishi_M/0/1/0/all/0/1\">Mamoru Mitsuishi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Spatio-Temporal Interaction Learning for Video Deraining: A Faster and Better Framework. (arXiv:2103.12318v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12318","description":"<p>Video deraining is an important task in computer vision as the unwanted rain\nhampers the visibility of videos and deteriorates the robustness of most\noutdoor vision systems. Despite the significant success which has been achieved\nfor video deraining recently, two major challenges remain: 1) how to exploit\nthe vast information among continuous frames to extract powerful\nspatio-temporal features across both the spatial and temporal domains, and 2)\nhow to restore high-quality derained videos with a high-speed approach. In this\npaper, we present a new end-to-end video deraining framework, named Enhanced\nSpatio-Temporal Interaction Network (ESTINet), which considerably boosts\ncurrent state-of-the-art video deraining quality and speed. The ESTINet takes\nthe advantage of deep residual networks and convolutional long short-term\nmemory, which can capture the spatial features and temporal correlations among\ncontinuing frames at the cost of very little computational source. Extensive\nexperiments on three public datasets show that the proposed ESTINet can achieve\nfaster speed than the competitors, while maintaining better performance than\nthe state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MT3: Meta Test-Time Training for Self-Supervised Test-Time Adaption. (arXiv:2103.16201v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16201","description":"<p>An unresolved problem in Deep Learning is the ability of neural networks to\ncope with domain shifts during test-time, imposed by commonly fixing network\nparameters after training. Our proposed method Meta Test-Time Training (MT3),\nhowever, breaks this paradigm and enables adaption at test-time. We combine\nmeta-learning, self-supervision and test-time training to learn to adapt to\nunseen test distributions. By minimizing the self-supervised loss, we learn\ntask-specific model parameters for different tasks. A meta-model is optimized\nsuch that its adaption to the different task-specific models leads to higher\nperformance on those tasks. During test-time a single unlabeled image is\nsufficient to adapt the meta-model parameters. This is achieved by minimizing\nonly the self-supervised loss component resulting in a better prediction for\nthat image. Our approach significantly improves the state-of-the-art results on\nthe CIFAR-10-Corrupted image classification benchmark. Our implementation is\navailable on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartler_A/0/1/0/all/0/1\">Alexander Bartler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhler_A/0/1/0/all/0/1\">Andre B&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiewel_F/0/1/0/all/0/1\">Felix Wiewel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobler_M/0/1/0/all/0/1\">Mario D&#xf6;bler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Aliased Resizing and Surprising Subtleties in GAN Evaluation. (arXiv:2104.11222v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11222","description":"<p>Metrics for evaluating generative models aim to measure the discrepancy\nbetween real and generated images. The often-used Frechet Inception Distance\n(FID) metric, for example, extracts \"high-level\" features using a deep network\nfrom the two sets. However, we find that the differences in \"low-level\"\npreprocessing, specifically image resizing and compression, can induce large\nvariations and have unforeseen consequences. For instance, when resizing an\nimage, e.g., with a bilinear or bicubic kernel, signal processing principles\nmandate adjusting prefilter width depending on the downsampling factor, to\nantialias to the appropriate bandwidth. However, commonly-used implementations\nuse a fixed-width prefilter, resulting in aliasing artifacts. Such aliasing\nleads to corruptions in the feature extraction downstream. Next, lossy\ncompression, such as JPEG, is commonly used to reduce the file size of an\nimage. Although designed to minimally degrade the perceptual quality of an\nimage, the operation also produces variations downstream. Furthermore, we show\nthat if compression is used on real training images, FID can actually improve\nif the generated images are also subsequently compressed. This paper shows that\nchoices in low-level image processing have been an underappreciated aspect of\ngenerative modeling. We identify and characterize variations in generative\nmodeling development pipelines, provide recommendations based on signal\nprocessing principles, and release a reference implementation to facilitate\nfuture comparisons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_G/0/1/0/all/0/1\">Gaurav Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richard Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images. (arXiv:2106.11944v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11944","description":"<p>In this paper, we aim to create generalizable and controllable neural signed\ndistance fields (SDFs) that represent clothed humans from monocular depth\nobservations. Recent advances in deep learning, especially neural implicit\nrepresentations, have enabled human shape reconstruction and controllable\navatar generation from different sensor inputs. However, to generate realistic\ncloth deformations from novel input poses, watertight meshes or dense full-body\nscans are usually needed as inputs. Furthermore, due to the difficulty of\neffectively modeling pose-dependent cloth deformations for diverse body shapes\nand cloth types, existing approaches resort to per-subject/cloth-type\noptimization from scratch, which is computationally expensive. In contrast, we\npropose an approach that can quickly generate realistic clothed human avatars,\nrepresented as controllable neural SDFs, given only monocular depth images. We\nachieve this by using meta-learning to learn an initialization of a\nhypernetwork that predicts the parameters of neural SDFs. The hypernetwork is\nconditioned on human poses and represents a clothed neural avatar that deforms\nnon-rigidly according to the input poses. Meanwhile, it is meta-learned to\neffectively incorporate priors of diverse body shapes and cloth types and thus\ncan be much faster to fine-tune, compared to models trained from scratch. We\nqualitatively and quantitatively show that our approach outperforms\nstate-of-the-art approaches that require complete meshes as inputs while our\napproach requires only depth frames as inputs and runs orders of magnitudes\nfaster. Furthermore, we demonstrate that our meta-learned hypernetwork is very\nrobust, being the first to generate avatars with realistic dynamic cloth\ndeformations given as few as 8 monocular depth frames.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihajlovic_M/0/1/0/all/0/1\">Marko Mihajlovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MASS: Multi-Attentional Semantic Segmentation of LiDAR Data for Dense Top-View Understanding. (arXiv:2107.00346v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00346","description":"<p>At the heart of all automated driving systems is the ability to sense the\nsurroundings, e.g., through semantic segmentation of LiDAR sequences, which\nexperienced a remarkable progress due to the release of large datasets such as\nSemanticKITTI and nuScenes-LidarSeg. While most previous works focus on sparse\nsegmentation of the LiDAR input, dense output masks provide self-driving cars\nwith almost complete environment information. In this paper, we introduce MASS\n- a Multi-Attentional Semantic Segmentation model specifically built for dense\ntop-view understanding of the driving scenes. Our framework operates on pillar-\nand occupancy features and comprises three attention-based building blocks: (1)\na keypoint-driven graph attention, (2) an LSTM-based attention computed from a\nvector embedding of the spatial input, and (3) a pillar-based attention,\nresulting in a dense 360-degree segmentation mask. With extensive experiments\non both, SemanticKITTI and nuScenes-LidarSeg, we quantitatively demonstrate the\neffectiveness of our model, outperforming the state of the art by 19.0% on\nSemanticKITTI and reaching 30.4% in mIoU on nuScenes-LidarSeg, where MASS is\nthe first work addressing the dense segmentation task. Furthermore, our\nmulti-attention model is shown to be very effective for 3D object detection\nvalidated on the KITTI-3D dataset, showcasing its high generalizability to\nother tasks related to 3D vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1\">Juncong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieder_F/0/1/0/all/0/1\">Frank Bieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidenreich_P/0/1/0/all/0/1\">Philipp Heidenreich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining EfficientNet and Vision Transformers for Video Deepfake Detection. (arXiv:2107.02612v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02612","description":"<p>Deepfakes are the result of digital manipulation to forge realistic yet fake\nimagery. With the astonishing advances in deep generative models, fake images\nor videos are nowadays obtained using variational autoencoders (VAEs) or\nGenerative Adversarial Networks (GANs). These technologies are becoming more\naccessible and accurate, resulting in fake videos that are very difficult to be\ndetected. Traditionally, Convolutional Neural Networks (CNNs) have been used to\nperform video deepfake detection, with the best results obtained using methods\nbased on EfficientNet B7. In this study, we focus on video deep fake detection\non faces, given that most methods are becoming extremely accurate in the\ngeneration of realistic human faces. Specifically, we combine various types of\nVision Transformers with a convolutional EfficientNet B0 used as a feature\nextractor, obtaining comparable results with some very recent methods that use\nVision Transformers. Differently from the state-of-the-art approaches, we use\nneither distillation nor ensemble methods. Furthermore, we present a\nstraightforward inference procedure based on a simple voting scheme for\nhandling multiple faces in the same video shot. The best model achieved an AUC\nof 0.951 and an F1 score of 88.0%, very close to the state-of-the-art on the\nDeepFake Detection Challenge (DFDC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coccomini_D/0/1/0/all/0/1\">Davide Coccomini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messina_N/0/1/0/all/0/1\">Nicola Messina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1\">Claudio Gennaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1\">Fabrizio Falchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MitoDet: Simple and robust mitosis detection. (arXiv:2109.01485v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.01485","description":"<p>Mitotic figure detection is a challenging task in digital pathology that has\na direct impact on therapeutic decisions. While automated methods often achieve\nacceptable results under laboratory conditions, they frequently fail in the\nclinical deployment phase. This problem can be mainly attributed to a\nphenomenon called domain shift. An important source of a domain shift is\nintroduced by different microscopes and their camera systems, which noticeably\nchange the color representation of digitized images. In this method description\nwe present our submitted algorithm for the Mitosis Domain Generalization\nChallenge, which employs a RetinaNet trained with strong data augmentation and\nachieves an F1 score of 0.7138 on the preliminary test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dexl_J/0/1/0/all/0/1\">Jakob Dexl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benz_M/0/1/0/all/0/1\">Michaela Benz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bruns_V/0/1/0/all/0/1\">Volker Bruns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuritcyn_P/0/1/0/all/0/1\">Petr Kuritcyn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wittenberg_T/0/1/0/all/0/1\">Thomas Wittenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Learned Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.03082","description":"<p>This paper proposes a Perceptual Learned Video Compression (PLVC) approach\nwith recurrent conditional GAN. In our approach, we employ the recurrent\nauto-encoder-based compression network as the generator, and the most\nimportantly, we propose a recurrent conditional discriminator, which judges raw\nand compressed video conditioned on both spatial and temporal features,\nincluding the latent representation, temporal motion and hidden states in\nrecurrent cells. This way, in the adversarial training, it pushes the generated\nvideo to be not only spatially photo-realistic but also temporally consistent\nwith groundtruth and coherent among video frames. The experimental results show\nthat the proposed PLVC model learns to compress video towards good perceptual\nquality at low bit-rate, and outperforms the official HEVC test model (HM\n16.20) and the previous learned approaches on several perceptual quality\nmetrics and user studies. The codes will be released at the project page:\nhttps://github.com/RenYang-home/PLVC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ren Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mesh convolutional neural networks for wall shear stress estimation in 3D artery models. (arXiv:2109.04797v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.04797","description":"<p>Computational fluid dynamics (CFD) is a valuable tool for personalised,\nnon-invasive evaluation of hemodynamics in arteries, but its complexity and\ntime-consuming nature prohibit large-scale use in practice. Recently, the use\nof deep learning for rapid estimation of CFD parameters like wall shear stress\n(WSS) on surface meshes has been investigated. However, existing approaches\ntypically depend on a hand-crafted re-parametrisation of the surface mesh to\nmatch convolutional neural network architectures. In this work, we propose to\ninstead use mesh convolutional neural networks that directly operate on the\nsame finite-element surface mesh as used in CFD. We train and evaluate our\nmethod on two datasets of synthetic coronary artery models with and without\nbifurcation, using a ground truth obtained from CFD simulation. We show that\nour flexible deep learning model can accurately predict 3D WSS vectors on this\nsurface mesh. Our method processes new meshes in less than 5 [s], consistently\nachieves a normalised mean absolute error of $\\leq$ 1.6 [%], and peaks at 90.5\n[%] median approximation accuracy over the held-out test set, comparing\nfavourably to previously published work. This demonstrates the feasibility of\nCFD surrogate modelling using mesh convolutional neural networks for\nhemodynamic parameter estimation in artery models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suk_J/0/1/0/all/0/1\">Julian Suk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1\">Pim de Haan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1\">Phillip Lippe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1\">Christoph Brune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolterink_J/0/1/0/all/0/1\">Jelmer M. Wolterink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S-Extension Patch: A simple and efficient way to extend an object detection model. (arXiv:2110.02670v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02670","description":"<p>While building convolutional network-based systems, the toll it takes to\ntrain the network is something that cannot be ignored. In cases where we need\nto append additional capabilities to the existing model, the attention\nimmediately goes towards retraining techniques. In this paper, I show how to\nleverage knowledge about the dataset to append the class faster while\nmaintaining the speed of inference as well as the accuracies; while reducing\nthe amount of time and data required. The method can extend a class in the\nexisting object detection model in 1/10th of the time compared to the other\nexisting methods. S-Extension patch not only offers faster training but also\nspeed and ease of adaptation, as it can be appended to any existing system,\ngiven it fulfills the similarity threshold condition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Dishant Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Depth Completion for Active Stereo. (arXiv:2110.03234v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03234","description":"<p>Active stereo systems are used in many robotic applications that require 3D\ninformation. These depth sensors, however, suffer from stereo artefacts and do\nnot provide dense depth estimates.In this work, we present the first\nself-supervised depth completion method for active stereo systems that predicts\naccurate dense depth maps. Our system leverages a feature-based visual inertial\nSLAM system to produce motion estimates and accurate (but sparse) 3D landmarks.\nThe 3D landmarks are used both as model input and as supervision during\ntraining. The motion estimates are used in our novel reconstruction loss that\nrelies on a combination of passive and active stereo frames, resulting in\nsignificant improvements in textureless areas that are common in indoor\nenvironments. Due to the nonexistence of publicly available active stereo\ndatasets, we release a real dataset together with additional information for a\npublicly available synthetic dataset (TartanAir [42]) needed for active depth\ncompletion and prediction. Through rigorous evaluations we show that our method\noutperforms state of the art on both datasets. Additionally we show how our\nmethod obtains more complete, and therefore safer, 3D maps when used in a\nrobotic platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Warburg_F/0/1/0/all/0/1\">Frederik Warburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Juarez_D/0/1/0/all/0/1\">Daniel Hernandez-Juarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarrio_J/0/1/0/all/0/1\">Juan Tarrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakhitov_A/0/1/0/all/0/1\">Alexander Vakhitov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonde_U/0/1/0/all/0/1\">Ujwal Bonde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcantarilla_P/0/1/0/all/0/1\">Pablo F. Alcantarilla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset Structural Index: Leveraging a machine's perspective towards visual data. (arXiv:2110.04070v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04070","description":"<p>With advances in vision and perception architectures, we have realized that\nworking with data is equally crucial, if not more, than the algorithms. Till\ntoday, we have trained machines based on our knowledge and perspective of the\nworld. The entire concept of Dataset Structural Index(DSI) revolves around\nunderstanding a machine`s perspective of the dataset. With DSI, I show two meta\nvalues with which we can get more information over a visual dataset and use it\nto optimize data, create better architectures, and have an ability to guess\nwhich model would work best. These two values are the Variety contribution\nratio and Similarity matrix. In the paper, I show many applications of DSI, one\nof which is how the same level of accuracy can be achieved with the same model\narchitectures trained over less amount of data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Dishant Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shared Visual Representations of Drawing for Communication: How do different biases affect human interpretability and intent?. (arXiv:2110.08203v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08203","description":"<p>We present an investigation into how representational losses can affect the\ndrawings produced by artificial agents playing a communication game. Building\nupon recent advances, we show that a combination of powerful pretrained encoder\nnetworks, with appropriate inductive biases, can lead to agents that draw\nrecognisable sketches, whilst still communicating well. Further, we start to\ndevelop an approach to help automatically analyse the semantic content being\nconveyed by a sketch and demonstrate that current approaches to inducing\nperceptual biases lead to a notion of objectness being a key feature despite\nthe agent training being self-supervised.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihai_D/0/1/0/all/0/1\">Daniela Mihai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BINAS: Bilinear Interpretable Neural Architecture Search. (arXiv:2110.12399v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.12399","description":"<p>Practical use of neural networks often involves requirements on latency,\nenergy and memory among others. A popular approach to find networks under such\nrequirements is through constrained Neural Architecture Search (NAS). However,\nprevious methods use complicated predictors for the accuracy of the network.\nThose predictors are hard to interpret and sensitive to many hyperparameters to\nbe tuned, hence, the resulting accuracy of the generated models is often\nharmed. In this work we resolve this by introducing Bilinear Interpretable\nNeural Architecture Search (BINAS), that is based on an accurate and simple\nbilinear formulation of both an accuracy estimator and the expected resource\nrequirement, together with a scalable search method with theoretical\nguarantees. The simplicity of our proposed estimator together with the\nintuitive way it is constructed bring interpretability through many insights\nabout the contribution of different design choices. For example, we find that\nin the examined search space, adding depth and width is more effective at\ndeeper stages of the network and at the beginning of each resolution stage. Our\nexperiments show that BINAS generates comparable to or better architectures\nthan other state-of-the-art NAS methods within a reduced marginal search cost,\nwhile strictly satisfying the resource constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayman_N/0/1/0/all/0/1\">Niv Nayman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aflalo_Y/0/1/0/all/0/1\">Yonathan Aflalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Transformers Excel at Class-agnostic Object Detection. (arXiv:2111.11430v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11430","description":"<p>What constitutes an object? This has been a long-standing question in\ncomputer vision. Towards this goal, numerous learning-free and learning-based\napproaches have been developed to score objectness. However, they generally do\nnot scale well across new domains and for unseen objects. In this paper, we\nadvocate that existing methods lack a top-down supervision signal governed by\nhuman-understandable semantics. To bridge this gap, we explore recent\nMulti-modal Vision Transformers (MViT) that have been trained with aligned\nimage-text pairs. Our extensive experiments across various domains and novel\nobjects show the state-of-the-art performance of MViTs to localize generic\nobjects in images. Based on these findings, we develop an efficient and\nflexible MViT architecture using multi-scale feature processing and deformable\nself-attention that can adaptively generate proposals given a specific language\nquery. We show the significance of MViT proposals in a diverse range of\napplications including open-world object detection, salient and camouflage\nobject detection, supervised and self-supervised detection tasks. Further,\nMViTs offer enhanced interactability with intelligible text queries. Code:\nhttps://git.io/J1HPY.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maaz_M/0/1/0/all/0/1\">Muhammad Maaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_H/0/1/0/all/0/1\">Hanoona Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1\">Rao Muhammad Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector. (arXiv:2112.06102v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2112.06102","description":"<p>Vertebrate retinas are highly-efficient in processing trivial visual tasks\nsuch as detecting moving objects, yet a complex task for modern computers. The\ndetection of object motion is done by specialised retinal ganglion cells named\nObject-motion-sensitive ganglion cells (OMS-GC). OMS-GC process continuous\nsignals and generate spike patterns that are post-processed by the Visual\nCortex. The Neuromorphic Hybrid Spiking Motion Detector (NeuroHSMD) proposed in\nthis work accelerates the HSMD algorithm using Field-Programmable Gate Arrays\n(FPGAs). The Hybrid Spiking Motion Detector (HSMD) algorithm was the first\nhybrid algorithm to enhance dynamic background subtraction (DBS) algorithms\nwith a customised 3-layer spiking neural network (SNN) that generates OMS-GC\nspiking-like responses. The NeuroHSMD algorithm was compared against the HSMD\nalgorithm, using the same 2012 change detection (CDnet2012) and 2014 change\ndetection (CDnet2014) benchmark datasets. The results show that the NeuroHSMD\nhas produced the same results as the HSMD algorithm in real-time without\ndegradation of quality. Moreover, the NeuroHSMD proposed in this paper was\ncompletely implemented in Open Computer Language (OpenCL) and therefore is\neasily replicated in other devices such as Graphical Processor Units (GPUs) and\nclusters of Central Processor Units (CPUs).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1\">Pedro Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomou_A/0/1/0/all/0/1\">Andreas Oikonomou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PrintsGAN: Synthetic Fingerprint Generator. (arXiv:2201.03674v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03674","description":"<p>A major impediment to researchers working in the area of fingerprint\nrecognition is the lack of publicly available, large-scale, fingerprint\ndatasets. The publicly available datasets that do exist contain very few\nidentities and impressions per finger. This limits research on a number of\ntopics, including e.g., using deep networks to learn fixed length fingerprint\nembeddings. Therefore, we propose PrintsGAN, a synthetic fingerprint generator\ncapable of generating unique fingerprints along with multiple impressions for a\ngiven fingerprint. Using PrintsGAN, we synthesize a database of 525k\nfingerprints (35K distinct fingers, each with 15 impressions). Next, we show\nthe utility of the PrintsGAN generated dataset by training a deep network to\nextract a fixed-length embedding from a fingerprint. In particular, an\nembedding model trained on our synthetic fingerprints and fine-tuned on a small\nnumber of publicly available real fingerprints (25K prints from NIST SD302)\nobtains a TAR of 87.03% @ FAR=0.01% on the NIST SD4 database (a boost from\nTAR=73.37% when only trained on NIST SD302). Prevailing synthetic fingerprint\ngeneration methods do not enable such performance gains due to i) lack of\nrealism or ii) inability to generate multiple impressions per finger. We plan\nto release our database of synthetic fingerprints to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Engelsma_J/0/1/0/all/0/1\">Joshua J. Engelsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosz_S/0/1/0/all/0/1\">Steven A. Grosz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiview Transformers for Video Recognition. (arXiv:2201.04288v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04288","description":"<p>Video understanding requires reasoning at multiple spatiotemporal resolutions\n-- from short fine-grained motions to events taking place over longer\ndurations. Although transformer architectures have recently advanced the\nstate-of-the-art, they have not explicitly modelled different spatiotemporal\nresolutions. To this end, we present Multiview Transformers for Video\nRecognition (MTV). Our model consists of separate encoders to represent\ndifferent views of the input video with lateral connections to fuse information\nacross views. We present thorough ablation studies of our model and show that\nMTV consistently performs better than single-view counterparts in terms of\naccuracy and computational cost across a range of model sizes. Furthermore, we\nachieve state-of-the-art results on five standard datasets, and improve even\nfurther with large-scale pretraining. We will release code and pretrained\ncheckpoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1\">Xuehan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Get your Foes Fooled: Proximal Gradient Split Learning for Defense against Model Inversion Attacks on IoMT data. (arXiv:2201.04569v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2201.04569","description":"<p>The past decade has seen a rapid adoption of Artificial Intelligence (AI),\nspecifically the deep learning networks, in Internet of Medical Things (IoMT)\necosystem. However, it has been shown recently that the deep learning networks\ncan be exploited by adversarial attacks that not only make IoMT vulnerable to\nthe data theft but also to the manipulation of medical diagnosis. The existing\nstudies consider adding noise to the raw IoMT data or model parameters which\nnot only reduces the overall performance concerning medical inferences but also\nis ineffective to the likes of deep leakage from gradients method. In this\nwork, we propose proximal gradient split learning (PSGL) method for defense\nagainst the model inversion attacks. The proposed method intentionally attacks\nthe IoMT data when undergoing the deep neural network training process at\nclient side. We propose the use of proximal gradient method to recover gradient\nmaps and a decision-level fusion strategy to improve the recognition\nperformance. Extensive analysis show that the PGSL not only provides effective\ndefense mechanism against the model inversion attacks but also helps in\nimproving the recognition performance on publicly available datasets. We report\n17.9$\\%$ and 36.9$\\%$ gains in accuracy over reconstructed and adversarial\nattacked images, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khowaja_S/0/1/0/all/0/1\">Sunder Ali Khowaja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Ik Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_K/0/1/0/all/0/1\">Kapal Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarwar_M/0/1/0/all/0/1\">Muhammad Aslam Jarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qureshi_N/0/1/0/all/0/1\">Nawab Muhammad Faseeh Qureshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Cross-Modality Retinal Vessel Segmentation via Disentangling Representation Style Transfer and Collaborative Consistency Learning. (arXiv:2201.04812v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.04812","description":"<p>Various deep learning models have been developed to segment anatomical\nstructures from medical images, but they typically have poor performance when\ntested on another target domain with different data distribution. Recently,\nunsupervised domain adaptation methods have been proposed to alleviate this\nso-called domain shift issue, but most of them are designed for scenarios with\nrelatively small domain shifts and are likely to fail when encountering a large\ndomain gap. In this paper, we propose DCDA, a novel cross-modality unsupervised\ndomain adaptation framework for tasks with large domain shifts, e.g.,\nsegmenting retinal vessels from OCTA and OCT images. DCDA mainly consists of a\ndisentangling representation style transfer (DRST) module and a collaborative\nconsistency learning (CCL) module. DRST decomposes images into content\ncomponents and style codes and performs style transfer and image\nreconstruction. CCL contains two segmentation models, one for source domain and\nthe other for target domain. The two models use labeled data (together with the\ncorresponding transferred images) for supervised learning and perform\ncollaborative consistency learning on unlabeled data. Each model focuses on the\ncorresponding single domain and aims to yield an expertized domain-specific\nsegmentation model. Through extensive experiments on retinal vessel\nsegmentation, our framework achieves Dice scores close to target-trained oracle\nboth from OCTA to OCT and from OCT to OCTA, significantly outperforming other\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Linkai Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Ziqi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Landscape of Neural Architecture Search across sensors: how much do they differ ?. (arXiv:2201.06321v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.06321","description":"<p>With the rapid rise of neural architecture search, the ability to understand\nits complexity from the perspective of a search algorithm is desirable.\nRecently, Traor\\'e et al. have proposed the framework of Fitness Landscape\nFootprint to help describe and compare neural architecture search problems. It\nattempts at describing why a search strategy might be successful, struggle or\nfail on a target task. Our study leverages this methodology in the context of\nsearching across sensors, including sensor data fusion. In particular, we apply\nthe Fitness Landscape Footprint to the real-world image classification problem\nof So2Sat LCZ42, in order to identify the most beneficial sensor to our neural\nnetwork hyper-parameter optimization problem. From the perspective of\ndistributions of fitness, our findings indicate a similar behaviour of the\nsearch space for all sensors: the longer the training time, the larger the\noverall fitness, and more flatness in the landscapes (less ruggedness and\ndeviation). Regarding sensors, the better the fitness they enable (Sentinel-2),\nthe better the search trajectories (smoother, higher persistence). Results also\nindicate very similar search behaviour for sensors that can be decently fitted\nby the search space (Sentinel-2 and fusion).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Traore_K/0/1/0/all/0/1\">Kalifou Ren&#xe9; Traor&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camero_A/0/1/0/all/0/1\">Andr&#xe9;s Camero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can We Find Neurons that Cause Unrealistic Images in Deep Generative Networks?. (arXiv:2201.06346v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06346","description":"<p>Even though image generation with Generative Adversarial Networks has been\nshowing remarkable ability to generate high-quality images, GANs do not always\nguarantee photorealistic images will be generated. Sometimes they generate\nimages that have defective or unnatural objects, which are referred to as\n'artifacts'. Research to determine why the artifacts emerge and how they can be\ndetected and removed has not been sufficiently carried out. To analyze this, we\nfirst hypothesize that rarely activated neurons and frequently activated\nneurons have different purposes and responsibilities for the progress of\ngenerating images. By analyzing the statistics and the roles for those neurons,\nwe empirically show that rarely activated neurons are related to failed results\nof making diverse objects and lead to artifacts. In addition, we suggest a\ncorrection method, called 'sequential ablation', to repair the defective part\nof the generated images without complex computational cost and manual efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hwanil Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">Wonjoon Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaesik Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-fidelity 3D Model Compression based on Key Spheres. (arXiv:2201.07486v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07486","description":"<p>In recent years, neural signed distance function (SDF) has become one of the\nmost effective representation methods for 3D models. By learning continuous\nSDFs in 3D space, neural networks can predict the distance from a given query\nspace point to its closest object surface,whose positive and negative signs\ndenote inside and outside of the object, respectively. Training a specific\nnetwork for each 3D model, which individually embeds its shape, can realize\ncompressed representation of objects by storing fewer network (and possibly\nlatent) parameters. Consequently, reconstruction through network inference and\nsurface recovery can be achieved. In this paper, we propose an SDF prediction\nnetwork using explicit key spheres as input. Key spheres are extracted from the\ninternal space of objects, whose centers either have relatively larger SDF\nvalues (sphere radii), or are located at essential positions. By inputting the\nspatial information of multiple spheres which imply different local shapes, the\nproposed method can significantly improve the reconstruction accuracy with a\nnegligible storage cost. Compared to previous works, our method achieves the\nhigh-fidelity and high-compression 3D object coding and reconstruction.\nExperiments conducted on three datasets verify the superior performance of our\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shen Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look Closer: Bridging Egocentric and Third-Person Views with Transformers for Robotic Manipulation. (arXiv:2201.07779v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2201.07779","description":"<p>Learning to solve precision-based manipulation tasks from visual feedback\nusing Reinforcement Learning (RL) could drastically reduce the engineering\nefforts required by traditional robot systems. However, performing fine-grained\nmotor control from visual inputs alone is challenging, especially with a static\nthird-person camera as often used in previous work. We propose a setting for\nrobotic manipulation in which the agent receives visual feedback from both a\nthird-person camera and an egocentric camera mounted on the robot's wrist.\nWhile the third-person camera is static, the egocentric camera enables the\nrobot to actively control its vision to aid in precise manipulation. To fuse\nvisual information from both cameras effectively, we additionally propose to\nuse Transformers with a cross-view attention mechanism that models spatial\nattention from one view to another (and vice-versa), and use the learned\nfeatures as input to an RL policy. Our method improves learning over strong\nsingle-view and multi-view baselines, and successfully transfers to a set of\nchallenging manipulation tasks on a real robot with uncalibrated cameras, no\naccess to state information, and a high degree of task variability. In a hammer\nmanipulation task, our method succeeds in 75% of trials versus 38% and 13% for\nmulti-view and single-view baselines, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jangir_R/0/1/0/all/0/1\">Rishabh Jangir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1\">Nicklas Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_S/0/1/0/all/0/1\">Sambaran Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Mohit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fantastic Data and How to Query Them. (arXiv:2201.05026v1 [cs.AI] CROSS LISTED)","link":"http://arxiv.org/abs/2201.05026","description":"<p>It is commonly acknowledged that the availability of the huge amount of\n(training) data is one of the most important factors for many recent advances\nin Artificial Intelligence (AI). However, datasets are often designed for\nspecific tasks in narrow AI sub areas and there is no unified way to manage and\naccess them. This not only creates unnecessary overheads when training or\ndeploying Machine Learning models but also limits the understanding of the\ndata, which is very important for data-centric AI. In this paper, we present\nour vision about a unified framework for different datasets so that they can be\nintegrated and queried easily, e.g., using standard query languages. We\ndemonstrate this in our ongoing work to create a framework for datasets in\nComputer Vision and show its advantages in different scenarios. Our\ndemonstration is available at https://vision.semkg.org.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trung-Kien Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Tuan_A/0/1/0/all/0/1\">Anh Le-Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Duc_M/0/1/0/all/0/1\">Manh Nguyen-Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jicheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Phuoc_D/0/1/0/all/0/1\">Danh Le-Phuoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}